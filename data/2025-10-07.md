<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 169]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CG](#cs.CG) [Total: 1]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.SY](#eess.SY) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [eess.IV](#eess.IV) [Total: 7]
- [cs.GR](#cs.GR) [Total: 10]
- [cs.LG](#cs.LG) [Total: 17]
- [physics.optics](#physics.optics) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SoC-DT: Standard-of-Care Aligned Digital Twins for Patient-Specific Tumor Dynamics](https://arxiv.org/abs/2510.03287)
*Moinak Bhattacharya,Gagandeep Singh,Prateek Prasanna*

Main category: cs.CV

TL;DR: 提出了一种名为SoC-DT的可微分框架，用于预测标准治疗下肿瘤轨迹，结合反应扩散模型、治疗干预和个性化因素，在合成和真实胶质瘤数据上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 准确预测标准治疗下的肿瘤轨迹是肿瘤学的关键需求，但传统反应扩散模型无法捕捉异质性治疗范式下的肿瘤动态，需要能够真实模拟标准治疗干预的计算框架。

Method: SoC-DT框架统一了反应扩散肿瘤生长模型、离散标准治疗干预（手术、化疗、放疗）以及基因组和人口统计学个性化，并提出了IMEX-SoC隐式-显式指数时间差分求解器确保稳定性。

Result: 在合成数据和真实世界胶质瘤数据上的评估显示，SoC-DT在预测肿瘤动态方面始终优于经典PDE基线和纯数据驱动的神经模型。

Conclusion: SoC-DT通过将机械可解释性与现代可微分求解器相结合，为肿瘤学中的患者特异性数字孪生建立了原则性基础，实现了生物学一致的肿瘤动态估计。

Abstract: Accurate prediction of tumor trajectories under standard-of-care (SoC)
therapies remains a major unmet need in oncology. This capability is essential
for optimizing treatment planning and anticipating disease progression.
Conventional reaction-diffusion models are limited in scope, as they fail to
capture tumor dynamics under heterogeneous therapeutic paradigms. There is
hence a critical need for computational frameworks that can realistically
simulate SoC interventions while accounting for inter-patient variability in
genomics, demographics, and treatment regimens. We introduce Standard-of-Care
Digital Twin (SoC-DT), a differentiable framework that unifies
reaction-diffusion tumor growth models, discrete SoC interventions (surgery,
chemotherapy, radiotherapy) along with genomic and demographic personalization
to predict post-treatment tumor structure on imaging. An implicit-explicit
exponential time-differencing solver, IMEX-SoC, is also proposed, which ensures
stability, positivity, and scalability in SoC treatment situations. Evaluated
on both synthetic data and real world glioma data, SoC-DT consistently
outperforms classical PDE baselines and purely data-driven neural models in
predicting tumor dynamics. By bridging mechanistic interpretability with modern
differentiable solvers, SoC-DT establishes a principled foundation for
patient-specific digital twins in oncology, enabling biologically consistent
tumor dynamics estimation. Code will be made available upon acceptance.

</details>


### [2] [Visualizing Celebrity Dynamics in Video Content: A Proposed Approach Using Face Recognition Timestamp Data](https://arxiv.org/abs/2510.03292)
*Doğanay Demir,İlknur Durgar Elkahlout*

Main category: cs.CV

TL;DR: 提出了一种结合分布式多GPU推理系统和交互式可视化平台的混合框架，用于分析视频中名人动态。


<details>
  <summary>Details</summary>
Motivation: 在视频内容主导的时代，理解视频结构和动态变得日益重要，需要高效处理大量视频数据并提供多维分析。

Method: 使用优化的ONNX模型、异构批量推理和高吞吐量并行处理，生成带时间戳的出现记录，并通过多种可视化图表展示分析结果。

Result: 系统能够有效处理大规模视频数据，生成包括出现频率、时长分析、共现矩阵等在内的多维度可视化分析。

Conclusion: 该框架通过结合分布式识别和结构化可视化分析，为娱乐分析、内容创作策略和观众参与研究提供了新的可能性。

Abstract: In an era dominated by video content, understanding its structure and
dynamics has become increasingly important. This paper presents a hybrid
framework that combines a distributed multi-GPU inference system with an
interactive visualization platform for analyzing celebrity dynamics in video
episodes. The inference framework efficiently processes large volumes of video
data by leveraging optimized ONNX models, heterogeneous batch inference, and
high-throughput parallelism, ensuring scalable generation of timestamped
appearance records. These records are then transformed into a comprehensive
suite of visualizations, including appearance frequency charts, duration
analyses, pie charts, co-appearance matrices, network graphs, stacked area
charts, seasonal comparisons, and heatmaps. Together, these visualizations
provide multi-dimensional insights into video content, revealing patterns in
celebrity prominence, screen-time distribution, temporal dynamics,
co-appearance relationships, and intensity across episodes and seasons. The
interactive nature of the system allows users to dynamically explore data,
identify key moments, and uncover evolving relationships between individuals.
By bridging distributed recognition with structured, visually-driven analytics,
this work enables new possibilities for entertainment analytics, content
creation strategies, and audience engagement studies.

</details>


### [3] [Domain-Robust Marine Plastic Detection Using Vision Models](https://arxiv.org/abs/2510.03294)
*Saanvi Kataria*

Main category: cs.CV

TL;DR: 该研究比较了不同模型在水下塑料垃圾跨域检测中的性能，发现轻量级CNN模型MobileNetV2表现最佳，而零样本模型CLIP和Gemini在精确度和召回率上各有优势。


<details>
  <summary>Details</summary>
Motivation: 海洋塑料污染是严峻的环境威胁，需要可靠的水下垃圾自动检测系统。但视觉系统在一个数据集上训练后，往往因域偏移问题在新图像上性能下降。

Method: 研究比较了卷积神经网络（MobileNetV2、ResNet-18、EfficientNet-B0）和视觉变换器（DeiT-Tiny、ViT-B16）在标记水下数据集上的训练效果，并在跨域测试集上评估。同时评估了零样本模型CLIP ViT-L14和Gemini 2.0 Flash。

Result: 轻量级MobileNetV2取得最佳跨域性能（F1分数0.97），所有微调模型精确度均达99%左右，但召回率不同。零样本CLIP召回率约80%但精确度仅56%，Gemini则精确度99%但召回率81%。

Conclusion: 紧凑型CNN通过监督训练能有效泛化到跨域水下检测，而大型预训练视觉语言模型提供互补优势，常见错误包括与珊瑚纹理、悬浮颗粒和镜面反射的混淆。

Abstract: Marine plastic pollution is a pressing environmental threat, making reliable
automation for underwater debris detection essential. However, vision systems
trained on one dataset often degrade on new imagery due to domain shift. This
study benchmarks models for cross-domain robustness, training convolutional
neural networks - CNNs (MobileNetV2, ResNet-18, EfficientNet-B0) and vision
transformers (DeiT-Tiny, ViT-B16) on a labeled underwater dataset and then
evaluates them on a balanced cross-domain test set built from plastic-positive
images drawn from a different source and negatives from the training domain.
Two zero-shot models were assessed, CLIP ViT-L14 and Google's Gemini 2.0 Flash,
that leverage pretraining to classify images without fine-tuning. Results show
the lightweight MobileNetV2 delivers the strongest cross-domain performance (F1
0.97), surpassing larger models. All fine-tuned models achieved high Precision
(around 99%), but differ in Recall, indicating varying sensitivity to plastic
instances. Zero-shot CLIP is comparatively sensitive (Recall around 80%) yet
prone to false positives (Precision around 56%), whereas Gemini exhibits the
inverse profile (Precision around 99%, Recall around 81%). Error analysis
highlights recurring confusions with coral textures, suspended particulates,
and specular glare. Overall, compact CNNs with supervised training can
generalize effectively for cross-domain underwater detection, while large
pretrained vision-language models provide complementary strengths.

</details>


### [4] [Multimodal Arabic Captioning with Interpretable Visual Concept Integration](https://arxiv.org/abs/2510.03295)
*Passant Elchafei,Amany Fashwan*

Main category: cs.CV

TL;DR: VLCAP是一个阿拉伯语图像描述框架，通过CLIP视觉标签检索和多模态文本生成相结合，生成可解释且文化一致的阿拉伯语图像描述。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端图像描述方法缺乏可解释性，VLCAP旨在通过视觉概念提取和标签检索来增强阿拉伯语图像描述的可解释性和文化一致性。

Method: 使用三种多语言编码器（mCLIP、AraCLIP、Jina V4）进行视觉标签检索，构建包含训练数据和Visual Genome数据集的混合词汇表，然后将检索到的标签转换为阿拉伯语提示，与图像一起输入视觉语言模型（Qwen-VL和Gemini Pro Vision）生成描述。

Result: mCLIP + Gemini Pro Vision组合在BLEU-1（5.34%）和余弦相似度（60.01%）上表现最佳，而AraCLIP + Qwen-VL在LLM-judge评分（36.33%）上最高。

Conclusion: VLCAP的可解释流水线能够生成文化一致且上下文准确的阿拉伯语图像描述，为阿拉伯语多模态理解提供了有效解决方案。

Abstract: We present VLCAP, an Arabic image captioning framework that integrates
CLIP-based visual label retrieval with multimodal text generation. Rather than
relying solely on end-to-end captioning, VLCAP grounds generation in
interpretable Arabic visual concepts extracted with three multilingual
encoders, mCLIP, AraCLIP, and Jina V4, each evaluated separately for label
retrieval. A hybrid vocabulary is built from training captions and enriched
with about 21K general domain labels translated from the Visual Genome dataset,
covering objects, attributes, and scenes. The top-k retrieved labels are
transformed into fluent Arabic prompts and passed along with the original image
to vision-language models. In the second stage, we tested Qwen-VL and Gemini
Pro Vision for caption generation, resulting in six encoder-decoder
configurations. The results show that mCLIP + Gemini Pro Vision achieved the
best BLEU-1 (5.34%) and cosine similarity (60.01%), while AraCLIP + Qwen-VL
obtained the highest LLM-judge score (36.33%). This interpretable pipeline
enables culturally coherent and contextually accurate Arabic captions.

</details>


### [5] [Convolutional Neural Nets vs Vision Transformers: A SpaceNet Case Study with Balanced vs Imbalanced Regimes](https://arxiv.org/abs/2510.03297)
*Akshar Gothi*

Main category: cs.CV

TL;DR: 对比卷积神经网络EfficientNet-B0和Vision Transformer在SpaceNet数据集上的表现，分析在标签分布不平衡和平衡两种情况下的性能差异


<details>
  <summary>Details</summary>
Motivation: 比较不同架构的深度学习模型在遥感图像分类任务中的表现，特别是在处理类别不平衡问题时的能力差异

Method: 使用相同的预处理（224x224分辨率，ImageNet标准化）、轻量级数据增强和40个epoch的训练预算，在NVIDIA P100上对比EfficientNet-B0和ViT-Base模型

Result: 在不平衡数据集上，EfficientNet-B0达到93%测试准确率且延迟更低；在平衡数据集上，两者表现都很强，EfficientNet-B0达到99%准确率，ViT-Base保持竞争力

Conclusion: 数据平衡可以缩小架构间的性能差距，但CNN在效率方面仍保持优势；同时发布了完整的实验数据以支持可复现性

Abstract: We present a controlled comparison of a convolutional neural network
(EfficientNet-B0) and a Vision Transformer (ViT-Base) on SpaceNet under two
label-distribution regimes: a naturally imbalanced five-class split and a
balanced-resampled split with 700 images per class (70:20:10 train/val/test).
With matched preprocessing (224x224, ImageNet normalization), lightweight
augmentations, and a 40-epoch budget on a single NVIDIA P100, we report
accuracy, macro-F1, balanced accuracy, per-class recall, and deployment metrics
(model size and latency). On the imbalanced split, EfficientNet-B0 reaches 93%
test accuracy with strong macro-F1 and lower latency; ViT-Base is competitive
at 93% with a larger parameter count and runtime. On the balanced split, both
models are strong; EfficientNet-B0 reaches 99% while ViT-Base remains
competitive, indicating that balancing narrows architecture gaps while CNNs
retain an efficiency edge. We release manifests, logs, and per-image
predictions to support reproducibility.

</details>


### [6] [A Comprehensive Review on Artificial Intelligence Empowered Solutions for Enhancing Pedestrian and Cyclist Safety](https://arxiv.org/abs/2510.03314)
*Shucheng Zhang,Yan Shi,Bingzhang Wang,Yuang Zhang,Muhammad Monjurul Karim,Kehua Chen,Chenxi Liu,Mehrdad Nasri,Yinhai Wang*

Main category: cs.CV

TL;DR: 本文对基于摄像头的AI感知系统在弱势道路使用者安全方面的最新进展进行了综述，重点分析了检测分类、跟踪重识别、轨迹预测和意图识别四个核心任务，并指出了数据、模型和部署方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有关于AI在弱势道路使用者保护应用的综述主要关注检测任务，对其他视觉任务覆盖有限，而全面理解弱势道路使用者需要多任务协同。

Method: 系统回顾了过去五年基于摄像头的AI感知系统在弱势道路使用者安全方面的研究进展，重点分析了四个核心任务：检测分类、跟踪重识别、轨迹预测和意图识别。

Result: 建立了AI赋能的主动式弱势道路使用者保护系统的技术框架，识别了当前研究的主要进展和局限性。

Conclusion: 通过将视觉AI进展与实际部署考虑相结合，本综述为开发下一代感知系统以增强弱势道路使用者安全提供了基础参考，并指出了未来研究的四个主要挑战方向。

Abstract: Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and
cyclists, remains a critical global challenge, as conventional
infrastructure-based measures often prove inadequate in dynamic urban
environments. Recent advances in artificial intelligence (AI), particularly in
visual perception and reasoning, open new opportunities for proactive and
context-aware VRU protection. However, existing surveys on AI applications for
VRUs predominantly focus on detection, offering limited coverage of other
vision-based tasks that are essential for comprehensive VRU understanding and
protection. This paper presents a state-of-the-art review of recent progress in
camera-based AI sensing systems for VRU safety, with an emphasis on
developments from the past five years and emerging research trends. We
systematically examine four core tasks, namely detection and classification,
tracking and reidentification, trajectory prediction, and intent recognition
and prediction, which together form the backbone of AI-empowered proactive
solutions for VRU protection in intelligent transportation systems. To guide
future research, we highlight four major open challenges from the perspectives
of data, model, and deployment. By linking advances in visual AI with practical
considerations for real-world implementation, this survey aims to provide a
foundational reference for the development of next-generation sensing systems
to enhance VRU safety.

</details>


### [7] [The View From Space: Navigating Instrumentation Differences with EOFMs](https://arxiv.org/abs/2510.03316)
*Ryan P. Demilt,Nicholas LaHaye,Karis Tenneson*

Main category: cs.CV

TL;DR: EOFMs的表示空间对传感器架构高度敏感，当前EOFMs设计存在跨模态应用的陷阱


<details>
  <summary>Details</summary>
Motivation: 现有EOFMs大多在单一模态数据上训练，但被应用于跨模态任务，不清楚不同传感器架构对EOFMs内部表示的影响

Method: 分析EOFMs表示空间对传感器架构的敏感性

Result: EOFMs的表示空间高度依赖传感器架构，跨模态直接应用存在问题

Conclusion: 理解传感器架构差异对EOFMs表示的影响，为模型开发者和用户提供了重要指导，强调了基于稳健遥感科学的社区发展

Abstract: Earth Observation Foundation Models (EOFMs) have exploded in prevalence as
tools for processing the massive volumes of remotely sensed and other earth
observation data, and for delivering impact on the many essential earth
monitoring tasks. An emerging trend posits using the outputs of pre-trained
models as 'embeddings' which summarize high dimensional data to be used for
generic tasks such as similarity search and content-specific queries. However,
most EOFM models are trained only on single modalities of data and then applied
or benchmarked by matching bands across different modalities. It is not clear
from existing work what impact diverse sensor architectures have on the
internal representations of the present suite of EOFMs. We show in this work
that the representation space of EOFMs is highly sensitive to sensor
architecture and that understanding this difference gives a vital perspective
on the pitfalls of current EOFM design and signals for how to move forward as
model developers, users, and a community guided by robust remote-sensing
science.

</details>


### [8] [Photorealistic Inpainting for Perturbation-based Explanations in Ecological Monitoring](https://arxiv.org/abs/2510.03317)
*Günel Aghakishiyeva,Jiayi Zhou,Saagar Arya,James David Poling,Holly R. Houliston,Jamie N. Womble,David W. Johnston,Brinnae Bent*

Main category: cs.CV

TL;DR: 提出了一种基于修复引导的扰动解释技术，用于生态监测中的视觉模型，通过生成逼真的局部编辑来揭示模型预测的关键形态特征。


<details>
  <summary>Details</summary>
Motivation: 生态监测中自动化视觉模型的预测不透明性限制了信任和实际应用，需要更直观的解释方法来支持AI在生态学中的可信部署。

Method: 使用修复引导的扰动技术，结合Segment-Anything-Model精炼的掩码，进行对象移除/替换和背景替换等干预，生成保持场景上下文的逼真编辑。

Result: 该方法能够定位诊断性结构，避免传统扰动常见的删除伪影，产生生态学相关的见解，支持专家验证。

Conclusion: 该解释技术为生态学AI应用提供了更可信的解释方法，有助于提高模型透明度和领域专家的接受度。

Abstract: Ecological monitoring is increasingly automated by vision models, yet opaque
predictions limit trust and field adoption. We present an inpainting-guided,
perturbation-based explanation technique that produces photorealistic,
mask-localized edits that preserve scene context. Unlike masking or blurring,
these edits stay in-distribution and reveal which fine-grained morphological
cues drive predictions in tasks such as species recognition and trait
attribution. We demonstrate the approach on a YOLOv9 detector fine-tuned for
harbor seal detection in Glacier Bay drone imagery, using
Segment-Anything-Model-refined masks to support two interventions: (i) object
removal/replacement (e.g., replacing seals with plausible ice/water or boats)
and (ii) background replacement with original animals composited onto new
scenes. Explanations are assessed by re-scoring perturbed images (flip rate,
confidence drop) and by expert review for ecological plausibility and
interpretability. The resulting explanations localize diagnostic structures,
avoid deletion artifacts common to traditional perturbations, and yield
domain-relevant insights that support expert validation and more trustworthy
deployment of AI in ecology.

</details>


### [9] [Advances in Medical Image Segmentation: A Comprehensive Survey with a Focus on Lumbar Spine Applications](https://arxiv.org/abs/2510.03318)
*Ahmed Kabil,Ghada Khoriba,Mina Yousef,Essam A. Rashed*

Main category: cs.CV

TL;DR: 这是一篇关于医学图像分割方法的系统性综述论文，涵盖了从传统图像处理技术到现代深度学习方法的全面调研，特别关注了腰椎分割的案例研究。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割是医学图像分析的核心技术，在精确诊断、治疗规划和病情监测中起着关键作用。本文旨在弥合传统图像处理技术与现代深度学习方法之间的差距，为研究者提供全面的技术概览。

Method: 采用系统性综述方法，涵盖了阈值分割、边缘检测、区域分割、聚类算法、模型驱动技术，以及CNN、FCN、U-Net等深度学习架构，还包括注意力机制、半监督学习、GAN和Transformer等先进技术。

Result: 论文系统地整理了医学图像分割领域的技术发展脉络，识别了当前的研究趋势和挑战，并通过腰椎分割案例展示了实际应用中的问题和进展。

Conclusion: 尽管医学图像分割取得了显著进展，但仍面临数据集偏差、领域适应、模型可解释性以及临床工作流集成等关键挑战，需要进一步研究解决。

Abstract: Medical Image Segmentation (MIS) stands as a cornerstone in medical image
analysis, playing a pivotal role in precise diagnostics, treatment planning,
and monitoring of various medical conditions. This paper presents a
comprehensive and systematic survey of MIS methodologies, bridging the gap
between traditional image processing techniques and modern deep learning
approaches. The survey encompasses thresholding, edge detection, region-based
segmentation, clustering algorithms, and model-based techniques while also
delving into state-of-the-art deep learning architectures such as Convolutional
Neural Networks (CNNs), Fully Convolutional Networks (FCNs), and the widely
adopted U-Net and its variants. Moreover, integrating attention mechanisms,
semi-supervised learning, generative adversarial networks (GANs), and
Transformer-based models is thoroughly explored. In addition to covering
established methods, this survey highlights emerging trends, including hybrid
architectures, cross-modality learning, federated and distributed learning
frameworks, and active learning strategies, which aim to address challenges
such as limited labeled datasets, computational complexity, and model
generalizability across diverse imaging modalities. Furthermore, a specialized
case study on lumbar spine segmentation is presented, offering insights into
the challenges and advancements in this relatively underexplored anatomical
region. Despite significant progress in the field, critical challenges persist,
including dataset bias, domain adaptation, interpretability of deep learning
models, and integration into real-world clinical workflows.

</details>


### [10] [DECOR: Deep Embedding Clustering with Orientation Robustness](https://arxiv.org/abs/2510.03328)
*Fiona Victoria Stanley Jothiraj,Arunaggiri Pandian Karunanidhi,Seth A. Eichmeyer*

Main category: cs.CV

TL;DR: DECOR是一种具有方向鲁棒性的深度聚类框架，用于对晶圆图中的复杂缺陷模式进行一致聚类，无需手动调整即可在MixedWM38数据集上发现聚类，并能处理晶圆图的方向变化。


<details>
  <summary>Details</summary>
Motivation: 半导体制造中早期检测晶圆缺陷对产品良率优化至关重要，但原始晶圆数据复杂、无标签、不平衡且单个晶圆可能包含多个缺陷，需要设计在非理想数据条件下仍可靠的聚类方法。

Method: DECOR框架通过深度聚类结合方向鲁棒性，显式处理晶圆图的方向变化，确保空间相似的缺陷无论其旋转或对齐方式如何都能被一致聚类。

Result: 在MixedWM38数据集上的实验表明，DECOR优于现有的聚类基线方法，为自动视觉检测系统提供了可靠且可扩展的解决方案。

Conclusion: DECOR框架能够有效处理复杂晶圆缺陷数据的聚类问题，特别是在处理方向变化方面表现出色，为半导体制造中的缺陷检测提供了实用工具。

Abstract: In semiconductor manufacturing, early detection of wafer defects is critical
for product yield optimization. However, raw wafer data from wafer quality
tests are often complex, unlabeled, imbalanced and can contain multiple defects
on a single wafer, making it crucial to design clustering methods that remain
reliable under such imperfect data conditions. We introduce DECOR, a deep
clustering with orientation robustness framework that groups complex defect
patterns from wafer maps into consistent clusters. We evaluate our method on
the open source MixedWM38 dataset, demonstrating its ability to discover
clusters without manual tuning. DECOR explicitly accounts for orientation
variations in wafer maps, ensuring that spatially similar defects are
consistently clustered regardless of its rotation or alignment. Experiments
indicate that our method outperforms existing clustering baseline methods, thus
providing a reliable and scalable solution in automated visual inspection
systems.

</details>


### [11] [Error correction in multiclass image classification of facial emotion on unbalanced samples](https://arxiv.org/abs/2510.03337)
*Andrey A. Lebedev,Victor B. Kazantsev,Sergey V. Stasenko*

Main category: cs.CV

TL;DR: 该论文提出了一种基于LSTM和注意力机制的方法来解决面部图像多分类中的类别不平衡问题，通过误差校正技术提升小类别的分类性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决面部表情分类中存在的类别不平衡问题，即某些情绪类别样本数量显著多于其他类别，这会影响分类器对小类别的识别能力。

Method: 使用基于LSTM的神经网络模型，结合注意力机制聚焦于面部关键区域。训练时采用六类子集配置，然后对第七类进行误差校正。

Result: 实验结果显示所有类别都能进行校正，但成功率不同。在测试样本上，校正后小类别的关键质量指标有所提升。

Conclusion: 该方法可有效应用于面部表情分析系统，特别适合类别分布不平衡的分类任务，如反欺诈系统中的罕见事件检测。

Abstract: This paper considers the problem of error correction in multi-class
classification of face images on unbalanced samples. The study is based on the
analysis of a data frame containing images labeled by seven different emotional
states of people of different ages. Particular attention is paid to the problem
of class imbalance, in which some emotions significantly prevail over others.
To solve the classification problem, a neural network model based on LSTM with
an attention mechanism focusing on key areas of the face that are informative
for emotion recognition is used. As part of the experiments, the model is
trained on all possible configurations of subsets of six classes with
subsequent error correction for the seventh class, excluded at the training
stage. The results show that correction is possible for all classes, although
the degree of success varies: some classes are better restored, others are
worse. In addition, on the test sample, when correcting some classes, an
increase in key quality metrics for small classes was recorded, which indicates
the promise of the proposed approach in solving applied problems related to the
search for rare events, for example, in anti-fraud systems. Thus, the proposed
method can be effectively applied in facial expression analysis systems and in
tasks requiring stable classification under skewed class distribution.

</details>


### [12] [OpusAnimation: Code-Based Dynamic Chart Generation](https://arxiv.org/abs/2510.03341)
*Bozheng Li,Miao Yang,Zhenhan Chen,Jiawang Cao,Mushui Liu,Yi Lu,Yongliang Wu,Bin Zhang,Yangguang Ji,Licheng Tang,Jay Wu,Wenbo Zhu*

Main category: cs.CV

TL;DR: DCG-Bench是首个评估多模态大语言模型在动态图表生成任务上的基准，包含三个维度任务，并提出了两阶段训练方法和联合代码视觉奖励优化，构建的Qwen2.5-VL-DCG-3B模型在三个任务上平均性能提升8.31%。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在静态图表生成和理解方面已有显著进步，但它们在动态图表生成和理解方面的潜力仍未得到充分探索，存在研究空白。

Method: 构建了DCG-8K高质量数据集，包含指令-代码-视频三元组和QA对；提出了两阶段训练方法，使用联合代码视觉奖励进行组相对策略优化，构建了专家模型Qwen2.5-VL-DCG-3B。

Result: 基准测试揭示了现有MLLM在视觉到图表任务中的不足；提出的模型在三个任务上平均性能提升8.31%，仅用3B参数就达到了与专有模型相当的性能。

Conclusion: 提出的训练方法有效，DCG-Bench填补了动态图表生成评估的空白，为未来研究提供了重要基准。

Abstract: Dynamic Chart Generation (DCG) involves producing code-rendered animated
visualizations as charts. While recent advances in multi-modal large language
models (MLLMs) have significantly improved their capability on static chart
generation and comprehension, MLLMs' potential for handling dynamic chart
generation and understanding remains underexplored. To bridge this research
gap, we introduce DCG-Bench (Dynamic Chart Generation Benchmark), the first
benchmark evaluating MLLM's capability on dynamic chart generation tasks from
three dimensions: Simple Text-to-Chart, Detailed Text-to-Chart, and
Video-to-Chart tasks. We construct DCG-8K, a high-quality DCG dataset with
annotations covering instruction-code-video triplets and QA pairs for both code
and video evaluation. Based on DCG-8K, we explored a two-stage training recipe,
proposing Joint-Code-Visual Reward for group relative policy optimization to
construct expert MLLM Qwen2.5-VL-DCG-3B for the DCG task. Our benchmarking
result reveals shortcomings of existing MLLMs in the visual-to-chart task, and
our model beats the best open-sourced MLLM with an average 8.31% performance
gain across three tasks, and shows on par performance against proprietary
models with only 3B parameters, proving the effectiveness of our training
recipe. Our code and dataset will be publicly available.

</details>


### [13] [Visual Odometry with Transformers](https://arxiv.org/abs/2510.03348)
*Vlardimir Yugay,Duy-Kien Nguyen,Theo Gevers,Cees G. M. Snoek,Martin R. Oswald*

Main category: cs.CV

TL;DR: VoT是一种端到端的单目视觉里程计Transformer模型，通过时空注意力直接预测相机运动，无需传统的手工组件如光束法平差、特征匹配等。


<details>
  <summary>Details</summary>
Motivation: 现有的单目视觉里程计方法通常结合预训练的深度学习组件和优化模块，形成复杂管道，严重依赖相机标定和超参数调整，在未见过的真实场景中表现不佳。需要一种更简单、更通用的方法。

Method: 提出VoT模型，通过提取特征并使用时序和空间注意力建模全局关系，直接预测相机运动而无需估计密集几何结构，仅依赖相机位姿进行监督。框架模块化灵活，可集成各种预训练编码器作为特征提取器。

Result: 实验结果表明VoT能有效扩展到更大数据集，从更强的预训练骨干网络中获益显著，在不同相机运动和标定设置下具有良好泛化能力，性能优于传统方法且运行速度快3倍以上。

Conclusion: VoT证明了单目视觉里程计可以端到端方式有效解决，消除了对手工组件的需求，提供了更简单、更通用的解决方案。

Abstract: Modern monocular visual odometry methods typically combine pre-trained deep
learning components with optimization modules, resulting in complex pipelines
that rely heavily on camera calibration and hyperparameter tuning, and often
struggle in unseen real-world scenarios. Recent large-scale 3D models trained
on massive amounts of multi-modal data have partially alleviated these
challenges, providing generalizable dense reconstruction and camera pose
estimation. Still, they remain limited in handling long videos and providing
accurate per-frame estimates, which are required for visual odometry. In this
work, we demonstrate that monocular visual odometry can be addressed
effectively in an end-to-end manner, thereby eliminating the need for
handcrafted components such as bundle adjustment, feature matching, camera
calibration, or dense 3D reconstruction. We introduce VoT, short for Visual
odometry Transformer, which processes sequences of monocular frames by
extracting features and modeling global relationships through temporal and
spatial attention. Unlike prior methods, VoT directly predicts camera motion
without estimating dense geometry and relies solely on camera poses for
supervision. The framework is modular and flexible, allowing seamless
integration of various pre-trained encoders as feature extractors. Experimental
results demonstrate that VoT scales effectively with larger datasets, benefits
substantially from stronger pre-trained backbones, generalizes across diverse
camera motions and calibration settings, and outperforms traditional methods
while running more than 3 times faster. The code will be released.

</details>


### [14] [Inference-Time Search using Side Information for Diffusion-based Image Reconstruction](https://arxiv.org/abs/2510.03352)
*Mahdi Farahbakhsh,Vishnu Teja Kunde,Dileep Kalathil,Krishna Narayanan,Jean-Francois Chamberland*

Main category: cs.CV

TL;DR: 本文提出了一种新的推理时搜索算法，利用侧信息来指导扩散模型的采样过程，从而在逆问题中实现更准确和可靠的图像重建。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型方法通常忽略了侧信息，而这些信息在严重不适定的逆问题设置中可以显著提高重建质量。梯度引导方法容易产生奖励破解伪影，因此需要一种更平衡的探索和利用策略。

Method: 提出了一种推理时搜索算法，通过侧信息来引导采样过程，平衡探索和利用。该方法可以无缝集成到现有的基于扩散的图像重建流程中。

Result: 在多种逆问题（如框内修复、超分辨率、运动去模糊、高斯去模糊、非线性去模糊和盲去模糊）上的实验表明，该方法在定性和定量上均能一致提升基于扩散的图像重建算法的性能，且优于其他基线方法（包括基于奖励梯度的引导算法）。

Conclusion: 所提出的侧信息搜索方法为扩散模型在逆问题中的应用提供了一种有效的替代方案，能够显著提高重建质量并减少伪影。

Abstract: Diffusion models have emerged as powerful priors for solving inverse
problems. However, existing approaches typically overlook side information that
could significantly improve reconstruction quality, especially in severely
ill-posed settings. In this work, we propose a novel inference-time search
algorithm that guides the sampling process using the side information in a
manner that balances exploration and exploitation. This enables more accurate
and reliable reconstructions, providing an alternative to the gradient-based
guidance that is prone to reward-hacking artifacts. Our approach can be
seamlessly integrated into a wide range of existing diffusion-based image
reconstruction pipelines. Through extensive experiments on a number of inverse
problems, such as box inpainting, super-resolution, and various deblurring
tasks including motion, Gaussian, nonlinear, and blind deblurring, we show that
our approach consistently improves the qualitative and quantitative performance
of diffusion-based image reconstruction algorithms. We also show the superior
performance of our approach with respect to other baselines, including reward
gradient-based guidance algorithms. The code is available at
\href{https://github.com/mhdfb/sideinfo-search-reconstruction}{this
repository}.

</details>


### [15] [Sonar Image Datasets: A Comprehensive Survey of Resources, Challenges, and Applications](https://arxiv.org/abs/2510.03353)
*Larissa S. Gomes,Gustavo P. Almeida,Bryan U. Moreira,Marco Quiroz,Breno Xavier,Lucas Soares,Stephanie L. Brião,Felipe G. Oliveira,Paulo L. J. Drews-Jr*

Main category: cs.CV

TL;DR: 这篇论文对声纳图像数据集进行了全面综述，旨在解决数据稀缺问题，为水下声学数据分析研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 公开可用的、标注良好的声纳图像数据集稀缺，严重制约了鲁棒机器学习模型的发展，阻碍了水下探索、自主导航和生态系统监测等领域的进展。

Method: 通过系统梳理不同声纳模态（SSS、FLS、SAS、MBES、DIDSON）的公开数据集，分析其在分类、检测、分割和3D重建等应用中的特点，并整合成主表和时序图进行对比分析。

Result: 生成了一个包含数据集特征、规模和标注细节的综合比较框架，为研究人员提供了清晰的资源导航和现状评估。

Conclusion: 该综述不仅总结了现有声纳图像数据集资源，还识别了数据缺口，为水下声学数据分析领域的研究人员提供了实用的路线图和基础指南。

Abstract: Sonar images are relevant for advancing underwater exploration, autonomous
navigation, and ecosystem monitoring. However, the progress depends on data
availability. The scarcity of publicly available, well-annotated sonar image
datasets creates a significant bottleneck for the development of robust machine
learning models. This paper presents a comprehensive and concise review of the
current landscape of sonar image datasets, seeking not only to catalog existing
resources but also to contextualize them, identify gaps, and provide a clear
roadmap, serving as a base guide for researchers of any kind who wish to start
or advance in the field of underwater acoustic data analysis. We mapped
publicly accessible datasets across various sonar modalities, including Side
Scan Sonar (SSS), Forward-Looking Sonar (FLS), Synthetic Aperture Sonar (SAS),
Multibeam Echo Sounder (MBES), and Dual-Frequency Identification Sonar
(DIDSON). An analysis was conducted on applications such as classification,
detection, segmentation, and 3D reconstruction. This work focuses on
state-of-the-art advancements, incorporating newly released datasets. The
findings are synthesized into a master table and a chronological timeline,
offering a clear and accessible comparison of characteristics, sizes, and
annotation details datasets.

</details>


### [16] [Learned Display Radiance Fields with Lensless Cameras](https://arxiv.org/abs/2510.03356)
*Ziyang Chen,Yuta Itoh,Kaan Akşit*

Main category: cs.CV

TL;DR: 提出了一种基于无透镜相机和隐式神经表示的显示校准方法，无需专业设备即可从多角度捕获显示特性


<details>
  <summary>Details</summary>
Motivation: 传统显示校准需要专业设备和暗室环境，对普通用户来说难以实现，需要开发更便捷的校准方案

Method: 联合设计无透镜相机和基于隐式神经表示的算法，通过46.6°×37.6°视锥角高效重建显示发出的光场

Result: 能够从多角度捕获显示特性，为轻松显示校准和表征提供了初步解决方案

Conclusion: 该方法为无需专业硬件的显示校准开辟了新途径，有望使显示校准变得更加便捷

Abstract: Calibrating displays is a basic and regular task that content creators must
perform to maintain optimal visual experience, yet it remains a troublesome
issue. Measuring display characteristics from different viewpoints often
requires specialized equipment and a dark room, making it inaccessible to most
users. To avoid specialized hardware requirements in display calibrations, our
work co-designs a lensless camera and an Implicit Neural Representation based
algorithm for capturing display characteristics from various viewpoints. More
specifically, our pipeline enables efficient reconstruction of light fields
emitted from a display from a viewing cone of 46.6{\deg} X 37.6{\deg}. Our
emerging pipeline paves the initial steps towards effortless display
calibration and characterization.

</details>


### [17] [Provenance Networks: End-to-End Exemplar-Based Explainability](https://arxiv.org/abs/2510.03361)
*Ali Kayyam,Anusha Madan Gopal,M. Anthony Lewis*

Main category: cs.CV

TL;DR: 本文提出了一种名为溯源网络的新型神经网络模型，通过将可解释性嵌入到架构本身，实现端到端的训练数据驱动可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统深度神经网络的不透明性、幻觉问题以及数据贡献者信用分配等关键挑战，提高神经模型的透明度、鲁棒性和可信度。

Method: 模型类似于学习的KNN，将每个预测直接链接到其支持训练样本，通过联合优化主任务和可解释性目标来学习特征空间中加权的具体范例。

Result: 溯源网络能够系统研究记忆与泛化的权衡，验证输入是否包含在训练集中，检测错误标记或异常数据点，增强对输入扰动的鲁棒性，并识别相似输入对新数据点的贡献。

Conclusion: 虽然模型引入了额外的计算成本且目前适用于中等规模数据集，但它为现有可解释性技术提供了互补方法，显著改善了神经模型的透明度和可信度。

Abstract: We introduce provenance networks, a novel class of neural models designed to
provide end-to-end, training-data-driven explainability. Unlike conventional
post-hoc methods, provenance networks learn to link each prediction directly to
its supporting training examples as part of the model's normal operation,
embedding interpretability into the architecture itself. Conceptually, the
model operates similarly to a learned KNN, where each output is justified by
concrete exemplars weighted by relevance in the feature space. This approach
facilitates systematic investigations of the trade-off between memorization and
generalization, enables verification of whether a given input was included in
the training set, aids in the detection of mislabeled or anomalous data points,
enhances resilience to input perturbations, and supports the identification of
similar inputs contributing to the generation of a new data point. By jointly
optimizing the primary task and the explainability objective, provenance
networks offer insights into model behavior that traditional deep networks
cannot provide. While the model introduces additional computational cost and
currently scales to moderately sized datasets, it provides a complementary
approach to existing explainability techniques. In particular, it addresses
critical challenges in modern deep learning, including model opaqueness,
hallucination, and the assignment of credit to data contributors, thereby
improving transparency, robustness, and trustworthiness in neural models.

</details>


### [18] [Unified Unsupervised Anomaly Detection via Matching Cost Filtering](https://arxiv.org/abs/2510.03363)
*Zhe Zhang,Mingxiu Cai,Gaochang Wu,Jing Zhang,Lingqiao Liu,Dacheng Tao,Tianyou Chai,Xiatian Zhu*

Main category: cs.CV

TL;DR: 本文提出统一成本过滤（UCF）框架，通过匹配视角统一处理单模态和多模态无监督异常检测，利用多层注意力引导的可学习过滤模块来抑制匹配噪声并突出细微异常。


<details>
  <summary>Details</summary>
Motivation: 现有无监督异常检测方法主要关注单模态RGB数据，且存在匹配噪声问题未被充分解决。随着多模态技术的发展，需要统一框架来处理不同模态的异常检测任务。

Method: 构建异常成本体积，通过测试样本与正常样本的匹配，然后使用具有多层注意力引导的可学习过滤模块进行后处理优化。

Result: 在22个不同基准测试上的综合实验表明，UCF能有效提升各种UAD方法的性能，在单模态和多模态场景下均达到新的最先进水平。

Conclusion: UCF为无监督异常检测提供了一个通用的后处理优化框架，能够统一处理不同模态的异常检测任务，显著提升检测性能。

Abstract: Unsupervised anomaly detection (UAD) aims to identify image- and pixel-level
anomalies using only normal training data, with wide applications such as
industrial inspection and medical analysis, where anomalies are scarce due to
privacy concerns and cold-start constraints. Existing methods, whether
reconstruction-based (restoring normal counterparts) or embedding-based
(pretrained representations), fundamentally conduct image- or feature-level
matching to generate anomaly maps. Nonetheless, matching noise has been largely
overlooked, limiting their detection ability. Beyond earlier focus on unimodal
RGB-based UAD, recent advances expand to multimodal scenarios, e.g., RGB--3D
and RGB--Text, enabled by point cloud sensing and vision--language models.
Despite shared challenges, these lines remain largely isolated, hindering a
comprehensive understanding and knowledge transfer. In this paper, we advocate
unified UAD for both unimodal and multimodal settings in the matching
perspective. Under this insight, we present Unified Cost Filtering (UCF), a
generic post-hoc refinement framework for refining anomaly cost volume of any
UAD model. The cost volume is constructed by matching a test sample against
normal samples from the same or different modalities, followed by a learnable
filtering module with multi-layer attention guidance from the test sample,
mitigating matching noise and highlighting subtle anomalies. Comprehensive
experiments on 22 diverse benchmarks demonstrate the efficacy of UCF in
enhancing a variety of UAD methods, consistently achieving new state-of-the-art
results in both unimodal (RGB) and multimodal (RGB--3D, RGB--Text) UAD
scenarios. Code and models will be released at
https://github.com/ZHE-SAPI/CostFilter-AD.

</details>


### [19] [Visual Language Model as a Judge for Object Detection in Industrial Diagrams](https://arxiv.org/abs/2510.03376)
*Sanjukta Ghosh*

Main category: cs.CV

TL;DR: 本文提出了一种使用视觉语言模型(VLMs)来自动评估和优化工业图纸中对象检测质量的框架


<details>
  <summary>Details</summary>
Motivation: 工业图纸（如P&ID图）的数字化是实现工业自动化和数字孪生的关键步骤，但现有方法缺乏对对象检测结果质量的自动评估机制

Method: 利用视觉语言模型的多模态能力，识别检测结果中的缺失或不一致之处，从而指导检测结果的优化

Result: 该方法能够实现自动化的质量评估，并提高复杂工业图纸上的整体检测性能

Conclusion: 所提出的框架填补了工业图纸数字化过程中对象检测质量评估的空白，为构建更可靠的数字孪生系统提供了技术支持

Abstract: Industrial diagrams such as piping and instrumentation diagrams (P&IDs) are
essential for the design, operation, and maintenance of industrial plants.
Converting these diagrams into digital form is an important step toward
building digital twins and enabling intelligent industrial automation. A
central challenge in this digitalization process is accurate object detection.
Although recent advances have significantly improved object detection
algorithms, there remains a lack of methods to automatically evaluate the
quality of their outputs. This paper addresses this gap by introducing a
framework that employs Visual Language Models (VLMs) to assess object detection
results and guide their refinement. The approach exploits the multimodal
capabilities of VLMs to identify missing or inconsistent detections, thereby
enabling automated quality assessment and improving overall detection
performance on complex industrial diagrams.

</details>


### [20] [Spatial-ViLT: Enhancing Visual Spatial Reasoning through Multi-Task Learning](https://arxiv.org/abs/2510.03441)
*Chashi Mahiul Islam,Oteo Mamo,Samuel Jacob Chacko,Xiuwen Liu,Weikuan Yu*

Main category: cs.CV

TL;DR: 提出了SpatialViLT模型，通过集成深度图、3D坐标和边缘图等空间特征来增强视觉语言模型的空间推理能力，在VSR数据集上取得了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前视觉语言模型在3D场景和复杂物体配置空间推理方面的挑战，提升AI系统的空间智能水平。

Method: 采用多任务学习框架集成空间特征，提出SpatialViLT和MaskedSpatialViLT两个变体，以及结合两者的SpatialEnsemble方法。

Result: 模型在方向性、拓扑和邻近关系等空间推理类别上表现出色，在VSR数据集上达到最先进准确率。

Conclusion: 这项工作显著提升了AI系统的空间智能，对高级多模态理解和实际应用具有重要意义。

Abstract: Vision-language models (VLMs) have advanced multimodal reasoning but still
face challenges in spatial reasoning for 3D scenes and complex object
configurations. To address this, we introduce SpatialViLT, an enhanced VLM that
integrates spatial features like depth maps, 3D coordinates, and edge maps
through a multi-task learning framework. This approach enriches multimodal
embeddings with spatial understanding. We propose two variants: SpatialViLT and
MaskedSpatialViLT, focusing on full and masked object regions, respectively.
Additionally, SpatialEnsemble combines both approaches, achieving
state-of-the-art accuracy. Our models excel in spatial reasoning categories
such as directional, topological, and proximity relations, as demonstrated on
the challenging Visual Spatial Reasoning (VSR) dataset. This work represents a
significant step in enhancing the spatial intelligence of AI systems, crucial
for advanced multimodal understanding and real-world applications.

</details>


### [21] [Denoising of Two-Phase Optically Sectioned Structured Illumination Reconstructions Using Encoder-Decoder Networks](https://arxiv.org/abs/2510.03452)
*Allison Davis,Yezhi Shen,Xiaoyu Ji,Fengqing Zhu*

Main category: cs.CV

TL;DR: 该论文研究使用编码器-解码器网络来减少两相光学切片结构照明（OS-SI）中的伪影，通过合成训练数据解决缺乏清洁地面真实数据的问题。


<details>
  <summary>Details</summary>
Motivation: 两相OS-SI中减少采集时间会引入残留伪影，传统去噪方法难以有效抑制，而深度学习又受限于缺乏清洁的光学切片地面真实数据。

Method: 使用非对称去噪自编码器（DAE）和U-Net，在由真实伪影场应用于合成图像形成的合成训练对上训练，然后评估真实OS-SI图像。

Result: 两种网络都提高了图像清晰度，各自在不同类型的伪影上表现优异。

Conclusion: 合成训练能够实现OS-SI图像的监督去噪，编码器-解码器网络有潜力简化重建工作流程。

Abstract: Structured illumination (SI) enhances image resolution and contrast by
projecting patterned light onto a sample. In two-phase optical-sectioning SI
(OS-SI), reduced acquisition time introduces residual artifacts that
conventional denoising struggles to suppress. Deep learning offers an
alternative to traditional methods; however, supervised training is limited by
the lack of clean, optically sectioned ground-truth data. We investigate
encoder-decoder networks for artifact reduction in two-phase OS-SI, using
synthetic training pairs formed by applying real artifact fields to synthetic
images. An asymmetrical denoising autoencoder (DAE) and a U-Net are trained on
the synthetic data, then evaluated on real OS-SI images. Both networks improve
image clarity, with each excelling against different artifact types. These
results demonstrate that synthetic training enables supervised denoising of
OS-SI images and highlight the potential of encoder-decoder networks to
streamline reconstruction workflows.

</details>


### [22] [PEaRL: Pathway-Enhanced Representation Learning for Gene and Pathway Expression Prediction from Histology](https://arxiv.org/abs/2510.03455)
*Sejuti Majumder,Saarthak Kapse,Moinak Bhattacharya,Xuan Xu,Alisa Yurovsky,Prateek Prasanna*

Main category: cs.CV

TL;DR: PEaRL是一个多模态框架，通过通路激活分数整合组织病理学和空间转录组学，超越基因层面分析，提供更生物可信和可解释的多模态模型


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖少量高变基因，限制了预测范围并忽略了协调组织表型的生物学程序

Method: 使用ssGSEA计算通路激活分数，通过transformer编码生物学通路信号，并通过对比学习与组织学特征对齐

Result: 在三个癌症数据集上表现优于SOTA方法，基因和通路水平表达预测的Pearson相关系数分别提高58.9%和20.4%

Conclusion: 基于通路的转录组表示产生更生物可信和可解释的多模态模型，推进计算病理学超越基因层面嵌入

Abstract: Integrating histopathology with spatial transcriptomics (ST) provides a
powerful opportunity to link tissue morphology with molecular function. Yet
most existing multimodal approaches rely on a small set of highly variable
genes, which limits predictive scope and overlooks the coordinated biological
programs that shape tissue phenotypes. We present PEaRL (Pathway Enhanced
Representation Learning), a multimodal framework that represents
transcriptomics through pathway activation scores computed with ssGSEA. By
encoding biologically coherent pathway signals with a transformer and aligning
them with histology features via contrastive learning, PEaRL reduces
dimensionality, improves interpretability, and strengthens cross-modal
correspondence. Across three cancer ST datasets (breast, skin, and lymph node),
PEaRL consistently outperforms SOTA methods, yielding higher accuracy for both
gene- and pathway-level expression prediction (up to 58.9 percent and 20.4
percent increase in Pearson correlation coefficient compared to SOTA). These
results demonstrate that grounding transcriptomic representation in pathways
produces more biologically faithful and interpretable multimodal models,
advancing computational pathology beyond gene-level embeddings.

</details>


### [23] [DuPLUS: Dual-Prompt Vision-Language Framework for Universal Medical Image Segmentation and Prognosis](https://arxiv.org/abs/2510.03483)
*Numan Saeed,Tausifa Jan Saleem,Fadillah Maani,Muhammad Ridzuan,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: DuPLUS是一个用于多模态医学图像分析的新型深度学习框架，通过分层语义提示和双提示机制实现细粒度任务控制，在分割任务上超越现有方法，并能扩展到预后预测等新任务。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像深度学习面临任务特定模型缺乏通用性、现有通用方法语义理解能力差的问题，需要开发既能通用化又具备医学语义理解能力的框架。

Method: 引入分层语义提示的视觉语言框架，采用独特的双提示机制和文本控制架构，支持参数高效微调以适应新任务和模态。

Result: 在10个医学数据集上，DuPLUS在8个数据集上优于最先进的特定任务和通用模型，在头颈癌数据集上预后预测的Concordance Index达到0.69。

Conclusion: DuPLUS建立了作为医学图像分析的通用且临床相关解决方案，能够快速适应来自不同中心的新任务和模态。

Abstract: Deep learning for medical imaging is hampered by task-specific models that
lack generalizability and prognostic capabilities, while existing 'universal'
approaches suffer from simplistic conditioning and poor medical semantic
understanding. To address these limitations, we introduce DuPLUS, a deep
learning framework for efficient multi-modal medical image analysis. DuPLUS
introduces a novel vision-language framework that leverages hierarchical
semantic prompts for fine-grained control over the analysis task, a capability
absent in prior universal models. To enable extensibility to other medical
tasks, it includes a hierarchical, text-controlled architecture driven by a
unique dual-prompt mechanism. For segmentation, DuPLUS is able to generalize
across three imaging modalities, ten different anatomically various medical
datasets, encompassing more than 30 organs and tumor types. It outperforms the
state-of-the-art task specific and universal models on 8 out of 10 datasets. We
demonstrate extensibility of its text-controlled architecture by seamless
integration of electronic health record (EHR) data for prognosis prediction,
and on a head and neck cancer dataset, DuPLUS achieved a Concordance Index (CI)
of 0.69. Parameter-efficient fine-tuning enables rapid adaptation to new tasks
and modalities from varying centers, establishing DuPLUS as a versatile and
clinically relevant solution for medical image analysis. The code for this work
is made available at: https://anonymous.4open.science/r/DuPLUS-6C52

</details>


### [24] [Real-Time Threaded Houbara Detection and Segmentation for Wildlife Conservation using Mobile Platforms](https://arxiv.org/abs/2510.03501)
*Lyes Saad Saoud,Loic Lesobre,Enrico Sorato,Irfan Hussain*

Main category: cs.CV

TL;DR: 提出了一种移动优化的两阶段深度学习框架，通过线程化检测模型并行化YOLOv10检测和MobileSAM分割，在Houbara鸨鸟检测任务中实现实时性能和高精度。


<details>
  <summary>Details</summary>
Motivation: 野生动物保护需要实时动物检测和分割技术，但受限于计算资源和物种隐蔽外观等挑战，需要开发高效的移动端解决方案。

Method: 采用两阶段框架：YOLOv10负责目标检测，MobileSAM负责轻量级分割，通过线程化并行执行以减少延迟。构建了包含40,000张标注图像的Houbara数据集。

Result: 在Houbara鸨鸟检测中达到mAP50为0.9627，mAP75为0.7731，mAP95为0.7178，MobileSAM mIoU为0.7421，YOLOv10每帧处理时间43.7ms，满足实时性要求。

Conclusion: 该框架在移动设备上实现了高效的实时动物检测和分割，为野生动物保护提供了可行的技术方案，代码和数据集已开源。

Abstract: Real-time animal detection and segmentation in natural environments are vital
for wildlife conservation, enabling non-invasive monitoring through remote
camera streams. However, these tasks remain challenging due to limited
computational resources and the cryptic appearance of many species. We propose
a mobile-optimized two-stage deep learning framework that integrates a
Threading Detection Model (TDM) to parallelize YOLOv10-based detection and
MobileSAM-based segmentation. Unlike prior YOLO+SAM pipelines, our approach
improves real-time performance by reducing latency through threading. YOLOv10
handles detection while MobileSAM performs lightweight segmentation, both
executed concurrently for efficient resource use. On the cryptic Houbara
Bustard, a conservation-priority species, our model achieves mAP50 of 0.9627,
mAP75 of 0.7731, mAP95 of 0.7178, and a MobileSAM mIoU of 0.7421. YOLOv10
operates at 43.7 ms per frame, confirming real-time readiness. We introduce a
curated Houbara dataset of 40,000 annotated images to support model training
and evaluation across diverse conditions. The code and dataset used in this
study are publicly available on GitHub at
https://github.com/LyesSaadSaoud/mobile-houbara-detseg. For interactive demos
and additional resources, visit
https://lyessaadsaoud.github.io/LyesSaadSaoud-Threaded-YOLO-SAM-Houbara.

</details>


### [25] [Platonic Transformers: A Solid Choice For Equivariance](https://arxiv.org/abs/2510.03511)
*Mohammad Mohaiminul Islam,Rishabh Anand,David R. Wessels,Friso de Kruiff,Thijs P. Kuipers,Rex Ying,Clara I. Sánchez,Sharvaree Vadgama,Georg Bökman,Erik J. Bekkers*

Main category: cs.CV

TL;DR: Platonic Transformer通过引入柏拉图立体对称群的参考框架，在保持标准Transformer架构和计算成本的同时，实现了对连续平移和柏拉图对称性的等变性，解决了Transformer缺乏几何对称性归纳偏置的问题。


<details>
  <summary>Details</summary>
Motivation: Transformer在科学和计算机视觉领域缺乏几何对称性的归纳偏置，而现有的等变方法往往通过复杂、计算密集的设计牺牲了Transformer的效率和灵活性。

Method: 通过定义基于柏拉图立体对称群参考框架的注意力机制，诱导出原则性的权重共享方案，使模型能够学习自适应几何滤波器，并实现了高度可扩展的线性时间卷积变体。

Result: 在计算机视觉（CIFAR-10）、3D点云（ScanObjectNN）和分子性质预测（QM9, OMol25）等多个基准测试中，Platonic Transformer在不增加额外成本的情况下，通过利用几何约束实现了有竞争力的性能。

Conclusion: Platonic Transformer成功解决了Transformer缺乏几何对称性归纳偏置的问题，在保持原有架构效率和灵活性的同时，实现了对几何对称性的等变性，为科学和计算机视觉应用提供了有效的解决方案。

Abstract: While widespread, Transformers lack inductive biases for geometric symmetries
common in science and computer vision. Existing equivariant methods often
sacrifice the efficiency and flexibility that make Transformers so effective
through complex, computationally intensive designs. We introduce the Platonic
Transformer to resolve this trade-off. By defining attention relative to
reference frames from the Platonic solid symmetry groups, our method induces a
principled weight-sharing scheme. This enables combined equivariance to
continuous translations and Platonic symmetries, while preserving the exact
architecture and computational cost of a standard Transformer. Furthermore, we
show that this attention is formally equivalent to a dynamic group convolution,
which reveals that the model learns adaptive geometric filters and enables a
highly scalable, linear-time convolutional variant. Across diverse benchmarks
in computer vision (CIFAR-10), 3D point clouds (ScanObjectNN), and molecular
property prediction (QM9, OMol25), the Platonic Transformer achieves
competitive performance by leveraging these geometric constraints at no
additional cost.

</details>


### [26] [Domain Generalization for Semantic Segmentation: A Survey](https://arxiv.org/abs/2510.03540)
*Manuel Schwonberg,Hanno Gottschalk*

Main category: cs.CV

TL;DR: 这篇论文是关于领域泛化语义分割的综述性文章，系统回顾了该快速发展的领域，特别关注了基于基础模型的范式转变，并进行了全面的性能比较。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在未知领域的泛化能力仍然是一个重大挑战，特别是在语义分割任务中，这对生物医学和自动驾驶等领域至关重要。需要系统总结领域泛化方法的最新进展。

Method: 采用综述研究方法，对现有领域泛化语义分割方法进行分类和回顾，重点关注基于基础模型的范式转变，并进行广泛的性能比较分析。

Result: 研究发现基础模型对领域泛化性能有显著影响，识别了该领域向基础模型方法的范式转变，并提供了各种方法的全面性能对比。

Conclusion: 该综述旨在推动领域泛化研究发展，启发科学家探索新的研究方向，特别是基于基础模型的领域泛化方法具有重要前景。

Abstract: The generalization of deep neural networks to unknown domains is a major
challenge despite their tremendous progress in recent years. For this reason,
the dynamic area of domain generalization (DG) has emerged. In contrast to
unsupervised domain adaptation, there is no access to or knowledge about the
target domains, and DG methods aim to generalize across multiple different
unseen target domains. Domain generalization is particularly relevant for the
task semantic segmentation which is used in several areas such as biomedicine
or automated driving. This survey provides a comprehensive overview of the
rapidly evolving topic of domain generalized semantic segmentation. We cluster
and review existing approaches and identify the paradigm shift towards
foundation-model-based domain generalization. Finally, we provide an extensive
performance comparison of all approaches, which highlights the significant
influence of foundation models on domain generalization. This survey seeks to
advance domain generalization research and inspire scientists to explore new
research directions.

</details>


### [27] [From Scope to Script: An Automated Report Generation Model for Gastrointestinal Endoscopy](https://arxiv.org/abs/2510.03543)
*Evandros Kaklamanos,Kristjana Kristinsdottir,Jonathan Huang,Dustin Carlson,Rajesh Keswani,John Pandolfino,Mozziyar Etemadi*

Main category: cs.CV

TL;DR: 提出基于transformer的自动化内窥镜报告生成模型，通过两阶段训练减少医生文档负担


<details>
  <summary>Details</summary>
Motivation: 内窥镜检查的文档记录负担给胃肠病学家带来巨大压力，导致临床工作流程低效和医生职业倦怠

Method: 使用基于transformer的视觉编码器和文本解码器，采用两阶段训练框架：先在图像/文本描述对上预训练，再在图像/报告对上微调

Result: 模型能够生成具有临床意义的检查结果，简化文档流程

Conclusion: 该方法有望减轻医生工作负担并改善患者护理

Abstract: Endoscopic procedures such as esophagogastroduodenoscopy (EGD) and
colonoscopy play a critical role in diagnosing and managing gastrointestinal
(GI) disorders. However, the documentation burden associated with these
procedures place significant strain on gastroenterologists, contributing to
inefficiencies in clinical workflows and physician burnout. To address this
challenge, we propose a novel automated report generation model that leverages
a transformer-based vision encoder and text decoder within a two-stage training
framework. In the first stage, both components are pre-trained on image/text
caption pairs to capture generalized vision-language features, followed by
fine-tuning on images/report pairs to generate clinically meaningful findings.
Our approach not only streamlines the documentation process but also holds
promise for reducing physician workload and improving patient care.

</details>


### [28] [SketchPlan: Diffusion Based Drone Planning From Human Sketches](https://arxiv.org/abs/2510.03545)
*Sixten Norelius,Aaron O. Feldman,Mac Schwager*

Main category: cs.CV

TL;DR: SketchPlan是一个基于扩散模型的无人机路径规划系统，通过2D手绘草图在深度图像上生成3D飞行路径，实现零样本从仿真到现实的迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的无人机路径规划方法通常需要复杂的编程或精确的3D建模，难以让非专业人士快速指定飞行意图。手绘草图作为直观的人机交互方式，能够简化无人机控制，但需要解决从2D草图到3D路径的映射问题。

Method: SketchPlan包含两个组件：SketchAdapter学习将手绘草图映射到2D投影路径，DiffPath扩散模型从2D投影和第一人称深度图像推断3D轨迹。使用32k合成数据集训练，其中872条路径有人工标注的真实手绘草图。

Result: 在真实世界无人机测试中，SketchPlan在低/中密度障碍物环境中达到100%成功率，在未见过的高密度障碍物环境中达到40%成功率，比关键消融实验高出20-60%的任务完成率。

Conclusion: 混合人工标注和自动标注数据的训练策略，结合模块化设计，显著提升了模型理解人类意图和推断3D路径的能力，实现了有效的零样本仿真到现实迁移。

Abstract: We propose SketchPlan, a diffusion-based planner that interprets 2D
hand-drawn sketches over depth images to generate 3D flight paths for drone
navigation. SketchPlan comprises two components: a SketchAdapter that learns to
map the human sketches to projected 2D paths, and DiffPath, a diffusion model
that infers 3D trajectories from 2D projections and a first person view depth
image. Our model achieves zero-shot sim-to-real transfer, generating accurate
and safe flight paths in previously unseen real-world environments. To train
the model, we build a synthetic dataset of 32k flight paths using a diverse set
of photorealistic 3D Gaussian Splatting scenes. We automatically label the data
by computing 2D projections of the 3D flight paths onto the camera plane, and
use this to train the DiffPath diffusion model. However, since real human 2D
sketches differ significantly from ideal 2D projections, we additionally label
872 of the 3D flight paths with real human sketches and use this to train the
SketchAdapter to infer the 2D projection from the human sketch. We demonstrate
SketchPlan's effectiveness in both simulated and real-world experiments, and
show through ablations that training on a mix of human labeled and auto-labeled
data together with a modular design significantly boosts its capabilities to
correctly interpret human intent and infer 3D paths. In real-world drone tests,
SketchPlan achieved 100\% success in low/medium clutter and 40\% in unseen
high-clutter environments, outperforming key ablations by 20-60\% in task
completion.

</details>


### [29] [Unmasking Puppeteers: Leveraging Biometric Leakage to Disarm Impersonation in AI-based Videoconferencing](https://arxiv.org/abs/2510.03548)
*Danial Samadi Vahdati,Tai Duc Nguyen,Ekta Prashnani,Koki Nagano,David Luebke,Orazio Gallo,Matthew Stamm*

Main category: cs.CV

TL;DR: 该论文提出了一种基于生物特征泄漏检测的防御方法，用于防止AI视频会议系统中的身份劫持攻击，通过分析姿态-表情潜在空间中的生物特征信息来检测非法身份交换。


<details>
  <summary>Details</summary>
Motivation: AI视频会议系统使用紧凑的姿态-表情潜在空间来减少带宽，但攻击者可以利用这个潜在空间实时劫持受害者的身份，而现有的深度伪造检测器无法检测这种攻击，因为每一帧都是合成的。

Method: 提出了一种姿态条件的大间隔对比编码器，该编码器能够从传输的潜在空间中分离出持久的身份特征，同时消除瞬时的姿态和表情信息，通过简单的余弦测试来检测非法身份交换。

Result: 实验表明，该方法在多个说话人生成模型上均优于现有的防御方法，能够实时运行，并且在分布外场景下表现出很强的泛化能力。

Conclusion: 该方法通过利用姿态-表情潜在空间中的生物特征信息，有效解决了AI视频会议系统中的身份劫持安全问题，为实时深度伪造检测提供了新的解决方案。

Abstract: AI-based talking-head videoconferencing systems reduce bandwidth by sending a
compact pose-expression latent and re-synthesizing RGB at the receiver, but
this latent can be puppeteered, letting an attacker hijack a victim's likeness
in real time. Because every frame is synthetic, deepfake and synthetic video
detectors fail outright. To address this security problem, we exploit a key
observation: the pose-expression latent inherently contains biometric
information of the driving identity. Therefore, we introduce the first
biometric leakage defense without ever looking at the reconstructed RGB video:
a pose-conditioned, large-margin contrastive encoder that isolates persistent
identity cues inside the transmitted latent while cancelling transient pose and
expression. A simple cosine test on this disentangled embedding flags illicit
identity swaps as the video is rendered. Our experiments on multiple
talking-head generation models show that our method consistently outperforms
existing puppeteering defenses, operates in real-time, and shows strong
generalization to out-of-distribution scenarios.

</details>


### [30] [Streaming Drag-Oriented Interactive Video Manipulation: Drag Anything, Anytime!](https://arxiv.org/abs/2510.03550)
*Junbao Zhou,Yuan Zhou,Kesen Zhao,Qingshan Xu,Beier Zhu,Richang Hong,Hanwang Zhang*

Main category: cs.CV

TL;DR: REVEL提出了一种新的流式拖拽视频编辑任务，DragStream通过自适应分布自校正和空间频率选择性优化解决了拖拽过程中的潜在分布漂移和上下文干扰问题。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频扩散模型难以实现细粒度的流式控制，用户无法在生成过程中随时对任意内容进行交互式拖拽编辑。

Method: 提出DragStream方法：1）自适应分布自校正策略，利用相邻帧统计信息约束潜在嵌入漂移；2）空间频率选择性优化机制，选择性传播视觉线索以减少上下文干扰。

Result: 方法可无缝集成到现有自回归视频扩散模型中，大量实验验证了DragStream的有效性。

Conclusion: REVEL任务和DragStream方法为流式、细粒度的交互式视频编辑提供了有效解决方案，显著提升了视频生成模型的可控性。

Abstract: Achieving streaming, fine-grained control over the outputs of autoregressive
video diffusion models remains challenging, making it difficult to ensure that
they consistently align with user expectations. To bridge this gap, we propose
\textbf{stReaming drag-oriEnted interactiVe vidEo manipuLation (REVEL)}, a new
task that enables users to modify generated videos \emph{anytime} on
\emph{anything} via fine-grained, interactive drag. Beyond DragVideo and
SG-I2V, REVEL unifies drag-style video manipulation as editing and animating
video frames with both supporting user-specified translation, deformation, and
rotation effects, making drag operations versatile. In resolving REVEL, we
observe: \emph{i}) drag-induced perturbations accumulate in latent space,
causing severe latent distribution drift that halts the drag process;
\emph{ii}) streaming drag is easily disturbed by context frames, thereby
yielding visually unnatural outcomes. We thus propose a training-free approach,
\textbf{DragStream}, comprising: \emph{i}) an adaptive distribution
self-rectification strategy that leverages neighboring frames' statistics to
effectively constrain the drift of latent embeddings; \emph{ii}) a
spatial-frequency selective optimization mechanism, allowing the model to fully
exploit contextual information while mitigating its interference via
selectively propagating visual cues along generation. Our method can be
seamlessly integrated into existing autoregressive video diffusion models, and
extensive experiments firmly demonstrate the effectiveness of our DragStream.

</details>


### [31] [GAS-MIL: Group-Aggregative Selection Multi-Instance Learning for Ensemble of Foundation Models in Digital Pathology Image Analysis](https://arxiv.org/abs/2510.03555)
*Peiran Quan,Zifan Gu,Zhuo Zhao,Qin Zhou,Donghan M. Yang,Ruichen Rong,Yang Xie,Guanghua Xiao*

Main category: cs.CV

TL;DR: 提出GAS-MIL框架，通过集成多个基础模型特征来提升病理诊断性能，无需手动特征选择或精细调优


<details>
  <summary>Details</summary>
Motivation: 基础模型在计算病理学中提供了强大的特征提取能力，但针对特定诊断任务进行适配和基准测试耗时耗力，特别是考虑到模型的规模和多样性

Method: Group-Aggregative Selection多示例学习框架，无缝集成多个基础模型的特征，保留其互补优势

Result: 在三种癌症数据集（前列腺癌PANDA、卵巢癌UBC-OCEAN、乳腺癌TCGA-BrCa）的分类任务中，GAS-MIL始终优于或与单个基础模型和现有MIL方法相当

Conclusion: GAS-MIL通过高效集成异构基础模型，简化了病理学模型部署，为未来多模态和精准肿瘤学应用提供了可扩展的基础

Abstract: Foundation models (FMs) have transformed computational pathology by providing
powerful, general-purpose feature extractors. However, adapting and
benchmarking individual FMs for specific diagnostic tasks is often
time-consuming and resource-intensive, especially given their scale and
diversity. To address this challenge, we introduce Group-Aggregative Selection
Multi-Instance Learning (GAS-MIL), a flexible ensemble framework that
seamlessly integrates features from multiple FMs, preserving their
complementary strengths without requiring manual feature selection or extensive
task-specific fine-tuning. Across classification tasks in three cancer
datasets-prostate (PANDA), ovarian (UBC-OCEAN), and breast (TCGA-BrCa)-GAS-MIL
consistently achieves superior or on-par performance relative to individual FMs
and established MIL methods, demonstrating its robustness and generalizability.
By enabling efficient integration of heterogeneous FMs, GAS-MIL streamlines
model deployment for pathology and provides a scalable foundation for future
multimodal and precision oncology applications.

</details>


### [32] [Real-Time Assessment of Bystander Situation Awareness in Drone-Assisted First Aid](https://arxiv.org/abs/2510.03558)
*Shen Chang,Renran Tian,Nicole Adams,Nan Kong*

Main category: cs.CV

TL;DR: 本文提出了一种基于无人机辅助纳洛酮递送的实时情境感知评估框架，通过图嵌入和Transformer模型来评估旁观者在阿片类药物过量紧急情况下的情境感知能力。


<details>
  <summary>Details</summary>
Motivation: 解决在阿片类药物过量紧急情况(OEES)中，无人机递送纳洛酮需要有效指导未经医疗培训的旁观者，而实时评估旁观者情境感知(SA)是实现人机协作(HAT)的关键研究空白。

Method: 利用Drone-Assisted Naloxone Delivery Simulation Dataset (DANDSD)数据集，提出视频基的实时SA评估框架，整合几何、运动学和交互图特征等视觉感知和理解线索，采用图嵌入和Transformer模型进行实时SA预测。

Result: 该方法在SA预测方面表现出高性能，时间分割准确率显著优于FINCH基线，Mean over Frames (MoF)提高9%，Intersection over Union (IoU)提高5%。

Conclusion: 该工作支持开发能够有效指导旁观者的自适应无人机系统，最终改善紧急响应结果并挽救生命。

Abstract: Rapid naloxone delivery via drones offers a promising solution for responding
to opioid overdose emergencies (OOEs), by extending lifesaving interventions to
medically untrained bystanders before emergency medical services (EMS) arrive.
Recognizing the critical role of bystander situational awareness (SA) in
human-autonomy teaming (HAT), we address a key research gap in real-time SA
assessment by introducing the Drone-Assisted Naloxone Delivery Simulation
Dataset (DANDSD). This pioneering dataset captures HAT during simulated OOEs,
where college students without medical training act as bystanders tasked with
administering intranasal naloxone to a mock overdose victim. Leveraging this
dataset, we propose a video-based real-time SA assessment framework that
utilizes graph embeddings and transformer models to assess bystander SA in real
time. Our approach integrates visual perception and comprehension cues--such as
geometric, kinematic, and interaction graph features--and achieves
high-performance SA prediction. It also demonstrates strong temporal
segmentation accuracy, outperforming the FINCH baseline by 9% in Mean over
Frames (MoF) and 5% in Intersection over Union (IoU). This work supports the
development of adaptive drone systems capable of guiding bystanders
effectively, ultimately improving emergency response outcomes and saving lives.

</details>


### [33] [Evaluating OCR performance on food packaging labels in South Africa](https://arxiv.org/abs/2510.03570)
*Mayimunah Nagayi,Alice Khan,Tamryn Frank,Rina Swart,Clement Nyirenda*

Main category: cs.CV

TL;DR: 本研究评估了四种开源OCR系统（Tesseract、EasyOCR、PaddleOCR和TrOCR）在真实食品包装图像上的性能，重点关注成分表和营养事实面板的提取能力。


<details>
  <summary>Details</summary>
Motivation: 准确的包装OCR对于合规性和营养监测很重要，但由于多语言文本、密集布局、字体变化、反光和曲面等挑战而困难重重。

Method: 使用231个产品（1,628张图像）的数据集，通过所有四种模型处理来评估速度和覆盖率，并创建了113张图像（60个产品）的真实标注子集进行准确性评估。评估指标包括字符错误率、词错误率、BLEU、ROUGE-L、F1、覆盖率和执行时间。

Result: 在真实标注子集上，Tesseract获得了最低的字符错误率（0.912）和最高的BLEU分数（0.245）。EasyOCR在准确性和多语言支持之间取得了良好平衡。PaddleOCR实现了近乎完整的覆盖率但速度较慢，TrOCR尽管有GPU加速但表现最弱。

Conclusion: 这些结果为包装OCR提供了特定基准，建立了基线，并指出了布局感知方法和文本定位的发展方向。

Abstract: This study evaluates four open-source Optical Character Recognition (OCR)
systems which are Tesseract, EasyOCR, PaddleOCR, and TrOCR on real world food
packaging images. The aim is to assess their ability to extract ingredient
lists and nutrition facts panels. Accurate OCR for packaging is important for
compliance and nutrition monitoring but is challenging due to multilingual
text, dense layouts, varied fonts, glare, and curved surfaces. A dataset of 231
products (1,628 images) was processed by all four models to assess speed and
coverage, and a ground truth subset of 113 images (60 products) was created for
accuracy evaluation. Metrics include Character Error Rate (CER), Word Error
Rate (WER), BLEU, ROUGE-L, F1, coverage, and execution time. On the ground
truth subset, Tesseract achieved the lowest CER (0.912) and the highest BLEU
(0.245). EasyOCR provided a good balance between accuracy and multilingual
support. PaddleOCR achieved near complete coverage but was slower because it
ran on CPU only due to GPU incompatibility, and TrOCR produced the weakest
results despite GPU acceleration. These results provide a packaging-specific
benchmark, establish a baseline, and highlight directions for layout-aware
methods and text localization.

</details>


### [34] [FrameOracle: Learning What to See and How Much to See in Videos](https://arxiv.org/abs/2510.03584)
*Chaoyu Li,Tianzhi Li,Fei Tao,Zhenyu Zhao,Ziqian Wu,Maozheng Zhao,Juntong Song,Cheng Niu,Pooyan Fazli*

Main category: cs.CV

TL;DR: FrameOracle是一个轻量级插件模块，通过预测关键帧和所需帧数来优化视频理解模型的帧采样效率，在保持或提升准确率的同时大幅减少处理帧数。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs处理帧数有限，传统均匀采样策略无法适应信息密度和任务复杂度的变化，导致效率低下和信息丢失。

Method: 提出四阶段课程训练方法，前三个阶段使用跨模态相似度等弱代理信号，第四阶段利用新构建的FrameOracle-41K数据集提供的关键帧标注进行强监督训练。

Result: 在5个VLMs和6个基准测试中，FrameOracle将16帧输入平均减少到10.4帧且准确率不变；从64帧候选帧中平均减少到13.9帧，同时准确率提升1.4%。

Conclusion: FrameOracle实现了最先进的效率-准确率权衡，为可扩展视频理解提供了有效解决方案。

Abstract: Vision-language models (VLMs) have advanced video understanding, but their
performance is limited by the number of input frames they can process. Existing
frame sampling strategies, such as uniform or fixed-budget selection, often
fail to adapt to variations in information density or task complexity,
resulting in inefficiency and information loss. To address this, we present
FrameOracle, a lightweight and plug-and-play module that predicts both (1)
which frames are most relevant to a given query and (2) how many frames are
needed. FrameOracle is trained using a four-stage curriculum, with the first
three stages relying on weak proxy signals such as cross-modal similarity. In
the final stage, it leverages stronger supervision from a new dataset we
introduce, FrameOracle-41K, the first large-scale VideoQA collection to provide
keyframe annotations specifying the minimal set of frames required to answer
each question. Extensive experiments across five VLMs and six benchmarks
demonstrate that FrameOracle reduces 16-frame inputs to an average of 10.4
frames without any loss in accuracy. When starting from 64-frame candidates, it
reduces the input to an average of 13.9 frames while improving accuracy by
1.4%, achieving state-of-the-art efficiency-accuracy trade-offs for scalable
video understanding.

</details>


### [35] [A Hybrid Co-Finetuning Approach for Visual Bug Detection in Video Games](https://arxiv.org/abs/2510.03591)
*Faliu Yi,Sherif Abdelfattah,Wei Huang,Adrian Brown*

Main category: cs.CV

TL;DR: 提出了一种混合Co-FineTuning方法，通过结合标记和未标记数据来改进游戏视觉bug检测，减少对目标游戏标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 游戏视觉bug的手动识别成本高且需要专业知识，监督学习模型依赖大量标记数据，但视觉bug出现频率低，标记数据获取困难。

Method: 采用混合Co-FineTuning方法，整合目标游戏和同领域游戏的标记样本，并加入未标记数据来增强特征表示学习。

Result: 该方法在多个游戏环境中表现出优于传统基线的性能，即使仅使用50%目标游戏标记数据也能保持竞争力。

Conclusion: CFT方法提高了游戏视觉bug检测的可扩展性和适应性，有效利用了所有可用数据资源。

Abstract: Manual identification of visual bugs in video games is a resource-intensive
and costly process, often demanding specialized domain knowledge. While
supervised visual bug detection models offer a promising solution, their
reliance on extensive labeled datasets presents a significant challenge due to
the infrequent occurrence of such bugs. To overcome this limitation, we propose
a hybrid Co-FineTuning (CFT) method that effectively integrates both labeled
and unlabeled data. Our approach leverages labeled samples from the target game
and diverse co-domain games, additionally incorporating unlabeled data to
enhance feature representation learning. This strategy maximizes the utility of
all available data, substantially reducing the dependency on labeled examples
from the specific target game. The developed framework demonstrates enhanced
scalability and adaptability, facilitating efficient visual bug detection
across various game titles. Our experimental results show the robustness of the
proposed method for game visual bug detection, exhibiting superior performance
compared to conventional baselines across multiple gaming environments.
Furthermore, CFT maintains competitive performance even when trained with only
50% of the labeled data from the target game.

</details>


### [36] [Exploring the Hierarchical Reasoning Model for Small Natural-Image Classification Without Augmentation](https://arxiv.org/abs/2510.03598)
*Alexander V. Mantzaris*

Main category: cs.CV

TL;DR: HRM在MNIST上表现良好(98%准确率)，但在CIFAR-10和CIFAR-100上表现不佳，过拟合严重，不如简单的卷积网络，表明当前HRM缺乏足够的图像特定归纳偏置。


<details>
  <summary>Details</summary>
Motivation: 探索HRM（包含Transformer模块、DEQ风格训练、深度监督等）是否能作为实用的图像分类器，特别是在原始训练条件下（无数据增强、相同优化器设置）。

Method: 在MNIST、CIFAR-10和CIFAR-100上评估HRM，采用原始训练策略：无数据增强、相同优化器（一周期预热+余弦衰减）、标签平滑。与简单的CNN基线对比。

Result: MNIST：HRM达到98%测试准确率；CIFAR-10：HRM仅65.0%，CNN基线77.2%；CIFAR-100：HRM仅29.7%，CNN基线45.3%。HRM训练速度慢30倍且过拟合严重。

Conclusion: 对于小分辨率图像分类且无数据增强的情况，当前HRM不如简单卷积架构，但模型修改可能带来改进空间。

Abstract: This paper asks whether the Hierarchical Reasoning Model (HRM) with the two
Transformer-style modules $(f_L,f_H)$, one step (DEQ-style) training, deep
supervision, Rotary Position Embeddings, and RMSNorm can serve as a practical
image classifier. It is evaluated on MNIST, CIFAR-10, and CIFAR-100 under a
deliberately raw regime: no data augmentation, identical optimizer family with
one-epoch warmup then cosine-floor decay, and label smoothing. HRM optimizes
stably and performs well on MNIST ($\approx 98\%$ test accuracy), but on small
natural images it overfits and generalizes poorly: on CIFAR-10, HRM reaches
65.0\% after 25 epochs, whereas a two-stage Conv--BN--ReLU baseline attains
77.2\% while training $\sim 30\times$ faster per epoch; on CIFAR-100, HRM
achieves only 29.7\% test accuracy despite 91.5\% train accuracy, while the
same CNN reaches 45.3\% test with 50.5\% train accuracy. Loss traces and error
analyses indicate healthy optimization but insufficient image-specific
inductive bias for HRM in this regime. It is concluded that, for
small-resolution image classification without augmentation, HRM is not
competitive with even simple convolutional architectures as the HRM currently
exist but this does not exclude possibilities that modifications to the model
may allow it to improve greatly.

</details>


### [37] [Unsupervised Transformer Pre-Training for Images: Self-Distillation, Mean Teachers, and Random Crops](https://arxiv.org/abs/2510.03606)
*Mattia Scardecchia*

Main category: cs.CV

TL;DR: 本文是对DINOv2自监督学习方法的综述，分析了其核心思想（多裁剪视图增强和自蒸馏）、性能表现以及局限性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习（SSL）的最新进展使得学习通用视觉特征成为可能，DINOv2在大多数基准测试中超越了弱监督方法（如OpenCLIP），需要系统分析其方法原理和性能。

Method: 多裁剪视图增强和自蒸馏结合均值教师的方法，使用Transformer骨干网络学习特征。

Result: DINOv2在多种下游任务中超越了其他SSL和WSL方法，并展示了其学习特征的显著涌现特性。

Conclusion: 虽然DINOv2表现出色，但仍存在局限性，需要进一步研究其影响和未来发展方向。

Abstract: Recent advances in self-supervised learning (SSL) have made it possible to
learn general-purpose visual features that capture both the high-level
semantics and the fine-grained spatial structure of images. Most notably, the
recent DINOv2 has established a new state of the art by surpassing weakly
supervised methods (WSL) like OpenCLIP on most benchmarks. In this survey, we
examine the core ideas behind its approach, multi-crop view augmentation and
self-distillation with a mean teacher, and trace their development in previous
work. We then compare the performance of DINO and DINOv2 with other SSL and WSL
methods across various downstream tasks, and highlight some remarkable emergent
properties of their learned features with transformer backbones. We conclude by
briefly discussing DINOv2's limitations, its impact, and future research
directions.

</details>


### [38] [Diffusion-Classifier Synergy: Reward-Aligned Learning via Mutual Boosting Loop for FSCIL](https://arxiv.org/abs/2510.03608)
*Ruitao Wu,Yifan Zhao,Guangyao Chen,Jia Li*

Main category: cs.CV

TL;DR: 提出了Diffusion-Classifier Synergy (DCS)框架，通过扩散模型和分类器的协同进化解决小样本类增量学习中的稳定性和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 当前FSCIL方法因依赖有限数据集而泛化能力不足，直接应用扩散模型进行数据增强可能导致语义不对齐或引导无效。

Method: 采用奖励对齐学习策略，建立扩散模型和分类器之间的相互促进循环，通过特征级和logits级的多层次奖励函数指导扩散模型生成。

Result: 在FSCIL基准测试中达到最先进性能，显著提升了知识保留和新类学习能力。

Conclusion: DCS框架通过协同进化机制有效解决了FSCIL中的关键挑战，证明了扩散模型与分类器协同学习的有效性。

Abstract: Few-Shot Class-Incremental Learning (FSCIL) challenges models to sequentially
learn new classes from minimal examples without forgetting prior knowledge, a
task complicated by the stability-plasticity dilemma and data scarcity. Current
FSCIL methods often struggle with generalization due to their reliance on
limited datasets. While diffusion models offer a path for data augmentation,
their direct application can lead to semantic misalignment or ineffective
guidance. This paper introduces Diffusion-Classifier Synergy (DCS), a novel
framework that establishes a mutual boosting loop between diffusion model and
FSCIL classifier. DCS utilizes a reward-aligned learning strategy, where a
dynamic, multi-faceted reward function derived from the classifier's state
directs the diffusion model. This reward system operates at two levels: the
feature level ensures semantic coherence and diversity using prototype-anchored
maximum mean discrepancy and dimension-wise variance matching, while the logits
level promotes exploratory image generation and enhances inter-class
discriminability through confidence recalibration and cross-session
confusion-aware mechanisms. This co-evolutionary process, where generated
images refine the classifier and an improved classifier state yields better
reward signals, demonstrably achieves state-of-the-art performance on FSCIL
benchmarks, significantly enhancing both knowledge retention and new class
learning.

</details>


### [39] [MonitorVLM:A Vision Language Framework for Safety Violation Detection in Mining Operations](https://arxiv.org/abs/2510.03666)
*Jiang Wu,Sichao Wu,Yinsong Ma,Guangyuan Yu,Haoyuan Xu,Lifang Zheng,Jingliang Duan*

Main category: cs.CV

TL;DR: MonitorVLM是一个新颖的视觉-语言框架，用于从监控视频流中自动检测安全违规行为，特别针对采矿行业的高风险环境。


<details>
  <summary>Details</summary>
Motivation: 传统人工检查在大型动态环境中劳动密集、易出错且效率低下，迫切需要智能自动化的安全监控解决方案。

Method: 提出三个关键创新：1)包含9000个VQA样本的领域特定违规数据集；2)动态选择Top-K相关条款的条款过滤器模块；3)增强工人区域的行为放大模块。

Result: 在精度、召回率和F1分数上分别比基线模型提升22.01%、34.22%和28.37%，推理延迟降低13.56%。

Conclusion: 该研究展示了多模态大模型在提升采矿及其他行业职业安全监控方面的潜力。

Abstract: Industrial accidents, particularly in high-risk domains such as surface and
underground mining, are frequently caused by unsafe worker behaviors.
Traditional manual inspection remains labor-intensive, error-prone, and
insufficient for large-scale, dynamic environments, highlighting the urgent
need for intelligent and automated safety monitoring. In this paper, we present
MonitorVLM, a novel vision--language framework designed to detect safety
violations directly from surveillance video streams. MonitorVLM introduces
three key innovations: (1) a domain-specific violation dataset comprising 9,000
vision--question--answer (VQA) samples across 40 high-frequency mining
regulations, enriched with augmentation and auxiliary detection cues; (2) a
clause filter (CF) module that dynamically selects the Top-$K$ most relevant
clauses, reducing inference latency by 13.56\% while maintaining accuracy; and
(3) a behavior magnifier (BM) module that enhances worker regions to improve
fine-grained action recognition, yielding additional gains of 3.45% in
precision and 8.62% in recall. Experimental results demonstrate that MonitorVLM
significantly outperforms baseline vision--language models, achieving
improvements of 22.01% in precision, 34.22\% in recall, and 28.37% in F1 score
over the 72B unfine-tuned baseline. A lightweight web-based interface further
integrates MonitorVLM into practical workflows, enabling automatic violation
reporting with video timestamping. This study highlights the potential of
multimodal large models to enhance occupational safety monitoring in mining and
beyond.

</details>


### [40] [A Novel Cloud-Based Diffusion-Guided Hybrid Model for High-Accuracy Accident Detection in Intelligent Transportation Systems](https://arxiv.org/abs/2510.03675)
*Siva Sai,Saksham Gupta,Vinay Chamola,Rajkumar Buyya*

Main category: cs.CV

TL;DR: 提出了一种将扩散模型与智能交通系统结合的混合模型，用于交通事故检测，达到了97.32%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统分类方法在理解复杂数据分布方面存在不足，扩散模型具有有效理解复杂数据分布的内在能力，可以克服传统方法的局限性。

Method: 使用ExceptionNet架构的输出作为扩散模型的输入，图像张量作为条件，构建多条件模块来调节线性投影，通过时间嵌入和图像协变量嵌入使网络在扩散过程中动态调整行为。采用云部署解决计算密集型问题。

Result: 在公开数据集上的评估显示，该扩散模型在基于图像的交通事故检测中表现最佳，准确率达到97.32%。

Conclusion: 扩散模型与智能交通系统的集成显著提升了事故检测性能，通过系统研究扩散特性（如时间步调度器、编码技术等）验证了方法的有效性。

Abstract: The integration of Diffusion Models into Intelligent Transportation Systems
(ITS) is a substantial improvement in the detection of accidents. We present a
novel hybrid model integrating guidance classification with diffusion
techniques. By leveraging fine-tuned ExceptionNet architecture outputs as input
for our proposed diffusion model and processing image tensors as our
conditioning, our approach creates a robust classification framework. Our model
consists of multiple conditional modules, which aim to modulate the linear
projection of inputs using time embeddings and image covariate embeddings,
allowing the network to adapt its behavior dynamically throughout the diffusion
process. To address the computationally intensive nature of diffusion models,
our implementation is cloud-based, enabling scalable and efficient processing.
Our strategy overcomes the shortcomings of conventional classification
approaches by leveraging diffusion models inherent capacity to effectively
understand complicated data distributions. We investigate important diffusion
characteristics, such as timestep schedulers, timestep encoding techniques,
timestep count, and architectural design changes, using a thorough ablation
study, and have conducted a comprehensive evaluation of the proposed model
against the baseline models on a publicly available dataset. The proposed
diffusion model performs best in image-based accident detection with an
accuracy of 97.32%.

</details>


### [41] [SAMSOD: Rethinking SAM Optimization for RGB-T Salient Object Detection](https://arxiv.org/abs/2510.03689)
*Zhengyi Liu,Xinrui Wang,Xianyong Fang,Zhengzheng Tu,Linbo Wang*

Main category: cs.CV

TL;DR: 提出SAMSOD模型，通过单模态监督和梯度去冲突解决RGB-T显著性检测中的模态不平衡和梯度差异问题


<details>
  <summary>Details</summary>
Motivation: 现有方法忽略了两种模态的不平衡收敛以及高激活和低激活之间的显著梯度差异，限制了性能提升

Method: 使用单模态监督增强非主导模态学习，采用梯度去冲突减少冲突梯度对模型收敛的影响，利用两个解耦适配器分别掩码高激活和低激活神经元

Result: 在RGB-T SOD基准数据集、涂鸦监督RGB-T SOD、全监督RGB-D SOD数据集以及全监督RGB-D铁路表面缺陷检测上的实验证明了方法的有效性

Conclusion: SAMSOD方法通过解决模态不平衡和梯度差异问题，显著提升了RGB-T显著性检测性能

Abstract: RGB-T salient object detection (SOD) aims to segment attractive objects by
combining RGB and thermal infrared images. To enhance performance, the Segment
Anything Model has been fine-tuned for this task. However, the imbalance
convergence of two modalities and significant gradient difference between high-
and low- activations are ignored, thereby leaving room for further performance
enhancement. In this paper, we propose a model called \textit{SAMSOD}, which
utilizes unimodal supervision to enhance the learning of non-dominant modality
and employs gradient deconfliction to reduce the impact of conflicting
gradients on model convergence. The method also leverages two decoupled
adapters to separately mask high- and low-activation neurons, emphasizing
foreground objects by enhancing background learning. Fundamental experiments on
RGB-T SOD benchmark datasets and generalizability experiments on scribble
supervised RGB-T SOD, fully supervised RGB-D SOD datasets and full-supervised
RGB-D rail surface defect detection all demonstrate the effectiveness of our
proposed method.

</details>


### [42] [Referring Expression Comprehension for Small Objects](https://arxiv.org/abs/2510.03701)
*Kanoko Goto,Takumi Hirose,Mahiro Ukai,Shuhei Kurita,Nakamasa Inoue*

Main category: cs.CV

TL;DR: 本文提出了一种针对小目标物体的指代表达式理解新方法，包括SOREC数据集和PIZA适配器模块，显著提升了小目标定位精度。


<details>
  <summary>Details</summary>
Motivation: 尽管指代表达式理解任务在视觉语言学习方面取得了显著进展，但在自动驾驶等实际应用中，定位极小的目标物体仍然是一个重大挑战。

Method: 提出了小目标REC数据集（SOREC）和渐进式迭代缩放适配器（PIZA），该适配器模块通过参数高效微调使模型能够逐步放大并定位小物体。

Result: 在SOREC数据集上的实验表明，PIZA应用于GroundingDINO模型后，小目标定位精度得到显著提升。

Conclusion: 该方法为解决小目标物体定位问题提供了有效解决方案，相关数据集、代码和预训练模型已公开。

Abstract: Referring expression comprehension (REC) aims to localize the target object
described by a natural language expression. Recent advances in vision-language
learning have led to significant performance improvements in REC tasks.
However, localizing extremely small objects remains a considerable challenge
despite its importance in real-world applications such as autonomous driving.
To address this issue, we introduce a novel dataset and method for REC
targeting small objects. First, we present the small object REC (SOREC)
dataset, which consists of 100,000 pairs of referring expressions and
corresponding bounding boxes for small objects in driving scenarios. Second, we
propose the progressive-iterative zooming adapter (PIZA), an adapter module for
parameter-efficient fine-tuning that enables models to progressively zoom in
and localize small objects. In a series of experiments, we apply PIZA to
GroundingDINO and demonstrate a significant improvement in accuracy on the
SOREC dataset. Our dataset, codes and pre-trained models are publicly available
on the project page.

</details>


### [43] [Artery-Vein Segmentation from Fundus Images using Deep Learning](https://arxiv.org/abs/2510.03717)
*Sharan SK,Subin Sahayam,Umarani Jayaraman,Lakshmi Priya A*

Main category: cs.CV

TL;DR: 提出了一种基于注意力机制的Attention-WNet深度学习模型，用于视网膜血管的动脉-静脉分割，在HRF和DRIVE数据集上表现优于现有最先进模型。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割是视网膜血管分析的前提，能够为各种视网膜眼病的识别和诊断提供潜在生物标志物。血管规律性和宽度的变化可以反映全身血管系统的健康状况，有助于识别中风和心肌梗死等血管疾病的高危患者。

Method: 将注意力机制整合到WNet深度学习模型中，构建了Attention-WNet模型，专门用于视网膜动脉-静脉分割任务。

Result: 在公开的HRF和DRIVE数据集上进行测试，提出的方法在性能上超越了文献中现有的最先进模型。

Conclusion: 基于注意力机制的Attention-WNet模型在视网膜动脉-静脉分割任务中表现出色，为视网膜血管分析提供了有效的技术方案。

Abstract: Segmenting of clinically important retinal blood vessels into arteries and
veins is a prerequisite for retinal vessel analysis. Such analysis can provide
potential insights and bio-markers for identifying and diagnosing various
retinal eye diseases. Alteration in the regularity and width of the retinal
blood vessels can act as an indicator of the health of the vasculature system
all over the body. It can help identify patients at high risk of developing
vasculature diseases like stroke and myocardial infarction. Over the years,
various Deep Learning architectures have been proposed to perform retinal
vessel segmentation. Recently, attention mechanisms have been increasingly used
in image segmentation tasks. The work proposes a new Deep Learning approach for
artery-vein segmentation. The new approach is based on the Attention mechanism
that is incorporated into the WNet Deep Learning model, and we call the model
as Attention-WNet. The proposed approach has been tested on publicly available
datasets such as HRF and DRIVE datasets. The proposed approach has outperformed
other state-of-art models available in the literature.

</details>


### [44] [Person-Centric Annotations of LAION-400M: Auditing Bias and Its Transfer to Models](https://arxiv.org/abs/2510.03721)
*Leander Girrbach,Stephan Alaniz,Genevieve Smith,Trevor Darrell,Zeynep Akata*

Main category: cs.CV

TL;DR: 该研究为LAION-400M数据集创建了人口统计注释，揭示了训练数据组成与下游模型偏见之间的经验联系，发现60-70%的性别偏见可由数据中的直接共现线性解释。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在大规模多模态数据集上训练显示出强烈的人口统计偏见，但训练数据在产生这些偏见中的作用尚不清楚，主要障碍是缺乏人口统计注释。

Method: 通过结合目标检测、多模态字幕生成和微调分类器的验证自动标注流程，为整个数据集创建以人为中心的注释，包括超过2.76亿个边界框、感知性别和种族/民族标签以及自动生成的标题。

Result: 揭示了人口统计不平衡和有害关联，例如男性和被感知为黑人或中东裔的个体与犯罪相关和负面内容的不成比例关联。60-70%的CLIP和Stable Diffusion中的性别偏见可由数据中的直接共现线性解释。

Conclusion: 建立的资源首次在大规模上确立了数据集组成与下游模型偏见之间的经验联系，为理解和缓解模型偏见提供了重要基础。

Abstract: Vision-language models trained on large-scale multimodal datasets show strong
demographic biases, but the role of training data in producing these biases
remains unclear. A major barrier has been the lack of demographic annotations
in web-scale datasets such as LAION-400M. We address this gap by creating
person-centric annotations for the full dataset, including over 276 million
bounding boxes, perceived gender and race/ethnicity labels, and automatically
generated captions. These annotations are produced through validated automatic
labeling pipelines combining object detection, multimodal captioning, and
finetuned classifiers. Using them, we uncover demographic imbalances and
harmful associations, such as the disproportionate linking of men and
individuals perceived as Black or Middle Eastern with crime-related and
negative content. We also show that 60-70% of gender bias in CLIP and Stable
Diffusion can be linearly explained by direct co-occurrences in the data. Our
resources establish the first large-scale empirical link between dataset
composition and downstream model bias.

</details>


### [45] [Mapping Rio de Janeiro's favelas: general-purpose vs. satellite-specific neural networks](https://arxiv.org/abs/2510.03725)
*Thomas Hallopeau,Joris Guérin,Laurent Demagistri,Youssef Fouzai,Renata Gracie,Vanderlei Pascoal De Matos,Helen Gurgel,Nadine Dessay*

Main category: cs.CV

TL;DR: 比较两种预训练神经网络在里约热内卢贫民窟检测中的性能：通用网络（大数据量）与专业卫星图像网络（任务特异性），研究任务特异性与数据量哪个对检测性能更重要


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法尚未充分利用预训练神经网络的潜力，需要探索在非正式定居点检测中任务特异性和数据量哪个因素更关键

Method: 使用两种预训练神经网络：1）在大型多样化非特定图像数据集上预训练的通用网络；2）在卫星图像上预训练的专业网络，应用于里约热内卢贫民窟检测任务

Result: 论文通过实验比较两种网络的性能，旨在确定任务特异性或数据量哪个因素对检测性能影响更大

Conclusion: 研究结果为非正式定居点检测领域提供了关于预训练策略选择的指导，明确了任务特异性和数据量之间的权衡关系

Abstract: While deep learning methods for detecting informal settlements have already
been developed, they have not yet fully utilized the potential offered by
recent pretrained neural networks. We compare two types of pretrained neural
networks for detecting the favelas of Rio de Janeiro: 1. Generic networks
pretrained on large diverse datasets of unspecific images, 2. A specialized
network pretrained on satellite imagery. While the latter is more specific to
the target task, the former has been pretrained on significantly more images.
Hence, this research investigates whether task specificity or data volume
yields superior performance in urban informal settlement detection.

</details>


### [46] [LoRA Patching: Exposing the Fragility of Proactive Defenses against Deepfakes](https://arxiv.org/abs/2510.03747)
*Zuomin Qu,Yimao Guo,Qianyue Hu,Wei Lu*

Main category: cs.CV

TL;DR: LoRA patching是一种新型的Deepfake攻击方法，通过向Deepfake生成器注入可插拔的LoRA补丁来绕过最先进的防御系统。该方法还提出了防御性LoRA修补作为补充解决方案。


<details>
  <summary>Details</summary>
Motivation: 当前针对Deepfake的主动防御系统存在鲁棒性和可靠性不足的问题，需要开发更有效的攻击方法来揭示这些防御的弱点。

Method: 提出LoRA修补方法，包括：1）向Deepfake生成器注入LoRA补丁；2）可学习的门控机制防止梯度爆炸；3）多模态特征对齐损失函数确保语义层面的特征对齐。

Result: 仅使用1000个面部样本和单轮微调，LoRA修补就能成功绕过多种主动防御系统，证明了当前防御策略的脆弱性。

Conclusion: 研究揭示了当前Deepfake防御范式的关键弱点，强调需要开发更鲁棒的防御策略。该方法代码已开源。

Abstract: Deepfakes pose significant societal risks, motivating the development of
proactive defenses that embed adversarial perturbations in facial images to
prevent manipulation. However, in this paper, we show that these preemptive
defenses often lack robustness and reliability. We propose a novel approach,
Low-Rank Adaptation (LoRA) patching, which injects a plug-and-play LoRA patch
into Deepfake generators to bypass state-of-the-art defenses. A learnable
gating mechanism adaptively controls the effect of the LoRA patch and prevents
gradient explosions during fine-tuning. We also introduce a Multi-Modal Feature
Alignment (MMFA) loss, encouraging the features of adversarial outputs to align
with those of the desired outputs at the semantic level. Beyond bypassing, we
present defensive LoRA patching, embedding visible warnings in the outputs as a
complementary solution to mitigate this newly identified security
vulnerability. With only 1,000 facial examples and a single epoch of
fine-tuning, LoRA patching successfully defeats multiple proactive defenses.
These results reveal a critical weakness in current paradigms and underscore
the need for more robust Deepfake defense strategies. Our code is available at
https://github.com/ZOMIN28/LoRA-Patching.

</details>


### [47] [The Overlooked Value of Test-time Reference Sets in Visual Place Recognition](https://arxiv.org/abs/2510.03751)
*Mubariz Zaffar,Liangliang Nan,Sebastian Scherer,Julian F. P. Kooij*

Main category: cs.CV

TL;DR: 提出一种基于参考集微调的方法，通过在测试时参考集上微调视觉地点识别模型，显著提升在具有挑战性的数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有VPR方法在测试环境与训练数据集差异较大时性能下降，而测试时的参考集包含目标域信息但未被充分利用。

Method: 提出参考集微调方法，在测试前使用参考集中的图像和姿态信息对预训练VPR模型进行微调。

Result: 在具有挑战性的数据集上平均Recall@1提升约2.3%，且微调后的模型仍保持泛化能力。

Conclusion: 利用测试时参考集进行微调是提升VPR性能的有效方法，能够有效缩小训练-测试域差距。

Abstract: Given a query image, Visual Place Recognition (VPR) is the task of retrieving
an image of the same place from a reference database with robustness to
viewpoint and appearance changes. Recent works show that some VPR benchmarks
are solved by methods using Vision-Foundation-Model backbones and trained on
large-scale and diverse VPR-specific datasets. Several benchmarks remain
challenging, particularly when the test environments differ significantly from
the usual VPR training datasets. We propose a complementary, unexplored source
of information to bridge the train-test domain gap, which can further improve
the performance of State-of-the-Art (SOTA) VPR methods on such challenging
benchmarks. Concretely, we identify that the test-time reference set, the
"map", contains images and poses of the target domain, and must be available
before the test-time query is received in several VPR applications. Therefore,
we propose to perform simple Reference-Set-Finetuning (RSF) of VPR models on
the map, boosting the SOTA (~2.3% increase on average for Recall@1) on these
challenging datasets. Finetuned models retain generalization, and RSF works
across diverse test datasets.

</details>


### [48] [Adaptively Sampling-Reusing-Mixing Decomposed Gradients to Speed Up Sharpness Aware Minimization](https://arxiv.org/abs/2510.03763)
*Jiaxin Deng,Junbiao Pang*

Main category: cs.CV

TL;DR: ARSAM通过自适应采样-重用-混合分解梯度的方法，在保持SAM模型泛化能力的同时，显著加速了Sharpness-Aware Minimization（SAM）的训练过程，提供约40%的速度提升。


<details>
  <summary>Details</summary>
Motivation: SAM虽然能提升模型泛化能力，但计算成本是SGD的两倍，需要每步优化计算两次梯度。为了降低计算开销，需要开发更高效的优化方法。

Method: 将SAM梯度分解为SGD梯度和二阶梯度在一阶梯度上的投影（PSF），并发现PSF在训练过程中动态演变。ARSAM通过重用PSF和及时更新PSF来维持模型泛化能力，同时减少梯度计算次数。

Result: 在CIFAR-10/100等数据集上，ARSAM达到与SAM相当的准确率，同时提供约40%的速度提升。在人体姿态估计和模型量化等挑战性任务中也能有效加速优化而不牺牲性能。

Conclusion: ARSAM是一种高效的SAM加速方法，具有广泛的实用性，能在保持性能的同时显著降低计算成本。

Abstract: Sharpness-Aware Minimization (SAM) improves model generalization but doubles
the computational cost of Stochastic Gradient Descent (SGD) by requiring twice
the gradient calculations per optimization step. To mitigate this, we propose
Adaptively sampling-Reusing-mixing decomposed gradients to significantly
accelerate SAM (ARSAM). Concretely, we firstly discover that SAM's gradient can
be decomposed into the SGD gradient and the Projection of the Second-order
gradient onto the First-order gradient (PSF). Furthermore, we observe that the
SGD gradient and PSF dynamically evolve during training, emphasizing the
growing role of the PSF to achieve a flat minima. Therefore, ARSAM is proposed
to the reused PSF and the timely updated PSF still maintain the model's
generalization ability. Extensive experiments show that ARSAM achieves
state-of-the-art accuracies comparable to SAM across diverse network
architectures. On CIFAR-10/100, ARSAM is comparable to SAM while providing a
speedup of about 40\%. Moreover, ARSAM accelerates optimization for the various
challenge tasks (\textit{e.g.}, human pose estimation, and model quantization)
without sacrificing performance, demonstrating its broad practicality.% The
code is publicly accessible at: https://github.com/ajiaaa/ARSAM.

</details>


### [49] [CoPA: Hierarchical Concept Prompting and Aggregating Network for Explainable Diagnosis](https://arxiv.org/abs/2510.03767)
*Yiheng Dong,Yi Lin,Xin Yang*

Main category: cs.CV

TL;DR: CoPA框架通过概念提示和聚合机制，从视觉编码器的多层提取概念表示，提升临床诊断中深度学习模型的可解释性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型在概念捕获能力上存在局限，仅依赖最终层特征而忽略浅层和多尺度特征，缺乏有效的概念编码指导，阻碍细粒度概念提取。

Method: 提出CoPA框架，包含概念感知嵌入生成器(CEG)从视觉编码器各层提取概念表示，通过概念提示调优(CPT)放大关键概念相关视觉线索，聚合多层视觉表示与文本概念表示对齐。

Result: 在三个公开数据集上的实验表明，CoPA在概念和疾病预测性能上优于现有最先进方法。

Conclusion: CoPA能够有效捕获和利用图像中的概念信息，显著提升概念瓶颈模型在临床诊断中的表现。

Abstract: The transparency of deep learning models is essential for clinical
diagnostics. Concept Bottleneck Model provides clear decision-making processes
for diagnosis by transforming the latent space of black-box models into
human-understandable concepts. However, concept-based methods still face
challenges in concept capture capabilities. These methods often rely on encode
features solely from the final layer, neglecting shallow and multiscale
features, and lack effective guidance in concept encoding, hindering
fine-grained concept extraction. To address these issues, we introduce Concept
Prompting and Aggregating (CoPA), a novel framework designed to capture
multilayer concepts under prompt guidance. This framework utilizes the
Concept-aware Embedding Generator (CEG) to extract concept representations from
each layer of the visual encoder. Simultaneously, these representations serve
as prompts for Concept Prompt Tuning (CPT), steering the model towards
amplifying critical concept-related visual cues. Visual representations from
each layer are aggregated to align with textual concept representations. With
the proposed method, valuable concept-wise information in the images is
captured and utilized effectively, thus improving the performance of concept
and disease prediction. Extensive experimental results demonstrate that CoPA
outperforms state-of-the-art methods on three public datasets. Code is
available at https://github.com/yihengd/CoPA.

</details>


### [50] [Efficiency vs. Efficacy: Assessing the Compression Ratio-Dice Score Relationship through a Simple Benchmarking Framework for Cerebrovascular 3D Segmentation](https://arxiv.org/abs/2510.03769)
*Shimaa Elbana,Ahmad Kamal,Shahd Ahmed Ali,Ahmad Al-Kabbany*

Main category: cs.CV

TL;DR: ZFP压缩技术可以在保持脑血管分割质量的同时，大幅减少3D医学影像数据的大小（最高22.89:1压缩比），促进医学研究的协作和可转移性。


<details>
  <summary>Details</summary>
Motivation: 3D医学影像数据集的规模和复杂性日益增加，给协作研究和数据转移带来了显著障碍，需要有效的压缩技术来缓解这些问题。

Method: 在包含真实血管分割的大规模3D医学数据集上应用ZFP压缩技术的容错模式和固定速率模式，并与未压缩基线的分割质量进行严格比较。

Result: ZFP实现了显著的数据缩减（最高22.89:1压缩比），同时保持了高保真度，平均Dice系数保持在0.87656（基线为0.8774）。

Conclusion: ZFP是一种可行且强大的工具，能够实现大规模医学数据集更高效和可访问的研究，促进社区内的广泛协作。

Abstract: The increasing size and complexity of medical imaging datasets, particularly
in 3D formats, present significant barriers to collaborative research and
transferability. This study investigates whether the ZFP compression technique
can mitigate these challenges without compromising the performance of automated
cerebrovascular segmentation, a critical first step in intracranial aneurysm
detection. We apply ZFP in both its error tolerance and fixed-rate modes to a
large scale, and one of the most recent, datasets in the literature, 3D medical
dataset containing ground-truth vascular segmentations. The segmentation
quality on the compressed volumes is rigorously compared to the uncompressed
baseline (Dice approximately equals 0.8774). Our findings reveal that ZFP can
achieve substantial data reduction--up to a 22.89:1 ratio in error tolerance
mode--while maintaining a high degree of fidelity, with the mean Dice
coefficient remaining high at 0.87656. These results demonstrate that ZFP is a
viable and powerful tool for enabling more efficient and accessible research on
large-scale medical datasets, fostering broader collaboration across the
community.

</details>


### [51] [MambaCAFU: Hybrid Multi-Scale and Multi-Attention Model with Mamba-Based Fusion for Medical Image Segmentation](https://arxiv.org/abs/2510.03786)
*T-Mai Bui,Fares Bougourzi,Fadi Dornaika,Vinh Truong Hoang*

Main category: cs.CV

TL;DR: 提出了一种混合分割架构，结合CNN、Transformer和Mamba注意力融合机制，用于医学图像分割，在保持计算效率的同时提高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在医学图像分割中往往任务特定，性能在不同模态和解剖区域间差异较大，且难以平衡模型复杂性和性能，特别是在临床环境中准确性和效率都至关重要。

Method: 采用三分支编码器整合CNN、Transformer和Mamba注意力融合机制来捕获局部、全局和长程依赖关系，使用多尺度注意力CNN解码器重建细粒度分割图，并通过共注意力门增强特征选择。

Result: 在多个基准数据集上的实验表明，该方法在准确性和泛化性方面优于最先进方法，同时保持可比较的计算复杂度。

Conclusion: 该架构通过有效平衡效率和效果，为多样化医学成像任务提供了实用且可扩展的解决方案。

Abstract: In recent years, deep learning has shown near-expert performance in
segmenting complex medical tissues and tumors. However, existing models are
often task-specific, with performance varying across modalities and anatomical
regions. Balancing model complexity and performance remains challenging,
particularly in clinical settings where both accuracy and efficiency are
critical. To address these issues, we propose a hybrid segmentation
architecture featuring a three-branch encoder that integrates CNNs,
Transformers, and a Mamba-based Attention Fusion (MAF) mechanism to capture
local, global, and long-range dependencies. A multi-scale attention-based CNN
decoder reconstructs fine-grained segmentation maps while preserving contextual
consistency. Additionally, a co-attention gate enhances feature selection by
emphasizing relevant spatial and semantic information across scales during both
encoding and decoding, improving feature interaction and cross-scale
communication. Extensive experiments on multiple benchmark datasets show that
our approach outperforms state-of-the-art methods in accuracy and
generalization, while maintaining comparable computational complexity. By
effectively balancing efficiency and effectiveness, our architecture offers a
practical and scalable solution for diverse medical imaging tasks. Source code
and trained models will be publicly released upon acceptance to support
reproducibility and further research.

</details>


### [52] [Road Damage and Manhole Detection using Deep Learning for Smart Cities: A Polygonal Annotation Approach](https://arxiv.org/abs/2510.03797)
*Rasel Hossen,Diptajoy Mistry,Mushiur Rahman,Waki As Sami Atikur Rahman Hridoy,Sajib Saha,Muhammad Ibrahim*

Main category: cs.CV

TL;DR: 本文提出了一种基于YOLOv9算法的深度学习方法来检测道路损坏和井盖，使用多边形标注提高定位精度，在孟加拉国达卡收集的数据集上取得了78.1%的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 城市安全和基础设施维护是智慧城市发展的关键组成部分，手动监测道路损坏耗时、成本高且容易出错。

Method: 采用YOLOv9算法结合多边形标注，开发了包含1000多张图像的数据集，训练模型检测Broken、Not Broken和Manhole三个类别。

Result: 整体图像级准确率达到78.1%，Broken类F1分数86.7%，Not Broken类89.2%，但Manhole类由于类别不平衡问题F1分数仅为18.2%。

Conclusion: 该方法为发展中国家城市基础设施监测提供了高效且可扩展的解决方案。

Abstract: Urban safety and infrastructure maintenance are critical components of smart
city development. Manual monitoring of road damages is time-consuming, highly
costly, and error-prone. This paper presents a deep learning approach for
automated road damage and manhole detection using the YOLOv9 algorithm with
polygonal annotations. Unlike traditional bounding box annotation, we employ
polygonal annotations for more precise localization of road defects. We develop
a novel dataset comprising more than one thousand images which are mostly
collected from Dhaka, Bangladesh. This dataset is used to train a YOLO-based
model for three classes, namely Broken, Not Broken, and Manhole. We achieve
78.1% overall image-level accuracy. The YOLOv9 model demonstrates strong
performance for Broken (86.7% F1-score) and Not Broken (89.2% F1-score)
classes, with challenges in Manhole detection (18.2% F1-score) due to class
imbalance. Our approach offers an efficient and scalable solution for
monitoring urban infrastructure in developing countries.

</details>


### [53] [Contrastive-SDE: Guiding Stochastic Differential Equations with Contrastive Learning for Unpaired Image-to-Image Translation](https://arxiv.org/abs/2510.03821)
*Venkata Narendra Kotyada,Revanth Eranki,Nagesh Bhattu Sristy*

Main category: cs.CV

TL;DR: 提出了一种结合对比学习和扩散模型的非配对图像转换方法，通过时间相关的对比学习保留领域不变特征，并用对比模型指导预训练的SDE进行图像转换。


<details>
  <summary>Details</summary>
Motivation: 非配对图像转换需要学习源域和目标域之间的映射关系，但缺乏对齐样本。扩散模型在生成任务中表现出色，而对比学习能够学习语义相似性，两者结合可以更好地处理非配对场景。

Method: 使用时间相关的对比学习方法（SimCLR），将图像与其领域不变特征作为正样本对进行训练，学习到的对比模型用于指导预训练的随机微分方程（SDE）进行图像转换。

Result: 在三个常见的非配对图像转换任务上，使用四个评估指标与多个基线方法进行比较，Contrastive-SDE在多个指标上达到与最先进方法相当的结果，且收敛速度显著更快，无需标签监督或分类器训练。

Conclusion: Contrastive-SDE是一种更高效的非配对图像转换替代方案，结合了对比学习和扩散模型的优势，在保持性能的同时提高了效率。

Abstract: Unpaired image-to-image translation involves learning mappings between source
domain and target domain in the absence of aligned or corresponding samples.
Score based diffusion models have demonstrated state-of-the-art performance in
generative tasks. Their ability to approximate complex data distributions
through stochastic differential equations (SDEs) enables them to generate
high-fidelity and diverse outputs, making them particularly well-suited for
unpaired I2I settings. In parallel, contrastive learning provides a powerful
framework for learning semantic similarities without the need for explicit
supervision or paired data. By pulling together representations of semantically
similar samples and pushing apart dissimilar ones, contrastive methods are
inherently aligned with the objectives of unpaired translation. Its ability to
selectively enforce semantic consistency at the feature level makes contrastive
learning particularly effective for guiding generation in unpaired scenarios.
In this work, we propose a time-dependent contrastive learning approach where a
model is trained with SimCLR by considering an image and its domain invarient
feature as a positive pair, enabling the preservation of domain-invariant
features and the discarding of domain-specific ones. The learned contrastive
model then guides the inference of a pretrained SDE for the I2I translation
task. We empirically compare Contrastive-SDE with several baselines across
three common unpaired I2I tasks, using four metrics for evaluation.
Constrastive-SDE achieves comparable results to the state-of-the-art on several
metrics. Furthermore, we observe that our model converges significantly faster
and requires no label supervision or classifier training, making it a more
efficient alternative for this task.

</details>


### [54] [LIBERO-PRO: Towards Robust and Fair Evaluation of Vision-Language-Action Models Beyond Memorization](https://arxiv.org/abs/2510.03827)
*Xueyang Zhou,Yangming Xu,Guiyao Tie,Yongchao Chen,Guowen Zhang,Duanfeng Chu,Pan Zhou,Lichao Sun*

Main category: cs.CV

TL;DR: LIBERO-PRO是LIBERO基准的扩展版本，通过系统性地评估模型在四个维度（操纵对象、初始状态、任务指令和环境）的扰动下的性能，揭示了现有模型严重依赖训练数据的记忆而非真正理解。


<details>
  <summary>Details</summary>
Motivation: 当前LIBERO基准的训练和评估设置存在问题，导致性能估计被夸大，无法公平比较模型。需要更严格的评估方法来检验模型的泛化能力和真实理解。

Method: 引入LIBERO-PRO基准，在四个关键维度上施加合理扰动：操纵对象替换、初始状态变化、任务指令修改和环境布局调整，系统评估模型在扰动条件下的表现。

Result: 实验结果显示，现有模型在标准LIBERO评估中达到90%以上准确率，但在LIBERO-PRO的广义设置下性能崩溃至0.0%，表明模型仅记忆动作序列和环境布局，缺乏真实理解。

Conclusion: 当前评估方法存在严重缺陷，呼吁社区放弃误导性方法，转向对模型泛化能力和理解力的稳健评估。

Abstract: LIBERO has emerged as a widely adopted benchmark for evaluating
Vision-Language-Action (VLA) models; however, its current training and
evaluation settings are problematic, often leading to inflated performance
estimates and preventing fair model comparison. To address these issues, we
introduce LIBERO-PRO, an extended LIBERO benchmark that systematically
evaluates model performance under reasonable perturbations across four
dimensions: manipulated objects, initial states, task instructions, and
environments. Experimental results reveal that, although existing models
achieve over 90% accuracy under the standard LIBERO evaluation, their
performance collapses to 0.0% under our generalized setting. Crucially, this
discrepancy exposes the models' reliance on rote memorization of action
sequences and environment layouts from the training set, rather than genuine
task understanding or environmental perception. For instance, models persist in
executing grasping actions when the target object is replaced with irrelevant
items, and their outputs remain unchanged even when given corrupted
instructions or even messy tokens. These findings expose the severe flaws in
current evaluation practices, and we call on the community to abandon
misleading methodologies in favor of robust assessments of model generalization
and comprehension. Our code is available at:
https://github.com/Zxy-MLlab/LIBERO-PRO.

</details>


### [55] [Mirage: Unveiling Hidden Artifacts in Synthetic Images with Large Vision-Language Models](https://arxiv.org/abs/2510.03840)
*Pranav Sharma,Shivank Garg,Durga Toshniwal*

Main category: cs.CV

TL;DR: 本文介绍了Mirage数据集，用于研究AI生成图像的检测问题，并探索了大型视觉语言模型在可解释AI图像检测中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成模型的进步使得生成的合成图像越来越难以被标准AI检测器识别，但人类仍能区分。为了识别这种差异，需要研究新的检测方法。

Method: 构建了Mirage数据集，包含各种具有可见伪影的AI生成图像，并研究大型视觉语言模型作为可解释AI图像检测工具的可行性。

Result: 实验表明，大型视觉语言模型在检测具有可见伪影的AI生成图像方面非常有效，但在面对缺乏此类线索的图像时性能下降。

Conclusion: 大型视觉语言模型在AI图像检测方面具有潜力，但其性能依赖于图像中是否存在可见的生成伪影，需要进一步研究来提高对无伪影图像的检测能力。

Abstract: Recent advances in image generation models have led to models that produce
synthetic images that are increasingly difficult for standard AI detectors to
identify, even though they often remain distinguishable by humans. To identify
this discrepancy, we introduce \textbf{Mirage}, a curated dataset comprising a
diverse range of AI-generated images exhibiting visible artifacts, where
current state-of-the-art detection methods largely fail. Furthermore, we
investigate whether Large Vision-Language Models (LVLMs), which are
increasingly employed as substitutes for human judgment in various tasks, can
be leveraged for explainable AI image detection. Our experiments on both Mirage
and existing benchmark datasets demonstrate that while LVLMs are highly
effective at detecting AI-generated images with visible artifacts, their
performance declines when confronted with images lacking such cues.

</details>


### [56] [UGround: Towards Unified Visual Grounding with Unrolled Transformers](https://arxiv.org/abs/2510.03853)
*Rui Qian,Xin Yin,Chuanhang Deng,Zhiyuan Peng,Jian Xiong,Wei Zhai,Dejing Dou*

Main category: cs.CV

TL;DR: UGround是一种统一的视觉定位范式，通过动态选择Transformer中间层作为"掩码提示"，解决了传统方法依赖固定最后一层和缺乏显式空间线索的问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉定位方法存在两个主要问题：(1) 依赖固定的最后一层隐藏层，导致层间传播误差累积；(2) 使用<SEG>作为提示，缺乏显式的空间线索。

Method: 提出Policy-Prompted Masking方法，包含两个关键组件：Stochastic Skip Connection（SSC）通过强化学习策略动态选择连接层，Mask as Prompt（MasP）使用相似度图作为软逻辑掩码来提示SAM生成掩码。

Result: UGround首次在单一框架内统一了从传统参考表达式分割到推理分割、单目标到多目标、正查询到错误前提等各种视觉定位任务。

Conclusion: UGround通过动态层选择和显式空间提示，有效改进了视觉定位性能，代码和模型已开源。

Abstract: We present UGround, a \textbf{U}nified visual \textbf{Ground}ing paradigm
that dynamically selects intermediate layers across \textbf{U}nrolled
transformers as ``mask as prompt'', diverging from the prevailing pipeline that
leverages the fixed last hidden layer as ``\texttt{<SEG>} as prompt''. UGround
addresses two primary challenges posed by the prevailing paradigm: (1) its
reliance on the fixed last hidden layer, which sequentially amplifies
cumulative errors arising from layer-by-layer propagation without intermediate
correction, and (2) its use of \texttt{<SEG>} as a prompt, which implicitly
projects textual embeddings into visual space without explicit spatial cues
(\eg, coordinates). Central to UGround is Policy-Prompted Masking, which
comprises two key components: Stochastic Skip Connection (SSC) and Mask as
Prompt (MasP). SSC is a reinforcement learning policy that, via stochastic
sampling, allows each \texttt{<SEG>} token to slide across unrolled transformer
layers, enabling dynamic layer selection at which it connects to the vision
model (\eg, SAM) in a skip-connection fashion. Given the selected hidden layer,
MasP uses the similarity map derived from the \texttt{<SEG>} token and image
tokens as a soft logit mask to prompt SAM for mask generation, offering
explicit spatial cues through its activation regions. To validate the
effectiveness of UGround, we, for the first time, have unified visual grounding
within a single framework from an attribute perspective, spanning from
traditional refer expression segmentation to newly proposed reasoning
segmentation, single-target to multi-target, positive query to false premise
(empty target). All codes and models are publicly available at
\href{https://github.com/rui-qian/UGround}{https://github.com/rui-qian/UGround}.

</details>


### [57] [Optimized Minimal 4D Gaussian Splatting](https://arxiv.org/abs/2510.03857)
*Minseo Lee,Byeonghyeon Lee,Lucas Yunkyu Lee,Eunsoo Lee,Sangmin Kim,Seunghyeon Song,Joo Chan Lee,Jong Hwan Ko,Jaesik Park,Eunbyung Park*

Main category: cs.CV

TL;DR: OMG4是一个优化的4D高斯泼溅框架，通过三阶段渐进式高斯修剪和压缩技术，显著减少模型存储开销（超过60%）同时保持重建质量。


<details>
  <summary>Details</summary>
Motivation: 4D高斯泼溅技术面临存储开销大的挑战，现有方法在压缩比和视觉质量之间存在权衡限制。

Method: 采用三阶段渐进式修剪：高斯采样识别关键基元、高斯修剪去除冗余、高斯合并融合相似基元；集成隐式外观压缩和广义子向量量化技术。

Result: 在标准基准数据集上的实验表明，OMG4显著优于现有最先进方法，模型大小减少超过60%的同时保持重建质量。

Conclusion: OMG4是紧凑4D场景表示的重要进展，为广泛应用开辟了新可能性。

Abstract: 4D Gaussian Splatting has emerged as a new paradigm for dynamic scene
representation, enabling real-time rendering of scenes with complex motions.
However, it faces a major challenge of storage overhead, as millions of
Gaussians are required for high-fidelity reconstruction. While several studies
have attempted to alleviate this memory burden, they still face limitations in
compression ratio or visual quality. In this work, we present OMG4 (Optimized
Minimal 4D Gaussian Splatting), a framework that constructs a compact set of
salient Gaussians capable of faithfully representing 4D Gaussian models. Our
method progressively prunes Gaussians in three stages: (1) Gaussian Sampling to
identify primitives critical to reconstruction fidelity, (2) Gaussian Pruning
to remove redundancies, and (3) Gaussian Merging to fuse primitives with
similar characteristics. In addition, we integrate implicit appearance
compression and generalize Sub-Vector Quantization (SVQ) to 4D representations,
further reducing storage while preserving quality. Extensive experiments on
standard benchmark datasets demonstrate that OMG4 significantly outperforms
recent state-of-the-art methods, reducing model sizes by over 60% while
maintaining reconstruction quality. These results position OMG4 as a
significant step forward in compact 4D scene representation, opening new
possibilities for a wide range of applications. Our source code is available at
https://minshirley.github.io/OMG4/.

</details>


### [58] [Cross-View Open-Vocabulary Object Detection in Aerial Imagery](https://arxiv.org/abs/2510.03858)
*Jyoti Kini,Rohit Gupta,Mubarak Shah*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的开放词汇目标检测框架，专门针对航空图像领域，通过结构化域对齐方法将地面视图的预训练模型知识迁移到航空图像中，在零样本设置下显著提升了多个航空数据集的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统目标检测模型需要固定类别训练，缺乏灵活性且难以扩展新类别。虽然开放词汇检测可以识别未见类别，但地面视图预训练模型直接应用于航空图像存在域偏移、视角变化和尺度差异等问题，需要专门的适应策略。

Method: 提出结构化域对齐框架：1）对比图像到图像对齐，增强航空和地面视图嵌入的相似性；2）多实例词汇关联，对齐航空图像与文本嵌入。利用对比训练的预训练模型进行知识迁移。

Result: 在xView、DOTAv2、VisDrone、DIOR和HRRSD数据集上验证，零样本设置下相比微调的封闭词汇模型，在DOTAv2上提升+6.32 mAP，VisDrone上+4.16 mAP，HRRSD上+3.46 mAP。

Conclusion: 该方法为航空应用中的目标检测系统提供了更灵活和可扩展的解决方案，有效解决了跨域知识迁移的挑战。

Abstract: Traditional object detection models are typically trained on a fixed set of
classes, limiting their flexibility and making it costly to incorporate new
categories. Open-vocabulary object detection addresses this limitation by
enabling models to identify unseen classes without explicit training.
Leveraging pretrained models contrastively trained on abundantly available
ground-view image-text classification pairs provides a strong foundation for
open-vocabulary object detection in aerial imagery. Domain shifts, viewpoint
variations, and extreme scale differences make direct knowledge transfer across
domains ineffective, requiring specialized adaptation strategies. In this
paper, we propose a novel framework for adapting open-vocabulary
representations from ground-view images to solve object detection in aerial
imagery through structured domain alignment. The method introduces contrastive
image-to-image alignment to enhance the similarity between aerial and
ground-view embeddings and employs multi-instance vocabulary associations to
align aerial images with text embeddings. Extensive experiments on the xView,
DOTAv2, VisDrone, DIOR, and HRRSD datasets are used to validate our approach.
Our open-vocabulary model achieves improvements of +6.32 mAP on DOTAv2, +4.16
mAP on VisDrone (Images), and +3.46 mAP on HRRSD in the zero-shot setting when
compared to finetuned closed-vocabulary dataset-specific model performance,
thus paving the way for more flexible and scalable object detection systems in
aerial applications.

</details>


### [59] [Exploring the Challenge and Value of Deep Learning in Automated Skin Disease Diagnosis](https://arxiv.org/abs/2510.03869)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 这篇综述论文探讨了深度学习在皮肤癌诊断中的应用，重点分析了当前面临的挑战（如复杂特征、图像噪声、类内变异等）以及应对策略（数据增强、混合模型、特征融合等），并讨论了将DL模型整合到临床工作流程中的潜力。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌是全球最普遍和致命的癌症之一，早期检测和诊断对改善患者预后至关重要。深度学习在自动皮肤疾病诊断中显示出巨大潜力，但仍面临诸多挑战需要解决。

Method: 本文采用基于PRISMA框架的综合方法，综述了近期研究，分析了DL在皮肤癌诊断中的创新方法，包括数据增强、混合模型和特征融合等技术。

Result: 研究发现深度学习模型能够显著提高皮肤癌诊断的准确性和效率，但需要解决数据不平衡、类间相似性等挑战才能充分发挥其临床潜力。

Conclusion: 深度学习有望彻底改变皮肤疾病诊断方式并改善临床决策，但需要持续的技术进步才能完全释放其在皮肤病护理中的变革潜力。

Abstract: Skin cancer is one of the most prevalent and deadly forms of cancer
worldwide, which highlights the critical importance of early detection and
diagnosis in improving patient outcomes. Deep learning (DL) has shown
significant promise in enhancing the accuracy and efficiency of automated skin
disease diagnosis, particularly in detecting and evaluating skin lesions and
classification. However, there are still several challenges for DL-based skin
cancer diagnosis, including complex features, image noise, intra-class
variation, inter-class similarity, and data imbalance. By synthesizing recent
research, this review discusses innovative approaches to cope with these
challenges, such as data augmentation, hybrid models, and feature fusion, etc.
Furthermore, the review highlights the integration of DL models into clinical
workflows, offering insights into the potential of deep learning to
revolutionize skin disease diagnosis and improve clinical decision-making. This
article follows a comprehensive methodology based on the PRISMA framework and
emphasizes the need for continued advancements to fully unlock the
transformative potential of DL in dermatological care.

</details>


### [60] [SDAKD: Student Discriminator Assisted Knowledge Distillation for Super-Resolution Generative Adversarial Networks](https://arxiv.org/abs/2510.03870)
*Nikolaos Kaparinos,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 提出了一种名为SDAKD的新方法，通过引入学生判别器来解决GAN知识蒸馏中的容量不匹配问题，在图像超分辨率任务上取得了优于基线方法和现有SOTA方法的效果。


<details>
  <summary>Details</summary>
Motivation: GANs在生成任务中表现出色，但计算需求大，难以部署在资源受限设备上。知识蒸馏是GAN压缩的有前景方向，但由于学生生成器和教师判别器之间的容量不匹配，有效训练较小的学生生成器具有挑战性。

Method: 提出SDAKD方法，引入学生判别器来缓解容量不匹配问题。采用三阶段训练策略，并在最后两个训练阶段集成适应的特征图蒸馏方法。在GCFSR和Real-ESRGAN两个超分辨率GAN上进行了评估。

Result: 实验表明，SDAKD在基线方法和SOTA GAN知识蒸馏方法上取得了一致的改进效果。

Conclusion: SDAKD是一种有效的GAN蒸馏方法，通过学生判别器成功解决了容量不匹配问题，在超分辨率任务上表现优异。

Abstract: Generative Adversarial Networks (GANs) achieve excellent performance in
generative tasks, such as image super-resolution, but their computational
requirements make difficult their deployment on resource-constrained devices.
While knowledge distillation is a promising research direction for GAN
compression, effectively training a smaller student generator is challenging
due to the capacity mismatch between the student generator and the teacher
discriminator. In this work, we propose Student Discriminator Assisted
Knowledge Distillation (SDAKD), a novel GAN distillation methodology that
introduces a student discriminator to mitigate this capacity mismatch. SDAKD
follows a three-stage training strategy, and integrates an adapted feature map
distillation approach in its last two training stages. We evaluated SDAKD on
two well-performing super-resolution GANs, GCFSR and Real-ESRGAN. Our
experiments demonstrate consistent improvements over the baselines and SOTA GAN
knowledge distillation methods. The SDAKD source code will be made openly
available upon acceptance of the paper.

</details>


### [61] [PoseGaze-AHP: A Knowledge-Based 3D Dataset for AI-Driven Ocular and Postural Diagnosis](https://arxiv.org/abs/2510.03873)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 提出了PoseGaze-AHP数据集，这是首个专门用于AI驱动的眼源性异常头位诊断的公开3D数据集，同步采集头部姿态和注视运动信息。


<details>
  <summary>Details</summary>
Motivation: 现有数据集分别关注头部姿态和眼部运动，限制了眼源性异常头位综合诊断方法的发展和AI在AHP分析中的进步。

Method: 使用Claude 3.5 Sonnet模型通过迭代过程从医学文献中提取结构化临床数据，采用逐步、分层和复杂提示策略，然后使用神经头部化身框架将提取的记录系统化填充并转换为3D表示。

Result: 数据集包含7,920张图像，涵盖广泛的眼部病症，提取方法整体准确率达到91.92%，证明了临床数据集构建的可靠性。

Conclusion: PoseGaze-AHP是首个公开可用的眼源性AHP诊断资源，支持开发准确且符合隐私要求的诊断工具。

Abstract: Diagnosing ocular-induced abnormal head posture (AHP) requires a
comprehensive analysis of both head pose and ocular movements. However,
existing datasets focus on these aspects separately, limiting the development
of integrated diagnostic approaches and restricting AI-driven advancements in
AHP analysis. To address this gap, we introduce PoseGaze-AHP, a novel 3D
dataset that synchronously captures head pose and gaze movement information for
ocular-induced AHP assessment. Structured clinical data were extracted from
medical literature using large language models (LLMs) through an iterative
process with the Claude 3.5 Sonnet model, combining stepwise, hierarchical, and
complex prompting strategies. The extracted records were systematically imputed
and transformed into 3D representations using the Neural Head Avatar (NHA)
framework. The dataset includes 7,920 images generated from two head textures,
covering a broad spectrum of ocular conditions. The extraction method achieved
an overall accuracy of 91.92%, demonstrating its reliability for clinical
dataset construction. PoseGaze-AHP is the first publicly available resource
tailored for AI-driven ocular-induced AHP diagnosis, supporting the development
of accurate and privacy-compliant diagnostic tools.

</details>


### [62] [DHQA-4D: Perceptual Quality Assessment of Dynamic 4D Digital Human](https://arxiv.org/abs/2510.03874)
*Yunhao Li,Sijing Wu,Yucheng Zhu,Huiyu Duan,Zicheng Zhang,Guangtao Zhai*

Main category: cs.CV

TL;DR: 该论文提出了DHQA-4D数据集和DynaMesh-Rater方法，用于动态4D数字人网格的质量评估。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描技术的发展，动态4D数字人网格在应用过程中容易受到各种噪声影响，需要有效的质量评估方法。

Method: 构建包含32个高质量4D网格序列和1920个失真样本的DHQA-4D数据集；提出DynaMesh-Rater方法，通过提取视觉特征、运动特征和几何特征，利用大语言模型进行多模态特征融合和LoRA指令调优。

Result: 在DHQA-4D数据集上的实验结果表明，DynaMesh-Rater方法优于现有的质量评估方法。

Conclusion: 该工作为动态4D数字人网格质量评估提供了有效的数据集和方法，具有重要的应用价值。

Abstract: With the rapid development of 3D scanning and reconstruction technologies,
dynamic digital human avatars based on 4D meshes have become increasingly
popular. A high-precision dynamic digital human avatar can be applied to
various fields such as game production, animation generation, and remote
immersive communication. However, these 4D human avatar meshes are prone to
being degraded by various types of noise during the processes of collection,
compression, and transmission, thereby affecting the viewing experience of
users. In light of this fact, quality assessment of dynamic 4D digital humans
becomes increasingly important. In this paper, we first propose a large-scale
dynamic digital human quality assessment dataset, DHQA-4D, which contains 32
high-quality real-scanned 4D human mesh sequences, 1920 distorted textured 4D
human meshes degraded by 11 textured distortions, as well as their
corresponding textured and non-textured mean opinion scores (MOSs). Equipped
with DHQA-4D dataset, we analyze the influence of different types of distortion
on human perception for textured dynamic 4D meshes and non-textured dynamic 4D
meshes. Additionally, we propose DynaMesh-Rater, a novel large multimodal model
(LMM) based approach that is able to assess both textured 4D meshes and
non-textured 4D meshes. Concretely, DynaMesh-Rater elaborately extracts
multi-dimensional features, including visual features from a projected 2D
video, motion features from cropped video clips, and geometry features from the
4D human mesh to provide comprehensive quality-related information. Then we
utilize a LMM model to integrate the multi-dimensional features and conduct a
LoRA-based instruction tuning technique to teach the LMM model to predict the
quality scores. Extensive experimental results on the DHQA-4D dataset
demonstrate the superiority of our DynaMesh-Rater method over previous quality
assessment methods.

</details>


### [63] [Skin Lesion Classification Based on ResNet-50 Enhanced With Adaptive Spatial Feature Fusion](https://arxiv.org/abs/2510.03876)
*Runhao Liu,Ziming Chen,Peng Zhang*

Main category: cs.CV

TL;DR: 提出了一种改进的ResNet-50模型，结合自适应空间特征融合(ASFF)技术，用于皮肤癌分类任务，在ISIC 2020数据集上取得了93.18%的准确率和优异的性能指标。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌分类面临类间相似性高、类内变异大以及皮肤镜图像噪声等问题，需要更有效的特征表示方法来提升分类性能。

Method: 基于ResNet-50架构，引入自适应空间特征融合机制，采用双分支设计融合高级语义特征和中级细节特征，通过全局平均池化和全连接层生成自适应权重进行加权融合。

Result: 在ISIC 2020数据集(3297张图像)上，模型达到93.18%准确率，AUC值分别为0.9670(PR曲线)和0.9717(ROC曲线)，优于5种经典CNN模型。Grad-CAM可视化证明模型能有效聚焦病灶区域。

Conclusion: 该方法为计算机辅助皮肤癌诊断提供了更有效和高效的解决方案，通过自适应特征融合增强了特征学习能力。

Abstract: Skin cancer classification remains a challenging problem due to high
inter-class similarity, intra-class variability, and image noise in dermoscopic
images. To address these issues, we propose an improved ResNet-50 model
enhanced with Adaptive Spatial Feature Fusion (ASFF), which adaptively
integrates multi-scale semantic and surface features to improve feature
representation and reduce overfitting. The ResNet-50 model is enhanced with an
adaptive feature fusion mechanism to achieve more effective multi-scale feature
extraction and improve overall performance. Specifically, a dual-branch design
fuses high-level semantic and mid-level detail features, which are processed
through global average pooling and fully connected layers to generate adaptive
weights for weighted fusion, thereby strengthening feature learning and
reducing the impact of noise on classification. The method is evaluated on a
subset of the ISIC 2020 dataset containing 3297 benign and malignant skin
lesion images. Experimental results show that the proposed ASFF-based ResNet-50
achieves the best overall performance compared with 5 classic convolutional
neural networks (CNNs) models. The proposed model reached an accuracy of 93.18%
along with higher precision, recall, specificity, and F1 score. The improved
model achieves an AUC value of 0.9670 and 0.9717 in the P-R and ROC curve,
respectively. Then, the evaluation based on Grad-CAM further proved that the
improved model adaptively focuses on lesion-relevant regions while suppressing
irrelevant background information, thereby validating its enhanced feature
learning capability from a deep representation perspective. These findings
demonstrate that the proposed approach provides a more effective and efficient
solution for computer-aided skin cancer diagnosis.

</details>


### [64] [Multi-Modal Oral Cancer Detection Using Weighted Ensemble Convolutional Neural Networks](https://arxiv.org/abs/2510.03878)
*Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R Ajo Babu George,Sreehari J R*

Main category: cs.CV

TL;DR: 开发多模态深度学习框架，通过加权集成DenseNet-121 CNN整合临床、放射学和病理学图像，提高口腔鳞状细胞癌的早期检测能力


<details>
  <summary>Details</summary>
Motivation: 口腔鳞状细胞癌的晚期诊断导致高死亡率，超过50%的病例在晚期被发现，5年生存率低于50%，需要改进早期检测方法

Method: 使用公开数据集训练三个DenseNet-121 CNN模型，分别对应不同医学成像模态，采用数据增强和模态特定预处理，通过验证加权集成策略融合预测结果

Result: 放射学模态验证准确率100%，病理学模态95.12%，临床图像63.10%（因视觉异质性），集成模型在多模态验证集上达到84.58%的总体准确率

Conclusion: 多模态集成框架为当前诊断流程提供非侵入性AI辅助分诊工具，支持临床决策，符合全球肿瘤学指南，有助于减少诊断延迟和改善患者预后

Abstract: Aims Late diagnosis of Oral Squamous Cell Carcinoma (OSCC) contributes
significantly to its high global mortality rate, with over 50\% of cases
detected at advanced stages and a 5-year survival rate below 50\% according to
WHO statistics. This study aims to improve early detection of OSCC by
developing a multimodal deep learning framework that integrates clinical,
radiological, and histopathological images using a weighted ensemble of
DenseNet-121 convolutional neural networks (CNNs). Material and Methods A
retrospective study was conducted using publicly available datasets
representing three distinct medical imaging modalities. Each modality-specific
dataset was used to train a DenseNet-121 CNN via transfer learning.
Augmentation and modality-specific preprocessing were applied to increase
robustness. Predictions were fused using a validation-weighted ensemble
strategy. Evaluation was performed using accuracy, precision, recall, F1-score.
Results High validation accuracy was achieved for radiological (100\%) and
histopathological (95.12\%) modalities, with clinical images performing lower
(63.10\%) due to visual heterogeneity. The ensemble model demonstrated improved
diagnostic robustness with an overall accuracy of 84.58\% on a multimodal
validation dataset of 55 samples. Conclusion The multimodal ensemble framework
bridges gaps in the current diagnostic workflow by offering a non-invasive,
AI-assisted triage tool that enhances early identification of high-risk
lesions. It supports clinicians in decision-making, aligning with global
oncology guidelines to reduce diagnostic delays and improve patient outcomes.

</details>


### [65] [Exploring Instruction Data Quality for Explainable Image Quality Assessment](https://arxiv.org/abs/2510.03880)
*Yunhao Li,Sijing Wu,Huiyu Duan,Yucheng Zhu,Qi Jia,Guangtao Zhai*

Main category: cs.CV

TL;DR: 本文挑战了扩展定律，提出了一种基于聚类的数据选择框架IQA-Select，用于可解释图像质量评估的指令调优，仅需10%数据即可达到甚至超过全量数据性能。


<details>
  <summary>Details</summary>
Motivation: 当前可解释图像质量评估方法依赖大规模指令调优数据，但大量数据会导致计算成本高且存在冗余，反而可能损害模型性能。

Method: 提出三阶段聚类数据选择框架：聚类特征提取、聚类配额分配和聚类采样策略，系统分析各阶段选择并开发IQA-Select方法。

Result: 在Q-Bench和AesBench上，仅使用10%选定数据即可分别达到全量微调性能的102.1%和103.7%，显著降低计算成本。

Conclusion: 数据质量比数量更重要，IQA-Select方法能有效减少冗余数据，在保持性能的同时大幅降低计算开销。

Abstract: In recent years, with the rapid development of powerful multimodal large
language models (MLLMs), explainable image quality assessment (IQA) has
gradually become popular, aiming at providing quality-related descriptions and
answers of images. To achieve this goal, recent methods seek to construct a
large-scale instruction tuning dataset to empower the MLLM with quality
perception ability following the well-known scaling law. However, a large
amount of instruction tuning data may cause substantial computational costs and
redundant data, which in turn will cause harm to the performance of the model.
To cope with this problem, in this paper, we challenge the scaling law and
systematically investigate the role of data quality of the instruction tuning
dataset for explainable IQA. Using a powerful pre-trained MLLM, we first
investigate the changes in model performance after fine-tuning with different
sizes of instruction tuning data. We find that selecting a subset of the data
set randomly using an appropriate ratio can even lead to better results than
training with the entire instruction tuning dataset, demonstrating the
redundancy of current explainable IQA instruction tuning data. Beyond randomly
sampling a subset, we propose a clustering-based data selection framework with
three stages: clustering feature extraction, cluster quota allocation, and
cluster sampling strategy. Then we systematically analyze the choices of each
stage and propose a simple but efficient data selection method IQA-Select for
explainable IQA. The experimental results demonstrate that IQA-Select can
achieve 102.1% and 103.7% performance of full fine-tuning using only 10%
selected data in Q-Bench and AesBench respectively, significantly reducing
computational costs while achieving better performance.

</details>


### [66] [Bridge Thinking and Acting: Unleashing Physical Potential of VLM with Generalizable Action Expert](https://arxiv.org/abs/2510.03896)
*Mingyu Liu,Zheng Huang,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Yating Wang,Haoyi Zhu,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 该论文提出了一种基于可泛化动作专家的框架，使用稀疏3D轨迹作为中间表示，将VLM的高层规划能力与低层物理动作模块连接起来，解决传统VLA模型泛化能力差和双系统方法语义模糊的问题。


<details>
  <summary>Details</summary>
Motivation: 传统视觉-语言-动作模型由于数据稀缺和领域狭窄而泛化能力差，双系统方法又存在动作模块语义模糊问题，导致跨任务训练困难且需要在新环境中进行微调。

Method: 采用"动作预训练、点云微调"范式，VLM仅生成粗略3D路径点，可泛化动作专家通过实时点云观测将其细化为密集可执行动作序列。

Result: 该方法结合了VLM在视觉理解和规划方面的广泛泛化能力与动作专家在动作级别的细粒度泛化能力。

Conclusion: 提出的框架有效解决了VLA模型在物理世界部署时的泛化挑战，为机器人在新环境中的适应性提供了可行方案。

Abstract: Although Vision-Language Models (VLM) have demonstrated impressive planning
and reasoning capabilities, translating these abilities into the physical world
introduces significant challenges. Conventional Vision-Language-Action (VLA)
models, which integrate reasoning and action into a monolithic architecture,
generalize poorly because they are constrained by scarce, narrow-domain data.
While recent dual-system approaches attempt to decouple "thinking" from
"acting", they are often constrained by semantic ambiguities within the action
module. This ambiguity makes large-scale, cross-task training infeasible.
Consequently, these systems typically necessitate fine-tuning on newly
collected data when deployed to novel environments, and the cooperation
mechanism between the two systems remains ill-defined. To address these
limitations, we introduce, for the first time, a framework centered around a
generalizable action expert. Our approach utilizes sparse 3D trajectories as an
intermediate representation, effectively bridging the high-level planning
capabilities of the VLM with the low-level physical action module. During the
planning phase, the VLM is only required to generate coarse 3D waypoints. These
waypoints are then processed by our generalizable action expert, which refines
them into dense, executable action sequences by sampling real-time point cloud
observations of the environment. To promote training efficiency and robust
generalization, we introduce a novel "Action Pre-training, Pointcloud
Fine-tuning" paradigm. Our method combines the broad generalization
capabilities of VLMs in visual understanding and planning with the
fine-grained, action-level generalization of action expert.

</details>


### [67] [Zero-Shot Fine-Grained Image Classification Using Large Vision-Language Models](https://arxiv.org/abs/2510.03903)
*Md. Atabuzzaman,Andrew Zhang,Chris Thomas*

Main category: cs.CV

TL;DR: 提出了一种将零样本细粒度图像分类转化为视觉问答框架的新方法，利用大视觉语言模型的综合理解能力，并通过注意力干预技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索大视觉语言模型在零样本细粒度图像分类中的潜力，该任务需要精确区分视觉上相似的类别，但目前研究不足。

Method: 将零样本细粒度图像分类转化为视觉问答框架，利用LVLMs的综合理解能力而非直接生成类别名称，采用新颖的注意力干预技术提升性能，并开发更全面的类别描述基准数据集。

Result: 在多个细粒度图像分类基准测试中，该方法始终优于当前最先进的方法，证明了方法的有效性以及LVLMs在零样本细粒度分类任务中的广泛潜力。

Conclusion: 该方法成功展示了LVLMs在零样本细粒度图像分类中的强大能力，为相关领域提供了新的研究方向和应用可能性。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated impressive performance
on vision-language reasoning tasks. However, their potential for zero-shot
fine-grained image classification, a challenging task requiring precise
differentiation between visually similar categories, remains underexplored. We
present a novel method that transforms zero-shot fine-grained image
classification into a visual question-answering framework, leveraging LVLMs'
comprehensive understanding capabilities rather than relying on direct class
name generation. We enhance model performance through a novel attention
intervention technique. We also address a key limitation in existing datasets
by developing more comprehensive and precise class description benchmarks. We
validate the effectiveness of our method through extensive experimentation
across multiple fine-grained image classification benchmarks. Our proposed
method consistently outperforms the current state-of-the-art (SOTA) approach,
demonstrating both the effectiveness of our method and the broader potential of
LVLMs for zero-shot fine-grained classification tasks. Code and Datasets:
https://github.com/Atabuzzaman/Fine-grained-classification

</details>


### [68] [From Filters to VLMs: Benchmarking Defogging Methods through Object Detection and Segmentation Performance](https://arxiv.org/abs/2510.03906)
*Ardalan Aryashad,Parsa Razmara,Amin Mahjoub,Seyedarmin Azizi,Mahdi Salmani,Arad Firouzkouhi*

Main category: cs.CV

TL;DR: 本文对去雾方法进行了系统实证研究，评估了传统滤波、现代去雾网络、级联方法及视觉语言模型在图像质量和下游任务性能上的表现，揭示了不同方法在自动驾驶感知系统中的实际效果。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知系统在雾天条件下特别脆弱，现有的去雾方法在图像保真度上的改进并不总能转化为更好的检测和分割性能，且先前评估多依赖合成数据，对真实世界适用性存疑。

Method: 使用Foggy Cityscapes数据集，系统评估了(i)传统滤波、(ii)现代去雾网络、(iii)级联变体（滤波→模型、模型→滤波）和(iv)基于提示的视觉语言图像编辑模型，同时评估图像质量和下游任务性能。

Result: 分析揭示了去雾何时有效、级联何时产生协同或退化效应，以及VLM编辑器与专用方法的比较。VLM评判的定性评分与任务指标（特别是mAP）显示出强相关性。

Conclusion: 研究建立了一个透明、任务导向的去雾方法基准，明确了预处理在恶劣天气下真正改善自动驾驶感知的条件。

Abstract: Autonomous driving perception systems are particularly vulnerable in foggy
conditions, where light scattering reduces contrast and obscures fine details
critical for safe operation. While numerous defogging methods exist-from
handcrafted filters to learned restoration models-improvements in image
fidelity do not consistently translate into better downstream detection and
segmentation. Moreover, prior evaluations often rely on synthetic data, leaving
questions about real-world transferability. We present a structured empirical
study that benchmarks a comprehensive set of pipelines, including (i) classical
filters, (ii) modern defogging networks, (iii) chained variants
(filter$\rightarrow$model, model$\rightarrow$filter), and (iv) prompt-driven
visual--language image editing models (VLM) applied directly to foggy images.
Using Foggy Cityscapes, we assess both image quality and downstream performance
on object detection (mAP) and segmentation (PQ, RQ, SQ). Our analysis reveals
when defogging helps, when chaining yields synergy or degradation, and how
VLM-based editors compare to dedicated approaches. In addition, we evaluate
qualitative rubric-based scores from a VLM judge and quantify their alignment
with task metrics, showing strong correlations with mAP. Together, these
results establish a transparent, task-oriented benchmark for defogging methods
and highlight the conditions under which preprocessing genuinely improves
autonomous perception in adverse weather.

</details>


### [69] [Generating Human Motion Videos using a Cascaded Text-to-Video Framework](https://arxiv.org/abs/2510.03909)
*Hyelin Nam,Hyojun Go,Byeongjun Park,Byung-Hoon Kim,Hyungjin Chung*

Main category: cs.CV

TL;DR: CAMEO是一个用于通用人体运动视频生成的分层框架，通过桥接文本到运动模型和条件视频扩散模型，解决了现有方法在训练和推理中的子优化问题。


<details>
  <summary>Details</summary>
Motivation: 尽管视频扩散模型在视频生成方面取得了快速进展，但它们在通用人体视频生成方面的应用仍然不足，大多数工作局限于图像到视频设置或舞蹈视频等狭窄领域。

Method: 提出了CAMEO框架，通过精心设计的组件分析并准备文本提示和视觉条件来有效训练视频扩散模型，确保运动描述、条件信号和生成视频之间的鲁棒对齐。还引入了相机感知条件模块，自动选择与输入文本对齐的视点。

Result: 在MovieGen基准和新引入的T2M-VDM组合基准上证明了方法的有效性，并展示了其在多样化用例中的多功能性。

Conclusion: CAMEO框架成功解决了通用人体运动视频生成的挑战，通过桥接文本到运动和条件视频扩散模型，提供了更鲁棒和连贯的视频生成解决方案。

Abstract: Human video generation is becoming an increasingly important task with broad
applications in graphics, entertainment, and embodied AI. Despite the rapid
progress of video diffusion models (VDMs), their use for general-purpose human
video generation remains underexplored, with most works constrained to
image-to-video setups or narrow domains like dance videos. In this work, we
propose CAMEO, a cascaded framework for general human motion video generation.
It seamlessly bridges Text-to-Motion (T2M) models and conditional VDMs,
mitigating suboptimal factors that may arise in this process across both
training and inference through carefully designed components. Specifically, we
analyze and prepare both textual prompts and visual conditions to effectively
train the VDM, ensuring robust alignment between motion descriptions,
conditioning signals, and the generated videos. Furthermore, we introduce a
camera-aware conditioning module that connects the two stages, automatically
selecting viewpoints aligned with the input text to enhance coherence and
reduce manual intervention. We demonstrate the effectiveness of our approach on
both the MovieGen benchmark and a newly introduced benchmark tailored to the
T2M-VDM combination, while highlighting its versatility across diverse use
cases.

</details>


### [70] [OpenFLAME: Federated Visual Positioning System to Enable Large-Scale Augmented Reality Applications](https://arxiv.org/abs/2510.03915)
*Sagar Bharadwaj,Harrison Williams,Luke Wang,Michael Liang,Tao Jin,Srinivasan Seshan,Anthony Rowe*

Main category: cs.CV

TL;DR: OpenFLAME是一个联邦视觉定位系统，解决了集中式VPS在覆盖私有室内空间时的隐私和覆盖问题。


<details>
  <summary>Details</summary>
Motivation: 集中式VPS无法覆盖私有室内空间，存在隐私担忧、法规限制和3D扫描维护的劳动力瓶颈。

Method: 提出联邦VPS后端，允许独立组织分别3D扫描和维护自己的VPS服务，实现数据分片和联邦图像定位。

Result: 实现了室内3D扫描的访问控制、VPS后端的分布式维护，并促进了更大的覆盖范围。

Conclusion: OpenFLAME通过联邦方法解决了VPS服务分片带来的挑战，提供了跨地图数据管理和合并的参考解决方案。

Abstract: World-scale augmented reality (AR) applications need a ubiquitous 6DoF
localization backend to anchor content to the real world consistently across
devices. Large organizations such as Google and Niantic are 3D scanning outdoor
public spaces in order to build their own Visual Positioning Systems (VPS).
These centralized VPS solutions fail to meet the needs of many future AR
applications -- they do not cover private indoor spaces because of privacy
concerns, regulations, and the labor bottleneck of updating and maintaining 3D
scans. In this paper, we present OpenFLAME, a federated VPS backend that allows
independent organizations to 3D scan and maintain a separate VPS service for
their own spaces. This enables access control of indoor 3D scans, distributed
maintenance of the VPS backend, and encourages larger coverage. Sharding of VPS
services introduces several unique challenges -- coherency of localization
results across spaces, quality control of VPS services, selection of the right
VPS service for a location, and many others. We introduce the concept of
federated image-based localization and provide reference solutions for managing
and merging data across maps without sharing private data.

</details>


### [71] [Talking Tennis: Language Feedback from 3D Biomechanical Action Recognition](https://arxiv.org/abs/2510.03921)
*Arushi Dashore,Aryan Anumala,Emily Hui,Olivia Yang*

Main category: cs.CV

TL;DR: 该研究开发了一个结合CNN-LSTM模型和大型语言模型的新框架，从网球击球动作数据中提取生物力学特征，并生成可操作的语言反馈，弥补了现有系统在提供有意义反馈方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有网球击球分析系统虽然能准确分类动作，但缺乏将生物力学洞察转化为对球员和教练有意义、可操作的语言反馈的能力。

Method: 使用CNN-LSTM模型从运动数据中提取关键生物力学特征（如关节角度、肢体速度、动力链模式），然后利用大型语言模型分析这些特征与击球效果和受伤风险的关系，生成技术准确且可操作的反馈。

Result: 基于THETIS数据集和特征提取技术，该框架在分类性能和可解释性方面进行了评估，成功连接了可解释AI与运动生物力学。

Conclusion: 该研究提出的框架能够生成技术上准确、基于生物力学且对终端用户可操作的反馈，弥补了现有系统的关键空白。

Abstract: Automated tennis stroke analysis has advanced significantly with the
integration of biomechanical motion cues alongside deep learning techniques,
enhancing stroke classification accuracy and player performance evaluation.
Despite these advancements, existing systems often fail to connect
biomechanical insights with actionable language feedback that is both
accessible and meaningful to players and coaches. This research project
addresses this gap by developing a novel framework that extracts key
biomechanical features (such as joint angles, limb velocities, and kinetic
chain patterns) from motion data using Convolutional Neural Network Long
Short-Term Memory (CNN-LSTM)-based models. These features are analyzed for
relationships influencing stroke effectiveness and injury risk, forming the
basis for feedback generation using large language models (LLMs). Leveraging
the THETIS dataset and feature extraction techniques, our approach aims to
produce feedback that is technically accurate, biomechanically grounded, and
actionable for end-users. The experimental setup evaluates this framework on
classification performance and interpretability, bridging the gap between
explainable AI and sports biomechanics.

</details>


### [72] [Harnessing Synthetic Preference Data for Enhancing Temporal Understanding of Video-LLMs](https://arxiv.org/abs/2510.03955)
*Sameep Vani,Shreyas Jena,Maitreya Patel,Chitta Baral,Somak Aditya,Yezhou Yang*

Main category: cs.CV

TL;DR: TimeWarp方法通过创建针对性合成时间数据集来改进视频大语言模型的细粒度时间理解能力，在七个基准测试中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型在需要细粒度时间理解的任务上表现不佳，主要原因是微调数据集缺乏视觉复杂性和时间细节，导致模型过度依赖语言推理而非真正理解视频动态

Method: 提出TimeWarp系统方法，创建大规模针对性合成时间偏好数据集，捕捉常被忽略的复杂时间动态，将模型响应锚定到视觉和时间信息

Result: 将TimeWarp应用于现有模型后，在时间理解基准测试中性能显著提升，在七个基准测试中均取得绝对改进

Conclusion: TimeWarp方法有效推进了视频大语言模型的时间理解能力，证明了所提出数据集在提升时间理解方面的有效性

Abstract: While Video Large Language Models (Video-LLMs) have demonstrated remarkable
performance across general video understanding benchmarks-particularly in video
captioning and descriptive tasks-they consistently underperform on tasks that
require fine-grained temporal understanding. This limitation arises due to the
lack of visual complexity and temporal nuance in current fine-tuning datasets,
leading these models to rely heavily on language-based reasoning rather than
truly understanding video dynamics. In this work, we propose TimeWarp, a
systematic method to create a targeted synthetic temporal dataset to fine-tune
the model's responses to encourage it to focus on the given input video. We
introduce a large-scale preference dataset, created using TimeWarp, that
captures intricate temporal dynamics often overlooked, grounding the model's
responses to visual and temporal information. We demonstrate that when our
method is applied to existing models, it significantly improves performance on
temporal understanding benchmarks, highlighting the effectiveness of our
proposed datasets in advancing temporal understanding in Video-LLMs, resulting
in an absolute improvement in performance across seven benchmarks. Code is
available at https://github.com/sameepv21/timewarp.

</details>


### [73] [No Tokens Wasted: Leveraging Long Context in Biomedical Vision-Language Models](https://arxiv.org/abs/2510.03978)
*Min Woo Sun,Alejandro Lozano,Javier Gamazo Tejero,Vishwesh Nath,Xiao Xiao Sun,James Burgess,Yuhui Zhang,Kun Yuan,Robert Tibshirani,Sean Huver,Serena Yeung-Levy*

Main category: cs.CV

TL;DR: 该研究探讨了在生物医学视觉语言模型中扩展文本上下文长度对性能的影响，发现长上下文训练能显著提升检索和分类性能，并提出了BMC-LongCLIP模型和BIOMEDICA-LongCAP数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型通常使用短文本窗口（<77个token）进行预训练，导致长格式生物医学描述被截断。然而，大规模开源文献显示大量生物医学描述远超77个token，因此需要研究长上下文训练对模型性能的影响。

Method: 通过扩展视觉语言模型中文本编码器的上下文长度，使用新构建的BIOMEDICA-LongCAP数据集（包含100万张图像-描述对）进行训练，开发了支持512个token窗口的BMC-LongCLIP模型。

Result: BMC-LongCLIP将上下文容量扩展了6.6倍，token浪费率从55%降至2.2%。在长描述检索基准测试中，Recall@1获得高达30%的绝对增益，分类任务平均提升2%，且收敛速度比短上下文模型更快。

Conclusion: 长上下文建模是推进生物医学视觉语言模型发展的有前景方向，能够有效利用长格式描述中的额外监督信息，显著提升模型性能。

Abstract: Embedding vision-language models (VLMs) are typically pretrained with short
text windows (<77 tokens), which forces the truncation of long-format captions.
Yet, the distribution of biomedical captions from large-scale open source
literature reveals that a huge portion of captions far exceed 77 tokens. To
this end, we investigate the impact of pretraining on long-format biomedical
captions by extending the context length of text encoders in VLMs. We find that
longer context (thus, enabling additional supervision provided in long-format
captions) correlates with better retrieval and classification performance.
Given this finding, we introduce BIOMEDICA-LongCAP, a dataset of 1M
image-caption pairs enriched with context-aware descriptions from full-text
articles, providing longer and additional textual supervision. Using
BIOMEDICA-LongCAP, we train BMC-LongCLIP, a long-context biomedical VLM with a
text encoder supporting windows of up to 512 tokens. Our model extends context
capacity by 6.6x, reducing token waste from 55% to just 2.2%. On long-caption
retrieval benchmarks, BMC-LongCLIP achieves up to +30% absolute gains in
Recall@1 and +2% average improvements in classification, while also converging
faster than short-context. Our results demonstrate that long-context modeling
is a promising direction for advancing biomedical VLMs.

</details>


### [74] [Keep It on a Leash: Controllable Pseudo-label Generation Towards Realistic Long-Tailed Semi-Supervised Learning](https://arxiv.org/abs/2510.03993)
*Yaxin Hou,Bo Han,Yuheng Jia,Hui Liu,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了CPG框架，通过可控的伪标签生成机制解决长尾半监督学习中未标记数据分布未知的问题


<details>
  <summary>Details</summary>
Motivation: 现有方法假设未标记数据遵循特定分布，但实际中未标记数据的分布是未知且任意的，这限制了模型性能

Method: 采用动态可控过滤机制选择可靠伪标签，构建贝叶斯最优分类器，并结合类感知自适应增强模块和辅助分支

Result: 在多个基准数据集上取得显著提升，准确率最高超过现有最优方法15.97%

Conclusion: CPG框架能够有效应对未标记数据的任意分布问题，通过自增强优化循环显著降低泛化误差

Abstract: Current long-tailed semi-supervised learning methods assume that labeled data
exhibit a long-tailed distribution, and unlabeled data adhere to a typical
predefined distribution (i.e., long-tailed, uniform, or inverse long-tailed).
However, the distribution of the unlabeled data is generally unknown and may
follow an arbitrary distribution. To tackle this challenge, we propose a
Controllable Pseudo-label Generation (CPG) framework, expanding the labeled
dataset with the progressively identified reliable pseudo-labels from the
unlabeled dataset and training the model on the updated labeled dataset with a
known distribution, making it unaffected by the unlabeled data distribution.
Specifically, CPG operates through a controllable self-reinforcing optimization
cycle: (i) at each training step, our dynamic controllable filtering mechanism
selectively incorporates reliable pseudo-labels from the unlabeled dataset into
the labeled dataset, ensuring that the updated labeled dataset follows a known
distribution; (ii) we then construct a Bayes-optimal classifier using logit
adjustment based on the updated labeled data distribution; (iii) this improved
classifier subsequently helps identify more reliable pseudo-labels in the next
training step. We further theoretically prove that this optimization cycle can
significantly reduce the generalization error under some conditions.
Additionally, we propose a class-aware adaptive augmentation module to further
improve the representation of minority classes, and an auxiliary branch to
maximize data utilization by leveraging all labeled and unlabeled samples.
Comprehensive evaluations on various commonly used benchmark datasets show that
CPG achieves consistent improvements, surpassing state-of-the-art methods by up
to \textbf{15.97\%} in accuracy. The code is available at
https://github.com/yaxinhou/CPG.

</details>


### [75] [Enhancing OCR for Sino-Vietnamese Language Processing via Fine-tuned PaddleOCRv5](https://arxiv.org/abs/2510.04003)
*Minh Hoang Nguyen,Su Nguyen Thiet*

Main category: cs.CV

TL;DR: 提出了一种基于PaddleOCRv5的微调方法，用于提升古典汉字（汉喃）文本的识别准确率，在噪声图像条件下将准确率从37.5%提升至50.0%。


<details>
  <summary>Details</summary>
Motivation: 现有OCR系统在处理越南历史文献中的退化扫描、非标准字形和手写变体时表现不佳，而古典汉字文本的识别对越南历史文献数字化和跨语言语义研究至关重要。

Method: 使用精选的越南古典汉字手稿子集重新训练PaddleOCRv5的文本识别模块，建立了完整的训练流程（包括预处理、LMDB转换、评估和可视化）。

Result: 微调后的模型在准确率上显著提升（从37.5%到50.0%），特别是在噪声图像条件下表现更好，并开发了交互式演示系统对比微调前后的识别效果。

Conclusion: 该方法有效提升了古典汉字文本的识别性能，支持汉越语义对齐、机器翻译和历史语言学研究等下游应用，相关演示已公开可用。

Abstract: Recognizing and processing Classical Chinese (Han-Nom) texts play a vital
role in digitizing Vietnamese historical documents and enabling cross-lingual
semantic research. However, existing OCR systems struggle with degraded scans,
non-standard glyphs, and handwriting variations common in ancient sources. In
this work, we propose a fine-tuning approach for PaddleOCRv5 to improve
character recognition on Han-Nom texts. We retrain the text recognition module
using a curated subset of ancient Vietnamese Chinese manuscripts, supported by
a full training pipeline covering preprocessing, LMDB conversion, evaluation,
and visualization. Experimental results show a significant improvement over the
base model, with exact accuracy increasing from 37.5 percent to 50.0 percent,
particularly under noisy image conditions. Furthermore, we develop an
interactive demo that visually compares pre- and post-fine-tuning recognition
results, facilitating downstream applications such as Han-Vietnamese semantic
alignment, machine translation, and historical linguistics research. The demo
is available at https://huggingface.co/spaces/MinhDS/Fine-tuned-PaddleOCRv5.

</details>


### [76] [Fit Pixels, Get Labels: Meta-learned Implicit Networks for Image Segmentation](https://arxiv.org/abs/2510.04021)
*Kushal Vyas,Ashok Veeraraghavan,Guha Balakrishnan*

Main category: cs.CV

TL;DR: MetaSeg是一个基于元学习的隐式神经表示框架，用于医学图像分割，能够在保持与U-Net相当的分割性能的同时，减少90%的参数数量。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示在信号表示方面表现出色，但不适合需要学习语义结构的预测任务（如分割）。为了解决这个问题，研究者开发了MetaSeg框架。

Method: MetaSeg使用一个基础INR同时预测像素强度值和类别标签，通过元学习过程在训练数据集上找到最优初始参数，使得INR只需对未见过的测试图像进行微调即可自动解码类别标签。

Result: 在2D和3D脑部MRI分割任务上评估，MetaSeg的Dice分数与常用的U-Net模型相当，但参数数量减少了90%。

Conclusion: MetaSeg为医学图像分割提供了一个新颖、可扩展的替代方案，相比传统的资源密集型架构（如U-Net和视觉变换器）更具优势。

Abstract: Implicit neural representations (INRs) have achieved remarkable successes in
learning expressive yet compact signal representations. However, they are not
naturally amenable to predictive tasks such as segmentation, where they must
learn semantic structures over a distribution of signals. In this study, we
introduce MetaSeg, a meta-learning framework to train INRs for medical image
segmentation. MetaSeg uses an underlying INR that simultaneously predicts per
pixel intensity values and class labels. It then uses a meta-learning procedure
to find optimal initial parameters for this INR over a training dataset of
images and segmentation maps, such that the INR can simply be fine-tuned to fit
pixels of an unseen test image, and automatically decode its class labels. We
evaluated MetaSeg on 2D and 3D brain MRI segmentation tasks and report Dice
scores comparable to commonly used U-Net models, but with $90\%$ fewer
parameters. MetaSeg offers a fresh, scalable alternative to traditional
resource-heavy architectures such as U-Nets and vision transformers for medical
image segmentation. Our project is available at
https://kushalvyas.github.io/metaseg.html .

</details>


### [77] [Video-in-the-Loop: Span-Grounded Long Video QA with Interleaved Reasoning](https://arxiv.org/abs/2510.04022)
*Chendong Wang,Donglin Bai,Yifan Yang,Xiao Jin,Anlan Zhang,Rui Wang,Shiqi Jiang,Yuqing Yang,Hao Wu,Qi Dai,Chong Luo,Ting Cao,Lili Qiu,Suman Banerjee*

Main category: cs.CV

TL;DR: Video-in-the-Loop (ViTL) 是一个两阶段长视频问答框架，通过局部化问题相关区间和重新分配视觉token来提高效率，在固定token预算下实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 解决长视频问答中的计算效率问题，传统方法在处理长视频时需要大量计算资源，ViTL旨在通过智能token分配来降低计算成本同时保持性能。

Method: 两阶段方法：1）使用低帧率浏览定位问题相关区间；2）在高帧率下进行span感知的视觉token重新分配，端到端训练结合时间IoU和答案正确性的联合目标。

Result: 在固定token预算下，ViTL在长视频问答和时间定位任务上达到最高8.6%的性能提升，同时减少50%的帧输入，span感知token重新分配始终优于均匀采样。

Conclusion: ViTL和配套数据集提供了一个可解释、计算高效的解决方案，为可扩展的长视频问答提供了有效方法。

Abstract: We present \emph{Video-in-the-Loop} (ViTL), a two-stage long-video QA
framework that preserves a fixed token budget by first \emph{localizing}
question-relevant interval(s) with a low-fps skim and then \emph{answering} via
span-aware reallocation of visual tokens at higher effective frame rate,
emitting an interleaved output with both spans and the final option for direct
attribution. We also introduce \dataname{}, which converts description based
event graphs into \emph{span-grounded} multiple-choice QA by pairing each
question with \emph{ground-truth} time span(s) and related reasoning. ViTL is
trained end-to-end with an interleaved group-relative objective that couples
temporal IoU for localization with answer correctness, allowing credit to flow
from answers back to spans without increasing compute. Under fixed token
budgets, ViTL attains up to 8.6% with 50% less frame input on long-video QA and
temporal grounding (e.g., Charades-STA, ActivityNet-Captions) and ablations
show that span-aware token reallocation consistently surpasses uniform
sampling. Together, \dataname{} and ViTL provide an interpretable,
compute-efficient recipe for scalable long-video QA.

</details>


### [78] [Enhancing Fake News Video Detection via LLM-Driven Creative Process Simulation](https://arxiv.org/abs/2510.04024)
*Yuyan Bu,Qiang Sheng,Juan Cao,Shaofei Wang,Peng Qi,Yuhui Shi,Beizhe Hu*

Main category: cs.CV

TL;DR: 提出了AgentAug数据增强框架，通过模拟假新闻视频的创作过程生成多样化训练数据，提升短视频假新闻检测器的性能


<details>
  <summary>Details</summary>
Motivation: 当前假新闻视频检测器受限于训练数据不足和多样性不够，导致模式偏差和性能受限，主要原因是真实场景中视频素材与虚假新闻事件存在复杂的多对多关系，而现有数据集无法充分反映这种关系

Method: AgentAug框架包含多个LLM驱动的四种伪造类别的新闻视频创作流程，结合基于不确定性采样的主动学习策略在训练过程中选择有用的增强样本

Result: 在两个基准数据集上的实验结果表明，AgentAug能够持续提升短视频假新闻检测器的性能

Conclusion: AgentAug通过数据增强有效解决了假新闻视频检测中的数据稀缺和多样性不足问题，为假新闻检测提供了新的解决方案

Abstract: The emergence of fake news on short video platforms has become a new
significant societal concern, necessitating automatic video-news-specific
detection. Current detectors primarily rely on pattern-based features to
separate fake news videos from real ones. However, limited and less diversified
training data lead to biased patterns and hinder their performance. This
weakness stems from the complex many-to-many relationships between video
material segments and fabricated news events in real-world scenarios: a single
video clip can be utilized in multiple ways to create different fake
narratives, while a single fabricated event often combines multiple distinct
video segments. However, existing datasets do not adequately reflect such
relationships due to the difficulty of collecting and annotating large-scale
real-world data, resulting in sparse coverage and non-comprehensive learning of
the characteristics of potential fake news video creation. To address this
issue, we propose a data augmentation framework, AgentAug, that generates
diverse fake news videos by simulating typical creative processes. AgentAug
implements multiple LLM-driven pipelines of four fabrication categories for
news video creation, combined with an active learning strategy based on
uncertainty sampling to select the potentially useful augmented samples during
training. Experimental results on two benchmark datasets demonstrate that
AgentAug consistently improves the performance of short video fake news
detectors.

</details>


### [79] [Prompt-to-Prompt: Text-Based Image Editing Via Cross-Attention Mechanisms -- The Research of Hyperparameters and Novel Mechanisms to Enhance Existing Frameworks](https://arxiv.org/abs/2510.04034)
*Linn Bieske,Carla Lorente*

Main category: cs.CV

TL;DR: 本文研究了通过优化超参数来提升基于提示到提示的图像编辑框架的精度和可靠性，提出了新的方法和框架来解决现有问题。


<details>
  <summary>Details</summary>
Motivation: 图像编辑从手动像素操作转向使用稳定扩散模型等深度学习方法，但引入了结果可变性（如头发颜色不一致）的问题。研究旨在通过探索和优化超参数来增强提示到提示图像编辑框架的精度和可靠性。

Method: 1. 对'词替换'方法进行全面研究；2. 开发'注意力重加权方法'以提高适应性；3. 提出'CL P2P'框架解决循环不一致等现有限制。

Result: 研究有助于理解并改进超参数设置与神经网络模型架构选择（特别是注意力机制）之间的相互作用，这些因素显著影响生成图像的构图和质量。

Conclusion: 通过优化超参数和注意力机制，可以显著提升文本驱动图像编辑的精度和可靠性，为解决现有框架的局限性提供了有效方法。

Abstract: Recent advances in image editing have shifted from manual pixel manipulation
to employing deep learning methods like stable diffusion models, which now
leverage cross-attention mechanisms for text-driven control. This transition
has simplified the editing process but also introduced variability in results,
such as inconsistent hair color changes. Our research aims to enhance the
precision and reliability of prompt-to-prompt image editing frameworks by
exploring and optimizing hyperparameters. We present a comprehensive study of
the "word swap" method, develop an "attention re-weight method" for better
adaptability, and propose the "CL P2P" framework to address existing
limitations like cycle inconsistency. This work contributes to understanding
and improving the interaction between hyperparameter settings and the
architectural choices of neural network models, specifically their attention
mechanisms, which significantly influence the composition and quality of the
generated images.

</details>


### [80] [\textsc{GUI-Spotlight}: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding](https://arxiv.org/abs/2510.04039)
*Bin Lei,Nuo Xu,Ali Payani,Mingyi Hong,Chunhua Liao,Yu Cao,Caiwen Ding*

Main category: cs.CV

TL;DR: GUI-Spotlight是一个用于图像推理的模型，通过动态调用多个专用工具来迭代缩小屏幕相关区域，显著提高了视觉定位准确性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图形用户界面系统中已展现出强大能力，但实际应用仍受限于视觉定位的可靠性，即无法准确将文本引用映射到屏幕上的具体元素，这限制了系统执行指针级操作（如点击或拖动）的准确性。

Method: 引入GUI-Spotlight模型，该模型通过训练实现图像推理，动态调用多个专用工具来迭代缩小屏幕相关区域的焦点，从而改进视觉定位。

Result: 在ScreenSpot-Pro基准测试中，GUI-Spotlight仅使用18.5K训练样本就达到了52.8%的准确率，超过了V2P-7B（50.6%，使用9.6M训练样本）和GTA-1-7B（50.1%，使用1.56M训练样本）。

Conclusion: GUI-Spotlight通过高效的训练策略和迭代聚焦方法，显著提升了GUI系统中的视觉定位准确性，为实际应用提供了更可靠的解决方案。

Abstract: Multimodal large language models (MLLMs) have markedly expanded the
competence of graphical user-interface (GUI) systems, propelling them beyond
controlled simulations into complex, real-world environments across diverse
platforms. However, practical usefulness is still bounded by the reliability of
visual grounding, i.e., mapping textual references to exact on-screen elements.
This limitation prevents the system from accurately performing pointer-level
actions such as clicking or dragging. To address it, we introduce GUI-Spotlight
-- a model trained for image-grounded reasoning that dynamically invokes
multiple specialized tools to iteratively narrow its focus to the relevant
region of the screen, thereby substantially improving visual grounding
accuracy. On the ScreenSpot-Pro benchmark, GUI-Spotlight trained with only
18.5K training samples achieves 52.8\% accuracy, surpassing V2P-7B (50.6\% with
9.6M training samples) and GTA-1-7B (50.1\% with 1.56M training samples).

</details>


### [81] [Quantization Range Estimation for Convolutional Neural Networks](https://arxiv.org/abs/2510.04044)
*Bingtao Yang,Yujia Wang,Mengzhi Jiao,Hongwei Huo*

Main category: cs.CV

TL;DR: 提出一种基于范围估计的后训练量化方法，通过层间局部最小值优化来最小化量化误差，在图像分类任务中实现高精度低比特量化


<details>
  <summary>Details</summary>
Motivation: 后训练量化虽然能有效减少深度神经网络模型的存储需求，但低比特量化同时保持模型精度是一个具有挑战性的问题

Method: 将范围估计建模为通过层间局部最小值最小化量化误差的优化问题，证明该问题局部凸性并提出高效搜索算法，在变换权重空间应用该算法进一步改进

Result: 在ResNet系列模型和Inception-v3模型上，8位和6位量化几乎无精度损失，4位量化精度显著提升，优于现有最佳方法

Conclusion: 该方法能有效解决后训练量化中的精度保持问题，为低比特量化提供了实用解决方案

Abstract: Post-training quantization for reducing the storage of deep neural network
models has been demonstrated to be an effective way in various tasks. However,
low-bit quantization while maintaining model accuracy is a challenging problem.
In this paper, we present a range estimation method to improve the quantization
performance for post-training quantization. We model the range estimation into
an optimization problem of minimizing quantization errors by layer-wise local
minima. We prove this problem is locally convex and present an efficient search
algorithm to find the optimal solution. We propose the application of the above
search algorithm to the transformed weights space to do further improvement in
practice. Our experiments demonstrate that our method outperforms
state-of-the-art performance generally on top-1 accuracy for image
classification tasks on the ResNet series models and Inception-v3 model. The
experimental results show that the proposed method has almost no loss of top-1
accuracy in 8-bit and 6-bit settings for image classifications, and the
accuracy of 4-bit quantization is also significantly improved. The code is
available at https://github.com/codeiscommitting/REQuant.

</details>


### [82] [MetaFind: Scene-Aware 3D Asset Retrieval for Coherent Metaverse Scene Generation](https://arxiv.org/abs/2510.04057)
*Zhenyu Pan,Yucheng Lu,Han Liu*

Main category: cs.CV

TL;DR: MetaFind是一个面向元宇宙场景生成的三模态组合检索框架，通过文本、图像和3D模态的任意组合查询来检索3D资产，解决现有方法在空间、语义和风格一致性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D资产检索方法存在的两个核心问题：(i)忽略空间、语义和风格约束的不一致检索；(ii)缺乏专门针对3D资产检索的标准化检索范式。

Method: 提出可插拔的等变布局编码器ESSGNN，联合建模对象级特征（包括外观）和场景级布局结构，支持任意模态组合查询和迭代式场景构建。

Result: 实证评估显示MetaFind在各种检索任务中相比基线方法具有更好的空间和风格一致性。

Conclusion: MetaFind提供了一个灵活且标准化的3D资产检索框架，能够有效提升元宇宙场景生成的质量和一致性。

Abstract: We present MetaFind, a scene-aware tri-modal compositional retrieval
framework designed to enhance scene generation in the metaverse by retrieving
3D assets from large-scale repositories. MetaFind addresses two core
challenges: (i) inconsistent asset retrieval that overlooks spatial, semantic,
and stylistic constraints, and (ii) the absence of a standardized retrieval
paradigm specifically tailored for 3D asset retrieval, as existing approaches
mainly rely on general-purpose 3D shape representation models. Our key
innovation is a flexible retrieval mechanism that supports arbitrary
combinations of text, image, and 3D modalities as queries, enhancing spatial
reasoning and style consistency by jointly modeling object-level features
(including appearance) and scene-level layout structures. Methodologically,
MetaFind introduces a plug-and-play equivariant layout encoder ESSGNN that
captures spatial relationships and object appearance features, ensuring
retrieved 3D assets are contextually and stylistically coherent with the
existing scene, regardless of coordinate frame transformations. The framework
supports iterative scene construction by continuously adapting retrieval
results to current scene updates. Empirical evaluations demonstrate the
improved spatial and stylistic consistency of MetaFind in various retrieval
tasks compared to baseline methods.

</details>


### [83] [Ordinal Encoding as a Regularizer in Binary Loss for Solar Flare Prediction](https://arxiv.org/abs/2510.04063)
*Chetraj Pandey,Jinsu Hong,Anli Ji,Rafal A. Angryk,Berkay Aydin*

Main category: cs.CV

TL;DR: 提出了一种改进的损失函数，将太阳耀斑子类别的序数信息整合到二元交叉熵损失中，以解决二元分类中忽略序数关系的问题。


<details>
  <summary>Details</summary>
Motivation: 太阳耀斑预测通常采用二元分类方法，但这种方法忽略了FL和NF类别内子类别的序数关系。研究发现，模型在预测阈值附近最容易出现误分类，表明模型难以区分强度相似但位于二元阈值两侧的事件。

Method: 提出了一种改进的损失函数，将耀斑标签子类别的序数信息整合到传统的二元交叉熵损失中。这种方法作为一种序数感知的数据驱动正则化方法，在模型优化过程中对靠近预测阈值的错误预测施加更重的惩罚。

Result: 通过将序数加权整合到损失函数中，旨在利用数据的序数特性增强模型的学习过程，从而提高整体性能。

Conclusion: 该研究提出了一种新的损失函数设计，通过考虑太阳耀斑子类别的序数关系来改进二元分类模型的性能，特别是在预测阈值附近的分类准确性。

Abstract: The prediction of solar flares is typically formulated as a binary
classification task, distinguishing events as either Flare (FL) or No-Flare
(NF) according to a specified threshold (for example, greater than or equal to
C-class, M-class, or X-class). However, this binary framework neglects the
inherent ordinal relationships among the sub-classes contained within each
category (FL and NF). Several studies on solar flare prediction have
empirically shown that the most frequent misclassifications occur near this
prediction threshold. This suggests that the models struggle to differentiate
events that are similar in intensity but fall on opposite sides of the binary
threshold. To mitigate this limitation, we propose a modified loss function
that integrates the ordinal information among the sub-classes of the binarized
flare labels into the conventional binary cross-entropy (BCE) loss. This
approach serves as an ordinality-aware, data-driven regularization method that
penalizes the incorrect predictions of flare events in close proximity to the
prediction threshold more heavily than those away from the boundary during
model optimization. By incorporating ordinal weighting into the loss function,
we aim to enhance the model's learning process by leveraging the ordinal
characteristics of the data, thereby improving its overall performance.

</details>


### [84] [QuantDemoire: Quantization with Outlier Aware for Image Demoiréing](https://arxiv.org/abs/2510.04066)
*Zheng Chen,Kewei Zhang,Xiaoyang Liu,Weihang Zhang,Mengfan Wang,Yifan Fu,Yulun Zhang*

Main category: cs.CV

TL;DR: QuantDemoire是一个针对去摩尔纹任务的专用后训练量化框架，通过异常值感知量化器和频率感知校准策略，在保持质量的同时大幅减少模型参数和计算量。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习去摩尔纹方法需要大量计算资源，难以在边缘设备部署。直接应用现有量化方法会导致严重的性能下降，主要原因是分布异常值和平滑区域表示弱化。

Method: 1. 异常值感知量化器：使用基于采样的范围估计减少激活异常值，并将少量极端权重保持为FP16格式；2. 频率感知校准策略：在微调过程中强调低中频分量，减轻低比特量化导致的带状伪影。

Result: 在W4A4量化配置下，QuantDemoire比现有量化方法性能提升超过4 dB，同时大幅减少了参数和计算量。

Conclusion: QuantDemoire为去摩尔纹任务提供了有效的量化解决方案，在保持性能的同时实现了模型的高效部署。

Abstract: Demoir\'eing aims to remove moir\'e artifacts that often occur in images.
While recent deep learning-based methods have achieved promising results, they
typically require substantial computational resources, limiting their
deployment on edge devices. Model quantization offers a compelling solution.
However, directly applying existing quantization methods to demoir\'eing models
introduces severe performance degradation. The main reasons are distribution
outliers and weakened representations in smooth regions. To address these
issues, we propose QuantDemoire, a post-training quantization framework
tailored to demoir\'eing. It contains two key components. **First}, we
introduce an outlier-aware quantizer to reduce errors from outliers. It uses
sampling-based range estimation to reduce activation outliers, and keeps a few
extreme weights in FP16 with negligible cost. **Second**, we design a
frequency-aware calibration strategy. It emphasizes low- and mid-frequency
components during fine-tuning, which mitigates banding artifacts caused by
low-bit quantization. Extensive experiments validate that our QuantDemoire
achieves large reductions in parameters and computation while maintaining
quality. Meanwhile, it outperforms existing quantization methods by over **4
dB** on W4A4. Code is released at:
https://github.com/zhengchen1999/QuantDemoire.

</details>


### [85] [Diffusion Low Rank Hybrid Reconstruction for Sparse View Medical Imaging](https://arxiv.org/abs/2510.04069)
*Zongyin Deng,Qing Zhou,Yuhao Fang,Zijian Wang,Yao Lu,Ye Zhang,Chun Li*

Main category: cs.CV

TL;DR: TV-LoRA是一种结合扩散生成先验和多正则化约束的低剂量稀疏视图CT重建方法，在ADMM框架下实现了高效3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决极稀疏视图下CT重建的病态问题和纹理丢失问题，提高低剂量稀疏采样场景下的重建质量。

Method: 结合扩散生成先验（NCSN++与SDE建模）和各向异性TV、核范数（LoRA）正则化约束，采用ADMM框架，利用2D切片策略、FFT加速和并行优化。

Result: 在AAPM-2016、CTHD和LIDC数据集上，TV-LoRA在SSIM、纹理恢复、边缘清晰度和伪影抑制方面均优于基准方法，展现出强鲁棒性和泛化能力。

Conclusion: TV-LoRA实现了高保真、高效的3D CT重建，在低剂量稀疏采样场景下具有广泛的临床应用前景。

Abstract: This work presents TV-LoRA, a novel method for low-dose sparse-view CT
reconstruction that combines a diffusion generative prior (NCSN++ with SDE
modeling) and multi-regularization constraints, including anisotropic TV and
nuclear norm (LoRA), within an ADMM framework. To address ill-posedness and
texture loss under extremely sparse views, TV-LoRA integrates generative and
physical constraints, and utilizes a 2D slice-based strategy with FFT
acceleration and tensor-parallel optimization for efficient inference.
Experiments on AAPM-2016, CTHD, and LIDC datasets with
$N_{\mathrm{view}}=8,4,2$ show that TV-LoRA consistently surpasses benchmarks
in SSIM, texture recovery, edge clarity, and artifact suppression,
demonstrating strong robustness and generalizability. Ablation studies confirm
the complementary effects of LoRA regularization and diffusion priors, while
the FFT-PCG module provides a speedup. Overall, Diffusion + TV-LoRA achieves
high-fidelity, efficient 3D CT reconstruction and broad clinical applicability
in low-dose, sparse-sampling scenarios.

</details>


### [86] [TOPO-Bench: An Open-Source Topological Mapping Evaluation Framework with Quantifiable Perceptual Aliasing](https://arxiv.org/abs/2510.04100)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Harold Soh*

Main category: cs.CV

TL;DR: 该论文提出了拓扑映射领域标准化评估协议，包括拓扑一致性度量、数据集模糊性量化方法，并发布了基准数据集和基线系统。


<details>
  <summary>Details</summary>
Motivation: 拓扑映射领域缺乏标准化评估指标、数据集和协议，现有系统在不同环境和标准下评估，无法进行公平可复现的比较，且感知混淆问题未得到充分量化。

Method: 1) 形式化拓扑一致性作为拓扑映射的基本属性，提出定位精度作为高效可解释的替代指标；2) 提出首个数据集模糊性量化方法；3) 构建具有校准模糊度水平的多样化基准数据集，实现并发布深度学习基线系统。

Result: 实验分析揭示了当前方法在感知混淆情况下的局限性，所有数据集、基线和评估工具均已开源，促进拓扑映射研究的持续性和可复现性。

Conclusion: 该研究为拓扑映射领域建立了标准化评估框架，解决了长期存在的评估不一致问题，并为未来研究提供了可靠的基准和工具。

Abstract: Topological mapping offers a compact and robust representation for
navigation, but progress in the field is hindered by the lack of standardized
evaluation metrics, datasets, and protocols. Existing systems are assessed
using different environments and criteria, preventing fair and reproducible
comparisons. Moreover, a key challenge - perceptual aliasing - remains
under-quantified, despite its strong influence on system performance. We
address these gaps by (1) formalizing topological consistency as the
fundamental property of topological maps and showing that localization accuracy
provides an efficient and interpretable surrogate metric, and (2) proposing the
first quantitative measure of dataset ambiguity to enable fair comparisons
across environments. To support this protocol, we curate a diverse benchmark
dataset with calibrated ambiguity levels, implement and release deep-learned
baseline systems, and evaluate them alongside classical methods. Our
experiments and analysis yield new insights into the limitations of current
approaches under perceptual aliasing. All datasets, baselines, and evaluation
tools are fully open-sourced to foster consistent and reproducible research in
topological mapping.

</details>


### [87] [Learning Efficient Meshflow and Optical Flow from Event Cameras](https://arxiv.org/abs/2510.04111)
*Xinglong Luo,Ao Luo,Kunming Luo,Zhengning Wang,Ping Tan,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: 该论文提出了事件相机下的网格流估计新任务，开发了HREM高分辨率事件网格流数据集和EEMFlow轻量级网络，实现了30倍加速的实时性能，并通过密度自适应模块将性能提升8-10%。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机领域缺乏网格流专用数据集和方法的问题，以及事件数据密度变化带来的挑战，旨在实现高效准确的事件网格流估计。

Method: 构建HREM高分辨率事件网格流数据集，设计EEMFlow轻量级编码器-解码器网络，提出置信度诱导细节补全模块处理密集光流，并开发自适应密度模块优化不同密度事件数据的处理。

Result: EEMFlow模型在性能和运行效率上显著优于现有方法（30倍加速），ADM模块将EEMFlow和EEMFlow+的性能分别提升8%和10%。

Conclusion: 该研究填补了事件相机网格流估计的空白，提出的数据集、网络架构和密度自适应方法为事件视觉领域提供了有效的解决方案，具有实际应用价值。

Abstract: In this paper, we explore the problem of event-based meshflow estimation, a
novel task that involves predicting a spatially smooth sparse motion field from
event cameras. To start, we review the state-of-the-art in event-based flow
estimation, highlighting two key areas for further research: i) the lack of
meshflow-specific event datasets and methods, and ii) the underexplored
challenge of event data density. First, we generate a large-scale
High-Resolution Event Meshflow (HREM) dataset, which showcases its superiority
by encompassing the merits of high resolution at 1280x720, handling dynamic
objects and complex motion patterns, and offering both optical flow and
meshflow labels. These aspects have not been fully explored in previous works.
Besides, we propose Efficient Event-based MeshFlow (EEMFlow) network, a
lightweight model featuring a specially crafted encoder-decoder architecture to
facilitate swift and accurate meshflow estimation. Furthermore, we upgrade
EEMFlow network to support dense event optical flow, in which a
Confidence-induced Detail Completion (CDC) module is proposed to preserve sharp
motion boundaries. We conduct comprehensive experiments to show the exceptional
performance and runtime efficiency (30x faster) of our EEMFlow model compared
to the recent state-of-the-art flow method. As an extension, we expand HREM
into HREM+, a multi-density event dataset contributing to a thorough study of
the robustness of existing methods across data with varying densities, and
propose an Adaptive Density Module (ADM) to adjust the density of input event
data to a more optimal range, enhancing the model's generalization ability. We
empirically demonstrate that ADM helps to significantly improve the performance
of EEMFlow and EEMFlow+ by 8% and 10%, respectively. Code and dataset are
released at https://github.com/boomluo02/EEMFlowPlus.

</details>


### [88] [Joint Learning of Pose Regression and Denoising Diffusion with Score Scaling Sampling for Category-level 6D Pose Estimation](https://arxiv.org/abs/2510.04125)
*Seunghyun Lee,Tae-Kyun Kim*

Main category: cs.CV

TL;DR: 提出一种新的6D物体姿态估计方法，通过预训练编码器和联合学习策略加速训练收敛，同时利用采样指导技术消除额外评估网络需求，在多个基准测试中达到最先进精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的6D物体姿态估计方法存在训练收敛慢、需要额外网络过滤低质量姿态候选等问题，需要更高效准确的解决方案。

Method: 1. 预训练编码器并使用直接姿态回归头，联合学习回归头和去噪扩散头；2. 提出基于时间相关分数缩放的采样指导技术，平衡探索-利用权衡。

Result: 在REAL275、HouseCat6D和ROPE等多个基准测试中实现最先进精度，即使使用单姿态推理也能获得高质量结果，训练和推理效率更高。

Conclusion: 该方法简单有效，通过预训练和联合学习策略显著加速训练收敛，采样指导技术有效处理对称物体的多模态特性，无需额外评估网络即可实现高质量姿态估计。

Abstract: Latest diffusion models have shown promising results in category-level 6D
object pose estimation by modeling the conditional pose distribution with depth
image input. The existing methods, however, suffer from slow convergence during
training, learning its encoder with the diffusion denoising network in
end-to-end fashion, and require an additional network that evaluates sampled
pose hypotheses to filter out low-quality pose candidates. In this paper, we
propose a novel pipeline that tackles these limitations by two key components.
First, the proposed method pretrains the encoder with the direct pose
regression head, and jointly learns the networks via the regression head and
the denoising diffusion head, significantly accelerating training convergence
while achieving higher accuracy. Second, sampling guidance via time-dependent
score scaling is proposed s.t. the exploration-exploitation trade-off is
effectively taken, eliminating the need for the additional evaluation network.
The sampling guidance maintains multi-modal characteristics of symmetric
objects at early denoising steps while ensuring high-quality pose generation at
final steps. Extensive experiments on multiple benchmarks including REAL275,
HouseCat6D, and ROPE, demonstrate that the proposed method, simple yet
effective, achieves state-of-the-art accuracies even with single-pose
inference, while being more efficient in both training and inference.

</details>


### [89] [Learning from All: Concept Alignment for Autonomous Distillation from Multiple Drifting MLLMs](https://arxiv.org/abs/2510.04142)
*Xiaoyu Yang,Jie Lu,En Yu*

Main category: cs.CV

TL;DR: 本文提出了一种解决多模态大语言模型蒸馏中概念漂移问题的新方法——自主偏好优化（APO），通过"学习、比较、批判"范式提升学生模型的鲁棒性和一致性。


<details>
  <summary>Details</summary>
Motivation: 多教师模型在知识蒸馏过程中产生的推理轨迹存在概念漂移问题，导致推理分布不可预测地演化，并将偏见传递给学生模型，最终影响其性能。

Method: 提出自主偏好优化（APO）方法，采用"学习、比较、批判"范式：学生模型在教师指导下学习并自我蒸馏偏好思维，通过比较多个教师进行批判性反思，实现概念对齐。

Result: 实验证明该方法在知识蒸馏中具有优越的一致性、鲁棒性和泛化性能，并贡献了包含170,982条蒸馏推理轨迹的大规模数据集CXR-MAX。

Conclusion: APO方法有效解决了多教师蒸馏中的概念漂移问题，能够产生更稳健、一致且可泛化的模型。

Abstract: This paper identifies a critical yet underexplored challenge in distilling
from multimodal large language models (MLLMs): the reasoning trajectories
generated by multiple drifting teachers exhibit concept drift, whereby their
reasoning distributions evolve unpredictably and transmit biases to the student
model, ultimately compromising its performance. To tackle this issue, we
pioneer a theoretical connection between concept drift and knowledge
distillation, casting the non-stationary reasoning dynamics from multiple MLLM
teachers as next-token prediction of multi-stream reasoning trajectories.Guided
by concept drift, we introduce the "learn, compare, critique" paradigm,
culminating in autonomous preference optimization (APO). Under the active
guidance of the teachers, the student model first learns and self-distils
preferred thinking by comparing multiple teachers. It then engages in critical
reflection over the drifting inference from teachers, performing concept
alignment through APO, ultimately yielding a robust, consistent, and
generalizable model.Extensive experiments demonstrate our superior performance
of consistency, robustness and generalization within knowledge distillation.
Besides, we also contributed a large-scale dataset, CXR-MAX (Multi-teachers
Alignment X-rays), comprising 170,982 distilled reasoning trajectories derived
from publicly accessible MLLMs based on MIMIC-CXR. Our code and data are public
at: https://anonymous.4open.science/r/Autonomous-Distillation/.

</details>


### [90] [Automating construction safety inspections using a multi-modal vision-language RAG framework](https://arxiv.org/abs/2510.04145)
*Chenxin Wang,Elyas Asadi Shamsabadi,Zhaohui Chen,Luming Shen,Alireza Ahmadian Fard Fini,Daniel Dias-da-Costa*

Main category: cs.CV

TL;DR: 本文提出了SiteShield，一个基于多模态大视觉语言模型的检索增强生成框架，用于自动化建筑安全检查报告生成，通过整合视觉和音频输入来提高效率。


<details>
  <summary>Details</summary>
Motivation: 传统建筑安全检查方法效率低下，需要处理大量信息。现有的大语言模型应用存在响应不相关、模态输入受限和幻觉问题，且缺乏实时适应性。

Method: 开发了SiteShield框架，采用多模态LVLM和检索增强生成技术，整合视觉和音频输入来生成安全检查报告。

Result: 使用真实世界数据测试，SiteShield在F1分数（0.82）、汉明损失（0.04）、精确率（0.76）和召回率（0.96）方面均优于不使用RAG的单模态LLM。

Conclusion: SiteShield为增强信息检索和生成安全报告的效率提供了一条新途径。

Abstract: Conventional construction safety inspection methods are often inefficient as
they require navigating through large volume of information. Recent advances in
large vision-language models (LVLMs) provide opportunities to automate safety
inspections through enhanced visual and linguistic understanding. However,
existing applications face limitations including irrelevant or unspecific
responses, restricted modal inputs and hallucinations. Utilisation of Large
Language Models (LLMs) for this purpose is constrained by availability of
training data and frequently lack real-time adaptability. This study introduces
SiteShield, a multi-modal LVLM-based Retrieval-Augmented Generation (RAG)
framework for automating construction safety inspection reports by integrating
visual and audio inputs. Using real-world data, SiteShield outperformed
unimodal LLMs without RAG with an F1 score of 0.82, hamming loss of 0.04,
precision of 0.76, and recall of 0.96. The findings indicate that SiteShield
offers a novel pathway to enhance information retrieval and efficiency in
generating safety reports.

</details>


### [91] [BLADE: Bias-Linked Adaptive DEbiasing](https://arxiv.org/abs/2510.04174)
*Piyush Arora,Navlika Singh,Vasubhya Diwan,Pratik Mazumder*

Main category: cs.CV

TL;DR: BLADE是一种无需先验知识或偏见冲突样本的生成式去偏框架，通过跨偏见域图像翻译和自适应精炼来缓解神经网络中的隐式偏见问题


<details>
  <summary>Details</summary>
Motivation: 神经网络容易学习训练数据中的隐式偏见和虚假相关性，而现有方法需要先验知识或偏见冲突样本，这在现实场景中往往不切实际

Method: BLADE首先训练生成模型在偏见域之间翻译图像并保留任务相关特征，然后基于图像对偏见的敏感性自适应精炼每个图像与其合成对应物，通过对齐任务相关特征相同但偏见不同的图像来鼓励鲁棒表示

Result: 在多个基准数据集上，BLADE显著优于最先进方法，在损坏的CIFAR-10数据集最差组设置下比最接近的基线绝对提升约18%

Conclusion: BLADE在无需显式监督的情况下建立了偏见缓解的新基准，展示了开发更鲁棒深度学习模型的潜力

Abstract: Neural networks have revolutionized numerous fields, yet they remain
vulnerable to a critical flaw: the tendency to learn implicit biases, spurious
correlations between certain attributes and target labels in training data.
These biases are often more prevalent and easier to learn, causing models to
rely on superficial patterns rather than task-relevant features necessary for
generalization. Existing methods typically rely on strong assumptions, such as
prior knowledge of these biases or access to bias-conflicting samples, i.e.,
samples that contradict spurious correlations and counterbalance bias-aligned
samples, samples that conform to these spurious correlations. However, such
assumptions are often impractical in real-world settings. We propose BLADE
({B}ias-{L}inked {A}daptive {DE}biasing), a generative debiasing framework that
requires no prior knowledge of bias or bias-conflicting samples. BLADE first
trains a generative model to translate images across bias domains while
preserving task-relevant features. Then, it adaptively refines each image with
its synthetic counterpart based on the image's susceptibility to bias. To
encourage robust representations, BLADE aligns an image with its
bias-translated synthetic counterpart that shares task-relevant features but
differs in bias, while misaligning it with samples sharing the same bias. We
evaluate BLADE on multiple benchmark datasets and show that it significantly
outperforms state-of-the-art methods. Notably, it exceeds the closest baseline
by an absolute margin of around 18% on the corrupted CIFAR-10 dataset under the
worst group setting, establishing a new benchmark in bias mitigation and
demonstrating its potential for developing more robust deep learning models
without explicit supervision.

</details>


### [92] [From Segments to Concepts: Interpretable Image Classification via Concept-Guided Segmentation](https://arxiv.org/abs/2510.04180)
*Ran Eisenberg,Amit Rozner,Ethan Fetaya,Ofir Lindenbaum*

Main category: cs.CV

TL;DR: SEG-MIL-CBM是一个结合概念引导图像分割和注意力多实例学习的新框架，通过语义分割区域实现概念级可解释性，无需概念标注即可提供空间定位的解释。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络缺乏决策透明性，在安全关键应用中存在信任问题。现有概念瓶颈模型需要昂贵的概念标注且缺乏空间定位能力，无法识别支持每个概念的具体区域。

Method: 将概念引导图像分割集成到注意力多实例学习框架中，将每个分割区域视为实例，模型学习跨区域聚合证据，通过语义有意义的区域进行推理。

Result: 在涉及虚假相关性、输入损坏和大规模基准测试的各种设置中实现稳健性能，同时提供透明的概念级解释。

Conclusion: SEG-MIL-CBM通过无监督的概念区域分割和证据聚合，实现了既有鲁棒性又有可解释性的视觉模型，为安全关键应用提供了可靠的透明决策支持。

Abstract: Deep neural networks have achieved remarkable success in computer vision;
however, their black-box nature in decision-making limits interpretability and
trust, particularly in safety-critical applications. Interpretability is
crucial in domains where errors have severe consequences. Existing models not
only lack transparency but also risk exploiting unreliable or misleading
features, which undermines both robustness and the validity of their
explanations. Concept Bottleneck Models (CBMs) aim to improve transparency by
reasoning through human-interpretable concepts. Still, they require costly
concept annotations and lack spatial grounding, often failing to identify which
regions support each concept. We propose SEG-MIL-CBM, a novel framework that
integrates concept-guided image segmentation into an attention-based multiple
instance learning (MIL) framework, where each segmented region is treated as an
instance and the model learns to aggregate evidence across them. By reasoning
over semantically meaningful regions aligned with high-level concepts, our
model highlights task-relevant evidence, down-weights irrelevant cues, and
produces spatially grounded, concept-level explanations without requiring
annotations of concepts or groups. SEG-MIL-CBM achieves robust performance
across settings involving spurious correlations (unintended dependencies
between background and label), input corruptions (perturbations that degrade
visual quality), and large-scale benchmarks, while providing transparent,
concept-level explanations.

</details>


### [93] [Let Features Decide Their Own Solvers: Hybrid Feature Caching for Diffusion Transformers](https://arxiv.org/abs/2510.04188)
*Shikang Zheng,Guantao Chen,Qinming Zhou,Yuqi Lin,Lixuan He,Chang Zou,Peiliang Cai,Jiacheng Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: HyCa是一种基于混合ODE求解器的缓存框架，通过维度级缓存策略实现Diffusion Transformer的加速，在多个模型上达到5-6倍加速且几乎无损。


<details>
  <summary>Details</summary>
Motivation: Diffusion Transformer的迭代采样过程计算成本高，现有特征缓存方法对所有特征维度采用统一策略，忽略了不同维度的动态行为差异。

Method: 将隐藏特征演化建模为跨维度的ODE混合模型，提出HyCa框架应用维度级缓存策略，基于混合ODE求解器智能决定何时重用或预测特征。

Result: 在FLUX、HunyuanVideo、Qwen-Image等模型上分别实现5.55倍、5.56倍、6.24倍加速，效果接近原始模型。

Conclusion: HyCa证明了维度级缓存策略的有效性，为Diffusion Transformer提供了一种无需重新训练的高效加速方案。

Abstract: Diffusion Transformers offer state-of-the-art fidelity in image and video
synthesis, but their iterative sampling process remains a major bottleneck due
to the high cost of transformer forward passes at each timestep. To mitigate
this, feature caching has emerged as a training-free acceleration technique
that reuses or forecasts hidden representations. However, existing methods
often apply a uniform caching strategy across all feature dimensions, ignoring
their heterogeneous dynamic behaviors. Therefore, we adopt a new perspective by
modeling hidden feature evolution as a mixture of ODEs across dimensions, and
introduce HyCa, a Hybrid ODE solver inspired caching framework that applies
dimension-wise caching strategies. HyCa achieves near-lossless acceleration
across diverse domains and models, including 5.55 times speedup on FLUX, 5.56
times speedup on HunyuanVideo, 6.24 times speedup on Qwen-Image and
Qwen-Image-Edit without retraining.

</details>


### [94] [World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge](https://arxiv.org/abs/2510.04201)
*Moo Hyun Son,Jintaek Oh,Sun Bin Mun,Jaechul Roh,Sehyun Choi*

Main category: cs.CV

TL;DR: World-To-Image框架通过代理驱动的世界知识增强T2I生成，解决模型对新颖或分布外实体生成质量下降的问题。


<details>
  <summary>Details</summary>
Motivation: 文本到图像模型在遇到新颖或分布外实体时性能显著下降，因为存在固有的知识截止限制。

Method: 设计一个代理动态搜索网络，检索基础模型未知概念的图像，然后进行多模态提示优化，引导生成模型实现准确合成。

Result: 在NICE基准测试中，World-To-Image在语义对齐和视觉美学方面显著优于最先进方法，准确率提升8.1%，且仅需不到3次迭代即可高效实现。

Conclusion: 该框架为T2I系统更好地反映不断变化的现实世界铺平了道路。

Abstract: While text-to-image (T2I) models can synthesize high-quality images, their
performance degrades significantly when prompted with novel or
out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We
introduce World-To-Image, a novel framework that bridges this gap by empowering
T2I generation with agent-driven world knowledge. We design an agent that
dynamically searches the web to retrieve images for concepts unknown to the
base model. This information is then used to perform multimodal prompt
optimization, steering powerful generative backbones toward an accurate
synthesis. Critically, our evaluation goes beyond traditional metrics,
utilizing modern assessments like LLMGrader and ImageReward to measure true
semantic fidelity. Our experiments show that World-To-Image substantially
outperforms state-of-the-art methods in both semantic alignment and visual
aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated
NICE benchmark. Our framework achieves these results with high efficiency in
less than three iterations, paving the way for T2I systems that can better
reflect the ever-changing real world. Our demo code is available
here\footnote{https://github.com/mhson-kyle/World-To-Image}.

</details>


### [95] [MASC: Boosting Autoregressive Image Generation with a Manifold-Aligned Semantic Clustering](https://arxiv.org/abs/2510.04220)
*Lixuan He,Shikang Zheng,Linfeng Zhang*

Main category: cs.CV

TL;DR: MASC是一个用于自回归图像生成模型的框架，通过构建层次化语义树来优化视觉token的预测空间，显著提升训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统自回归模型将视觉token视为扁平词汇表，忽略了嵌入空间的内在结构，导致预测任务复杂、训练效率低且生成质量受限。

Method: 提出MASC框架，使用几何感知距离度量和密度驱动的凝聚构造方法，从token嵌入的内在结构中构建层次化语义树，将扁平预测任务转化为结构化层次任务。

Result: 实验验证MASC有效性：训练加速达57%，LlamaGen-XL的FID从2.87降至2.58，显著提升生成质量。

Conclusion: MASC证明结构化预测空间与架构创新同等重要，可将现有自回归框架提升至与最先进方法竞争的水平。

Abstract: Autoregressive (AR) models have shown great promise in image generation, yet
they face a fundamental inefficiency stemming from their core component: a
vast, unstructured vocabulary of visual tokens. This conventional approach
treats tokens as a flat vocabulary, disregarding the intrinsic structure of the
token embedding space where proximity often correlates with semantic
similarity. This oversight results in a highly complex prediction task, which
hinders training efficiency and limits final generation quality. To resolve
this, we propose Manifold-Aligned Semantic Clustering (MASC), a principled
framework that constructs a hierarchical semantic tree directly from the
codebook's intrinsic structure. MASC employs a novel geometry-aware distance
metric and a density-driven agglomerative construction to model the underlying
manifold of the token embeddings. By transforming the flat, high-dimensional
prediction task into a structured, hierarchical one, MASC introduces a
beneficial inductive bias that significantly simplifies the learning problem
for the AR model. MASC is designed as a plug-and-play module, and our extensive
experiments validate its effectiveness: it accelerates training by up to 57%
and significantly improves generation quality, reducing the FID of LlamaGen-XL
from 2.87 to 2.58. MASC elevates existing AR frameworks to be highly
competitive with state-of-the-art methods, establishing that structuring the
prediction space is as crucial as architectural innovation for scalable
generative modeling.

</details>


### [96] [Zoom-In to Sort AI-Generated Images Out](https://arxiv.org/abs/2510.04225)
*Yikun Ji,Yan Hong,Bowen Deng,jun lan,Huijia Zhu,Weiqiang Wang,Liqing Zhang,Jianfu Zhang*

Main category: cs.CV

TL;DR: ZoomIn是一个两阶段取证框架，通过模仿人类视觉检查来提高AI生成图像检测的准确性和可解释性，在高质量合成图像上达到96.39%的准确率


<details>
  <summary>Details</summary>
Motivation: AI生成图像的快速增长模糊了真实与合成内容的界限，对数字完整性构成严重威胁。现有的视觉语言模型在检测高质量合成图像的细微伪影方面存在不足

Method: 提出ZoomIn两阶段框架：首先扫描图像定位可疑区域，然后对这些放大区域进行聚焦分析以得出有依据的判断。同时创建了MagniFake数据集，包含20,000张真实和高质量合成图像，带有边界框和取证解释

Result: 该方法实现了96.39%的检测准确率，并具有强大的泛化能力，同时提供基于视觉证据的人类可理解解释

Conclusion: ZoomIn框架有效解决了高质量合成图像检测的挑战，在准确性和可解释性方面都表现出色，为数字取证提供了可靠的解决方案

Abstract: The rapid growth of AI-generated imagery has blurred the boundary between
real and synthetic content, raising critical concerns for digital integrity.
Vision-language models (VLMs) offer interpretability through explanations but
often fail to detect subtle artifacts in high-quality synthetic images. We
propose ZoomIn, a two-stage forensic framework that improves both accuracy and
interpretability. Mimicking human visual inspection, ZoomIn first scans an
image to locate suspicious regions and then performs a focused analysis on
these zoomed-in areas to deliver a grounded verdict. To support training, we
introduce MagniFake, a dataset of 20,000 real and high-quality synthetic images
annotated with bounding boxes and forensic explanations, generated through an
automated VLM-based pipeline. Our method achieves 96.39% accuracy with robust
generalization, while providing human-understandable explanations grounded in
visual evidence.

</details>


### [97] [A Recursive Pyramidal Algorithm for Solving the Image Registration Problem](https://arxiv.org/abs/2510.04231)
*Stefan Dirnstorfer*

Main category: cs.CV

TL;DR: 本文提出了一种简单、端到端可训练的图像配准算法，只需少量Python代码即可实现，在训练数据少、训练时间短的情况下仍能获得准确结果。


<details>
  <summary>Details</summary>
Motivation: 解决图像配准问题，即找到将两幅图像对齐的变换，使得对应点位于相同位置。传统方法可能复杂且需要大量训练数据。

Method: 开发了一种端到端可训练的简单算法，仅需少量Python代码即可实现。使用74张图像在19x15输入窗口上进行训练，展示了在立体视觉中的应用。

Result: 该算法在训练数据有限、训练时间短的情况下仍能获得准确结果，代码简洁（仅需十几行Python代码）。

Conclusion: 该算法在简洁性方面表现优异，可为训练数据、训练时间或代码复杂度受限的相关场景提供良好起点。

Abstract: The problem of image registration is finding a transformation that aligns two
images, such that the corresponding points are in the same location. This paper
introduces a simple, end-to-end trainable algorithm that is implementable in a
few lines of Python code. The approach is shown to work with very little
training data and training time, while achieving accurate results in some
settings. An example application to stereo vision was trained from 74 images on
a 19x15 input window. With just a dozen lines of Python code this algorithm
excels in brevity and may serve as a good start in related scenarios with
limitations to training data, training time or code complexity.

</details>


### [98] [Detection of retinal diseases using an accelerated reused convolutional network](https://arxiv.org/abs/2510.04232)
*Amin Ahmadi Kasani,Hedieh Sajedi*

Main category: cs.CV

TL;DR: 提出了一种新的卷积层ArConv，构建了参数更少但精度更高的轻量级CNN模型，适用于移动设备上的眼病诊断。


<details>
  <summary>Details</summary>
Motivation: 提高深度神经网络的可访问性，使其能在移动设备上运行，用于眼病早期诊断。现有方法计算复杂度高，限制了实际应用。

Method: 重新设计和优化卷积层，提出新型ArConv层，构建仅含130万参数的轻量级模型。

Result: 在RfMiD数据集上，模型准确率达到0.9328，优于MobileNetV2的0.9266，且参数更少。

Conclusion: ArConv层能有效降低模型复杂度，同时保持高精度，为移动设备上的医疗诊断应用提供了可行方案。

Abstract: Convolutional neural networks are continually evolving, with some efforts
aimed at improving accuracy, others at increasing speed, and some at enhancing
accessibility. Improving accessibility broadens the application of neural
networks across a wider range of tasks, including the detection of eye
diseases. Early diagnosis of eye diseases and consulting an ophthalmologist can
prevent many vision disorders. Given the importance of this issue, various
datasets have been collected from the cornea to facilitate the process of
making neural network models. However, most of the methods introduced in the
past are computationally complex. In this study, we tried to increase the
accessibility of deep neural network models. We did this at the most
fundamental level, specifically by redesigning and optimizing the convolutional
layers. By doing so, we created a new general model that incorporates our novel
convolutional layer named ArConv layers. Thanks to the efficient performance of
this new layer, the model has suitable complexity for use in mobile phones and
can perform the task of diagnosing the presence of disease with high accuracy.
The final model we present contains only 1.3 million parameters. In comparison
to the MobileNetV2 model, which has 2.2 million parameters, our model
demonstrated better accuracy when trained and evaluated on the RfMiD dataset
under identical conditions, achieving an accuracy of 0.9328 versus 0.9266 on
the RfMiD test set.

</details>


### [99] [Scaling Sequence-to-Sequence Generative Neural Rendering](https://arxiv.org/abs/2510.04236)
*Shikun Liu,Kam Woh Ng,Wonbong Jang,Jiadong Guo,Junlin Han,Haozhe Liu,Yiannis Douratsos,Juan C. Pérez,Zijian Zhou,Chi Phung,Tao Xiang,Juan-Manuel Pérez-Rúa*

Main category: cs.CV

TL;DR: Kaleido是一个基于序列到序列图像合成任务的生成式神经渲染模型，通过将3D视为视频的特殊子域，实现了无需显式3D表示的光电真实感对象和场景级渲染。


<details>
  <summary>Details</summary>
Motivation: 传统3D渲染方法依赖显式3D表示和大量相机标注数据，限制了模型的泛化能力和训练效率。Kaleido旨在通过统一3D和视频建模，利用大规模视频数据进行预训练，减少对稀缺3D数据的依赖。

Method: 采用掩码自回归框架和仅解码器的整流流变换器，将3D渲染转化为序列到序列任务，支持任意数量参考视图生成任意数量6自由度目标视图。

Result: 在多个视图合成基准测试中达到新SOTA，零样本性能在少视图设置下显著优于其他生成方法，在多视图设置下首次达到逐场景优化方法的质量水平。

Conclusion: Kaleido证明了将3D建模统一到视频生成框架中的有效性，通过大规模视频预训练显著提升了空间一致性和渲染质量，为神经渲染提供了新的发展方向。

Abstract: We present Kaleido, a family of generative models designed for
photorealistic, unified object- and scene-level neural rendering. Kaleido
operates on the principle that 3D can be regarded as a specialised sub-domain
of video, expressed purely as a sequence-to-sequence image synthesis task.
Through a systemic study of scaling sequence-to-sequence generative neural
rendering, we introduce key architectural innovations that enable our model to:
i) perform generative view synthesis without explicit 3D representations; ii)
generate any number of 6-DoF target views conditioned on any number of
reference views via a masked autoregressive framework; and iii) seamlessly
unify 3D and video modelling within a single decoder-only rectified flow
transformer. Within this unified framework, Kaleido leverages large-scale video
data for pre-training, which significantly improves spatial consistency and
reduces reliance on scarce, camera-labelled 3D datasets -- all without any
architectural modifications. Kaleido sets a new state-of-the-art on a range of
view synthesis benchmarks. Its zero-shot performance substantially outperforms
other generative methods in few-view settings, and, for the first time, matches
the quality of per-scene optimisation methods in many-view settings.

</details>


### [100] [The best performance in the CARE 2025 -- Liver Task (LiSeg-Contrast): Contrast-Aware Semi-Supervised Segmentation with Domain Generalization and Test-Time Adaptation](https://arxiv.org/abs/2510.04243)
*Jincan Lou,Jingkun Chen,Haoquan Li,Hang Li,Wenjian Huang,Weihua Chen,Fan Wang,Jianguo Zhang*

Main category: cs.CV

TL;DR: 提出CoSSeg-TTA框架，在nnU-Netv2基础上结合半监督学习和领域自适应技术，提升Gd-EOB-DTPA增强MRI肝脏分割的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决对比增强MRI肝脏分割面临的标注数据有限、增强协议异构、跨扫描仪和机构领域偏移等挑战，传统图像翻译方法存在结构扭曲和训练不稳定问题。

Method: 基于nnU-Netv2构建紧凑分割框架，采用半监督均值教师方案利用未标注数据，结合随机直方图风格外观转换和可训练对比感知网络的领域自适应模块，并采用持续测试时适应策略。

Result: 在低标注条件下显著超越nnU-Netv2基线，获得更高的Dice分数和Hausdorff距离，对未见领域表现出强泛化能力。

Conclusion: CoSSeg-TTA框架有效解决了单模态MRI肝脏分割的领域泛化问题，为临床诊断和治疗规划提供了可靠工具。

Abstract: Accurate liver segmentation from contrast-enhanced MRI is essential for
diagnosis, treatment planning, and disease monitoring. However, it remains
challenging due to limited annotated data, heterogeneous enhancement protocols,
and significant domain shifts across scanners and institutions. Traditional
image-to-image translation frameworks have made great progress in domain
generalization, but their application is not straightforward. For example,
Pix2Pix requires image registration, and cycle-GAN cannot be integrated
seamlessly into segmentation pipelines. Meanwhile, these methods are originally
used to deal with cross-modality scenarios, and often introduce structural
distortions and suffer from unstable training, which may pose drawbacks in our
single-modality scenario. To address these challenges, we propose CoSSeg-TTA, a
compact segmentation framework for the GED4 (Gd-EOB-DTPA enhanced hepatobiliary
phase MRI) modality built upon nnU-Netv2 and enhanced with a semi-supervised
mean teacher scheme to exploit large amounts of unlabeled volumes. A domain
adaptation module, incorporating a randomized histogram-based style appearance
transfer function and a trainable contrast-aware network, enriches domain
diversity and mitigates cross-center variability. Furthermore, a continual
test-time adaptation strategy is employed to improve robustness during
inference. Extensive experiments demonstrate that our framework consistently
outperforms the nnU-Netv2 baseline, achieving superior Dice score and Hausdorff
Distance while exhibiting strong generalization to unseen domains under
low-annotation conditions.

</details>


### [101] [Concept-Based Masking: A Patch-Agnostic Defense Against Adversarial Patch Attacks](https://arxiv.org/abs/2510.04245)
*Ayushi Mehrotra,Derek Peng,Dipkamal Bhusal,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 提出了一种不依赖补丁大小和位置先验知识的防御方法，通过概念激活向量识别和抑制来中和对抗性补丁攻击的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗性补丁防御方法通常需要预先知道补丁的大小或位置，这限制了它们的实际应用。因此，需要一种不依赖这些先验知识的防御策略。

Method: 利用基于概念的解释方法，识别并抑制最具影响力的概念激活向量，从而在不显式检测补丁的情况下中和其影响。

Result: 在Imagenette数据集上使用ResNet-50进行评估，该方法在鲁棒性和清洁准确率上均优于当前最先进的PatchCleanser方法，并在不同补丁大小和位置下保持强健性能。

Conclusion: 结合可解释性和鲁棒性具有潜力，概念驱动的防御策略为对抗性补丁攻击提供了一种可扩展的保护机器学习模型的方法。

Abstract: Adversarial patch attacks pose a practical threat to deep learning models by
forcing targeted misclassifications through localized perturbations, often
realized in the physical world. Existing defenses typically assume prior
knowledge of patch size or location, limiting their applicability. In this
work, we propose a patch-agnostic defense that leverages concept-based
explanations to identify and suppress the most influential concept activation
vectors, thereby neutralizing patch effects without explicit detection.
Evaluated on Imagenette with a ResNet-50, our method achieves higher robust and
clean accuracy than the state-of-the-art PatchCleanser, while maintaining
strong performance across varying patch sizes and locations. Our results
highlight the promise of combining interpretability with robustness and suggest
concept-driven defenses as a scalable strategy for securing machine learning
models against adversarial patch attacks.

</details>


### [102] [Flexible and Efficient Spatio-Temporal Transformer for Sequential Visual Place Recognition](https://arxiv.org/abs/2510.04282)
*Yu Kiu,Lau,Chao Chen,Ge Jin,Chen Feng*

Main category: cs.CV

TL;DR: 提出Adapt-STformer方法解决Seq-VPR中transformer模型缺乏灵活性和效率的问题，通过循环可变形编码器实现变长序列支持、快速推理和低内存使用


<details>
  <summary>Details</summary>
Motivation: 现有transformer-based Seq-VPR方法过于注重性能而牺牲了灵活性和效率，无法满足实时应用的seq-length灵活性、快速推理和低内存需求

Method: 基于循环可变形Transformer编码器（Recurrent-DTE），采用迭代循环机制融合多帧序列信息，支持变长序列

Result: 在Nordland、Oxford和NuScenes数据集上，召回率提升最高17%，序列提取时间减少36%，内存使用降低35%

Conclusion: Adapt-STformer在保持高性能的同时，显著提升了Seq-VPR方法的灵活性和效率，满足实际应用需求

Abstract: Sequential Visual Place Recognition (Seq-VPR) leverages transformers to
capture spatio-temporal features effectively; however, existing approaches
prioritize performance at the expense of flexibility and efficiency. In
practice, a transformer-based Seq-VPR model should be flexible to the number of
frames per sequence (seq-length), deliver fast inference, and have low memory
usage to meet real-time constraints. To our knowledge, no existing
transformer-based Seq-VPR method achieves both flexibility and efficiency. To
address this gap, we propose Adapt-STformer, a Seq-VPR method built around our
novel Recurrent Deformable Transformer Encoder (Recurrent-DTE), which uses an
iterative recurrent mechanism to fuse information from multiple sequential
frames. This design naturally supports variable seq-lengths, fast inference,
and low memory usage. Experiments on the Nordland, Oxford, and NuScenes
datasets show that Adapt-STformer boosts recall by up to 17% while reducing
sequence extraction time by 36% and lowering memory usage by 35% compared to
the second-best baseline.

</details>


### [103] [ChronoEdit: Towards Temporal Reasoning for Image Editing and World Simulation](https://arxiv.org/abs/2510.04290)
*Jay Zhangjie Wu,Xuanchi Ren,Tianchang Shen,Tianshi Cao,Kai He,Yifan Lu,Ruiyuan Gao,Enze Xie,Shiyi Lan,Jose M. Alvarez,Jun Gao,Sanja Fidler,Zian Wang,Huan Ling*

Main category: cs.CV

TL;DR: ChronoEdit是一个将图像编辑重构为视频生成问题的框架，通过利用预训练视频生成模型的时间一致性来确保物理一致性，在视觉保真度和物理合理性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前大型生成模型在图像编辑和上下文图像生成方面取得了显著进展，但在确保物理一致性方面存在关键差距，即编辑后的对象必须保持连贯性，这对于世界模拟相关任务尤为重要。

Method: ChronoEdit将输入图像和编辑后图像视为视频的首尾帧，利用预训练视频生成模型捕获物体外观和隐含物理运动。引入时间推理阶段，在推理时联合去噪目标帧和推理标记，想象合理的编辑轨迹，约束解空间到物理可行的变换。推理标记在几步后被丢弃以避免完整视频渲染的高计算成本。

Result: 提出了PBench-Edit基准测试，验证ChronoEdit在需要物理一致性的上下文中的表现。实验表明ChronoEdit在视觉保真度和物理合理性方面超越了最先进的基线方法。

Conclusion: ChronoEdit通过将图像编辑重构为视频生成问题，有效解决了物理一致性问题，为世界模拟相关任务提供了可靠的解决方案，并发布了14B和2B变体的代码和模型。

Abstract: Recent advances in large generative models have significantly advanced image
editing and in-context image generation, yet a critical gap remains in ensuring
physical consistency, where edited objects must remain coherent. This
capability is especially vital for world simulation related tasks. In this
paper, we present ChronoEdit, a framework that reframes image editing as a
video generation problem. First, ChronoEdit treats the input and edited images
as the first and last frames of a video, allowing it to leverage large
pretrained video generative models that capture not only object appearance but
also the implicit physics of motion and interaction through learned temporal
consistency. Second, ChronoEdit introduces a temporal reasoning stage that
explicitly performs editing at inference time. Under this setting, the target
frame is jointly denoised with reasoning tokens to imagine a plausible editing
trajectory that constrains the solution space to physically viable
transformations. The reasoning tokens are then dropped after a few steps to
avoid the high computational cost of rendering a full video. To validate
ChronoEdit, we introduce PBench-Edit, a new benchmark of image-prompt pairs for
contexts that require physical consistency, and demonstrate that ChronoEdit
surpasses state-of-the-art baselines in both visual fidelity and physical
plausibility. Code and models for both the 14B and 2B variants of ChronoEdit
will be released on the project page:
https://research.nvidia.com/labs/toronto-ai/chronoedit

</details>


### [104] [CARE-PD: A Multi-Site Anonymized Clinical Dataset for Parkinson's Disease Gait Assessment](https://arxiv.org/abs/2510.04312)
*Vida Adeli,Ivan Klabucar,Javad Rajabi,Benjamin Filtjens,Soroush Mehraban,Diwei Wang,Hyewon Seo,Trung-Hieu Hoang,Minh N. Do,Candice Muller,Claudia Oliveira,Daniel Boari Coelho,Pieter Ginis,Moran Gilat,Alice Nieuwboer,Joke Spildooren,Lucas Mckay,Hyeokhyen Kwon,Gari Clifford,Christine Esper,Stewart Factor,Imari Genias,Amirhossein Dadashzadeh,Leia Shum,Alan Whone,Majid Mirmehdi,Andrea Iaboni,Babak Taati*

Main category: cs.CV

TL;DR: CARE-PD是最大的公开帕金森病3D步态数据集，包含来自8个临床中心的9个队列数据，支持临床评分预测和运动预训练任务，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的客观步态评估缺乏大规模、多样化且临床注释的运动数据集，限制了相关研究的进展。

Method: 将RGB视频或运动捕捉数据通过统一预处理流程转换为匿名SMPL网格，建立两个基准任务：监督式临床评分预测和无监督运动预训练任务。

Result: 在CARE-PD上预训练可将MPJPE从60.8mm降至7.5mm，PD严重程度macro-F1提升17个百分点，运动编码器始终优于手工特征。

Conclusion: CARE-PD证明了临床标注、多样化训练数据的价值，为帕金森病研究提供了重要资源，所有数据和代码已公开供非商业研究使用。

Abstract: Objective gait assessment in Parkinson's Disease (PD) is limited by the
absence of large, diverse, and clinically annotated motion datasets. We
introduce CARE-PD, the largest publicly available archive of 3D mesh gait data
for PD, and the first multi-site collection spanning 9 cohorts from 8 clinical
centers. All recordings (RGB video or motion capture) are converted into
anonymized SMPL meshes via a harmonized preprocessing pipeline. CARE-PD
supports two key benchmarks: supervised clinical score prediction (estimating
Unified Parkinson's Disease Rating Scale, UPDRS, gait scores) and unsupervised
motion pretext tasks (2D-to-3D keypoint lifting and full-body 3D
reconstruction). Clinical prediction is evaluated under four generalization
protocols: within-dataset, cross-dataset, leave-one-dataset-out, and
multi-dataset in-domain adaptation. To assess clinical relevance, we compare
state-of-the-art motion encoders with a traditional gait-feature baseline,
finding that encoders consistently outperform handcrafted features. Pretraining
on CARE-PD reduces MPJPE (from 60.8mm to 7.5mm) and boosts PD severity macro-F1
by 17 percentage points, underscoring the value of clinically curated, diverse
training data. CARE-PD and all benchmark code are released for non-commercial
research at https://neurips2025.care-pd.ca/.

</details>


### [105] [GenAR: Next-Scale Autoregressive Generation for Spatial Gene Expression Prediction](https://arxiv.org/abs/2510.04315)
*Jiarui Ouyang,Yihui Wang,Yihang Gao,Yingxue Xu,Shu Yang,Hao Chen*

Main category: cs.CV

TL;DR: GenAR是一个多尺度自回归框架，通过从H&E染色图像预测空间转录组学数据，解决了现有方法忽略基因共表达结构和连续回归导致的生物学不合理问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法独立预测每个基因且采用连续回归，忽略了基因间的共表达关系，导致预测结果生物学不合理且影响下游分析。

Method: GenAR将基因聚类为层次化组以捕捉跨基因依赖关系，采用无码本离散标记生成直接预测原始计数，并在解码时融合组织学和空间嵌入信息。

Result: 在四个不同组织类型的空间转录组学数据集上，GenAR实现了最先进的性能表现。

Conclusion: GenAR框架为精准医学和成本效益高的分子分析提供了潜在应用价值，代码已开源。

Abstract: Spatial Transcriptomics (ST) offers spatially resolved gene expression but
remains costly. Predicting expression directly from widely available
Hematoxylin and Eosin (H&E) stained images presents a cost-effective
alternative. However, most computational approaches (i) predict each gene
independently, overlooking co-expression structure, and (ii) cast the task as
continuous regression despite expression being discrete counts. This mismatch
can yield biologically implausible outputs and complicate downstream analyses.
We introduce GenAR, a multi-scale autoregressive framework that refines
predictions from coarse to fine. GenAR clusters genes into hierarchical groups
to expose cross-gene dependencies, models expression as codebook-free discrete
token generation to directly predict raw counts, and conditions decoding on
fused histological and spatial embeddings. From an information-theoretic
perspective, the discrete formulation avoids log-induced biases and the
coarse-to-fine factorization aligns with a principled conditional
decomposition. Extensive experimental results on four Spatial Transcriptomics
datasets across different tissue types demonstrate that GenAR achieves
state-of-the-art performance, offering potential implications for precision
medicine and cost-effective molecular profiling. Code is publicly available at
https://github.com/oyjr/genar.

</details>


### [106] [RAP: 3D Rasterization Augmented End-to-End Planning](https://arxiv.org/abs/2510.04333)
*Lan Feng,Yang Gao,Eloi Zablocki,Quanyi Li,Wuyang Li,Sichao Liu,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于3D栅格化的轻量级数据增强方法RAP，用于解决端到端驾驶规划中缺乏恢复数据的问题，通过语义保真而非照片级真实感来提升模型的闭环鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 模仿学习训练的端到端驾驶策略缺乏恢复数据，小错误会快速累积导致失败。现有方法使用神经渲染或游戏引擎生成数字孪生，但成本高、速度慢，主要用于评估。作者认为驾驶规划不需要照片级真实感，而是需要语义保真和可扩展性。

Method: 提出3D栅格化方法，用轻量级栅格化替代昂贵渲染，支持反事实恢复机动和跨智能体视图合成。引入栅格到实物的特征空间对齐来弥合仿真到现实的差距。这些组件构成RAP数据增强管道。

Result: RAP在四个主要基准测试中排名第一：NAVSIM v1/v2、Waymo开放数据集视觉端到端驾驶和Bench2Drive，实现了最先进的闭环鲁棒性和长尾泛化能力。

Conclusion: 轻量级栅格化结合特征对齐足以扩展端到端训练，为照片级渲染提供了实用替代方案。

Abstract: Imitation learning for end-to-end driving trains policies only on expert
demonstrations. Once deployed in a closed loop, such policies lack recovery
data: small mistakes cannot be corrected and quickly compound into failures. A
promising direction is to generate alternative viewpoints and trajectories
beyond the logged path. Prior work explores photorealistic digital twins via
neural rendering or game engines, but these methods are prohibitively slow and
costly, and thus mainly used for evaluation. In this work, we argue that
photorealism is unnecessary for training end-to-end planners. What matters is
semantic fidelity and scalability: driving depends on geometry and dynamics,
not textures or lighting. Motivated by this, we propose 3D Rasterization, which
replaces costly rendering with lightweight rasterization of annotated
primitives, enabling augmentations such as counterfactual recovery maneuvers
and cross-agent view synthesis. To transfer these synthetic views effectively
to real-world deployment, we introduce a Raster-to-Real feature-space alignment
that bridges the sim-to-real gap. Together, these components form Rasterization
Augmented Planning (RAP), a scalable data augmentation pipeline for planning.
RAP achieves state-of-the-art closed-loop robustness and long-tail
generalization, ranking first on four major benchmarks: NAVSIM v1/v2, Waymo
Open Dataset Vision-based E2E Driving, and Bench2Drive. Our results show that
lightweight rasterization with feature alignment suffices to scale E2E
training, offering a practical alternative to photorealistic rendering. Project
page: https://alan-lanfeng.github.io/RAP/.

</details>


### [107] [Diffusion^2: Dual Diffusion Model with Uncertainty-Aware Adaptive Noise for Momentary Trajectory Prediction](https://arxiv.org/abs/2510.04365)
*Yuhao Luo,Yuang Zhang,Kehua Chen,Xinyu Zheng,Shucheng Zhang,Sikai Chen,Yinhai Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion^2框架，专门用于瞬时轨迹预测，通过两个串联的扩散模型分别生成未观测历史轨迹和预测未来轨迹，在ETH/UCY和Stanford Drone数据集上达到新的最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现实场景中（如行人突然从盲区出现）往往缺乏足够的观测数据（瞬时轨迹），这使得准确预测变得困难并增加交通事故风险，因此需要研究极端场景下的行人轨迹预测。

Method: Diffusion^2框架包含两个串联的扩散模型：反向预测模型生成未观测历史轨迹，前向预测模型预测未来轨迹。提出了双头参数化机制估计生成历史轨迹的偶然不确定性，并设计了时间自适应噪声模块动态调节前向扩散过程的噪声尺度。

Result: 在ETH/UCY和Stanford Drone数据集上的实验表明，Diffusion^2在瞬时轨迹预测任务中达到了新的最先进水平。

Conclusion: 该研究为解决瞬时轨迹预测这一具有挑战性的问题提供了有效解决方案，通过生成未观测历史轨迹来弥补数据不足，显著提升了极端场景下的预测准确性。

Abstract: Accurate pedestrian trajectory prediction is crucial for ensuring safety and
efficiency in autonomous driving and human-robot interaction scenarios. Earlier
studies primarily utilized sufficient observational data to predict future
trajectories. However, in real-world scenarios, such as pedestrians suddenly
emerging from blind spots, sufficient observational data is often unavailable
(i.e. momentary trajectory), making accurate prediction challenging and
increasing the risk of traffic accidents. Therefore, advancing research on
pedestrian trajectory prediction under extreme scenarios is critical for
enhancing traffic safety. In this work, we propose a novel framework termed
Diffusion^2, tailored for momentary trajectory prediction. Diffusion^2 consists
of two sequentially connected diffusion models: one for backward prediction,
which generates unobserved historical trajectories, and the other for forward
prediction, which forecasts future trajectories. Given that the generated
unobserved historical trajectories may introduce additional noise, we propose a
dual-head parameterization mechanism to estimate their aleatoric uncertainty
and design a temporally adaptive noise module that dynamically modulates the
noise scale in the forward diffusion process. Empirically, Diffusion^2 sets a
new state-of-the-art in momentary trajectory prediction on ETH/UCY and Stanford
Drone datasets.

</details>


### [108] [MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator](https://arxiv.org/abs/2510.04390)
*Xuehai He,Shijie Zhou,Thivyanth Venkateswaran,Kaizhi Zheng,Ziyu Wan,Achuta Kadambi,Xin Eric Wang*

Main category: cs.CV

TL;DR: MorphoSim是一个语言引导的框架，能够生成具有多视角一致性和对象级控制的4D场景，支持从自然语言指令创建动态环境并进行交互式编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频模型局限于2D视图且交互性有限，需要开发能够支持可控可编辑时空环境的世界模型，以服务于机器人技术中的可扩展训练数据、可重复评估和灵活任务设计。

Method: MorphoSim集成了轨迹引导生成和特征场蒸馏技术，允许在不完全重新生成的情况下进行交互式编辑，从自然语言指令生成多视角一致的动态环境。

Result: 实验表明MorphoSim在保持高场景保真度的同时实现了可控性和可编辑性，能够对对象进行定向、重新着色或移除等操作，并支持任意视角观察。

Conclusion: MorphoSim框架成功解决了现有文本到视频模型的局限性，为机器人技术提供了更强大的4D场景生成和编辑能力，代码已开源。

Abstract: World models that support controllable
  and editable spatiotemporal environments are valuable
  for robotics, enabling scalable training data, repro ducible evaluation, and
flexible task design. While
  recent text-to-video models generate realistic dynam ics, they are
constrained to 2D views and offer limited
  interaction. We introduce MorphoSim, a language guided framework that
generates 4D scenes with
  multi-view consistency and object-level controls. From
  natural language instructions, MorphoSim produces
  dynamic environments where objects can be directed,
  recolored, or removed, and scenes can be observed
  from arbitrary viewpoints. The framework integrates
  trajectory-guided generation with feature field dis tillation, allowing edits
to be applied interactively
  without full re-generation. Experiments show that Mor phoSim maintains high
scene fidelity while enabling
  controllability and editability. The code is available
  at https://github.com/eric-ai-lab/Morph4D.

</details>


### [109] [Your Vision-Language Model Can't Even Count to 20: Exposing the Failures of VLMs in Compositional Counting](https://arxiv.org/abs/2510.04401)
*Xuyang Guo,Zekai Huang,Zhenmei Shi,Zhao Song,Jiahao Zhang*

Main category: cs.CV

TL;DR: 提出了VLMCountBench基准测试，发现当前视觉语言模型在单一形状计数时表现可靠，但在组合形状计数时存在显著失败，揭示了模型的基本局限性


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在各种任务上表现出色，但它们在基本计数能力方面的表现尚不明确，特别是能否正确计数物体

Method: 设计了VLMCountBench基准测试，使用基本几何形状及其组合，在严格控制变量的简约设置下系统研究颜色、大小和提示词细化等简单属性的影响

Result: 实验结果显示，VLMs在单一形状类型存在时可以可靠计数，但在组合多种形状类型时表现出显著的计数失败

Conclusion: 当前VLMs在组合计数方面存在根本性的经验限制，这为未来研究指明了重要方向

Abstract: Vision-Language Models (VLMs) have become a central focus of today's AI
community, owing to their impressive abilities gained from training on
large-scale vision-language data from the Web. These models have demonstrated
strong performance across diverse tasks, including image understanding, video
understanding, complex visual reasoning, and embodied AI. Despite these
noteworthy successes, a fundamental question remains: Can VLMs count objects
correctly? In this paper, we introduce a simple yet effective benchmark,
VLMCountBench, designed under a minimalist setting with only basic geometric
shapes (e.g., triangles, circles) and their compositions, focusing exclusively
on counting tasks without interference from other factors. We adopt strict
independent variable control and systematically study the effects of simple
properties such as color, size, and prompt refinement in a controlled ablation.
Our empirical results reveal that while VLMs can count reliably when only one
shape type is present, they exhibit substantial failures when multiple shape
types are combined (i.e., compositional counting). This highlights a
fundamental empirical limitation of current VLMs and motivates important
directions for future research.

</details>


### [110] [CodeFormer++: Blind Face Restoration Using Deformable Registration and Deep Metric Learning](https://arxiv.org/abs/2510.04410)
*Venkata Bharath Reddy Reddem,Akshay P Sarashetti,Ranjith Merugu,Amit Satish Unde*

Main category: cs.CV

TL;DR: CodeFormer++是一个新颖的盲人脸恢复框架，通过分解任务为身份保持恢复、高质量生成和动态融合，解决了现有方法在视觉质量和身份保真度之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有盲人脸恢复方法在集成生成先验时存在视觉质量与身份保真度之间的权衡，导致要么身份失真，要么退化去除不理想。

Method: 提出三个关键贡献：基于学习的可变形人脸配准模块、纹理引导恢复网络以及深度度量学习集成，通过语义对齐、纹理传递和特征融合实现高质量恢复。

Result: 在真实世界和合成数据集上的广泛实验表明，CodeFormer++在视觉保真度和身份一致性方面均达到优越性能。

Conclusion: CodeFormer++通过最大化生成先验的效用，成功实现了高质量人脸恢复同时保持身份特征，解决了现有方法的局限性。

Abstract: Blind face restoration (BFR) has attracted increasing attention with the rise
of generative methods. Most existing approaches integrate generative priors
into the restoration pro- cess, aiming to jointly address facial detail
generation and identity preservation. However, these methods often suffer from
a trade-off between visual quality and identity fidelity, leading to either
identity distortion or suboptimal degradation removal. In this paper, we
present CodeFormer++, a novel framework that maximizes the utility of
generative priors for high-quality face restoration while preserving identity.
We decompose BFR into three sub-tasks: (i) identity- preserving face
restoration, (ii) high-quality face generation, and (iii) dynamic fusion of
identity features with realistic texture details. Our method makes three key
contributions: (1) a learning-based deformable face registration module that
semantically aligns generated and restored faces; (2) a texture guided
restoration network to dynamically extract and transfer the texture of
generated face to boost the quality of identity-preserving restored face; and
(3) the integration of deep metric learning for BFR with the generation of
informative positive and hard negative samples to better fuse identity-
preserving and generative features. Extensive experiments on real-world and
synthetic datasets demonstrate that, the pro- posed CodeFormer++ achieves
superior performance in terms of both visual fidelity and identity consistency.

</details>


### [111] [A.I.R.: Enabling Adaptive, Iterative, and Reasoning-based Frame Selection For Video Question Answering](https://arxiv.org/abs/2510.04428)
*Yuanhao Zou,Shengji Jin,Andong Deng,Youpeng Zhao,Jun Wang,Chen Chen*

Main category: cs.CV

TL;DR: 提出A.I.R.方法，通过自适应、迭代和基于推理的帧选择策略，在视频问答任务中平衡计算效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决当前帧选择方法面临的权衡问题：轻量级相似性模型无法捕捉复杂查询的细微差别，而使用VLM进行深度分析的方法计算成本过高。

Method: 利用强大的VLM对复杂查询进行深度语义分析，并在计算高效的迭代循环中仅处理少量高潜力帧，实现训练自适应的帧选择。

Result: 在多个VideoQA基准测试中表现出色，优于现有帧选择方法，显著提升了基础VLM的性能，并大幅提高了计算效率。

Conclusion: A.I.R.方法有效解决了视频问答中帧选择的准确性与计算效率之间的平衡问题，为VLM在视频理解任务中的应用提供了实用解决方案。

Abstract: Effectively applying Vision-Language Models (VLMs) to Video Question
Answering (VideoQA) hinges on selecting a concise yet comprehensive set of
frames, as processing entire videos is computationally infeasible. However,
current frame selection methods face a critical trade-off: approaches relying
on lightweight similarity models, such as CLIP, often fail to capture the
nuances of complex queries, resulting in inaccurate similarity scores that
cannot reflect the authentic query-frame relevance, which further undermines
frame selection. Meanwhile, methods that leverage a VLM for deeper analysis
achieve higher accuracy but incur prohibitive computational costs. To address
these limitations, we propose A.I.R., a training-free approach for Adaptive,
Iterative, and Reasoning-based frame selection. We leverage a powerful VLM to
perform deep, semantic analysis on complex queries, and this analysis is
deployed within a cost-effective iterative loop that processes only a small
batch of the most high-potential frames at a time. Extensive experiments on
various VideoQA benchmarks demonstrate that our approach outperforms existing
frame selection methods, significantly boosts the performance of the foundation
VLM, and achieves substantial gains in computational efficiency over other
VLM-based techniques.

</details>


### [112] [REAR: Rethinking Visual Autoregressive Models via Generator-Tokenizer Consistency Regularization](https://arxiv.org/abs/2510.04450)
*Qiyuan He,Yicong Li,Haotian Ye,Jinghao Wang,Xinyao Liao,Pheng-Ann Heng,Stefano Ermon,James Zou,Angela Yao*

Main category: cs.CV

TL;DR: reAR是一种简单的训练策略，通过引入token-wise正则化目标来解决视觉自回归生成中的生成器-分词器不一致问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归生成性能不如扩散模型，主要瓶颈在于生成器-分词器不一致——AR生成的token可能无法被分词器很好地解码。

Method: 提出reAR训练策略：在预测下一个token时，因果变换器同时训练恢复当前token的视觉嵌入，并在噪声上下文中预测目标token的嵌入。该方法无需改变分词器、生成顺序、推理流程或外部模型。

Result: 在ImageNet上，gFID从3.02降至1.86，IS提升至316.9。应用于先进分词器时，仅用1.77亿参数就达到gFID 1.42，与更大的扩散模型（6.75亿参数）性能相当。

Conclusion: reAR通过简单的正则化策略有效解决了生成器-分词器不一致问题，显著提升了视觉自回归生成的性能，达到了与更大扩散模型相当的水平。

Abstract: Visual autoregressive (AR) generation offers a promising path toward unifying
vision and language models, yet its performance remains suboptimal against
diffusion models. Prior work often attributes this gap to tokenizer limitations
and rasterization ordering. In this work, we identify a core bottleneck from
the perspective of generator-tokenizer inconsistency, i.e., the AR-generated
tokens may not be well-decoded by the tokenizer. To address this, we propose
reAR, a simple training strategy introducing a token-wise regularization
objective: when predicting the next token, the causal transformer is also
trained to recover the visual embedding of the current token and predict the
embedding of the target token under a noisy context. It requires no changes to
the tokenizer, generation order, inference pipeline, or external models.
Despite its simplicity, reAR substantially improves performance. On ImageNet,
it reduces gFID from 3.02 to 1.86 and improves IS to 316.9 using a standard
rasterization-based tokenizer. When applied to advanced tokenizers, it achieves
a gFID of 1.42 with only 177M parameters, matching the performance with larger
state-of-the-art diffusion models (675M).

</details>


### [113] [SPEGNet: Synergistic Perception-Guided Network for Camouflaged Object Detection](https://arxiv.org/abs/2510.04472)
*Baber Jan,Saeed Anwar,Aiman H. El-Maleh,Abdul Jabbar Siddiqui,Abdul Bais*

Main category: cs.CV

TL;DR: SPEGNet提出了一种统一的伪装目标检测方法，通过通道校准和空间增强集成多尺度特征，避免传统方法中复杂组件累积带来的计算负担和细节丢失问题。


<details>
  <summary>Details</summary>
Motivation: 当前伪装目标检测方法依赖累积复杂组件（如边界模块、注意力机制、多尺度处理器），导致计算负担增加但性能提升有限。这些方法通常需要在降低分辨率下处理，从而丢失对伪装检测至关重要的精细细节。

Method: SPEGNet采用统一设计架构，通过通道校准和空间增强集成多尺度特征。边界直接从上下文丰富的表示中产生，保持语义-空间对齐。渐进式细化实现尺度自适应边缘调制，在中间分辨率达到峰值影响。

Result: SPEGNet在CAMO数据集上达到0.887 Sα，COD10K上0.890，NC4K上0.895，同时具有实时推理速度。该方法在各种尺度上都表现优异，从微小复杂物体到大型模式相似物体，并能处理遮挡和模糊边界。

Conclusion: SPEGNet通过统一设计平衡了边界精度和区域一致性，解决了传统方法因组件累积导致的复杂性问题，在保持高性能的同时实现了实时推理。

Abstract: Camouflaged object detection segments objects with intrinsic similarity and
edge disruption. Current detection methods rely on accumulated complex
components. Each approach adds components such as boundary modules, attention
mechanisms, and multi-scale processors independently. This accumulation creates
a computational burden without proportional gains. To manage this complexity,
they process at reduced resolutions, eliminating fine details essential for
camouflage. We present SPEGNet, addressing fragmentation through a unified
design. The architecture integrates multi-scale features via channel
calibration and spatial enhancement. Boundaries emerge directly from
context-rich representations, maintaining semantic-spatial alignment.
Progressive refinement implements scale-adaptive edge modulation with peak
influence at intermediate resolutions. This design strikes a balance between
boundary precision and regional consistency. SPEGNet achieves 0.887 $S_\alpha$
on CAMO, 0.890 on COD10K, and 0.895 on NC4K, with real-time inference speed.
Our approach excels across scales, from tiny, intricate objects to large,
pattern-similar ones, while handling occlusion and ambiguous boundaries. Code,
model weights, and results are available on
\href{https://github.com/Baber-Jan/SPEGNet}{https://github.com/Baber-Jan/SPEGNet}.

</details>


### [114] [MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models](https://arxiv.org/abs/2510.04477)
*Soo Yong Kim,Suin Cho,Vincent-Daniel Yun,Gyeongyeon Hwang*

Main category: cs.CV

TL;DR: MedCLM是一个自动化管道，将检测数据集转换为大规模医学视觉问答数据，通过集成链式思维推理和课程学习策略，在多个医学VQA基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中临床诊断推理与AI结合的核心挑战，将检测数据集转化为具有结构化推理的大规模医学VQA数据。

Method: 提出MedCLM自动化管道，将病变框与器官分割和结构化原理链接，生成带CoT推理的问答对；采用集成CoT-课程策略，分为易、中、难三个阶段进行渐进式学习。

Result: 在多个医学VQA基准测试中达到最先进性能，为开发临床对齐的医学视觉语言模型提供了可扩展框架。

Conclusion: MedCLM通过结合链式思维推理和课程学习策略，成功解决了医学影像诊断推理与AI结合的挑战，提供了有效的可扩展解决方案。

Abstract: Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

</details>


### [115] [VaseVQA-3D: Benchmarking 3D VLMs on Ancient Greek Pottery](https://arxiv.org/abs/2510.04479)
*Nonghai Zhang,Zeyu Zhang,Jiazi Wang,Yang Zhao,Hao Tang*

Main category: cs.CV

TL;DR: 该论文针对视觉语言模型在文化遗产3D花瓶分析中的局限性，提出了首个3D花瓶视觉问答数据集VaseVQA-3D和VaseVLM模型，通过领域自适应训练显著提升了3D花瓶文物的识别和理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理文化遗产等专业领域时面临数据稀缺和领域知识不足的问题，特别是在3D花瓶文物分析方面表现不佳。

Method: 构建了包含664个古希腊花瓶3D模型的VaseVQA-3D数据集，并开发了VaseVLM模型，通过领域自适应训练来提升模型在花瓶文物分析中的性能。

Result: 在VaseVQA-3D数据集上，R@1指标提升了12.8%，词汇相似度提升了6.6%，显著优于现有最先进方法。

Conclusion: 该方法为数字文化遗产保护研究提供了新的技术路径，有效解决了专业领域视觉语言模型的应用挑战。

Abstract: Vision-Language Models (VLMs) have achieved significant progress in
multimodal understanding tasks, demonstrating strong capabilities particularly
in general tasks such as image captioning and visual reasoning. However, when
dealing with specialized cultural heritage domains like 3D vase artifacts,
existing models face severe data scarcity issues and insufficient domain
knowledge limitations. Due to the lack of targeted training data, current VLMs
struggle to effectively handle such culturally significant specialized tasks.
To address these challenges, we propose the VaseVQA-3D dataset, which serves as
the first 3D visual question answering dataset for ancient Greek pottery
analysis, collecting 664 ancient Greek vase 3D models with corresponding
question-answer data and establishing a complete data construction pipeline. We
further develop the VaseVLM model, enhancing model performance in vase artifact
analysis through domain-adaptive training. Experimental results validate the
effectiveness of our approach, where we improve by 12.8% on R@1 metrics and by
6.6% on lexical similarity compared with previous state-of-the-art on the
VaseVQA-3D dataset, significantly improving the recognition and understanding
of 3D vase artifacts, providing new technical pathways for digital heritage
preservation research.

</details>


### [116] [TBStar-Edit: From Image Editing Pattern Shifting to Consistency Enhancement](https://arxiv.org/abs/2510.04483)
*Hao Fang,Zechao Zhan,Weixin Feng,Ziwei Huang,XuBin Li,Tiezheng Ge*

Main category: cs.CV

TL;DR: TBStar-Edit是一个专门为电商领域设计的图像编辑模型，通过数据工程、模型架构设计和两阶段训练策略，解决了通用模型在电商场景中的一致性限制问题。


<details>
  <summary>Details</summary>
Motivation: 通用图像生成和编辑模型在电商场景中经常遇到一致性限制，无法很好地保持产品外观和布局的完整性，因此需要专门针对电商领域开发定制化模型。

Method: 采用三部分方法：1）建立全面的数据构建管道进行数据收集、构建、过滤和增强；2）设计分层模型框架，包含基础模型、模式转换模块和一致性增强模块；3）采用两阶段训练策略，第一阶段进行编辑模式转换，第二阶段进行一致性增强。

Result: 在自建的电商基准测试中，TBStar-Edit在客观指标（VIE Score）和主观用户偏好方面均优于现有的通用领域编辑模型。

Conclusion: TBStar-Edit通过专门针对电商领域的设计，成功实现了精确和高保真度的图像编辑，同时保持了产品外观和布局的完整性，为电商图像编辑提供了有效的解决方案。

Abstract: Recent advances in image generation and editing technologies have enabled
state-of-the-art models to achieve impressive results in general domains.
However, when applied to e-commerce scenarios, these general models often
encounter consistency limitations. To address this challenge, we introduce
TBStar-Edit, an new image editing model tailored for the e-commerce domain.
Through rigorous data engineering, model architecture design and training
strategy, TBStar-Edit achieves precise and high-fidelity image editing while
maintaining the integrity of product appearance and layout. Specifically, for
data engineering, we establish a comprehensive data construction pipeline,
encompassing data collection, construction, filtering, and augmentation, to
acquire high-quality, instruction-following, and strongly consistent editing
data to support model training. For model architecture design, we design a
hierarchical model framework consisting of a base model, pattern shifting
modules, and consistency enhancement modules. For model training, we adopt a
two-stage training strategy to enhance the consistency preservation: first
stage for editing pattern shifting, and second stage for consistency
enhancement. Each stage involves training different modules with separate
datasets. Finally, we conduct extensive evaluations of TBStar-Edit on a
self-proposed e-commerce benchmark, and the results demonstrate that
TBStar-Edit outperforms existing general-domain editing models in both
objective metrics (VIE Score) and subjective user preference.

</details>


### [117] [Asynchronous Denoising Diffusion Models for Aligning Text-to-Image Generation](https://arxiv.org/abs/2510.04504)
*Zijing Hu,Yunze Tong,Fengda Zhang,Junkun Yuan,Jun Xiao,Kun Kuang*

Main category: cs.CV

TL;DR: 提出异步扩散模型框架，通过为不同像素分配不同时间步长，让提示相关区域比无关区域更渐进地去噪，从而改善文本到图像的对齐效果


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型采用同步去噪，所有像素同时从噪声演变为清晰图像，导致提示相关区域只能参考同样噪声水平的无关区域，无法获得清晰上下文，最终影响文本到图像对齐质量

Method: 提出异步扩散模型，动态调制单个像素的时间步长调度，让提示相关区域比无关区域更渐进地去噪，使其能够利用更清晰的像素间上下文

Result: 大量实验表明，异步扩散模型能够显著提高各种提示下的文本到图像对齐效果

Conclusion: 异步扩散模型通过差异化像素去噪时间安排，有效解决了同步去噪导致的文本对齐问题，为扩散模型提供了新的改进方向

Abstract: Diffusion models have achieved impressive results in generating high-quality
images. Yet, they often struggle to faithfully align the generated images with
the input prompts. This limitation arises from synchronous denoising, where all
pixels simultaneously evolve from random noise to clear images. As a result,
during generation, the prompt-related regions can only reference the unrelated
regions at the same noise level, failing to obtain clear context and ultimately
impairing text-to-image alignment. To address this issue, we propose
asynchronous diffusion models -- a novel framework that allocates distinct
timesteps to different pixels and reformulates the pixel-wise denoising
process. By dynamically modulating the timestep schedules of individual pixels,
prompt-related regions are denoised more gradually than unrelated regions,
thereby allowing them to leverage clearer inter-pixel context. Consequently,
these prompt-related regions achieve better alignment in the final images.
Extensive experiments demonstrate that our asynchronous diffusion models can
significantly improve text-to-image alignment across diverse prompts. The code
repository for this work is available at https://github.com/hu-zijing/AsynDM.

</details>


### [118] [TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling](https://arxiv.org/abs/2510.04533)
*Hyunmin Cho,Donghoon Ahn,Susung Hong,Jee Eun Kim,Seungryong Kim,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了TAG（Tangential Amplifying Guidance）方法，通过放大切向分量来修正扩散模型的采样轨迹，提高生成质量而不增加计算负担


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像生成中常出现语义不一致或幻觉问题，现有推理时引导方法依赖外部信号或架构修改，计算开销大

Method: 利用中间样本作为投影基，放大估计分数的切向分量来修正采样轨迹，基于一阶泰勒展开形式化指导过程

Result: TAG是一种即插即用、架构无关的模块，能以最小计算成本提高扩散采样保真度

Conclusion: TAG为扩散引导提供了新视角，通过直接操作轨迹信号实现高效引导

Abstract: Recent diffusion models achieve the state-of-the-art performance in image
generation, but often suffer from semantic inconsistencies or hallucinations.
While various inference-time guidance methods can enhance generation, they
often operate indirectly by relying on external signals or architectural
modifications, which introduces additional computational overhead. In this
paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and
direct guidance method that operates solely on trajectory signals without
modifying the underlying diffusion model. TAG leverages an intermediate sample
as a projection basis and amplifies the tangential components of the estimated
scores with respect to this basis to correct the sampling trajectory. We
formalize this guidance process by leveraging a first-order Taylor expansion,
which demonstrates that amplifying the tangential component steers the state
toward higher-probability regions, thereby reducing inconsistencies and
enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module
that improves diffusion sampling fidelity with minimal computational addition,
offering a new perspective on diffusion guidance.

</details>


### [119] [Conditional Representation Learning for Customized Tasks](https://arxiv.org/abs/2510.04564)
*Honglin Liu,Chao Sun,Peng Hu,Yunfan Li,Xi Peng*

Main category: cs.CV

TL;DR: 本文提出条件表示学习（CRL）方法，通过用户指定的条件生成定制化的特征表示，解决通用表示学习与下游任务需求不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 传统表示学习方法学习的是通用表示，主要捕捉主导语义，可能与特定下游任务需求不一致。例如在动物栖息地分析中，研究者关注场景相关特征，而通用嵌入强调类别语义，导致结果不理想。现有方法需要监督微调，计算和标注成本高。

Method: CRL利用语义空间的基础由基向量决定的原理，通过大语言模型生成描述性文本来构建语义基，然后利用视觉语言模型将图像表示投影到这个条件特征空间中。

Result: 在分类和检索任务上的大量实验证明了CRL的优越性和通用性。

Conclusion: 条件表示学习能够为任意用户指定的条件提取定制化的表示，更好地捕捉特定标准下的语义信息，适用于多种定制化任务。

Abstract: Conventional representation learning methods learn a universal representation
that primarily captures dominant semantics, which may not always align with
customized downstream tasks. For instance, in animal habitat analysis,
researchers prioritize scene-related features, whereas universal embeddings
emphasize categorical semantics, leading to suboptimal results. As a solution,
existing approaches resort to supervised fine-tuning, which however incurs high
computational and annotation costs. In this paper, we propose Conditional
Representation Learning (CRL), aiming to extract representations tailored to
arbitrary user-specified criteria. Specifically, we reveal that the semantics
of a space are determined by its basis, thereby enabling a set of descriptive
words to approximate the basis for a customized feature space. Building upon
this insight, given a user-specified criterion, CRL first employs a large
language model (LLM) to generate descriptive texts to construct the semantic
basis, then projects the image representation into this conditional feature
space leveraging a vision-language model (VLM). The conditional representation
better captures semantics for the specific criterion, which could be utilized
for multiple customized tasks. Extensive experiments on classification and
retrieval tasks demonstrate the superiority and generality of the proposed CRL.
The code is available at https://github.com/XLearning-SCU/2025-NeurIPS-CRL.

</details>


### [120] [Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior](https://arxiv.org/abs/2510.04587)
*Sheng Wang,Ruiming Wu,Charles Herndon,Yihang Liu,Shunsuke Koga,Jeanne Shen,Zhi Huang*

Main category: cs.CV

TL;DR: 该论文提出了一种将病理医生日常查看WSI图像的行为记录转化为可扩展监督数据的方法，并基于此构建了Pathologist-o3智能代理系统，在胃肠道淋巴结转移检测任务上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的病理基础模型虽然强大，但缺乏实用的智能代理系统来模拟病理医生的多阶段诊断过程（包括调整放大倍数、移动视野等）。主要障碍是缺乏临床对齐的专家行为监督数据，这些行为是经验性的而非教科书式的。

Method: 开发了AI Session Recorder工具，记录病理医生在标准WSI查看器中的导航行为，并将其转化为标准化行为命令。通过轻量级人工审核将AI生成的解释转化为Pathology-CoT数据集。基于此构建了两阶段的Pathologist-o3代理，先提出感兴趣区域，再进行行为引导的推理。

Result: 在胃肠道淋巴结转移检测任务上，Pathologist-o3达到了84.5%的精确率、100.0%的召回率和75.4%的准确率，超过了最先进的OpenAI o3模型，并在不同骨干网络上表现出良好的泛化性。

Conclusion: 该框架通过将日常查看记录转化为可扩展的专家验证监督数据，使智能病理代理变得实用，并为人类对齐、可升级的临床AI建立了路径。

Abstract: Diagnosing a whole-slide image is an interactive, multi-stage process
involving changes in magnification and movement between fields. Although recent
pathology foundation models are strong, practical agentic systems that decide
what field to examine next, adjust magnification, and deliver explainable
diagnoses are still lacking. The blocker is data: scalable, clinically aligned
supervision of expert viewing behavior that is tacit and experience-based, not
written in textbooks or online, and therefore absent from large language model
training. We introduce the AI Session Recorder, which works with standard WSI
viewers to unobtrusively record routine navigation and convert the viewer logs
into standardized behavioral commands (inspect or peek at discrete
magnifications) and bounding boxes. A lightweight human-in-the-loop review
turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired
"where to look" and "why it matters" supervision produced at roughly six times
lower labeling time. Using this behavioral data, we build Pathologist-o3, a
two-stage agent that first proposes regions of interest and then performs
behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection,
it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the
state-of-the-art OpenAI o3 model and generalizing across backbones. To our
knowledge, this constitutes one of the first behavior-grounded agentic systems
in pathology. Turning everyday viewer logs into scalable, expert-validated
supervision, our framework makes agentic pathology practical and establishes a
path to human-aligned, upgradeable clinical AI.

</details>


### [121] [A Spatial-Spectral-Frequency Interactive Network for Multimodal Remote Sensing Classification](https://arxiv.org/abs/2510.04628)
*Hao Liu,Yunhao Gao,Wei Li,Mingyang Zhang,Maoguo Gong,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 本文提出了一种空间-光谱-频率交互网络（S²Fin），通过引入频域学习来建模关键和稀疏的细节特征，解决了多模态遥感图像分类中结构特征和细节特征提取困难的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态遥感图像分类方法在从异构和冗余的多模态图像中提取结构和细节特征方面存在困难，需要引入频域学习来优化特征提取。

Method: 提出了S²Fin网络，包括高频稀疏增强变换器（使用稀疏空间-光谱注意力优化高频滤波器参数）、两级空间-频率融合策略（自适应频率通道模块融合低频结构和高频细节，高频共振掩码通过相位相似性强调锐利边缘）以及空间-光谱注意力融合模块。

Result: 在四个基准多模态数据集上的实验表明，S²Fin在有限标注数据情况下实现了优越的分类性能，超越了现有最先进方法。

Conclusion: S²Fin通过空间-光谱-频率域的交互融合，有效提升了多模态遥感图像分类的准确性和特征提取能力，证明了频域学习在多模态遥感数据分析中的价值。

Abstract: Deep learning-based methods have achieved significant success in remote
sensing Earth observation data analysis. Numerous feature fusion techniques
address multimodal remote sensing image classification by integrating global
and local features. However, these techniques often struggle to extract
structural and detail features from heterogeneous and redundant multimodal
images. With the goal of introducing frequency domain learning to model key and
sparse detail features, this paper introduces the spatial-spectral-frequency
interaction network (S$^2$Fin), which integrates pairwise fusion modules across
the spatial, spectral, and frequency domains. Specifically, we propose a
high-frequency sparse enhancement transformer that employs sparse
spatial-spectral attention to optimize the parameters of the high-frequency
filter. Subsequently, a two-level spatial-frequency fusion strategy is
introduced, comprising an adaptive frequency channel module that fuses
low-frequency structures with enhanced high-frequency details, and a
high-frequency resonance mask that emphasizes sharp edges via phase similarity.
In addition, a spatial-spectral attention fusion module further enhances
feature extraction at intermediate layers of the network. Experiments on four
benchmark multimodal datasets with limited labeled data demonstrate that
S$^2$Fin performs superior classification, outperforming state-of-the-art
methods. The code is available at https://github.com/HaoLiu-XDU/SSFin.

</details>


### [122] [SFANet: Spatial-Frequency Attention Network for Deepfake Detection](https://arxiv.org/abs/2510.04630)
*Vrushank Ahire,Aniruddh Muley,Shivam Zample,Siddharth Verma,Pranav Menon,Surbhi Madan,Abhinav Dhall*

Main category: cs.CV

TL;DR: 提出了一种结合Transformer架构和纹理方法的集成框架，用于检测深度伪造媒体，在DFWild-Cup数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以在不同数据集和生成技术间泛化，需要更鲁棒的检测方案。

Method: 采用集成学习框架，结合Swin Transformers、ViTs和纹理方法，引入数据分割、顺序训练、频率分割、基于补丁的注意力和面部分割等技术。

Result: 在包含八个深度伪造数据集的DFWild-Cup数据集上实现了最先进的检测性能。

Conclusion: 混合模型能有效应对深度伪造检测的挑战，为实际应用提供鲁棒解决方案。

Abstract: Detecting manipulated media has now become a pressing issue with the recent
rise of deepfakes. Most existing approaches fail to generalize across diverse
datasets and generation techniques. We thus propose a novel ensemble framework,
combining the strengths of transformer-based architectures, such as Swin
Transformers and ViTs, and texture-based methods, to achieve better detection
accuracy and robustness. Our method introduces innovative data-splitting,
sequential training, frequency splitting, patch-based attention, and face
segmentation techniques to handle dataset imbalances, enhance high-impact
regions (e.g., eyes and mouth), and improve generalization. Our model achieves
state-of-the-art performance when tested on the DFWild-Cup dataset, a diverse
subset of eight deepfake datasets. The ensemble benefits from the
complementarity of these approaches, with transformers excelling in global
feature extraction and texturebased methods providing interpretability. This
work demonstrates that hybrid models can effectively address the evolving
challenges of deepfake detection, offering a robust solution for real-world
applications.

</details>


### [123] [Do Superpixel Segmentation Methods Influence Deforestation Image Classification?](https://arxiv.org/abs/2510.04645)
*Hugo Resende,Fabio A. Faria,Eduardo B. Neto,Isabela Borlido,Victor Sundermann,Silvio Jamil F. Guimarães,Álvaro L. Fazenda*

Main category: cs.CV

TL;DR: 本文研究了不同超像素分割方法对森林砍伐检测分类器性能的影响，发现通过分类器融合方法可以显著提升检测精度。


<details>
  <summary>Details</summary>
Motivation: 在ForestEyes项目中，传统使用SLIC算法进行图像分割，但研究表明其他超像素方法在遥感图像分割中表现更好，可能更适合森林砍伐检测任务。

Method: 比较了四种最佳分割方法与SLIC算法，使用PyCaret AutoML库选择前五名分类器，并应用分类器融合方法（集成学习）。

Result: 最初结果显示不同分割方法间性能差异不大，但应用分类器融合后，平衡准确率有显著提升。

Conclusion: 分割方法的选择和基于机器学习的模型组合对于森林砍伐检测任务都至关重要，集成学习方法能有效提高检测性能。

Abstract: Image segmentation is a crucial step in various visual applications,
including environmental monitoring through remote sensing. In the context of
the ForestEyes project, which combines citizen science and machine learning to
detect deforestation in tropical forests, image segments are used for labeling
by volunteers and subsequent model training. Traditionally, the Simple Linear
Iterative Clustering (SLIC) algorithm is adopted as the segmentation method.
However, recent studies have indicated that other superpixel-based methods
outperform SLIC in remote sensing image segmentation, and might suggest that
they are more suitable for the task of detecting deforested areas. In this
sense, this study investigated the impact of the four best segmentation
methods, together with SLIC, on the training of classifiers for the target
application. Initially, the results showed little variation in performance
among segmentation methods, even when selecting the top five classifiers using
the PyCaret AutoML library. However, by applying a classifier fusion approach
(ensemble of classifiers), noticeable improvements in balanced accuracy were
observed, highlighting the importance of both the choice of segmentation method
and the combination of machine learning-based models for deforestation
detection tasks.

</details>


### [124] [EduPersona: Benchmarking Subjective Ability Boundaries of Virtual Student Agents](https://arxiv.org/abs/2510.04648)
*Buyuan Zhu,Shiyu Hu,Yiping Ma,Yuanming Zhang,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduPersona是一个专注于评估AI虚拟学生在课堂环境中主观能力的大规模基准测试，涵盖2种语言、3个学科和10种人格类型，包含13万+对话轮次，通过三层任务评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在教育领域的应用增多，虚拟学生代理在课堂模拟和教师培训中变得重要，但其课堂导向的主观能力缺乏系统评估，限制了模型边界理解和可信部署。

Method: 构建EduPersona基准数据集，包含1,308个真实课堂对话和人格风格化扩展的12.8万轮对话；设计三层评估任务：基础一致性、学生真实性和长期人格一致性；对3个代表性LLM及其10个人格微调变体进行系统实验。

Result: 人格微调模型在所有任务上均取得显著提升：TASK1 +33.6%、TASK2 +30.6%、TASK3 +14.9%，证明了数据集的有效性和研究价值，同时揭示了人格建模的异质性难度。

Conclusion: EduPersona提供了首个以主观能力为中心的课堂基准，建立了可解耦和可验证的研究范式，将开源数据集和框架支持教育AI研究社区发展可信赖、类人AI。

Abstract: As large language models are increasingly integrated into education, virtual
student agents are becoming vital for classroom simulation and teacher
training. Yet their classroom-oriented subjective abilities remain largely
unassessed, limiting understanding of model boundaries and hindering
trustworthy deployment. We present EduPersona, a large-scale benchmark spanning
two languages, three subjects, and ten persona types based on the Big Five
theory. The dataset contains 1,308 authentic classroom dialogue rounds,
corresponding to 12,814 teacher-student Q&A turns, and is further expanded
through persona stylization into roughly 10 times larger scale (128k turns),
providing a solid foundation for evaluation. Building on this resource, we
decompose hard-to-quantify subjective performance into three progressive tasks:
TASK1 basic coherence (whether behavior, emotion, expression, and voice align
with classroom context), TASK2 student realism, and TASK3 long-term persona
consistency, thereby establishing an evaluation framework grounded in
educational theory and research value. We conduct systematic experiments on
three representative LLMs, comparing their original versions with ten
persona-fine-tuned variants trained on EduPersona. Results show consistent and
significant average improvements across all tasks: TASK1 +33.6%, TASK2 +30.6%,
and TASK3 +14.9%. These improvements highlight the dataset's effectiveness and
research value, while also revealing the heterogeneous difficulty of persona
modeling. In summary, EduPersona delivers the first classroom benchmark
centered on subjective abilities, establishes a decoupled and verifiable
research paradigm, and we will open-source both the dataset and the framework
to support the broader research community in advancing trustworthy and
human-like AI for education.

</details>


### [125] [MoME: Estimating Psychological Traits from Gait with Multi-Stage Mixture of Movement Experts](https://arxiv.org/abs/2510.04654)
*Andy Cǎtrunǎ,Adrian Cosma,Emilian Rǎdoi*

Main category: cs.CV

TL;DR: 该论文提出了一种多阶段混合运动专家（MoME）架构，用于从步态序列中预测心理属性，在PsyMo基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 步态编码了丰富的生物特征和行为信息，但利用行走方式推断心理特征仍然是一个具有挑战性且未被充分探索的问题。

Method: 采用分层多阶段混合运动专家架构，通过四个运动复杂度阶段处理行走周期，使用轻量级专家模型提取时空特征，并通过任务特定的门控模块自适应加权不同特征和阶段。

Result: 在涵盖17个心理特征的PsyMo基准测试中，该方法在运行级别达到37.47%的加权F1分数，在主题级别达到44.6%，优于现有步态分析模型。

Conclusion: 研究证明了多任务步态学习在心理特征估计中的可行性，并为未来基于运动信息的心理推断研究奠定了基础。

Abstract: Gait encodes rich biometric and behavioural information, yet leveraging the
manner of walking to infer psychological traits remains a challenging and
underexplored problem. We introduce a hierarchical Multi-Stage Mixture of
Movement Experts (MoME) architecture for multi-task prediction of psychological
attributes from gait sequences represented as 2D poses. MoME processes the
walking cycle in four stages of movement complexity, employing lightweight
expert models to extract spatio-temporal features and task-specific gating
modules to adaptively weight experts across traits and stages. Evaluated on the
PsyMo benchmark covering 17 psychological traits, our method outperforms
state-of-the-art gait analysis models, achieving a 37.47% weighted F1 score at
the run level and 44.6% at the subject level. Our experiments show that
integrating auxiliary tasks such as identity recognition, gender prediction,
and BMI estimation further improves psychological trait estimation. Our
findings demonstrate the viability of multi-task gait-based learning for
psychological trait estimation and provide a foundation for future research on
movement-informed psychological inference.

</details>


### [126] [ConceptSplit: Decoupled Multi-Concept Personalization of Diffusion Models via Token-wise Adaptation and Attention Disentanglement](https://arxiv.org/abs/2510.04668)
*Habin Lim,Yeongseob Won,Juwon Seo,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: ConceptSplit是一个解决多概念个性化文本到图像生成中概念混合问题的新框架，包含训练阶段的Token-wise Value Adaptation和推理阶段的Latent Optimization for Disentangled Attention两个关键组件。


<details>
  <summary>Details</summary>
Motivation: 多概念个性化T2I扩散模型面临的主要挑战是概念混合问题，即多个学习的概念在输出图像中产生不希望的干扰或混合。

Method: 1. Token-wise Value Adaptation (ToVA)：仅调整交叉注意力中的值投影的训练方法，避免修改键投影对注意力机制的破坏；2. Latent Optimization for Disentangled Attention (LODA)：在推理阶段通过优化输入潜变量来缓解注意力纠缠。

Result: 通过广泛的定性和定量实验证明，ConceptSplit能够实现稳健的多概念个性化，减轻意外的概念干扰。

Conclusion: ConceptSplit框架有效解决了多概念个性化中的概念混合问题，通过创新的训练和推理方法实现了更好的概念分离效果。

Abstract: In recent years, multi-concept personalization for text-to-image (T2I)
diffusion models to represent several subjects in an image has gained much more
attention. The main challenge of this task is "concept mixing", where multiple
learned concepts interfere or blend undesirably in the output image. To address
this issue, in this paper, we present ConceptSplit, a novel framework to split
the individual concepts through training and inference. Our framework comprises
two key components. First, we introduce Token-wise Value Adaptation (ToVA), a
merging-free training method that focuses exclusively on adapting the value
projection in cross-attention. Based on our empirical analysis, we found that
modifying the key projection, a common approach in existing methods, can
disrupt the attention mechanism and lead to concept mixing. Second, we propose
Latent Optimization for Disentangled Attention (LODA), which alleviates
attention entanglement during inference by optimizing the input latent. Through
extensive qualitative and quantitative experiments, we demonstrate that
ConceptSplit achieves robust multi-concept personalization, mitigating
unintended concept interference. Code is available at
https://github.com/KU-VGI/ConceptSplit

</details>


### [127] [Label-Efficient Cross-Modality Generalization for Liver Segmentation in Multi-Phase MRI](https://arxiv.org/abs/2510.04705)
*Quang-Khai Bui-Tran,Minh-Toan Dinh,Thanh-Huy Nguyen,Ba-Thinh Lam,Mai-Anh Vu,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出一种标签高效的肝脏分割方法，通过基础模型微调、交叉伪监督协同训练和标准化预处理，实现在多期相、多厂商MRI中无需空间配准的跨模态泛化。


<details>
  <summary>Details</summary>
Motivation: 多期相MRI肝脏分割对于肝纤维化评估至关重要，但现实临床环境中存在标注数据稀缺、不同成像模态和厂商系统数据分布不均、空间错位和缺失期相等挑战。

Method: 集成基础级3D分割骨干网络进行微调，采用交叉伪监督协同训练利用未标注数据，配合标准化预处理流程，无需空间配准即可实现跨模态泛化。

Result: 模型在标注和未标注领域均表现出稳健的分割性能，验证了该方法在多期相、多厂商MRI肝脏分割中的有效性。

Conclusion: 该方法展示了基础模型适应与协同训练相结合在真实世界临床影像任务中的潜力，为标签高效的肝脏分割提供了有效基线。

Abstract: Accurate liver segmentation in multi-phase MRI is vital for liver fibrosis
assessment, yet labeled data is often scarce and unevenly distributed across
imaging modalities and vendor systems. We propose a label-efficient
segmentation approach that promotes cross-modality generalization under
real-world conditions, where GED4 hepatobiliary-phase annotations are limited,
non-contrast sequences (T1WI, T2WI, DWI) are unlabeled, and spatial
misalignment and missing phases are common. Our method integrates a
foundation-scale 3D segmentation backbone adapted via fine-tuning, co-training
with cross pseudo supervision to leverage unlabeled volumes, and a standardized
preprocessing pipeline. Without requiring spatial registration, the model
learns to generalize across MRI phases and vendors, demonstrating robust
segmentation performance in both labeled and unlabeled domains. Our results
exhibit the effectiveness of our proposed label-efficient baseline for liver
segmentation in multi-phase, multi-vendor MRI and highlight the potential of
combining foundation model adaptation with co-training for real-world clinical
imaging tasks.

</details>


### [128] [ID-Consistent, Precise Expression Generation with Blendshape-Guided Diffusion](https://arxiv.org/abs/2510.04706)
*Foivos Paraperas Papantoniou,Stefanos Zafeiriou*

Main category: cs.CV

TL;DR: 本文提出了一个基于扩散模型的框架，能够忠实再现任何主题在任何特定面部表情下的图像，解决了身份一致性和表情控制之间的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持面部身份一致性方面取得了进展，但在不损害身份的前提下实现细粒度表情控制仍然具有挑战性。

Method: 基于ID一致的人脸基础模型，采用组合设计，包含由FLAME blendshape参数引导的表情交叉注意力模块，并在丰富的图像和视频数据上进行训练。

Result: 模型能够生成超越基本情绪的微妙微表情和表情过渡，在定量和定性评估中均优于现有方法。

Conclusion: 该框架在保持身份一致性的同时实现了精确的表情控制，为AI驱动的故事叙述提供了有效解决方案。

Abstract: Human-centric generative models designed for AI-driven storytelling must
bring together two core capabilities: identity consistency and precise control
over human performance. While recent diffusion-based approaches have made
significant progress in maintaining facial identity, achieving fine-grained
expression control without compromising identity remains challenging. In this
work, we present a diffusion-based framework that faithfully reimagines any
subject under any particular facial expression. Building on an ID-consistent
face foundation model, we adopt a compositional design featuring an expression
cross-attention module guided by FLAME blendshape parameters for explicit
control. Trained on a diverse mixture of image and video data rich in
expressive variation, our adapter generalizes beyond basic emotions to subtle
micro-expressions and expressive transitions, overlooked by prior works. In
addition, a pluggable Reference Adapter enables expression editing in real
images by transferring the appearance from a reference frame during synthesis.
Extensive quantitative and qualitative evaluations show that our model
outperforms existing methods in tailored and identity-consistent expression
generation. Code and models can be found at
https://github.com/foivospar/Arc2Face.

</details>


### [129] [ReactDiff: Fundamental Multiple Appropriate Facial Reaction Diffusion Model](https://arxiv.org/abs/2510.04712)
*Luo Cheng,Song Siyang,Yan Siyuan,Yu Zhen,Ge Zongyuan*

Main category: cs.CV

TL;DR: ReactDiff是一个基于时间扩散的框架，用于在对话中生成多样且真实的面部反应，通过结合时空面部运动学和面部动作单元依赖关系来提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以模拟真实人类反应的随机性和动态特性，无法生成自然流畅的面部反应。

Method: 提出ReactDiff框架，在扩散过程中引入两个关键先验：时间面部行为运动学和面部动作单元依赖关系，以约束生成过程符合人类面部解剖学特征。

Result: 在REACT2024数据集上的实验表明，该方法在反应质量、多样性和反应适切性方面均达到最先进水平。

Conclusion: ReactDiff通过引入生理约束成功解决了面部反应生成的现实性问题，为人类-计算机交互系统提供了更自然的面部反应生成方案。

Abstract: The automatic generation of diverse and human-like facial reactions in dyadic
dialogue remains a critical challenge for human-computer interaction systems.
Existing methods fail to model the stochasticity and dynamics inherent in real
human reactions. To address this, we propose ReactDiff, a novel temporal
diffusion framework for generating diverse facial reactions that are
appropriate for responding to any given dialogue context. Our key insight is
that plausible human reactions demonstrate smoothness, and coherence over time,
and conform to constraints imposed by human facial anatomy. To achieve this,
ReactDiff incorporates two vital priors (spatio-temporal facial kinematics)
into the diffusion process: i) temporal facial behavioral kinematics and ii)
facial action unit dependencies. These two constraints guide the model toward
realistic human reaction manifolds, avoiding visually unrealistic jitters,
unstable transitions, unnatural expressions, and other artifacts. Extensive
experiments on the REACT2024 dataset demonstrate that our approach not only
achieves state-of-the-art reaction quality but also excels in diversity and
reaction appropriateness.

</details>


### [130] [Object-Centric Representation Learning for Enhanced 3D Scene Graph Prediction](https://arxiv.org/abs/2510.04714)
*KunHo Heo,GiHyun Kim,SuYeon Kim,MyeongAh Cho*

Main category: cs.CV

TL;DR: 本文提出了一种新的3D语义场景图预测方法，通过设计高区分度的物体特征编码器和对比预训练策略，显著提升了物体和关系预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D语义场景图预测中过度依赖图神经网络，但物体和关系特征的表示能力不足，导致整体性能受限。研究发现物体特征质量对场景图精度至关重要。

Method: 设计高区分度的物体特征编码器，采用对比预训练策略将物体表示学习与场景图预测解耦，同时有效结合几何和语义特征进行关系预测。

Result: 在3DSSG数据集上的实验表明，该方法在所有评估指标上均显著优于现有最先进方法，将预训练编码器集成到现有框架中也能带来大幅性能提升。

Conclusion: 该方法通过改进物体特征表示和关系预测策略，有效解决了3D语义场景图预测中的关键挑战，为机器人和AR/VR应用提供了更可靠的技术支持。

Abstract: 3D Semantic Scene Graph Prediction aims to detect objects and their semantic
relationships in 3D scenes, and has emerged as a crucial technology for
robotics and AR/VR applications. While previous research has addressed dataset
limitations and explored various approaches including Open-Vocabulary settings,
they frequently fail to optimize the representational capacity of object and
relationship features, showing excessive reliance on Graph Neural Networks
despite insufficient discriminative capability. In this work, we demonstrate
through extensive analysis that the quality of object features plays a critical
role in determining overall scene graph accuracy. To address this challenge, we
design a highly discriminative object feature encoder and employ a contrastive
pretraining strategy that decouples object representation learning from the
scene graph prediction. This design not only enhances object classification
accuracy but also yields direct improvements in relationship prediction.
Notably, when plugging in our pretrained encoder into existing frameworks, we
observe substantial performance improvements across all evaluation metrics.
Additionally, whereas existing approaches have not fully exploited the
integration of relationship information, we effectively combine both geometric
and semantic features to achieve superior relationship prediction.
Comprehensive experiments on the 3DSSG dataset demonstrate that our approach
significantly outperforms previous state-of-the-art methods. Our code is
publicly available at https://github.com/VisualScienceLab-KHU/OCRL-3DSSG-Codes.

</details>


### [131] [Benchmark on Monocular Metric Depth Estimation in Wildlife Setting](https://arxiv.org/abs/2510.04723)
*Niccolò Niccoli,Lorenzo Seidenari,Ilaria Greco,Francesco Rovero*

Main category: cs.CV

TL;DR: 本文提出了首个野生动物监测条件下的单目度量深度估计基准，评估了四种先进方法在相机陷阱图像上的性能，发现Depth Anything V2表现最佳，为保护监测系统提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 相机陷阱在野生动物监测中广泛应用，但从单目图像中提取准确距离测量仍具挑战性。尽管单目深度估计方法已取得显著进展，但它们在自然野生动物环境中的性能尚未得到系统评估。

Method: 在93张带有校准ChARUCO图案获取的真实距离数据的相机陷阱图像上，评估了四种最先进的MDE方法（Depth Anything V2、ML Depth Pro、ZoeDepth和Metric3D）以及几何基线方法。

Result: Depth Anything V2实现了最佳整体性能，平均绝对误差为0.454m，相关性为0.962；而ZoeDepth在户外自然环境中的表现显著下降（MAE：3.087m）。基于中位数的深度提取在所有深度学习方法中始终优于基于均值的方法。

Conclusion: 该基准为野生动物应用建立了性能基线，并为在保护监测系统中实施深度估计提供了实用指导。Depth Anything V2在准确性和速度之间提供了最佳平衡。

Abstract: Camera traps are widely used for wildlife monitoring, but extracting accurate
distance measurements from monocular images remains challenging due to the lack
of depth information. While monocular depth estimation (MDE) methods have
advanced significantly, their performance in natural wildlife environments has
not been systematically evaluated. This work introduces the first benchmark for
monocular metric depth estimation in wildlife monitoring conditions. We
evaluate four state-of-the-art MDE methods (Depth Anything V2, ML Depth Pro,
ZoeDepth, and Metric3D) alongside a geometric baseline on 93 camera trap images
with ground truth distances obtained using calibrated ChARUCO patterns. Our
results demonstrate that Depth Anything V2 achieves the best overall
performance with a mean absolute error of 0.454m and correlation of 0.962,
while methods like ZoeDepth show significant degradation in outdoor natural
environments (MAE: 3.087m). We find that median-based depth extraction
consistently outperforms mean-based approaches across all deep learning
methods. Additionally, we analyze computational efficiency, with ZoeDepth being
fastest (0.17s per image) but least accurate, while Depth Anything V2 provides
an optimal balance of accuracy and speed (0.22s per image). This benchmark
establishes performance baselines for wildlife applications and provides
practical guidance for implementing depth estimation in conservation monitoring
systems.

</details>


### [132] [ExposureEngine: Oriented Logo Detection and Sponsor Visibility Analytics in Sports Broadcasts](https://arxiv.org/abs/2510.04739)
*Mehdi Houshmand Sarkhoosh,Frøy Øye,Henrik Nestor Sørlie,Nam Hoang Vu,Dag Johansen,Cise Midoglu,Tomas Kupka,Pål Halvorsen*

Main category: cs.CV

TL;DR: 本文提出了ExposureEngine系统，通过旋转感知的定向边界框（OBB）检测技术，实现体育转播中赞助商标识的精确可见性分析，解决了传统水平边界框（HBB）方法在旋转和透视变形情况下的不准确问题。


<details>
  <summary>Details</summary>
Motivation: 传统体育转播赞助商可见性分析依赖人工、主观且不可扩展的方法，而现有自动化系统使用水平边界框（HBB）在标识旋转或透视变形时会导致曝光指标不准确。

Method: 开发了基于定向边界框（OBB）的端到端检测系统，构建了包含1,103帧瑞典精英足球比赛图像的数据集，训练了精确的标识检测模型，并集成了语言驱动的智能分析层。

Result: 模型在mAP@0.5上达到0.859，精确度为0.96，召回率为0.87，能够准确计算曝光时长和屏幕覆盖率等可见性指标。

Conclusion: ExposureEngine系统为体育媒体赞助商测量提供了可审计、可解释的全面解决方案，显著提升了分析的准确性和自动化水平。

Abstract: Quantifying sponsor visibility in sports broadcasts is a critical marketing
task traditionally hindered by manual, subjective, and unscalable analysis
methods. While automated systems offer an alternative, their reliance on
axis-aligned Horizontal Bounding Box (HBB) leads to inaccurate exposuremetrics
when logos appear rotated or skewed due to dynamic camera angles and
perspective distortions. This paper introduces ExposureEngine, an end-to-end
system designed for accurate, rotation-aware sponsor visibility analytics in
sports broadcasts, demonstrated in a soccer case study. Our approach predicts
Oriented Bounding Box (OBB) to provide a geometrically precise fit to each logo
regardless of the orientation on-screen. To train and evaluate our detector, we
developed a new dataset comprising 1,103 frames from Swedish elite soccer,
featuring 670 unique sponsor logos annotated with OBBs. Our model achieves a
mean Average Precision (mAP@0.5) of 0.859, with a precision of 0.96 and recall
of 0.87, demonstrating robust performance in localizing logos under diverse
broadcast conditions. The system integrates these detections into an analytical
pipeline that calculates precise visibility metrics, such as exposure duration
and on-screen coverage. Furthermore, we incorporate a language-driven agentic
layer, enabling users to generate reports, summaries, and media content through
natural language queries. The complete system, including the dataset and the
analytics dashboard, provides a comprehensive solution for auditable and
interpretable sponsor measurement in sports media. An overview of the
ExposureEngine is available online: https://youtu.be/tRw6OBISuW4 .

</details>


### [133] [Anomaly-Aware YOLO: A Frugal yet Robust Approach to Infrared Small Target Detection](https://arxiv.org/abs/2510.04741)
*Alina Ciocarlan,Sylvie Le Hégarat-Mascle,Sidonie Lefebvre*

Main category: cs.CV

TL;DR: AA-YOLO将统计异常检测集成到YOLO检测头中，有效控制红外小目标检测中的误报率，在各种YOLO骨干网络上具有良好泛化性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在国防应用中面临复杂背景和小目标尺寸的挑战，传统目标检测器容易产生大量误报。

Method: 提出Anomaly-Aware YOLO (AA-YOLO)，将统计异常检测测试集成到YOLO检测头中，将小目标视为背景中的异常模式。

Result: 在多个IRSTD基准测试中取得竞争性性能，在训练数据有限、噪声和域偏移场景下表现出显著鲁棒性，可应用于各种YOLO骨干网络和实例分割YOLO。

Conclusion: AA-YOLO设计具有高度通用性，是资源受限实际部署的有吸引力的解决方案。

Abstract: Infrared Small Target Detection (IRSTD) is a challenging task in defense
applications, where complex backgrounds and tiny target sizes often result in
numerous false alarms using conventional object detectors. To overcome this
limitation, we propose Anomaly-Aware YOLO (AA-YOLO), which integrates a
statistical anomaly detection test into its detection head. By treating small
targets as unexpected patterns against the background, AA-YOLO effectively
controls the false alarm rate. Our approach not only achieves competitive
performance on several IRSTD benchmarks, but also demonstrates remarkable
robustness in scenarios with limited training data, noise, and domain shifts.
Furthermore, since only the detection head is modified, our design is highly
generic and has been successfully applied across various YOLO backbones,
including lightweight models. It also provides promising results when
integrated into an instance segmentation YOLO. This versatility makes AA-YOLO
an attractive solution for real-world deployments where resources are
constrained. The code will be publicly released.

</details>


### [134] [Beyond Appearance: Transformer-based Person Identification from Conversational Dynamics](https://arxiv.org/abs/2510.04753)
*Masoumeh Chapariniya,Teodora Vukovic,Sarah Ebling,Volker Dellwo*

Main category: cs.CV

TL;DR: 本文研究了基于Transformer架构在自然面对面对话场景中的人员识别性能，通过双流框架分别建模空间配置和时间运动模式，发现领域特定训练优于迁移学习，空间信息比时间动态更具判别性，特征级融合可达到98.03%的准确率。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer架构在自然面对面对话场景中的人员识别性能，探索空间配置和时间运动模式在人员识别中的相对重要性。

Method: 采用双流框架分别处理133个COCO WholeBody关键点的空间配置和时间运动模式，比较预训练和从头训练策略，研究速度特征的使用，并引入多尺度时间Transformer进行分层运动建模。

Result: 领域特定训练显著优于迁移学习；空间配置比时间动态携带更多判别信息（空间Transformer准确率95.74%，多尺度时间Transformer准确率93.90%）；特征级融合将性能提升至98.03%。

Conclusion: Transformer架构在自然交互中的人员识别具有巨大潜力，空间和时间信息具有互补性，为未来多模态和跨文化研究提供了重要见解。

Abstract: This paper investigates the performance of transformer-based architectures
for person identification in natural, face-to-face conversation scenario. We
implement and evaluate a two-stream framework that separately models spatial
configurations and temporal motion patterns of 133 COCO WholeBody keypoints,
extracted from a subset of the CANDOR conversational corpus. Our experiments
compare pre-trained and from-scratch training, investigate the use of velocity
features, and introduce a multi-scale temporal transformer for hierarchical
motion modeling. Results demonstrate that domain-specific training
significantly outperforms transfer learning, and that spatial configurations
carry more discriminative information than temporal dynamics. The spatial
transformer achieves 95.74% accuracy, while the multi-scale temporal
transformer achieves 93.90%. Feature-level fusion pushes performance to 98.03%,
confirming that postural and dynamic information are complementary. These
findings highlight the potential of transformer architectures for person
identification in natural interactions and provide insights for future
multimodal and cross-cultural studies.

</details>


### [135] [Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction](https://arxiv.org/abs/2510.04759)
*Chi Yan,Dan Xu*

Main category: cs.CV

TL;DR: PG-Occ是一个创新的渐进式高斯变换器框架，用于解决3D占用预测中的开放词汇查询问题，通过渐进式在线密集化和各向异性感知采样策略，在稀疏和密集表示之间取得平衡，实现了14.3%的相对mIoU提升。


<details>
  <summary>Details</summary>
Motivation: 传统3D占用预测方法局限于固定语义类别，而现有文本对齐方法存在稀疏表示难以捕捉小物体、密集表示计算开销大的权衡问题。

Method: 提出渐进式高斯变换器框架，采用渐进式在线密集化策略逐步增强3D高斯表示，并引入各向异性感知采样策略与时空融合，自适应分配不同尺度和阶段的感受野。

Result: 在广泛评估中，PG-Occ实现了最先进的性能，相比之前最佳方法相对提升了14.3%的mIoU。

Conclusion: PG-Occ框架有效解决了文本对齐场景建模中的权衡问题，能够实现更精确和详细的场景理解，为开放词汇3D占用预测提供了创新解决方案。

Abstract: The 3D occupancy prediction task has witnessed remarkable progress in recent
years, playing a crucial role in vision-based autonomous driving systems. While
traditional methods are limited to fixed semantic categories, recent approaches
have moved towards predicting text-aligned features to enable open-vocabulary
text queries in real-world scenes. However, there exists a trade-off in
text-aligned scene modeling: sparse Gaussian representation struggles to
capture small objects in the scene, while dense representation incurs
significant computational overhead. To address these limitations, we present
PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables
open-vocabulary 3D occupancy prediction. Our framework employs progressive
online densification, a feed-forward strategy that gradually enhances the 3D
Gaussian representation to capture fine-grained scene details. By iteratively
enhancing the representation, the framework achieves increasingly precise and
detailed scene understanding. Another key contribution is the introduction of
an anisotropy-aware sampling strategy with spatio-temporal fusion, which
adaptively assigns receptive fields to Gaussians at different scales and
stages, enabling more effective feature aggregation and richer scene
information capture. Through extensive evaluations, we demonstrate that PG-Occ
achieves state-of-the-art performance with a relative 14.3% mIoU improvement
over the previous best performing method. Code and pretrained models will be
released upon publication on our project page:
https://yanchi-3dv.github.io/PG-Occ

</details>


### [136] [Beyond the Seen: Bounded Distribution Estimation for Open-Vocabulary Learning](https://arxiv.org/abs/2510.04770)
*Xiaomeng Fan,Yuchuan Mao,Zhi Gao,Yuwei Wu,Jin Chen,Yunde Jia*

Main category: cs.CV

TL;DR: 本文提出了一种新的开放词汇学习方法，通过生成未见类数据来估计开放环境中的分布，从而有效解决现有方法因未见类数据缺失导致的估计误差问题。


<details>
  <summary>Details</summary>
Motivation: 开放词汇学习需要在开放环境中建模数据分布，但现有方法仅使用已见类数据进行估计，由于未见类的缺失导致估计误差无法识别。为了有效估计分布并约束估计误差，需要学习超越已见类的知识。

Method: 提出了一种包含类域级数据生成流程和分布对齐算法的开放词汇学习方法。数据生成流程在层次语义树和从已见类数据推断的域信息指导下生成未见类数据，分布对齐算法利用生成数据估计并最大化后验概率以增强泛化能力。

Result: 在11个数据集上的广泛实验表明，该方法比基线方法性能提升高达14%，证明了其有效性和优越性。

Conclusion: 理论证明和实验结果表明，通过生成未见类数据可以有效估计开放环境中的分布，并约束估计误差，为开放词汇学习提供了新的有效解决方案。

Abstract: Open-vocabulary learning requires modeling the data distribution in open
environments, which consists of both seen-class and unseen-class data.
  Existing methods estimate the distribution in open environments using
seen-class data, where the absence of unseen classes makes the estimation error
inherently unidentifiable.
  Intuitively, learning beyond the seen classes is crucial for distribution
estimation to bound the estimation error.
  We theoretically demonstrate that the distribution can be effectively
estimated by generating unseen-class data, through which the estimation error
is upper-bounded.
  Building on this theoretical insight, we propose a novel open-vocabulary
learning method, which generates unseen-class data for estimating the
distribution in open environments. The method consists of a class-domain-wise
data generation pipeline and a distribution alignment algorithm. The data
generation pipeline generates unseen-class data under the guidance of a
hierarchical semantic tree and domain information inferred from the seen-class
data, facilitating accurate distribution estimation. With the generated data,
the distribution alignment algorithm estimates and maximizes the posterior
probability to enhance generalization in open-vocabulary learning. Extensive
experiments on $11$ datasets demonstrate that our method outperforms baseline
approaches by up to $14\%$, highlighting its effectiveness and superiority.

</details>


### [137] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战赛旨在评估联邦学习在手术视频分类中的泛化能力和适应性，发现ViViT模型表现最佳，但存在泛化能力有限、类别不平衡敏感等问题。


<details>
  <summary>Details</summary>
Motivation: 建立联邦学习在手术视频分类领域的首个基准测试，评估方法在未见临床中心的泛化能力以及通过本地微调的适应性，同时实现不共享患者数据的协作模型开发。

Method: 使用多中心Appendix300视频数据集，参与者开发了基础模型线性探测、度量学习（三元组损失）和多种FL聚合方案（FedAvg、FedMedian、FedSAM），评估泛化任务和适应性任务。

Result: 泛化任务中跨中心性能有限，适应性任务中所有团队通过微调都有改进但排名稳定性低。ViViT模型表现最强，时空建模和上下文感知预处理显示出潜力。

Conclusion: 该挑战赛建立了手术视频分类中FL策略的首个基准，强调了本地个性化与全局鲁棒性之间的权衡，以及架构选择、预处理和损失设计的重要性。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art
in federated learning for surgical video classification. Its goal was to assess
how well current methods generalize to unseen clinical centers and adapt
through local fine-tuning while enabling collaborative model development
without sharing patient data. Methods: Participants developed strategies to
classify inflammation stages in appendicitis using a preliminary version of the
multi-center Appendix300 video dataset. The challenge evaluated two tasks:
generalization to an unseen center and center-specific adaptation after
fine-tuning. Submitted approaches included foundation models with linear
probing, metric learning with triplet loss, and various FL aggregation schemes
(FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and
Expected Cost, with ranking robustness evaluated via bootstrapping and
statistical testing. Results: In the generalization task, performance across
centers was limited. In the adaptation task, all teams improved after
fine-tuning, though ranking stability was low. The ViViT-based submission
achieved the strongest overall performance. The challenge highlighted
limitations in generalization, sensitivity to class imbalance, and difficulties
in hyperparameter tuning in decentralized training, while spatiotemporal
modeling and context-aware preprocessing emerged as promising strategies.
Conclusion: The FedSurg Challenge establishes the first benchmark for
evaluating FL strategies in surgical video classification. Findings highlight
the trade-off between local personalization and global robustness, and
underscore the importance of architecture choice, preprocessing, and loss
design. This benchmarking offers a reference point for future development of
imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [138] [Hands-Free Heritage: Automated 3D Scanning for Cultural Heritage Digitization](https://arxiv.org/abs/2510.04781)
*Javed Ahmad,Federico Dassiè,Selene Frascella,Gabriele Marchello,Ferdinando Cannella,Arianna Traviglia*

Main category: cs.CV

TL;DR: 提出了一种自动化双机器人3D扫描系统，用于文化遗产保护，通过协调的机器人操作和高分辨率3D扫描取代传统手动或半自动工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统3D扫描方法需要专业知识和手动干预来维持最佳扫描条件和覆盖范围，限制了文化遗产保护工作的效率和可及性。

Method: 系统将扫描空间参数化为不同区域，通过扫描机器人（配备扫描仪）和托盘处理机器人之间的协调运动规划，优化轨迹规划和路径点分布以确保全面表面覆盖。

Result: 实验结果显示，该方法在Chamfer距离和F-score指标上显著优于基线方法，实现了更高的几何精度和数字化效率。

Conclusion: 该自动化双机器人系统能够有效减少对专家操作员的依赖，提高文化遗产保护的数字化效率和质量。

Abstract: High-fidelity 3D scanning is essential for preserving cultural heritage
artefacts, supporting documentation, analysis, and long-term conservation.
However, conventional methods typically require specialized expertise and
manual intervention to maintain optimal scanning conditions and coverage. We
present an automated two-robot scanning system that eliminates the need for
handheld or semi-automatic workflows by combining coordinated robotic
manipulation with high-resolution 3D scanning. Our system parameterizes the
scanning space into distinct regions, enabling coordinated motion planning
between a scanner-equipped robot and a tray-handling robot. Optimized
trajectory planning and waypoint distribution ensure comprehensive surface
coverage, minimize occlusions, and balance reconstruction accuracy with system
efficiency. Experimental results show that our approach achieves significantly
lower Chamfer Distance and higher F-score compared to baseline methods,
offering superior geometric accuracy, improved digitization efficiency, and
reduced reliance on expert operators.

</details>


### [139] [A Comparative Study of Vision Transformers and CNNs for Few-Shot Rigid Transformation and Fundamental Matrix Estimation](https://arxiv.org/abs/2510.04794)
*Alon Kaya,Igal Bilik,Inna Stainvas*

Main category: cs.CV

TL;DR: 本文比较了Vision Transformers和大型卷积神经网络在几何估计任务中的表现，发现在大数据场景下ViTs优于CNNs，但在小数据场景下CNNs的归纳偏置使其表现更好。ViTs在跨域评估中展现出更强的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究预训练视觉模型在几何估计任务（如图像刚性变换和基础矩阵估计）中的效率，特别是在低数据量情况下的表现。这些任务需要平衡局部和全局特征，挑战了直接将分类或对比学习预训练模型作为骨干网络的适用性。

Method: 系统比较了大型CNN（ResNet、EfficientNet、CLIP-ResNet）和ViT基础模型（CLIP-ViT变体和DINO）在不同数据量设置下的表现，包括少样本场景。通过实证分析评估这些预训练模型在几何估计任务中的性能。

Result: 在大数据下游任务中，ViTs在精调时优于CNNs；在小数据场景下，CNNs的归纳偏置和较小容量使其表现更好，能够与ViT匹敌；ViTs在跨域评估中展现出更强的泛化能力。

Conclusion: 精调时需要仔细选择模型架构，未来研究应致力于开发能够平衡局部和全局表示的混合架构。

Abstract: Vision-transformers (ViTs) and large-scale convolution-neural-networks (CNNs)
have reshaped computer vision through pretrained feature representations that
enable strong transfer learning for diverse tasks. However, their efficiency as
backbone architectures for geometric estimation tasks involving image
deformations in low-data regimes remains an open question. This work considers
two such tasks: 1) estimating 2D rigid transformations between pairs of images
and 2) predicting the fundamental matrix for stereo image pairs, an important
problem in various applications, such as autonomous mobility, robotics, and 3D
scene reconstruction. Addressing this intriguing question, this work
systematically compares large-scale CNNs (ResNet, EfficientNet, CLIP-ResNet)
with ViT-based foundation models (CLIP-ViT variants and DINO) in various data
size settings, including few-shot scenarios. These pretrained models are
optimized for classification or contrastive learning, encouraging them to focus
mostly on high-level semantics. The considered tasks require balancing local
and global features differently, challenging the straightforward adoption of
these models as the backbone. Empirical comparative analysis shows that,
similar to training from scratch, ViTs outperform CNNs during refinement in
large downstream-data scenarios. However, in small data scenarios, the
inductive bias and smaller capacity of CNNs improve their performance, allowing
them to match that of a ViT. Moreover, ViTs exhibit stronger generalization in
cross-domain evaluation where the data distribution changes. These results
emphasize the importance of carefully selecting model architectures for
refinement, motivating future research towards hybrid architectures that
balance local and global representations.

</details>


### [140] [DiT-VTON: Diffusion Transformer Framework for Unified Multi-Category Virtual Try-On and Virtual Try-All with Integrated Image Editing](https://arxiv.org/abs/2510.04797)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: DiT-VTON是一个基于扩散Transformer的虚拟试穿框架，通过多种DiT配置和数据扩展，实现了跨品类虚拟试穿和高级图像编辑功能，在细节保留和鲁棒性方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术面临细节保留不足、对真实图像鲁棒性差、采样效率低、编辑功能有限以及跨品类泛化能力弱等问题。

Method: 采用扩散Transformer架构，探索了上下文标记拼接、通道拼接和ControlNet集成等多种图像条件配置，在扩展数据集上训练模型以增强鲁棒性。

Result: 在VITON-HD数据集上超越现有方法，实现了更好的细节保留和鲁棒性，无需额外条件编码器；在包含数千品类的多样化数据集上表现出色。

Conclusion: DiT-VTON重新定义了虚拟试穿任务，提供了通用的"虚拟试穿一切"解决方案，支持姿态保持、局部编辑、纹理迁移和对象级定制等高级功能。

Abstract: The rapid growth of e-commerce has intensified the demand for Virtual Try-On
(VTO) technologies, enabling customers to realistically visualize products
overlaid on their own images. Despite recent advances, existing VTO models face
challenges with fine-grained detail preservation, robustness to real-world
imagery, efficient sampling, image editing capabilities, and generalization
across diverse product categories. In this paper, we present DiT-VTON, a novel
VTO framework that leverages a Diffusion Transformer (DiT), renowned for its
performance on text-conditioned image generation, adapted here for the
image-conditioned VTO task. We systematically explore multiple DiT
configurations, including in-context token concatenation, channel
concatenation, and ControlNet integration, to determine the best setup for VTO
image conditioning.
  To enhance robustness, we train the model on an expanded dataset encompassing
varied backgrounds, unstructured references, and non-garment categories,
demonstrating the benefits of data scaling for VTO adaptability. DiT-VTON also
redefines the VTO task beyond garment try-on, offering a versatile Virtual
Try-All (VTA) solution capable of handling a wide range of product categories
and supporting advanced image editing functionalities such as pose
preservation, localized editing, texture transfer, and object-level
customization. Experimental results show that our model surpasses
state-of-the-art methods on VITON-HD, achieving superior detail preservation
and robustness without reliance on additional condition encoders. It also
outperforms models with VTA and image editing capabilities on a diverse dataset
spanning thousands of product categories.

</details>


### [141] [Did you just see that? Arbitrary view synthesis for egocentric replay of operating room workflows from ambient sensors](https://arxiv.org/abs/2510.04802)
*Han Zhang,Lalithkumar Seenivasan,Jose L. Porras,Roger D. Soberanis-Mukul,Hao Ding,Hongchao Shu,Benjamin D. Killeen,Ankita Ghosh,Lonny Yarmus,Masaru Ishii,Angela Christine Argento,Mathias Unberath*

Main category: cs.CV

TL;DR: EgoSurg是一个从固定摄像头视频重建手术室人员动态第一人称视角的框架，通过几何驱动神经渲染和扩散增强技术实现高保真度视角合成。


<details>
  <summary>Details</summary>
Motivation: 传统手术观察方法依赖固定视角或回忆，无法记录指导临床决策的第一人称视觉视角，限制了手术安全、培训和流程优化的分析。

Method: 结合几何驱动的神经渲染和基于扩散的视角增强技术，从壁挂式固定摄像头视频重建任意时刻的任意第一人称视角。

Result: 在多站点手术案例和对照研究中，EgoSurg能够以高视觉质量和保真度重建特定人员的视觉场和任意视角。

Conclusion: EgoSurg将现有手术室摄像头基础设施转化为可导航的动态3D记录，为沉浸式手术数据科学奠定新基础，使手术实践可以从各个角度可视化、体验和分析。

Abstract: Observing surgical practice has historically relied on fixed vantage points
or recollections, leaving the egocentric visual perspectives that guide
clinical decisions undocumented. Fixed-camera video can capture surgical
workflows at the room-scale, but cannot reconstruct what each team member
actually saw. Thus, these videos only provide limited insights into how
decisions that affect surgical safety, training, and workflow optimization are
made. Here we introduce EgoSurg, the first framework to reconstruct the
dynamic, egocentric replays for any operating room (OR) staff directly from
wall-mounted fixed-camera video, and thus, without intervention to clinical
workflow. EgoSurg couples geometry-driven neural rendering with diffusion-based
view enhancement, enabling high-visual fidelity synthesis of arbitrary and
egocentric viewpoints at any moment. In evaluation across multi-site surgical
cases and controlled studies, EgoSurg reconstructs person-specific visual
fields and arbitrary viewpoints with high visual quality and fidelity. By
transforming existing OR camera infrastructure into a navigable dynamic 3D
record, EgoSurg establishes a new foundation for immersive surgical data
science, enabling surgical practice to be visualized, experienced, and analyzed
from every angle.

</details>


### [142] [Visual Representations inside the Language Model](https://arxiv.org/abs/2510.04819)
*Benlin Liu,Amita Kamath,Madeleine Grunde-McLaughlin,Winson Han,Ranjay Krishna*

Main category: cs.CV

TL;DR: 该论文研究了多模态语言模型（MLMs）在感知任务上表现不佳的原因，通过分析视觉键值令牌的信息流动，发现语言模型未能充分利用视觉信息，并提出改进方法。


<details>
  <summary>Details</summary>
Motivation: 理解为什么多模态语言模型在感知密集型任务上表现不佳，特别是视觉键值令牌在信息处理中的作用。

Method: 通过分析LLaVA-OneVision、Qwen2.5-VL和Llama-3-LLaVA-NeXT等流行MLMs，研究视觉信息在语言模型中的流动，包括分割、语义对应、时间对应和参考表达检测等任务。

Result: 发现图像值令牌包含足够信息支持零样本感知任务，但语言模型对视觉信息的利用不足，且在后期层中存在降低感知能力的伪影。添加文本前缀可以改善视觉表示。

Conclusion: 揭示了键值令牌在多模态系统中的作用，为MLMs的机制解释提供了新见解，并提出了改进视觉编码器和语言模型组件训练的新方向。

Abstract: Despite interpretability work analyzing VIT encoders and transformer
activations, we don't yet understand why Multimodal Language Models (MLMs)
struggle on perception-heavy tasks. We offer an under-studied perspective by
examining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and
Llama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the
flow of visual information through the language model, finding that image value
tokens encode sufficient information to perform several perception-heavy tasks
zero-shot: segmentation, semantic correspondence, temporal correspondence, and
referring expression detection. We find that while the language model does
augment the visual information received from the projection of input visual
encodings-which we reveal correlates with overall MLM perception capability-it
contains less visual information on several tasks than the equivalent visual
encoder (SigLIP) that has not undergone MLM finetuning. Further, we find that
the visual information corresponding to input-agnostic image key tokens in
later layers of language models contains artifacts which reduce perception
capability of the overall MLM. Next, we discuss controlling visual information
in the language model, showing that adding a text prefix to the image input
improves perception capabilities of visual representations. Finally, we reveal
that if language models were able to better control their visual information,
their perception would significantly improve; e.g., in 33.3% of Art Style
questions in the BLINK benchmark, perception information present in the
language model is not surfaced to the output! Our findings reveal insights into
the role of key-value tokens in multimodal systems, paving the way for deeper
mechanistic interpretability of MLMs and suggesting new directions for training
their visual encoder and language model components.

</details>


### [143] [AvatarVTON: 4D Virtual Try-On for Animatable Avatars](https://arxiv.org/abs/2510.04822)
*Zicheng Jiang,Jixin Gao,Shengfeng He,Xinzhe Li,Yulong Zheng,Zhaotong Yang,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: AvatarVTON是首个4D虚拟试穿框架，能从单张服装图像生成逼真的试穿效果，支持自由姿态控制、新视角渲染和多样化服装选择。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多视角服装捕捉或物理先验，无法在单视角监督下实现动态服装交互。AvatarVTON旨在解决这一限制，推动虚拟试穿技术的发展。

Method: 框架包含两个关键模块：1）互逆流校正器，一种无先验的光流校正策略，稳定虚拟形象拟合并确保时间一致性；2）非线性变形器，将高斯图分解为视角姿态不变和视角姿态特定分量，实现自适应非线性服装变形。

Result: 通过扩展现有基线并建立统一模块进行公平比较，实验表明AvatarVTON在保真度、多样性和动态服装真实感方面表现出色。

Conclusion: AvatarVTON的高质量效果使其适用于AR/VR、游戏和数字人应用，为4D虚拟试穿建立了新基准。

Abstract: We propose AvatarVTON, the first 4D virtual try-on framework that generates
realistic try-on results from a single in-shop garment image, enabling free
pose control, novel-view rendering, and diverse garment choices. Unlike
existing methods, AvatarVTON supports dynamic garment interactions under
single-view supervision, without relying on multi-view garment captures or
physics priors. The framework consists of two key modules: (1) a Reciprocal
Flow Rectifier, a prior-free optical-flow correction strategy that stabilizes
avatar fitting and ensures temporal coherence; and (2) a Non-Linear Deformer,
which decomposes Gaussian maps into view-pose-invariant and view-pose-specific
components, enabling adaptive, non-linear garment deformations. To establish a
benchmark for 4D virtual try-on, we extend existing baselines with unified
modules for fair qualitative and quantitative comparisons. Extensive
experiments show that AvatarVTON achieves high fidelity, diversity, and dynamic
garment realism, making it well-suited for AR/VR, gaming, and digital-human
applications.

</details>


### [144] [Flow Matching for Conditional MRI-CT and CBCT-CT Image Synthesis](https://arxiv.org/abs/2510.04823)
*Arnela Hadzic,Simon Johannes Joham,Martin Urschler*

Main category: cs.CV

TL;DR: 该论文采用完全3D流匹配框架从MRI或CBCT生成合成CT，用于改善放疗精度并减少患者辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 通过生成合成CT来支持MRI-only和CBCT-based自适应放疗，提高治疗精度同时减少患者辐射暴露。

Method: 使用完全3D流匹配框架，将高斯噪声体积通过学习的流匹配速度场转换为sCT图像，该场基于轻量级3D编码器从输入MRI或CBCT提取的特征进行条件化。

Result: 在SynthRAD2025挑战基准上评估，结果显示方法能准确重建全局解剖结构，但由于训练分辨率较低，细部细节保留有限。

Conclusion: 未来工作将探索基于patch的训练和潜在空间流模型，以提高分辨率和局部结构保真度。

Abstract: Generating synthetic CT (sCT) from MRI or CBCT plays a crucial role in
enabling MRI-only and CBCT-based adaptive radiotherapy, improving treatment
precision while reducing patient radiation exposure. To address this task, we
adopt a fully 3D Flow Matching (FM) framework, motivated by recent work
demonstrating FM's efficiency in producing high-quality images. In our
approach, a Gaussian noise volume is transformed into an sCT image by
integrating a learned FM velocity field, conditioned on features extracted from
the input MRI or CBCT using a lightweight 3D encoder. We evaluated the method
on the SynthRAD2025 Challenge benchmark, training separate models for MRI
$\rightarrow$ sCT and CBCT $\rightarrow$ sCT across three anatomical regions:
abdomen, head and neck, and thorax. Validation and testing were performed
through the challenge submission system. The results indicate that the method
accurately reconstructs global anatomical structures; however, preservation of
fine details was limited, primarily due to the relatively low training
resolution imposed by memory and runtime constraints. Future work will explore
patch-based training and latent-space flow models to improve resolution and
local structural fidelity.

</details>


### [145] [Beyond Random: Automatic Inner-loop Optimization in Dataset Distillation](https://arxiv.org/abs/2510.04838)
*Muquan Li,Hang Gou,Dongyang Zhang,Shuang Liang,Xiurui Xie,Deqiang Ouyang,Ke Qin*

Main category: cs.CV

TL;DR: 提出了AT-BPTT框架，通过动态调整截断位置和窗口大小来解决数据集蒸馏中随机截断策略的局限性，在多个数据集上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有数据集蒸馏方法依赖随机截断策略，缺乏灵活性且效果不佳。研究发现神经网络在不同训练阶段具有不同的学习动态，随机截断策略无法适应这种动态变化

Method: AT-BPTT框架包含三个关键组件：基于概率的阶段感知时间步选择机制、基于梯度变化的自适应窗口大小策略、以及低秩Hessian近似来降低计算开销

Result: 在CIFAR-10、CIFAR-100、Tiny-ImageNet和ImageNet-1K上平均准确率提升6.16%，内循环优化加速3.9倍，节省63%内存成本

Conclusion: AT-BPTT通过动态适应神经网络的学习动态，显著提升了数据集蒸馏的性能和效率，为高效深度学习提供了有效解决方案

Abstract: The growing demand for efficient deep learning has positioned dataset
distillation as a pivotal technique for compressing training dataset while
preserving model performance. However, existing inner-loop optimization methods
for dataset distillation typically rely on random truncation strategies, which
lack flexibility and often yield suboptimal results. In this work, we observe
that neural networks exhibit distinct learning dynamics across different
training stages-early, middle, and late-making random truncation ineffective.
To address this limitation, we propose Automatic Truncated Backpropagation
Through Time (AT-BPTT), a novel framework that dynamically adapts both
truncation positions and window sizes according to intrinsic gradient behavior.
AT-BPTT introduces three key components: (1) a probabilistic mechanism for
stage-aware timestep selection, (2) an adaptive window sizing strategy based on
gradient variation, and (3) a low-rank Hessian approximation to reduce
computational overhead. Extensive experiments on CIFAR-10, CIFAR-100,
Tiny-ImageNet, and ImageNet-1K show that AT-BPTT achieves state-of-the-art
performance, improving accuracy by an average of 6.16% over baseline methods.
Moreover, our approach accelerates inner-loop optimization by 3.9x while saving
63% memory cost.

</details>


### [146] [Detailed Aerial Mapping of Photovoltaic Power Plants Through Semantically Significant Keypoints](https://arxiv.org/abs/2510.04840)
*Viktor Kozák,Jan Chudoba,Libor Přeučil*

Main category: cs.CV

TL;DR: 提出了一种基于航空图像的光伏电站自动映射方法，能够实现从组件级别到整体的详细建模


<details>
  <summary>Details</summary>
Motivation: 光伏电站的准确模型对于优化运维至关重要，但现有模型获取困难且依赖第三方数据

Method: 利用航空图像进行视觉分割，识别光伏组件和结构布局，通过视觉关键点融合多图像检测结果，保持结构完整性

Result: 在两个不同电站上进行了实验验证，成功构建了包含3D位置和语义结构的紧凑地理参考模型

Conclusion: 该方法能够自动化光伏电站映射过程，消除对第三方数据的依赖，为电站维护提供有效支持

Abstract: An accurate and up-to-date model of a photovoltaic (PV) power plant is
essential for its optimal operation and maintenance. However, such a model may
not be easily available. This work introduces a novel approach for PV power
plant mapping based on aerial overview images. It enables the automation of the
mapping process while removing the reliance on third-party data. The presented
mapping method takes advantage of the structural layout of the power plants to
achieve detailed modeling down to the level of individual PV modules. The
approach relies on visual segmentation of PV modules in overview images and the
inference of structural information in each image, assigning modules to
individual benches, rows, and columns. We identify visual keypoints related to
the layout and use these to merge detections from multiple images while
maintaining their structural integrity. The presented method was experimentally
verified and evaluated on two different power plants. The final fusion of 3D
positions and semantic structures results in a compact georeferenced model
suitable for power plant maintenance.

</details>


### [147] [From Actions to Kinesics: Extracting Human Psychological States through Bodily Movements](https://arxiv.org/abs/2510.04844)
*Cheyu Lin,Katherine A. Flanigan*

Main category: cs.CV

TL;DR: 提出了一种基于3D骨骼关节数据的运动学识别框架，通过结合时空图卷积网络和卷积神经网络，利用迁移学习来推断人类活动的交际功能，从而建模人类心理状态与建筑环境之间的动态关系。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖理论模型或问卷调查，存在范围有限、静态且劳动密集的问题，无法在保护隐私的同时捕捉人类心理状态。需要一种既通用又保护隐私的方法来建模人类与建筑环境的互动。

Method: 使用时空图卷积网络（ST-GCN）和卷积神经网络（CNN）的组合框架，通过迁移学习避免手动定义物理动作与心理类别之间的映射关系，直接从3D骨骼关节数据推断运动学功能。

Result: 在Dyadic User EngagemenT（DUET）数据集上的结果表明，该方法能够实现可扩展、准确且以人为中心的行为建模。

Conclusion: 该方法为增强强化学习驱动的人类-环境互动模拟提供了新途径，能够在保护用户匿名性的同时揭示反映认知和情感状态的身体运动潜在结构。

Abstract: Understanding the dynamic relationship between humans and the built
environment is a key challenge in disciplines ranging from environmental
psychology to reinforcement learning (RL). A central obstacle in modeling these
interactions is the inability to capture human psychological states in a way
that is both generalizable and privacy preserving. Traditional methods rely on
theoretical models or questionnaires, which are limited in scope, static, and
labor intensive. We present a kinesics recognition framework that infers the
communicative functions of human activity -- known as kinesics -- directly from
3D skeleton joint data. Combining a spatial-temporal graph convolutional
network (ST-GCN) with a convolutional neural network (CNN), the framework
leverages transfer learning to bypass the need for manually defined mappings
between physical actions and psychological categories. The approach preserves
user anonymity while uncovering latent structures in bodily movements that
reflect cognitive and emotional states. Our results on the Dyadic User
EngagemenT (DUET) dataset demonstrate that this method enables scalable,
accurate, and human-centered modeling of behavior, offering a new pathway for
enhancing RL-driven simulations of human-environment interaction.

</details>


### [148] [Read the Room: Inferring Social Context Through Dyadic Interaction Recognition in Cyber-physical-social Infrastructure Systems](https://arxiv.org/abs/2510.04854)
*Cheyu Lin,John Martins,Katherine A. Flanigan,Ph. D*

Main category: cs.CV

TL;DR: 该论文比较了五种基于骨架的交互识别算法，用于识别12种双人交互类型，旨在解决传统CPS系统忽视社会效益的问题，提出了隐私保护的深度传感器替代方案。


<details>
  <summary>Details</summary>
Motivation: 传统网络物理系统(CPS)主要关注经济目标如性能和安全，但忽视了人类中心的社会效益。网络物理社会基础设施系统(CPSIS)旨在通过将CPS与社会目标对齐来解决这一问题，需要开发隐私保护的人类交互测量方法。

Method: 研究比较了五种基于骨架的交互识别算法，使用深度传感器分析骨骼运动来识别12种双人交互类型。这些交互按沟通类型分类，如象征性动作和情感表达，避免了RGB摄像头的隐私问题。

Result: 研究在12种双人交互数据集上评估了五种骨架识别算法的性能，为理解人类互动的文化情感层面提供了基础。

Conclusion: 基于骨架的交互识别方法为隐私保护的社会行为测量提供了可行方案，为CPSIS系统实现社会效益目标奠定了基础，特别是通过分析双人互动来理解人类互动的深层含义。

Abstract: Cyber-physical systems (CPS) integrate sensing, computing, and control to
improve infrastructure performance, focusing on economic goals like performance
and safety. However, they often neglect potential human-centered (or
''social'') benefits. Cyber-physical-social infrastructure systems (CPSIS) aim
to address this by aligning CPS with social objectives. This involves defining
social benefits, understanding human interactions with each other and
infrastructure, developing privacy-preserving measurement methods, modeling
these interactions for prediction, linking them to social benefits, and
actuating the physical environment to foster positive social outcomes. This
paper delves into recognizing dyadic human interactions using real-world data,
which is the backbone to measuring social behavior. This lays a foundation to
address the need to enhance understanding of the deeper meanings and mutual
responses inherent in human interactions. While RGB cameras are informative for
interaction recognition, privacy concerns arise. Depth sensors offer a
privacy-conscious alternative by analyzing skeletal movements. This study
compares five skeleton-based interaction recognition algorithms on a dataset of
12 dyadic interactions. Unlike single-person datasets, these interactions,
categorized into communication types like emblems and affect displays, offer
insights into the cultural and emotional aspects of human interactions.

</details>


### [149] [ERDE: Entropy-Regularized Distillation for Early-exit](https://arxiv.org/abs/2510.04856)
*Martial Guidez,Stefan Duffner,Yannick Alpou,Oscar Röth,Christophe Garcia*

Main category: cs.CV

TL;DR: 该论文提出了一种结合早期退出和知识蒸馏的神经网络压缩方法，通过引入新的基于熵的损失函数来优化准确性和效率之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然性能优异，但计算成本高，难以应用于实时和边缘场景。现有压缩技术需要在保持精度的同时降低计算复杂度，动态架构能够根据资源情况调整压缩级别。

Method: 将早期退出和知识蒸馏两种技术结合，训练一个简化的学生早期退出模型，使用更复杂的教师早期退出模型进行指导。创新点在于对教师分类错误的图像引入基于熵的新损失函数。

Result: 在CIFAR10、CIFAR100和SVHN图像分类数据集上的实验验证了该方法的有效性，显著降低了计算复杂度而不影响分类性能。

Conclusion: 该方法为神经网络压缩提供了新的解决方案，并为知识蒸馏在其他场景中的应用开辟了新的研究视角。

Abstract: Although deep neural networks and in particular Convolutional Neural Networks
have demonstrated state-of-the-art performance in image classification with
relatively high efficiency, they still exhibit high computational costs, often
rendering them impractical for real-time and edge applications. Therefore, a
multitude of compression techniques have been developed to reduce these costs
while maintaining accuracy. In addition, dynamic architectures have been
introduced to modulate the level of compression at execution time, which is a
desirable property in many resource-limited application scenarios. The proposed
method effectively integrates two well-established optimization techniques:
early exits and knowledge distillation, where a reduced student early-exit
model is trained from a more complex teacher early-exit model. The primary
contribution of this research lies in the approach for training the student
early-exit model. In comparison to the conventional Knowledge Distillation
loss, our approach incorporates a new entropy-based loss for images where the
teacher's classification was incorrect. The proposed method optimizes the
trade-off between accuracy and efficiency, thereby achieving significant
reductions in computational complexity without compromising classification
performance. The validity of this approach is substantiated by experimental
results on image classification datasets CIFAR10, CIFAR100 and SVHN, which
further opens new research perspectives for Knowledge Distillation in other
contexts.

</details>


### [150] [μDeepIQA: deep learning-based fast and robust image quality assessment with local predictions for optical microscopy](https://arxiv.org/abs/2510.04859)
*Elena Corbetta,Thomas Bocklitz*

Main category: cs.CV

TL;DR: μDeepIQA是一种基于深度学习的图像质量评估方法，专门针对光学显微镜图像，能够快速稳定地预测图像质量，并支持局部质量可视化


<details>
  <summary>Details</summary>
Motivation: 传统图像质量评估方法在处理大规模数据集时耗时且计算成本高，同时对超出理想域的图像表现不稳定。深度学习方法的优势在于提供更好的性能、更强的泛化能力和快速预测

Method: 采用为自然图像设计的深度卷积神经网络架构，重新训练以预测光学显微镜数据的个体质量指标和全局质量分数。该方法提供图像块的局部质量预测和空间质量可视化

Result: μDeepIQA在光学显微镜图像上表现出稳定的质量预测性能，即使在标准方法理想范围之外也能泛化质量估计，同时能够快速处理图像并提供局部质量分析

Conclusion: 深度学习模型由于其面对异常值时的稳定性能、评估小图像块的能力以及快速预测的优势，使光学显微镜研究能够从深度学习模型的泛化能力中受益

Abstract: Optical microscopy is one of the most widely used techniques in research
studies for life sciences and biomedicine. These applications require reliable
experimental pipelines to extract valuable knowledge from the measured samples
and must be supported by image quality assessment (IQA) to ensure correct
processing and analysis of the image data. IQA methods are implemented with
variable complexity. However, while most quality metrics have a straightforward
implementation, they might be time consuming and computationally expensive when
evaluating a large dataset. In addition, quality metrics are often designed for
well-defined image features and may be unstable for images out of the ideal
domain.
  To overcome these limitations, recent works have proposed deep learning-based
IQA methods, which can provide superior performance, increased generalizability
and fast prediction. Our method, named $\mathrm{\mu}$DeepIQA, is inspired by
previous studies and applies a deep convolutional neural network designed for
IQA on natural images to optical microscopy measurements. We retrained the same
architecture to predict individual quality metrics and global quality scores
for optical microscopy data. The resulting models provide fast and stable
predictions of image quality by generalizing quality estimation even outside
the ideal range of standard methods. In addition, $\mathrm{\mu}$DeepIQA
provides patch-wise prediction of image quality and can be used to visualize
spatially varying quality in a single image. Our study demonstrates that
optical microscopy-based studies can benefit from the generalizability of deep
learning models due to their stable performance in the presence of outliers,
the ability to assess small image patches, and rapid predictions.

</details>


### [151] [In-Field Mapping of Grape Yield and Quality with Illumination-Invariant Deep Learning](https://arxiv.org/abs/2510.04864)
*Ciem Cornelissen,Sander De Coninck,Axel Willekens,Sam Leroux,Pieter Simoens*

Main category: cs.CV

TL;DR: 提出了一种端到端的物联网机器人系统，用于无损、实时、空间分辨的葡萄产量和质量（糖度、酸度）测绘。系统包含葡萄串检测与重量估计模块，以及基于光照不变光谱自编码器（LISA）的质量评估框架，能够克服光照变化带来的域偏移问题。


<details>
  <summary>Details</summary>
Motivation: 解决葡萄园中传统破坏性采样方法的局限性，实现非破坏性、实时的产量和质量监测，为精准葡萄栽培提供数据驱动的决策支持。

Method: 系统整合了两个关键模块：高性能的葡萄串检测与重量估计模型，以及基于LISA（一种域对抗框架）的深度学习质量评估框架，能够从未经校准的高光谱数据中学习光照不变特征。

Result: 在包含实验室人工光照、早晨和下午自然光照的三个不同光照域的数据集上验证，系统在葡萄串检测上达到0.82的召回率，重量预测R²为0.76，LISA模块相比基线将质量预测泛化能力提高了20%以上。

Conclusion: 该系统成功生成了高分辨率、地理参考的葡萄产量和质量数据，为精准葡萄栽培提供了可操作的数据驱动见解。

Abstract: This paper presents an end-to-end, IoT-enabled robotic system for the
non-destructive, real-time, and spatially-resolved mapping of grape yield and
quality (Brix, Acidity) in vineyards. The system features a comprehensive
analytical pipeline that integrates two key modules: a high-performance model
for grape bunch detection and weight estimation, and a novel deep learning
framework for quality assessment from hyperspectral (HSI) data. A critical
barrier to in-field HSI is the ``domain shift" caused by variable illumination.
To overcome this, our quality assessment is powered by the Light-Invariant
Spectral Autoencoder (LISA), a domain-adversarial framework that learns
illumination-invariant features from uncalibrated data. We validated the
system's robustness on a purpose-built HSI dataset spanning three distinct
illumination domains: controlled artificial lighting (lab), and variable
natural sunlight captured in the morning and afternoon. Results show the
complete pipeline achieves a recall (0.82) for bunch detection and a $R^2$
(0.76) for weight prediction, while the LISA module improves quality prediction
generalization by over 20% compared to the baselines. By combining these robust
modules, the system successfully generates high-resolution, georeferenced data
of both grape yield and quality, providing actionable, data-driven insights for
precision viticulture.

</details>


### [152] [BenthiCat: An opti-acoustic dataset for advancing benthic classification and habitat mapping](https://arxiv.org/abs/2510.04876)
*Hayat Rajani,Valerio Franchi,Borja Martinez-Clavel Valles,Raimon Ramos,Rafael Garcia,Nuno Gracias*

Main category: cs.CV

TL;DR: 本文介绍了一个多模态海底栖息地测绘数据集，包含约100万个侧扫声纳图块、测深图和光学图像，其中36000个图块带有手动分割标注，旨在为机器学习模型提供标准化基准。


<details>
  <summary>Details</summary>
Motivation: 海底栖息地测绘对于理解海洋生态系统至关重要，但当前缺乏大规模标注数据集限制了机器学习模型的发展和基准测试。

Method: 收集加泰罗尼亚海岸的侧扫声纳数据，结合测深图和AUV采集的光学图像，手动标注36000个分割掩码，开发多传感器数据融合方法实现跨模态表示学习。

Result: 发布了包含原始传感器数据、镶嵌图和预处理工具的综合数据集，建立了海底栖息地测绘的标准化基准。

Conclusion: 该资源将促进自主海底分类和多传感器集成技术的进步，为水下栖息地测绘研究提供重要支持。

Abstract: Benthic habitat mapping is fundamental for understanding marine ecosystems,
guiding conservation efforts, and supporting sustainable resource management.
Yet, the scarcity of large, annotated datasets limits the development and
benchmarking of machine learning models in this domain. This paper introduces a
thorough multi-modal dataset, comprising about a million side-scan sonar (SSS)
tiles collected along the coast of Catalonia (Spain), complemented by
bathymetric maps and a set of co-registered optical images from targeted
surveys using an autonomous underwater vehicle (AUV). Approximately \num{36000}
of the SSS tiles have been manually annotated with segmentation masks to enable
supervised fine-tuning of classification models. All the raw sensor data,
together with mosaics, are also released to support further exploration and
algorithm development. To address challenges in multi-sensor data fusion for
AUVs, we spatially associate optical images with corresponding SSS tiles,
facilitating self-supervised, cross-modal representation learning. Accompanying
open-source preprocessing and annotation tools are provided to enhance
accessibility and encourage research. This resource aims to establish a
standardized benchmark for underwater habitat mapping, promoting advancements
in autonomous seafloor classification and multi-sensor integration.

</details>


### [153] [Comparative Analysis of YOLOv5, Faster R-CNN, SSD, and RetinaNet for Motorbike Detection in Kigali Autonomous Driving Context](https://arxiv.org/abs/2510.04912)
*Ngeyen Yinkfu,Sunday Nwovu,Jonathan Kayizzi,Angelique Uwamahoro*

Main category: cs.CV

TL;DR: 本研究比较了YOLOv5、Faster R-CNN、SSD和RetinaNet四种目标检测模型在卢旺达基加利摩托车检测任务中的性能，旨在为资源受限环境下的自动驾驶系统提供实时导航解决方案。


<details>
  <summary>Details</summary>
Motivation: 基加利的摩托车出租车作为主要交通工具，经常不遵守交通规则且行驶不可预测，给自动驾驶系统带来重大挑战。需要开发适合发展中国家资源受限环境的实时检测模型。

Method: 使用在基加利收集的198张图像组成自定义数据集，在PyTorch框架下采用迁移学习实现四种目标检测模型（YOLOv5、Faster R-CNN、SSD、RetinaNet），评估其准确性、定位能力和推理速度。

Result: 研究识别了数据集限制和模型复杂性等实施挑战，并评估了各模型在实时导航应用中的适用性。

Conclusion: 建议未来工作采用简化架构，以提高发展中国家（如卢旺达）自动驾驶系统的可访问性。

Abstract: In Kigali, Rwanda, motorcycle taxis are a primary mode of transportation,
often navigating unpredictably and disregarding traffic rules, posing
significant challenges for autonomous driving systems. This study compares four
object detection models--YOLOv5, Faster R-CNN, SSD, and RetinaNet--for
motorbike detection using a custom dataset of 198 images collected in Kigali.
Implemented in PyTorch with transfer learning, the models were evaluated for
accuracy, localization, and inference speed to assess their suitability for
real-time navigation in resource-constrained settings. We identify
implementation challenges, including dataset limitations and model
complexities, and recommend simplified architectures for future work to enhance
accessibility for autonomous systems in developing countries like Rwanda.

</details>


### [154] [A Semantics-Aware Hierarchical Self-Supervised Approach to Classification of Remote Sensing Images](https://arxiv.org/abs/2510.04916)
*Giulio Weikmann,Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 提出了一种新颖的语义感知层次共识方法，通过整合层次特定的分类头来学习层次特征和关系，解决了遥感图像分类中层次结构被忽视的问题。


<details>
  <summary>Details</summary>
Motivation: 遥感图像分类任务通常包含预定义的标签层次结构，但现有方法大多忽略这些层次关系，只关注细粒度分类方案。

Method: SAHC方法在深度网络架构中整合了层次特定的分类头，每个分类头专门处理不同粒度的类别。使用可训练的层次矩阵以自监督方式学习层次结构，并引入层次共识机制确保不同层次级别概率分布的一致性。

Result: 在三个具有不同层次复杂度的基准数据集上评估，使用不同骨干架构，实验结果表明该方法在指导网络学习和层次共识方面具有有效性和鲁棒性。

Conclusion: 所提出的SAHC方法能够有效利用层次分类任务的固有结构，在遥感图像分类任务中表现出良好的适应性和性能。

Abstract: Deep learning has become increasingly important in remote sensing image
classification due to its ability to extract semantic information from complex
data. Classification tasks often include predefined label hierarchies that
represent the semantic relationships among classes. However, these hierarchies
are frequently overlooked, and most approaches focus only on fine-grained
classification schemes. In this paper, we present a novel Semantics-Aware
Hierarchical Consensus (SAHC) method for learning hierarchical features and
relationships by integrating hierarchy-specific classification heads within a
deep network architecture, each specialized in different degrees of class
granularity. The proposed approach employs trainable hierarchy matrices, which
guide the network through the learning of the hierarchical structure in a
self-supervised manner. Furthermore, we introduce a hierarchical consensus
mechanism to ensure consistent probability distributions across different
hierarchical levels. This mechanism acts as a weighted ensemble being able to
effectively leverage the inherent structure of the hierarchical classification
task. The proposed SAHC method is evaluated on three benchmark datasets with
different degrees of hierarchical complexity on different tasks, using distinct
backbone architectures to effectively emphasize its adaptability. Experimental
results show both the effectiveness of the proposed approach in guiding network
learning and the robustness of the hierarchical consensus for remote sensing
image classification tasks.

</details>


### [155] [REN: Anatomically-Informed Mixture-of-Experts for Interstitial Lung Disease Diagnosis](https://arxiv.org/abs/2510.04923)
*Alec K. Peltekian,Halil Ertugrul Aktas,Gorkem Durak,Kevin Grudzinski,Bradford C. Bemiss,Carrie Richardson,Jane E. Dematte,G. R. Scott Budinger,Anthony J. Esposito,Alexander Misharin,Alok Choudhary,Ankit Agrawal,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出了REN框架，首个针对医学影像的解剖学引导混合专家网络，通过区域专家网络和放射组学引导的门控机制，在间质性肺病分类中显著提升性能


<details>
  <summary>Details</summary>
Motivation: 传统MoE架构缺乏医学影像所需的解剖学约束，而肺部解剖结构和区域疾病异质性对病理模式有重要影响，需要专门针对医学影像的解剖学引导框架

Method: 使用解剖学先验训练7个专门专家网络，分别处理不同肺叶和双侧肺组合；采用多模态门控机制整合放射组学特征和深度学习特征来优化专家权重分配

Result: REN在ILD分类中平均AUC达到0.8646，比SwinUNETR基线提升12.5%；下叶区域专家AUC达0.88-0.90，显著优于传统DL方法

Conclusion: REN展示了强大的泛化能力和临床可解释性，为结构化医学影像应用提供了可扩展的解剖学引导方法

Abstract: Mixture-of-Experts (MoE) architectures have significantly contributed to
scalable machine learning by enabling specialized subnetworks to tackle complex
tasks efficiently. However, traditional MoE systems lack domain-specific
constraints essential for medical imaging, where anatomical structure and
regional disease heterogeneity strongly influence pathological patterns. Here,
we introduce Regional Expert Networks (REN), the first anatomically-informed
MoE framework tailored specifically for medical image classification. REN
leverages anatomical priors to train seven specialized experts, each dedicated
to distinct lung lobes and bilateral lung combinations, enabling precise
modeling of region-specific pathological variations. Multi-modal gating
mechanisms dynamically integrate radiomics biomarkers and deep learning (DL)
features (CNN, ViT, Mamba) to weight expert contributions optimally. Applied to
interstitial lung disease (ILD) classification, REN achieves consistently
superior performance: the radiomics-guided ensemble reached an average AUC of
0.8646 +/- 0.0467, a +12.5 percent improvement over the SwinUNETR baseline (AUC
0.7685, p = 0.031). Region-specific experts further revealed that lower-lobe
models achieved AUCs of 0.88-0.90, surpassing DL counterparts (CNN: 0.76-0.79)
and aligning with known disease progression patterns. Through rigorous
patient-level cross-validation, REN demonstrates strong generalizability and
clinical interpretability, presenting a scalable, anatomically-guided approach
readily extensible to other structured medical imaging applications.

</details>


### [156] [Unsupervised Active Learning via Natural Feature Progressive Framework](https://arxiv.org/abs/2510.04939)
*Yuxi Liu,Catherine Lalman,Yimin Yang*

Main category: cs.CV

TL;DR: 本文提出了NFPF框架，通过SFLM量化样本重要性并利用重构差异进行样本选择，显著提升了无监督主动学习的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无监督主动学习方法依赖局部梯度评分，容易受噪声干扰且无法充分代表数据分布，性能不及监督方法。

Method: 提出NFPF框架，使用SFLM量化样本对模型性能的贡献度，并定义重构差异指标进行初始样本选择。

Result: NFPF在视觉数据集上显著超越现有无监督方法，性能与监督方法相当，具有更好的鲁棒性和数据分布覆盖。

Conclusion: NFPF通过新的样本重要性度量方法，为无监督主动学习提供了更有效的解决方案。

Abstract: The effectiveness of modern deep learning models is predicated on the
availability of large-scale, human-annotated datasets, a process that is
notoriously expensive and time-consuming. While Active Learning (AL) offers a
strategic solution by labeling only the most informative and representative
data, its iterative nature still necessitates significant human involvement.
Unsupervised Active Learning (UAL) presents an alternative by shifting the
annotation burden to a single, post-selection step. Unfortunately, prevailing
UAL methods struggle to achieve state-of-the-art performance. These approaches
typically rely on local, gradient-based scoring for sample importance
estimation, which not only makes them vulnerable to ambiguous and noisy data
but also hinders their capacity to select samples that adequately represent the
full data distribution. Moreover, their use of shallow, one-shot linear
selection falls short of a true UAL paradigm. In this paper, we propose the
Natural Feature Progressive Framework (NFPF), a UAL method that revolutionizes
how sample importance is measured. At its core, NFPF employs a Specific Feature
Learning Machine (SFLM) to effectively quantify each sample's contribution to
model performance. We further utilize the SFLM to define a powerful
Reconstruction Difference metric for initial sample selection. Our
comprehensive experiments show that NFPF significantly outperforms all
established UAL methods and achieves performance on par with supervised AL
methods on vision datasets. Detailed ablation studies and qualitative
visualizations provide compelling evidence for NFPF's superior performance,
enhanced robustness, and improved data distribution coverage.

</details>


### [157] [Bidirectional Mammogram View Translation with Column-Aware and Implicit 3D Conditional Diffusion](https://arxiv.org/abs/2510.04947)
*Xin Li,Kaixiang Yang,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出CA3D-Diff方法解决乳腺X光双视图转换问题，通过列感知注意力机制和隐式3D重建来提升跨视图生成质量


<details>
  <summary>Details</summary>
Motivation: 实际临床中乳腺X光检查可能缺失或损坏某个视图，影响诊断效果。由于乳腺组织的非刚性变形和组织重叠，视图间转换极具挑战性

Method: 基于条件扩散模型，设计列感知交叉注意力机制利用解剖位置对应关系，并引入隐式3D结构重建模块增强解剖一致性

Result: 实验表明CA3D-Diff在双向视图转换任务中优于现有方法，生成视图在视觉质量和结构一致性上表现优异

Conclusion: 该方法能有效提升单视图恶性分类性能，在真实诊断场景中具有实用价值

Abstract: Dual-view mammography, including craniocaudal (CC) and mediolateral oblique
(MLO) projections, offers complementary anatomical views crucial for breast
cancer diagnosis. However, in real-world clinical workflows, one view may be
missing, corrupted, or degraded due to acquisition errors or compression
artifacts, limiting the effectiveness of downstream analysis. View-to-view
translation can help recover missing views and improve lesion alignment. Unlike
natural images, this task in mammography is highly challenging due to large
non-rigid deformations and severe tissue overlap in X-ray projections, which
obscure pixel-level correspondences. In this paper, we propose Column-Aware and
Implicit 3D Diffusion (CA3D-Diff), a novel bidirectional mammogram view
translation framework based on conditional diffusion model. To address
cross-view structural misalignment, we first design a column-aware
cross-attention mechanism that leverages the geometric property that
anatomically corresponding regions tend to lie in similar column positions
across views. A Gaussian-decayed bias is applied to emphasize local column-wise
correlations while suppressing distant mismatches. Furthermore, we introduce an
implicit 3D structure reconstruction module that back-projects noisy 2D latents
into a coarse 3D feature volume based on breast-view projection geometry. The
reconstructed 3D structure is refined and injected into the denoising UNet to
guide cross-view generation with enhanced anatomical awareness. Extensive
experiments demonstrate that CA3D-Diff achieves superior performance in
bidirectional tasks, outperforming state-of-the-art methods in visual fidelity
and structural consistency. Furthermore, the synthesized views effectively
improve single-view malignancy classification in screening settings,
demonstrating the practical value of our method in real-world diagnostics.

</details>


### [158] [SSDD: Single-Step Diffusion Decoder for Efficient Image Tokenization](https://arxiv.org/abs/2510.04961)
*Théophane Vallaeys,Jakob Verbeek,Matthieu Cord*

Main category: cs.CV

TL;DR: SSDD是一种新的像素扩散解码器架构，通过蒸馏技术实现单步重建，在无对抗训练的情况下超越了KL-VAE的性能，提供更高的重建质量和更快的采样速度。


<details>
  <summary>Details</summary>
Motivation: 当前基于KL-VAE的tokenizer虽然性能优异，但扩散解码器需要对抗损失和迭代采样，导致训练复杂且速度慢。需要一种既能保持高质量又能快速采样的替代方案。

Method: 提出新的像素扩散解码器架构，利用transformer组件和无GAN训练提升扩展性和稳定性。通过蒸馏技术将扩散解码器性能复制到高效的单步解码器中。

Result: SSDD将重建FID从0.87提升到0.50，吞吐量提高1.4倍，DiT生成质量保持不变的同时采样速度加快3.8倍。

Conclusion: SSDD可作为KL-VAE的直接替代品，用于构建更高质量和更快的生成模型。

Abstract: Tokenizers are a key component of state-of-the-art generative image models,
extracting the most important features from the signal while reducing data
dimension and redundancy. Most current tokenizers are based on KL-regularized
variational autoencoders (KL-VAE), trained with reconstruction, perceptual and
adversarial losses. Diffusion decoders have been proposed as a more principled
alternative to model the distribution over images conditioned on the latent.
However, matching the performance of KL-VAE still requires adversarial losses,
as well as a higher decoding time due to iterative sampling. To address these
limitations, we introduce a new pixel diffusion decoder architecture for
improved scaling and training stability, benefiting from transformer components
and GAN-free training. We use distillation to replicate the performance of the
diffusion decoder in an efficient single-step decoder. This makes SSDD the
first diffusion decoder optimized for single-step reconstruction trained
without adversarial losses, reaching higher reconstruction quality and faster
sampling than KL-VAE. In particular, SSDD improves reconstruction FID from
$0.87$ to $0.50$ with $1.4\times$ higher throughput and preserve generation
quality of DiTs with $3.8\times$ faster sampling. As such, SSDD can be used as
a drop-in replacement for KL-VAE, and for building higher-quality and faster
generative models.

</details>


### [159] [ActiveMark: on watermarking of visual foundation models via massive activations](https://arxiv.org/abs/2510.04966)
*Anna Chistyakova,Mikhail Pautov*

Main category: cs.CV

TL;DR: 提出一种视觉基础模型所有权验证方法，通过微调模型层和编码器-解码器网络在内部表示中嵌入数字水印，确保水印在模型微调后仍可检测


<details>
  <summary>Details</summary>
Motivation: 保护视觉基础模型知识产权，防止非法重新分发，需要可靠的所有权验证工具来区分受保护模型的副本和独立模型

Method: 微调视觉基础模型的小部分表达层和一个小型编码器-解码器网络，将数字水印嵌入到保留输入图像的内部表示中

Result: 理论和实验证明该方法具有低概率的误检测（非水印模型被错误检测）和低概率的漏检测（水印模型未被检测）

Conclusion: 该方法为视觉基础模型提供了有效的所有权验证机制，水印在模型功能副本中保持可检测性

Abstract: Being trained on large and vast datasets, visual foundation models (VFMs) can
be fine-tuned for diverse downstream tasks, achieving remarkable performance
and efficiency in various computer vision applications. The high computation
cost of data collection and training motivates the owners of some VFMs to
distribute them alongside the license to protect their intellectual property
rights. However, a dishonest user of the protected model's copy may illegally
redistribute it, for example, to make a profit. As a consequence, the
development of reliable ownership verification tools is of great importance
today, since such methods can be used to differentiate between a redistributed
copy of the protected model and an independent model. In this paper, we propose
an approach to ownership verification of visual foundation models by
fine-tuning a small set of expressive layers of a VFM along with a small
encoder-decoder network to embed digital watermarks into an internal
representation of a hold-out set of input images. Importantly, the watermarks
embedded remain detectable in the functional copies of the protected model,
obtained, for example, by fine-tuning the VFM for a particular downstream task.
Theoretically and experimentally, we demonstrate that the proposed method
yields a low probability of false detection of a non-watermarked model and a
low probability of false misdetection of a watermarked model.

</details>


### [160] [Latent Uncertainty Representations for Video-based Driver Action and Intention Recognition](https://arxiv.org/abs/2510.05006)
*Koen Vellenga,H. Joe Steinhauer,Jonas Andersson,Anders Sjögren*

Main category: cs.CV

TL;DR: 该论文提出了一种新的潜在不确定性表示方法（LUR和RLUR），用于深度神经网络的不确定性估计和分布外检测，在资源受限的安全关键任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在资源受限的安全关键任务中应用日益广泛，但现有的最后一层概率深度学习方法在分布外检测性能上存在差异，需要更高效的不确定性估计方法。

Method: 通过在预训练DNN上添加变换层来生成多个潜在表示以估计不确定性，提出了LUR和排斥训练的RLUR方法。

Result: 在四个视频驾驶行为识别数据集上的实验表明，LUR和RLUR在分类性能上与其他方法相当，在分布外检测方面与最佳方法匹配，且训练更高效、调参更容易。

Conclusion: LUR方法提供了一种高效且易于调参的不确定性估计方案，特别适用于资源受限环境中的安全关键应用。

Abstract: Deep neural networks (DNNs) are increasingly applied to safety-critical tasks
in resource-constrained environments, such as video-based driver action and
intention recognition. While last layer probabilistic deep learning (LL-PDL)
methods can detect out-of-distribution (OOD) instances, their performance
varies. As an alternative to last layer approaches, we propose extending
pre-trained DNNs with transformation layers to produce multiple latent
representations to estimate the uncertainty. We evaluate our latent uncertainty
representation (LUR) and repulsively trained LUR (RLUR) approaches against
eight PDL methods across four video-based driver action and intention
recognition datasets, comparing classification performance, calibration, and
uncertainty-based OOD detection. We also contribute 28,000 frame-level action
labels and 1,194 video-level intention labels for the NuScenes dataset. Our
results show that LUR and RLUR achieve comparable in-distribution
classification performance to other LL-PDL approaches. For uncertainty-based
OOD detection, LUR matches top-performing PDL methods while being more
efficient to train and easier to tune than approaches that require Markov-Chain
Monte Carlo sampling or repulsive training procedures.

</details>


### [161] [Exploring the Efficacy of Modified Transfer Learning in Identifying Parkinson's Disease Through Drawn Image Patterns](https://arxiv.org/abs/2510.05015)
*Nabil Daiyan,Md Rakibul Haque*

Main category: cs.CV

TL;DR: 该研究提出了一种基于机器学习的帕金森病早期诊断方法，使用手绘螺旋和波浪图像作为生物标志物，通过CNN、迁移学习和注意力机制达到93.3%的整体准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病早期诊断对预防不良影响至关重要，但传统诊断方法繁琐且昂贵。需要开发非侵入性、成本效益高的解决方案。

Method: 使用卷积神经网络、迁移学习和注意力机制，结合数据增强技术。采用三阶段架构：预训练CNN、自定义卷积层和集成投票。通过硬投票聚合多个模型的预测。

Result: 螺旋图像精确率、召回率和F1分数为90%，波浪图像为96.67%。通过集成硬投票后整体准确率达到93.3%。

Conclusion: 机器学习在帕金森病早期诊断中具有巨大潜力，能够提供非侵入性且成本效益高的解决方案来改善患者预后。

Abstract: Parkinson's disease (PD) is a progressive neurodegenerative condition
characterized by the death of dopaminergic neurons, leading to various movement
disorder symptoms. Early diagnosis of PD is crucial to prevent adverse effects,
yet traditional diagnostic methods are often cumbersome and costly. In this
study, a machine learning-based approach is proposed using hand-drawn spiral
and wave images as potential biomarkers for PD detection. Our methodology
leverages convolutional neural networks (CNNs), transfer learning, and
attention mechanisms to improve model performance and resilience against
overfitting. To enhance the diversity and richness of both spiral and wave
categories, the training dataset undergoes augmentation to increase the number
of images. The proposed architecture comprises three phases: utilizing
pre-trained CNNs, incorporating custom convolutional layers, and ensemble
voting. Employing hard voting further enhances performance by aggregating
predictions from multiple models. Experimental results show promising accuracy
rates. For spiral images, weighted average precision, recall, and F1-score are
90%, and for wave images, they are 96.67%. After combining the predictions
through ensemble hard voting, the overall accuracy is 93.3%. These findings
underscore the potential of machine learning in early PD diagnosis, offering a
non-invasive and cost-effective solution to improve patient outcomes.

</details>


### [162] [Video-LMM Post-Training: A Deep Dive into Video Reasoning with Large Multimodal Models](https://arxiv.org/abs/2510.05034)
*Yunlong Tang,Jing Bi,Pinxin Liu,Zhenyu Pan,Zhangyun Tan,Qianxiang Shen,Jiani Liu,Hang Hua,Junjia Guo,Yunzhong Xiao,Chao Huang,Zhiyuan Wang,Susan Liang,Xinyi Liu,Yizhi Song,Yuhe Nie,Jia-Xing Zhong,Bozheng Li,Daiqing Qi,Ziyun Zeng,Ali Vosoughi,Luchuan Song,Zeliang Zhang,Daiki Shimada,Han Liu,Jiebo Luo,Chenliang Xu*

Main category: cs.CV

TL;DR: 这篇论文是关于视频大语言模型（Video-LMMs）后训练方法的首次全面调查，重点分析了监督微调、强化学习和测试时扩展三大支柱技术。


<details>
  <summary>Details</summary>
Motivation: 视频理解是计算机视觉中最具挑战性的前沿领域，需要模型能够处理复杂的时空关系和长期依赖。虽然Video-LMMs在视频理解任务中表现出色，但其后训练阶段的研究仍然分散且缺乏系统性。

Method: 论文提出了一个结构化分类法，涵盖三大后训练方法：带思维链的监督微调（SFT）、基于可验证目标的强化学习（RL）以及通过增强推理计算的测试时扩展（TTS）。

Result: 通过系统分析代表性方法，论文综合了关键设计原则、见解和评估协议，并识别了奖励设计、可扩展性和成本性能优化等关键开放挑战。

Conclusion: 该调查为研究人员和从业者提供了一个统一的框架，用于推进Video-LMM的能力，并整理了必要的基准、数据集和指标以促进严格评估。

Abstract: Video understanding represents the most challenging frontier in computer
vision, requiring models to reason about complex spatiotemporal relationships,
long-term dependencies, and multimodal evidence. The recent emergence of
Video-Large Multimodal Models (Video-LMMs), which integrate visual encoders
with powerful decoder-based language models, has demonstrated remarkable
capabilities in video understanding tasks. However, the critical phase that
transforms these models from basic perception systems into sophisticated
reasoning engines, post-training, remains fragmented across the literature.
This survey provides the first comprehensive examination of post-training
methodologies for Video-LMMs, encompassing three fundamental pillars:
supervised fine-tuning (SFT) with chain-of-thought, reinforcement learning (RL)
from verifiable objectives, and test-time scaling (TTS) through enhanced
inference computation. We present a structured taxonomy that clarifies the
roles, interconnections, and video-specific adaptations of these techniques,
addressing unique challenges such as temporal localization, spatiotemporal
grounding, long video efficiency, and multimodal evidence integration. Through
systematic analysis of representative methods, we synthesize key design
principles, insights, and evaluation protocols while identifying critical open
challenges in reward design, scalability, and cost-performance optimization. We
further curate essential benchmarks, datasets, and metrics to facilitate
rigorous assessment of post-training effectiveness. This survey aims to provide
researchers and practitioners with a unified framework for advancing Video-LMM
capabilities. Additional resources and updates are maintained at:
https://github.com/yunlong10/Awesome-Video-LMM-Post-Training

</details>


### [163] [SegMASt3R: Geometry Grounded Segment Matching](https://arxiv.org/abs/2510.05051)
*Rohit Jayanti,Swayam Agrawal,Vansh Garg,Siddharth Tourani,Muhammad Haris Khan,Sourav Garg,Madhava Krishna*

Main category: cs.CV

TL;DR: 本文提出了一种利用3D基础模型的空间理解能力来解决宽基线分割匹配问题的新方法，在极端视角变化下实现分割匹配，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 分割匹配是计算机视觉中的重要中间任务，相比关键点匹配更能捕捉结构化区域，对遮挡、光照变化和视角变化具有更强鲁棒性。但宽基线分割匹配（涉及极端视角变化）仍然是一个挑战性问题。

Method: 提出了一种利用3D基础模型归纳偏置的架构，能够匹配视角变化高达180度的图像对中的分割区域。

Result: 在ScanNet++和Replica数据集上的实验表明，该方法在AUPRC指标上优于最先进方法（包括SAM2视频传播器和局部特征匹配方法）高达30%。

Conclusion: 该方法在3D实例分割和图像目标导航等相关下游任务中也表现出优势，证明了其在复杂视觉任务中的实用性。

Abstract: Segment matching is an important intermediate task in computer vision that
establishes correspondences between semantically or geometrically coherent
regions across images. Unlike keypoint matching, which focuses on localized
features, segment matching captures structured regions, offering greater
robustness to occlusions, lighting variations, and viewpoint changes. In this
paper, we leverage the spatial understanding of 3D foundation models to tackle
wide-baseline segment matching, a challenging setting involving extreme
viewpoint shifts. We propose an architecture that uses the inductive bias of
these 3D foundation models to match segments across image pairs with up to 180
degree view-point change. Extensive experiments show that our approach
outperforms state-of-the-art methods, including the SAM2 video propagator and
local feature matching methods, by upto 30% on the AUPRC metric, on ScanNet++
and Replica datasets. We further demonstrate benefits of the proposed model on
relevant downstream tasks, including 3D instance segmentation and image-goal
navigation. Project Page: https://segmast3r.github.io/

</details>


### [164] [No-reference Quality Assessment of Contrast-distorted Images using Contrast-enhanced Pseudo Reference](https://arxiv.org/abs/2510.05053)
*Mohammad-Ali Mahmoudpour,Saeed Mahmoudpour*

Main category: cs.CV

TL;DR: 提出了一种针对对比度失真图像的无参考图像质量评估（NR-IQA）方法，通过生成伪参考图像将NR问题转化为全参考（FR）评估，提高了评估准确性。


<details>
  <summary>Details</summary>
Motivation: 对比度变化是影响图像质量的重要因素，但在图像质量评估中常被忽视。现有方法主要关注模糊和噪声等失真，而对比度失真的视觉影响和特性与传统失真类型不同。

Method: 使用一组对比度增强算法生成视觉上接近实际参考图像的伪参考图像，训练分类网络根据图像内容和失真选择最合适的对比度增强算法，然后在FR模式下评估对比度增强图像与退化图像之间的质量差异。

Result: 在三个包含对比度失真的数据库（CCID2014、TID2013和CSIQ）上的性能评估表明，该方法具有有前景的性能。

Conclusion: 该方法成功地将NR-IQA问题转化为FR评估，通过伪参考图像生成提高了对比度失真图像质量评估的准确性。

Abstract: Contrast change is an important factor that affects the quality of images.
During image capturing, unfavorable lighting conditions can cause contrast
change and visual quality loss. While various methods have been proposed to
assess the quality of images under different distortions such as blur and
noise, contrast distortion has been largely overlooked as its visual impact and
properties are different from other conventional types of distortions. In this
paper, we propose a no-reference image quality assessment (NR-IQA) metric for
contrast-distorted images. Using a set of contrast enhancement algorithms, we
aim to generate pseudo-reference images that are visually close to the actual
reference image, such that the NR problem is transformed to a Full-reference
(FR) assessment with higher accuracy. To this end, a large dataset of
contrast-enhanced images is produced to train a classification network that can
select the most suitable contrast enhancement algorithm based on image content
and distortion for pseudo-reference image generation. Finally, the evaluation
is performed in the FR manner to assess the quality difference between the
contrast-enhanced (pseudoreference) and degraded images. Performance evaluation
of the proposed method on three databases containing contrast distortions
(CCID2014, TID2013, and CSIQ), indicates the promising performance of the
proposed method.

</details>


### [165] [Neuroplastic Modular Framework: Cross-Domain Image Classification of Garbage and Industrial Surfaces](https://arxiv.org/abs/2510.05071)
*Debojyoti Ghosh,Soumya K Ghosh,Adrijit Goswami*

Main category: cs.CV

TL;DR: 本文提出了一种名为神经可塑性模块分类器的新型混合架构，用于动态环境中的鲁棒自适应图像分类，结合了ResNet-50、Vision Transformer和FAISS相似性检索技术，在垃圾分类和工业缺陷检测任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 高效准确的废物分类和工业表面缺陷检测对于可持续废物管理和质量控制至关重要，需要能够适应动态环境的鲁棒分类模型。

Method: 模型采用混合架构：ResNet-50用于局部特征提取，Vision Transformer捕获全局语义上下文，FAISS相似性检索提供记忆参考。核心创新是神经可塑性模块设计，包含可扩展的学习块，在训练性能停滞时动态增长。

Result: 在KolektorSDD2工业缺陷数据集和垃圾分类任务上的实验结果表明，该架构在准确性和适应性方面均优于传统静态模型。

Conclusion: 神经可塑性模块分类器为现实世界图像分类提供了可扩展的高性能解决方案，在环境和工业领域具有强适用性。

Abstract: Efficient and accurate classification of waste and industrial surface defects
is essential for ensuring sustainable waste management and maintaining high
standards in quality control. This paper introduces the Neuroplastic Modular
Classifier, a novel hybrid architecture designed for robust and adaptive image
classification in dynamic environments. The model combines a ResNet-50 backbone
for localized feature extraction with a Vision Transformer (ViT) to capture
global semantic context. Additionally, FAISS-based similarity retrieval is
incorporated to provide a memory-like reference to previously encountered data,
enriching the model's feature space. A key innovation of our architecture is
the neuroplastic modular design composed of expandable, learnable blocks that
dynamically grow during training when performance plateaus. Inspired by
biological learning systems, this mechanism allows the model to adapt to data
complexity over time, improving generalization. Beyond garbage classification,
we validate the model on the Kolektor Surface Defect Dataset 2 (KolektorSDD2),
which involves industrial defect detection on metal surfaces. Experimental
results across domains show that the proposed architecture outperforms
traditional static models in both accuracy and adaptability. The Neuroplastic
Modular Classifier offers a scalable, high-performance solution for real-world
image classification, with strong applicability in both environmental and
industrial domains.

</details>


### [166] [Factuality Matters: When Image Generation and Editing Meet Structured Visuals](https://arxiv.org/abs/2510.05091)
*Le Zhuo,Songhao Han,Yuandong Pu,Boxiang Qiu,Sayak Paul,Yue Liao,Yihao Liu,Jie Shao,Xi Chen,Si Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 该论文提出了针对结构化视觉内容（如图表、图表和数学图形）生成和编辑的统一方法，包括大规模数据集构建、模型训练和评估基准。


<details>
  <summary>Details</summary>
Motivation: 现代视觉生成模型在创建美观的自然图像方面表现出色，但在生成或编辑需要构图规划、文本渲染和多模态推理的结构化视觉内容时表现不佳。

Method: 构建了130万高质量结构化图像对数据集，训练了一个集成VLM和FLUX.1 Kontext的统一模型，采用三阶段训练课程，并在推理时使用外部推理器增强性能。

Result: 评估了15个模型，发现即使是领先的闭源系统也远不能令人满意。作者模型在编辑任务上表现强劲，推理时推理在不同架构上都带来了一致的性能提升。

Conclusion: 通过发布数据集、模型和基准，旨在推进结构化视觉内容的统一多模态基础。

Abstract: While modern visual generation models excel at creating aesthetically
pleasing natural images, they struggle with producing or editing structured
visuals like charts, diagrams, and mathematical figures, which demand
composition planning, text rendering, and multimodal reasoning for factual
fidelity. To address this, we present the first comprehensive, systematic
investigation of this domain, encompassing data construction, model training,
and an evaluation benchmark. First, we construct a large-scale dataset of 1.3
million high-quality structured image pairs derived from executable drawing
programs and augmented with chain-of-thought reasoning annotations. Building on
it, we train a unified model that integrates a VLM with FLUX.1 Kontext via a
lightweight connector for enhanced multimodal understanding. A three-stage
training curriculum enables progressive feature alignment, knowledge infusion,
and reasoning-augmented generation, further boosted by an external reasoner at
inference time. Finally, we introduce StructBench, a novel benchmark for
generation and editing with over 1,700 challenging instances, and an
accompanying evaluation metric, StructScore, which employs a multi-round Q\&A
protocol to assess fine-grained factual accuracy. Evaluations of 15 models
reveal that even leading closed-source systems remain far from satisfactory.
Our model attains strong editing performance, and inference-time reasoning
yields consistent gains across diverse architectures. By releasing the dataset,
model, and benchmark, we aim to advance unified multimodal foundations for
structured visuals.

</details>


### [167] [Character Mixing for Video Generation](https://arxiv.org/abs/2510.05093)
*Tingting Liao,Chongjian Ge,Guangyi Liu,Hao Li,Yi Zhou*

Main category: cs.CV

TL;DR: 提出一个框架解决跨角色视频生成中的身份保持和风格一致性问题，通过交叉角色嵌入和增强技术实现不同世界角色的自然互动。


<details>
  <summary>Details</summary>
Motivation: 研究如何在文本到视频生成中让来自不同世界的角色自然互动，同时保持各自的身份特征和行为逻辑，避免风格混淆问题。

Method: 采用交叉角色嵌入（CCE）学习跨模态源的身份和行为逻辑，以及交叉角色增强（CCA）通过合成共存和混合风格数据丰富训练。

Result: 在包含10个角色的卡通和真人剧集基准测试中，在身份保持、互动质量和风格一致性方面取得明显改进。

Conclusion: 该框架能够实现先前不共存角色之间的自然互动，同时保持风格保真度，为生成式故事讲述开辟了新形式。

Abstract: Imagine Mr. Bean stepping into Tom and Jerry--can we generate videos where
characters interact naturally across different worlds? We study inter-character
interaction in text-to-video generation, where the key challenge is to preserve
each character's identity and behaviors while enabling coherent cross-context
interaction. This is difficult because characters may never have coexisted and
because mixing styles often causes style delusion, where realistic characters
appear cartoonish or vice versa. We introduce a framework that tackles these
issues with Cross-Character Embedding (CCE), which learns identity and
behavioral logic across multimodal sources, and Cross-Character Augmentation
(CCA), which enriches training with synthetic co-existence and mixed-style
data. Together, these techniques allow natural interactions between previously
uncoexistent characters without losing stylistic fidelity. Experiments on a
curated benchmark of cartoons and live-action series with 10 characters show
clear improvements in identity preservation, interaction quality, and
robustness to style delusion, enabling new forms of generative
storytelling.Additional results and videos are available on our project page:
https://tingtingliao.github.io/mimix/.

</details>


### [168] [VChain: Chain-of-Visual-Thought for Reasoning in Video Generation](https://arxiv.org/abs/2510.05094)
*Ziqi Huang,Ning Yu,Gordon Chen,Haonan Qiu,Paul Debevec,Ziwei Liu*

Main category: cs.CV

TL;DR: VChain是一个推理时视觉思维链框架，通过多模态模型生成关键帧来指导视频生成，提升复杂动态场景的视频质量


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型难以合成具有连贯因果链的复杂动态，而多模态模型具备强大的视觉状态推理能力，需要将两者优势结合

Method: 利用多模态模型生成稀疏关键帧作为快照，然后仅在关键时刻对预训练视频生成器进行稀疏推理时微调

Result: 在复杂多步骤场景的广泛实验中，VChain显著提升了生成视频的质量

Conclusion: VChain是一种调优高效、开销最小的视频生成方法，通过视觉推理信号注入有效解决了复杂动态建模的挑战

Abstract: Recent video generation models can produce smooth and visually appealing
clips, but they often struggle to synthesize complex dynamics with a coherent
chain of consequences. Accurately modeling visual outcomes and state
transitions over time remains a core challenge. In contrast, large language and
multimodal models (e.g., GPT-4o) exhibit strong visual state reasoning and
future prediction capabilities. To bridge these strengths, we introduce VChain,
a novel inference-time chain-of-visual-thought framework that injects visual
reasoning signals from multimodal models into video generation. Specifically,
VChain contains a dedicated pipeline that leverages large multimodal models to
generate a sparse set of critical keyframes as snapshots, which are then used
to guide the sparse inference-time tuning of a pre-trained video generator only
at these key moments. Our approach is tuning-efficient, introduces minimal
overhead and avoids dense supervision. Extensive experiments on complex,
multi-step scenarios show that VChain significantly enhances the quality of
generated videos.

</details>


### [169] [Paper2Video: Automatic Video Generation from Scientific Papers](https://arxiv.org/abs/2510.05096)
*Zeyu Zhu,Kevin Qinghong Lin,Mike Zheng Shou*

Main category: cs.CV

TL;DR: PaperTalker是首个用于学术演示视频生成的多智能体框架，包含数据集、评估指标和生成系统，能够自动从研究论文生成高质量的演示视频。


<details>
  <summary>Details</summary>
Motivation: 学术演示视频制作耗时耗力，需要协调幻灯片、字幕、语音和演讲者等多个模态，现有方法难以处理这种复杂的多模态协调问题。

Method: 提出多智能体框架，集成幻灯片生成、布局优化、字幕生成、语音合成和说话人头像渲染，采用并行化处理和新颖的树搜索视觉选择方法。

Result: 在Paper2Video数据集上的实验表明，该方法生成的演示视频比现有基线更忠实和内容丰富，建立了自动化学术视频生成的实用步骤。

Conclusion: PaperTalker为学术演示视频生成提供了首个完整的解决方案，包括基准数据集、评估指标和生成框架，推动了该领域的实用化发展。

Abstract: Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [170] [Model-Guided Microstimulation Steers Primate Visual Behavior](https://arxiv.org/abs/2510.03684)
*Johannes Mehrer,Ben Lonnqvist,Anna Mitola,Abdulkadir Gokce,Paolo Papale,Martin Schrimpf*

Main category: q-bio.NC

TL;DR: 该论文提出了一个计算框架来建模和指导高级视觉皮层的微刺激，通过三个关键组件实现：扰动模块、地形模型和映射程序，并在猴子实验中验证了模型预测与行为之间的强相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉假体方法受硬件限制和低级皮层表征特性的限制，而高级视觉区域编码更复杂的物体表征，但确定可靠引发物体级感知的刺激目标具有挑战性。

Method: 开发了一个包含三个组件的计算框架：(1)扰动模块将微刺激参数转换为神经活动的空间变化，(2)地形模型捕捉皮层神经元的空间组织，(3)映射程序将模型优化的刺激位点映射回灵长类皮层。

Result: 在两只猕猴的视觉识别任务中，模型预测的刺激实验产生了显著的感知选择变化，模型预测与猴子行为强相关，图像生成显示面部选择性位点的刺激与患者报告的面部幻视具有定性相似性。

Conclusion: 这项原理验证为模型引导的微刺激奠定了基础，并指向能够诱导更复杂视觉体验的下一代视觉假体。

Abstract: Brain stimulation is a powerful tool for understanding cortical function and
holds promise for therapeutic interventions in neuropsychiatric disorders.
Initial visual prosthetics apply electric microstimulation to early visual
cortex which can evoke percepts of simple symbols such as letters. However,
these approaches are fundamentally limited by hardware constraints and the
low-level representational properties of this cortical region. In contrast,
higher-level visual areas encode more complex object representations and
therefore constitute a promising target for stimulation - but determining
representational targets that reliably evoke object-level percepts constitutes
a major challenge. We here introduce a computational framework to causally
model and guide stimulation of high-level cortex, comprising three key
components: (1) a perturbation module that translates microstimulation
parameters into spatial changes to neural activity, (2) topographic models that
capture the spatial organization of cortical neurons and thus enable
prototyping of stimulation experiments, and (3) a mapping procedure that links
model-optimized stimulation sites back to primate cortex. Applying this
framework in two macaque monkeys performing a visual recognition task,
model-predicted stimulation experiments produced significant in-vivo changes in
perceptual choices. Per-site model predictions and monkey behavior were
strongly correlated, underscoring the promise of model-guided stimulation.
Image generation further revealed a qualitative similarity between in-silico
stimulation of face-selective sites and a patient's report of facephenes. This
proof-of-principle establishes a foundation for model-guided microstimulation
and points toward next-generation visual prosthetics capable of inducing more
complex visual experiences.

</details>


<div id='cs.CG'></div>

# cs.CG [[Back]](#toc)

### [171] [Fast Witness Persistence for MRI Volumes via Hybrid Landmarking](https://arxiv.org/abs/2510.04553)
*Jorge Leonardo Ruiz Williams*

Main category: cs.CG

TL;DR: 提出了一种可扩展的基于见证点的持久同调管道，用于全脑MRI体积分析，结合密度感知的地标选择和GPU就绪的见证过滤，在保持拓扑特征的同时显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法如Cech、Vietoris-Rips和alpha过滤在计算全脑MRI体积的持久同调时面临组合爆炸问题，需要更高效且可扩展的解决方案。

Method: 使用混合度量评分候选地标，平衡几何覆盖和逆核密度，通过密度感知的地标选择和GPU优化的见证过滤实现高效计算。

Result: 在BrainWeb、IXI和合成流形上的基准测试显示，在单个NVIDIA RTX 4090 GPU上执行时间少于10秒，平均成对距离比随机或仅密度基线减少30-60%。

Conclusion: 该方法提供了一个高效、可扩展的持久同调计算管道，适用于医学影像工作流，已作为whale-tda包在PyPI上发布。

Abstract: We introduce a scalable witness-based persistent homology pipeline for
full-brain MRI volumes that couples density-aware landmark selection with a
GPU-ready witness filtration. Candidates are scored by a hybrid metric that
balances geometric coverage against inverse kernel density, yielding landmark
sets that shrink mean pairwise distances by 30-60% over random or density-only
baselines while preserving topological features. Benchmarks on BrainWeb, IXI,
and synthetic manifolds execute in under ten seconds on a single NVIDIA RTX
4090 GPU, avoiding the combinatorial blow-up of Cech, Vietoris-Rips, and alpha
filtrations. The package is distributed on PyPI as whale-tda (installable via
pip); source and issues are hosted at https://github.com/jorgeLRW/whale. The
release also exposes a fast preset (mri_deep_dive_fast) for exploratory sweeps,
and ships with reproducibility-focused scripts and artifacts for drop-in use in
medical imaging workflows.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [172] [MoME: Mixture of Matryoshka Experts for Audio-Visual Speech Recognition](https://arxiv.org/abs/2510.04136)
*Umberto Cappellazzo,Minsu Kim,Pingchuan Ma,Honglie Chen,Xubo Liu,Stavros Petridis,Maja Pantic*

Main category: eess.AS

TL;DR: MoME框架将稀疏混合专家(MoE)与Matryoshka表示学习(MRL)相结合，为音频-视觉语音识别(AVSR)提供动态可调节的token压缩方案，在保持高性能的同时显著减少参数需求。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM在AVSR任务中计算需求高、token压缩方法缺乏灵活性、以及MRL方法在跨尺度泛化和鲁棒性方面的局限性。

Method: 提出MoME框架，在冻结的LLM基础上添加top-k路由和共享专家，实现跨尺度和模态的动态容量分配，通过共享路由器促进跨粒度的一致性专家激活。

Result: 在LRS2和LRS3数据集上，MoME在AVSR、ASR和VSR任务中达到最先进性能，同时参数需求显著减少，在噪声环境下保持鲁棒性。

Conclusion: MoME成功统一了MRL的适应性和MoE的效率，为资源感知的语音识别提供了可扩展且可解释的解决方案。

Abstract: Large language models (LLMs) have recently shown strong potential in
audio-visual speech recognition (AVSR), but their high computational demands
and sensitivity to token granularity limit their practicality in
resource-constrained settings. Token compression methods can reduce inference
cost, but they require fixing a compression rate in advance and produce a
single fixed-length output, offering no flexibility to balance information
density and efficiency at inference time. Matryoshka representation learning
(MRL) addresses this by enabling a single model to operate across multiple
token granularities, allowing compression rates to be adjusted dynamically.
However, current MRL-based methods treat each scale independently during
training, limiting cross-scale generalization, robustness at high compression,
and interpretability. To overcome these limitations, we propose MoME (Mixture
of Matryoshka Experts), a novel framework that integrates sparse
Mixture-of-Experts (MoE) into MRL-based LLMs for AVSR. MoME augments a frozen
LLM with top-k routed and shared experts, allowing dynamic capacity allocation
across scales and modalities. A shared router promotes consistent expert
activation across granularities, enabling compressed sequences to benefit from
representations learned at lower compression. Experiments on LRS2 and LRS3
demonstrate that MoME achieves state-of-the-art performance across AVSR, ASR,
and VSR tasks, while requiring significantly fewer parameters and maintaining
robustness under noise. MoME unifies the adaptability of MRL with the
efficiency of MoE, offering a scalable and interpretable solution for
resource-aware speech recognition.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [173] [Efficient Surgical Robotic Instrument Pose Reconstruction in Real World Conditions Using Unified Feature Detection](https://arxiv.org/abs/2510.03532)
*Zekai Liang,Kazuya Miyata,Xiao Liang,Florian Richter,Michael C. Yip*

Main category: cs.RO

TL;DR: 提出了一种统一检测几何基元（关键点和轴边缘）的新框架，通过共享编码实现高效的姿态估计，在具有挑战性的手术环境中实现了快速性能和最先进的精度。


<details>
  <summary>Details</summary>
Motivation: 微创手术机器人具有长运动链和部分自由度可见性，传统相机-机器人标定方法假设机器人刚性好且可见性佳，难以适用。现有方法在特征检测一致性或推理时间方面存在不足。

Method: 通过共享编码统一检测几何基元（关键点和轴边缘），在单次推理中同时检测关键点和边缘，利用投影几何进行姿态估计，使用大规模合成数据和投影标签进行训练。

Result: 在特征检测和姿态估计方面进行了评估，定性和定量结果表明在挑战性手术环境中具有快速性能和最先进的精度。

Conclusion: 该框架有效解决了微创手术机器人相机-机器人标定的挑战，实现了高效准确的姿态估计。

Abstract: Accurate camera-to-robot calibration is essential for any vision-based
robotic control system and especially critical in minimally invasive surgical
robots, where instruments conduct precise micro-manipulations. However, MIS
robots have long kinematic chains and partial visibility of their degrees of
freedom in the camera, which introduces challenges for conventional
camera-to-robot calibration methods that assume stiff robots with good
visibility. Previous works have investigated both keypoint-based and
rendering-based approaches to address this challenge in real-world conditions;
however, they often struggle with consistent feature detection or have long
inference times, neither of which are ideal for online robot control. In this
work, we propose a novel framework that unifies the detection of geometric
primitives (keypoints and shaft edges) through a shared encoding, enabling
efficient pose estimation via projection geometry. This architecture detects
both keypoints and edges in a single inference and is trained on large-scale
synthetic data with projective labeling. This method is evaluated across both
feature detection and pose estimation, with qualitative and quantitative
results demonstrating fast performance and state-of-the-art accuracy in
challenging surgical environments.

</details>


### [174] [EmbodiSwap for Zero-Shot Robot Imitation Learning](https://arxiv.org/abs/2510.03706)
*Eadom Dessalene,Pavan Mantripragada,Michael Maynord,Yiannis Aloimonos*

Main category: cs.RO

TL;DR: EmbodiSwap是一种生成逼真合成机器人覆盖层的方法，用于零样本模仿学习，通过V-JEPA视觉骨干网络在真实世界测试中达到82%的成功率


<details>
  <summary>Details</summary>
Motivation: 解决野外自我中心人类视频与目标机器人具身化之间的具身鸿沟，实现零样本模仿学习

Method: 使用EmbodiSwap方法在人类视频上生成逼真的合成机器人覆盖层，并利用V-JEPA作为视觉骨干网络进行闭环机器人操作策略训练

Result: 在真实世界测试中，零样本训练的V-JEPA模型达到82%的成功率，优于few-shot训练的π₀网络和基于EmbodiSwap数据训练的π₀

Conclusion: EmbodiSwap方法有效，V-JEPA在机器人视觉任务中表现出色，发布了代码、数据集和模型检查点以促进可重复研究

Abstract: We introduce EmbodiSwap - a method for producing photorealistic synthetic
robot overlays over human video. We employ EmbodiSwap for zero-shot imitation
learning, bridging the embodiment gap between in-the-wild ego-centric human
video and a target robot embodiment. We train a closed-loop robot manipulation
policy over the data produced by EmbodiSwap. We make novel use of V-JEPA as a
visual backbone, repurposing V-JEPA from the domain of video understanding to
imitation learning over synthetic robot videos. Adoption of V-JEPA outperforms
alternative vision backbones more conventionally used within robotics. In
real-world tests, our zero-shot trained V-JEPA model achieves an $82\%$ success
rate, outperforming a few-shot trained $\pi_0$ network as well as $\pi_0$
trained over data produced by EmbodiSwap. We release (i) code for generating
the synthetic robot overlays which takes as input human videos and an arbitrary
robot URDF and generates a robot dataset, (ii) the robot dataset we synthesize
over EPIC-Kitchens, HOI4D and Ego4D, and (iii) model checkpoints and inference
code, to facilitate reproducible research and broader adoption.

</details>


### [175] [NoTVLA: Narrowing of Dense Action Trajectories for Generalizable Robot Manipulation](https://arxiv.org/abs/2510.03895)
*Zheng Huang,Mingyu Liu,Xiaoyi Lin,Muzhi Zhu,Canyu Zhao,Zongze Du,Xiaoman Li,Yiduo Jia,Hao Zhong,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: NoTVLA框架通过稀疏轨迹规划解决VLA模型的灾难性遗忘问题，使用更少计算资源实现多任务泛化性能


<details>
  <summary>Details</summary>
Motivation: 解决Vision-Language-Action模型在现实部署中因依赖连续动作序列导致的灾难性遗忘问题

Method: 提出NoTVLA框架，采用稀疏轨迹而非密集动作轨迹，通过时间压缩和空间推理修剪专注于机器人末端执行器轨迹规划

Result: 在多任务评估中性能优于pi0，计算资源节省一个数量级，无需腕部摄像头，操作精度接近单任务专家模型

Conclusion: NoTVLA有效缓解灾难性遗忘，保持语言能力，支持多平台统一部署和零样本泛化

Abstract: Vision-Language-Action (VLA) models represent a pivotal advance in embodied
intelligence, yet they confront critical barriers to real-world deployment,
most notably catastrophic forgetting. This issue stems from their overreliance
on continuous action sequences or action chunks, which inadvertently create
isolated data silos that disrupt knowledge retention across tasks. To tackle
these challenges, we propose the Narrowing of Trajectory VLA (NoTVLA)
framework: a novel approach that narrows its focus to sparse trajectories,
thereby avoiding the catastrophic forgetting associated with dense trajectory
fine-tuning. A key innovation of NoTVLA lies in its trajectory planning
strategy: instead of centering on the target object's trajectory, it leverages
temporal compression and spatial reasoning pruning specifically for the robot
end effector's trajectory. Furthermore, training is conducted using these
sparse trajectories rather than dense action trajectories, an optimization that
delivers remarkable practical advantages with better performance in zero-shot.
In multi-task evaluation scenarios, NoTVLA achieves superior performance and
generalization compared to pi0 while operating under two critical constraints:
it uses over an order of magnitude less computing power than pi0 and requires
no wrist-mounted camera. This design ensures that NoTVLA's operational accuracy
closely approximates that of single-task expert models. Crucially, it also
preserves the model's inherent language capabilities, enabling zero-shot
generalization in specific scenarios, supporting unified model deployment
across multiple robot platforms, and fostering a degree of generalization even
when perceiving tasks from novel perspectives.

</details>


### [176] [CLEAR-IR: Clarity-Enhanced Active Reconstruction of Infrared Imagery](https://arxiv.org/abs/2510.04883)
*Nathan Shankar,Pawel Ladosz,Hujun Yin*

Main category: cs.RO

TL;DR: 提出基于U-Net的架构，从含发射器图案的红外图像中重建干净图像，提升暗光环境下机器人感知的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 红外流在低光条件下比RGB更抗噪，但受主动发射器图案干扰，影响物体检测、跟踪和定位等高级任务

Method: 使用U-Net架构重建干净的红外图像，消除发射器图案的干扰

Result: 该方法优于现有增强技术，能在从良好光照到极端低光场景下实现可靠的视觉驱动机器人系统操作

Conclusion: 该方法是暗光环境下实现鲁棒机器人感知的有效解决方案

Abstract: This paper presents a novel approach for enabling robust robotic perception
in dark environments using infrared (IR) stream. IR stream is less susceptible
to noise than RGB in low-light conditions. However, it is dominated by active
emitter patterns that hinder high-level tasks such as object detection,
tracking and localisation. To address this, a U-Net-based architecture is
proposed that reconstructs clean IR images from emitter-populated input,
improving both image quality and downstream robotic performance. This approach
outperforms existing enhancement techniques and enables reliable operation of
vision-driven robotic systems across illumination conditions from well-lit to
extreme low-light scenes.

</details>


### [177] [StaMo: Unsupervised Learning of Generalizable Robot Motion from Compact State Representation](https://arxiv.org/abs/2510.05057)
*Mingyu Liu,Jiuhe Shu,Hui Chen,Zeju Li,Canyu Zhao,Jiange Yang,Shenyuan Gao,Hao Chen,Chunhua Shen*

Main category: cs.RO

TL;DR: 提出了一种名为StaMo的无监督方法，学习高度压缩的双token状态表示，利用预训练的Diffusion Transformer解码器生成高效、可解释的状态表示，并能自然产生有效的潜在动作。


<details>
  <summary>Details</summary>
Motivation: 解决具身智能中状态表示平衡问题——现有方法要么过于冗余，要么缺乏任务关键信息，需要开发表达性强且紧凑的状态表示。

Method: 使用轻量级编码器和预训练Diffusion Transformer解码器学习双token状态表示，通过潜在插值获得潜在动作，无需复杂架构和视频数据。

Result: 在LIBERO上性能提升14.3%，真实世界任务成功率提高30%，潜在动作增强策略协同训练，优于先前方法10.4%，且能有效扩展到多种数据源。

Conclusion: StaMo方法从静态图像学习紧凑状态表示，挑战了依赖复杂架构和视频数据学习潜在动作的主流方法，展示了结构化动态的捕获能力。

Abstract: A fundamental challenge in embodied intelligence is developing expressive and
compact state representations for efficient world modeling and decision making.
However, existing methods often fail to achieve this balance, yielding
representations that are either overly redundant or lacking in task-critical
information. We propose an unsupervised approach that learns a highly
compressed two-token state representation using a lightweight encoder and a
pre-trained Diffusion Transformer (DiT) decoder, capitalizing on its strong
generative prior. Our representation is efficient, interpretable, and
integrates seamlessly into existing VLA-based models, improving performance by
14.3% on LIBERO and 30% in real-world task success with minimal inference
overhead. More importantly, we find that the difference between these tokens,
obtained via latent interpolation, naturally serves as a highly effective
latent action, which can be further decoded into executable robot actions. This
emergent capability reveals that our representation captures structured
dynamics without explicit supervision. We name our method StaMo for its ability
to learn generalizable robotic Motion from compact State representation, which
is encoded from static images, challenging the prevalent dependence to learning
latent action on complex architectures and video data. The resulting latent
actions also enhance policy co-training, outperforming prior methods by 10.4%
with improved interpretability. Moreover, our approach scales effectively
across diverse data sources, including real-world robot data, simulation, and
human egocentric video.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [178] [Bridging the Gap Between Multimodal Foundation Models and World Models](https://arxiv.org/abs/2510.03727)
*Xuehai He*

Main category: cs.AI

TL;DR: 该论文探讨如何将多模态基础模型提升为有效的世界模型，通过增强推理能力和生成能力来实现对物理世界的深度理解与模拟。


<details>
  <summary>Details</summary>
Motivation: 受人类多感官整合理解世界的启发，当前多模态基础模型缺乏作为世界模型的关键能力，如反事实推理、动态模拟、时空信息理解和可控生成等。

Method: 通过判别性任务提升模型推理能力，引入因果推理、反事实思维和时空推理等结构化推理技能；在生成方面开发结构化可控生成框架，利用场景图、多模态条件和多模态对齐策略指导生成过程。

Result: 实现了多模态基础模型在图像和视频模态上的可控生成能力，特别是扩展到可交互、可编辑的4D生成，支持时空维度上的对象合成与变形。

Conclusion: 该研究为多模态基础模型向世界模型的演进提供了系统性方法，增强了模型对物理世界的深度理解和生成控制能力。

Abstract: Humans understand the world through the integration of multiple sensory
modalities, enabling them to perceive, reason about, and imagine dynamic
physical processes. Inspired by this capability, multimodal foundation models
(MFMs) have emerged as powerful tools for multimodal understanding and
generation. However, today's MFMs fall short of serving as effective world
models. They lack the essential ability such as perform counterfactual
reasoning, simulate dynamics, understand the spatiotemporal information,
control generated visual outcomes, and perform multifaceted reasoning. We
investigates what it takes to bridge the gap between multimodal foundation
models and world models. We begin by improving the reasoning capabilities of
MFMs through discriminative tasks and equipping MFMs with structured reasoning
skills, such as causal inference, counterfactual thinking, and spatiotemporal
reasoning, enabling them to go beyond surface correlations and understand
deeper relationships within visual and textual data. Next, we explore
generative capabilities of multimodal foundation models across both image and
video modalities, introducing new frameworks for structured and controllable
generation. Our approaches incorporate scene graphs, multimodal conditioning,
and multimodal alignment strategies to guide the generation process, ensuring
consistency with high-level semantics and fine-grained user intent. We further
extend these techniques to controllable 4D generation, enabling interactive,
editable, and morphable object synthesis over time and space.

</details>


### [179] [ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering](https://arxiv.org/abs/2510.04514)
*Rachneet Kaur,Nishan Srishankar,Zhen Zeng,Sumitra Ganesh,Manuela Veloso*

Main category: cs.AI

TL;DR: ChartAgent是一个新颖的智能体框架，通过视觉推理和图表交互来解决未标注图表理解问题，在ChartBench和ChartX基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态LLM在基于图表的视觉问答中表现良好，但在需要精确视觉解释而非依赖文本捷径的未标注图表上性能急剧下降。

Method: ChartAgent采用迭代式视觉推理方法，将查询分解为视觉子任务，通过专门的视觉工具（如绘制注释、裁剪区域、定位坐标轴等）主动操作和交互图表图像。

Result: ChartAgent在ChartBench和ChartX基准测试中超越了先前方法，整体绝对增益达16.07%，在未标注和数值密集型查询上增益达17.31%。

Conclusion: ChartAgent是首批使用工具增强的多模态智能体进行视觉基础推理的图表理解工作之一，能够有效提升各种底层LLM的性能，展现了跨不同图表类型和复杂性级别的优异表现。

Abstract: Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

</details>


### [180] [Watch and Learn: Learning to Use Computers from Online Videos](https://arxiv.org/abs/2510.04673)
*Chan Hee Song,Yiwen Song,Palash Goyal,Yu Su,Oriana Riva,Hamid Palangi,Tomas Pfister*

Main category: cs.AI

TL;DR: 提出了Watch & Learn (W&L)框架，将网络上的真人演示视频大规模转化为可执行的UI轨迹，解决了计算机使用代理训练数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理需要基于多样化、不断变化的应用和环境规划任务流程，但目标应用领域的大规模高质量训练数据稀缺，现有数据集存在领域特定、静态且标注成本高等局限性。

Method: 将问题转化为逆向动力学目标：从连续屏幕状态预测用户动作。开发了包含任务感知视频检索的逆向动力学标注流程，从原始网络视频生成了超过53k条高质量轨迹。

Result: 在OSWorld基准测试中，W&L提取的UI轨迹持续提升了通用框架和SOTA框架的上下文性能，并为开源模型在监督训练下带来了更强的性能提升。

Conclusion: 网络规模的真人演示视频是推进计算机使用代理向实际部署发展的实用且可扩展的基础。

Abstract: Computer use agents (CUAs) need to plan task workflows grounded in diverse,
ever-changing applications and environments, but learning is hindered by the
scarcity of large-scale, high-quality training data in the target application.
Existing datasets are domain-specific, static, and costly to annotate, while
current synthetic data generation methods often yield simplistic or misaligned
task demonstrations. To address these limitations, we introduce Watch & Learn
(W&L), a framework that converts human demonstration videos readily available
on the Internet into executable UI trajectories at scale. Instead of directly
generating trajectories or relying on ad hoc reasoning heuristics, we cast the
problem as an inverse dynamics objective: predicting the user's action from
consecutive screen states. This formulation reduces manual engineering, is
easier to learn, and generalizes more robustly across applications. Concretely,
we develop an inverse dynamics labeling pipeline with task-aware video
retrieval, generate over 53k high-quality trajectories from raw web videos, and
demonstrate that these trajectories improve CUAs both as in-context
demonstrations and as supervised training data. On the challenging OSWorld
benchmark, UI trajectories extracted with W&L consistently enhance both
general-purpose and state-of-the-art frameworks in-context, and deliver
stronger gains for open-source models under supervised training. These results
highlight web-scale human demonstration videos as a practical and scalable
foundation for advancing CUAs towards real-world deployment.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [181] [Use of Quadcopter Wakes to Supplement Strawberry Pollination](https://arxiv.org/abs/2510.03974)
*Sadie Cutler,Ben DeFay,Scott McArt,Kirstin Petersen*

Main category: eess.SY

TL;DR: 本文研究了一种基于风媒授粉的新型人工授粉方法，使用四轴飞行器辅助自然授粉，实验室研究显示该方法有潜力但田间实验结果尚不明确。


<details>
  <summary>Details</summary>
Motivation: 传粉者对全球生态系统和粮食供应至关重要，但最近研究发现包括草莓在内的多种作物存在授粉不足问题，而野生和人工管理的传粉者数量正在下降，需要寻找补充授粉解决方案。

Method: 确定侧向气流最大化的高度后，在田间进行四轴飞行器辅助自然授粉的实验，同时在实验室进行相关研究。

Result: 田间实验结果不明确，但实验室研究表明该方法具有潜力，可以进一步优化以获得更好的田间效果。

Conclusion: 基于风媒授粉的四轴飞行器辅助授粉方法在实验室中显示出前景，但需要进一步改进以适应田间条件。

Abstract: Pollinators are critical to the world's ecosystems and food supply, yet
recent studies have found pollination shortfalls in several crops, including
strawberry. This is troubling because wild and managed pollinators are
currently experiencing declines. One possibility is to try and provide
supplemental pollination solutions. These solutions should be affordable and
simple for farmers to implement if their use is to be widespread; quadcopters
are a great example, already used for monitoring on many farms. This paper
investigates a new method for artificial pollination based on wind pollination
that bears further investigation. After determining the height where the
lateral flow is maximized, we performed field experiments with a quadcopter
assisting natural pollinators. Although our results in the field were
inconclusive, lab studies show that the idea shows promise and could be adapted
for better field results.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [182] [Visual Lifelog Retrieval through Captioning-Enhanced Interpretation](https://arxiv.org/abs/2510.04010)
*Yu-Fei Shih,An-Zi Yen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.IR

TL;DR: 提出了CIVIL检索系统，通过生成视觉生活日志的文本描述，然后使用文本嵌入模型将描述和用户查询映射到共享向量空间，实现基于文本查询的图像检索。


<details>
  <summary>Details</summary>
Motivation: 人们经常难以记住过去经历的细节，需要重新访问这些记忆。视觉生活日志（通过可穿戴相机拍摄）提供第一人称视角，需要解释相机背后个体的活动而不仅仅是描述场景。

Method: 提出三种不同的方法：单描述方法、集体描述方法和合并描述方法，每种方法都旨在解释生活记录者的生活经历。系统首先生成视觉生活日志的文本描述，然后使用文本嵌入模型将描述和用户查询投影到共享向量空间。

Result: 实验结果表明，该方法能有效描述第一人称视觉图像，提高了生活日志检索的效果。

Conclusion: 构建了一个将视觉生活日志转换为描述的文本数据集，从而重建个人生活经历，证明该方法在生活日志检索中的有效性。

Abstract: People often struggle to remember specific details of past experiences, which
can lead to the need to revisit these memories. Consequently, lifelog retrieval
has emerged as a crucial application. Various studies have explored methods to
facilitate rapid access to personal lifelogs for memory recall assistance. In
this paper, we propose a Captioning-Integrated Visual Lifelog (CIVIL) Retrieval
System for extracting specific images from a user's visual lifelog based on
textual queries. Unlike traditional embedding-based methods, our system first
generates captions for visual lifelogs and then utilizes a text embedding model
to project both the captions and user queries into a shared vector space.
Visual lifelogs, captured through wearable cameras, provide a first-person
viewpoint, necessitating the interpretation of the activities of the individual
behind the camera rather than merely describing the scene. To address this, we
introduce three distinct approaches: the single caption method, the collective
caption method, and the merged caption method, each designed to interpret the
life experiences of lifeloggers. Experimental results show that our method
effectively describes first-person visual images, enhancing the outcomes of
lifelog retrieval. Furthermore, we construct a textual dataset that converts
visual lifelogs into captions, thereby reconstructing personal life
experiences.

</details>


### [183] [Learning-Based Hashing for ANN Search: Foundations and Early Advances](https://arxiv.org/abs/2510.04127)
*Sean Moran*

Main category: cs.IR

TL;DR: 这篇论文是对早期基于学习的哈希方法的基础性综述，重点介绍了用于近似最近邻搜索的哈希技术核心思想，包括监督、无监督和半监督方法。


<details>
  <summary>Details</summary>
Motivation: 近似最近邻搜索是信息检索中的基础问题，哈希方法通过将高维数据映射到紧凑二进制码来实现快速相似性计算。论文旨在介绍基于学习的哈希的概念基础，而不是提供最新方法的详尽说明。

Method: 综述了投影函数设计以生成有意义的嵌入，以及量化策略将这些嵌入转换为二进制码的方法。还研究了多比特和多阈值模型的扩展，以及跨模态检索的早期进展。

Result: 通过将这些早期模型置于历史背景中，为读者提供了对该领域原则、权衡和开放挑战的结构化理解。

Conclusion: 这篇综述旨在为读者提供继续指导当前研究的基于学习哈希的原则、权衡和开放挑战的结构化理解。

Abstract: Approximate Nearest Neighbour (ANN) search is a fundamental problem in
information retrieval, underpinning large-scale applications in computer
vision, natural language processing, and cross-modal search. Hashing-based
methods provide an efficient solution by mapping high-dimensional data into
compact binary codes that enable fast similarity computations in Hamming space.
Over the past two decades, a substantial body of work has explored learning to
hash, where projection and quantisation functions are optimised from data
rather than chosen at random.
  This article offers a foundational survey of early learning-based hashing
methods, with an emphasis on the core ideas that shaped the field. We review
supervised, unsupervised, and semi-supervised approaches, highlighting how
projection functions are designed to generate meaningful embeddings and how
quantisation strategies convert these embeddings into binary codes. We also
examine extensions to multi-bit and multi-threshold models, as well as early
advances in cross-modal retrieval.
  Rather than providing an exhaustive account of the most recent methods, our
goal is to introduce the conceptual foundations of learning-based hashing for
ANN search. By situating these early models in their historical context, we aim
to equip readers with a structured understanding of the principles, trade-offs,
and open challenges that continue to inform current research in this area.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [184] [Real-time nonlinear inversion of magnetic resonance elastography with operator learning](https://arxiv.org/abs/2510.03372)
*Juampablo E. Heras Rivera,Caitlin M. Neher,Mehmet Kurt*

Main category: eess.IV

TL;DR: 开发并评估用于脑磁共振弹性成像（MRE）非线性反演的算子学习框架，实现实时弹性图反演，精度与非线性反演相当


<details>
  <summary>Details</summary>
Motivation: 传统非线性反演（NLI）方法计算成本高，无法实时应用。需要开发能够保持NLI空间精度但速度大幅提升的新方法

Method: 使用61名受试者的3D MRE数据，开发预测性深度算子学习框架（oNLI），采用10折交叉验证。输入为测量位移场的复旋度，输出为NLI参考弹性图。结合结构先验机制提高空间精度

Result: oNLI在全脑绝对百分比误差为8.4±0.5（μ'）和10.0±0.7（μ''），显著优于CNN的15.8±0.8和26.1±1.1。在所有脑区和子区域均表现更好（p<0.05）

Conclusion: oNLI框架实现实时MRE反演（30,000倍加速），优于CNN方法，保持NLI的精细空间精度

Abstract: $\textbf{Purpose:}$ To develop and evaluate an operator learning framework
for nonlinear inversion (NLI) of brain magnetic resonance elastography (MRE)
data, which enables real-time inversion of elastograms with comparable spatial
accuracy to NLI.
  $\textbf{Materials and Methods:}$ In this retrospective study, 3D MRE data
from 61 individuals (mean age, 37.4 years; 34 female) were used for development
of the framework. A predictive deep operator learning framework (oNLI) was
trained using 10-fold cross-validation, with the complex curl of the measured
displacement field as inputs and NLI-derived reference elastograms as outputs.
A structural prior mechanism, analogous to Soft Prior Regularization in the MRE
literature, was incorporated to improve spatial accuracy. Subject-level
evaluation metrics included Pearson's correlation coefficient, absolute
relative error, and structural similarity index measure between predicted and
reference elastograms across brain regions of different sizes to understand
accuracy. Statistical analyses included paired t-tests comparing the proposed
oNLI variants to the convolutional neural network baselines.
  $\textbf{Results:}$ Whole brain absolute percent error was 8.4 $\pm$ 0.5
($\mu'$) and 10.0 $\pm$ 0.7 ($\mu''$) for oNLI and 15.8 $\pm$ 0.8 ($\mu'$) and
26.1 $\pm$ 1.1 ($\mu''$) for CNNs. Additionally, oNLI outperformed
convolutional architectures as per Pearson's correlation coefficient, $r$, in
the whole brain and across all subregions for both the storage modulus and loss
modulus (p < 0.05).
  $\textbf{Conclusion:}$ The oNLI framework enables real-time MRE inversion
(30,000x speedup), outperforming CNN-based approaches and maintaining the
fine-grained spatial accuracy achievable with NLI in the brain.

</details>


### [185] [How We Won BraTS-SSA 2025: Brain Tumor Segmentation in the Sub-Saharan African Population Using Segmentation-Aware Data Augmentation and Model Ensembling](https://arxiv.org/abs/2510.03568)
*Claudia Takyi Ankomah,Livingstone Eli Ayivor,Ireneaus Nyame,Leslie Wambo,Patrick Yeboah Bonsu,Aondona Moses Iorumbur,Raymond Confidence,Toufiq Musah*

Main category: eess.IV

TL;DR: 该研究通过数据增强和模型集成方法，在非洲脑肿瘤数据集上提高了脑胶质瘤分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型主要基于同质化高资源数据集训练，在资源匮乏地区部署时鲁棒性不足，需要针对多样化数据集开发更稳健的分割方法。

Method: 使用分割感知的离线数据增强技术扩大BraTS-Africa数据集样本量和多样性，构建包含MedNeXt、SegMamba和Residual-Encoder U-Net三种架构的集成模型。

Result: MedNeXt模型在1000轮训练后获得最佳平均病灶Dice分数0.86和归一化表面距离0.81，而500轮训练的集成模型在不同肿瘤亚区表现出最均衡的分割性能。

Conclusion: 先进的数据增强和模型集成策略能够有效提升在多样化和代表性不足数据集上的分割准确性和鲁棒性。

Abstract: Brain tumors, particularly gliomas, pose significant chall-enges due to their
complex growth patterns, infiltrative nature, and the variability in brain
structure across individuals, which makes accurate diagnosis and monitoring
difficult. Deep learning models have been developed to accurately delineate
these tumors. However, most of these models were trained on relatively
homogenous high-resource datasets, limiting their robustness when deployed in
underserved regions. In this study, we performed segmentation-aware offline
data augmentation on the BraTS-Africa dataset to increase the data sample size
and diversity to enhance generalization. We further constructed an ensemble of
three distinct architectures, MedNeXt, SegMamba, and Residual-Encoder U-Net, to
leverage their complementary strengths. Our best-performing model, MedNeXt, was
trained on 1000 epochs and achieved the highest average lesion-wise dice and
normalized surface distance scores of 0.86 and 0.81 respectively. However, the
ensemble model trained for 500 epochs produced the most balanced segmentation
performance across the tumour subregions. This work demonstrates that a
combination of advanced augmentation and model ensembling can improve
segmentation accuracy and robustness on diverse and underrepresented datasets.
Code available at:
https://github.com/SPARK-Academy-2025/SPARK-2025/tree/main/SPARK2025_BraTs_MODELS/SPARK_NeuroAshanti

</details>


### [186] [Towards Robust and Generalizable Continuous Space-Time Video Super-Resolution with Events](https://arxiv.org/abs/2510.03833)
*Shuoyan Wei,Feng Li,Shengeng Tang,Runmin Cong,Yao Zhao,Meng Wang,Huihui Bai*

Main category: eess.IV

TL;DR: EvEnhancer是一个结合事件流数据实现连续时空视频超分辨率的创新方法，通过事件自适应合成和局部隐式视频变换器，能够生成任意分辨率和帧率的视频，并在OOD尺度上保持优秀泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有连续时空视频超分辨率方法在分布外尺度上泛化能力差，难以处理任意空间和时间尺度的视频重建需求。

Method: 使用事件流数据的高时间分辨率和高动态范围特性，结合事件自适应合成捕捉长期运动轨迹，以及局部隐式视频变换器学习连续视频表示。EvEnhancerPlus增加了可控切换机制和交叉导数训练策略。

Result: 在合成和真实数据集上达到最先进性能，在OOD尺度上保持优越泛化能力，同时显著降低计算开销。

Conclusion: 该方法成功解决了连续时空视频超分辨率的泛化问题，为任意尺度的视频重建提供了有效解决方案。

Abstract: Continuous space-time video super-resolution (C-STVSR) has garnered
increasing interest for its capability to reconstruct high-resolution and
high-frame-rate videos at arbitrary spatial and temporal scales. However,
prevailing methods often generalize poorly, producing unsatisfactory results
when applied to out-of-distribution (OOD) scales. To overcome this limitation,
we present EvEnhancer, a novel approach that marries the unique properties of
high temporal resolution and high dynamic range encapsulated in event streams
to achieve robust and generalizable C-STVSR. Our approach incorporates
event-adapted synthesis that capitalizes on the spatiotemporal correlations
between frames and events to capture long-term motion trajectories, enabling
adaptive interpolation and fusion across space and time. This is then coupled
with a local implicit video transformer that integrates local implicit video
neural function with cross-scale spatiotemporal attention to learn continuous
video representations and generate plausible videos at arbitrary resolutions
and frame rates. We further develop EvEnhancerPlus, which builds a controllable
switching mechanism that dynamically determines the reconstruction difficulty
for each spatiotemporal pixel based on local event statistics. This allows the
model to adaptively route reconstruction along the most suitable pathways at a
fine-grained pixel level, substantially reducing computational overhead while
maintaining excellent performance. Furthermore, we devise a cross-derivative
training strategy that stabilizes the convergence of such a multi-pathway
framework through staged cross-optimization. Extensive experiments demonstrate
that our method achieves state-of-the-art performance on both synthetic and
real-world datasets, while maintaining superior generalizability at OOD scales.
The code is available at https://github.com/W-Shuoyan/EvEnhancerPlus.

</details>


### [187] [AI-Assisted Pleural Effusion Volume Estimation from Contrast-Enhanced CT Images](https://arxiv.org/abs/2510.03856)
*Sanhita Basu,Tomas Fröding,Ali Teymur Kahraman,Dimitris Toumpanakis,Tobias Sjöblom*

Main category: eess.IV

TL;DR: 开发了一种名为TTAS的半监督深度学习框架，用于从CT扫描中准确分割和量化胸腔积液，相比现有方法在分割性能和体积测量精度方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 胸腔积液是常见临床问题，但通过CT扫描准确测量其体积具有挑战性。需要改进PE分割和量化方法以提升临床管理效果。

Method: 使用CT肺动脉造影数据，开发了教师-助教-学生（TTAS）半监督深度学习框架，在100例手动标注数据上训练，并在未分割检查中实现高效训练。

Result: TTAS模型在分割性能上显著优于最先进模型，平均Dice得分为0.82（vs nnU-Net的0.73），平均绝对体积差异降低四倍至6.49mL（vs nnU-Net的23.16mL）。

Conclusion: TTAS框架提供了卓越的PE分割能力，有助于从CT扫描中准确确定胸腔积液体积。

Abstract: Background: Pleural Effusions (PE) is a common finding in many different
clinical conditions, but accurately measuring their volume from CT scans is
challenging. Purpose: To improve PE segmentation and quantification for
enhanced clinical management, we have developed and trained a semi-supervised
deep learning framework on contrast-enhanced CT volumes. Materials and Methods:
This retrospective study collected CT Pulmonary Angiogram (CTPA) data from
internal and external datasets. A subset of 100 cases was manually annotated
for model training, while the remaining cases were used for testing and
validation. A novel semi-supervised deep learning framework, Teacher-Teaching
Assistant-Student (TTAS), was developed and used to enable efficient training
in non-segmented examinations. Segmentation performance was compared to that of
state-of-the-art models. Results: 100 patients (mean age, 72 years, 28
[standard deviation]; 55 men) were included in the study. The TTAS model
demonstrated superior segmentation performance compared to state-of-the-art
models, achieving a mean Dice score of 0.82 (95% CI, 0.79 - 0.84) versus 0.73
for nnU-Net (p < 0.0001, Student's T test). Additionally, TTAS exhibited a
four-fold lower mean Absolute Volume Difference (AbVD) of 6.49 mL (95% CI, 4.80
- 8.20) compared to nnU-Net's AbVD of 23.16 mL (p < 0.0001). Conclusion: The
developed TTAS framework offered superior PE segmentation, aiding accurate
volume determination from CT scans.

</details>


### [188] [Sliding Window Attention for Learned Video Compression](https://arxiv.org/abs/2510.03926)
*Alexander Kopte,André Kaup*

Main category: eess.IV

TL;DR: 本文提出了一种名为3D滑动窗口注意力（SWA）的无补丁局部注意力机制，用于改进视频压缩中的transformer模型。该方法通过统一的解码器架构和均匀感受野，显著提升了率失真性能，同时大幅降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的视频压缩transformer模型采用分块处理方式，存在感受野不规则和计算冗余等问题。特别是当应用于时间自回归模型时，需要计算冗余的重叠窗口，影响了模型效率。

Method: 提出了3D滑动窗口注意力（SWA）机制，这是一种无补丁的局部注意力形式。该方法采用纯解码器架构，统一处理空间和时间上下文，提供均匀的感受野，消除了对重叠窗口的需求。

Result: 相比VCT基线模型，该方法实现了高达18.6%的Bjøntegaard Delta-rate节省。同时，解码器复杂度降低了2.8倍，熵模型效率提升了近3.5倍。研究还发现，虽然模型受益于长程时间上下文，但过长的上下文会降低性能。

Conclusion: 3D滑动窗口注意力机制通过消除补丁划分带来的架构缺陷，在保持高性能的同时显著降低了计算复杂度，为视频压缩transformer模型提供了一种更有效的局部注意力解决方案。

Abstract: To manage the complexity of transformers in video compression, local
attention mechanisms are a practical necessity. The common approach of
partitioning frames into patches, however, creates architectural flaws like
irregular receptive fields. When adapted for temporal autoregressive models,
this paradigm, exemplified by the Video Compression Transformer (VCT), also
necessitates computationally redundant overlapping windows. This work
introduces 3D Sliding Window Attention (SWA), a patchless form of local
attention. By enabling a decoder-only architecture that unifies spatial and
temporal context processing, and by providing a uniform receptive field, our
method significantly improves rate-distortion performance, achieving
Bj{\o}rntegaard Delta-rate savings of up to 18.6 % against the VCT baseline.
Simultaneously, by eliminating the need for overlapping windows, our method
reduces overall decoder complexity by a factor of 2.8, while its entropy model
is nearly 3.5 times more efficient. We further analyze our model's behavior and
show that while it benefits from long-range temporal context, excessive context
can degrade performance.

</details>


### [189] [The method of the approximate inverse for limited-angle CT](https://arxiv.org/abs/2510.04369)
*Bernadette Hahn,Gael Rigaud,Richard Schmähl*

Main category: eess.IV

TL;DR: 提出了一种基于近似逆方法的新模型驱动方法，用于解决有限角度CT重建问题，能够避免传统方法产生的条纹伪影，并通过正则化策略处理病态问题。


<details>
  <summary>Details</summary>
Motivation: 有限角度CT在工业和医学中具有重要应用价值，但传统方法如滤波反投影算法或总变分方法会产生各种伪影，而深度学习等方法需要大量数据集。因此需要开发新的模型驱动方法来解决这些问题。

Method: 采用近似逆方法，通过预计算重建核来评估测量数据的线性泛函。开发了约束有限角度重建核（CLARK）方法，结合谱滤波、近似逆方法和自定义边缘保持去噪技术来稳定整个重建过程。

Result: 该方法能够完全重建物体而不产生条纹伪影，即使在大角度限制下也能保持良好性能。在合成数据和真实数据上的验证表明该方法有效。

Conclusion: 该方法为未来的学习策略提供了新的起点，通过正则化策略成功解决了有限角度CT重建中的病态问题，为有限角度成像提供了有效的解决方案。

Abstract: Limited-angle computerized tomography stands for one of the most difficult
challenges in imaging. Although it opens the way to faster data acquisition in
industry and less dangerous scans in medicine, standard approaches, such as the
filtered backprojection (FBP) algorithm or the widely used total-variation
functional, often produce various artefacts that hinder the diagnosis. With the
rise of deep learning, many modern techniques have proven themselves successful
in removing such artefacts but at the cost of large datasets. In this paper, we
propose a new model-driven approach based on the method of the approximate
inverse, which could serve as new starting point for learning strategies in the
future. In contrast to FBP-type approaches, our reconstruction step consists in
evaluating linear functionals on the measured data using reconstruction kernels
that are precomputed as solution of an auxiliary problem. With this problem
being uniquely solvable, the derived limited-angle reconstruction kernel (LARK)
is able to fully reconstruct the object without the well-known streak
artefacts, even for large limited angles. However, it inherits severe
ill-conditioning which leads to a different kind of artefacts arising from the
singular functions of the limited-angle Radon transform. The problem becomes
particularly challenging when working on semi-discrete (real or analytical)
measurements. We develop a general regularization strategy, named constrained
limited-angle reconstruction kernel (CLARK), by combining spectral filter, the
method of the approximate inverse and custom edge-preserving denoising in order
to stabilize the whole process. We further derive and interpret error estimates
for the application on real, i.e. semi-discrete, data and we validate our
approach on synthetic and real data.

</details>


### [190] [Adaptive double-phase Rudin--Osher--Fatemi denoising model](https://arxiv.org/abs/2510.04382)
*Wojciech Górny,Michał Łasica,Alexandros Matsoukas*

Main category: eess.IV

TL;DR: 提出一种基于双相型变增长总变差正则化的新图像去噪模型，通过自适应权重减少阶梯效应，同时保持边缘保护效果。


<details>
  <summary>Details</summary>
Motivation: 传统Rudin-Osher-Fatemi模型存在阶梯效应问题，需要开发既能减少阶梯效应又能保持边缘保护的新方法。

Method: 采用双相型变增长总变差正则化，结合自适应权重机制，在1D和2D合成及自然图像上进行测试。

Result: 模型在不同噪声水平下表现出良好的去噪性能，有效减少了阶梯效应。

Conclusion: 该双相型变增长总变差正则化模型在图像去噪中具有优越性能，是ROF模型的有效改进。

Abstract: We propose a new image denoising model based on a variable-growth total
variation regularization of double-phase type with adaptive weight. It is
designed to reduce staircasing with respect to the classical
Rudin--Osher--Fatemi model, while preserving the edges of the image in a
similar fashion. We implement the model and test its performance on synthetic
and natural images in 1D and 2D over a range of noise levels.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [191] [Creative synthesis of kinematic mechanisms](https://arxiv.org/abs/2510.03308)
*Jiong Lin,Jialong Ning,Judah Goldfeder,Hod Lipson*

Main category: cs.GR

TL;DR: 本文提出将平面连杆机构的运动综合问题转化为跨域图像生成任务，使用基于RGB图像的表示方法，通过变分自编码器(VAE)来合成未见过的运动曲线和模拟新运动学。


<details>
  <summary>Details</summary>
Motivation: 传统机械设计方法在处理复杂连杆机构时效率较低，需要探索基于图像生成的新方法来提高机械设计的自动化和创新性。

Method: 使用共享潜在空间的变分自编码器，将连杆机构的运动轨迹表示为RGB图像，通过颜色梯度编码轨迹点的绘制速度，支持基于轨迹形状和速度剖面的条件运动综合。

Result: 在三个复杂度递增的数据集上验证了方法的有效性：标准四杆机构集、四杆和曲柄滑块混合机构集、包含多环机构的复杂集。初步结果表明图像表示在生成式机械设计中的有效性。

Conclusion: 基于图像的表示方法能够在一个统一的图像生成框架内表示和综合具有转动副、移动副以及可能包含凸轮和齿轮的机构，为生成式机械设计提供了新思路。

Abstract: In this paper, we formulate the problem of kinematic synthesis for planar
linkages as a cross-domain image generation task. We develop a planar linkages
dataset using RGB image representations, covering a range of mechanisms: from
simple types such as crank-rocker and crank-slider to more complex eight-bar
linkages like Jansen's mechanism. A shared-latent variational autoencoder (VAE)
is employed to explore the potential of image generative models for
synthesizing unseen motion curves and simulating novel kinematics. By encoding
the drawing speed of trajectory points as color gradients, the same
architecture also supports kinematic synthesis conditioned on both trajectory
shape and velocity profiles. We validate our method on three datasets of
increasing complexity: a standard four-bar linkage set, a mixed set of four-bar
and crank-slider mechanisms, and a complex set including multi-loop mechanisms.
Preliminary results demonstrate the effectiveness of image-based
representations for generative mechanical design, showing that mechanisms with
revolute and prismatic joints, and potentially cams and gears, can be
represented and synthesized within a unified image generation framework.

</details>


### [192] [Universal Beta Splatting](https://arxiv.org/abs/2510.03312)
*Rong Liu,Zhongpai Gao,Benjamin Planche,Meida Chen,Van Nguyen Nguyen,Meng Zheng,Anwesa Choudhuri,Terrence Chen,Yue Wang,Andrew Feng,Ziyan Wu*

Main category: cs.GR

TL;DR: Universal Beta Splatting (UBS) 是一个统一框架，将3D高斯泼溅推广到N维各向异性Beta核，用于显式辐射场渲染。


<details>
  <summary>Details</summary>
Motivation: 传统的固定高斯基元无法灵活建模空间、角度和时间维度之间的依赖关系，需要统一的表示方法来捕捉复杂的光传输效果、处理各向异性视图相关外观和场景动态。

Method: 使用各向异性Beta核替代高斯核，Beta核能够在单个表示中可控地建模空间、角度和时间维度的依赖关系，无需辅助网络或特定颜色编码。

Result: UBS在静态、视图相关和动态基准测试中一致优于现有方法，实现了实时渲染，同时学习的Beta参数自然地将场景属性分解为可解释的分量。

Conclusion: Beta核被确立为辐射场渲染的可扩展通用基元，UBS保持了向后兼容性，将高斯泼溅作为特例进行近似。

Abstract: We introduce Universal Beta Splatting (UBS), a unified framework that
generalizes 3D Gaussian Splatting to N-dimensional anisotropic Beta kernels for
explicit radiance field rendering. Unlike fixed Gaussian primitives, Beta
kernels enable controllable dependency modeling across spatial, angular, and
temporal dimensions within a single representation. Our unified approach
captures complex light transport effects, handles anisotropic view-dependent
appearance, and models scene dynamics without requiring auxiliary networks or
specific color encodings. UBS maintains backward compatibility by approximating
to Gaussian Splatting as a special case, guaranteeing plug-in usability and
lower performance bounds. The learned Beta parameters naturally decompose scene
properties into interpretable without explicit supervision: spatial (surface
vs. texture), angular (diffuse vs. specular), and temporal (static vs.
dynamic). Our CUDA-accelerated implementation achieves real-time rendering
while consistently outperforming existing methods across static,
view-dependent, and dynamic benchmarks, establishing Beta kernels as a scalable
universal primitive for radiance field rendering. Our project website is
available at https://rongliu-leo.github.io/universal-beta-splatting/.

</details>


### [193] [Diverse Text-to-Image Generation via Contrastive Noise Optimization](https://arxiv.org/abs/2510.03813)
*Byungjun Kim,Soobin Um,Jong Chul Ye*

Main category: cs.GR

TL;DR: 本文提出了一种名为对比噪声优化的方法，通过优化初始噪声来提升文本到图像生成模型的多样性，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在强文本引导下往往输出模式相似，缺乏多样性。现有方法通常优化中间潜变量或文本条件，但效果有限且对超参数敏感。

Method: 提出对比噪声优化方法，在Tweedie数据空间中定义对比损失函数，优化一批噪声潜变量。通过对比优化使批次内实例相互排斥以最大化多样性，同时保持与参考样本的锚定以确保保真度。

Result: 在多个T2I骨干网络上的实验表明，该方法在质量-多样性帕累托前沿上表现优越，且对超参数选择具有鲁棒性。

Conclusion: 对比噪声优化是一种简单有效的预处理方法，能够显著提升文本到图像生成的多样性，同时保持图像质量。

Abstract: Text-to-image (T2I) diffusion models have demonstrated impressive performance
in generating high-fidelity images, largely enabled by text-guided inference.
However, this advantage often comes with a critical drawback: limited
diversity, as outputs tend to collapse into similar modes under strong text
guidance. Existing approaches typically optimize intermediate latents or text
conditions during inference, but these methods deliver only modest gains or
remain sensitive to hyperparameter tuning. In this work, we introduce
Contrastive Noise Optimization, a simple yet effective method that addresses
the diversity issue from a distinct perspective. Unlike prior techniques that
adapt intermediate latents, our approach shapes the initial noise to promote
diverse outputs. Specifically, we develop a contrastive loss defined in the
Tweedie data space and optimize a batch of noise latents. Our contrastive
optimization repels instances within the batch to maximize diversity while
keeping them anchored to a reference sample to preserve fidelity. We further
provide theoretical insights into the mechanism of this preprocessing to
substantiate its effectiveness. Extensive experiments across multiple T2I
backbones demonstrate that our approach achieves a superior quality-diversity
Pareto frontier while remaining robust to hyperparameter choices.

</details>


### [194] [Joint Neural SDF Reconstruction and Semantic Segmentation for CAD Models](https://arxiv.org/abs/2510.03837)
*Shen Fan,Przemyslaw Musialski*

Main category: cs.GR

TL;DR: 提出了一个简单高效的数据增强流程，通过结合基于神经SDF的CAD部件隐式重建网络和基于PartField生成监督的部分分割头，实现无需固定分类法的CAD网格分割。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于固定的分类法，无法处理任意数量部件的CAD网格。本文旨在开发一种能够处理任意部件数量、产生几何对齐标签的通用分割方法。

Method: 在Flat-CAD SDF主干网络上附加轻量级分割头，使用PartField生成的监督进行训练。该方法接受任意部件数量的网格输入，在单次前向传播中生成一致的分割标签。

Result: 在ABC数据集上评估显示，该方法在重建指标（CDL1/CDL2, F1-micro, NC）和分割指标（mIoU, Accuracy）上表现强劲。分割一致性指标显示标签平滑性良好，即使在薄壁或复杂几何的退化重建情况下，分割仍保持准确和标签一致性。

Conclusion: 该方法为语义结构化CAD网格提供了一条实用路径，无需精心策划的分类法或精确的调色板匹配。主要局限性在于边界精度，未来工作将关注边界感知训练和更高分辨率标签。

Abstract: We propose a simple, data-efficient pipeline that augments an implicit
reconstruction network based on neural SDF-based CAD parts with a
part-segmentation head trained under PartField-generated supervision. Unlike
methods tied to fixed taxonomies, our model accepts meshes with any number of
parts and produces coherent, geometry-aligned labels in a single pass. We
evaluate on randomly sampled CAD meshes from the ABC dataset with intentionally
varied part cardinalities, including over-segmented shapes, and report strong
performance across reconstruction (CDL1/CDL2, F1-micro, NC) and segmentation
(mIoU, Accuracy), together with a new Segmentation Consistency metric that
captures local label smoothness. We attach a lightweight segmentation head to
the Flat-CAD SDF trunk; on a paired evaluation it does not alter reconstruction
while providing accurate part labels for meshes with any number of parts. Even
under degraded reconstructions on thin or intricate geometries, segmentation
remains accurate and label-coherent, often preserving the correct part count.
Our approach therefore offers a practical route to semantically structured CAD
meshes without requiring curated taxonomies or exact palette matches. We
discuss limitations in boundary precision, partly due to per-face supervision,
and outline paths toward boundary-aware training and higher resolution labels.

</details>


### [195] [3Dify: a Framework for Procedural 3D-CG Generation Assisted by LLMs Using MCP and RAG](https://arxiv.org/abs/2510.04536)
*Shun-ichiro Hayashi,Daichi Mukunoki,Tetsuya Hoshino,Satoshi Ohshima,Takahiro Katagiri*

Main category: cs.GR

TL;DR: 3Dify是一个基于大语言模型的程序化3D图形生成框架，允许用户通过自然语言指令生成3D内容，支持多种DCC工具自动化和本地LLM集成。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D内容创作的技术门槛高、操作复杂的问题，让用户能够通过简单的自然语言指令来生成3D-CG内容，降低创作难度。

Method: 基于Dify平台构建，采用MCP协议自动化DCC工具操作，使用CUA方法处理不支持MCP的GUI自动化，集成RAG技术，支持用户反馈学习和本地LLM部署。

Result: 开发出了一个完整的3D-CG生成框架，能够实现自然语言驱动的3D内容创作，支持多工具协同和个性化定制。

Conclusion: 3Dify框架成功地将LLM技术应用于3D内容生成领域，为降低3D创作门槛提供了有效解决方案，并具有成本效益和可扩展性。

Abstract: This paper proposes "3Dify," a procedural 3D computer graphics (3D-CG)
generation framework utilizing Large Language Models (LLMs). The framework
enables users to generate 3D-CG content solely through natural language
instructions. 3Dify is built upon Dify, an open-source platform for AI
application development, and incorporates several state-of-the-art LLM-related
technologies such as the Model Context Protocol (MCP) and Retrieval-Augmented
Generation (RAG). For 3D-CG generation support, 3Dify automates the operation
of various Digital Content Creation (DCC) tools via MCP. When DCC tools do not
support MCP-based interaction, the framework employs the Computer-Using Agent
(CUA) method to automate Graphical User Interface (GUI) operations. Moreover,
to enhance image generation quality, 3Dify allows users to provide feedback by
selecting preferred images from multiple candidates. The LLM then learns
variable patterns from these selections and applies them to subsequent
generations. Furthermore, 3Dify supports the integration of locally deployed
LLMs, enabling users to utilize custom-developed models and to reduce both time
and monetary costs associated with external API calls by leveraging their own
computational resources.

</details>


### [196] [C3Editor: Achieving Controllable Consistency in 2D Model for 3D Editing](https://arxiv.org/abs/2510.04539)
*Zeng Tao,Zheng Ding,Zeyuan Chen,Xiang Zhang,Leizhi Li,Zhuowen Tu*

Main category: cs.GR

TL;DR: C3Editor是一个可控且一致的2D提升式3D编辑框架，通过选择性建立视图一致的2D编辑模型来解决现有方法的不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有2D提升式3D编辑方法存在不一致性挑战，主要源于缺乏视图一致的2D编辑模型和难以确保多视图编辑一致性。

Method: 首先选择GT视图及其编辑图像作为优化目标，然后在GT视图和多个视图中微调2D编辑模型以对齐GT编辑图像并确保多视图一致性，使用独立的LoRA模块进行针对性微调。

Result: 该方法在定性和定量评估中均优于现有2D提升式方法，提供更一致和可控的2D和3D编辑结果。

Conclusion: C3Editor框架通过视图一致的2D编辑模型和针对性微调策略，有效解决了3D编辑中的一致性问题，实现了更优的编辑效果。

Abstract: Existing 2D-lifting-based 3D editing methods often encounter challenges
related to inconsistency, stemming from the lack of view-consistent 2D editing
models and the difficulty of ensuring consistent editing across multiple views.
To address these issues, we propose C3Editor, a controllable and consistent
2D-lifting-based 3D editing framework. Given an original 3D representation and
a text-based editing prompt, our method selectively establishes a
view-consistent 2D editing model to achieve superior 3D editing results. The
process begins with the controlled selection of a ground truth (GT) view and
its corresponding edited image as the optimization target, allowing for
user-defined manual edits. Next, we fine-tune the 2D editing model within the
GT view and across multiple views to align with the GT-edited image while
ensuring multi-view consistency. To meet the distinct requirements of GT view
fitting and multi-view consistency, we introduce separate LoRA modules for
targeted fine-tuning. Our approach delivers more consistent and controllable 2D
and 3D editing results than existing 2D-lifting-based methods, outperforming
them in both qualitative and quantitative evaluations.

</details>


### [197] [Social Agent: Mastering Dyadic Nonverbal Behavior Generation via Conversational LLM Agents](https://arxiv.org/abs/2510.04637)
*Zeyi Zhang,Yanju Zhou,Heyuan Yao,Tenglong Ao,Xiaohang Zhan,Libin Liu*

Main category: cs.GR

TL;DR: 提出Social Agent框架，使用LLM驱动的智能体系统来生成双人对话中协调的非语言行为，结合自回归扩散模型实现逼真的手势生成和动态交互反馈。


<details>
  <summary>Details</summary>
Motivation: 解决双人对话中非语言行为（如手势）的自然生成问题，使虚拟角色能够产生上下文相关、协调一致的非语言行为，提升交互的真实感。

Method: 1. LLM驱动的智能体系统控制对话流程和行为决策；2. 基于自回归扩散模型的双人手势生成器；3. 运动检查和意图推断的反馈循环机制。

Result: 用户研究和定量评估表明，该模型显著提升了双人交互质量，生成了自然、同步的非语言行为。

Conclusion: Social Agent框架能够有效合成逼真的双人对话非语言行为，为虚拟角色交互提供了新的解决方案。

Abstract: We present Social Agent, a novel framework for synthesizing realistic and
contextually appropriate co-speech nonverbal behaviors in dyadic conversations.
In this framework, we develop an agentic system driven by a Large Language
Model (LLM) to direct the conversation flow and determine appropriate
interactive behaviors for both participants. Additionally, we propose a novel
dual-person gesture generation model based on an auto-regressive diffusion
model, which synthesizes coordinated motions from speech signals. The output of
the agentic system is translated into high-level guidance for the gesture
generator, resulting in realistic movement at both the behavioral and motion
levels. Furthermore, the agentic system periodically examines the movements of
interlocutors and infers their intentions, forming a continuous feedback loop
that enables dynamic and responsive interactions between the two participants.
User studies and quantitative evaluations show that our model significantly
improves the quality of dyadic interactions, producing natural, synchronized
nonverbal behaviors.

</details>


### [198] [Bridging Text and Video Generation: A Survey](https://arxiv.org/abs/2510.04999)
*Nilay Kumar,Priyansh Bhandari,G. Maragatham*

Main category: cs.GR

TL;DR: 本文对文本到视频（T2V）生成技术进行了全面综述，从早期GAN和VAE模型发展到混合扩散-Transformer架构，分析了模型发展、数据集、训练配置、评估指标及当前挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: T2V技术在教育、营销、娱乐和辅助技术等领域具有巨大潜力，但面临对齐、长程连贯性和计算效率等挑战。本文旨在系统梳理该领域发展历程，为未来研究提供参考。

Method: 采用综述研究方法，系统分析T2V模型从GAN、VAE到Diffusion-Transformer架构的技术演进，详细比较不同模型的训练配置、数据集和评估指标。

Result: 总结了T2V技术的发展轨迹，识别了当前模型在质量、连贯性和控制方面的局限性，并提出了更全面的感知对齐评估策略。

Conclusion: T2V技术已取得显著进展但仍面临诸多挑战，未来需要在新架构设计、评估方法和应用场景等方面继续探索，以推动该领域的进一步发展。

Abstract: Text-to-video (T2V) generation technology holds potential to transform
multiple domains such as education, marketing, entertainment, and assistive
technologies for individuals with visual or reading comprehension challenges,
by creating coherent visual content from natural language prompts. From its
inception, the field has advanced from adversarial models to diffusion-based
models, yielding higher-fidelity, temporally consistent outputs. Yet challenges
persist, such as alignment, long-range coherence, and computational efficiency.
Addressing this evolving landscape, we present a comprehensive survey of
text-to-video generative models, tracing their development from early GANs and
VAEs to hybrid Diffusion-Transformer (DiT) architectures, detailing how these
models work, what limitations they addressed in their predecessors, and why
shifts toward new architectural paradigms were necessary to overcome challenges
in quality, coherence, and control. We provide a systematic account of the
datasets, which the surveyed text-to-video models were trained and evaluated
on, and, to support reproducibility and assess the accessibility of training
such models, we detail their training configurations, including their hardware
specifications, GPU counts, batch sizes, learning rates, optimizers, epochs,
and other key hyperparameters. Further, we outline the evaluation metrics
commonly used for evaluating such models and present their performance across
standard benchmarks, while also discussing the limitations of these metrics and
the emerging shift toward more holistic, perception-aligned evaluation
strategies. Finally, drawing from our analysis, we outline the current open
challenges and propose a few promising future directions, laying out a
perspective for future researchers to explore and build upon in advancing T2V
research and applications.

</details>


### [199] [SAEdit: Token-level control for continuous image editing via Sparse AutoEncoder](https://arxiv.org/abs/2510.05081)
*Ronen Kamenetsky,Sara Dorfman,Daniel Garibi,Roni Paiss,Or Patashnik,Daniel Cohen-Or*

Main category: cs.GR

TL;DR: 提出了一种通过文本嵌入的token级别操作来实现解耦和连续控制编辑的方法，使用稀疏自编码器识别语义隔离的编辑方向


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型已成为现代图像编辑的支柱，但仅靠文本提示无法提供足够的编辑控制，需要解耦和连续控制两个关键属性

Method: 通过稀疏自编码器识别文本嵌入中的语义隔离维度，然后沿这些方向操纵嵌入来实现连续强度的编辑控制，无需修改扩散过程

Result: 实验表明该方法能够在不同属性和领域中实现直观高效的连续控制操作

Conclusion: 该方法为文本到图像编辑提供了模型无关的解耦连续控制解决方案，具有广泛适用性

Abstract: Large-scale text-to-image diffusion models have become the backbone of modern
image editing, yet text prompts alone do not offer adequate control over the
editing process. Two properties are especially desirable: disentanglement,
where changing one attribute does not unintentionally alter others, and
continuous control, where the strength of an edit can be smoothly adjusted. We
introduce a method for disentangled and continuous editing through token-level
manipulation of text embeddings. The edits are applied by manipulating the
embeddings along carefully chosen directions, which control the strength of the
target attribute. To identify such directions, we employ a Sparse Autoencoder
(SAE), whose sparse latent space exposes semantically isolated dimensions. Our
method operates directly on text embeddings without modifying the diffusion
process, making it model agnostic and broadly applicable to various image
synthesis backbones. Experiments show that it enables intuitive and efficient
manipulations with continuous control across diverse attributes and domains.

</details>


### [200] [Pulp Motion: Framing-aware multimodal camera and human motion generation](https://arxiv.org/abs/2510.05097)
*Robin Courant,Xi Wang,David Loiseaux,Marc Christie,Vicky Kalogeiton*

Main category: cs.GR

TL;DR: 本文提出了一种联合生成人类动作和相机轨迹的框架，通过屏幕空间框架作为辅助模态来确保多模态一致性，解决了传统方法将两者分离处理的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将人类动作和相机轨迹生成分开处理，忽略了电影摄影中演员表演和镜头工作之间的紧密联系。本文旨在通过联合生成来保持一致的屏幕构图。

Method: 设计了一个联合自动编码器学习共享潜在空间，通过轻量级线性变换将人类和相机潜在表示映射到框架潜在表示，并引入辅助采样来引导生成过程。

Result: 在DiT和MAR架构上的广泛实验表明，该方法能有效生成屏幕空间一致的人类-相机运动，并在文本对齐方面取得提升，产生了更具电影摄影意义的构图。

Conclusion: 该方法通过屏幕框架作为桥梁，实现了人类动作和相机轨迹的协调生成，为该任务设立了新的技术水平。

Abstract: Treating human motion and camera trajectory generation separately overlooks a
core principle of cinematography: the tight interplay between actor performance
and camera work in the screen space. In this paper, we are the first to cast
this task as a text-conditioned joint generation, aiming to maintain consistent
on-screen framing while producing two heterogeneous, yet intrinsically linked,
modalities: human motion and camera trajectories. We propose a simple,
model-agnostic framework that enforces multimodal coherence via an auxiliary
modality: the on-screen framing induced by projecting human joints onto the
camera. This on-screen framing provides a natural and effective bridge between
modalities, promoting consistency and leading to more precise joint
distribution. We first design a joint autoencoder that learns a shared latent
space, together with a lightweight linear transform from the human and camera
latents to a framing latent. We then introduce auxiliary sampling, which
exploits this linear transform to steer generation toward a coherent framing
modality. To support this task, we also introduce the PulpMotion dataset, a
human-motion and camera-trajectory dataset with rich captions, and high-quality
human motions. Extensive experiments across DiT- and MAR-based architectures
show the generality and effectiveness of our method in generating on-frame
coherent human-camera motions, while also achieving gains on textual alignment
for both modalities. Our qualitative results yield more cinematographically
meaningful framings setting the new state of the art for this task. Code,
models and data are available in our
\href{https://www.lix.polytechnique.fr/vista/projects/2025_pulpmotion_courant/}{project
page}.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [201] [VIFO: Visual Feature Empowered Multivariate Time Series Forecasting with Cross-Modal Fusion](https://arxiv.org/abs/2510.03244)
*Yanlong Wang,Hang Yu,Jian Xu,Fei Ma,Hongkang Zhang,Tongtong Feng,Zijian Zhang,Shao-Lun Huang,Danny Dongning Sun,Xiao-Ping Zhang*

Main category: cs.LG

TL;DR: VIFO是一个跨模态时间序列预测模型，通过将多元时间序列转换为图像，利用预训练的大型视觉模型提取通道间依赖关系，并与时间序列模态特征对齐融合，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模时间序列基础模型通常采用通道独立架构，忽略了重要的跨通道依赖关系；同时现有的多模态方法未能充分利用大型视觉模型来解释时空数据；不同模态信息提取的优势尚未被充分探索以提升时间序列预测性能。

Method: VIFO将多元时间序列渲染成图像，利用预训练的大型视觉模型提取复杂的跨通道模式，这些视觉特征与时间序列模态的表征进行对齐和融合。通过冻结视觉模型并仅训练其7.45%的参数，实现了高效的跨变量关系捕捉。

Result: VIFO在多个基准测试中取得了有竞争力的性能，为捕捉跨变量关系提供了一个高效且有效的解决方案。

Conclusion: VIFO通过创新的跨模态方法成功解决了时间序列预测中跨通道依赖关系捕捉的挑战，证明了视觉模态与时间序列模态融合的有效性，为时间序列分析提供了新的思路。

Abstract: Large time series foundation models often adopt channel-independent
architectures to handle varying data dimensions, but this design ignores
crucial cross-channel dependencies. Concurrently, existing multimodal
approaches have not fully exploited the power of large vision models (LVMs) to
interpret spatiotemporal data. Additionally, there remains significant
unexplored potential in leveraging the advantages of information extraction
from different modalities to enhance time series forecasting performance. To
address these gaps, we propose the VIFO, a cross-modal forecasting model. VIFO
uniquely renders multivariate time series into image, enabling pre-trained LVM
to extract complex cross-channel patterns that are invisible to
channel-independent models. These visual features are then aligned and fused
with representations from the time series modality. By freezing the LVM and
training only 7.45% of its parameters, VIFO achieves competitive performance on
multiple benchmarks, offering an efficient and effective solution for capturing
cross-variable relationships in

</details>


### [202] [Frequency-Aware Model Parameter Explorer: A new attribution method for improving explainability](https://arxiv.org/abs/2510.03245)
*Ali Yavari,Alireza Mohamadi,Elham Beydaghi,Rainer A. Leitgeb*

Main category: cs.LG

TL;DR: 提出了一种可转移的频率感知对抗攻击方法，并基于此开发了新的归因方法FAMPE，在解释深度神经网络方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决深度神经网络在真实世界噪声和对抗扰动下的可靠性问题，改进现有归因方法的效能。

Method: 提出了可转移的频率感知对抗攻击，通过高低频分量进行频率感知探索，并基于此开发了Frequency-Aware Model Parameter Explorer (FAMPE)归因方法。

Result: FAMPE在插入分数上相比当前最先进方法AttEXplore平均提升13.02%，通过消融研究验证了高低频分量在可解释性中的作用。

Conclusion: 频率感知方法有效提升了深度神经网络的可解释性，高低频分量在归因分析中都发挥重要作用。

Abstract: Ensuring the reliability of deep neural networks (DNNs) in the presence of
real world noise and intentional perturbations remains a significant challenge.
To address this, attribution methods have been proposed, though their efficacy
remains suboptimal and necessitates further refinement. In this paper, we
propose a novel category of transferable adversarial attacks, called
transferable frequency-aware attacks, enabling frequency-aware exploration via
both high-and low-frequency components. Based on this type of attacks, we also
propose a novel attribution method, named Frequency-Aware Model Parameter
Explorer (FAMPE), which improves the explainability for DNNs. Relative to the
current state-of-the-art method AttEXplore, our FAMPE attains an average gain
of 13.02% in Insertion Score, thereby outperforming existing approaches.
Through detailed ablation studies, we also investigate the role of both high-
and low-frequency components in explainability.

</details>


### [203] [Real-Time Brain Biomechanics Prediction with Neural Operators: Toward Clinically Deployable Traumatic Brain Injury Models](https://arxiv.org/abs/2510.03248)
*Anusha Agarwal,Dibakar Roy Sarkar,Somdatta Goswami*

Main category: cs.LG

TL;DR: 该研究比较了四种神经算子架构用于快速预测脑位移场，旨在实现创伤性脑损伤的实时建模。MG-FNO在精度上表现最佳，F-FNO收敛速度最快，DeepONet推理速度最快，所有方法都将计算时间从小时级降至毫秒级。


<details>
  <summary>Details</summary>
Motivation: 创伤性脑损伤是全球重大公共卫生问题，传统有限元模型计算成本高，限制了临床快速决策应用。需要开发快速、患者特异性的脑位移预测方法。

Method: 将TBI建模为算子学习问题，使用四种神经算子架构（FNO、F-FNO、MG-FNO、DeepONet）从患者MRI、MRE刚度图和人口统计学特征预测3D脑位移场。在249个MRE数据集上训练评估。

Result: MG-FNO精度最高（MSE=0.0023，94.3%空间保真度），F-FNO收敛速度比标准FNO快2倍，DeepONet推理速度最快（14.5次迭代/秒），比MG-FNO快7倍。所有方法都实现了从小时到毫秒级的计算加速。

Conclusion: 神经算子为脑变形预测提供了高效、分辨率不变的方法，为实现实时、患者特异性的TBI风险评估、临床分诊支持和防护设备优化打开了大门，展示了基于神经算子的数字孪生大脑在临床和公共卫生领域的潜力。

Abstract: Traumatic brain injury (TBI) remains a major public health concern, with over
69 million cases annually worldwide. Finite element (FE) models offer
high-fidelity predictions of brain deformation but are computationally
expensive, requiring hours per simulation and limiting their clinical utility
for rapid decision-making. This study benchmarks state-of-the-art neural
operator (NO) architectures for rapid, patient-specific prediction of brain
displacement fields, aiming to enable real-time TBI modeling in clinical and
translational settings. We formulated TBI modeling as an operator learning
problem, mapping subject-specific anatomical MRI, magnetic resonance
elastography (MRE) stiffness maps, and demographic features to full-field 3D
brain displacement predictions. Four architectures - Fourier Neural Operator
(FNO), Factorized FNO (F-FNO), Multi-Grid FNO (MG-FNO), and Deep Operator
Network (DeepONet) were trained and evaluated on 249 MRE datasets across
physiologically relevant frequencies (20 - 90 Hz). MG-FNO achieved the highest
accuracy (MSE = 0.0023, 94.3\% spatial fidelity) and preserved fine-scale
features, while F-FNO converged 2$\times$ faster than standard FNO. DeepONet
offered the fastest inference (14.5 iterations/s) with a 7$\times$
computational speed-up over MG-FNO, suggesting utility for embedded or edge
computing applications. All NOs reduced computation time from hours to
milliseconds without sacrificing anatomical realism. NOs provide an efficient,
resolution-invariant approach for predicting brain deformation, opening the
door to real-time, patient-specific TBI risk assessment, clinical triage
support, and optimization of protective equipment. These results highlight the
potential for NO-based digital twins of the human brain, enabling scalable,
on-demand biomechanical modeling in both clinical and population health
contexts.

</details>


### [204] [Universal Multi-Domain Translation via Diffusion Routers](https://arxiv.org/abs/2510.03252)
*Duc Kieu,Kien Do,Tuan Hoang,Thao Minh Le,Tung Kieu,Dang Nguyen,Thin Nguyen*

Main category: cs.LG

TL;DR: 本文提出了通用多域翻译（UMDT）框架，使用扩散路由器（DR）方法，只需K-1个配对数据集就能实现K个域之间的任意双向翻译，在多个基准测试中达到最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有多域翻译方法要么需要完全对齐的元组，要么只能处理训练中见过的域对，限制了实用性和跨域映射的范围。

Method: 提出扩散路由器（DR）框架，使用单一噪声预测器建模所有中心域与非中心域之间的翻译，通过中心域路由实现间接翻译，并引入变分下界目标和Tweedie精炼程序支持直接映射。

Result: 在三个大规模UMDT基准测试中，DR在间接和直接翻译上都取得了最先进的结果，同时降低了采样成本，并解锁了草图↔分割等新任务。

Conclusion: DR被证明是一个可扩展且通用的多域翻译框架，能够有效处理跨多个域的通用翻译任务。

Abstract: Multi-domain translation (MDT) aims to learn translations between multiple
domains, yet existing approaches either require fully aligned tuples or can
only handle domain pairs seen in training, limiting their practicality and
excluding many cross-domain mappings. We introduce universal MDT (UMDT), a
generalization of MDT that seeks to translate between any pair of $K$ domains
using only $K-1$ paired datasets with a central domain. To tackle this problem,
we propose Diffusion Router (DR), a unified diffusion-based framework that
models all central$\leftrightarrow$non-central translations with a single noise
predictor conditioned on the source and target domain labels. DR enables
indirect non-central translations by routing through the central domain. We
further introduce a novel scalable learning strategy with a variational-bound
objective and an efficient Tweedie refinement procedure to support direct
non-central mappings. Through evaluation on three large-scale UMDT benchmarks,
DR achieves state-of-the-art results for both indirect and direct translations,
while lowering sampling cost and unlocking novel tasks such as
sketch$\leftrightarrow$segmentation. These results establish DR as a scalable
and versatile framework for universal translation across multiple domains.

</details>


### [205] [Rethinking Inter-LoRA Orthogonality in Adapter Merging: Insights from Orthogonal Monte Carlo Dropout](https://arxiv.org/abs/2510.03262)
*Andi Zhang,Xuan Ding,Haofan Wang,Steven McDonagh,Samuel Kaski*

Main category: cs.LG

TL;DR: 提出了正交蒙特卡洛丢弃方法，在合并LoRA模块时强制正交性以避免语义干扰，但实证发现正交性本身不足以实现语义组合性


<details>
  <summary>Details</summary>
Motivation: LoRA模块合并时语义向量会相互干扰，需要一种机制来保证正交性从而避免直接干扰

Method: 正交蒙特卡洛丢弃方法，在理论层面和运行时层面强制保证合并的LoRA模块保持正交性，且不增加额外时间复杂度

Result: 实证分析表明，正交性本身并不能带来语义解耦或组合性，这与之前组合适配研究中的发现相矛盾

Conclusion: 仅靠LoRA间的正交性可能不足以实现真正的语义组合性，需要重新审视其在适配器合并中的作用

Abstract: We propose Orthogonal Monte Carlo Dropout, a mechanism that enforces strict
orthogonality when combining sparse semantic vectors without extra time
complexity. LoRA, a popular fine-tuning method for large models, typically
trains a module to represent a specific concept such as an object or a style.
When multiple LoRAs are merged, for example to generate an object in a
particular style, their semantic vectors may interfere with each other. Our
method guarantees, at the theoretical and runtime levels, that merged LoRAs
remain orthogonal and thus free from direct interference. However, empirical
analysis reveals that such orthogonality does not lead to the semantic
disentanglement or compositionality highlighted in prior work on compositional
adaptation. This finding suggests that inter-LoRA orthogonality alone may be
insufficient for achieving true semantic compositionality, prompting a
re-examination of its role in adapter merging.

</details>


### [206] [SDQ-LLM: Sigma-Delta Quantization for 1-bit LLMs of any size](https://arxiv.org/abs/2510.03275)
*Junhao Xia,Ming Zhao,Limin Xiao,Xiujun Zhang*

Main category: cs.LG

TL;DR: SDQ-LLM是一种新型的1位LLM量化框架，通过Sigma-Delta量化器和过采样技术实现极低比特量化，同时保持语言推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型面临显著的计算和内存挑战，极低比特量化对于其高效部署至关重要。

Method: 使用上采样结合Sigma-Delta量化器对LLM权重进行二值化或三值化，将高精度参数编码为1位或1.58位表示，并用加法替代线性层中的乘法运算。结合Hadamard权重平滑和基于权重重力的细粒度OSR分配策略MultiOSR。

Result: 在OPT和LLaMA模型系列上的广泛实验表明，SDQ-LLM即使在高度激进的低OSR设置下也能实现更高效和高精度的性能。

Conclusion: SDQ-LLM框架为LLM的极低比特量化提供了一种有效的解决方案，能够在保持模型精度的同时显著提升推理效率。

Abstract: Large language models (LLMs) face significant computational and memory
challenges, making extremely low-bit quantization crucial for their efficient
deployment. In this work, we introduce SDQ-LLM: Sigma-Delta Quantization for
1-bit LLMs of any size, a novel framework that enables extremely low-bit
quantization of LLMs while preserving their linguistic reasoning capabilities.
A distinctive feature of SDQ-LLM is the continuous adjustability of the
Over-Sampling Ratio (OSR), enabling dynamic adaptation to memory or VRAM
constraints by selecting fractional OSR (e.g. 2.5 times) for an optimal
trade-off between model size and accuracy. SDQ-LLM uses upsampling combined
with Sigma-Delta Quantizer to binarize or ternarize LLMs weights, encoding
high-precision parameters into 1-bit or 1.58-bit representations, replacing the
multiplication operations within linear layers with addition. This approach
significantly enhances inference efficiency under extremely low-bit
quantization. To further reduce the loss of quantization precision, we
incorporate Hadamard-based weight smoothing prior to quantization, improving
the stability and robustness of the weight representations. Furthermore, to
fully leverage the continuity of the OSR and reduce precision loss, recognizing
the correlation between quantization sensitivity and weight variance, we
propose a fine-grained, layer- and linear-wise OSR allocation strategy,
MultiOSR. This strategy distributes OSR both across layers and within each
layer, based on weight variance and parameter scale. Finally, extensive
experiments on OPT and LLaMA model families demonstrate that SDQ-LLM achieves a
more efficient and high-precision performance even under highly aggressive
low-OSR settings. Our code is available at
https://github.com/Dreamlittlecat/LLM-Quant-Factory.

</details>


### [207] [Revoking Amnesia: RL-based Trajectory Optimization to Resurrect Erased Concepts in Diffusion Models](https://arxiv.org/abs/2510.03302)
*Daiheng Gao,Nanxiang Jiang,Andi Zhang,Shilin Lu,Yufei Tang,Wenbo Zhou,Weiming Zhang,Zhaoxin Fan*

Main category: cs.LG

TL;DR: 现有概念擦除方法在新型扩散模型中效果下降，研究发现这些方法并非真正移除概念，而是通过偏置采样轨迹制造遗忘假象。作者提出RevAm框架，无需修改模型权重即可通过轨迹优化复活被擦除的概念。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型架构演进到Flux等新一代模型，现有概念擦除方法（如ESD、UCE、AC）效果下降，需要揭示其真正机制并评估擦除安全性。

Method: 提出RevAm（Revoking Amnesia）框架，基于强化学习的轨迹优化方法，通过动态引导去噪过程来复活被擦除的概念。该方法将Group Relative Policy Optimization（GRPO）适配到扩散模型中，通过轨迹级奖励探索多样恢复轨迹。

Result: 实验表明RevAm能够以10倍计算时间减少实现更优的概念复活保真度，暴露了当前安全机制的关键漏洞。

Conclusion: 当前基于轨迹操纵的擦除技术存在根本可逆性，需要开发超越轨迹操纵的更鲁棒擦除技术，区分表面安全性与真正的概念移除。

Abstract: Concept erasure techniques have been widely deployed in T2I diffusion models
to prevent inappropriate content generation for safety and copyright
considerations. However, as models evolve to next-generation architectures like
Flux, established erasure methods (\textit{e.g.}, ESD, UCE, AC) exhibit
degraded effectiveness, raising questions about their true mechanisms. Through
systematic analysis, we reveal that concept erasure creates only an illusion of
``amnesia": rather than genuine forgetting, these methods bias sampling
trajectories away from target concepts, making the erasure fundamentally
reversible. This insight motivates the need to distinguish superficial safety
from genuine concept removal. In this work, we propose \textbf{RevAm}
(\underline{Rev}oking \underline{Am}nesia), an RL-based trajectory optimization
framework that resurrects erased concepts by dynamically steering the denoising
process without modifying model weights. By adapting Group Relative Policy
Optimization (GRPO) to diffusion models, RevAm explores diverse recovery
trajectories through trajectory-level rewards, overcoming local optima that
limit existing methods. Extensive experiments demonstrate that RevAm achieves
superior concept resurrection fidelity while reducing computational time by
10$\times$, exposing critical vulnerabilities in current safety mechanisms and
underscoring the need for more robust erasure techniques beyond trajectory
manipulation.

</details>


### [208] [Conditional Pseudo-Supervised Contrast for Data-Free Knowledge Distillation](https://arxiv.org/abs/2510.03375)
*Renrong Shao,Wei Zhang,Jun wang*

Main category: cs.LG

TL;DR: 提出了一种新的无数据知识蒸馏方法CPSC-DFKD，通过条件生成对抗网络生成类别特定的多样化图像，改进生成器模块以区分不同类别分布，并基于师生视角提出伪监督对比学习来增强多样性。


<details>
  <summary>Details</summary>
Motivation: 当前无数据知识蒸馏方法存在三个主要问题：1）缺乏伪监督学习范式；2）无法区分不同类别样本分布，产生模糊样本；3）无法优化类别多样性样本，影响学生模型性能。

Method: 使用条件生成对抗网络生成类别特定的多样化图像，改进生成器模块以更好区分类别分布，并提出基于师生视角的伪监督对比学习来增强样本多样性。

Result: 在三个常用数据集上的综合实验验证了CPSC-DFKD对学生模型和生成器性能的提升。

Conclusion: CPSC-DFKD通过引入条件生成和伪监督对比学习，有效解决了当前无数据知识蒸馏方法的局限性，在保持隐私保护的同时实现了更好的模型压缩和传输效果。

Abstract: Data-free knowledge distillation~(DFKD) is an effective manner to solve model
compression and transmission restrictions while retaining privacy protection,
which has attracted extensive attention in recent years. Currently, the
majority of existing methods utilize a generator to synthesize images to
support the distillation. Although the current methods have achieved great
success, there are still many issues to be explored. Firstly, the outstanding
performance of supervised learning in deep learning drives us to explore a
pseudo-supervised paradigm on DFKD. Secondly, current synthesized methods
cannot distinguish the distributions of different categories of samples, thus
producing ambiguous samples that may lead to an incorrect evaluation by the
teacher. Besides, current methods cannot optimize the category-wise diversity
samples, which will hinder the student model learning from diverse samples and
further achieving better performance. In this paper, to address the above
limitations, we propose a novel learning paradigm, i.e., conditional
pseudo-supervised contrast for data-free knowledge distillation~(CPSC-DFKD).
The primary innovations of CPSC-DFKD are: (1) introducing a conditional
generative adversarial network to synthesize category-specific diverse images
for pseudo-supervised learning, (2) improving the modules of the generator to
distinguish the distributions of different categories, and (3) proposing
pseudo-supervised contrastive learning based on teacher and student views to
enhance diversity. Comprehensive experiments on three commonly-used datasets
validate the performance lift of both the student and generator brought by
CPSC-DFKD. The code is available at https://github.com/RoryShao/CPSC-DFKD.git

</details>


### [209] [Longitudinal Flow Matching for Trajectory Modeling](https://arxiv.org/abs/2510.03569)
*Mohammad Mohaiminul Islam,Thijs P. Kuipers,Sharvaree Vadgama,Coen de Vente,Afsana Khan,Clara I. Sánchez,Erik J. Bekkers*

Main category: cs.LG

TL;DR: 提出IMMFM框架，通过多时间点联合优化学习连续随机动力学，解决稀疏采样高维轨迹的学习问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在处理稀疏采样和高维序列数据时，通常只能学习两两状态转移，难以捕捉完整动态过程

Method: 使用分段二次插值路径作为流匹配的平滑目标，联合优化漂移项和数据驱动的扩散系数，理论保证学习稳定性

Result: 在合成基准和真实神经影像数据集上，IMMFM在预测精度和下游任务中优于现有方法

Conclusion: IMMFM能有效捕捉内在随机性，处理不规则稀疏采样，生成特定对象的轨迹

Abstract: Generative models for sequential data often struggle with sparsely sampled
and high-dimensional trajectories, typically reducing the learning of dynamics
to pairwise transitions. We propose \textit{Interpolative Multi-Marginal Flow
Matching} (IMMFM), a framework that learns continuous stochastic dynamics
jointly consistent with multiple observed time points. IMMFM employs a
piecewise-quadratic interpolation path as a smooth target for flow matching and
jointly optimizes drift and a data-driven diffusion coefficient, supported by a
theoretical condition for stable learning. This design captures intrinsic
stochasticity, handles irregular sparse sampling, and yields subject-specific
trajectories. Experiments on synthetic benchmarks and real-world longitudinal
neuroimaging datasets show that IMMFM outperforms existing methods in both
forecasting accuracy and further downstream tasks.

</details>


### [210] [Efficient Test-Time Scaling for Small Vision-Language Models](https://arxiv.org/abs/2510.03574)
*Mehmet Onurcan Kaya,Desmond Elliott,Dim P. Papadopoulos*

Main category: cs.LG

TL;DR: 本文提出两种高效的测试时扩展策略TTAug和TTAdapt，通过利用模型内部特征而非外部监督，在小视觉语言模型上实现性能提升并保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 小视觉语言模型计算效率高但泛化能力和下游任务性能较弱，现有测试时扩展方法通常计算量大，与小型模型的资源高效设计目标相矛盾。

Method: 提出两种策略：(i)测试时增强(TTAug)：生成多个增强输入并在token级别聚合输出，无需参数更新；(ii)测试时适应(TTAdapt)：使用TTAug的共识伪标签在推理过程中自适应模型参数。

Result: 在九个基准测试上的广泛实验表明，该方法在保持适合资源受限环境的计算效率的同时，实现了持续的性能改进。

Conclusion: 该方法在不同规模模型和不同VLM之间具有通用性，无需额外调优即可实现性能提升。

Abstract: Small Vision-Language Models (VLMs) provide a computationally efficient
alternative to larger models, at the cost of weaker generalization abilities
and downstream task performance. These shortcomings could be addressed by
test-time scaling techniques, but existing methods are typically
computationally demanding, contradicting the resource-efficient design goals of
small models. To address these limitations, we propose two novel and efficient
test-time scaling strategies that leverage the model-internal features rather
than external supervision: (i) Test-Time Augmentation (TTAug), which generates
multiple augmented inputs and aggregates outputs at the token level without
parameter updates, and (ii) Test-Time Adaptation (TTAdapt), which adapts model
parameters during inference using consensus-based pseudolabels from TTAug.
Through extensive experiments across nine benchmarks, we demonstrate consistent
performance improvements while maintaining computational efficiency suitable
for resource-constrained environments. The generality of our approach is
demonstrated both within models at different scales and across different VLMs
without additional tuning.

</details>


### [211] [Using predefined vector systems as latent space configuration for neural network supervised training on data with arbitrarily large number of classes](https://arxiv.org/abs/2510.04090)
*Nikita Gabdullin*

Main category: cs.LG

TL;DR: 提出一种独立于类别数量的神经网络训练方法，使用预定义向量系统作为目标潜空间配置，实现统一架构训练


<details>
  <summary>Details</summary>
Motivation: 监督学习方法在分类任务中参数数量依赖于类别数量，限制了在类别数量极大或未知情况下的应用

Method: 使用An根系统的随机扰动向量作为目标潜空间配置，通过匹配神经网络预测与预定义向量来训练编码器和视觉变换器

Result: 在Cinic-10和ImageNet-1K数据集上成功训练，并在128万类别的数据集上验证了方法的有效性

Conclusion: 该方法适用于极端多类别场景，并在持续学习和神经网络蒸馏方面具有潜在应用价值

Abstract: Supervised learning (SL) methods are indispensable for neural network (NN)
training used to perform classification tasks. While resulting in very high
accuracy, SL training often requires making NN parameter number dependent on
the number of classes, limiting their applicability when the number of classes
is extremely large or unknown in advance. In this paper we propose a
methodology that allows one to train the same NN architecture regardless of the
number of classes. This is achieved by using predefined vector systems as the
target latent space configuration (LSC) during NN training. We discuss the
desired properties of target configurations and choose randomly perturbed
vectors of An root system for our experiments. These vectors are used to
successfully train encoders and visual transformers (ViT) on Cinic-10 and
ImageNet-1K in low- and high-dimensional cases by matching NN predictions with
the predefined vectors. Finally, ViT is trained on a dataset with 1.28 million
classes illustrating the applicability of the method to training on datasets
with extremely large number of classes. In addition, potential applications of
LSC in lifelong learning and NN distillation are discussed illustrating
versatility of the proposed methodology.

</details>


### [212] [DoRAN: Stabilizing Weight-Decomposed Low-Rank Adaptation via Noise Injection and Auxiliary Networks](https://arxiv.org/abs/2510.04331)
*Nghiem T. Diep,Hien Dang,Tuan Truong,Tan Dinh,Huy Nguyen,Nhat Ho*

Main category: cs.LG

TL;DR: DoRAN是DoRA的改进版本，通过噪声注入和动态低秩矩阵生成来提升训练稳定性和样本效率，在视觉和语言基准测试中优于现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: DoRA虽然改进了LoRA的学习能力和训练稳定性，但仍有进一步优化的空间，特别是在训练稳定性和样本效率方面。

Method: 1）在DoRA权重分解的分母中注入噪声作为自适应正则化器；2）用辅助网络动态生成低秩矩阵，实现跨层参数耦合。

Result: 在视觉和语言基准测试中，DoRAN一致优于LoRA、DoRA和其他PEFT基线方法。

Conclusion: 噪声正则化和网络参数生成的结合为基础模型的稳健高效微调提供了有前景的方向。

Abstract: Parameter-efficient fine-tuning (PEFT) methods have become the standard
paradigm for adapting large-scale models. Among these techniques,
Weight-Decomposed Low-Rank Adaptation (DoRA) has been shown to improve both the
learning capacity and training stability of the vanilla Low-Rank Adaptation
(LoRA) method by explicitly decomposing pre-trained weights into magnitude and
directional components. In this work, we propose DoRAN, a new variant of DoRA
designed to further stabilize training and boost the sample efficiency of DoRA.
Our approach includes two key stages: (i) injecting noise into the denominator
of DoRA's weight decomposition, which serves as an adaptive regularizer to
mitigate instabilities; and (ii) replacing static low-rank matrices with
auxiliary networks that generate them dynamically, enabling parameter coupling
across layers and yielding better sample efficiency in both theory and
practice. Comprehensive experiments on vision and language benchmarks show that
DoRAN consistently outperforms LoRA, DoRA, and other PEFT baselines. These
results underscore the effectiveness of combining stabilization through
noise-based regularization with network-based parameter generation, offering a
promising direction for robust and efficient fine-tuning of foundation models.

</details>


### [213] [Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions](https://arxiv.org/abs/2510.04417)
*Wenyuan Zhao,Adithya Balachandran,Chao Tian,Paul Pu Liang*

Main category: cs.LG

TL;DR: 本文提出了一种高效的基于高斯假设的部分信息分解方法（GPID），通过梯度优化算法和编码器转换技术，解决了传统PID方法在连续高维模态数据中的计算成本和精度问题。


<details>
  <summary>Details</summary>
Motivation: 传统部分信息分解方法依赖联合分布优化，对于连续高维模态数据计算成本高且精度不足。需要一种更高效准确的方法来量化多模态信息交互。

Method: 提出GPID框架，利用高斯分布假设简化优化问题；开发梯度优化算法提高计算效率；设计信息保持编码器将非高斯数据转换为高斯分布；解决GPID联合高斯解的最优性问题。

Result: 在合成数据实验中，该方法比现有基线提供更准确高效的PID估计；在大型多模态基准测试中验证了其在真实应用中的实用性。

Conclusion: GPID方法为多模态数据分析提供了更有效的工具，能够准确量化信息交互并辅助高性能模型选择，解决了传统方法的计算瓶颈。

Abstract: The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

</details>


### [214] [Real-time Prediction of Urban Sound Propagation with Conditioned Normalizing Flows](https://arxiv.org/abs/2510.04510)
*Achim Eckerle,Martin Spitznagel,Janis Keuper*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件归一化流（Full-Glow）的实时城市噪声预测模型，能够快速生成符合标准的声压地图，比传统物理求解器快2000倍以上，并在非视距场景下提高了24%的准确性。


<details>
  <summary>Details</summary>
Motivation: 城市噪声预测对公共健康和城市规划至关重要，但传统物理求解器速度太慢，无法满足实时交互式"假设分析"需求。欧盟环境噪声指令要求定期更新噪声地图和行动计划，需要快速计算工具。

Method: 使用条件归一化流（Full-Glow）模型，从2D城市布局实时生成256x256的声压地图。模型在单个RTX 4090显卡上运行，支持基线、衍射和反射三种声学场景。

Result: 模型生成速度比参考求解器快2000倍以上，在非视距场景下达到0.65 dB平均绝对误差，比之前深度学习模型精度提高24%。模型能够准确再现衍射和干涉模式。

Conclusion: 该模型为城市规划、合规映射和运营管理提供了实用的实时计算引擎，支持源位置或几何形状变化的即时重新计算，适用于临时道路封闭、夜间工作评估等场景。

Abstract: Accurate and fast urban noise prediction is pivotal for public health and for
regulatory workflows in cities, where the Environmental Noise Directive
mandates regular strategic noise maps and action plans, often needed in
permission workflows, right-of-way allocation, and construction scheduling.
Physics-based solvers are too slow for such time-critical, iterative "what-if"
studies. We evaluate conditional Normalizing Flows (Full-Glow) for generating
for generating standards-compliant urban sound-pressure maps from 2D urban
layouts in real time per 256x256 map on a single RTX 4090), enabling
interactive exploration directly on commodity hardware. On datasets covering
Baseline, Diffraction, and Reflection regimes, our model accelerates map
generation by >2000 times over a reference solver while improving NLoS accuracy
by up to 24% versus prior deep models; in Baseline NLoS we reach 0.65 dB MAE
with high structural fidelity. The model reproduces diffraction and
interference patterns and supports instant recomputation under source or
geometry changes, making it a practical engine for urban planning, compliance
mapping, and operations (e.g., temporary road closures, night-work variance
assessments).

</details>


### [215] [Post-training quantization of vision encoders needs prefixing registers](https://arxiv.org/abs/2510.04547)
*Seunghyeon Kim,Jinho Kim,Taesun Yeom,Wonpyo Park,Kyuyeun Kim,Jaeho Lee*

Main category: cs.LG

TL;DR: RegCache是一种无需训练的算法，通过引入前缀令牌来缓解视觉编码器中的异常值问题，使得量化后的模型在8位精度下保持更高准确率。


<details>
  <summary>Details</summary>
Motivation: Transformer视觉编码器（如CLIP）在多模态应用中需要实时处理大量视觉数据，降低推理成本至关重要。后训练量化是实用方法，但由于大规模激活值中的异常值，即使在8位精度下仍具挑战性。

Method: 提出RegCache算法，通过添加易产生异常值但语义无意义的前缀令牌来防止其他令牌出现异常值。针对视觉编码器异常值与语言模型不同的特性，创新性地提出中间层前缀化和令牌删除技术。

Result: 实验表明该方法在文本监督和自监督视觉编码器上都能一致提升量化模型的准确率。

Conclusion: RegCache有效解决了视觉编码器量化中的异常值问题，为降低多模态应用推理成本提供了实用解决方案。

Abstract: Transformer-based vision encoders -- such as CLIP -- are central to
multimodal intelligence, powering applications from autonomous web agents to
robotic control. Since these applications often demand real-time processing of
massive visual data, reducing the inference cost of vision encoders is
critical. Post-training quantization offers a practical path, but remains
challenging even at 8-bit precision due to massive-scale activations (i.e.,
outliers). In this work, we propose $\textit{RegCache}$, a training-free
algorithm to mitigate outliers in vision encoders, enabling quantization with
significantly smaller accuracy drops. The proposed RegCache introduces
outlier-prone yet semantically meaningless prefix tokens to the target vision
encoder, which prevents other tokens from having outliers. Notably, we observe
that outliers in vision encoders behave differently from those in language
models, motivating two technical innovations: middle-layer prefixing and token
deletion. Experiments show that our method consistently improves the accuracy
of quantized models across both text-supervised and self-supervised vision
encoders.

</details>


### [216] [SONA: Learning Conditional, Unconditional, and Mismatching-Aware Discriminator](https://arxiv.org/abs/2510.04576)
*Yuhta Takida,Satoshi Hayakawa,Takashi Shibuya,Masaaki Imaizumi,Naoki Murata,Bac Nguyen,Toshimitsu Uesaka,Chieh-Hsin Lai,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: 提出一种新的GAN判别器设计SONA，通过分离自然度和对齐度的评估，结合自适应权重机制，解决了条件生成中真实性和条件对齐的平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成对抗网络在判别器中难以同时平衡样本的真实性和条件对齐这两个目标，导致生成质量受限。

Method: 提出SONA判别器设计，包含三个关键能力：无条件判别、匹配感知监督和自适应权重。使用分离的自然度和对齐度投影，配合专门的损失函数和自适应权重机制。

Result: 在类条件生成任务中，SONA在样本质量和条件对齐方面优于现有最先进方法。在文本到图像生成任务中也表现出色。

Conclusion: SONA方法具有通用性和鲁棒性，能够有效解决条件生成中的真实性与对齐平衡问题。

Abstract: Deep generative models have made significant advances in generating complex
content, yet conditional generation remains a fundamental challenge. Existing
conditional generative adversarial networks often struggle to balance the dual
objectives of assessing authenticity and conditional alignment of input samples
within their conditional discriminators. To address this, we propose a novel
discriminator design that integrates three key capabilities: unconditional
discrimination, matching-aware supervision to enhance alignment sensitivity,
and adaptive weighting to dynamically balance all objectives. Specifically, we
introduce Sum of Naturalness and Alignment (SONA), which employs separate
projections for naturalness (authenticity) and alignment in the final layer
with an inductive bias, supported by dedicated objective functions and an
adaptive weighting mechanism. Extensive experiments on class-conditional
generation tasks show that \ours achieves superior sample quality and
conditional alignment compared to state-of-the-art methods. Furthermore, we
demonstrate its effectiveness in text-to-image generation, confirming the
versatility and robustness of our approach.

</details>


### [217] [On Structured State-Space Duality](https://arxiv.org/abs/2510.04944)
*Jerry Yao-Chieh Hu,Xiwen Zhang,Weimin Wu,Han Liu*

Main category: cs.LG

TL;DR: SSD揭示了结构化状态空间模型与掩码注意力机制之间的等价性，将标量恒等状态矩阵扩展到一般对角SSM，建立了SSM与1-半可分掩码注意力的充要条件，但无法扩展到标准softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 弥合循环SSM与Transformer之间的桥梁，为表达力强且高效的序列模型拓宽设计空间。

Method: 将SSD从标量恒等情况扩展到一般对角SSM，证明对角SSM在保持训练复杂度下界的同时支持更丰富的动态特性，建立SSM与1-半可分掩码注意力等价的充要条件。

Result: 对角SSM匹配标量情况的训练复杂度下界且支持更丰富动态，确立了SSM与1-半可分掩码注意力的等价条件，但发现无法扩展到标准softmax注意力。

Conclusion: 研究结果加强了循环SSM与Transformer之间的联系，为设计表达力强且高效的序列模型提供了更广阔的设计空间。

Abstract: Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [218] [Super-resolution image projection over an extended depth of field using a diffractive decoder](https://arxiv.org/abs/2510.03938)
*Hanlong Chen,Cagatay Isil,Tianyi Gan,Mona Jarrahi,Aydogan Ozcan*

Main category: physics.optics

TL;DR: 提出了一种混合图像投影系统，结合CNN数字编码器和全光学衍射解码器，实现扩展景深和超分辨率投影，同时降低数据存储和传输需求。


<details>
  <summary>Details</summary>
Motivation: 传统图像投影系统需要在数据存储、计算和传输效率与高空间带宽积输出之间取得平衡，现有系统往往功耗较高或景深有限。

Method: 使用CNN编码器将输入图像压缩为紧凑相位表示，通过低分辨率投影器显示，再由被动衍射光学解码器进行全光学图像重建，实现像素超分辨和扩展景深。

Result: 在THz频谱验证中，系统实现约267倍波长的扩展景深，每个横向平面提供约16倍空间带宽积提升，且光学解码器无需额外功耗。

Conclusion: 该混合投影架构可显著降低显示系统的数据存储和传输需求，其原理可扩展到光学计量和显微镜等应用领域。

Abstract: Image projection systems must be efficient in data storage, computation and
transmission while maintaining a large space-bandwidth-product (SBP) at their
output. Here, we introduce a hybrid image projection system that achieves
extended depth-of-field (DOF) with improved resolution, combining a
convolutional neural network (CNN)-based digital encoder with an all-optical
diffractive decoder. A CNN-based encoder compresses input images into compact
phase representations, which are subsequently displayed by a low-resolution
(LR) projector and processed by an analog diffractive decoder for all-optical
image reconstruction. This optical decoder is completely passive, designed to
synthesize pixel super-resolved image projections that feature an extended DOF
while eliminating the need for additional power consumption for super-resolved
image reconstruction. Our pixel super-resolution (PSR) image projection system
demonstrates high-fidelity image synthesis over an extended DOF of ~267xW,
where W is the illumination wavelength, concurrently offering up to ~16-fold
SBP improvement at each lateral plane. The proof of concept of this approach is
validated through an experiment conducted in the THz spectrum, and the system
is scalable across different parts of the electromagnetic spectrum. This image
projection architecture can reduce data storage and transmission requirements
for display systems without imposing additional power constraints on the
optical decoder. Beyond extended DOF PSR image projection, the underlying
principles of this approach can be extended to various applications, including
optical metrology and microscopy.

</details>
