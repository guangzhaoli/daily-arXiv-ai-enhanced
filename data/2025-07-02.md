<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 99]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.CR](#cs.CR) [Total: 1]
- [eess.IV](#eess.IV) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Moment Sampling in Video LLMs for Long-Form Video QA](https://arxiv.org/abs/2507.00033)
*Mustafa Chasmai,Gauri Jagatap,Gouthaman KV,Grant Van Horn,Subhransu Maji,Andrea Fanelli*

Main category: cs.CV

TL;DR: 提出了一种名为“moment sampling”的新方法，通过文本到视频时刻检索模型指导帧采样，提升长视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长视频中表现不佳，帧子采样常丢失关键帧或包含冗余信息，影响模型准确性和效率。

Method: 使用轻量级时刻检索模型优先选择与问题最相关的帧，提出“moment sampling”方法。

Result: 在四个长视频问答数据集和四种先进视频大语言模型上验证了方法的有效性。

Conclusion: “moment sampling”方法显著提升了长视频问答的性能，同时减少了冗余计算。

Abstract: Recent advancements in video large language models (Video LLMs) have
significantly advanced the field of video question answering (VideoQA). While
existing methods perform well on short videos, they often struggle with
long-range reasoning in longer videos. To scale Video LLMs for longer video
content, frame sub-sampling (selecting frames at regular intervals) is commonly
used. However, this approach is suboptimal, often leading to the loss of
crucial frames or the inclusion of redundant information from multiple similar
frames. Missing key frames impairs the model's ability to answer questions
accurately, while redundant frames lead the model to focus on irrelevant video
segments and increase computational resource consumption. In this paper, we
investigate the use of a general-purpose text-to-video moment retrieval model
to guide the frame sampling process. We propose "moment sampling", a novel,
model-agnostic approach that enables the model to select the most relevant
frames according to the context of the question. Specifically, we employ a
lightweight moment retrieval model to prioritize frame selection. By focusing
on the frames most pertinent to the given question, our method enhances
long-form VideoQA performance in Video LLMs. Through extensive experiments on
four long-form VideoQA datasets, using four state-of-the-art Video LLMs, we
demonstrate the effectiveness of the proposed approach.

</details>


### [2] [Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay](https://arxiv.org/abs/2507.00042)
*Xinrun Xu,Jianwen Yang,Qiuhong Zhang,Zhanbiao Lian,Zhiming Ding,Shan Jiang*

Main category: cs.CV

TL;DR: ER-EMU算法通过自适应经验回放和领域距离度量选择历史数据，有效缓解了交通监控中云边协作目标检测的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 动态交通环境中，模型在适应新数据分布时会遗忘旧知识，现有方法无法高效利用历史数据。

Method: 提出ER-EMU算法，结合FIFO经验缓冲区和DDM-ES算法，利用MK-MMD度量领域差异，选择最不相似的历史数据。

Result: 在Bellevue交通视频数据集上，ER-EMU显著提升了多种云边协作目标检测框架的性能。

Conclusion: ER-EMU通过优化历史数据选择和多样性训练，有效平衡了知识保留和新领域适应。

Abstract: Continually adapting edge models in cloud-edge collaborative object detection
for traffic monitoring suffers from catastrophic forgetting, where models lose
previously learned knowledge when adapting to new data distributions. This is
especially problematic in dynamic traffic environments characterised by
periodic variations (e.g., day/night, peak hours), where past knowledge remains
valuable. Existing approaches like experience replay and visual prompts offer
some mitigation, but struggle to effectively prioritize and leverage historical
data for optimal knowledge retention and adaptation. Specifically, simply
storing and replaying all historical data can be inefficient, while treating
all historical experiences as equally important overlooks their varying
relevance to the current domain. This paper proposes ER-EMU, an edge model
update algorithm based on adaptive experience replay, to address these
limitations. ER-EMU utilizes a limited-size experience buffer managed using a
First-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based
Experience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel
maximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target
domains, prioritizing the selection of historical data that is most dissimilar
to the current target domain. This ensures training diversity and facilitates
the retention of knowledge from a wider range of past experiences, while also
preventing overfitting to the new domain. The experience buffer is also updated
using a simple random sampling strategy to maintain a balanced representation
of previous domains. Experiments on the Bellevue traffic video dataset,
involving repeated day/night cycles, demonstrate that ER-EMU consistently
improves the performance of several state-of-the-art cloud-edge collaborative
object detection frameworks.

</details>


### [3] [MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations](https://arxiv.org/abs/2507.00043)
*Mehmet Yigit Avci,Pedro Borges,Paul Wright,Mehmet Yigitsoy,Sebastien Ourselin,Jorge Cardoso*

Main category: cs.CV

TL;DR: MR-CLIP是一种多模态对比学习框架，通过将MRI图像与DICOM元数据对齐，学习对比感知表示，无需依赖人工标签。


<details>
  <summary>Details</summary>
Motivation: MRI扫描的准确解释依赖于对图像对比度的精确理解，但现有标签粗糙且元数据不完整、嘈杂或不一致，这阻碍了临床应用。

Method: 提出MR-CLIP框架，利用对比学习对齐MRI图像和DICOM元数据，学习对比感知表示。

Result: 在多样化的临床数据集上训练，MR-CLIP能够捕捉对比度变化，支持跨模态检索和对比分类。

Conclusion: MR-CLIP展示了在临床应用中扩展的潜力，代码和权重已公开。

Abstract: Accurate interpretation of Magnetic Resonance Imaging scans in clinical
systems is based on a precise understanding of image contrast. This contrast is
primarily governed by acquisition parameters, such as echo time and repetition
time, which are stored in the DICOM metadata. To simplify contrast
identification, broad labels such as T1-weighted or T2-weighted are commonly
used, but these offer only a coarse approximation of the underlying acquisition
settings. In many real-world datasets, such labels are entirely missing,
leaving raw acquisition parameters as the only indicators of contrast. Adding
to this challenge, the available metadata is often incomplete, noisy, or
inconsistent. The lack of reliable and standardized metadata complicates tasks
such as image interpretation, retrieval, and integration into clinical
workflows. Furthermore, robust contrast-aware representations are essential to
enable more advanced clinical applications, such as achieving
modality-invariant representations and data harmonization. To address these
challenges, we propose MR-CLIP, a multimodal contrastive learning framework
that aligns MR images with their DICOM metadata to learn contrast-aware
representations, without relying on manual labels. Trained on a diverse
clinical dataset that spans various scanners and protocols, MR-CLIP captures
contrast variations across acquisitions and within scans, enabling
anatomy-invariant representations. We demonstrate its effectiveness in
cross-modal retrieval and contrast classification, highlighting its scalability
and potential for further clinical applications. The code and weights are
publicly available at https://github.com/myigitavci/MR-CLIP.

</details>


### [4] [HistoART: Histopathology Artifact Detection and Reporting Tool](https://arxiv.org/abs/2507.00044)
*Seyed Kahaki,Alexander R. Webber,Ghada Zamzmi,Adarsh Subbaswamy,Rucha Deshpande,Aldo Badano*

Main category: cs.CV

TL;DR: 论文提出了三种检测全切片图像（WSI）中伪影的方法，并比较了它们的性能，其中基于基础模型的方法表现最佳。


<details>
  <summary>Details</summary>
Motivation: 全切片图像在癌症诊断中广泛应用，但伪影会影响分析准确性，因此需要有效的检测方法。

Method: 比较了三种方法：基于基础模型的FMA、基于ResNet50的DLA和基于手工特征的KBA，用于检测六种常见伪影。

Result: FMA在AUROC上表现最佳（0.995），优于DLA（0.977）和KBA（0.940）。

Conclusion: FMA是检测WSI伪影的最有效方法，并开发了质量报告工具以辅助诊断。

Abstract: In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to
digitize tissue specimens for detailed, high-resolution examination; however,
other diagnostic approaches, such as liquid biopsy and molecular testing, are
also utilized based on the cancer type and clinical context. While WSI has
revolutionized digital histopathology by enabling automated, precise analysis,
it remains vulnerable to artifacts introduced during slide preparation and
scanning. These artifacts can compromise downstream image analysis. To address
this challenge, we propose and compare three robust artifact detection
approaches for WSIs: (1) a foundation model-based approach (FMA) using a
fine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning
approach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach
(KBA) leveraging handcrafted features from texture, color, and frequency-based
metrics. The methods target six common artifact types: tissue folds,
out-of-focus regions, air bubbles, tissue damage, marker traces, and blood
contamination. Evaluations were conducted on 50,000+ image patches from diverse
scanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA
achieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),
outperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])
and the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into
actionable insights, we developed a quality report scorecard that quantifies
high-quality patches and visualizes artifact distributions.

</details>


### [5] [CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning](https://arxiv.org/abs/2507.00045)
*Ming Li,Chenguang Wang,Yijun Liang,Xiyao Wang,Yuhang Zhou,Xiyang Wu,Yuqing Zhang,Ruiyi Zhang,Tianyi Zhou*

Main category: cs.CV

TL;DR: 论文探讨了GPT-o3在复杂视觉推理任务中的局限性，提出了名为CaughtCheating的挑战性任务，揭示了现有MLLMs在人类侦探级能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多数基准测试中表现优异，但在某些复杂场景（如侦探级视觉推理任务）中表现不佳，需要更难的测试任务来评估其真实能力。

Method: 通过设计CaughtCheating任务（基于社交媒体中检测伴侣照片可疑线索的场景），对GPT-o3进行实验和分析。

Result: GPT-o3在CaughtCheating任务中表现极差（接近零分），揭示了其在细微视觉线索推理上的不足。

Conclusion: CaughtCheating任务为MLLMs提供了挑战性测试场景，推动其向人类级侦探感知与推理能力发展。

Abstract: Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have
achieved near-ceiling scores on various existing benchmarks, motivating a
demand for more challenging test tasks. These MLLMs have been reported to excel
in a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their
potential as a detective who can notice minuscule cues in an image and weave
them into coherent, situational explanations, leading to a reliable answer. But
can they match the performance of excellent human detectives? To answer this
question, we investigate some hard scenarios where GPT-o3 can still handle, and
find a common scenario where o3's performance drops to nearly zero, which we
name CaughtCheating. It is inspired by the social media requests that ask
others to detect suspicious clues from photos shared by the poster's partner.
We conduct extensive experiments and analysis to understand why existing MLLMs
lack sufficient capability to solve this kind of task. CaughtCheating provides
a class of challenging visual perception and reasoning tasks with great value
and practical usage. Success in these tasks paves the way for MLLMs to acquire
human-level detective perception and reasoning capabilities.

</details>


### [6] [Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process](https://arxiv.org/abs/2507.00046)
*Akshansh Mishra,Eyob Mesele Sefene,Shivraman Thapliyal*

Main category: cs.CV

TL;DR: 提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积（AFSD）过程中的完整性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过图像分割技术检测AFSD多层构建中的缺陷和特征，以优化工艺和评估质量。

Method: 结合粒子群优化（PSO）确定最佳分割阈值，并整合梯度幅度分析和距离变换，生成注意力加权可视化。

Result: PSO自动确定了最佳阈值（156-173），多通道可视化技术成功揭示了传统成像难以观察到的材料过渡区和潜在缺陷。

Conclusion: 注意力加权分析能有效识别AFSD接头中的不完全结合和不均匀区域，为工艺优化和质量评估提供了定量指标。

Abstract: This work proposes an evolutionary computing-based image segmentation
approach for analyzing soundness in Additive Friction Stir Deposition (AFSD)
processes. Particle Swarm Optimization (PSO) was employed to determine optimal
segmentation thresholds for detecting defects and features in multilayer AFSD
builds. The methodology integrates gradient magnitude analysis with distance
transforms to create novel attention-weighted visualizations that highlight
critical interface regions. Five AFSD samples processed under different
conditions were analyzed using multiple visualization techniques i.e.
self-attention maps, and multi-channel visualization. These complementary
approaches reveal subtle material transition zones and potential defect regions
which were not readily observable through conventional imaging. The PSO
algorithm automatically identified optimal threshold values (ranging from
156-173) for each sample, enabling precise segmentation of material interfaces.
The multi-channel visualization technique effectively combines boundary
information (red channel), spatial relationships (green channel), and material
density data (blue channel) into cohesive representations that quantify
interface quality. The results demonstrate that attention-based analysis
successfully identifies regions of incomplete bonding and inhomogeneities in
AFSD joints, providing quantitative metrics for process optimization and
quality assessment of additively manufactured components.

</details>


### [7] [AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training](https://arxiv.org/abs/2507.00049)
*Feiyang Kang,Nadine Chang,Maying Shen,Marc T. Law,Rafid Mahmood,Ruoxi Jia,Jose M. Alvarez*

Main category: cs.CV

TL;DR: AdaDeDup是一种新型混合框架，结合密度剪枝和模型反馈，自适应地减少数据冗余，显著提升大规模模型训练的数据效率。


<details>
  <summary>Details</summary>
Motivation: 大规模数据集的计算负担和冗余问题挑战了现代机器学习模型的训练，现有方法存在任务无关性或计算成本高的问题。

Method: AdaDeDup通过密度剪枝和模型反馈的协同作用，自适应调整聚类剪枝阈值，减少冗余数据的同时保留关键信息。

Result: 在Waymo、COCO等基准测试中，AdaDeDup显著优于基线方法，减少性能下降（如Waymo上超过54%），并在剪枝20%数据时接近原始模型性能。

Conclusion: AdaDeDup有效提升大规模模型训练的数据效率，代码已开源。

Abstract: The computational burden and inherent redundancy of large-scale datasets
challenge the training of contemporary machine learning models. Data pruning
offers a solution by selecting smaller, informative subsets, yet existing
methods struggle: density-based approaches can be task-agnostic, while
model-based techniques may introduce redundancy or prove computationally
prohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid
framework that synergistically integrates density-based pruning with
model-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions
data and applies an initial density-based pruning. It then employs a proxy
model to evaluate the impact of this initial pruning within each cluster by
comparing losses on kept versus pruned samples. This task-aware signal
adaptively adjusts cluster-specific pruning thresholds, enabling more
aggressive pruning in redundant clusters while preserving critical data in
informative ones. Extensive experiments on large-scale object detection
benchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster
R-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms
prominent baselines, substantially reduces performance degradation (e.g., over
54% versus random sampling on Waymo), and achieves near-original model
performance while pruning 20% of data, highlighting its efficacy in enhancing
data efficiency for large-scale model training. Code is open-sourced.

</details>


### [8] [VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models](https://arxiv.org/abs/2507.00052)
*Binesh Sadanandan,Vahid Behzadan*

Main category: cs.CV

TL;DR: VSF--Med是一个端到端的医疗视觉语言模型（VLM）漏洞评分框架，包含文本提示攻击模板、视觉扰动和八维评分标准，用于评估医疗VLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 医疗VLM在临床工作流程中潜力巨大，但缺乏系统性的安全性评估，因此需要开发一个标准化框架来量化其漏洞。

Method: VSF--Med结合了文本攻击模板、视觉扰动和八维评分标准，通过z-score归一化生成0-32的综合风险指标。

Result: 实验显示，不同VLM在攻击效果持续性、提示注入有效性和安全绕过成功率上表现出显著差异，Llama-3.2-11B-Vision-Instruct和GPT-4o的漏洞尤为突出。

Conclusion: VSF--Med为医疗VLM的安全性评估提供了标准化工具，揭示了现有模型的潜在漏洞，有助于未来改进。

Abstract: Vision Language Models (VLMs) hold great promise for streamlining
labour-intensive medical imaging workflows, yet systematic security evaluations
in clinical settings remain scarce. We introduce VSF--Med, an end-to-end
vulnerability-scoring framework for medical VLMs that unites three novel
components: (i) a rich library of sophisticated text-prompt attack templates
targeting emerging threat vectors; (ii) imperceptible visual perturbations
calibrated by structural similarity (SSIM) thresholds to preserve clinical
realism; and (iii) an eight-dimensional rubric evaluated by two independent
judge LLMs, whose raw scores are consolidated via z-score normalization to
yield a 0--32 composite risk metric. Built entirely on publicly available
datasets and accompanied by open-source code, VSF--Med synthesizes over 30,000
adversarial variants from 5,000 radiology images and enables reproducible
benchmarking of any medical VLM with a single command. Our consolidated
analysis reports mean z-score shifts of $0.90\sigma$ for
persistence-of-attack-effects, $0.74\sigma$ for prompt-injection effectiveness,
and $0.63\sigma$ for safety-bypass success across state-of-the-art VLMs.
Notably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase
of $1.29\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases
of $0.69\sigma$ for that same vector and $0.28\sigma$ for prompt-injection
attacks.

</details>


### [9] [MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding](https://arxiv.org/abs/2507.00068)
*Ziqi Zhong,Daniel Tang*

Main category: cs.CV

TL;DR: MANTA是一个多模态学习框架，通过将视觉和听觉输入统一到结构化文本空间，解决了多模态表示和推理中的不一致性问题，显著提升了长视频问答和跨模态理解的性能。


<details>
  <summary>Details</summary>
Motivation: 当前多模态学习方法通常将不同模态分开处理，导致表示和推理的不一致性。MANTA旨在通过统一的文本空间解决这一问题。

Method: MANTA采用信息论优化实现跨模态语义对齐，自适应时间同步处理不同信息密度，分层内容表示支持多尺度理解，以及上下文感知的稀疏信息检索。

Result: 在长视频问答任务中，MANTA将最先进模型的准确率提高了22.6%，在超过30分钟的视频中提升达27.3%。在时间推理和跨模态理解任务中也有显著改进。

Conclusion: MANTA通过结构化文本统一多模态表示，为多模态学习提供了新的理论基础，并在性能上实现了显著提升。

Abstract: While multi-modal learning has advanced significantly, current approaches
often treat modalities separately, creating inconsistencies in representation
and reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization
via Textual Alignment), a theoretically-grounded framework that unifies visual
and auditory inputs into a structured textual space for seamless processing
with large language models. MANTA addresses four key challenges: (1) semantic
alignment across modalities with information-theoretic optimization, (2)
adaptive temporal synchronization for varying information densities, (3)
hierarchical content representation for multi-scale understanding, and (4)
context-aware retrieval of sparse information from long sequences. We formalize
our approach within a rigorous mathematical framework, proving its optimality
for context selection under token constraints. Extensive experiments on the
challenging task of Long Video Question Answering show that MANTA improves
state-of-the-art models by up to 22.6% in overall accuracy, with particularly
significant gains (27.3%) on videos exceeding 30 minutes. Additionally, we
demonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)
and cross-modal understanding (25.1% improvement). Our framework introduces
novel density estimation techniques for redundancy minimization while
preserving rare signals, establishing new foundations for unifying multimodal
representations through structured text.

</details>


### [10] [An efficient plant disease detection using transfer learning approach](https://arxiv.org/abs/2507.00070)
*Bosubabu Sambana,Hillary Sunday Nnadi,Mohd Anas Wajid,Nwosu Ogochukwu Fidelia,Claudia Camacho-Zuñiga,Henry Dozie Ajuzie,Edeh Michael Onyema*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLOv7和YOLOv8的植物病害检测系统，通过迁移学习方法在植物叶片图像数据集上实现了高精度的病害检测。


<details>
  <summary>Details</summary>
Motivation: 植物病害对农业产生严重影响，早期检测至关重要。技术发展为自动化监测提供了机会。

Method: 使用YOLOv7和YOLOv8模型，通过迁移学习在植物叶片图像数据集上进行微调，检测细菌、真菌和病毒病害。

Result: 模型在mAP、F1-score、Precision和Recall等指标上表现优异，YOLOv8效果最佳。

Conclusion: 该系统为植物病害早期检测提供了可扩展的自动化解决方案，有助于提高作物产量并支持可持续农业。

Abstract: Plant diseases pose significant challenges to farmers and the agricultural
sector at large. However, early detection of plant diseases is crucial to
mitigating their effects and preventing widespread damage, as outbreaks can
severely impact the productivity and quality of crops. With advancements in
technology, there are increasing opportunities for automating the monitoring
and detection of disease outbreaks in plants. This study proposed a system
designed to identify and monitor plant diseases using a transfer learning
approach. Specifically, the study utilizes YOLOv7 and YOLOv8, two
state-ofthe-art models in the field of object detection. By fine-tuning these
models on a dataset of plant leaf images, the system is able to accurately
detect the presence of Bacteria, Fungi and Viral diseases such as Powdery
Mildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's
performance was evaluated using several metrics, including mean Average
Precision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,
89.40, 91.22, and 87.66, respectively. The result demonstrates the superior
effectiveness and efficiency of YOLOv8 compared to other object detection
methods, highlighting its potential for use in modern agricultural practices.
The approach provides a scalable, automated solution for early any plant
disease detection, contributing to enhanced crop yield, reduced reliance on
manual monitoring, and supporting sustainable agricultural practices.

</details>


### [11] [Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics](https://arxiv.org/abs/2507.00153)
*Peter Mortimer,Mirko Maehlisch*

Main category: cs.CV

TL;DR: 论文提出了一种基于扩散模型的图像增强方法，用于解决自动驾驶车辆在雪地等分布外环境中的感知性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 基于学习的感知算法在分布外和代表性不足的环境中性能下降，尤其是户外机器人因光照、季节和天气变化导致训练数据不足。

Method: 采用扩散模型进行图像增强，利用公开可用的视觉基础模型生成更接近部署环境的训练数据，并通过开放词汇语义分割模型过滤幻觉候选。

Result: 扩散模型增强方法能够控制训练数据中地面语义分布，并针对部署环境微调模型。

Conclusion: 该方法可扩展至雪地以外的其他环境（如沙地和火山地形），具有广泛的应用潜力。

Abstract: The performance of leaning-based perception algorithms suffer when deployed
in out-of-distribution and underrepresented environments. Outdoor robots are
particularly susceptible to rapid changes in visual scene appearance due to
dynamic lighting, seasonality and weather effects that lead to scenes
underrepresented in the training data of the learning-based perception system.
In this conceptual paper, we focus on preparing our autonomous vehicle for
deployment in snow-filled environments. We propose a novel method for
diffusion-based image augmentation to more closely represent the deployment
environment in our training data. Diffusion-based image augmentations rely on
the public availability of vision foundation models learned on internet-scale
datasets. The diffusion-based image augmentations allow us to take control over
the semantic distribution of the ground surfaces in the training data and to
fine-tune our model for its deployment environment. We employ open vocabulary
semantic segmentation models to filter out augmentation candidates that contain
hallucinations. We believe that diffusion-based image augmentations can be
extended to many other environments apart from snow surfaces, like sandy
environments and volcanic terrains.

</details>


### [12] [FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion](https://arxiv.org/abs/2507.00162)
*Yu Lu,Yi Yang*

Main category: cs.CV

TL;DR: FreeLong和FreeLong++是无需训练的框架，通过平衡长视频特征的频率分布，解决了长视频生成中的高频失真问题，显著提升了时间一致性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在生成长视频时存在时间一致性和视觉保真度下降的问题，尤其是高频失真现象。

Method: FreeLong通过混合全局低频特征和局部高频特征来平衡频率分布；FreeLong++扩展为多分支架构，实现多频段融合。

Result: FreeLong++在长视频生成任务（如4倍和8倍原生长度）上优于现有方法，支持多提示视频生成和可控视频生成。

Conclusion: FreeLong++无需额外训练即可提升现有模型的长视频生成能力，解决了高频失真问题，适用于多种应用场景。

Abstract: Recent advances in video generation models have enabled high-quality short
video generation from text prompts. However, extending these models to longer
videos remains a significant challenge, primarily due to degraded temporal
consistency and visual fidelity. Our preliminary observations show that naively
applying short-video generation models to longer sequences leads to noticeable
quality degradation. Further analysis identifies a systematic trend where
high-frequency components become increasingly distorted as video length grows,
an issue we term high-frequency distortion. To address this, we propose
FreeLong, a training-free framework designed to balance the frequency
distribution of long video features during the denoising process. FreeLong
achieves this by blending global low-frequency features, which capture holistic
semantics across the full video, with local high-frequency features extracted
from short temporal windows to preserve fine details. Building on this,
FreeLong++ extends FreeLong dual-branch design into a multi-branch architecture
with multiple attention branches, each operating at a distinct temporal scale.
By arranging multiple window sizes from global to local, FreeLong++ enables
multi-band frequency fusion from low to high frequencies, ensuring both
semantic continuity and fine-grained motion dynamics across longer video
sequences. Without any additional training, FreeLong++ can be plugged into
existing video generation models (e.g. Wan2.1 and LTX-Video) to produce longer
videos with substantially improved temporal consistency and visual fidelity. We
demonstrate that our approach outperforms previous methods on longer video
generation tasks (e.g. 4x and 8x of native length). It also supports coherent
multi-prompt video generation with smooth scene transitions and enables
controllable video generation using long depth or pose sequences.

</details>


### [13] [SelvaBox: A high-resolution dataset for tropical tree crown detection](https://arxiv.org/abs/2507.00170)
*Hugo Baudchon,Arthur Ouaknine,Martin Weiss,Mélisande Teng,Thomas R. Walla,Antoine Caron-Guay,Christopher Pal,Etienne Laliberté*

Main category: cs.CV

TL;DR: SelvaBox是一个用于热带雨林树冠检测的最大公开数据集，包含83,000多个标记树冠，显著提升了检测精度和零样本性能。


<details>
  <summary>Details</summary>
Motivation: 热带雨林树冠检测对研究生态系统至关重要，但现有数据集稀缺，阻碍了模型开发。

Method: 引入SelvaBox数据集，并在多分辨率管道中进行联合训练。

Result: 高分辨率输入提升检测精度，SelvaBox训练的模型在零样本检测中表现优异。

Conclusion: SelvaBox为热带树冠检测提供了强大工具，公开数据集和代码促进了研究进展。

Abstract: Detecting individual tree crowns in tropical forests is essential to study
these complex and crucial ecosystems impacted by human interventions and
climate change. However, tropical crowns vary widely in size, structure, and
pattern and are largely overlapping and intertwined, requiring advanced remote
sensing methods applied to high-resolution imagery. Despite growing interest in
tropical tree crown detection, annotated datasets remain scarce, hindering
robust model development. We introduce SelvaBox, the largest open-access
dataset for tropical tree crown detection in high-resolution drone imagery. It
spans three countries and contains more than 83,000 manually labeled crowns -
an order of magnitude larger than all previous tropical forest datasets
combined. Extensive benchmarks on SelvaBox reveal two key findings: (1)
higher-resolution inputs consistently boost detection accuracy; and (2) models
trained exclusively on SelvaBox achieve competitive zero-shot detection
performance on unseen tropical tree crown datasets, matching or exceeding
competing methods. Furthermore, jointly training on SelvaBox and three other
datasets at resolutions from 3 to 10 cm per pixel within a unified
multi-resolution pipeline yields a detector ranking first or second across all
evaluated datasets. Our dataset, code, and pre-trained weights are made public.

</details>


### [14] [Graph-Based Deep Learning for Component Segmentation of Maize Plants](https://arxiv.org/abs/2507.00182)
*J. I. Ruíz,A. Méndez,E. Rodríguez*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络（GNN）和主成分分析（PCA）的新型深度学习架构，用于在LiDAR 3D点云数据中识别植物组件，准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理3D数据和识别植物组件时存在不足，需要更高效的解决方案。

Method: 结合GNN和PCA，通过KNN层建立点云图的边，使用Edge-Conv层增强特征，最后用GAT分类植物组件。

Result: 在IoU平均值上超过80%，优于现有基于点云的模型。

Conclusion: 该图神经网络方法显著提高了植物组件的分割精度，为精准农业提供了有效工具。

Abstract: In precision agriculture, one of the most important tasks when exploring crop
production is identifying individual plant components. There are several
attempts to accomplish this task by the use of traditional 2D imaging, 3D
reconstructions, and Convolutional Neural Networks (CNN). However, they have
several drawbacks when processing 3D data and identifying individual plant
components. Therefore, in this work, we propose a novel Deep Learning
architecture to detect components of individual plants on Light Detection and
Ranging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on
the concept of Graph Neural Networks (GNN), and feature enhancing with
Principal Component Analysis (PCA). For this, each point is taken as a vertex
and by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,
thus representing the 3D PC data set. Subsequently, Edge-Conv layers are used
to further increase the features of each point. Finally, Graph Attention
Networks (GAT) are applied to classify visible phenotypic components of the
plant, such as the leaf, stem, and soil. This study demonstrates that our
graph-based deep learning approach enhances segmentation accuracy for
identifying individual plant components, achieving percentages above 80% in the
IoU average, thus outperforming other existing models based on point clouds.

</details>


### [15] [Computer Vision for Objects used in Group Work: Challenges and Opportunities](https://arxiv.org/abs/2507.00224)
*Changsoo Jung,Sheikh Mannan,Jack Fitzgerald,Nathaniel Blanchard*

Main category: cs.CV

TL;DR: 论文提出FiboSB数据集，用于解决协作任务中6D姿态估计的挑战，并通过改进YOLO11-x算法提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有系统在协作任务中难以准确捕捉学生与物理对象的互动，6D姿态估计可解决这一问题。

Method: 引入FiboSB数据集，评估四种6D姿态估计算法，并改进YOLO11-x算法。

Result: 改进后的YOLO11-x在FiboSB数据集上达到mAP_50为0.898。

Conclusion: FiboSB数据集和算法改进为协作场景中的6D姿态估计奠定了基础。

Abstract: Interactive and spatially aware technologies are transforming educational
frameworks, particularly in K-12 settings where hands-on exploration fosters
deeper conceptual understanding. However, during collaborative tasks, existing
systems often lack the ability to accurately capture real-world interactions
between students and physical objects. This issue could be addressed with
automatic 6D pose estimation, i.e., estimation of an object's position and
orientation in 3D space from RGB images or videos. For collaborative groups
that interact with physical objects, 6D pose estimates allow AI systems to
relate objects and entities. As part of this work, we introduce FiboSB, a novel
and challenging 6D pose video dataset featuring groups of three participants
solving an interactive task featuring small hand-held cubes and a weight scale.
This setup poses unique challenges for 6D pose because groups are holistically
recorded from a distance in order to capture all participants -- this, coupled
with the small size of the cubes, makes 6D pose estimation inherently
non-trivial. We evaluated four state-of-the-art 6D pose estimation methods on
FiboSB, exposing the limitations of current algorithms on collaborative group
work. An error analysis of these methods reveals that the 6D pose methods'
object detection modules fail. We address this by fine-tuning YOLO11-x for
FiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,
and analysis of YOLO11-x errors presented here lay the groundwork for
leveraging the estimation of 6D poses in difficult collaborative contexts.

</details>


### [16] [VOCAL: Visual Odometry via ContrAstive Learning](https://arxiv.org/abs/2507.00243)
*Chi-Yao Huang,Zeel Bhatt,Yezhou Yang*

Main category: cs.CV

TL;DR: VOCAL是一种新型视觉里程计框架，通过对比学习将VO重新定义为标签排序问题，结合贝叶斯推理和表示学习，提升了解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有学习型VO方法依赖刚性几何假设，解释性差且缺乏理论基础，VOCAL旨在解决这些问题。

Method: VOCAL将VO视为标签排序问题，结合贝叶斯推理和表示学习，使视觉特征与相机状态对齐。

Result: 在KITTI数据集上的实验表明，VOCAL具有更高的解释性和灵活性。

Conclusion: VOCAL推动了VO向更通用和可解释的空间智能发展。

Abstract: Breakthroughs in visual odometry (VO) have fundamentally reshaped the
landscape of robotics, enabling ultra-precise camera state estimation that is
crucial for modern autonomous systems. Despite these advances, many
learning-based VO techniques rely on rigid geometric assumptions, which often
fall short in interpretability and lack a solid theoretical basis within fully
data-driven frameworks. To overcome these limitations, we introduce VOCAL
(Visual Odometry via ContrAstive Learning), a novel framework that reimagines
VO as a label ranking challenge. By integrating Bayesian inference with a
representation learning framework, VOCAL organizes visual features to mirror
camera states. The ranking mechanism compels similar camera states to converge
into consistent and spatially coherent representations within the latent space.
This strategic alignment not only bolsters the interpretability of the learned
features but also ensures compatibility with multimodal data sources. Extensive
evaluations on the KITTI dataset highlight VOCAL's enhanced interpretability
and flexibility, pushing VO toward more general and explainable spatial
intelligence.

</details>


### [17] [Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition](https://arxiv.org/abs/2507.00248)
*Nikita Nikitin,Eugene Fomin*

Main category: cs.CV

TL;DR: 提出了一种基于轻量级DNN的实时手语识别框架，解决了数据稀缺、高计算成本和帧率差异等问题，模型在边缘设备上实现低延迟高精度分类。


<details>
  <summary>Details</summary>
Motivation: 解决手语识别中的数据稀缺、高计算成本和训练与推理环境帧率差异等关键挑战。

Method: 通过将手语参数（如手形、手掌方向、动作和位置）编码为向量输入，并利用MediaPipe提取关键点，设计了一个优化为小于10MB的DNN架构。

Result: 模型在343个手语分类中实现了92%的准确率，延迟低于10ms，并已集成到'slait ai'应用中。

Conclusion: 该框架在边缘设备上实现了高效、低延迟的手语识别，具有实际应用价值。

Abstract: We present a novel framework for real-time sign language recognition using
lightweight DNNs trained on limited data. Our system addresses key challenges
in sign language recognition, including data scarcity, high computational
costs, and discrepancies in frame rates between training and inference
environments. By encoding sign language specific parameters, such as handshape,
palm orientation, movement, and location into vectorized inputs, and leveraging
MediaPipe for landmark extraction, we achieve highly separable input data
representations. Our DNN architecture, optimized for sub 10MB deployment,
enables accurate classification of 343 signs with less than 10ms latency on
edge devices. The data annotation platform 'slait data' facilitates structured
labeling and vector extraction. Our model achieved 92% accuracy in isolated
sign recognition and has been integrated into the 'slait ai' web application,
where it demonstrates stable inference.

</details>


### [18] [GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception](https://arxiv.org/abs/2507.00253)
*Zhuangzhuang Dai,Vincent Gbouna Zakka,Luis J. Manso,Chen Li*

Main category: cs.CV

TL;DR: GazeTarget360是一个用于从图像中估计360度视线目标的系统，适用于真实场景中的机器人交互。


<details>
  <summary>Details</summary>
Motivation: 机器人理解人类视线目标是实现注意力估计和运动预测等下游任务的关键，现有方法在视线离开相机时效果不佳。

Method: 系统整合了眼神接触检测器、预训练视觉编码器和多尺度融合解码器，通过条件推理引擎实现。

Result: 交叉验证表明，GazeTarget360在未见场景中能准确预测视线目标，且高效可部署。

Conclusion: GazeTarget360是首个能从真实相机素材中高效预测视线目标的系统，代码已开源。

Abstract: Enabling robots to understand human gaze target is a crucial step to allow
capabilities in downstream tasks, for example, attention estimation and
movement anticipation in real-world human-robot interactions. Prior works have
addressed the in-frame target localization problem with data-driven approaches
by carefully removing out-of-frame samples. Vision-based gaze estimation
methods, such as OpenFace, do not effectively absorb background information in
images and cannot predict gaze target in situations where subjects look away
from the camera. In this work, we propose a system to address the problem of
360-degree gaze target estimation from an image in generalized visual scenes.
The system, named GazeTarget360, integrates conditional inference engines of an
eye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion
decoder. Cross validation results show that GazeTarget360 can produce accurate
and reliable gaze target predictions in unseen scenarios. This makes a
first-of-its-kind system to predict gaze targets from realistic camera footage
which is highly efficient and deployable. Our source code is made publicly
available at: https://github.com/zdai257/DisengageNet.

</details>


### [19] [VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos](https://arxiv.org/abs/2507.00261)
*Zhiyin Lin,Purvi Goel,Joy Yun,C. Karen Liu,Joao Pedro Araujo*

Main category: cs.CV

TL;DR: VirtualFencer是一个无监督系统，能从视频中提取3D击剑动作和策略，并生成逼真的击剑行为。


<details>
  <summary>Details</summary>
Motivation: 击剑动作多样且受策略驱动，适合数据驱动建模。

Method: 从视频中无监督提取3D动作和策略，生成击剑行为。

Result: 系统能自我对战、与真实击剑手动作对战，并与专业击剑手互动。

Conclusion: VirtualFencer展示了数据驱动建模在击剑中的潜力。

Abstract: Fencing is a sport where athletes engage in diverse yet strategically logical
motions. While most motions fall into a few high-level actions (e.g. step,
lunge, parry), the execution can vary widely-fast vs. slow, large vs. small,
offensive vs. defensive. Moreover, a fencer's actions are informed by a
strategy that often comes in response to the opponent's behavior. This
combination of motion diversity with underlying two-player strategy motivates
the application of data-driven modeling to fencing. We present VirtualFencer, a
system capable of extracting 3D fencing motion and strategy from in-the-wild
video without supervision, and then using that extracted knowledge to generate
realistic fencing behavior. We demonstrate the versatile capabilities of our
system by having it (i) fence against itself (self-play), (ii) fence against a
real fencer's motion from online video, and (iii) fence interactively against a
professional fencer.

</details>


### [20] [Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections](https://arxiv.org/abs/2507.00263)
*Vignesh Ram Nithin Kappagantula,Shayan Hassantabar*

Main category: cs.CV

TL;DR: 提出了一种高效的方法来解决度假租赁平台中房间场景发现和分组问题，以及识别卧室床型，帮助旅行者理解空间布局。


<details>
  <summary>Details</summary>
Motivation: 度假租赁平台上的房产图片缺乏结构化分类，给旅行者理解空间布局带来挑战。

Method: 采用计算高效的机器学习流水线，包括房间类型检测模型、重叠检测模型和聚类算法，并结合多模态大语言模型识别床型。

Result: 流水线整体表现优异，显著优于对比学习和预训练嵌入聚类等现有方法。

Conclusion: 该方法在实时和数据稀缺环境下表现良好，有效解决了房产图片的组织问题。

Abstract: The rapid growth of vacation rental (VR) platforms has led to an increasing
volume of property images, often uploaded without structured categorization.
This lack of organization poses significant challenges for travelers attempting
to understand the spatial layout of a property, particularly when multiple
rooms of the same type are present. To address this issue, we introduce an
effective approach for solving the room scene discovery and grouping problem,
as well as identifying bed types within each bedroom group. This grouping is
valuable for travelers to comprehend the spatial organization, layout, and the
sleeping configuration of the property. We propose a computationally efficient
machine learning pipeline characterized by low latency and the ability to
perform effectively with sample-efficient learning, making it well-suited for
real-time and data-scarce environments. The pipeline integrates a supervised
room-type detection model, a supervised overlap detection model to identify the
overlap similarity between two images, and a clustering algorithm to group the
images of the same space together using the similarity scores. Additionally,
the pipeline maps each bedroom group to the corresponding bed types specified
in the property's metadata, based on the visual content present in the group's
images using a Multi-modal Large Language Model (MLLM) model. We evaluate the
aforementioned models individually and also assess the pipeline in its
entirety, observing strong performance that significantly outperforms
established approaches such as contrastive learning and clustering with
pretrained embeddings.

</details>


### [21] [Self-Supervised Multiview Xray Matching](https://arxiv.org/abs/2507.00287)
*Mohamad Dabboussi,Malo Huard,Yann Gousseau,Pietro Gori*

Main category: cs.CV

TL;DR: 提出了一种自监督方法，通过合成X射线视图生成多对多对应矩阵，无需手动标注，提升了多视图骨折检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法在多视图X射线分析中难以建立稳健的对应关系，影响临床评估的准确性。

Method: 利用数字重建放射影像（DRR）自动生成合成X射线视图，并通过基于Transformer的训练预测多视图对应关系。

Result: 在合成和真实X射线数据集上的评估表明，引入对应关系提升了多视图骨折分类性能。

Conclusion: 该方法通过自监督学习提升了多视图X射线分析的准确性，可作为预训练策略应用于真实数据。

Abstract: Accurate interpretation of multi-view radiographs is crucial for diagnosing
fractures, muscular injuries, and other anomalies. While significant advances
have been made in AI-based analysis of single images, current methods often
struggle to establish robust correspondences between different X-ray views, an
essential capability for precise clinical evaluations. In this work, we present
a novel self-supervised pipeline that eliminates the need for manual annotation
by automatically generating a many-to-many correspondence matrix between
synthetic X-ray views. This is achieved using digitally reconstructed
radiographs (DRR), which are automatically derived from unannotated CT volumes.
Our approach incorporates a transformer-based training phase to accurately
predict correspondences across two or more X-ray views. Furthermore, we
demonstrate that learning correspondences among synthetic X-ray views can be
leveraged as a pretraining strategy to enhance automatic multi-view fracture
detection on real data. Extensive evaluations on both synthetic and real X-ray
datasets show that incorporating correspondences improves performance in
multi-view fracture classification.

</details>


### [22] [Reducing Variability of Multiple Instance Learning Methods for Digital Pathology](https://arxiv.org/abs/2507.00292)
*Ali Mammadov,Loïc Le Folgoc,Guillaume Hocquet,Pietro Gori*

Main category: cs.CV

TL;DR: 提出了一种多保真度模型融合策略，用于减少多实例学习（MIL）方法在数字病理学中的性能波动。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像（WSIs）通常被分割为小块进行深度学习，但MIL方法在不同运行中性能波动较大（可达10-15 AUC点），影响可靠比较。

Method: 通过训练多个模型并基于验证分数平均最稳定和有前景的模型，提出了一种多保真度模型融合策略。

Result: 在2个数据集、3种初始化策略和5种MIL方法上进行了2000多次实验，验证了该方法的有效性。

Conclusion: 该方法可应用于任何现有MIL模型，减少性能波动，简化超参数调优，提高可重复性，同时保持计算效率。

Abstract: Digital pathology has revolutionized the field by enabling the digitization
of tissue samples into whole slide images (WSIs). However, the high resolution
and large size of WSIs present significant challenges when it comes to applying
Deep Learning models. As a solution, WSIs are often divided into smaller
patches with a global label (\textit{i.e., diagnostic}) per slide, instead of a
(too) costly pixel-wise annotation. By treating each slide as a bag of patches,
Multiple Instance Learning (MIL) methods have emerged as a suitable solution
for WSI classification. A major drawback of MIL methods is their high
variability in performance across different runs, which can reach up to 10-15
AUC points on the test set, making it difficult to compare different MIL
methods reliably. This variability mainly comes from three factors: i) weight
initialization, ii) batch (shuffling) ordering, iii) and learning rate. To
address that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL
methods. We first train multiple models for a few epochs and average the most
stable and promising ones based on validation scores. This approach can be
applied to any existing MIL model to reduce performance variability. It also
simplifies hyperparameter tuning and improves reproducibility while maintaining
computational efficiency. We extensively validate our approach on WSI
classification tasks using 2 different datasets, 3 initialization strategies
and 5 MIL methods, for a total of more than 2000 experiments.

</details>


### [23] [Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes](https://arxiv.org/abs/2507.00327)
*Chuyan Zhang,Kefan Wang,Yun Gu*

Main category: cs.CV

TL;DR: SR-LoRA是一种新型的低秩适应框架，通过利用预训练权重矩阵的稳定秩来动态分配秩，提高了在领域差距大的任务中的适应性，同时保持了计算效率。


<details>
  <summary>Details</summary>
Motivation: 固定低秩结构的LoRA在领域差距大的任务中适应性不足，现有方法依赖计算密集型技术。

Method: 利用预训练权重矩阵的稳定秩作为层间秩分配的自然先验，实现高效且原则性的秩重新分配。

Result: 在领域差距大的少样本任务中，SR-LoRA表现优于现有自适应LoRA变体，实现了性能与效率的更好平衡。

Conclusion: SR-LoRA通过稳定秩指导的秩分配，显著提升了低秩适应在复杂任务中的表现，且无需额外搜索成本。

Abstract: Low-Rank Adaptation (LoRA) has proven effective in reducing computational
costs while maintaining performance comparable to fully fine-tuned foundation
models across various tasks. However, its fixed low-rank structure restricts
its adaptability in scenarios with substantial domain gaps, where higher ranks
are often required to capture domain-specific complexities. Current adaptive
LoRA methods attempt to overcome this limitation by dynamically expanding or
selectively allocating ranks, but these approaches frequently depend on
computationally intensive techniques such as iterative pruning, rank searches,
or additional regularization. To address these challenges, we introduce Stable
Rank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the
stable rank of pre-trained weight matrices as a natural prior for layer-wise
rank allocation. By leveraging the stable rank, which reflects the intrinsic
dimensionality of the weights, SR-LoRA enables a principled and efficient
redistribution of ranks across layers, enhancing adaptability without incurring
additional search costs. Empirical evaluations on few-shot tasks with
significant domain gaps show that SR-LoRA consistently outperforms recent
adaptive LoRA variants, achieving a superior trade-off between performance and
efficiency. Our code is available at
https://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.

</details>


### [24] [MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms](https://arxiv.org/abs/2507.00328)
*Xuan Liu,Yinhao Ren,Marc D. Ryser,Lars J. Grimm,Joseph Y. Lo*

Main category: cs.CV

TL;DR: MammoTracker是一个用于乳腺X光片中病变跟踪的自动化框架，通过全局和局部搜索策略提高病变定位准确性。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光片中病变的自动跟踪对于乳腺癌进展监测和早期诊断至关重要，但目前计算机辅助诊断系统在此方面存在挑战。

Method: 采用粗到细的策略，结合全局搜索、局部搜索和分数细化三个关键模块。

Result: 在实验中，MammoTracker的平均重叠率为0.455，准确率为0.509，比基线模型提高了8%。

Conclusion: MammoTracker展示了在计算机辅助诊断系统中增强病变进展分析的潜力，并提供了一个大规模数据集支持未来研究。

Abstract: Accurate lesion tracking in temporal mammograms is essential for monitoring
breast cancer progression and facilitating early diagnosis. However, automated
lesion correspondence across exams remains a challenges in computer-aided
diagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,
a mask-guided lesion tracking framework that automates lesion localization
across consecutively exams. Our approach follows a coarse-to-fine strategy
incorporating three key modules: global search, local search, and score
refinement. To support large-scale training and evaluation, we introduce a new
dataset with curated prior-exam annotations for 730 mass and calcification
cases from the public EMBED mammogram dataset, yielding over 20000 lesion
pairs, making it the largest known resource for temporal lesion tracking in
mammograms. Experimental results demonstrate that MammoTracker achieves 0.455
average overlap and 0.509 accuracy, surpassing baseline models by 8%,
highlighting its potential to enhance CAD-based lesion progression analysis.
Our dataset will be available at
https://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.

</details>


### [25] [Populate-A-Scene: Affordance-Aware Human Video Generation](https://arxiv.org/abs/2507.00334)
*Mengyi Shan,Zecheng He,Haoyu Ma,Felix Juefei-Xu,Peizhao Zhang,Tingbo Hou,Ching-Yao Chuang*

Main category: cs.CV

TL;DR: 探索文本到视频模型是否可作为交互式世界模拟器，通过预测人-环境交互来感知场景的可供性。


<details>
  <summary>Details</summary>
Motivation: 研究如何利用预训练的视频生成模型感知场景的可供性，而无需显式条件（如边界框或姿态）。

Method: 通过微调模型，在单张场景图像中插入符合场景可供性的人，并确保行为、外观和场景的协调性。

Result: 通过跨注意力热图分析，发现预训练视频模型具有固有的可供性感知能力，无需标注数据集。

Conclusion: 视频生成模型可被重新用作交互式世界模拟器，具备感知场景可供性的潜力。

Abstract: Can a video generation model be repurposed as an interactive world simulator?
We explore the affordance perception potential of text-to-video models by
teaching them to predict human-environment interaction. Given a scene image and
a prompt describing human actions, we fine-tune the model to insert a person
into the scene, while ensuring coherent behavior, appearance, harmonization,
and scene affordance. Unlike prior work, we infer human affordance for video
generation (i.e., where to insert a person and how they should behave) from a
single scene image, without explicit conditions like bounding boxes or body
poses. An in-depth study of cross-attention heatmaps demonstrates that we can
uncover the inherent affordance perception of a pre-trained video model without
labeled affordance datasets.

</details>


### [26] [Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video](https://arxiv.org/abs/2507.00339)
*Alexander Moore,Amar Saini,Kylie Cancilla,Doug Poland,Carmen Carrano*

Main category: cs.CV

TL;DR: MOVi-MC-AC是一个多摄像头视角下的最大模态分割和首个模态内容数据集，提供了5.8百万个对象实例的标签。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏多摄像头共享场景的维度，MOVi-MC-AC填补了这一空白，为计算机视觉任务提供了新的复杂性。

Method: 通过模拟多摄像头视频中的杂乱家庭场景，提供一致的物体ID和模态内容标签。

Result: 数据集包含5.8百万个对象实例，是模态分割领域最大的数据集，并首次提供真实模态内容标签。

Conclusion: MOVi-MC-AC为对象检测、跟踪和分割任务提供了新的基准和资源。

Abstract: Amodal segmentation and amodal content completion require using object priors
to estimate occluded masks and features of objects in complex scenes. Until
now, no data has provided an additional dimension for object context: the
possibility of multiple cameras sharing a view of a scene. We introduce
MOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the
largest amodal segmentation and first amodal content dataset to date. Cluttered
scenes of generic household objects are simulated in multi-camera video.
MOVi-MC-AC contributes to the growing literature of object detection, tracking,
and segmentation by including two new contributions to the deep learning for
computer vision world. Multiple Camera (MC) settings where objects can be
identified and tracked between various unique camera perspectives are rare in
both synthetic and real-world video. We introduce a new complexity to synthetic
video by providing consistent object ids for detections and segmentations
between both frames and multiple cameras each with unique features and motion
patterns on a single scene. Amodal Content (AC) is a reconstructive task in
which models predict the appearance of target objects through occlusions. In
the amodal segmentation literature, some datasets have been released with
amodal detection, tracking, and segmentation labels. While other methods rely
on slow cut-and-paste schemes to generate amodal content pseudo-labels, they do
not account for natural occlusions present in the modal masks. MOVi-MC-AC
provides labels for ~5.8 million object instances, setting a new maximum in the
amodal dataset literature, along with being the first to provide ground-truth
amodal content. The full dataset is available at
https://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,

</details>


### [27] [CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation](https://arxiv.org/abs/2507.00356)
*Zhiwei Yi,Xin Cheng,Jingyu Ma,Ruifei Zhu,Junwei Tian,Yuanxiu Zhou,Xinge Zhao,Hongzhe Li*

Main category: cs.CV

TL;DR: 提出CGEarthEye框架，针对吉林一号卫星特性设计，包含5个骨干网络，参数达21亿，通过自监督学习数据集JLSSD提升模型表现，在10个基准数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 超高分辨率光学遥感影像获取渠道有限，限制了高分辨率遥感视觉基础模型的发展，吉林一号卫星资源丰富，但缺乏针对性模型。

Method: 设计CGEarthEye框架，结合季节性对比、增强对比和掩码补丁对比策略进行预训练，使用JLSSD数据集。

Result: 在10个基准数据集上表现优异，特征可视化、收敛性、参数效率和实际应用均优于现有方法。

Conclusion: CGEarthEye的卓越表现将推动吉林一号数据在传统地球观测中的更广泛应用。

Abstract: Deep learning methods have significantly advanced the development of
intelligent rinterpretation in remote sensing (RS), with foundational model
research based on large-scale pre-training paradigms rapidly reshaping various
domains of Earth Observation (EO). However, compared to the open accessibility
and high spatiotemporal coverage of medium-resolution data, the limited
acquisition channels for ultra-high-resolution optical RS imagery have
constrained the progress of high-resolution remote sensing vision foundation
models (RSVFM). As the world's largest sub-meter-level commercial RS satellite
constellation, the Jilin-1 constellation possesses abundant sub-meter-level
image resources. This study proposes CGEarthEye, a RSVFM framework specifically
designed for Jilin-1 satellite characteristics, comprising five backbones with
different parameter scales with totaling 2.1 billion parameters. To enhance the
representational capacity of the foundation model, we developed JLSSD, the
first 15-million-scale multi-temporal self-supervised learning (SSL) dataset
featuring global coverage with quarterly temporal sampling within a single
year, constructed through multi-level representation clustering and sampling
strategies. The framework integrates seasonal contrast, augmentation-based
contrast, and masked patch token contrastive strategies for pre-training.
Comprehensive evaluations across 10 benchmark datasets covering four typical RS
tasks demonstrate that the CGEarthEye consistently achieves state-of-the-art
(SOTA) performance. Further analysis reveals CGEarthEye's superior
characteristics in feature visualization, model convergence, parameter
efficiency, and practical mapping applications. This study anticipates that the
exceptional representation capabilities of CGEarthEye will facilitate broader
and more efficient applications of Jilin-1 data in traditional EO application.

</details>


### [28] [GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control](https://arxiv.org/abs/2507.00363)
*Xingjun Wang,Lianlei Shan*

Main category: cs.CV

TL;DR: 提出了一种改进3D高斯泼溅（3DGS）的方法，解决了初始化、优化和密度控制的挑战，实现了高保真实时渲染。


<details>
  <summary>Details</summary>
Motivation: 3DGS因其显式3D高斯表示而受欢迎，但其依赖准确的初始化，且在优化无序高斯分布为有序表面时存在困难，缺乏自适应密度控制机制。

Method: 提出了几何引导的初始化预测高斯参数，表面对齐的优化策略，以及动态自适应密度控制机制。

Result: 方法在复杂场景中实现了高保真实时渲染，视觉质量显著提升，与现有先进方法相当或更优。

Conclusion: 通过几何引导初始化、表面对齐优化和动态密度控制，显著提升了3DGS的渲染质量和效率。

Abstract: We propose a method to enhance 3D Gaussian Splatting (3DGS)~\cite{Kerbl2023},
addressing challenges in initialization, optimization, and density control.
Gaussian Splatting is an alternative for rendering realistic images while
supporting real-time performance, and it has gained popularity due to its
explicit 3D Gaussian representation. However, 3DGS heavily depends on accurate
initialization and faces difficulties in optimizing unstructured Gaussian
distributions into ordered surfaces, with limited adaptive density control
mechanism proposed so far. Our first key contribution is a geometry-guided
initialization to predict Gaussian parameters, ensuring precise placement and
faster convergence. We then introduce a surface-aligned optimization strategy
to refine Gaussian placement, improving geometric accuracy and aligning with
the surface normals of the scene. Finally, we present a dynamic adaptive
density control mechanism that adjusts Gaussian density based on regional
complexity, for visual fidelity. These innovations enable our method to achieve
high-fidelity real-time rendering and significant improvements in visual
quality, even in complex scenes. Our method demonstrates comparable or superior
results to state-of-the-art methods, rendering high-fidelity images in real
time.

</details>


### [29] [An Improved U-Net Model for Offline handwriting signature denoising](https://arxiv.org/abs/2507.00365)
*Wanghui Xiao*

Main category: cs.CV

TL;DR: 提出了一种基于改进U-net结构的签名去噪模型，通过离散小波变换和PCA变换增强去噪能力，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 手写签名识别中，干扰信息多导致识别困难，需提升签名图像的清晰度和可读性。

Method: 采用改进的U-net结构，结合离散小波变换和PCA变换增强去噪能力。

Result: 模型去噪效果显著优于传统方法，提升了签名图像的清晰度和可读性。

Conclusion: 该模型为签名分析和识别提供了更可靠的技术支持。

Abstract: Handwriting signatures, as an important means of identity recognition, are
widely used in multiple fields such as financial transactions, commercial
contracts and personal affairs due to their legal effect and uniqueness. In
forensic science appraisals, the analysis of offline handwriting signatures
requires the appraiser to provide a certain number of signature samples, which
are usually derived from various historical contracts or archival materials.
However, the provided handwriting samples are often mixed with a large amount
of interfering information, which brings severe challenges to handwriting
identification work. This study proposes a signature handwriting denoising
model based on the improved U-net structure, aiming to enhance the robustness
of the signature recognition system. By introducing discrete wavelet transform
and PCA transform, the model's ability to suppress noise has been enhanced. The
experimental results show that this modelis significantly superior to the
traditional methods in denoising effect, can effectively improve the clarity
and readability of the signed images, and provide more reliable technical
support for signature analysis and recognition.

</details>


### [30] [Out-of-Distribution Detection with Adaptive Top-K Logits Integration](https://arxiv.org/abs/2507.00368)
*Hikaru Shijo,Yutaka Yoshihama,Kenichi Yadani,Norifumi Murata*

Main category: cs.CV

TL;DR: 提出了一种名为ATLI的新方法，通过自适应选择top-k logits并结合最大logit，显著提升了OOD检测的性能。


<details>
  <summary>Details</summary>
Motivation: 神经网络对分布外（OOD）样本的预测往往过于自信，因此OOD检测对提升机器学习的安全性至关重要。

Method: ATLI方法自适应地确定每个模型特有的top-k logits，并将最大logit与其他top-k logits结合。

Result: 在ImageNet-1K基准测试中，ATLI将FPR95降低了6.73%（相比MaxLogit）和额外的2.67%（相比其他先进方法）。

Conclusion: ATLI方法通过有效利用多个logits，显著提升了OOD检测的性能。

Abstract: Neural networks often make overconfident predictions from out-of-distribution
(OOD) samples. Detection of OOD data is therefore crucial to improve the safety
of machine learning. The simplest and most powerful method for OOD detection is
MaxLogit, which uses the model's maximum logit to provide an OOD score. We have
discovered that, in addition to the maximum logit, some other logits are also
useful for OOD detection. Based on this finding, we propose a new method called
ATLI (Adaptive Top-k Logits Integration), which adaptively determines effective
top-k logits that are specific to each model and combines the maximum logit
with the other top-k logits. In this study we evaluate our proposed method
using ImageNet-1K benchmark. Extensive experiments showed our proposed method
to reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit
approach, and decreased FPR95 by an additional 2.67% compared to other
state-of-the-art methods.

</details>


### [31] [PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching](https://arxiv.org/abs/2507.00371)
*Xin Yang,Ruiming Du,Hanyang Huang,Jiayang Xie,Pengyao Xie,Leisen Fang,Ziyue Guo,Nanjun Jiang,Yu Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: PlantSegNeRF是一种新方法，通过多视角RGB图像序列直接生成高精度植物器官点云，显著提升了分割精度和通用性。


<details>
  <summary>Details</summary>
Motivation: 现有植物点云器官分割技术在分辨率、精度和跨物种通用性方面存在局限，需要更高效的解决方案。

Method: 结合2D实例分割、实例匹配模块和实例NeRF，生成包含颜色、密度、语义和实例信息的隐式场景，最终转换为高精度点云。

Result: 在语义和实例分割任务中，PlantSegNeRF表现优于现有方法，各项指标平均提升11.7%至38.2%。

Conclusion: PlantSegNeRF为植物器官表型分析提供了高质量3D数据，支持大规模植物科学模型开发。

Abstract: Organ segmentation of plant point clouds is a prerequisite for the
high-resolution and accurate extraction of organ-level phenotypic traits.
Although the fast development of deep learning has boosted much research on
segmentation of plant point clouds, the existing techniques for organ
segmentation still face limitations in resolution, segmentation accuracy, and
generalizability across various plant species. In this study, we proposed a
novel approach called plant segmentation neural radiance fields (PlantSegNeRF),
aiming to directly generate high-precision instance point clouds from
multi-view RGB image sequences for a wide range of plant species. PlantSegNeRF
performed 2D instance segmentation on the multi-view images to generate
instance masks for each organ with a corresponding ID. The multi-view instance
IDs corresponding to the same plant organ were then matched and refined using a
specially designed instance matching module. The instance NeRF was developed to
render an implicit scene, containing color, density, semantic and instance
information. The implicit scene was ultimately converted into high-precision
plant instance point clouds based on the volume density. The results proved
that in semantic segmentation of point clouds, PlantSegNeRF outperformed the
commonly used methods, demonstrating an average improvement of 16.1%, 18.3%,
17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the
second-best results on structurally complex datasets. More importantly,
PlantSegNeRF exhibited significant advantages in plant point cloud instance
segmentation tasks. Across all plant datasets, it achieved average improvements
of 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.
This study extends the organ-level plant phenotyping and provides a
high-throughput way to supply high-quality 3D data for the development of
large-scale models in plant science.

</details>


### [32] [Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur](https://arxiv.org/abs/2507.00372)
*Xinge Yang,Chuong Nguyen,Wenbin Wang,Kaizhang Kang,Wolfgang Heidrich,Xiaoxing Li*

Main category: cs.CV

TL;DR: 提出了一种高效且可扩展的数据集合成方法，用于解决大光圈相机因浅景深导致的图像模糊问题，无需依赖真实数据微调。


<details>
  <summary>Details</summary>
Motivation: 现代大光圈相机因浅景深导致焦外物体模糊，固定焦距相机（如智能眼镜）因尺寸和功耗限制难以添加自动对焦机制，现有开源数据集训练的深度学习模型因光学像差和散焦特性不匹配而表现不佳。

Method: 同时建模深度依赖的散焦和空间变化的光学像差，解决计算复杂性和高质量RGB-D数据集稀缺问题。

Result: 实验表明，在低分辨率合成图像上训练的网络能有效泛化到高分辨率（12MP）真实世界图像。

Conclusion: 提出的方法在无需真实数据微调的情况下，有效解决了浅景深和光学像差问题，具有实际应用潜力。

Abstract: Modern cameras with large apertures often suffer from a shallow depth of
field, resulting in blurry images of objects outside the focal plane. This
limitation is particularly problematic for fixed-focus cameras, such as those
used in smart glasses, where adding autofocus mechanisms is challenging due to
form factor and power constraints. Due to unmatched optical aberrations and
defocus properties unique to each camera system, deep learning models trained
on existing open-source datasets often face domain gaps and do not perform well
in real-world settings. In this paper, we propose an efficient and scalable
dataset synthesis approach that does not rely on fine-tuning with real-world
data. Our method simultaneously models depth-dependent defocus and spatially
varying optical aberrations, addressing both computational complexity and the
scarcity of high-quality RGB-D datasets. Experimental results demonstrate that
a network trained on our low resolution synthetic images generalizes
effectively to high resolution (12MP) real-world images across diverse scenes.

</details>


### [33] [Customizable ROI-Based Deep Image Compression](https://arxiv.org/abs/2507.00373)
*Ian Jin,Fanxin Xia,Feng Ding,Xinfeng Zhang,Meiqin Liu,Yao Zhao,Weisi Lin,Lili Meng*

Main category: cs.CV

TL;DR: 提出了一种可定制的基于ROI的深度图像压缩范式，通过文本控制ROI定义和可调节的质量权衡机制。


<details>
  <summary>Details</summary>
Motivation: 现有ROI图像压缩方案无法满足用户多样化的需求，如自定义ROI或调整ROI与非ROI的质量权衡。

Method: 设计了文本控制掩码获取模块（TMA）、可定制值分配机制（CVA）和潜在掩码注意力模块（LMA）。

Result: 实验证明该方法有效支持ROI自定义和质量权衡管理。

Conclusion: 该范式成功解决了ROI定义和质量权衡的定制需求。

Abstract: Region of Interest (ROI)-based image compression optimizes bit allocation by
prioritizing ROI for higher-quality reconstruction. However, as the users
(including human clients and downstream machine tasks) become more diverse,
ROI-based image compression needs to be customizable to support various
preferences. For example, different users may define distinct ROI or require
different quality trade-offs between ROI and non-ROI. Existing ROI-based image
compression schemes predefine the ROI, making it unchangeable, and lack
effective mechanisms to balance reconstruction quality between ROI and non-ROI.
This work proposes a paradigm for customizable ROI-based deep image
compression. First, we develop a Text-controlled Mask Acquisition (TMA) module,
which allows users to easily customize their ROI for compression by just
inputting the corresponding semantic \emph{text}. It makes the encoder
controlled by text. Second, we design a Customizable Value Assign (CVA)
mechanism, which masks the non-ROI with a changeable extent decided by users
instead of a constant one to manage the reconstruction quality trade-off
between ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)
module, where the latent spatial prior of the mask and the latent
Rate-Distortion Optimization (RDO) prior of the image are extracted and fused
in the latent space, and further used to optimize the latent representation of
the source image. Experimental results demonstrate that our proposed
customizable ROI-based deep image compression paradigm effectively addresses
the needs of customization for ROI definition and mask acquisition as well as
the reconstruction quality trade-off management between the ROI and non-ROI.

</details>


### [34] [MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis](https://arxiv.org/abs/2507.00377)
*Jianhao Xie,Ziang Zhang,Zhenyu Weng,Yuesheng Zhu,Guibo Luo*

Main category: cs.CV

TL;DR: MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医学图像。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中高质量训练数据稀缺的问题，同时克服扩散模型对大规模数据集和高图像质量的依赖。

Method: 提出MedDiff-FT方法，包括动态自适应引导掩码、轻量级随机掩码生成器和自动化质量评估协议。

Result: 在五个医学分割数据集上，MedDiff-FT的合成图像-掩码对将SOTA方法的Dice分数平均提高了1%。

Conclusion: MedDiff-FT在生成质量、多样性和计算效率之间取得了平衡，为医学数据增强提供了实用解决方案。

Abstract: Recent advancements in deep learning for medical image segmentation are often
limited by the scarcity of high-quality training data.While diffusion models
provide a potential solution by generating synthetic images, their
effectiveness in medical imaging remains constrained due to their reliance on
large-scale medical datasets and the need for higher image quality. To address
these challenges, we present MedDiff-FT, a controllable medical image
generation method that fine-tunes a diffusion foundation model to produce
medical images with structural dependency and domain specificity in a
data-efficient manner. During inference, a dynamic adaptive guiding mask
enforces spatial constraints to ensure anatomically coherent synthesis, while a
lightweight stochastic mask generator enhances diversity through hierarchical
randomness injection. Additionally, an automated quality assessment protocol
filters suboptimal outputs using feature-space metrics, followed by mask
corrosion to refine fidelity. Evaluated on five medical segmentation
datasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's
segmentation performance by an average of 1% in Dice score. The framework
effectively balances generation quality, diversity, and computational
efficiency, offering a practical solution for medical data augmentation. The
code is available at https://github.com/JianhaoXie1/MedDiff-FT.

</details>


### [35] [Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space](https://arxiv.org/abs/2507.00392)
*Yingping Liang,Yutao Hu,Wenqi Shao,Ying Fu*

Main category: cs.CV

TL;DR: 提出了一种名为Lift to Match (L2M)的两阶段框架，通过将2D图像提升到3D空间，利用大规模单视图图像实现鲁棒的特征匹配。


<details>
  <summary>Details</summary>
Motivation: 现有特征匹配方法依赖稀缺且干净的多视图图像集，限制了其在多样化和挑战性场景中的泛化能力；传统特征编码器通常基于单视图2D图像训练，难以捕捉3D感知的对应关系。

Method: 第一阶段通过多视图图像合成和3D特征高斯表示学习3D感知特征编码器；第二阶段结合新视图渲染策略和大规模单视图图像合成数据，学习特征解码器以实现跨域泛化。

Result: 实验表明，该方法在零样本评估基准上表现出优异的泛化能力。

Conclusion: L2M框架通过3D几何知识注入和大规模数据合成，显著提升了特征匹配的鲁棒性和泛化能力。

Abstract: Feature matching plays a fundamental role in many computer vision tasks, yet
existing methods heavily rely on scarce and clean multi-view image collections,
which constrains their generalization to diverse and challenging scenarios.
Moreover, conventional feature encoders are typically trained on single-view 2D
images, limiting their capacity to capture 3D-aware correspondences. In this
paper, we propose a novel two-stage framework that lifts 2D images to 3D space,
named as \textbf{Lift to Match (L2M)}, taking full advantage of large-scale and
diverse single-view images. To be specific, in the first stage, we learn a
3D-aware feature encoder using a combination of multi-view image synthesis and
3D feature Gaussian representation, which injects 3D geometry knowledge into
the encoder. In the second stage, a novel-view rendering strategy, combined
with large-scale synthetic data generation from single-view images, is employed
to learn a feature decoder for robust feature matching, thus achieving
generalization across diverse domains. Extensive experiments demonstrate that
our method achieves superior generalization across zero-shot evaluation
benchmarks, highlighting the effectiveness of the proposed framework for robust
feature matching.

</details>


### [36] [Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains](https://arxiv.org/abs/2507.00401)
*Xin Xu,Eibe Frank,Geoffrey Holmes*

Main category: cs.CV

TL;DR: 论文提出了一种名为'MIV-head'的新方法，用于解决在无法微调主干网络的情况下进行跨域少样本学习的问题，该方法在性能和成本上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在实际应用中无法微调主干网络时，如何处理低质量和静态嵌入的问题，提出了一种新的少样本分类表示方法。

Method: 方法是通过设计'MIV-head'，一种与预训练主干网络无关且计算高效的分类头，用于少样本域适应。

Result: 实验结果表明，MIV-head在跨域少样本图像分类任务中表现优异，性能接近最先进的适配器方法，且适应成本更低。

Conclusion: 结论是MIV-head是一种高效且性能优越的少样本学习方法，尤其适用于无法微调主干网络的场景。

Abstract: We investigate cross-domain few-shot learning under the constraint that
fine-tuning of backbones (i.e., feature extractors) is impossible or infeasible
-- a scenario that is increasingly common in practical use cases. Handling the
low-quality and static embeddings produced by frozen, "black-box" backbones
leads to a problem representation of few-shot classification as a series of
multiple instance verification (MIV) tasks. Inspired by this representation, we
introduce a novel approach to few-shot domain adaptation, named the "MIV-head",
akin to a classification head that is agnostic to any pretrained backbone and
computationally efficient. The core components designed for the MIV-head, when
trained on few-shot data from a target domain, collectively yield strong
performance on test data from that domain. Importantly, it does so without
fine-tuning the backbone, and within the "meta-testing" phase. Experimenting
under various settings and on an extension of the Meta-dataset benchmark for
cross-domain few-shot image classification, using representative off-the-shelf
convolutional neural network and vision transformer backbones pretrained on
ImageNet1K, we show that the MIV-head achieves highly competitive accuracy when
compared to state-of-the-art "adapter" (or partially fine-tuning) methods
applied to the same backbones, while incurring substantially lower adaptation
cost. We also find well-known "classification head" approaches lag far behind
in terms of accuracy. Ablation study empirically justifies the core components
of our approach. We share our code at https://github.com/xxweka/MIV-head.

</details>


### [37] [DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting](https://arxiv.org/abs/2507.00429)
*Jingyi Pan,Dan Xu,Qiong Luo*

Main category: cs.CV

TL;DR: DiGA3D是一个统一的3D修复管道，通过扩散模型在粗到细的过程中传播一致的外观和几何形状，解决了多视图修复中的一致性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的3D修复方法在处理远离参考视图的视角、多视图图像的外观不一致性以及几何变化时的几何不一致性方面存在挑战。

Method: DiGA3D采用多参考视图选择策略、注意力特征传播机制（AFP）和纹理-几何评分蒸馏采样（TG-SDS）损失来提高修复的一致性和质量。

Result: 实验表明，DiGA3D在多种3D修复任务中表现出色，有效解决了外观和几何一致性问题。

Conclusion: DiGA3D为3D修复提供了一个高效且统一的解决方案，显著提升了修复质量。

Abstract: Developing a unified pipeline that enables users to remove, re-texture, or
replace objects in a versatile manner is crucial for text-guided 3D inpainting.
However, there are still challenges in performing multiple 3D inpainting tasks
within a unified framework: 1) Single reference inpainting methods lack
robustness when dealing with views that are far from the reference view. 2)
Appearance inconsistency arises when independently inpainting multi-view images
with 2D diffusion priors; 3) Geometry inconsistency limits performance when
there are significant geometric changes in the inpainting regions. To tackle
these challenges, we introduce DiGA3D, a novel and versatile 3D inpainting
pipeline that leverages diffusion models to propagate consistent appearance and
geometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy
for selecting multiple reference views to reduce errors during propagation.
Next, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that
propagates attention features from the selected reference views to other views
via diffusion models to maintain appearance consistency. Furthermore, DiGA3D
introduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to
further improve the geometric consistency of inpainted 3D scenes. Extensive
experiments on multiple 3D inpainting tasks demonstrate the effectiveness of
our method. The project page is available at https://rorisis.github.io/DiGA3D/.

</details>


### [38] [MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2507.00430)
*Huanxin Yang,Qiwen Wang*

Main category: cs.CV

TL;DR: 该论文提出了一种结合频域分析的手写数学表达式识别方法（MFH），利用离散余弦变换（DCT）提升识别性能。


<details>
  <summary>Details</summary>
Motivation: 手写数学表达式识别（HMER）因复杂的公式结构和字符布局而面临挑战，频域信息可能提供结构分析的辅助。

Method: 提出MFH方法，将频域分析与HMER结合，利用DCT提取频域信息。

Result: 在多个基准模型上实现性能提升，MFH-CoMER在CROHME 2014/2016/2019测试集上分别达到61.66%/62.07%/63.72%的准确率。

Conclusion: 频域信息能有效提升HMER性能，MFH方法具有显著效果。

Abstract: Handwritten mathematical expression recognition (HMER) suffers from complex
formula structures and character layouts in sequence prediction. In this paper,
we incorporate frequency domain analysis into HMER and propose a method that
marries frequency domain with HMER (MFH), leveraging the discrete cosine
transform (DCT). We emphasize the structural analysis assistance of frequency
information for recognizing mathematical formulas. When implemented on various
baseline models, our network exhibits a consistent performance enhancement,
demonstrating the efficacy of frequency domain information. Experiments show
that our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on
the CROHME 2014/2016/2019 test sets. The source code is available at
https://github.com/Hryxyhe/MFH.

</details>


### [39] [Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration](https://arxiv.org/abs/2507.00447)
*Xin Luo,Menglin Zhang,Yunwei Lan,Tianyu Zhang,Rui Li,Chang Liu,Dong Liu*

Main category: cs.CV

TL;DR: Latent-PMRF 提出了一种在变分自编码器（VAE）潜在空间中重新定义 PMRF 的方法，以更好地与人类感知对齐，从而在盲脸恢复任务中实现更优的感知-失真权衡。


<details>
  <summary>Details</summary>
Motivation: PMRF 在像素空间建模时难以与人类感知对齐，因此需要一种新的方法来优化感知质量与保真度的平衡。

Method: 通过将 PMRF 重新定义在 VAE 的潜在空间中，利用潜在表示的最小失真估计来约束失真，并通过改进的 VAE 设计提升性能。

Result: Latent-PMRF 在盲脸恢复任务中表现出色，实现了比现有方法更优的感知-失真权衡，并在收敛效率上显著提升（FID 指标上比 PMRF 快 5.79 倍）。

Conclusion: Latent-PMRF 通过潜在空间建模和优化的 VAE 设计，显著提升了感知质量与保真度的平衡，为盲脸恢复任务提供了高效且性能优越的解决方案。

Abstract: The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face
restoration algorithms must balance perceptual quality and fidelity. To achieve
minimal distortion while maintaining perfect perceptual quality, Posterior-Mean
Rectified Flow (PMRF) proposes a flow based approach where source distribution
is minimum distortion estimations. Although PMRF is shown to be effective, its
pixel-space modeling approach limits its ability to align with human
perception, where human perception is defined as how humans distinguish between
two image distributions. In this work, we propose Latent-PMRF, which
reformulates PMRF in the latent space of a variational autoencoder (VAE),
facilitating better alignment with human perception during optimization. By
defining the source distribution on latent representations of minimum
distortion estimation, we bound the minimum distortion by the VAE's
reconstruction error. Moreover, we reveal the design of VAE is crucial, and our
proposed VAE significantly outperforms existing VAEs in both reconstruction and
restoration. Extensive experiments on blind face restoration demonstrate the
superiority of Latent-PMRF, offering an improved PD-tradeoff compared to
existing methods, along with remarkable convergence efficiency, achieving a
5.79X speedup over PMRF in terms of FID. Our code will be available as
open-source.

</details>


### [40] [ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales](https://arxiv.org/abs/2507.00454)
*Yihao Zhen,Qiang Wang,Yu Qiao,Liangqiong Qu,Huijie Fan*

Main category: cs.CV

TL;DR: ATSTrack通过对齐视觉和语言输入的时间和空间尺度，提升了视觉语言跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉输入和语言描述因目标运动导致的不对齐问题，以及两者在时间和空间尺度上的固有差异。

Method: 将语言描述分解为具有不同属性的短语，并细粒度地修改其特征；引入视觉-语言标记以利用前一帧的语言信息指导视觉特征提取。

Result: 实验表明ATSTrack性能与现有方法相当。

Conclusion: ATSTrack通过时间和空间尺度的对齐，有效提升了视觉语言跟踪的效果。

Abstract: A main challenge of Visual-Language Tracking (VLT) is the misalignment
between visual inputs and language descriptions caused by target movement.
Previous trackers have explored many effective feature modification methods to
preserve more aligned features. However, an important yet unexplored factor
ultimately hinders their capability, which is the inherent differences in the
temporal and spatial scale of information between visual and language inputs.
To address this issue, we propose a novel visual-language tracker that enhances
the effect of feature modification by \textbf{A}ligning \textbf{T}emporal and
\textbf{S}patial scale of different input components, named as
\textbf{ATSTrack}. Specifically, we decompose each language description into
phrases with different attributes based on their temporal and spatial
correspondence with visual inputs, and modify their features in a fine-grained
manner. Moreover, we introduce a Visual-Language token that comprises modified
linguistic information from the previous frame to guide the model to extract
visual features that are more relevant to language description, thereby
reducing the impact caused by the differences in spatial scale. Experimental
results show that our proposed ATSTrack achieves performance comparable to
existing methods. Our code will be released.

</details>


### [41] [Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation](https://arxiv.org/abs/2507.00462)
*Jizhou Han,Chenhao Ding,SongLin Dong,Yuhang He,Xinyuan Gao,Yihong Gong*

Main category: cs.CV

TL;DR: MS-TTA是一种无需训练的测试时适应方法，通过kNN Mean-Shift增强CLIP的特征表示，提升分布偏移下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖高置信度样本，忽略了低置信度样本的潜力，导致在分布偏移下表现不佳。

Method: 使用单步kNN Mean-Shift优化所有测试样本的特征表示，提高特征紧凑性和类别可分性。

Result: 在OOD和跨数据集基准测试中，MS-TTA表现优于现有方法，实现稳健适应。

Conclusion: MS-TTA无需额外训练即可显著提升CLIP在分布偏移下的性能。

Abstract: Visual-language models (VLMs) like CLIP exhibit strong generalization but
struggle with distribution shifts at test time. Existing training-free
test-time adaptation (TTA) methods operate strictly within CLIP's original
feature space, relying on high-confidence samples while overlooking the
potential of low-confidence ones. We propose MS-TTA, a training-free approach
that enhances feature representations beyond CLIP's space using a single-step
k-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA
improves feature compactness and class separability, leading to more stable
adaptation. Additionally, a cache of refined embeddings further enhances
inference by providing Mean Shift enhanced logits. Extensive evaluations on OOD
and cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms
state-of-the-art training-free TTA methods, achieving robust adaptation without
requiring additional training.

</details>


### [42] [Bisecle: Binding and Separation in Continual Learning for Video Language Understanding](https://arxiv.org/abs/2507.00469)
*Yue Tan,Xiaoqian Hu,Hao Xue,Celso De Melo,Flora D. Salim*

Main category: cs.CV

TL;DR: Bisecle是一种受海马体启发的视频语言持续学习方法，通过多方向监督模块和对比提示学习方案，解决了大型多模态基础模型在持续学习中的灾难性遗忘和更新冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频通常是不断变化的数据流，需要模型持续适应新的数据分布和场景。现有的持续学习框架在大型多模态基础模型中面临灾难性遗忘和更新冲突的挑战。

Method: 提出Bisecle方法，结合多方向监督模块捕捉跨模态关系，设计对比提示学习方案隔离任务特定知识，模拟海马体的快速绑定和模式分离机制。

Result: 在多个VideoQA基准测试中，Bisecle有效减轻了遗忘问题，并增强了跨任务泛化能力。

Conclusion: Bisecle通过模拟海马体的记忆机制，为视频语言持续学习提供了一种高效且鲁棒的解决方案。

Abstract: Frontier vision-language models (VLMs) have made remarkable improvements in
video understanding tasks. However, real-world videos typically exist as
continuously evolving data streams (e.g., dynamic scenes captured by wearable
glasses), necessitating models to continually adapt to shifting data
distributions and novel scenarios. Considering the prohibitive computational
costs of fine-tuning models on new tasks, usually, a small subset of parameters
is updated while the bulk of the model remains frozen. This poses new
challenges to existing continual learning frameworks in the context of large
multimodal foundation models, i.e., catastrophic forgetting and update
conflict. While the foundation models struggle with parameter-efficient
continual learning, the hippocampus in the human brain has evolved highly
efficient mechanisms for memory formation and consolidation. Inspired by the
rapid Binding and pattern separation mechanisms in the hippocampus, in this
work, we propose Bisecle for video-language continual learning, where a
multi-directional supervision module is used to capture more cross-modal
relationships and a contrastive prompt learning scheme is designed to isolate
task-specific knowledge to facilitate efficient memory storage. Binding and
separation processes further strengthen the ability of VLMs to retain complex
experiences, enabling robust and efficient continual learning in video
understanding tasks. We perform a thorough evaluation of the proposed Bisecle,
demonstrating its ability to mitigate forgetting and enhance cross-task
generalization on several VideoQA benchmarks.

</details>


### [43] [ARIG: Autoregressive Interactive Head Generation for Real-time Conversations](https://arxiv.org/abs/2507.00472)
*Ying Guo,Xi Liu,Cheng Zhen,Pengfei Yan,Xiaoming Wei*

Main category: cs.CV

TL;DR: 提出了一种基于自回归的帧级框架ARIG，用于实时生成更具交互真实感的头部运动。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在信号获取、上下文行为理解和切换平滑性上的局限性，以实现实时和真实的交互。

Method: 采用非向量量化的自回归过程建模运动预测，结合扩散过程表示运动分布；强调交互行为理解（IBU）和对话状态理解（CSU）。

Result: 实验验证了模型的有效性。

Conclusion: ARIG框架在实时性和交互真实感上表现优异。

Abstract: Face-to-face communication, as a common human activity, motivates the
research on interactive head generation. A virtual agent can generate motion
responses with both listening and speaking capabilities based on the audio or
motion signals of the other user and itself. However, previous clip-wise
generation paradigm or explicit listener/speaker generator-switching methods
have limitations in future signal acquisition, contextual behavioral
understanding, and switching smoothness, making it challenging to be real-time
and realistic. In this paper, we propose an autoregressive (AR) based
frame-wise framework called ARIG to realize the real-time generation with
better interaction realism. To achieve real-time generation, we model motion
prediction as a non-vector-quantized AR process. Unlike discrete codebook-index
prediction, we represent motion distribution using diffusion procedure,
achieving more accurate predictions in continuous space. To improve interaction
realism, we emphasize interactive behavior understanding (IBU) and detailed
conversational state understanding (CSU). In IBU, based on dual-track
dual-modal signals, we summarize short-range behaviors through
bidirectional-integrated learning and perform contextual understanding over
long ranges. In CSU, we use voice activity signals and context features of IBU
to understand the various states (interruption, feedback, pause, etc.) that
exist in actual conversations. These serve as conditions for the final
progressive motion prediction. Extensive experiments have verified the
effectiveness of our model.

</details>


### [44] [ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis](https://arxiv.org/abs/2507.00474)
*Yaofei Duan,Yuhao Huang,Xin Yang,Luyi Han,Xinyu Xie,Zhiyuan Zhu,Ping He,Ka-Hou Chan,Ligang Cui,Sio-Kei Im,Dong Ni,Tao Tan*

Main category: cs.CV

TL;DR: 提出了一种名为ADAptation的无监督主动学习框架，用于解决深度学习模型在跨域诊断中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在训练和测试域分布不一致时性能下降，而传统主动学习方法难以处理跨域数据差异。

Method: 利用扩散模型进行分布对齐，结合紧凑特征聚类和双评分机制选择样本。

Result: 在四个乳腺超声数据集上验证了方法的有效性，优于现有主动学习方法。

Conclusion: ADAptation框架在临床领域适应中表现出色，具有广泛的应用潜力。

Abstract: Deep learning-based diagnostic models often suffer performance drops due to
distribution shifts between training (source) and test (target) domains.
Collecting and labeling sufficient target domain data for model retraining
represents an optimal solution, yet is limited by time and scarce resources.
Active learning (AL) offers an efficient approach to reduce annotation costs
while maintaining performance, but struggles to handle the challenge posed by
distribution variations across different datasets. In this study, we propose a
novel unsupervised Active learning framework for Domain Adaptation, named
ADAptation, which efficiently selects informative samples from multi-domain
data pools under limited annotation budget. As a fundamental step, our method
first utilizes the distribution homogenization capabilities of diffusion models
to bridge cross-dataset gaps by translating target images into source-domain
style. We then introduce two key innovations: (a) a hypersphere-constrained
contrastive learning network for compact feature clustering, and (b) a
dual-scoring mechanism that quantifies and balances sample uncertainty and
representativeness. Extensive experiments on four breast ultrasound datasets
(three public and one in-house/multi-center) across five common deep
classifiers demonstrate that our method surpasses existing strong AL-based
competitors, validating its effectiveness and generalization for clinical
domain adaptation. The code is available at the anonymized link:
https://github.com/miccai25-966/ADAptation.

</details>


### [45] [Just Noticeable Difference for Large Multimodal Models](https://arxiv.org/abs/2507.00490)
*Zijian Chen,Yuan Tian,Yuze Sun,Wei Sun,Zicheng Zhang,Weisi Lin,Guangtao Zhai,Wenjun Zhang*

Main category: cs.CV

TL;DR: 本文提出了LMM-JND概念，用于量化大型多模态模型（LMMs）的视觉盲区，并构建了VPA-JND数据集，揭示了当前LMMs在视觉感知任务中的不足。


<details>
  <summary>Details</summary>
Motivation: 研究LMMs在视觉感知任务中的盲区，以解决潜在的安全问题和响应效率不足。

Method: 提出LMM-JND概念及其确定流程，构建包含21.5k参考图像和489k刺激的VPA-JND数据集，分析多种LMMs的表现。

Result: 发现当前LMMs（如GPT-4o和InternVL2.5）在基本比较查询中表现不佳，远低于人类水平。

Conclusion: LMM-JND为研究LMMs提供了新视角，其可预测性对安全至关重要。

Abstract: Just noticeable difference (JND), the minimum change that the human visual
system (HVS) can perceive, has been studied for decades. Although recent work
has extended this line of research into machine vision, there has been a
scarcity of studies systematically exploring its perceptual boundaries across
multiple tasks and stimulus types, particularly in the current era of rapidly
advancing large multimodal models (LMMs), where studying the multifaceted
capabilities of models has become a mainstream focus. Moreover, the perceptual
defects of LMMs are not investigated thoroughly, resulting in potential
security issues and suboptimal response efficiency. In this paper, we take an
initial attempt and demonstrate that there exist significant visual blind spots
in current LMMs. To systemically quantify this characteristic, we propose a new
concept, {\bf LMM-JND}, together with its determination pipeline. Targeting
uncovering the behavior commonalities in HVS-aligned visual perception tasks,
we delve into several LMM families and construct a large-scale dataset, named
VPA-JND, which contains 21.5k reference images with over 489k stimuli across 12
distortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where
state-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle
with basic comparison queries and fall significantly short of human-level
visual performance. We further explore the effects of vision and language
backbones and find a notable correlation between their design philosophy that
may instruct the future refinement of LMMs for their visual acuity. Together,
our research underscores the significance of LMM-JND as a unique perspective
for studying LMMs, and predictable LMM-JND is crucial for security concerns.
This work will be available at https://github.com/zijianchen98/LMM-JND.

</details>


### [46] [Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models](https://arxiv.org/abs/2507.00493)
*Fenil R. Doshi,Thomas Fel,Talia Konkle,George Alvarez*

Main category: cs.CV

TL;DR: 论文提出了一种新的形状评估方法（CSS），用于衡量模型对全局配置形状的敏感性，发现高CSS模型依赖长程交互，并展示了形状和纹理的整合对稳健视觉系统的重要性。


<details>
  <summary>Details</summary>
Motivation: 当代视觉模型主要依赖局部纹理线索，忽略了形状配置的重要性，导致特征脆弱且非组合性。论文旨在评估模型对形状配置的绝对能力。

Method: 提出Configural Shape Score (CSS)，通过Object-Anagram对测试模型对全局配置形状的敏感性，并分析不同模型的性能。

Result: 发现高CSS模型（如DINOv2、SigLIP2和EVA-CLIP）依赖长程交互，且CSS能预测其他形状依赖任务的表现。

Conclusion: 构建稳健、通用的视觉系统需要整合局部纹理和全局配置形状，而非在二者间做人为选择。

Abstract: Humans are able to recognize objects based on both local texture cues and the
configuration of object parts, yet contemporary vision models primarily harvest
local texture cues, yielding brittle, non-compositional features. Work on
shape-vs-texture bias has pitted shape and texture representations in
opposition, measuring shape relative to texture, ignoring the possibility that
models (and humans) can simultaneously rely on both types of cues, and
obscuring the absolute quality of both types of representation. We therefore
recast shape evaluation as a matter of absolute configural competence,
operationalized by the Configural Shape Score (CSS), which (i) measures the
ability to recognize both images in Object-Anagram pairs that preserve local
texture while permuting global part arrangement to depict different object
categories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)
uncovers a broad spectrum of configural sensitivity with fully self-supervised
and language-aligned transformers -- exemplified by DINOv2, SigLIP2 and
EVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes
reveal that (iii) high-CSS networks depend on long-range interactions:
radius-controlled attention masks abolish performance showing a distinctive
U-shaped integration profile, and representational-similarity analyses expose a
mid-depth transition from local to global coding. A BagNet control remains at
chance (iv), ruling out "border-hacking" strategies. Finally, (v) we show that
configural shape score also predicts other shape-dependent evals. Overall, we
propose that the path toward truly robust, generalizable, and human-like vision
systems may not lie in forcing an artificial choice between shape and texture,
but rather in architectural and learning frameworks that seamlessly integrate
both local-texture and global configural shape.

</details>


### [47] [Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing](https://arxiv.org/abs/2507.00501)
*Yongzhen Wang,Liangliang Chen,Bingwen Hu,Heng Liu,Xiao-Ping Zhang,Mingqiang Wei*

Main category: cs.CV

TL;DR: Laplace-Mamba结合拉普拉斯频率先验与混合Mamba-CNN架构，用于高效图像去雾，显著提升恢复质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决基于SSM的方法在重建局部结构和高维数据处理上的局限性，恢复精细图像特征效果不佳的问题。

Method: 通过拉普拉斯分解将图像分离为低频和高频成分，分别用SSM和CNN处理，结合信息保留的下采样提升效率。

Result: 在多个基准测试中，Laplace-Mamba在恢复质量和效率上优于现有方法。

Conclusion: Laplace-Mamba框架有效解决了图像去雾中的全局和局部处理问题，具有高效性和优越性能。

Abstract: Recent progress in image restoration has underscored Spatial State Models
(SSMs) as powerful tools for modeling long-range dependencies, owing to their
appealing linear complexity and computational efficiency. However, SSM-based
approaches exhibit limitations in reconstructing localized structures and tend
to be less effective when handling high-dimensional data, frequently resulting
in suboptimal recovery of fine image features. To tackle these challenges, we
introduce Laplace-Mamba, a novel framework that integrates Laplace frequency
prior with a hybrid Mamba-CNN architecture for efficient image dehazing.
Leveraging the Laplace decomposition, the image is disentangled into
low-frequency components capturing global texture and high-frequency components
representing edges and fine details. This decomposition enables specialized
processing via dual parallel pathways: the low-frequency branch employs SSMs
for global context modeling, while the high-frequency branch utilizes CNNs to
refine local structural details, effectively addressing diverse haze scenarios.
Notably, the Laplace transformation facilitates information-preserving
downsampling of low-frequency components in accordance with the Nyquist theory,
thereby significantly improving computational efficiency. Extensive evaluations
across multiple benchmarks demonstrate that our method outperforms
state-of-the-art approaches in both restoration quality and efficiency. The
source code and pretrained models are available at
https://github.com/yz-wang/Laplace-Mamba.

</details>


### [48] [ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation](https://arxiv.org/abs/2507.00502)
*JianChao Zhao,Songlin Dong*

Main category: cs.CV

TL;DR: ExPaMoE是一种基于可扩展并行混合专家架构的新框架，用于解决持续测试时间适应（CTTA）中的特征纠缠和灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有CTTA方法依赖跨所有域的共享模型参数，容易在大的或非平稳域偏移下出现特征纠缠和灾难性遗忘。

Method: ExPaMoE通过双分支专家设计和基于频率域线索的实时分布变化检测器（SODD）动态扩展专家池，解耦领域通用和领域特定知识。

Result: 在多个标准基准测试中，ExPaMoE表现优异，展示了强大的鲁棒性、可扩展性和抗遗忘能力。

Conclusion: ExPaMoE在复杂域演化下的长期适应中表现出色，显著优于现有方法。

Abstract: Continual Test-Time Adaptation (CTTA) aims to enable models to adapt
on-the-fly to a stream of unlabeled data under evolving distribution shifts.
However, existing CTTA methods typically rely on shared model parameters across
all domains, making them vulnerable to feature entanglement and catastrophic
forgetting in the presence of large or non-stationary domain shifts. To address
this limitation, we propose \textbf{ExPaMoE}, a novel framework based on an
\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples
domain-general and domain-specific knowledge via a dual-branch expert design
with token-guided feature separation, and dynamically expands its expert pool
based on a \emph{Spectral-Aware Online Domain Discriminator} (SODD) that
detects distribution changes in real-time using frequency-domain cues.
Extensive experiments demonstrate the superiority of ExPaMoE across diverse
CTTA scenarios. We evaluate our method on standard benchmarks including
CIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic
segmentation. Additionally, we introduce \textbf{ImageNet++}, a large-scale and
realistic CTTA benchmark built from multiple ImageNet-derived datasets, to
better reflect long-term adaptation under complex domain evolution. ExPaMoE
consistently outperforms prior arts, showing strong robustness, scalability,
and resistance to forgetting.

</details>


### [49] [LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs](https://arxiv.org/abs/2507.00505)
*Haoran Lou,Chunxiao Fan,Ziyan Liu,Yuexin Wu,Xinxiang Wang*

Main category: cs.CV

TL;DR: LLaVA-SP通过添加六个空间视觉标记增强视觉表示，提升多模态大语言模型的细节理解能力。


<details>
  <summary>Details</summary>
Motivation: CLIP-ViT在捕捉局部图像特征时表现不佳，影响多模态大语言模型的视觉理解能力。

Method: 提出LLaVA-SP，使用卷积核生成空间视觉标记，并通过交叉注意力机制融合细粒度视觉信息。

Result: 在多个多模态基准测试中表现优异，优于现有LLaVA-1.5模型。

Conclusion: LLaVA-SP通过简单有效的空间标记增强，显著提升了多模态大语言模型的视觉理解能力。

Abstract: The architecture of multimodal large language models (MLLMs) commonly
connects a vision encoder, often based on CLIP-ViT, to a large language model.
While CLIP-ViT works well for capturing global image features, it struggles to
model local relationships between adjacent patches, leading to weaker visual
representation, which in turn affects the detailed understanding ability of
MLLMs. To solve this, we propose LLaVA-SP, which \textbf{ only adds six spatial
visual tokens} to the original visual tokens to enhance the visual
representation. Our approach offers three key advantages: 1)We propose a novel
Projector, which uses convolutional kernels to derive visual spatial tokens
from ViT patch features, simulating two visual spatial ordering approaches:
``from central region to global" and ``from abstract to specific". Then, a
cross-attention mechanism is applied to fuse fine-grained visual information,
enriching the overall visual representation. 2) We present two model variants:
LLaVA-SP-Cropping, which focuses on detail features through progressive
cropping, and LLaVA-SP-Pooling, which captures global semantics through
adaptive pooling, enabling the model to handle diverse visual understanding
tasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,
achieves significant performance improvements across various multimodal
benchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple
tasks with nearly identical inference latency. The code and models are
available at
\href{https://github.com/CnFaker/LLaVA-SP}{\texttt{https://github.com/CnFaker/LLaVA-SP}}.

</details>


### [50] [SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning](https://arxiv.org/abs/2507.00506)
*Yunfei Xie,Yuxuan Cheng,Juncheng Wu,Haoyu Zhang,Yuyin Zhou,Shoudong Han*

Main category: cs.CV

TL;DR: 提出了一种名为SCING的简单有效框架，通过选择性跨模态提示调优增强跨模态对齐和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂适配器设计或模态特定调优，忽视跨模态交互，导致高计算成本或次优对齐。

Method: 引入选择性视觉提示融合（SVIP）和扰动驱动一致性对齐（PDCA）两种创新方法。

Result: 在多个基准测试中表现优异，实现了性能与计算开销的最佳平衡。

Conclusion: SCING框架在保持高效推理的同时，消除了复杂适配器，显著提升了跨模态对齐效果。

Abstract: Recent advancements in adapting vision-language pre-training models like CLIP
for person re-identification (ReID) tasks often rely on complex adapter design
or modality-specific tuning while neglecting cross-modal interaction, leading
to high computational costs or suboptimal alignment. To address these
limitations, we propose a simple yet effective framework named Selective
Cross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and
robustness against real-world perturbations. Our method introduces two key
innovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a
lightweight module that dynamically injects discriminative visual features into
text prompts via a cross-modal gating mechanism. Moreover, the proposed
Perturbation-Driven Consistency Alignment (PDCA) is a dual-path training
strategy that enforces invariant feature alignment under random image
perturbations by regularizing consistency between original and augmented
cross-modal embeddings. Extensive experiments are conducted on several popular
benchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,
and P-DukeMTMC, which demonstrate the impressive performance of the proposed
method. Notably, our framework eliminates heavy adapters while maintaining
efficient inference, achieving an optimal trade-off between performance and
computational overhead. The code will be released upon acceptance.

</details>


### [51] [Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection](https://arxiv.org/abs/2507.00519)
*Ruize Cui,Jiaan Zhang,Jialun Pei,Kai Wang,Pheng-Ann Heng,Jing Qin*

Main category: cs.CV

TL;DR: TopoNet是一种用于腹腔镜肝脏标志物检测的新型拓扑约束学习框架，结合RGB-D特征和拓扑约束损失，显著提高了检测精度。


<details>
  <summary>Details</summary>
Motivation: 肝脏标志物在腹腔镜手术中提供关键解剖引导，但其管状结构和术中动态变形对自动检测提出了挑战。

Method: 采用双路径编码器（snake-CNN）捕获RGB纹理和深度拓扑结构，提出边界感知拓扑融合模块（BTF）和拓扑约束损失函数。

Result: 在L3D和P2ILF数据集上表现出色，精度和计算复杂度均优。

Conclusion: TopoNet在腹腔镜肝脏手术中具有临床应用潜力。

Abstract: Liver landmarks provide crucial anatomical guidance to the surgeon during
laparoscopic liver surgery to minimize surgical risk. However, the tubular
structural properties of landmarks and dynamic intraoperative deformations pose
significant challenges for automatic landmark detection. In this study, we
introduce TopoNet, a novel topology-constrained learning framework for
laparoscopic liver landmark detection. Our framework adopts a snake-CNN
dual-path encoder to simultaneously capture detailed RGB texture information
and depth-informed topological structures. Meanwhile, we propose a
boundary-aware topology fusion (BTF) module, which adaptively merges RGB-D
features to enhance edge perception while preserving global topology.
Additionally, a topological constraint loss function is embedded, which
contains a center-line constraint loss and a topological persistence loss to
ensure homotopy equivalence between predictions and labels. Extensive
experiments on L3D and P2ILF datasets demonstrate that TopoNet achieves
outstanding accuracy and computational complexity, highlighting the potential
for clinical applications in laparoscopic liver surgery. Our code will be
available at https://github.com/cuiruize/TopoNet.

</details>


### [52] [Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving](https://arxiv.org/abs/2507.00525)
*Djamahl Etchegaray,Yuxia Fu,Zi Huang,Yadan Luo*

Main category: cs.CV

TL;DR: Box-QAymo是一个用于评估和微调视觉语言模型（VLMs）在用户指定对象上的时空推理能力的基准数据集，通过边界框标注实现直观查询。


<details>
  <summary>Details</summary>
Motivation: 现有驾驶导向的VQA数据集局限于全场景描述或路径点预测，无法评估VLMs对局部用户驱动查询的响应能力。

Method: 提出Box-QAymo数据集，采用边界框标注和分层评估协议，包括属性预测、运动理解和时空推理任务。

Result: 评估显示当前VLMs在感知问题上的表现存在显著局限性，揭示了与现实世界性能的差距。

Conclusion: 该工作为开发更鲁棒和可解释的自动驾驶系统奠定了基础，支持真实场景下的有效用户交互。

Abstract: Interpretable communication is essential for safe and trustworthy autonomous
driving, yet current vision-language models (VLMs) often operate under
idealized assumptions and struggle to capture user intent in real-world
scenarios. Existing driving-oriented VQA datasets are limited to full-scene
descriptions or waypoint prediction, preventing the assessment of whether VLMs
can respond to localized user-driven queries. We introduce Box-QAymo, a
box-referring dataset and benchmark designed to both evaluate and finetune VLMs
on spatial and temporal reasoning over user-specified objects. Users express
intent by drawing bounding boxes, offering a fast and intuitive interface for
focused queries in complex scenes. Specifically, we propose a hierarchical
evaluation protocol that begins with binary sanity-check questions to assess
basic model capacities, and progresses to (1) attribute prediction for
box-referred objects, (2) motion understanding of target instances, and (3)
spatiotemporal motion reasoning over inter-object dynamics across frames. To
support this, we crowd-sourced fine-grained object classes and visual
attributes that reflect the complexity drivers encounter, and extract object
trajectories to construct temporally grounded QA pairs. Rigorous quality
control through negative sampling, temporal consistency checks, and
difficulty-aware balancing guarantee dataset robustness and diversity. Our
comprehensive evaluation reveals significant limitations in current VLMs when
queried about perception questions, highlighting the gap in achieving
real-world performance. This work provides a foundation for developing more
robust and interpretable autonomous driving systems that can communicate
effectively with users under real-world conditions. Project page and dataset
are available at https://djamahl99.github.io/qaymo-pages/.

</details>


### [53] [Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation](https://arxiv.org/abs/2507.00537)
*Feng Lin,Marco Chen,Haokui Zhang,Xiaotian Yu,Guangming Lu,Rong Xiao*

Main category: cs.CV

TL;DR: 研究CLIP图像编码器中注意力头的作用，提出注意力消融技术（AAT）以提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 某些注意力头可能对最终表示产生负面影响，消融它们可以提升性能。

Method: 提出AAT方法，通过操纵注意力权重抑制特定头的贡献。

Result: AAT显著提升跨模态检索等任务性能，召回率最高提升11.1%。

Conclusion: AAT能高效优化大规模视觉语言模型，且不增加推理成本。

Abstract: This paper studies the role of attention heads in CLIP's image encoder. While
CLIP has exhibited robust performance across diverse applications, we
hypothesize that certain attention heads negatively affect final
representations and that ablating them can improve performance in downstream
tasks. To capitalize on this insight, we propose a simple yet effective method,
called Attention Ablation Technique (AAT), to suppress the contribution of
specific heads by manipulating attention weights. By integrating two
alternative strategies tailored for different application scenarios, AAT
systematically identifies and ablates detrimental attention heads to enhance
representation quality. Experiments demonstrate that AAT consistently improves
downstream task performance across various domains, boosting recall rate by up
to 11.1% on CLIP-family models for cross-modal retrieval. The results highlight
the potential of AAT to effectively refine large-scale vision-language models
with virtually no increase in inference cost.

</details>


### [54] [LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing](https://arxiv.org/abs/2507.00554)
*Zhenya Yang,Bingchen Gong,Kai Chen,Qi Dou*

Main category: cs.CV

TL;DR: LOD-GS提出了一种基于Level-of-Detail的滤波框架，用于解决3D高斯泼溅中的锯齿问题，通过动态预测滤波强度，显著提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖低通滤波来减少锯齿，但对采样率不敏感，容易导致滤波不足或过度平滑。

Method: LOD-GS为每个高斯基元引入一组基函数，以采样率为输入建模外观变化，实现采样率敏感的滤波，并通过端到端优化基函数参数。

Result: 在公开数据集和新收集的数据集上，LOD-GS实现了SOTA渲染质量，有效消除了锯齿。

Conclusion: LOD-GS通过动态滤波解决了3DGS中的锯齿问题，同时开源了代码和数据集。

Abstract: Despite the advancements in quality and efficiency achieved by 3D Gaussian
Splatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent
challenge. Existing approaches primarily rely on low-pass filtering to mitigate
aliasing. However, these methods are not sensitive to the sampling rate, often
resulting in under-filtering and over-smoothing renderings. To address this
limitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework
for Gaussian Splatting, which dynamically predicts the optimal filtering
strength for each 3D Gaussian primitive. Specifically, we introduce a set of
basis functions to each Gaussian, which take the sampling rate as input to
model appearance variations, enabling sampling-rate-sensitive filtering. These
basis function parameters are jointly optimized with the 3D Gaussian in an
end-to-end manner. The sampling rate is influenced by both focal length and
camera distance. However, existing methods and datasets rely solely on
down-sampling to simulate focal length changes for anti-aliasing evaluation,
overlooking the impact of camera distance. To enable a more comprehensive
assessment, we introduce a new synthetic dataset featuring objects rendered at
varying camera distances. Extensive experiments on both public datasets and our
newly collected dataset demonstrate that our method achieves SOTA rendering
quality while effectively eliminating aliasing. The code and dataset have been
open-sourced.

</details>


### [55] [Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment](https://arxiv.org/abs/2507.00566)
*Kai Zhou,Shuhai Zhang,Zeng You,Jinwu Hu,Mingkui Tan,Fei Liu*

Main category: cs.CV

TL;DR: PGFA提出了一种原型引导的特征对齐范式，用于零样本骨架动作识别，通过端到端跨模态对比训练和改进的文本特征对齐策略，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在骨架特征区分度和骨架-文本对齐偏差方面存在不足，导致零样本动作识别效果不佳。

Method: 采用端到端跨模态对比训练框架提升骨架-文本对齐，并引入原型引导的文本特征对齐策略以减少分布差异的影响。

Result: 在NTU-60、NTU-120和PKU-MMD数据集上，PGFA分别比SMIE方法提升了22.96%、12.53%和18.54%的准确率。

Conclusion: PGFA通过改进特征对齐和减少偏差，显著提升了零样本骨架动作识别的性能。

Abstract: Zero-shot skeleton-based action recognition aims to classify unseen
skeleton-based human actions without prior exposure to such categories during
training. This task is extremely challenging due to the difficulty in
generalizing from known to unknown actions. Previous studies typically use
two-stage training: pre-training skeleton encoders on seen action categories
using cross-entropy loss and then aligning pre-extracted skeleton and text
features, enabling knowledge transfer to unseen classes through skeleton-text
alignment and language models' generalization. However, their efficacy is
hindered by 1) insufficient discrimination for skeleton features, as the fixed
skeleton encoder fails to capture necessary alignment information for effective
skeleton-text alignment; 2) the neglect of alignment bias between skeleton and
unseen text features during testing. To this end, we propose a prototype-guided
feature alignment paradigm for zero-shot skeleton-based action recognition,
termed PGFA. Specifically, we develop an end-to-end cross-modal contrastive
training framework to improve skeleton-text alignment, ensuring sufficient
discrimination for skeleton features. Additionally, we introduce a
prototype-guided text feature alignment strategy to mitigate the adverse impact
of the distribution discrepancy during testing. We provide a theoretical
analysis to support our prototype-guided text feature alignment strategy and
empirically evaluate our overall PGFA on three well-known datasets. Compared
with the top competitor SMIE method, our PGFA achieves absolute accuracy
improvements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD
datasets, respectively.

</details>


### [56] [Out-of-distribution detection in 3D applications: a review](https://arxiv.org/abs/2507.00570)
*Zizhao Li,Xueyang Kang,Joseph West,Kourosh Khoshelham*

Main category: cs.CV

TL;DR: 本文综述了OOD检测在可信赖和不确定性AI中的重要性，涵盖了关键用例、基准数据集、评估指标、方法论比较以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 在3D应用（如自动驾驶）中，检测训练集中未见的对象至关重要，但现有方法通常假设所有对象类别在训练集中已知，限制了泛化能力。

Method: 通过比较OOD检测方法，探讨模型结构、不确定性指标、分布距离分类和不确定性校准技术。

Result: 提供了理论和实践见解，展示了3D视觉集成等新兴研究机会。

Conclusion: 本文帮助新研究者更有效地进入该领域，推动可靠、安全和鲁棒的AI系统发展。

Abstract: The ability to detect objects that are not prevalent in the training set is a
critical capability in many 3D applications, including autonomous driving.
Machine learning methods for object recognition often assume that all object
categories encountered during inference belong to a closed set of classes
present in the training data. This assumption limits generalization to the real
world, as objects not seen during training may be misclassified or entirely
ignored. As part of reliable AI, OOD detection identifies inputs that deviate
significantly from the training distribution. This paper provides a
comprehensive overview of OOD detection within the broader scope of trustworthy
and uncertain AI. We begin with key use cases across diverse domains, introduce
benchmark datasets spanning multiple modalities, and discuss evaluation
metrics. Next, we present a comparative analysis of OOD detection
methodologies, exploring model structures, uncertainty indicators, and
distributional distance taxonomies, alongside uncertainty calibration
techniques. Finally, we highlight promising research directions, including
adversarially robust OOD detection and failure identification, particularly
relevant to 3D applications. The paper offers both theoretical and practical
insights into OOD detection, showcasing emerging research opportunities such as
3D vision integration. These insights help new researchers navigate the field
more effectively, contributing to the development of reliable, safe, and robust
AI systems.

</details>


### [57] [AI-Generated Video Detection via Perceptual Straightening](https://arxiv.org/abs/2507.00583)
*Christian Internò,Robert Geirhos,Markus Olhofer,Sunny Liu,Barbara Hammer,David Klindt*

Main category: cs.CV

TL;DR: ReStraV利用神经表示几何学检测AI生成视频，通过分析时间曲率和步长距离差异，实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的快速发展导致合成视频真实性高，现有检测方法难以捕捉时间不一致性，亟需新方法。

Method: 基于“感知直线化”假说，利用DINOv2预训练模型量化视频表示域的时间曲率和步长距离，训练分类器。

Result: 在VidProM基准上达到97.17%准确率和98.63% AUROC，显著优于现有方法。

Conclusion: ReStraV为AI生成视频检测提供了高效、低成本的新思路，并揭示了神经表示几何学的应用潜力。

Abstract: The rapid advancement of generative AI enables highly realistic synthetic
videos, posing significant challenges for content authentication and raising
urgent concerns about misuse. Existing detection methods often struggle with
generalization and capturing subtle temporal inconsistencies. We propose
ReStraV(Representation Straightening Video), a novel approach to distinguish
natural from AI-generated videos. Inspired by the "perceptual straightening"
hypothesis -- which suggests real-world video trajectories become more straight
in neural representation domain -- we analyze deviations from this expected
geometric property. Using a pre-trained self-supervised vision transformer
(DINOv2), we quantify the temporal curvature and stepwise distance in the
model's representation domain. We aggregate statistics of these measures for
each video and train a classifier. Our analysis shows that AI-generated videos
exhibit significantly different curvature and distance patterns compared to
real videos. A lightweight classifier achieves state-of-the-art detection
performance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),
substantially outperforming existing image- and video-based methods. ReStraV is
computationally efficient, it is offering a low-cost and effective detection
solution. This work provides new insights into using neural representation
geometry for AI-generated video detection.

</details>


### [58] [Similarity Memory Prior is All You Need for Medical Image Segmentation](https://arxiv.org/abs/2507.00585)
*Tang Hao,Guo ZhiQing,Wang LieJun,Liu Chao*

Main category: cs.CV

TL;DR: Sim-MPNet利用动态记忆权重损失注意力和双相似性全局内部增强模块，提升了医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 受猕猴初级视觉皮层中“祖母细胞”启发，探索其在医学图像分割中的应用价值。

Method: 设计了Sim-MPNet，包括动态记忆权重损失注意力（DMW-LA）和双相似性全局内部增强模块（DS-GIM）。

Result: 在四个公开数据集上表现优于现有方法。

Conclusion: Sim-MPNet通过相似性记忆先验和动态更新策略，有效提升了医学图像分割的准确性。

Abstract: In recent years, it has been found that "grandmother cells" in the primary
visual cortex (V1) of macaques can directly recognize visual input with complex
shapes. This inspires us to examine the value of these cells in promoting the
research of medical image segmentation. In this paper, we design a Similarity
Memory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,
we propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and
remembers the category features of specific lesions or organs in medical images
through the similarity memory prior in the prototype memory bank, thus helping
the network to learn subtle texture changes between categories. DMW-LA also
dynamically updates the similarity memory prior in reverse through Weight-Loss
Dynamic (W-LD) update strategy, effectively assisting the network directly
extract category features. In addition, we propose the Double-Similarity Global
Internal Enhancement Module (DS-GIM) to deeply explore the internal differences
in the feature distribution of input data through cosine similarity and
euclidean distance. Extensive experiments on four public datasets show that
Sim-MPNet has better segmentation performance than other state-of-the-art
methods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.

</details>


### [59] [Context-Aware Academic Emotion Dataset and Benchmark](https://arxiv.org/abs/2507.00586)
*Luming Zhao,Jingwen Xuan,Jiamin Lou,Yonghui Yu,Wenwu Yang*

Main category: cs.CV

TL;DR: 本文提出了一个名为RAER的新数据集和CLIP-CAER方法，用于在自然学习环境中识别学术情感，通过结合面部表情和上下文线索显著提升了识别效果。


<details>
  <summary>Details</summary>
Motivation: 学术情感分析对评估学生学习过程中的参与度和认知状态至关重要，但目前缺乏公开的数据集和针对学术情感的识别方法。

Method: 提出了CLIP-CAER方法，利用CLIP模型的可学习文本提示，结合面部表情和上下文线索进行学术情感识别。

Result: 实验表明，CLIP-CAER在学术情感识别任务上显著优于现有方法，突出了上下文线索的重要性。

Conclusion: RAER数据集和CLIP-CAER方法填补了学术情感识别领域的空白，为未来研究提供了重要资源。

Abstract: Academic emotion analysis plays a crucial role in evaluating students'
engagement and cognitive states during the learning process. This paper
addresses the challenge of automatically recognizing academic emotions through
facial expressions in real-world learning environments. While significant
progress has been made in facial expression recognition for basic emotions,
academic emotion recognition remains underexplored, largely due to the scarcity
of publicly available datasets. To bridge this gap, we introduce RAER, a novel
dataset comprising approximately 2,700 video clips collected from around 140
students in diverse, natural learning contexts such as classrooms, libraries,
laboratories, and dormitories, covering both classroom sessions and individual
study. Each clip was annotated independently by approximately ten annotators
using two distinct sets of academic emotion labels with varying granularity,
enhancing annotation consistency and reliability. To our knowledge, RAER is the
first dataset capturing diverse natural learning scenarios. Observing that
annotators naturally consider context cues-such as whether a student is looking
at a phone or reading a book-alongside facial expressions, we propose CLIP-CAER
(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes
learnable text prompts within the vision-language model CLIP to effectively
integrate facial expression and context cues from videos. Experimental results
demonstrate that CLIP-CAER substantially outperforms state-of-the-art
video-based facial expression recognition methods, which are primarily designed
for basic emotions, emphasizing the crucial role of context in accurately
recognizing academic emotions. Project page: https://zgsfer.github.io/CAER

</details>


### [60] [Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods](https://arxiv.org/abs/2507.00593)
*Fernando Alonso-Fernandez,Talha Hanif Butt,Prayag Tiwari*

Main category: cs.CV

TL;DR: 研究通过分析卡车CAN总线数据，评估了三种分类器（ANN、RF、SVM）在超车检测中的性能，发现多车数据训练和分数级融合策略能提升准确性。


<details>
  <summary>Details</summary>
Motivation: 卡车安全超车对防止事故和交通效率至关重要，需通过ADAS系统准确预测超车行为。

Method: 使用五辆沃尔沃卡车的CAN总线数据，评估ANN、RF和SVM三种分类器在不同预处理配置下的性能。

Result: 多车数据训练提升泛化能力，分数级融合策略在多数情况下表现最佳，最终准确率为TNR=93%，TPR=86.5%。

Conclusion: 研究证实多车数据训练和分数级融合策略能有效提升超车检测的准确性，为ADAS系统提供了实用方法。

Abstract: Safe overtaking manoeuvres in trucks are vital for preventing accidents and
ensuring efficient traffic flow. Accurate prediction of such manoeuvres is
essential for Advanced Driver Assistance Systems (ADAS) to make timely and
informed decisions. In this study, we focus on overtake detection using
Controller Area Network (CAN) bus data collected from five in-service trucks
provided by the Volvo Group. We evaluate three common classifiers for vehicle
manoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and
Support Vector Machines (SVM), and analyse how different preprocessing
configurations affect performance. We find that variability in traffic
conditions strongly influences the signal patterns, particularly in the
no-overtake class, affecting classification performance if training data lacks
adequate diversity. Since the data were collected under unconstrained,
real-world conditions, class diversity cannot be guaranteed a priori. However,
training with data from multiple vehicles improves generalisation and reduces
condition-specific bias. Our pertruck analysis also reveals that classification
accuracy, especially for overtakes, depends on the amount of training data per
vehicle. To address this, we apply a score-level fusion strategy, which yields
the best per-truck performance across most cases. Overall, we achieve an
accuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True
Positive Rate). This research has been part of the BIG FUN project, which
explores how Artificial Intelligence can be applied to logged vehicle data to
understand and predict driver behaviour, particularly in relation to Camera
Monitor Systems (CMS), being introduced as digital replacements for traditional
exterior mirrors.

</details>


### [61] [World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model](https://arxiv.org/abs/2507.00603)
*Yupeng Zheng,Pengxuan Yang,Zebin Xing,Qichao Zhang,Yuhang Zheng,Yinfeng Gao,Pengfei Li,Teng Zhang,Zhongpu Xia,Peng Jia,Dongbin Zhao*

Main category: cs.CV

TL;DR: World4Drive是一个端到端自动驾驶框架，通过视觉基础模型构建潜在世界模型，实现无感知标注的规划轨迹生成与评估。


<details>
  <summary>Details</summary>
Motivation: 解决依赖昂贵感知监督的问题，构建自监督学习驱动的世界模型以实现无标注的端到端规划。

Method: 利用视觉基础模型提取场景特征，生成多模态规划轨迹，并通过世界模型选择器评估最佳轨迹。

Result: 在nuScenes和NavSim基准测试中表现优异，L2误差降低18.1%，碰撞率减少46.7%，训练收敛速度提升3.75倍。

Conclusion: World4Drive展示了无感知标注的端到端自动驾驶规划的可行性，性能显著提升。

Abstract: End-to-end autonomous driving directly generates planning trajectories from
raw sensor data, yet it typically relies on costly perception supervision to
extract scene information. A critical research challenge arises: constructing
an informative driving world model to enable perception annotation-free,
end-to-end planning via self-supervised learning. In this paper, we present
World4Drive, an end-to-end autonomous driving framework that employs vision
foundation models to build latent world models for generating and evaluating
multi-modal planning trajectories. Specifically, World4Drive first extracts
scene features, including driving intention and world latent representations
enriched with spatial-semantic priors provided by vision foundation models. It
then generates multi-modal planning trajectories based on current scene
features and driving intentions and predicts multiple intention-driven future
states within the latent space. Finally, it introduces a world model selector
module to evaluate and select the best trajectory. We achieve perception
annotation-free, end-to-end planning through self-supervised alignment between
actual future observations and predicted observations reconstructed from the
latent space. World4Drive achieves state-of-the-art performance without manual
perception annotations on both the open-loop nuScenes and closed-loop NavSim
benchmarks, demonstrating an 18.1\% relative reduction in L2 error, 46.7% lower
collision rate, and 3.75 faster training convergence. Codes will be accessed at
https://github.com/ucaszyp/World4Drive.

</details>


### [62] [De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection](https://arxiv.org/abs/2507.00608)
*Zehua Fu,Chenguang Liu,Yuyu Chen,Jiaqi Zhou,Qingjie Liu,Yunhong Wang*

Main category: cs.CV

TL;DR: DeSimPL通过减少简单样本比例提升自标记检测器性能。


<details>
  <summary>Details</summary>
Motivation: 解决自标记检测器因简单样本比例高（简单标签偏差）导致性能不足的问题。

Method: 提出DeSimPL方法，利用实例级记忆库更新伪标签，引入对抗样本增加困难样本比例，并采用自适应加权损失避免假阳性标签影响。

Result: 实验表明DeSimPL显著提升自标记检测器性能，四个基准测试验证了其有效性。

Conclusion: DeSimPL通过减少简单样本比例，成功弥补了自标记检测器与域对齐方法的性能差距。

Abstract: Despite its significant success, object detection in traffic and
transportation scenarios requires time-consuming and laborious efforts in
acquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation
(UDA) for object detection has recently gained increasing research attention.
UDA for object detection has been dominated by domain alignment methods, which
achieve top performance. Recently, self-labeling methods have gained popularity
due to their simplicity and efficiency. In this paper, we investigate the
limitations that prevent self-labeling detectors from achieving commensurate
performance with domain alignment methods. Specifically, we identify the high
proportion of simple samples during training, i.e., the simple-label bias, as
the central cause. We propose a novel approach called De-Simplifying Pseudo
Labels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level
memory bank to implement an innovative pseudo label updating strategy. Then,
adversarial samples are introduced during training to enhance the proportion.
Furthermore, we propose an adaptive weighted loss to avoid the model suffering
from an abundance of false positive pseudo labels in the late training period.
Experimental results demonstrate that DeSimPL effectively reduces the
proportion of simple samples during training, leading to a significant
performance improvement for self-labeling detectors. Extensive experiments
conducted on four benchmarks validate our analysis and conclusions.

</details>


### [63] [UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions](https://arxiv.org/abs/2507.00648)
*Siyuan Yao,Rui Zhu,Ziqi Wang,Wenqi Ren,Yanyang Yan,Xiaochun Cao*

Main category: cs.CV

TL;DR: UMDATrack是一种能够在恶劣天气条件下保持高质量目标状态预测的视觉跟踪方法，通过统一的域适应框架实现。


<details>
  <summary>Details</summary>
Motivation: 现有方法在白天数据上表现良好，但在恶劣天气条件下性能显著下降，因此需要一种适应多种天气条件的跟踪方法。

Method: 使用可控场景生成器合成少量未标记视频，设计域定制适配器（DCA）和目标感知置信对齐模块（TCA）。

Result: UMDATrack在实验中显著超越现有先进视觉跟踪器，达到新的最先进性能。

Conclusion: UMDATrack通过域适应框架和模块设计，有效解决了恶劣天气条件下的视觉跟踪问题。

Abstract: Visual object tracking has gained promising progress in past decades. Most of
the existing approaches focus on learning target representation in
well-conditioned daytime data, while for the unconstrained real-world scenarios
with adverse weather conditions, e.g. nighttime or foggy environment, the
tremendous domain shift leads to significant performance degradation. In this
paper, we propose UMDATrack, which is capable of maintaining high-quality
target state prediction under various adverse weather conditions within a
unified domain adaptation framework. Specifically, we first use a controllable
scenario generator to synthesize a small amount of unlabeled videos (less than
2% frames in source daytime datasets) in multiple weather conditions under the
guidance of different text prompts. Afterwards, we design a simple yet
effective domain-customized adapter (DCA), allowing the target objects'
representation to rapidly adapt to various weather conditions without redundant
model updating. Furthermore, to enhance the localization consistency between
source and target domains, we propose a target-aware confidence alignment
module (TCA) following optimal transport theorem. Extensive experiments
demonstrate that UMDATrack can surpass existing advanced visual trackers and
lead new state-of-the-art performance by a significant margin. Our code is
available at https://github.com/Z-Z188/UMDATrack.

</details>


### [64] [LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment](https://arxiv.org/abs/2507.00659)
*Juelin Zhu,Shuaibang Peng,Long Wang,Hanlin Tan,Yu Liu,Maojun Zhang,Shen Yan*

Main category: cs.CV

TL;DR: LoD-Loc v2 是一种新颖的空中视觉定位方法，通过粗到细的策略和显式轮廓对齐，实现了在低细节层次（LoD1）城市模型上的精确定位。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖高细节层次（LoD3或LoD2）城市模型，但全球多数可用或计划构建的模型为低细节层次（LoD1）。因此，开发适用于低LoD模型的定位方法对无人机全球城市定位至关重要。

Method: LoD-Loc v2 采用粗到细策略：1) 通过建筑分割网络提取轮廓；2) 构建姿态成本体积以选择粗姿态；3) 使用粒子滤波和多光束跟踪进行细姿态估计。

Result: 实验表明，LoD-Loc v2 在高LoD模型上提高了精度，首次实现了低LoD模型的定位，且性能大幅优于现有方法。

Conclusion: LoD-Loc v2 不仅扩展了定位模型的适用范围，还提升了精度和鲁棒性，为无人机全球城市定位提供了新可能。

Abstract: We propose a novel method for aerial visual localization over low
Level-of-Detail (LoD) city models. Previous wireframe-alignment-based method
LoD-Loc has shown promising localization results leveraging LoD models.
However, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the
majority of available models and those many countries plan to construct
nationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD
city models could unlock drones' potential for global urban localization. To
address these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine
strategy using explicit silhouette alignment to achieve accurate localization
over low-LoD city models in the air. Specifically, given a query image, LoD-Loc
v2 first applies a building segmentation network to shape building silhouettes.
Then, in the coarse pose selection stage, we construct a pose cost volume by
uniformly sampling pose hypotheses around a prior pose to represent the pose
probability distribution. Each cost of the volume measures the degree of
alignment between the projected and predicted silhouettes. We select the pose
with maximum value as the coarse pose. In the fine pose estimation stage, a
particle filtering method incorporating a multi-beam tracking approach is used
to efficiently explore the hypothesis space and obtain the final pose
estimation. To further facilitate research in this field, we release two
datasets with LoD1 city models covering 10.7 km , along with real RGB queries
and ground-truth pose annotations. Experimental results show that LoD-Loc v2
improves estimation accuracy with high-LoD models and enables localization with
low-LoD models for the first time. Moreover, it outperforms state-of-the-art
baselines by large margins, even surpassing texture-model-based methods, and
broadens the convergence basin to accommodate larger prior errors.

</details>


### [65] [A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation](https://arxiv.org/abs/2507.00676)
*Edward Effendy,Kuan-Wei Tseng,Rei Kawakami*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的全身抓取框架，结合姿态生成和运动填充，实现真实稳定的物体交互。


<details>
  <summary>Details</summary>
Motivation: 解决全身抓取任务中姿态生成和运动连续性的问题，并克服手-物体交互数据稀缺的挑战。

Method: 采用三阶段流程：抓取姿态生成、时间填充和LiftUp Transformer，并通过广义预训练提高数据效率。

Result: 在GRAB数据集上实验表明，方法在连贯性、稳定性和视觉真实感上优于现有基线。

Conclusion: 模块化设计易于适应其他人体运动应用，展现了良好的泛化能力。

Abstract: Accepted in the ICIP 2025
  We present a novel transformer-based framework for whole-body grasping that
addresses both pose generation and motion infilling, enabling realistic and
stable object interactions. Our pipeline comprises three stages: Grasp Pose
Generation for full-body grasp generation, Temporal Infilling for smooth motion
continuity, and a LiftUp Transformer that refines downsampled joints back to
high-resolution markers. To overcome the scarcity of hand-object interaction
data, we introduce a data-efficient Generalized Pretraining stage on large,
diverse motion datasets, yielding robust spatio-temporal representations
transferable to grasping tasks. Experiments on the GRAB dataset show that our
method outperforms state-of-the-art baselines in terms of coherence, stability,
and visual realism. The modular design also supports easy adaptation to other
human-motion applications.

</details>


### [66] [Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack](https://arxiv.org/abs/2507.00690)
*Keke Tang,Ziyong Du,Weilong Peng,Xiaofei Wang,Peican Zhu,Ligang Liu,Zhihong Tian*

Main category: cs.CV

TL;DR: CageAttack提出了一种基于笼形变形的对抗攻击框架，通过结构化变形生成自然的对抗点云，平衡了可转移性、不可防御性和合理性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击方法在点云上施加严格的几何约束，限制了可转移性和不可防御性；而现有非结构化变形方法可能导致不自然的扭曲。

Method: 构建目标对象的笼形结构，通过扰动笼顶点实现平滑、自然的变形，确保变形后的点云保持合理性。

Result: 在三个数据集上的七个3D深度神经网络分类器上，CageAttack在可转移性、不可防御性和合理性方面优于现有方法。

Conclusion: CageAttack通过结构化变形实现了更自然的对抗点云生成，为对抗攻击提供了新思路。

Abstract: Adversarial attacks on point clouds often impose strict geometric constraints
to preserve plausibility; however, such constraints inherently limit
transferability and undefendability. While deformation offers an alternative,
existing unstructured approaches may introduce unnatural distortions, making
adversarial point clouds conspicuous and undermining their plausibility. In
this paper, we propose CageAttack, a cage-based deformation framework that
produces natural adversarial point clouds. It first constructs a cage around
the target object, providing a structured basis for smooth, natural-looking
deformation. Perturbations are then applied to the cage vertices, which
seamlessly propagate to the point cloud, ensuring that the resulting
deformations remain intrinsic to the object and preserve plausibility.
Extensive experiments on seven 3D deep neural network classifiers across three
datasets show that CageAttack achieves a superior balance among
transferability, undefendability, and plausibility, outperforming
state-of-the-art methods. Codes will be made public upon acceptance.

</details>


### [67] [Rectifying Magnitude Neglect in Linear Attention](https://arxiv.org/abs/2507.00698)
*Qihang Fan,Huaibo Huang,Yuang Ai,ran He*

Main category: cs.CV

TL;DR: 论文提出了一种改进的线性注意力机制MALA，通过引入查询向量的幅度信息，使其在保持线性复杂度的同时，性能接近标准Softmax注意力。


<details>
  <summary>Details</summary>
Motivation: 标准Softmax注意力具有全局建模能力，但计算复杂度高；线性注意力复杂度低但性能较差。研究发现线性注意力忽略了查询向量的幅度信息，导致性能下降。

Method: 提出MALA（Magnitude-Aware Linear Attention），在线性注意力中引入查询向量的幅度信息，使其分布更接近Softmax注意力。

Result: MALA在图像分类、目标检测、实例分割、语义分割、自然语言处理、语音识别和图像生成等任务中表现优异。

Conclusion: MALA通过引入查询向量的幅度信息，显著提升了线性注意力的性能，同时保持了线性复杂度，适用于多种任务。

Abstract: As the core operator of Transformers, Softmax Attention exhibits excellent
global modeling capabilities. However, its quadratic complexity limits its
applicability to vision tasks. In contrast, Linear Attention shares a similar
formulation with Softmax Attention while achieving linear complexity, enabling
efficient global information modeling. Nevertheless, Linear Attention suffers
from a significant performance degradation compared to standard Softmax
Attention. In this paper, we analyze the underlying causes of this issue based
on the formulation of Linear Attention. We find that, unlike Softmax Attention,
Linear Attention entirely disregards the magnitude information of the Query.
This prevents the attention score distribution from dynamically adapting as the
Query scales. As a result, despite its structural similarity to Softmax
Attention, Linear Attention exhibits a significantly different attention score
distribution. Based on this observation, we propose Magnitude-Aware Linear
Attention (MALA), which modifies the computation of Linear Attention to fully
incorporate the Query's magnitude. This adjustment allows MALA to generate an
attention score distribution that closely resembles Softmax Attention while
exhibiting a more well-balanced structure. We evaluate the effectiveness of
MALA on multiple tasks, including image classification, object detection,
instance segmentation, semantic segmentation, natural language processing,
speech recognition, and image generation. Our MALA achieves strong results on
all of these tasks. Code will be available at https://github.com/qhfan/MALA

</details>


### [68] [BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving](https://arxiv.org/abs/2507.00707)
*Zeming Chen,Hang Zhao*

Main category: cs.CV

TL;DR: BEV-VAE提出了一种基于BEV（鸟瞰图）潜在空间的多视角图像生成方法，通过变分自编码器和潜在扩散变换器实现一致且可控的视图合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法多将多视角图像生成视为2D图像集生成任务，缺乏显式的3D建模，而结构化表示对自动驾驶场景生成至关重要。

Method: BEV-VAE首先训练多视角图像变分自编码器以构建紧凑统一的BEV潜在空间，然后通过潜在扩散变换器生成场景。

Result: 在nuScenes和Argoverse 2数据集上，BEV-VAE在3D一致重建和生成方面表现优异。

Conclusion: BEV-VAE通过结构化表示实现了多视角图像的一致生成，支持任意视角生成和可选3D布局输入。

Abstract: Multi-view image generation in autonomous driving demands consistent 3D scene
understanding across camera views. Most existing methods treat this problem as
a 2D image set generation task, lacking explicit 3D modeling. However, we argue
that a structured representation is crucial for scene generation, especially
for autonomous driving applications. This paper proposes BEV-VAE for consistent
and controllable view synthesis. BEV-VAE first trains a multi-view image
variational autoencoder for a compact and unified BEV latent space and then
generates the scene with a latent diffusion transformer. BEV-VAE supports
arbitrary view generation given camera configurations, and optionally 3D
layouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance
in both 3D consistent reconstruction and generation. The code is available at:
https://github.com/Czm369/bev-vae.

</details>


### [69] [TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving](https://arxiv.org/abs/2507.00709)
*Yiming Yang,Yueru Luo,Bingkun He,Hongbin Lin,Suzhong Fu,Chao Yan,Kun Tang,Xinrui Yan,Chao Zheng,Shuguang Cui,Zhen Li*

Main category: cs.CV

TL;DR: TopoStreamer提出了一种端到端的时序感知模型，用于车道段拓扑推理，通过流式属性约束、动态车道边界位置编码和车道段去噪，显著提升了车道段感知和中心线感知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在一致位置嵌入和时序多属性学习方面的局限性阻碍了准确的路网重建，影响了自动驾驶系统的道路依赖操作（如转弯和变道）。

Method: TopoStreamer引入了流式属性约束、动态车道边界位置编码和车道段去噪三项关键改进，以增强时序一致性和位置信息学习。

Result: 在OpenLane-V2数据集上，TopoStreamer在车道段感知任务中提升了3.4% mAP，在中心线感知任务中提升了2.1% OLS。

Conclusion: TopoStreamer通过改进时序一致性和位置信息学习，显著提升了车道段拓扑推理的准确性，为自动驾驶系统提供了更可靠的路网重建能力。

Abstract: Lane segment topology reasoning constructs a comprehensive road network by
capturing the topological relationships between lane segments and their
semantic types. This enables end-to-end autonomous driving systems to perform
road-dependent maneuvers such as turning and lane changing. However, the
limitations in consistent positional embedding and temporal multiple attribute
learning in existing methods hinder accurate roadnet reconstruction. To address
these issues, we propose TopoStreamer, an end-to-end temporal perception model
for lane segment topology reasoning. Specifically, TopoStreamer introduces
three key improvements: streaming attribute constraints, dynamic lane boundary
positional encoding, and lane segment denoising. The streaming attribute
constraints enforce temporal consistency in both centerline and boundary
coordinates, along with their classifications. Meanwhile, dynamic lane boundary
positional encoding enhances the learning of up-to-date positional information
within queries, while lane segment denoising helps capture diverse lane segment
patterns, ultimately improving model performance. Additionally, we assess the
accuracy of existing models using a lane boundary classification metric, which
serves as a crucial measure for lane-changing scenarios in autonomous driving.
On the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements
over state-of-the-art methods, achieving substantial performance gains of +3.4%
mAP in lane segment perception and +2.1% OLS in centerline perception tasks.

</details>


### [70] [UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement](https://arxiv.org/abs/2507.00721)
*Xiao Zhang,Fei Wei,Yong Wang,Wenda Zhao,Feiyi Li,Xiangxiang Chu*

Main category: cs.CV

TL;DR: UPRE框架通过联合优化文本提示和视觉表示，解决了零样本域适应中的任务与视觉语言模型不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注域分布偏移，忽略了检测任务与依赖手动提示的视觉语言模型之间的不对齐问题。

Method: UPRE框架引入多视角域提示和视觉表示增强模块，结合多级增强策略对齐多模态表示。

Result: 在九个基准数据集上的实验表明，UPRE在零样本域适应检测场景中表现优异。

Conclusion: UPRE框架通过优化提示和视觉表示，显著提升了零样本域适应的性能。

Abstract: Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the
lack of images in the target domain. Previous approaches leverage
Vision-Language Models (VLMs) to tackle this challenge, exploiting their
zero-shot learning capabilities. However, these methods primarily address
domain distribution shifts and overlook the misalignment between the detection
task and VLMs, which rely on manually crafted prompts. To overcome these
limitations, we propose the unified prompt and representation enhancement
(UPRE) framework, which jointly optimizes both textual prompts and visual
representations. Specifically, our approach introduces a multi-view domain
prompt that combines linguistic domain priors with detection-specific
knowledge, and a visual representation enhancement module that produces domain
style variations. Furthermore, we introduce multi-level enhancement strategies,
including relative domain distance and positive-negative separation, which
align multi-modal representations at the image level and capture diverse visual
representations at the instance level, respectively. Extensive experiments
conducted on nine benchmark datasets demonstrate the superior performance of
our framework in ZSDA detection scenarios. Code is available at
https://github.com/AMAP-ML/UPRE.

</details>


### [71] [Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features](https://arxiv.org/abs/2507.00724)
*Linghui Zhu,Yiming Li,Haiqin Weng,Yan Liu,Tianwei Zhang,Shu-Tao Xia,Zhi Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对个性化模型的无害所有权验证方法，通过解耦相似特征来防御模型窃取攻击。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法针对从头训练的模型设计，对微调模型存在安全风险或误判问题，需改进。

Method: 分三阶段：创建保留共同特征的影子模型，训练元分类器识别特定特征，通过假设检验验证所有权。

Result: 在基准数据集上验证了方法能有效检测多种模型窃取类型。

Conclusion: 该方法为个性化模型提供了安全且鲁棒的所有权验证方案。

Abstract: Large vision models achieve remarkable performance in various downstream
tasks, primarily by personalizing pre-trained models through fine-tuning with
private and valuable local data, which makes the personalized model a valuable
intellectual property for its owner. Similar to the era of traditional DNNs,
model stealing attacks also pose significant risks to these personalized
models. However, in this paper, we reveal that most existing defense methods
(developed for traditional DNNs), typically designed for models trained from
scratch, either introduce additional security risks, are prone to misjudgment,
or are even ineffective for fine-tuned models. To alleviate these problems,
this paper proposes a harmless model ownership verification method for
personalized models by decoupling similar common features. In general, our
method consists of three main stages. In the first stage, we create shadow
models that retain common features of the victim model while disrupting
dataset-specific features. We represent the dataset-specific features of the
victim model by the output differences between the shadow and victim models.
After that, a meta-classifier is trained to identify stolen models by
determining whether suspicious models contain the dataset-specific features of
the victim. In the third stage, we conduct model ownership verification by
hypothesis test to mitigate randomness and enhance robustness. Extensive
experiments on benchmark datasets verify the effectiveness of the proposed
method in detecting different types of model stealing simultaneously.

</details>


### [72] [Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network](https://arxiv.org/abs/2507.00739)
*An Le,Hung Nguyen,Sungbal Seo,You-Suk Bae,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种新型双正交可调小波单元，通过提升方案放松了正交性和等长滤波器约束，提高了滤波器设计的灵活性，显著提升了CNN中的图像分类和异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统小波变换在正交性和滤波器长度上的限制可能限制了其在CNN中的灵活性和性能。

Method: 采用提升方案构建双正交可调小波单元，放松正交性和等长滤波器约束，优化卷积、池化和下采样操作。

Result: 在ResNet-18中，CIFAR-10分类准确率提升2.12%，DTD提升9.73%；在ResNet-34中也有类似改进。在MVTec异常检测数据集中，分割和检测任务表现优异。

Conclusion: 该方法通过灵活的小波设计显著提升了CNN的性能，尤其在图像分类和异常检测任务中表现突出。

Abstract: This work introduces a novel biorthogonal tunable wavelet unit constructed
using a lifting scheme that relaxes both the orthogonality and equal filter
length constraints, providing greater flexibility in filter design. The
proposed unit enhances convolution, pooling, and downsampling operations,
leading to improved image classification and anomaly detection in convolutional
neural networks (CNN). When integrated into an 18-layer residual neural network
(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%
and on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its
effectiveness in capturing fine-grained details. Similar improvements were
observed in ResNet-34. For anomaly detection in the hazelnut category of the
MVTec Anomaly Detection dataset, the proposed method achieved competitive and
wellbalanced performance in both segmentation and detection tasks,
outperforming existing approaches in terms of accuracy and robustness.

</details>


### [73] [Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning](https://arxiv.org/abs/2507.00748)
*Bob Zhang,Haoran Li,Tao Zhang,Cilin Yan,Jiayin Cai,Xiaolong Jiang,Yanbin Hao*

Main category: cs.CV

TL;DR: 通过强化学习后训练策略提升多模态大语言模型在多图像任务中的推理性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在单图像场景中表现优异，但在多图像组合和多模态指令的实际应用中性能下降，揭示了跨图像推理和泛化的局限性。

Method: 采用强化学习后训练策略，包括合成高质量思维链数据冷启动初始化、低秩适应监督微调，以及基于规则的强化学习优化推理路径。

Result: 在MIG-Bench和多个域外推理基准上分别提升9.04%和4.98%，在BLINK和MMIU子集上分别提升3.1%和2.4%。

Conclusion: 该方法显著提升了多图像任务中的推理性能，并展现出强泛化能力。

Abstract: Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding
in single-image scenarios with textual references. However, their performance
degrades when handling real-world applications involving complex multi-image
compositions and multimodal instructions, which reveals limitations in
cross-image reasoning and generalization. To address these challenges, we adopt
a Reinforcement Learning (RL) based post-training strategy to improve the
reasoning performance of MLLMs in multi-image grounding tasks. Our approach
begins with synthesizing high-quality chain-of-thought (CoT) data for
cold-start initialization, followed by supervised fine-tuning (SFT) using
low-rank adaptation (LoRA). The cold-start training stage enables the model to
identify correct solutions. Subsequently, we perform rejection sampling using
the merged SFT model to curate high-quality RL data and leverage rule-based RL
to guide the model toward optimal reasoning paths. Extensive experimental
results demonstrate the effectiveness of our approach, achieving +9.04\%
improvements on MIG-Bench and +4.98\% improvements on several out-of-domain
reasoning grounding benchmarks over the SFT baseline. Furthermore, our approach
exhibits strong generalization in multi-image perception, with gains of +3.1\%
and +2.4\% over the base model on subsets of the BLINK and MMIU benchmarks,
respectively.

</details>


### [74] [Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation](https://arxiv.org/abs/2507.00752)
*Hao Xing,Kai Zhe Boey,Yuankai Wu,Darius Burschka,Gordon Cheng*

Main category: cs.CV

TL;DR: 提出了一种多模态图卷积网络（MMGCN），通过整合低帧率视觉数据和高帧率运动数据，减少动作分割中的过分割错误。


<details>
  <summary>Details</summary>
Motivation: 在协作机器人场景中，精确的时间动作分割至关重要，但现有方法因噪声问题导致过分割错误。

Method: 采用正弦编码策略增强空间表示，设计时间图融合模块对齐多模态输入，并提出SmoothLabelMix数据增强技术。

Result: 在Bimanual Actions Dataset上表现优异，F1@10达94.5%，F1@25达92.8%。

Conclusion: MMGCN通过多模态融合和数据增强，显著提升了动作分割的准确性和时间一致性。

Abstract: Accurate temporal segmentation of human actions is critical for intelligent
robots in collaborative settings, where a precise understanding of sub-activity
labels and their temporal structure is essential. However, the inherent noise
in both human pose estimation and object detection often leads to
over-segmentation errors, disrupting the coherence of action sequences. To
address this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that
integrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,
30 fps) motion data (skeleton and object detections) to mitigate fragmentation.
Our framework introduces three key contributions. First, a sinusoidal encoding
strategy that maps 3D skeleton coordinates into a continuous sin-cos space to
enhance spatial representation robustness. Second, a temporal graph fusion
module that aligns multi-modal inputs with differing resolutions via
hierarchical feature aggregation, Third, inspired by the smooth transitions
inherent to human actions, we design SmoothLabelMix, a data augmentation
technique that mixes input sequences and labels to generate synthetic training
examples with gradual action transitions, enhancing temporal consistency in
predictions and reducing over-segmentation artifacts.
  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for
human-object interaction understanding, demonstrate that our approach
outperforms state-of-the-art methods, especially in action segmentation
accuracy, achieving F1@10: 94.5% and F1@25: 92.8%.

</details>


### [75] [Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs](https://arxiv.org/abs/2507.00754)
*Selim Kuzucu,Muhammad Ferjad Naeem,Anna Kukleva,Federico Tombari,Bernt Schiele*

Main category: cs.CV

TL;DR: LUViT bridges the modality mismatch between LLMs and ViTs through a synergistic pre-training strategy, improving performance on vision tasks.


<details>
  <summary>Details</summary>
Motivation: Leverage LLMs' semantic knowledge for vision tasks despite the modality mismatch with ViTs.

Method: Co-adapt ViT and LLM blocks using MAE for ViT pre-training and LoRA layers for LLM alignment.

Result: LUViT significantly enhances performance on downstream vision tasks.

Conclusion: LUViT provides an effective way to integrate LLM knowledge into visual understanding.

Abstract: The integration of Large Language Model (LLMs) blocks with Vision
Transformers (ViTs) holds immense promise for vision-only tasks by leveraging
the rich semantic knowledge and reasoning capabilities of LLMs. However, a
fundamental challenge lies in the inherent modality mismatch between
text-centric pretraining of LLMs and vision-centric training of ViTs. Direct
fusion often fails to fully exploit the LLM's potential and suffers from
unstable finetuning. As a result, LLM blocks are kept frozen while only the
vision components are learned. As a remedy to these challenges, we introduce
Language-Unlocked Vision Transformers (LUViT), a novel approach that bridges
this modality mismatch through a synergistic pre-training strategy. LUViT
co-adapts a ViT backbone and an LLM fusion block by (1) employing Masked
Auto-Encoding (MAE) to pre-train the ViT for richer visual representations, and
(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM
block using the MAE objective. This joint optimization guides the ViT to
produce LLM-aligned features and the LLM to effectively interpret visual
information. We demonstrate through extensive experiments that LUViT
significantly improves performance on various downstream vision tasks,
showcasing a more effective and efficient pathway to harness LLM knowledge for
visual understanding.

</details>


### [76] [Towards Open-World Human Action Segmentation Using Graph Convolutional Networks](https://arxiv.org/abs/2507.00756)
*Hao Xing,Kai Zhe Boey,Gordon Cheng*

Main category: cs.CV

TL;DR: 论文提出了一种开放世界动作分割框架，通过增强的金字塔图卷积网络、Mixup训练和时序聚类损失，显著提升了开放场景下的动作分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在封闭世界动作分割中表现良好，但在开放世界场景中难以泛化，需要无需手动标注的模型来处理新动作。

Method: 提出EPGCN网络、Mixup训练和时序聚类损失，用于检测和分割未见动作。

Result: 在两个数据集上，开放集分割和分布外检测性能分别提升了16.9%和34.6%。

Conclusion: 框架在开放世界动作分割中表现优异，为动态多样的人类活动理解提供了有效解决方案。

Abstract: Human-object interaction segmentation is a fundamental task of daily activity
understanding, which plays a crucial role in applications such as assistive
robotics, healthcare, and autonomous systems. Most existing learning-based
methods excel in closed-world action segmentation, they struggle to generalize
to open-world scenarios where novel actions emerge. Collecting exhaustive
action categories for training is impractical due to the dynamic diversity of
human activities, necessitating models that detect and segment
out-of-distribution actions without manual annotation. To address this issue,
we formally define the open-world action segmentation problem and propose a
structured framework for detecting and segmenting unseen actions. Our framework
introduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional
Network (EPGCN) with a novel decoder module for robust spatiotemporal feature
upsampling. 2) Mixup-based training to synthesize out-of-distribution data,
eliminating reliance on manual annotations. 3) A novel Temporal Clustering loss
that groups in-distribution actions while distancing out-of-distribution
samples.
  We evaluate our framework on two challenging human-object interaction
recognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.
Experimental results demonstrate significant improvements over state-of-the-art
action segmentation models across multiple open-set evaluation metrics,
achieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and
out-of-distribution detection performances (AUROC), respectively. Additionally,
we conduct an in-depth ablation study to assess the impact of each proposed
component, identifying the optimal framework configuration for open-world
action segmentation.

</details>


### [77] [OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection](https://arxiv.org/abs/2507.00789)
*Ziji Lu*

Main category: cs.CV

TL;DR: OptiPrune是一个统一框架，通过结合分布感知的初始噪声优化和基于相似性的token剪枝，同时解决文本到图像扩散模型中的语义对齐和计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声优化或token剪枝时存在计算开销大或语义保真度低的问题，OptiPrune旨在同时解决这两个挑战。

Method: 1. 引入分布感知噪声优化模块，通过注意力分数引导初始噪声；2. 设计硬件高效的token剪枝策略，包括相似性选择、随机性增强和最大相似性恢复。

Result: 在Animal-Animal等基准数据集上，OptiPrune实现了最先进的提示-图像一致性，同时显著降低计算成本。

Conclusion: OptiPrune在保持语义对齐质量的同时，显著提升了计算效率，适用于资源受限的硬件部署。

Abstract: Text-to-image diffusion models often struggle to achieve accurate semantic
alignment between generated images and text prompts while maintaining
efficiency for deployment on resource-constrained hardware. Existing approaches
either incur substantial computational overhead through noise optimization or
compromise semantic fidelity by aggressively pruning tokens. In this work, we
propose OptiPrune, a unified framework that combines distribution-aware initial
noise optimization with similarity-based token pruning to address both
challenges simultaneously. Specifically, (1) we introduce a distribution-aware
noise optimization module guided by attention scores to steer the initial
latent noise toward semantically meaningful regions, mitigating issues such as
subject neglect and feature entanglement; (2) we design a hardware-efficient
token pruning strategy that selects representative base tokens via patch-wise
similarity, injects randomness to enhance generalization, and recovers pruned
tokens using maximum similarity copying before attention operations. Our method
preserves the Gaussian prior during noise optimization and enables efficient
inference without sacrificing alignment quality. Experiments on benchmark
datasets, including Animal-Animal, demonstrate that OptiPrune achieves
state-of-the-art prompt-image consistency with significantly reduced
computational cost.

</details>


### [78] [LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling](https://arxiv.org/abs/2507.00790)
*Huaqiu Li,Yong Wang,Tongwen Huang,Hailang Huang,Haoqian Wang,Xiangxiang Chu*

Main category: cs.CV

TL;DR: 提出了一种基于预训练潜在扩散模型的统一图像恢复方法，无需配对数据集，通过循环后验采样实现多任务处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么针对特定任务设计，泛化性差，要么依赖配对数据集，受限于闭集约束。

Method: 利用预训练潜在扩散模型，结合多模态理解模型提供语义先验，通过轻量级模块对齐退化输入，并采用循环细化进行后验采样。

Result: 实验表明，该方法优于现有技术，验证了其有效性和鲁棒性。

Conclusion: 该方法为统一图像恢复提供了一种高效、无需配对数据集的解决方案。

Abstract: Unified image restoration is a significantly challenging task in low-level
vision. Existing methods either make tailored designs for specific tasks,
limiting their generalizability across various types of degradation, or rely on
training with paired datasets, thereby suffering from closed-set constraints.
To address these issues, we propose a novel, dataset-free, and unified approach
through recurrent posterior sampling utilizing a pretrained latent diffusion
model. Our method incorporates the multimodal understanding model to provide
sematic priors for the generative model under a task-blind condition.
Furthermore, it utilizes a lightweight module to align the degraded input with
the generated preference of the diffusion model, and employs recurrent
refinement for posterior sampling. Extensive experiments demonstrate that our
method outperforms state-of-the-art methods, validating its effectiveness and
robustness. Our code and data will be available at
https://github.com/AMAP-ML/LD-RPS.

</details>


### [79] [Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters](https://arxiv.org/abs/2507.00792)
*Hendric Voss,Stefan Kopp*

Main category: cs.CV

TL;DR: 提出了一种基于TensorFlow的实时逆运动学求解器，用于生成逼真的人体运动，解决了误差累积和关节限制等挑战。


<details>
  <summary>Details</summary>
Motivation: 实时生成逼真的人体运动在计算机图形学、虚拟环境、机器人学和生物力学等领域具有重要意义。

Method: 利用TensorFlow的自动微分和即时编译技术，将正向和逆向运动学视为可微分操作，处理复杂的多约束问题。

Result: 在SMPLX人体骨骼模型上验证，相比现有方法（如CCD、FABRIK、IPOPT），实现了更快的收敛速度和更高的成功率。

Conclusion: 该求解器在实时性和准确性上表现优异，适用于复杂的人体运动建模。

Abstract: Generating accurate and realistic virtual human movements in real-time is of
high importance for a variety of applications in computer graphics, interactive
virtual environments, robotics, and biomechanics. This paper introduces a novel
real-time inverse kinematics (IK) solver specifically designed for realistic
human-like movement generation. Leveraging the automatic differentiation and
just-in-time compilation of TensorFlow, the proposed solver efficiently handles
complex articulated human skeletons with high degrees of freedom. By treating
forward and inverse kinematics as differentiable operations, our method
effectively addresses common challenges such as error accumulation and
complicated joint limits in multi-constrained problems, which are critical for
realistic human motion modeling. We demonstrate the solver's effectiveness on
the SMPLX human skeleton model, evaluating its performance against widely used
iterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,
and the nonlinear optimization algorithm IPOPT. Our experiments cover both
simple end-effector tasks and sophisticated, multi-constrained problems with
realistic joint limits. Results indicate that our IK solver achieves real-time
performance, exhibiting rapid convergence, minimal computational overhead per
iteration, and improved success rates compared to existing methods. The project
code is available at https://github.com/hvoss-techfak/TF-JAX-IK

</details>


### [80] [TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency](https://arxiv.org/abs/2507.00802)
*Minye Shao,Xingyu Miao,Haoran Duan,Zeyu Wang,Jingkun Chen,Yawen Huang,Xian Wu,Jingjing Deng,Yang Long,Yefeng Zheng*

Main category: cs.CV

TL;DR: TRACE是一个基于2D多模态条件扩散的框架，用于生成具有时空对齐的3D医学图像，解决了现有方法在解剖保真度、轴向长度和计算成本上的限制。


<details>
  <summary>Details</summary>
Motivation: 当前3D医学图像生成方法存在解剖保真度低、轴向长度受限和计算成本高的问题，限制了其在资源有限地区的应用。

Method: TRACE通过将2D切片建模为视频帧对，结合分割先验和放射学报告实现解剖对齐，并利用光流保持时间一致性。推理时采用重叠帧策略生成灵活长度的序列。

Result: 实验表明，TRACE在计算效率和保持解剖保真度与时空一致性之间取得了良好平衡。

Conclusion: TRACE为3D医学图像生成提供了一种高效且可靠的解决方案，适用于资源有限的临床环境。

Abstract: 3D medical image generation is essential for data augmentation and patient
privacy, calling for reliable and efficient models suited for clinical
practice. However, current methods suffer from limited anatomical fidelity,
restricted axial length, and substantial computational cost, placing them
beyond reach for regions with limited resources and infrastructure. We
introduce TRACE, a framework that generates 3D medical images with
spatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.
TRACE models sequential 2D slices as video frame pairs, combining segmentation
priors and radiology reports for anatomical alignment, incorporating optical
flow to sustain temporal coherence. During inference, an overlapping-frame
strategy links frame pairs into a flexible length sequence, reconstructed into
a spatiotemporally and anatomically aligned 3D volume. Experimental results
demonstrate that TRACE effectively balances computational efficiency with
preserving anatomical fidelity and spatiotemporal consistency. Code is
available at: https://github.com/VinyehShaw/TRACE.

</details>


### [81] [CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs](https://arxiv.org/abs/2507.00817)
*Jiaming Zhang,Rui Hu,Qing Guo,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: CAVALRY-V是一种针对视频多模态大语言模型（V-MLLMs）的对抗攻击框架，通过双目标语义-视觉损失函数和高效的两阶段生成器，显著提升了攻击效果。


<details>
  <summary>Details</summary>
Motivation: 探索V-MLLMs在对抗攻击下的脆弱性，解决跨模态推理、时间依赖性和计算限制等挑战。

Method: 提出双目标语义-视觉损失函数和两阶段生成器框架，结合大规模预训练和微调。

Result: 在视频理解基准测试中，CAVALRY-V平均提升22.8%的攻击效果，并在图像理解任务中表现优异。

Conclusion: CAVALRY-V为多模态系统的对抗研究提供了基础性方法，展示了其广泛应用的潜力。

Abstract: Video Multimodal Large Language Models (V-MLLMs) have shown impressive
capabilities in temporal reasoning and cross-modal understanding, yet their
vulnerability to adversarial attacks remains underexplored due to unique
challenges: complex cross-modal reasoning mechanisms, temporal dependencies,
and computational constraints. We present CAVALRY-V (Cross-modal
Language-Vision Adversarial Yielding for Videos), a novel framework that
directly targets the critical interface between visual perception and language
generation in V-MLLMs. Our approach introduces two key innovations: (1) a
dual-objective semantic-visual loss function that simultaneously disrupts the
model's text generation logits and visual representations to undermine
cross-modal integration, and (2) a computationally efficient two-stage
generator framework that combines large-scale pre-training for cross-model
transferability with specialized fine-tuning for spatiotemporal coherence.
Empirical evaluation on comprehensive video understanding benchmarks
demonstrates that CAVALRY-V significantly outperforms existing attack methods,
achieving 22.8% average improvement over the best baseline attacks on both
commercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,
InternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves
flexibility through implicit temporal coherence modeling rather than explicit
regularization, enabling significant performance improvements even on image
understanding (34.4% average gain). This capability demonstrates CAVALRY-V's
potential as a foundational approach for adversarial research across multimodal
systems.

</details>


### [82] [Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data](https://arxiv.org/abs/2507.00822)
*Yasser El Jarida,Youssef Iraqi,Loubna Mekouar*

Main category: cs.CV

TL;DR: 提出了一种基于CNN的方法，利用Blender生成的合成粒子图像进行PSD测量，评估了三种CNN架构，其中EfficientNet-B0在计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统PSD测量方法耗时且受限于粒子重叠，需要自动化、实时的解决方案。

Method: 使用Blender生成合成粒子图像，训练三种CNN架构（ResNet-50、InceptionV3、EfficientNet-B0）预测PSD参数。

Result: EfficientNet-B0在计算效率上表现最佳，适用于工业实时部署。

Conclusion: 合成数据能有效训练CNN，为工业PSD监测提供自动化潜力。

Abstract: Accurate particle size distribution (PSD) measurement is important in
industries such as mining, pharmaceuticals, and fertilizer manufacturing,
significantly influencing product quality and operational efficiency.
Traditional PSD methods like sieve analysis and laser diffraction are manual,
time-consuming, and limited by particle overlap. Recent developments in
convolutional neural networks (CNNs) enable automated, real-time PSD estimation
directly from particle images. In this work, we present a CNN-based methodology
trained on realistic synthetic particle imagery generated using Blender's
advanced rendering capabilities. Synthetic data sets using this method can
replicate various industrial scenarios by systematically varying particle
shapes, textures, lighting, and spatial arrangements that closely resemble the
actual configurations. We evaluated three CNN-based architectures, ResNet-50,
InceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,
d50, d90). Results demonstrated comparable accuracy across models, with
EfficientNet-B0 achieving the best computational efficiency suitable for
real-time industrial deployment. This approach shows the effectiveness of
realistic synthetic data for robust CNN training, which offers significant
potential for automated industrial PSD monitoring. The code is released at :
https://github.com/YasserElj/Synthetic-Granular-Gen

</details>


### [83] [High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery](https://arxiv.org/abs/2507.00825)
*Hongxing Peng,Lide Chen,Hui Zhu,Yan Chen*

Main category: cs.CV

TL;DR: HEGS-DETR是一种针对无人机图像目标检测的改进Transformer框架，通过高频增强语义网络、高效小目标金字塔等模块，显著提升了小目标和密集目标的检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机图像中的目标检测面临小目标、高密度分布和复杂背景等挑战，现有算法依赖手工设计组件且泛化能力有限，需要针对性解决方案。

Method: 提出HFESNet作为新骨干网络保留高频细节，ESOP策略融合高分辨率特征图，SQR和GAPE模块增强解码器稳定性和定位精度。

Result: 在VisDrone数据集上，HEGS-DETR比基线AP$_{50}$提升5.1%，AP提升3.8%，同时保持实时速度并减少4M参数。

Conclusion: HEGS-DETR有效解决了无人机图像目标检测的特定挑战，显著提升了性能并降低了计算成本。

Abstract: Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial
challenges, including small target sizes, high-density distributions, and
cluttered backgrounds in UAV imagery. Current algorithms often depend on
hand-crafted components like anchor boxes, which demand fine-tuning and exhibit
limited generalization, and Non-Maximum Suppression (NMS), which is
threshold-sensitive and prone to misclassifying dense objects. These generic
architectures thus struggle to adapt to aerial imaging characteristics,
resulting in performance limitations. Moreover, emerging end-to-end frameworks
have yet to effectively mitigate these aerial-specific challenges.To address
these issues, we propose HEGS-DETR, a comprehensively enhanced, real-time
Detection Transformer framework tailored for UAVs. First, we introduce the
High-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.
HFESNet preserves critical high-frequency spatial details to extract robust
semantic features, thereby improving discriminative capability for small and
occluded targets in complex backgrounds. Second, our Efficient Small Object
Pyramid (ESOP) strategy strategically fuses high-resolution feature maps with
minimal computational overhead, significantly boosting small object detection.
Finally, the proposed Selective Query Recollection (SQR) and Geometry-Aware
Positional Encoding (GAPE) modules enhance the detector's decoder stability and
localization accuracy, effectively optimizing bounding boxes and providing
explicit spatial priors for dense scenes. Experiments on the VisDrone dataset
demonstrate that HEGS-DETR achieves a 5.1\% AP$_{50}$ and 3.8\% AP increase
over the baseline, while maintaining real-time speed and reducing parameter
count by 4M.

</details>


### [84] [Do Echo Top Heights Improve Deep Learning Nowcasts?](https://arxiv.org/abs/2507.00845)
*Peter Pavlík,Marc Schleiss,Anna Bou Ezzeddine,Viera Rozinajová*

Main category: cs.CV

TL;DR: 论文探讨了在降水临近预报中使用Echo Top Height（ETH）作为辅助输入变量，以提升深度学习模型的预报能力。


<details>
  <summary>Details</summary>
Motivation: 降水临近预报对天气敏感行业至关重要，但现有方法多依赖2D雷达反射率数据，忽略了3D雷达体积中的垂直信息。

Method: 研究采用3D U-Net模型，将雷达反射率和ETH作为独立输入通道，分析ETH对降雨强度预测的影响。

Result: ETH在低雨强阈值下能提升预报能力，但在高雨强下效果不一致，且模型会系统性低估降水强度。

Conclusion: ETH在某些情况下有助于预报，但也可能增加误差，研究为评估额外变量对预报性能的潜在贡献奠定了基础。

Abstract: Precipitation nowcasting -- the short-term prediction of rainfall using
recent radar observations -- is critical for weather-sensitive sectors such as
transportation, agriculture, and disaster mitigation. While recent deep
learning models have shown promise in improving nowcasting skill, most
approaches rely solely on 2D radar reflectivity fields, discarding valuable
vertical information available in the full 3D radar volume. In this work, we
explore the use of Echo Top Height (ETH), a 2D projection indicating the
maximum altitude of radar reflectivity above a given threshold, as an auxiliary
input variable for deep learning-based nowcasting. We examine the relationship
between ETH and radar reflectivity, confirming its relevance for predicting
rainfall intensity. We implement a single-pass 3D U-Net that processes both the
radar reflectivity and ETH as separate input channels. While our models are
able to leverage ETH to improve skill at low rain-rate thresholds, results are
inconsistent at higher intensities and the models with ETH systematically
underestimate precipitation intensity. Three case studies are used to
illustrate how ETH can help in some cases, but also confuse the models and
increase the error variance. Nonetheless, the study serves as a foundation for
critically assessing the potential contribution of additional variables to
nowcasting performance.

</details>


### [85] [UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection](https://arxiv.org/abs/2507.00849)
*Wei Li,Jiaman Tang,Yang Li,Beihao Xia,Ligang Tan,Hongmao Qin*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，通过可变形令牌和跨模态特征融合提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 无人机目标检测面临遮挡、小目标和形状不规则等挑战，需要鲁棒且高效的多模态方法。

Method: 设计了可变形令牌Mamba块（DTMB）和融合Mamba块，结合RGB和红外模态特征，并引入跨空间和跨通道注意力机制。

Result: 在DroneVehicle数据集上，mAP指标比基线OAFA方法提高了3.6%。

Conclusion: UAVD-Mamba通过多模态特征融合和多尺度处理，显著提升了无人机目标检测性能。

Abstract: Unmanned Aerial Vehicle (UAV) object detection has been widely used in
traffic management, agriculture, emergency rescue, etc. However, it faces
significant challenges, including occlusions, small object sizes, and irregular
shapes. These challenges highlight the necessity for a robust and efficient
multimodal UAV object detection method. Mamba has demonstrated considerable
potential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a
multimodal UAV object detection framework based on Mamba architectures. To
improve geometric adaptability, we propose the Deformable Token Mamba Block
(DTMB) to generate deformable tokens by incorporating adaptive patches from
deformable convolutions alongside normal patches from normal convolutions,
which serve as the inputs to the Mamba Block. To optimize the multimodal
feature complementarity, we design two separate DTMBs for the RGB and infrared
(IR) modalities, with the outputs from both DTMBs integrated into the Mamba
Block for feature extraction and into the Fusion Mamba Block for feature
fusion. Additionally, to improve multiscale object detection, especially for
small objects, we stack four DTMBs at different scales to produce multiscale
feature representations, which are then sent to the Detection Neck for Mamba
(DNM). The DNM module, inspired by the YOLO series, includes modifications to
the SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In
particular, we employ cross-enhanced spatial attention before the DTMB and
cross-channel attention after the Fusion Mamba Block to extract more
discriminative features. Experimental results on the DroneVehicle dataset show
that our method outperforms the baseline OAFA method by 3.6% in the mAP metric.
Codes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.

</details>


### [86] [Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting](https://arxiv.org/abs/2507.00852)
*Fatemeh Sadat Daneshmand*

Main category: cs.CV

TL;DR: 论文提出了一种基于Mask R-CNN的计算机视觉系统，使工业机器人能够在非结构化环境中检测和抓取笔组件，无需固定位置约束，并在不同光照条件下保持稳定性能。


<details>
  <summary>Details</summary>
Motivation: 工业4.0中的柔性制造系统需要机器人能够在非结构化环境中处理物体，而无需严格的定位约束。

Method: 采用Mask R-CNN方法，在ZHAW的完整笔制造线上实现和评估，解决了无位置约束的物体检测、极端光照变化的鲁棒性以及低成本相机的可靠性能等挑战。

Result: 系统在多样化光照条件下实现了95%的检测准确率，消除了对结构化组件放置的需求，减少了30%的设置时间，显著提高了制造灵活性。

Conclusion: 通过四种不同光照场景的广泛测试验证了该方法的实用性，适用于实际工业部署。

Abstract: Flexible manufacturing systems in Industry 4.0 require robots capable of
handling objects in unstructured environments without rigid positioning
constraints. This paper presents a computer vision system that enables
industrial robots to detect and grasp pen components in arbitrary orientations
without requiring structured trays, while maintaining robust performance under
varying lighting conditions. We implement and evaluate a Mask R-CNN-based
approach on a complete pen manufacturing line at ZHAW, addressing three
critical challenges: object detection without positional constraints,
robustness to extreme lighting variations, and reliable performance with
cost-effective cameras. Our system achieves 95% detection accuracy across
diverse lighting conditions while eliminating the need for structured component
placement, demonstrating a 30% reduction in setup time and significant
improvement in manufacturing flexibility. The approach is validated through
extensive testing under four distinct lighting scenarios, showing practical
applicability for real-world industrial deployment.

</details>


### [87] [SafeMap: Robust HD Map Construction from Incomplete Observations](https://arxiv.org/abs/2507.00861)
*Xiaoshuai Hao,Lingdong Kong,Rong Yin,Pengwei Wang,Jing Zhang,Yunfeng Diao,Shu Zhao*

Main category: cs.CV

TL;DR: SafeMap是一种新颖的框架，通过G-PVR和D-BEVC模块，解决了多视角相机数据不完整时的HD地图构建问题，显著提升了鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角相机数据不完整时表现不佳，影响了自动驾驶中HD地图的构建。

Method: SafeMap结合了G-PVR模块（基于高斯分布的视角重建）和D-BEVC模块（基于蒸馏的BEV校正），动态优先处理信息丰富的区域并校正不完整观测的BEV表示。

Result: 实验表明，SafeMap在完整和不完整数据场景下均显著优于现有方法。

Conclusion: SafeMap提供了一种即插即用的解决方案，提升了HD地图构建的鲁棒性和可靠性。

Abstract: Robust high-definition (HD) map construction is vital for autonomous driving,
yet existing methods often struggle with incomplete multi-view camera data.
This paper presents SafeMap, a novel framework specifically designed to secure
accuracy even when certain camera views are missing. SafeMap integrates two key
components: the Gaussian-based Perspective View Reconstruction (G-PVR) module
and the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.
G-PVR leverages prior knowledge of view importance to dynamically prioritize
the most informative regions based on the relationships among available camera
views. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV
representations derived from incomplete observations. Together, these
components facilitate the end-to-end map reconstruction and robust HD map
generation. SafeMap is easy to implement and integrates seamlessly into
existing systems, offering a plug-and-play solution for enhanced robustness.
Experimental results demonstrate that SafeMap significantly outperforms
previous methods in both complete and incomplete scenarios, highlighting its
superior performance and reliability.

</details>


### [88] [Is Visual in-Context Learning for Compositional Medical Tasks within Reach?](https://arxiv.org/abs/2507.00868)
*Simon Reiß,Zdravko Marinov,Alexander Jaus,Constantin Seibold,M. Saquib Sarfraz,Erik Rodner,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: 探索视觉上下文学习的潜力，使单一模型能处理多任务并在测试时适应新任务，无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法无法适应任务序列的问题，目标是让模型灵活处理复杂的多步骤视觉任务。

Method: 研究视觉上下文学习架构的特性，提出基于合成组合任务生成引擎的新训练方法，并探索掩码训练目标。

Result: 成功训练出能处理组合任务的视觉上下文学习模型，尤其适用于多模态医疗任务序列。

Conclusion: 研究提供了重要见解，但也揭示了需要解决的挑战。

Abstract: In this paper, we explore the potential of visual in-context learning to
enable a single model to handle multiple tasks and adapt to new tasks during
test time without re-training. Unlike previous approaches, our focus is on
training in-context learners to adapt to sequences of tasks, rather than
individual tasks. Our goal is to solve complex tasks that involve multiple
intermediate steps using a single model, allowing users to define entire vision
pipelines flexibly at test time. To achieve this, we first examine the
properties and limitations of visual in-context learning architectures, with a
particular focus on the role of codebooks. We then introduce a novel method for
training in-context learners using a synthetic compositional task generation
engine. This engine bootstraps task sequences from arbitrary segmentation
datasets, enabling the training of visual in-context learners for compositional
tasks. Additionally, we investigate different masking-based training objectives
to gather insights into how to train models better for solving complex,
compositional tasks. Our exploration not only provides important insights
especially for multi-modal medical task sequences but also highlights
challenges that need to be addressed.

</details>


### [89] [GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond](https://arxiv.org/abs/2507.00886)
*Anna-Maria Halacheva,Jan-Nico Zaech,Xi Wang,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯散射的场景中心3D视觉语言模型，通过语言和任务感知的场景表示，解决了现有方法对物体检测器的依赖问题。


<details>
  <summary>Details</summary>
Motivation: 当前3D视觉语言模型对物体检测器有强依赖性，导致处理瓶颈和分类灵活性受限。

Method: 将语言特征直接嵌入3D场景表示中，通过任务和位置引导的双重稀疏化器生成紧凑的任务相关令牌。

Result: 在域外设置中，性能比现有3D视觉语言模型提高了五倍。

Conclusion: 该方法首次利用高斯散射技术，展示了强大的泛化能力，为3D场景理解提供了新思路。

Abstract: As multimodal language models advance, their application to 3D scene
understanding is a fast-growing frontier, driving the development of 3D
Vision-Language Models (VLMs). Current methods show strong dependence on object
detectors, introducing processing bottlenecks and limitations in taxonomic
flexibility. To address these limitations, we propose a scene-centric 3D VLM
for 3D Gaussian splat scenes that employs language- and task-aware scene
representations. Our approach directly embeds rich linguistic features into the
3D scene representation by associating language with each Gaussian primitive,
achieving early modality alignment. To process the resulting dense
representations, we introduce a dual sparsifier that distills them into
compact, task-relevant tokens via task-guided and location-guided pathways,
producing sparse, task-aware global and local scene tokens. Notably, we present
the first Gaussian splatting-based VLM, leveraging photorealistic 3D
representations derived from standard RGB images, demonstrating strong
generalization: it improves performance of prior 3D VLM five folds, in
out-of-the-domain settings.

</details>


### [90] [ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2507.00898)
*Zifu Wan,Ce Zhang,Silong Yong,Martin Q. Ma,Simon Stepputtis,Louis-Philippe Morency,Deva Ramanan,Katia Sycara,Yaqi Xie*

Main category: cs.CV

TL;DR: ONLY是一种无需训练的解码方法，通过单次查询和单层干预减少LVLM的幻觉问题，适用于实时应用。


<details>
  <summary>Details</summary>
Motivation: 解决现有LVLM在实时应用中因多查询导致的响应延迟问题，同时减少幻觉现象。

Method: 使用文本到视觉熵比选择性放大关键文本信息，仅需单次查询和单层干预。

Result: 在多个基准测试中优于现有方法，且实现简单、计算成本低。

Conclusion: ONLY是一种高效、低成本的解决方案，适用于LVLM的实时部署。

Abstract: Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm
for understanding and reasoning about image input through textual responses.
Although they have achieved remarkable performance across a range of
multi-modal tasks, they face the persistent challenge of hallucination, which
introduces practical weaknesses and raises concerns about their reliable
deployment in real-world applications. Existing work has explored contrastive
decoding approaches to mitigate this issue, where the output of the original
LVLM is compared and contrasted with that of a perturbed version. However,
these methods require two or more queries that slow down LVLM response
generation, making them less suitable for real-time applications. To overcome
this limitation, we propose ONLY, a training-free decoding approach that
requires only a single query and a one-layer intervention during decoding,
enabling efficient real-time deployment. Specifically, we enhance textual
outputs by selectively amplifying crucial textual information using a
text-to-visual entropy ratio for each token. Extensive experimental results
demonstrate that our proposed ONLY consistently outperforms state-of-the-art
methods across various benchmarks while requiring minimal implementation effort
and computational cost. Code is available at https://github.com/zifuwan/ONLY.

</details>


### [91] [Masks make discriminative models great again!](https://arxiv.org/abs/2507.00916)
*Tianshi Cao,Marie-Julie Rakotosaona,Ben Poole,Federico Tombari,Michael Niemeyer*

Main category: cs.CV

TL;DR: Image2GS提出了一种从单图像重建3D场景的新方法，专注于图像到3D的转换部分，通过分离可见区域和不可见区域的训练，显著提升了重建质量。


<details>
  <summary>Details</summary>
Motivation: 解决从单图像重建3D场景的挑战，特别是图像到3D转换的确定性任务，避免生成不可见区域的内容。

Method: 使用优化的3D高斯溅射生成的可见性掩码，在训练中排除不可见区域，专注于可见区域的重建。

Result: 在可见区域的重建质量显著优于基线方法，同时在完整场景评估中与现有方法竞争。

Conclusion: Image2GS展示了专注于可见区域重建的优势，揭示了判别模型在处理不可见区域时的局限性。

Abstract: We present Image2GS, a novel approach that addresses the challenging problem
of reconstructing photorealistic 3D scenes from a single image by focusing
specifically on the image-to-3D lifting component of the reconstruction
process. By decoupling the lifting problem (converting an image to a 3D model
representing what is visible) from the completion problem (hallucinating
content not present in the input), we create a more deterministic task suitable
for discriminative models. Our method employs visibility masks derived from
optimized 3D Gaussian splats to exclude areas not visible from the source view
during training. This masked training strategy significantly improves
reconstruction quality in visible regions compared to strong baselines.
Notably, despite being trained only on masked regions, Image2GS remains
competitive with state-of-the-art discriminative models trained on full target
images when evaluated on complete scenes. Our findings highlight the
fundamental struggle discriminative models face when fitting unseen regions and
demonstrate the advantages of addressing image-to-3D lifting as a distinct
problem with specialized techniques.

</details>


### [92] [MVP: Winning Solution to SMP Challenge 2025 Video Track](https://arxiv.org/abs/2507.00950)
*Liliang Ye,Yunyao Zhang,Yafeng Wu,Yi-Ping Phoebe Chen,Junqing Yu,Wei Yang,Zikai Song*

Main category: cs.CV

TL;DR: MVP是一种多模态视频流行度预测框架，结合了深度视频特征、用户元数据和上下文信息，在SMP Challenge 2025中获胜。


<details>
  <summary>Details</summary>
Motivation: 社交媒体视频的流行度预测对内容推荐、趋势检测和用户参与有重要价值。

Method: MVP通过预训练模型提取视频特征，结合用户元数据和上下文信息，使用梯度提升回归模型进行预测。

Result: MVP在官方评测中排名第一，证明了其有效性和可靠性。

Conclusion: MVP为社交媒体平台的多模态视频流行度预测提供了高效解决方案。

Abstract: Social media platforms serve as central hubs for content dissemination,
opinion expression, and public engagement across diverse modalities. Accurately
predicting the popularity of social media videos enables valuable applications
in content recommendation, trend detection, and audience engagement. In this
paper, we present Multimodal Video Predictor (MVP), our winning solution to the
Video Track of the SMP Challenge 2025. MVP constructs expressive post
representations by integrating deep video features extracted from pretrained
models with user metadata and contextual information. The framework applies
systematic preprocessing techniques, including log-transformations and outlier
removal, to improve model robustness. A gradient-boosted regression model is
trained to capture complex patterns across modalities. Our approach ranked
first in the official evaluation of the Video Track, demonstrating its
effectiveness and reliability for multimodal video popularity prediction on
social platforms. The source code is available at
https://anonymous.4open.science/r/SMPDVideo.

</details>


### [93] [Surgical Neural Radiance Fields from One Image](https://arxiv.org/abs/2507.00969)
*Alberto Neri,Maximilan Fehrentz,Veronica Penza,Leonardo S. Mattos,Nazim Haouchine*

Main category: cs.CV

TL;DR: 利用术前MRI数据和术中单张图像，通过神经风格迁移实现快速单图像NeRF训练，适用于手术场景。


<details>
  <summary>Details</summary>
Motivation: 传统NeRF依赖多视角数据，但手术中数据有限且时间紧迫，难以满足需求。

Method: 结合术前MRI定义视角，通过WTC2和STROTSS风格迁移生成训练集，实现单图像训练。

Result: 在四个神经外科案例中验证，重建质量和纹理保持良好，与真实数据相似度高。

Conclusion: 证明了单图像NeRF训练在手术中的可行性，突破了传统多视角方法的限制。

Abstract: Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D
reconstruction and view synthesis, yet their reliance on extensive multi-view
data limits their application in surgical intraoperative settings where only
limited data is available. In particular, collecting such extensive data
intraoperatively is impractical due to time constraints. This work addresses
this challenge by leveraging a single intraoperative image and preoperative
data to train NeRF efficiently for surgical scenarios.
  Methods: We leverage preoperative MRI data to define the set of camera
viewpoints and images needed for robust and unobstructed training.
Intraoperatively, the appearance of the surgical image is transferred to the
pre-constructed training set through neural style transfer, specifically
combining WTC2 and STROTSS to prevent over-stylization. This process enables
the creation of a dataset for instant and fast single-image NeRF training.
  Results: The method is evaluated with four clinical neurosurgical cases.
Quantitative comparisons to NeRF models trained on real surgical microscope
images demonstrate strong synthesis agreement, with similarity metrics
indicating high reconstruction fidelity and stylistic alignment. When compared
with ground truth, our method demonstrates high structural similarity,
confirming good reconstruction quality and texture preservation.
  Conclusion: Our approach demonstrates the feasibility of single-image NeRF
training in surgical settings, overcoming the limitations of traditional
multi-view methods.

</details>


### [94] [RTMap: Real-Time Recursive Mapping with Change Detection and Localization](https://arxiv.org/abs/2507.00980)
*Yuheng Du,Sheng Yang,Lingxuan Wang,Zhenghua Hou,Chengying Cai,Zhitao Tan,Mingxia Chen,Shi-Sheng Huang,Qiang Li*

Main category: cs.CV

TL;DR: RTMap通过多轨迹众包HD地图，解决了单次遍历方法的感知不准确、遮挡和多智能体观测融合问题。


<details>
  <summary>Details</summary>
Motivation: 现有在线HD地图方法存在感知不准确、交通密集时的遮挡问题，且无法融合多智能体观测。

Method: RTMap采用不确定性感知的位置建模、概率感知的定位和实时道路结构变化检测。

Result: 实验表明RTMap在众包地图质量和定位精度上表现优异，支持下游预测和规划模块。

Conclusion: RTMap能异步提升众包地图的准确性和新鲜度，代码将开源。

Abstract: While recent online HD mapping methods relieve burdened offline pipelines and
solve map freshness, they remain limited by perceptual inaccuracies, occlusion
in dense traffic, and an inability to fuse multi-agent observations. We propose
RTMap to enhance these single-traversal methods by persistently crowdsourcing a
multi-traversal HD map as a self-evolutional memory. On onboard agents, RTMap
simultaneously addresses three core challenges in an end-to-end fashion: (1)
Uncertainty-aware positional modeling for HD map elements, (2)
probabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)
real-time detection for possible road structural changes. Experiments on
several public autonomous driving datasets demonstrate our solid performance on
both the prior-aided map quality and the localization accuracy, demonstrating
our effectiveness of robustly serving downstream prediction and planning
modules while gradually improving the accuracy and freshness of the
crowdsourced prior-map asynchronously. Our source-code will be made publicly
available at https://github.com/CN-ADLab/RTMap (Camera ready version
incorporating reviewer suggestions will be updated soon).

</details>


### [95] [Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations](https://arxiv.org/abs/2507.00981)
*Jack Nugent,Siyang Wu,Zeyu Ma,Beining Han,Meenal Parakh,Abhishek Joshi,Lingjie Mei,Alexander Raistrick,Xinyuan Li,Jia Deng*

Main category: cs.CV

TL;DR: PDE（Procedural Depth Evaluation）是一个新的基准测试，用于系统评估单目深度估计模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准测试主要评估准确性而非鲁棒性，PDE填补了这一空白。

Method: 通过程序生成3D场景，测试模型对物体、相机、材质和光照变化的鲁棒性。

Result: 揭示了当前最先进深度模型在面对特定扰动时的挑战。

Conclusion: PDE为深度估计研究提供了新的评估工具，有助于推动鲁棒性研究。

Abstract: Recent years have witnessed substantial progress on monocular depth
estimation, particularly as measured by the success of large models on standard
benchmarks. However, performance on standard benchmarks does not offer a
complete assessment, because most evaluate accuracy but not robustness. In this
work, we introduce PDE (Procedural Depth Evaluation), a new benchmark which
enables systematic robustness evaluation. PDE uses procedural generation to
create 3D scenes that test robustness to various controlled perturbations,
including object, camera, material and lighting changes. Our analysis yields
interesting findings on what perturbations are challenging for state-of-the-art
depth models, which we hope will inform further research. Code and data are
available at https://github.com/princeton-vl/proc-depth-eval.

</details>


### [96] [UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis](https://arxiv.org/abs/2507.00992)
*Yuanrui Wang,Cong Han,YafeiLi,Zhipeng Jin,Xiawei Li,SiNan Du,Wen Tao,Yi Yang,shuanglong li,Chun Yuan,Liu Lin*

Main category: cs.CV

TL;DR: 提出了一种基于分割引导的文本到图像生成框架，通过像素级视觉文本掩码作为统一条件输入，解决了现有方法在字体风格和颜色保留上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预渲染的字符图像作为条件，难以保留原始字体风格和颜色线索，且模型设计复杂，增加了开销和灵活性限制。

Method: 采用双语分割模型提取精确的文本掩码，并结合自适应字符条件和区域特定损失的扩散模型，以保持文本内容和风格的高保真度。

Result: 在AnyText基准测试中表现优异，显著优于现有方法，并在新提出的GlyphMM-benchmark和MiniText-benchmark中大幅领先。

Conclusion: 该方法在复杂排版和小文本区域生成中表现出色，验证了其强大的泛化能力和部署准备就绪。

Abstract: Text-to-image generation has greatly advanced content creation, yet
accurately rendering visual text remains a key challenge due to blurred glyphs,
semantic drift, and limited style control. Existing methods often rely on
pre-rendered glyph images as conditions, but these struggle to retain original
font styles and color cues, necessitating complex multi-branch designs that
increase model overhead and reduce flexibility. To address these issues, we
propose a segmentation-guided framework that uses pixel-level visual text masks
-- rich in glyph shape, color, and spatial detail -- as unified conditional
inputs. Our method introduces two core components: (1) a fine-tuned bilingual
segmentation model for precise text mask extraction, and (2) a streamlined
diffusion model augmented with adaptive glyph conditioning and a
region-specific loss to preserve textual fidelity in both content and style.
Our approach achieves state-of-the-art performance on the AnyText benchmark,
significantly surpassing prior methods in both Chinese and English settings. To
enable more rigorous evaluation, we also introduce two new benchmarks:
GlyphMM-benchmark for testing layout and glyph consistency in complex
typesetting, and MiniText-benchmark for assessing generation quality in
small-scale text regions. Experimental results show that our model outperforms
existing methods by a large margin in both scenarios, particularly excelling at
small text rendering and complex layout preservation, validating its strong
generalization and deployment readiness.

</details>


### [97] [GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning](https://arxiv.org/abs/2507.01006)
*Wenyi Hong,Wenmeng Yu,Xiaotao Gu,Guo Wang,Guobing Gan,Haomiao Tang,Jiale Cheng,Ji Qi,Junhui Ji,Lihang Pan,Shuaiqi Duan,Weihan Wang,Yan Wang,Yean Cheng,Zehai He,Zhe Su,Zhen Yang,Ziyang Pan,Aohan Zeng,Baoxu Wang,Boyan Shi,Changyu Pang,Chenhui Zhang,Da Yin,Fan Yang,Guoqing Chen,Jiazheng Xu,Jiali Chen,Jing Chen,Jinhao Chen,Jinghao Lin,Jinjiang Wang,Junjie Chen,Leqi Lei,Leyi Pan,Mingzhi Zhang,Qinkai Zheng,Sheng Yang,Shi Zhong,Shiyu Huang,Shuyuan Zhao,Siyan Xue,Shangqin Tu,Shengbiao Meng,Tianshu Zhang,Tianwei Luo,Tianxiang Hao,Tianle Gong,Wenkai Li,Wei Jia,Xin Lyu,Xuancheng Huang,Yanling Wang,Yadong Xue,Yanfeng Wang,Yifan An,Yifan Du,Yiming Shi,Yiheng Huang,Yilin Niu,Yuan Wang,Yuanchang Yue,Yuchen Li,Yutao Zhang,Yuxuan Zhang,Zhanxiao Du,Zhenyu Hou,Zhao Xue,Zhengxiao Du,Zihan Wang,Peng Zhang,Debing Liu,Bin Xu,Juanzi Li,Minlie Huang,Yuxiao Dong,Jie Tang*

Main category: cs.CV

TL;DR: GLM-4.1V-Thinking是一个先进的视觉语言模型，通过强化学习和课程采样提升多模态推理能力，在多个任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 开发一个通用的多模态推理模型，提升在STEM问题解决、视频理解等任务上的性能。

Method: 采用大规模预训练和强化学习与课程采样（RLCS）框架，优化模型能力。

Result: 在28个公共基准测试中表现优异，优于同类规模模型，甚至在某些任务上超越更大的模型和闭源模型。

Conclusion: GLM-4.1V-Thinking展示了强大的多模态推理能力，为相关研究提供了开源资源。

Abstract: We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to
advance general-purpose multimodal reasoning. In this report, we share our key
findings in the development of the reasoning-centric training framework. We
first develop a capable vision foundation model with significant potential
through large-scale pre-training, which arguably sets the upper bound for the
final performance. Reinforcement Learning with Curriculum Sampling (RLCS) then
unlocks the full potential of the model, leading to comprehensive capability
enhancement across a diverse range of tasks, including STEM problem solving,
video understanding, content recognition, coding, grounding, GUI-based agents,
and long document understanding, among others. To facilitate research in this
field, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art
performance among models of comparable size. In a comprehensive evaluation
across 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all
tasks and achieves comparable or even superior performance on 18 benchmarks
relative to the significantly larger Qwen2.5-VL-72B. Notably,
GLM-4.1V-9B-Thinking also demonstrates competitive or superior performance
compared to closed-source models such as GPT-4o on challenging tasks including
long document understanding and STEM reasoning, further underscoring its strong
capabilities. Code, models and more information are released at
https://github.com/THUDM/GLM-4.1V-Thinking.

</details>


### [98] [ShapeEmbed: a self-supervised learning framework for 2D contour quantification](https://arxiv.org/abs/2507.01009)
*Anna Foix Romero,Craig Russell,Alexander Krull,Virginie Uhlmann*

Main category: cs.CV

TL;DR: ShapeEmbed是一种自监督表示学习框架，用于编码2D图像中对象的轮廓为形状描述符，具有平移、缩放、旋转、反射和点索引不变性。


<details>
  <summary>Details</summary>
Motivation: 形状量化需要确保提取的测量值在保持对象内在几何的变换下不变，如大小、方向和位置的变化。

Method: ShapeEmbed通过自监督学习将欧几里得距离矩阵编码为形状描述符。

Result: ShapeEmbed在自然和生物图像的形状分类任务中优于现有方法。

Conclusion: ShapeEmbed在生物成像应用中具有潜在的重要价值。

Abstract: The shape of objects is an important source of visual information in a wide
range of applications. One of the core challenges of shape quantification is to
ensure that the extracted measurements remain invariant to transformations that
preserve an object's intrinsic geometry, such as changing its size,
orientation, and position in the image. In this work, we introduce ShapeEmbed,
a self-supervised representation learning framework designed to encode the
contour of objects in 2D images, represented as a Euclidean distance matrix,
into a shape descriptor that is invariant to translation, scaling, rotation,
reflection, and point indexing. Our approach overcomes the limitations of
traditional shape descriptors while improving upon existing state-of-the-art
autoencoder-based approaches. We demonstrate that the descriptors learned by
our framework outperform their competitors in shape classification tasks on
natural and biological images. We envision our approach to be of particular
relevance to biological imaging applications.

</details>


### [99] [DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution](https://arxiv.org/abs/2507.01012)
*Zhe Kong,Le Li,Yong Zhang,Feng Gao,Shaoshu Yang,Tao Wang,Kaihao Zhang,Zhuoliang Kang,Xiaoming Wei,Guanying Chen,Wenhan Luo*

Main category: cs.CV

TL;DR: DAM-VSR提出了一种外观和运动解耦框架，通过结合视频扩散模型和图像超分辨率模型，解决了视频超分辨率中的时间一致性和细节生成问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的视频超分辨率面临复杂且不可预测的退化问题，现有方法难以生成时间一致的帧。

Method: DAM-VSR将视频超分辨率解耦为外观增强和运动控制问题，分别通过参考图像超分辨率和视频ControlNet实现。

Result: DAM-VSR在真实世界和AIGC数据上实现了最先进的性能，展示了强大的细节生成能力。

Conclusion: DAM-VSR通过解耦框架和双向采样策略，有效提升了视频超分辨率的性能。

Abstract: Real-world video super-resolution (VSR) presents significant challenges due
to complex and unpredictable degradations. Although some recent methods utilize
image diffusion models for VSR and have shown improved detail generation
capabilities, they still struggle to produce temporally consistent frames. We
attempt to use Stable Video Diffusion (SVD) combined with ControlNet to address
this issue. However, due to the intrinsic image-animation characteristics of
SVD, it is challenging to generate fine details using only low-quality videos.
To tackle this problem, we propose DAM-VSR, an appearance and motion
disentanglement framework for VSR. This framework disentangles VSR into
appearance enhancement and motion control problems. Specifically, appearance
enhancement is achieved through reference image super-resolution, while motion
control is achieved through video ControlNet. This disentanglement fully
leverages the generative prior of video diffusion models and the detail
generation capabilities of image super-resolution models. Furthermore, equipped
with the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can
conduct VSR on longer input videos. DAM-VSR achieves state-of-the-art
performance on real-world data and AIGC data, demonstrating its powerful detail
generation capabilities.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [100] [FreNBRDF: A Frequency-Rectified Neural Material Representation](https://arxiv.org/abs/2507.00476)
*Chenliang Zhou,Zheyuan Hu,Cengiz Oztireli*

Main category: cs.GR

TL;DR: FreNBRDF是一种频率校正的神经材质表示方法，通过球谐函数将频域考虑融入神经BRDF建模，提升了材质外观重建和编辑的准确性与鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖表格化的BRDF数据，而神经隐式表示虽灵活但频域行为理解不足，FreNBRDF旨在解决这一问题。

Method: 提出频率校正损失函数，结合球谐函数，构建通用且自适应的重建和编辑流程。

Result: 实验表明FreNBRDF在材质外观重建和编辑上优于现有方法，提升了保真度、适应性和效率。

Conclusion: FreNBRDF为下游任务提供了更结构化、可解释的解决方案。

Abstract: Accurate material modeling is crucial for achieving photorealistic rendering,
bridging the gap between computer-generated imagery and real-world photographs.
While traditional approaches rely on tabulated BRDF data, recent work has
shifted towards implicit neural representations, which offer compact and
flexible frameworks for a range of tasks. However, their behavior in the
frequency domain remains poorly understood. To address this, we introduce
FreNBRDF, a frequency-rectified neural material representation. By leveraging
spherical harmonics, we integrate frequency-domain considerations into neural
BRDF modeling. We propose a novel frequency-rectified loss, derived from a
frequency analysis of neural materials, and incorporate it into a generalizable
and adaptive reconstruction and editing pipeline. This framework enhances
fidelity, adaptability, and efficiency. Extensive experiments demonstrate that
\ours improves the accuracy and robustness of material appearance
reconstruction and editing compared to state-of-the-art baselines, enabling
more structured and interpretable downstream tasks and applications.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [101] [Rethink 3D Object Detection from Physical World](https://arxiv.org/abs/2507.00190)
*Satoshi Tanaka,Koji Minoda,Fumiya Watanabe,Takamasa Horibe*

Main category: cs.RO

TL;DR: 论文提出了两种新指标（L-AP和P-AP），用于更全面地评估实时3D目标检测，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在评估3D目标检测时，通常忽略速度与精度的权衡以及对运动规划的影响，缺乏对硬件差异的定量分析。

Method: 引入延迟感知AP（L-AP）和规划感知AP（P-AP）作为新指标，结合nuPlan数据集验证，并通过延迟感知超参数优化（L-HPO）开发高性能模型。

Result: 实验表明新指标能更全面地评估系统性能，并优化了硬件和模型选择，同时纠正了“点云越多性能越好”的错误假设。

Conclusion: 新指标为实时3D目标检测提供了更全面的评估框架，优化了系统性能，并揭示了点云数量与性能的非线性关系。

Abstract: High-accuracy and low-latency 3D object detection is essential for autonomous
driving systems. While previous studies on 3D object detection often evaluate
performance based on mean average precision (mAP) and latency, they typically
fail to address the trade-off between speed and accuracy, such as 60.0 mAP at
100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs
between different hardware devices and accelerators remains unexplored, despite
being critical for real-time applications. Furthermore, they overlook the
impact on collision avoidance in motion planning, for example, 60.0 mAP leading
to safer motion planning or 61.0 mAP leading to high-risk motion planning. In
this paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)
as new metrics, which consider the physical world such as the concept of time
and physical constraints, offering a more comprehensive evaluation for
real-time 3D object detection. We demonstrate the effectiveness of our metrics
for the entire autonomous driving system using nuPlan dataset, and evaluate 3D
object detection models accounting for hardware differences and accelerators.
We also develop a state-of-the-art performance model for real-time 3D object
detection through latency-aware hyperparameter optimization (L-HPO) using our
metrics. Additionally, we quantitatively demonstrate that the assumption "the
more point clouds, the better the recognition performance" is incorrect for
real-time applications and optimize both hardware and model selection using our
metrics.

</details>


### [102] [Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding](https://arxiv.org/abs/2507.00416)
*Tao Lin,Gen Li,Yilei Zhong,Yanwen Zou,Bo Zhao*

Main category: cs.RO

TL;DR: 提出了一种无需额外传感器的3D几何特征注入方法，显著提升了VLA模型的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型基于2D图像-文本对训练，缺乏精确的空间理解能力。

Method: 利用现成的视觉几何基础模型，隐式注入3D几何特征到VLA模型中。

Result: 在五项空间挑战任务中显著提升了SOTA VLA模型的性能。

Conclusion: 该方法为增强机器人空间感知提供了一种高效且无需额外硬件支持的解决方案。

Abstract: Vision-Language-Action (VLA) models have emerged as a promising framework for
enabling generalist robots capable of perceiving, reasoning, and acting in the
real world. These models usually build upon pretrained Vision-Language Models
(VLMs), which excel at semantic understanding due to large-scale text
pretraining. However, VLMs typically lack precise spatial understanding
capabilities, as they are primarily tuned on 2D image-text pairs without 3D
supervision. To address this limitation, recent approaches have incorporated
explicit 3D inputs such as point clouds or depth maps, but this necessitates
additional depth sensors or defective estimation. In contrast, our work
introduces a plug-and-play module that implicitly injects 3D geometry features
into VLA models by leveraging an off-the-shelf visual geometry foundation
models. We design five spatially challenging tasks that require precise spatial
understanding ability to validate effectiveness of our method. Extensive
evaluations show that our method significantly improves the performance of
state-of-the-art VLA models across diverse scenarios.

</details>


### [103] [RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation](https://arxiv.org/abs/2507.00435)
*Yi Ru Wang,Carter Ung,Grant Tannert,Jiafei Duan,Josephine Li,Amy Le,Rishabh Oswal,Markus Grotz,Wilbert Pumacay,Yuquan Deng,Ranjay Krishna,Dieter Fox,Siddhartha Srinivasa*

Main category: cs.RO

TL;DR: RoboEval是一个用于评估双手机器人操作策略的仿真基准和结构化评估框架，旨在揭示当前策略的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅报告二元任务成功率，掩盖了策略行为中的关键弱点，如协调不良、抓取时滑动或手臂使用不对称。

Method: RoboEval引入了一套分层、语义明确的任务，分解为技能特定阶段，并通过变体系统性地挑战空间、物理和协调能力。任务配有细粒度诊断指标和3000多个人类演示以支持模仿学习。

Result: 实验表明，成功率相近的策略在执行任务时表现差异显著，行为指标在超过一半的任务-指标对中与成功率相关，且在二元成功率饱和时仍具信息量。

Conclusion: RoboEval通过精确定位策略失败的时间和方式，提供了对机器人操作更深入、更具操作性的理解，并强调了超越单纯成功率的评估工具的必要性。

Abstract: We present RoboEval, a simulation benchmark and structured evaluation
framework designed to reveal the limitations of current bimanual manipulation
policies. While prior benchmarks report only binary task success, we show that
such metrics often conceal critical weaknesses in policy behavior -- such as
poor coordination, slipping during grasping, or asymmetric arm usage. RoboEval
introduces a suite of tiered, semantically grounded tasks decomposed into
skill-specific stages, with variations that systematically challenge spatial,
physical, and coordination capabilities. Tasks are paired with fine-grained
diagnostic metrics and 3000+ human demonstrations to support imitation
learning. Our experiments reveal that policies with similar success rates
diverge in how tasks are executed -- some struggle with alignment, others with
temporally consistent bimanual control. We find that behavioral metrics
correlate with success in over half of task-metric pairs, and remain
informative even when binary success saturates. By pinpointing when and how
policies fail, RoboEval enables a deeper, more actionable understanding of
robotic manipulation -- and highlights the need for evaluation tools that go
beyond success alone.

</details>


### [104] [Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery](https://arxiv.org/abs/2507.00635)
*Tinghe Hong,Shenlin Cai,Boyang Li,Kai Huang*

Main category: cs.RO

TL;DR: 提出了一种结合机器学习和传统算法的创新眼动追踪方法，解决了手术机器人术前导航中的眼动估计问题。


<details>
  <summary>Details</summary>
Motivation: 现有眼动估计技术在手术中依赖额外传感器、易受遮挡和需要面部检测，限制了术前导航的一致性和准确性。

Method: 结合机器学习和传统算法，无需依赖面部标志点，能在不同光照和阴影条件下稳定检测虹膜和估计视线方向。

Result: 实验显示，眼动方向估计的平均误差为0.58度，基于估计方向的机械臂运动控制平均误差为2.08度。

Conclusion: 该方法显著提高了眼动追踪的稳定性和精度，为手术机器人的术前导航提供了更可靠的解决方案。

Abstract: Ophthalmic surgical robots offer superior stability and precision by reducing
the natural hand tremors of human surgeons, enabling delicate operations in
confined surgical spaces. Despite the advancements in developing vision- and
force-based control methods for surgical robots, preoperative navigation
remains heavily reliant on manual operation, limiting the consistency and
increasing the uncertainty. Existing eye gaze estimation techniques in the
surgery, whether traditional or deep learning-based, face challenges including
dependence on additional sensors, occlusion issues in surgical environments,
and the requirement for facial detection. To address these limitations, this
study proposes an innovative eye localization and tracking method that combines
machine learning with traditional algorithms, eliminating the requirements of
landmarks and maintaining stable iris detection and gaze estimation under
varying lighting and shadow conditions. Extensive real-world experiment results
show that our proposed method has an average estimation error of 0.58 degrees
for eye orientation estimation and 2.08-degree average control error for the
robotic arm's movement based on the calculated orientation.

</details>


### [105] [RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles](https://arxiv.org/abs/2507.00937)
*David Hunt,Shaocheng Luo,Spencer Hallyburton,Shafii Nillongo,Yi Li,Tingjun Chen,Miroslav Pajic*

Main category: cs.RO

TL;DR: RaGNNarok是一个基于图神经网络的轻量级框架，用于增强雷达点云，适用于低成本室内移动机器人。


<details>
  <summary>Details</summary>
Motivation: 现有基于激光雷达和摄像头的解决方案在视觉遮挡环境中性能不佳，计算开销大且成本高，而毫米波雷达传感器虽成本低但存在点云稀疏、噪声和误检问题。

Method: 提出RaGNNarok，一种实时、轻量级的GNN框架，用于增强雷达点云，适用于复杂动态环境。

Result: 在低成本Raspberry Pi 5上推理时间仅7.3毫秒，无需额外计算资源，在定位、SLAM和自主导航任务中表现可靠且通用性强。

Conclusion: RaGNNarok为低成本室内移动机器人提供了稳健的解决方案。

Abstract: Low-cost indoor mobile robots have gained popularity with the increasing
adoption of automation in homes and commercial spaces. However, existing lidar
and camera-based solutions have limitations such as poor performance in
visually obscured environments, high computational overhead for data
processing, and high costs for lidars. In contrast, mmWave radar sensors offer
a cost-effective and lightweight alternative, providing accurate ranging
regardless of visibility. However, existing radar-based localization suffers
from sparse point cloud generation, noise, and false detections. Thus, in this
work, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph
neural network (GNN)-based framework to enhance radar point clouds, even in
complex and dynamic environments. With an inference time of just 7.3 ms on the
low-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such
resource-constrained devices, requiring no additional computational resources.
We evaluate its performance across key tasks, including localization, SLAM, and
autonomous navigation, in three different environments. Our results demonstrate
strong reliability and generalizability, making RaGNNarok a robust solution for
low-cost indoor mobile robots.

</details>


### [106] [Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation](https://arxiv.org/abs/2507.00984)
*Xihang Yu,Rajat Talak,Jingnan Shi,Ulrich Viereck,Igor Gilitschenski,Luca Carlone*

Main category: cs.RO

TL;DR: 提出了一种自监督领域适应方法，利用未标注数据提升仓库自动化系统中箱子姿态和形状估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代仓库自动化系统依赖智能机器人生成大量未标注数据，如何利用这些数据提升感知模型是关键。

Method: 开发了自监督领域适应流程，结合真实未标注数据，提出correct-and-certify流程用于箱子姿态和形状估计。

Result: 在模拟和真实工业场景中广泛评估，自监督模型显著优于纯仿真训练模型，并在零样本3D边界框估计基线上有显著提升。

Conclusion: 自监督方法能有效利用未标注数据，提升仓库自动化系统的感知模型性能。

Abstract: Modern warehouse automation systems rely on fleets of intelligent robots that
generate vast amounts of data -- most of which remains unannotated. This paper
develops a self-supervised domain adaptation pipeline that leverages
real-world, unlabeled data to improve perception models without requiring
manual annotations. Our work focuses specifically on estimating the pose and
shape of boxes and presents a correct-and-certify pipeline for self-supervised
box pose and shape estimation. We extensively evaluate our approach across a
range of simulated and real industrial settings, including adaptation to a
large-scale real-world dataset of 50,000 images. The self-supervised model
significantly outperforms models trained solely in simulation and shows
substantial improvements over a zero-shot 3D bounding box estimation baseline.

</details>


### [107] [Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations](https://arxiv.org/abs/2507.00990)
*Shivansh Patel,Shraddhaa Mohan,Hanlin Mai,Unnat Jain,Svetlana Lazebnik,Yunzhu Li*

Main category: cs.RO

TL;DR: RIGVid系统通过模仿AI生成的视频让机器人执行复杂任务，无需物理演示或机器人特定训练。


<details>
  <summary>Details</summary>
Motivation: 解决机器人执行复杂任务时依赖物理演示或特定训练的问题。

Method: 使用视频扩散模型生成演示视频，VLM过滤不符合命令的视频，6D姿态跟踪器提取轨迹并适配到机器人。

Result: 生成的视频效果与真实演示相当，性能随生成质量提升，优于关键点预测等方法。

Conclusion: AI生成的视频可作为机器人操作的有效监督来源。

Abstract: This work introduces Robots Imitating Generated Videos (RIGVid), a system
that enables robots to perform complex manipulation tasks--such as pouring,
wiping, and mixing--purely by imitating AI-generated videos, without requiring
any physical demonstrations or robot-specific training. Given a language
command and an initial scene image, a video diffusion model generates potential
demonstration videos, and a vision-language model (VLM) automatically filters
out results that do not follow the command. A 6D pose tracker then extracts
object trajectories from the video, and the trajectories are retargeted to the
robot in an embodiment-agnostic fashion. Through extensive real-world
evaluations, we show that filtered generated videos are as effective as real
demonstrations, and that performance improves with generation quality. We also
show that relying on generated videos outperforms more compact alternatives
such as keypoint prediction using VLMs, and that strong 6D pose tracking
outperforms other ways to extract trajectories, such as dense feature point
tracking. These findings suggest that videos produced by a state-of-the-art
off-the-shelf model can offer an effective source of supervision for robotic
manipulation.

</details>


### [108] [VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers](https://arxiv.org/abs/2507.01016)
*Yating Wang,Haoyi Zhu,Mingyu Liu,Jiange Yang,Hao-Shu Fang,Tong He*

Main category: cs.RO

TL;DR: 本文提出了一种基于向量量化的动作标记器，利用大规模数据集捕捉时空动态，显著提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法数据量不足，难以捕捉复杂动作轨迹的时空动态，限制了模型性能。

Method: 基于最大规模的动作轨迹数据集，构建向量量化动作标记器，利用合成数据训练并验证。

Result: 实验表明，合成数据量增加显著提升下游任务性能，真实任务成功率提高30%。

Conclusion: 该动作标记器为实时智能系统提供了可扩展的解决方案，有望提升机器人控制的效率和可靠性。

Abstract: In this paper, we introduce an innovative vector quantization based action
tokenizer built upon the largest-scale action trajectory dataset to date,
leveraging over 100 times more data than previous approaches. This extensive
dataset enables our tokenizer to capture rich spatiotemporal dynamics,
resulting in a model that not only accelerates inference but also generates
smoother and more coherent action outputs. Once trained, the tokenizer can be
seamlessly adapted to a wide range of downstream tasks in a zero-shot manner,
from short-horizon reactive behaviors to long-horizon planning. A key finding
of our work is that the domain gap between synthetic and real action
trajectories is marginal, allowing us to effectively utilize a vast amount of
synthetic data during training without compromising real-world performance. To
validate our approach, we conducted extensive experiments in both simulated
environments and on real robotic platforms. The results demonstrate that as the
volume of synthetic trajectory data increases, the performance of our tokenizer
on downstream tasks improves significantly-most notably, achieving up to a 30%
higher success rate on two real-world tasks in long-horizon scenarios. These
findings highlight the potential of our action tokenizer as a robust and
scalable solution for real-time embodied intelligence systems, paving the way
for more efficient and reliable robotic control in diverse application
domains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [109] [MuteSwap: Silent Face-based Voice Conversion](https://arxiv.org/abs/2507.00498)
*Yifan Liu,Yu Fang,Zhouhan Lin*

Main category: cs.SD

TL;DR: MuteSwap框架通过视觉输入实现无声视频中的语音转换，解决了传统方法依赖音频输入的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统语音转换依赖音频输入，但在无声视频或嘈杂环境中无法使用，因此需要一种仅依赖视觉输入的方法。

Method: 提出MuteSwap框架，利用对比学习对齐跨模态身份，并通过最小化互信息分离共享视觉特征。

Result: 实验表明，MuteSwap在语音合成和身份转换中表现优异，尤其在嘈杂环境下优于依赖音频的方法。

Conclusion: MuteSwap证明了仅依赖视觉输入的语音转换的可行性，并在复杂环境中表现出色。

Abstract: Conventional voice conversion modifies voice characteristics from a source
speaker to a target speaker, relying on audio input from both sides. However,
this process becomes infeasible when clean audio is unavailable, such as in
silent videos or noisy environments. In this work, we focus on the task of
Silent Face-based Voice Conversion (SFVC), which does voice conversion entirely
from visual inputs. i.e., given images of a target speaker and a silent video
of a source speaker containing lip motion, SFVC generates speech aligning the
identity of the target speaker while preserving the speech content in the
source silent video. As this task requires generating intelligible speech and
converting identity using only visual cues, it is particularly challenging. To
address this, we introduce MuteSwap, a novel framework that employs contrastive
learning to align cross-modality identities and minimize mutual information to
separate shared visual features. Experimental results show that MuteSwap
achieves impressive performance in both speech synthesis and identity
conversion, especially under noisy conditions where methods dependent on audio
input fail to produce intelligible results, demonstrating both the
effectiveness of our training approach and the feasibility of SFVC.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [110] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉定位和模态感知优化解决GUI中的多样性和模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 解决GUI中视觉元素多样、空间杂乱和语言模糊性带来的挑战。

Method: 将GUI分为文本和图标元素，利用通用视觉语言模型独立处理每种模态，并通过动态聚焦和分层细化优化结果。

Result: 在标准GUI基准测试中表现优于基线方法，验证了模态分离与区域聚焦推理的有效性。

Conclusion: DiMo-GUI通过动态和分层方法显著提升了GUI自然语言查询的准确性，无需额外训练或标注。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [111] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: TalentMine是一个基于LLM的框架，通过语义增强的表格表示解决人才管理系统中表格信息检索的挑战，显著提升了查询准确性。


<details>
  <summary>Details</summary>
Motivation: 传统表格提取方法在语义理解上表现不佳，导致检索增强型聊天应用中的信息检索失败。

Method: TalentMine采用多模态推理，将提取的表格转换为语义丰富的表示，保留结构和语义信息。

Result: 在员工福利文档集合的实验中，TalentMine实现了100%的查询准确率，远超现有方法。

Conclusion: TalentMine通过语义增强的表格表示和高效集成框架，显著提升了人才管理系统的性能。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [112] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)
*Emin Zerman,Jonas Carlsson,Mårten Sjöström*

Main category: cs.HC

TL;DR: 开发了一个射击可视化系统，结合第一人称视频和图形化指标，帮助教练和射手提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 当前射击训练主要依赖重复练习，教练无法实时观察射手视角，分析仅限于姿势和准确性。

Method: 开发了五种复合可视化视图，通过混合方法研究（包括任务测试、偏好比较和访谈）评估其效果。

Result: 仪表板式复合视图（结合原始视频、极坐标图和图表）在10个案例中有9个被偏好，且适用于不同技能水平的射手。

Conclusion: 第一人称视频与可视化分析的结合在精准运动中有广泛应用潜力。

Abstract: Marksmanship practices are required in various professions, including police,
military personnel, hunters, as well as sports shooters, such as Olympic
shooting, biathlon, and modern pentathlon. The current form of training and
coaching is mostly based on repetition, where the coach does not see through
the eyes of the shooter, and analysis is limited to stance and accuracy
post-session. In this study, we present a shooting visualization system and
evaluate its perceived effectiveness for both novice and expert shooters. To
achieve this, five composite visualizations were developed using first-person
shooting video recordings enriched with overlaid metrics and graphical
summaries. These views were evaluated with 10 participants (5 expert marksmen,
5 novices) through a mixed-methods study including shot-count and aiming
interpretation tasks, pairwise preference comparisons, and semi-structured
interviews. The results show that a dashboard-style composite view, combining
raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases
and supported understanding across skill levels. The insights gained from this
design study point to the broader value of integrating first-person video with
visual analytics for coaching, and we suggest directions for applying this
approach to other precision-based sports.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [113] [Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms](https://arxiv.org/abs/2507.00491)
*Zain Taufique,Aman Vyas,Antonio Miele,Pasi Liljeberg,Anil Kanduri*

Main category: cs.MA

TL;DR: Twill是一个运行时框架，用于在异构移动边缘平台上调度复合AI（cAI）系统的并发推理任务，通过任务亲和性集群映射和迁移、优先级感知的任务冻结/解冻以及DVFS，显著降低推理延迟。


<details>
  <summary>Details</summary>
Motivation: 复合AI系统（cAI）由多种AI模型（如DNNs、transformers和LLMs）组成，其动态工作负载变化和计算多样性使得在移动边缘平台上部署具有挑战性。现有方法无法有效处理cAI系统的并发推理需求。

Method: Twill框架通过任务亲和性集群映射和迁移、优先级感知的任务冻结/解冻以及动态电压频率调整（DVFS）来优化并发推理任务的调度。

Result: 在Nvidia Jetson Orin NX平台上实现并评估Twill，结果显示其平均降低推理延迟54%，同时满足功耗预算。

Conclusion: Twill为异构移动边缘平台上的cAI系统提供了一种高效的运行时调度解决方案，显著提升了推理性能。

Abstract: Compound AI (cAI) systems chain multiple AI models to solve complex problems.
cAI systems are typically composed of deep neural networks (DNNs),
transformers, and large language models (LLMs), exhibiting a high degree of
computational diversity and dynamic workload variation. Deploying cAI services
on mobile edge platforms poses a significant challenge in scheduling concurrent
DNN-transformer inference tasks, which arrive dynamically in an unknown
sequence. Existing mobile edge AI inference strategies manage multi-DNN or
transformer-only workloads, relying on design-time profiling, and cannot handle
concurrent inference of DNNs and transformers required by cAI systems. In this
work, we address the challenge of scheduling cAI systems on heterogeneous
mobile edge platforms. We present Twill, a run-time framework to handle
concurrent inference requests of cAI workloads through task affinity-aware
cluster mapping and migration, priority-aware task freezing/unfreezing, and
DVFS, while minimizing inference latency within power budgets. We implement and
deploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate
Twill against state-of-the-art edge AI inference techniques over contemporary
DNNs and LLMs, reducing inference latency by 54% on average, while honoring
power budgets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [114] [Gradient-based Fine-Tuning through Pre-trained Model Regularization](https://arxiv.org/abs/2507.00016)
*Xuanbo Liu,Liu Liu,Fuxiang Wu,Fusheng Hao,Xianglong Liu*

Main category: cs.LG

TL;DR: GRFT是一种高效的基于梯度和正则化的微调方法，通过更新权重矩阵的行或列来减少存储开销并提高参数选择效率。


<details>
  <summary>Details</summary>
Motivation: 预训练模型微调需要大量计算资源和存储，现有方法如GPS虽减少参数但增加资源需求。

Method: GRFT选择梯度平方和最高的行或列进行更新，并结合正则化增强知识迁移。

Result: GRFT在FGVC和VTAB数据集上仅需更新1.22%和0.30%的参数，性能优于GPS、Adapter Tuning和LoRA。

Conclusion: GRFT高效且有效，显著减少资源需求并提升性能。

Abstract: Large pre-trained models have demonstrated extensive applications across
various fields. However, fine-tuning these models for specific downstream tasks
demands significant computational resources and storage. One fine-tuning
method, gradient-based parameter selection (GPS), focuses on fine-tuning only
the parameters with high gradients in each neuron, thereby reducing the number
of training parameters. Nevertheless, this approach increases computational
resource requirements and storage demands. In this paper, we propose an
efficient gradient-based and regularized fine-tuning method (GRFT) that updates
the rows or columns of the weight matrix. We theoretically demonstrate that the
rows or columns with the highest sum of squared gradients are optimal for
updating. This strategy effectively reduces storage overhead and improves the
efficiency of parameter selection. Additionally, we incorporate regularization
to enhance knowledge transfer from the pre-trained model. GRFT achieves
state-of-the-art performance, surpassing existing methods such as GPS, Adapter
Tuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the
total parameters on FGVC and VTAB datasets, respectively, demonstrating its
high efficiency and effectiveness. The source code will be released soon.

</details>


### [115] [HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation](https://arxiv.org/abs/2507.00028)
*Lihuan Li,Hao Xue,Shuang Ao,Yang Song,Flora Salim*

Main category: cs.LG

TL;DR: HiT-JEPA 是一个统一框架，用于学习多尺度城市轨迹表示，通过分层设计整合局部动态和全局语义。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时捕捉轨迹的细粒度细节和高层摘要，限制了模型对长期依赖和局部细微差异的关注。

Method: HiT-JEPA 采用三层层次结构，逐步捕获点级细节、中间模式和高层轨迹抽象。

Result: 在多个真实数据集上的实验表明，HiT-JEPA 的分层设计能生成更丰富的多尺度表示。

Conclusion: HiT-JEPA 通过分层设计有效整合了轨迹的局部和全局信息，提升了轨迹相似性计算的性能。

Abstract: The representation of urban trajectory data plays a critical role in
effectively analyzing spatial movement patterns. Despite considerable progress,
the challenge of designing trajectory representations that can capture diverse
and complementary information remains an open research problem. Existing
methods struggle in incorporating trajectory fine-grained details and
high-level summary in a single model, limiting their ability to attend to both
long-term dependencies while preserving local nuances. To address this, we
propose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint
Embedding Predictive Architecture), a unified framework for learning
multi-scale urban trajectory representations across semantic abstraction
levels. HiT-JEPA adopts a three-layer hierarchy that progressively captures
point-level fine-grained details, intermediate patterns, and high-level
trajectory abstractions, enabling the model to integrate both local dynamics
and global semantics in one coherent structure. Extensive experiments on
multiple real-world datasets for trajectory similarity computation show that
HiT-JEPA's hierarchical design yields richer, multi-scale representations. Code
is available at: https://anonymous.4open.science/r/HiT-JEPA.

</details>


### [116] [Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience](https://arxiv.org/abs/2507.00320)
*Christiana Westlin,Ashutosh Singh,Deniz Erdogmus,Georgios Stratis,Lisa Feldman Barrett*

Main category: cs.LG

TL;DR: 重新分析情绪研究数据，发现情绪类别内部存在显著个体差异，质疑传统情绪分类的生物学和心理学假设。


<details>
  <summary>Details</summary>
Motivation: 传统情绪科学假设情绪类别具有生物学和心理学上的分类学特征，但作者认为情绪类别是多样化的实例集合，需要验证这一假设。

Method: 重新分析了一项基于分类学假设的研究数据，采用最小假设的方法分析脑模式变异。

Result: 未观察到原始研究中的情绪类别映射，而是发现情绪类别内部存在显著个体差异。

Conclusion: 研究假设和分析方法会显著影响科学结论，需通过多种分析方法验证假设。

Abstract: In the science of emotion, it is widely assumed that folk emotion categories
form a biological and psychological typology, and studies are routinely
designed and analyzed to identify emotion-specific patterns. This approach
shapes the observations that studies report, ultimately reinforcing the
assumption that guided the investigation. Here, we reanalyzed data from one
such typologically-guided study that reported mappings between individual brain
patterns and group-averaged ratings of 34 emotion categories. Our reanalysis
was guided by an alternative view of emotion categories as populations of
variable, situated instances, and which predicts a priori that there will be
significant variation in brain patterns within a category across instances.
Correspondingly, our analysis made minimal assumptions about the structure of
the variance present in the data. As predicted, we did not observe the original
mappings and instead observed significant variation across individuals. These
findings demonstrate how starting assumptions can ultimately impact scientific
conclusions and suggest that a hypothesis must be supported using multiple
analytic methods before it is taken seriously.

</details>


### [117] [GANs Secretly Perform Approximate Bayesian Model Selection](https://arxiv.org/abs/2507.00651)
*Maurizio Filippone,Marius P. Linhard*

Main category: cs.LG

TL;DR: GANs的成功与局限性通过概率生成模型解释，提出优化策略以提升性能。


<details>
  <summary>Details</summary>
Motivation: 解释GANs的成功与局限性，并探索其作为概率生成模型的潜力。

Method: 将GANs视为部分随机性的贝叶斯神经网络，优化边际似然代理。

Result: 实验表明提出的策略能提升性能，并有助于理解GANs的正则化策略。

Conclusion: 通过概率视角和优化策略，GANs的性能和理解得到提升。

Abstract: Generative Adversarial Networks (GANs) are popular and successful generative
models. Despite their success, optimization is notoriously challenging and they
require regularization against overfitting. In this work, we explain the
success and limitations of GANs by interpreting them as probabilistic
generative models. This interpretation enables us to view GANs as Bayesian
neural networks with partial stochasticity, allowing us to establish conditions
of universal approximation. We can then cast the adversarial-style optimization
of several variants of GANs as the optimization of a proxy for the marginal
likelihood. Taking advantage of the connection between marginal likelihood
optimization and Occam's razor, we can define regularization and optimization
strategies to smooth the loss landscape and search for solutions with minimum
description length, which are associated with flat minima and good
generalization. The results on a wide range of experiments indicate that these
strategies lead to performance improvements and pave the way to a deeper
understanding of regularization strategies for GANs.

</details>


### [118] [Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding](https://arxiv.org/abs/2507.00669)
*Duc Cao-Dinh,Khai Le-Duc,Anh Dao,Bach Phan Tat,Chris Ngo,Duy M. H. Nguyen,Nguyen X. Khanh,Thanh Nguyen-Tang*

Main category: cs.LG

TL;DR: Audio-3DVG框架通过分解任务为对象提及检测和音频引导注意力模块，提升了3D视觉定位性能。


<details>
  <summary>Details</summary>
Motivation: 利用语音识别和表示学习的进展，探索基于音频的3D视觉定位，弥补现有文本方法的不足。

Method: 提出Object Mention Detection和Audio-Guided Attention模块，结合音频与空间信息。

Result: 在ScanRefer、Sr3D和Nr3D数据集上达到新SOTA，性能媲美文本方法。

Conclusion: Audio-3DVG展示了语音与3D视觉任务结合的潜力。

Abstract: 3D Visual Grounding (3DVG) involves localizing target objects in 3D point
clouds based on natural language. While prior work has made strides using
textual descriptions, leveraging spoken language-known as Audio-based 3D Visual
Grounding-remains underexplored and challenging. Motivated by advances in
automatic speech recognition (ASR) and speech representation learning, we
propose Audio-3DVG, a simple yet effective framework that integrates audio and
spatial information for enhanced grounding. Rather than treating speech as a
monolithic input, we decompose the task into two complementary components.
First, we introduce Object Mention Detection, a multi-label classification task
that explicitly identifies which objects are referred to in the audio, enabling
more structured audio-scene reasoning. Second, we propose an Audio-Guided
Attention module that captures interactions between candidate objects and
relational speech cues, improving target discrimination in cluttered scenes. To
support benchmarking, we synthesize audio descriptions for standard 3DVG
datasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate
that Audio-3DVG not only achieves new state-of-the-art performance in
audio-based grounding, but also competes with text-based methods-highlighting
the promise of integrating spoken language into 3D vision tasks.

</details>


### [119] [Diffusion Classifier Guidance for Non-robust Classifiers](https://arxiv.org/abs/2507.00687)
*Philipp Vaeth,Dibyanshu Kumar,Benjamin Paassen,Magda Gregorová*

Main category: cs.LG

TL;DR: 论文提出了一种方法，使非鲁棒分类器也能用于扩散过程的分类器引导，解决了噪声条件下分类器性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分类器引导方法通常需要鲁棒分类器，限制了其应用范围。本文旨在扩展分类器引导的适用范围，使其适用于非鲁棒分类器。

Method: 通过一步去噪图像预测和随机优化方法（如指数移动平均）来稳定非鲁棒分类器的引导梯度。

Result: 实验表明，该方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。

Conclusion: 本文扩展了生成模型中条件采样技术的应用范围，使更多分类器可用于引导。

Abstract: Classifier guidance is intended to steer a diffusion process such that a
given classifier reliably recognizes the generated data point as a certain
class. However, most classifier guidance approaches are restricted to robust
classifiers, which were specifically trained on the noise of the diffusion
forward process. We extend classifier guidance to work with general,
non-robust, classifiers that were trained without noise. We analyze the
sensitivity of both non-robust and robust classifiers to noise of the diffusion
process on the standard CelebA data set, the specialized SportBalls data set
and the high-dimensional real-world CelebA-HQ data set. Our findings reveal
that non-robust classifiers exhibit significant accuracy degradation under
noisy conditions, leading to unstable guidance gradients. To mitigate these
issues, we propose a method that utilizes one-step denoised image predictions
and implements stabilization techniques inspired by stochastic optimization
methods, such as exponential moving averages. Experimental results demonstrate
that our approach improves the stability of classifier guidance while
maintaining sample diversity and visual quality. This work contributes to
advancing conditional sampling techniques in generative models, enabling a
broader range of classifiers to be used as guidance classifiers.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [120] [BadViM: Backdoor Attack against Vision Mamba](https://arxiv.org/abs/2507.00577)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: BadViM是一种针对Vision Mamba（ViM）的新型后门攻击框架，利用共振频率触发器和隐藏状态对齐损失，成功率高且对常见防御措施具有强韧性。


<details>
  <summary>Details</summary>
Motivation: 研究Vision Mamba（ViM）在后门攻击中的脆弱性，填补这一新兴架构在安全性方面的研究空白。

Method: 提出BadViM框架，使用共振频率触发器（RFT）和隐藏状态对齐损失（HSA）来实施攻击。

Result: 实验表明，BadViM攻击成功率高，且能抵抗PatchDrop、PatchShuffle和JPEG压缩等防御措施。

Conclusion: Vision Mamba对后门攻击具有显著脆弱性，BadViM为评估其安全性提供了重要工具。

Abstract: Vision State Space Models (SSMs), particularly architectures like Vision
Mamba (ViM), have emerged as promising alternatives to Vision Transformers
(ViTs). However, the security implications of this novel architecture,
especially their vulnerability to backdoor attacks, remain critically
underexplored. Backdoor attacks aim to embed hidden triggers into victim
models, causing the model to misclassify inputs containing these triggers while
maintaining normal behavior on clean inputs. This paper investigates the
susceptibility of ViM to backdoor attacks by introducing BadViM, a novel
backdoor attack framework specifically designed for Vision Mamba. The proposed
BadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency
sensitivity patterns of the victim model to create stealthy, distributed
triggers. To maximize attack efficacy, we propose a Hidden State Alignment loss
that strategically manipulates the internal representations of model by
aligning the hidden states of backdoor images with those of target classes.
Extensive experimental results demonstrate that BadViM achieves superior attack
success rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits
remarkable resilience against common defensive measures, including PatchDrop,
PatchShuffle and JPEG compression, which typically neutralize normal backdoor
attacks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [121] [Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures](https://arxiv.org/abs/2507.00051)
*Tianliang Yao,Zhiqiang Pei,Yong Li,Yixuan Yuan,Peng Qi*

Main category: eess.IV

TL;DR: 论文提出了一种基于Siamese网络和双注意力机制的导丝尖端跟踪框架，用于心血管疾病的图像引导治疗，提高了跟踪精度和速度。


<details>
  <summary>Details</summary>
Motivation: 临床实践中AI解决方案的引入提升了医疗服务的效率和效果，但导丝尖端跟踪任务仍面临视觉模糊、组织变形和成像伪影等挑战。

Method: 采用Siamese网络结合自注意力和交叉注意力策略，增强时空特征学习，以应对跟踪中的复杂情况。

Result: 在临床DSA序列上验证，平均定位误差为0.421±0.138 mm，最大误差1.736 mm，平均IoU为0.782，处理速度为57.2帧/秒。

Conclusion: 该框架在精度和速度上均满足临床需求，未来可结合机器人平台进一步优化诊疗流程。

Abstract: An ever-growing incorporation of AI solutions into clinical practices
enhances the efficiency and effectiveness of healthcare services. This paper
focuses on guidewire tip tracking tasks during image-guided therapy for
cardiovascular diseases, aiding physicians in improving diagnostic and
therapeutic quality. A novel tracking framework based on a Siamese network with
dual attention mechanisms combines self- and cross-attention strategies for
robust guidewire tip tracking. This design handles visual ambiguities, tissue
deformations, and imaging artifacts through enhanced spatial-temporal feature
learning. Validation occurred on 3 randomly selected clinical digital
subtraction angiography (DSA) sequences from a dataset of 15 sequences,
covering multiple interventional scenarios. The results indicate a mean
localization error of 0.421 $\pm$ 0.138 mm, with a maximum error of 1.736 mm,
and a mean Intersection over Union (IoU) of 0.782. The framework maintains an
average processing speed of 57.2 frames per second, meeting the temporal
demands of endovascular imaging. Further validations with robotic platforms for
automating diagnostics and therapies in clinical routines yielded tracking
errors of 0.708 $\pm$ 0.695 mm and 0.148 $\pm$ 0.057 mm in two distinct
experimental scenarios.

</details>


### [122] [Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)](https://arxiv.org/abs/2507.00185)
*Yang Zhou,Chrystie Wan Ning Quek,Jun Zhou,Yan Wang,Yang Bai,Yuhe Ke,Jie Yao,Laura Gutierrez,Zhen Ling Teo,Darren Shu Jeng Ting,Brian T. Soetikno,Christopher S. Nielsen,Tobias Elze,Zengxiang Li,Linh Le Dinh,Lionel Tim-Ee Cheng,Tran Nguyen Tuan Anh,Chee Leong Cheng,Tien Yin Wong,Nan Liu,Iain Beehuat Tan,Tony Kiat Hon Lim,Rick Siow Mong Goh,Yong Liu,Daniel Shu Wei Ting*

Main category: eess.IV

TL;DR: MerMED-FM是一种多模态、多专科的基础模型，通过自监督学习和记忆模块训练，在多种医学影像任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像AI模型多为单模态、单疾病，且多模态、多疾病模型临床准确性不一致，训练需要大量标注数据。

Method: 使用自监督学习和记忆模块，训练于3.3百万张医学影像，涵盖10个专科和7种模态。

Result: 在多种疾病和模态中表现优异，AUROC值在0.858至0.988之间。

Conclusion: MerMED-FM是一种高度适应性强、多功能的跨专科基础模型，适用于多样化的医学影像解读。

Abstract: Current artificial intelligence models for medical imaging are predominantly
single modality and single disease. Attempts to create multimodal and
multi-disease models have resulted in inconsistent clinical accuracy.
Furthermore, training these models typically requires large, labour-intensive,
well-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,
multi-specialty foundation model trained using self-supervised learning and a
memory module. MerMED-FM was trained on 3.3 million medical images from over
ten specialties and seven modalities, including computed tomography (CT), chest
X-rays (CXR), ultrasound (US), pathology patches, color fundus photography
(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was
evaluated across multiple diseases and compared against existing foundational
models. Strong performance was achieved across all modalities, with AUROCs of
0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894
(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,
versatile, cross-specialty foundation model that enables robust medical imaging
interpretation across diverse medical disciplines.

</details>


### [123] [Towards 3D Semantic Image Synthesis for Medical Imaging](https://arxiv.org/abs/2507.00206)
*Wenwu Tang,Khaled Seyam,Bin Yang*

Main category: eess.IV

TL;DR: 论文提出了一种名为Med-LSDM的3D语义扩散模型，用于生成合成医学图像以解决数据隐私和可用性问题。


<details>
  <summary>Details</summary>
Motivation: 医学领域数据获取困难且隐私保护严格，限制了机器学习在医学影像中的应用。

Method: Med-LSDM直接在3D域操作，利用去标识的语义地图生成合成数据，结合扩散模型和预训练的VQ-GAN潜在空间。

Result: 在Duke Breast数据集上，3D-FID得分为0.0054，Dice分数接近真实图像（0.70964 vs 0.71496）。

Conclusion: Med-LSDM生成的合成数据与真实数据差距小，适用于数据增强。

Abstract: In the medical domain, acquiring large datasets is challenging due to both
accessibility issues and stringent privacy regulations. Consequently, data
availability and privacy protection are major obstacles to applying machine
learning in medical imaging. To address this, our study proposes the Med-LSDM
(Latent Semantic Diffusion Model), which operates directly in the 3D domain and
leverages de-identified semantic maps to generate synthetic data as a method of
privacy preservation and data augmentation. Unlike many existing methods that
focus on generating 2D slices, Med-LSDM is designed specifically for 3D
semantic image synthesis, making it well-suited for applications requiring full
volumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D
image generation process by applying a diffusion model within the latent space
of a pre-trained VQ-GAN. By operating in the compressed latent space, the model
significantly reduces computational complexity while still preserving critical
3D spatial details. Our approach demonstrates strong performance in 3D semantic
medical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional
Duke Breast dataset and similar Dice scores (0.70964) to those of real images
(0.71496). These results demonstrate that the synthetic data from our model
have a small domain gap with real data and are useful for data augmentation.

</details>


### [124] [SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures](https://arxiv.org/abs/2507.00209)
*Fengyi Jiang,Xiaorui Zhang,Lingbo Jin,Ruixing Liang,Yuxin Chen,Adi Chola Venkatesh,Jason Culman,Tiantian Wu,Lirong Shao,Wenqing Sun,Cong Gao,Hallie McNamara,Jingpei Lu,Omid Mohareri*

Main category: eess.IV

TL;DR: SurgiSR4K是首个公开的4K分辨率手术影像数据集，专为机器人辅助微创手术设计，涵盖多种视觉场景，支持多种计算机视觉任务。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏公开的4K分辨率手术影像数据集，限制了高分辨率手术影像研究的进展。

Method: 通过采集机器人辅助手术中的真实4K影像，构建了包含多种挑战性场景的数据集。

Result: SurgiSR4K数据集为高分辨率手术影像研究提供了基础，支持多种计算机视觉任务。

Conclusion: 该数据集推动了高分辨率手术影像研究，有望提升机器人辅助手术的性能和安全性。

Abstract: High-resolution imaging is crucial for enhancing visual clarity and enabling
precise computer-assisted guidance in minimally invasive surgery (MIS). Despite
the increasing adoption of 4K endoscopic systems, there remains a significant
gap in publicly available native 4K datasets tailored specifically for
robotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible
surgical imaging and video dataset captured at a native 4K resolution,
representing realistic conditions of robotic-assisted procedures. SurgiSR4K
comprises diverse visual scenarios including specular reflections, tool
occlusions, bleeding, and soft tissue deformations, meticulously designed to
reflect common challenges faced during laparoscopic and robotic surgeries. This
dataset opens up possibilities for a broad range of computer vision tasks that
might benefit from high resolution data, such as super resolution (SR), smoke
removal, surgical instrument detection, 3D tissue reconstruction, monocular
depth estimation, instance segmentation, novel view synthesis, and
vision-language model (VLM) development. SurgiSR4K provides a robust foundation
for advancing research in high-resolution surgical imaging and fosters the
development of intelligent imaging technologies aimed at enhancing performance,
safety, and usability in image-guided robotic surgeries.

</details>


### [125] [Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound](https://arxiv.org/abs/2507.00398)
*Jian Wang,Qiongying Ni,Hongkui Yu,Ruixuan Yao,Jinqiao Ying,Bin Zhang,Xingyi Yang,Jin Peng,Jiongquan Chen,Junxuan Yu,Wenlong Shi,Chaoyu Chen,Zhongnuo Yan,Mingyuan Luo,Gaocheng Cai,Dong Ni,Jing Lu,Xin Yang*

Main category: eess.IV

TL;DR: 提出了一种基于3D胎儿超声体积直接估计胎儿出生体重的方法，结合多尺度特征融合网络和合成样本学习框架，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有临床方法效率低且依赖操作者，而基于2D超声的深度学习方法缺乏空间信息，限制了准确性。

Method: 采用多尺度特征融合网络（MFFN）和合成样本学习框架（SSLF），结合通道和空间注意力机制以及半监督学习。

Result: 平均绝对误差为166.4±155.9克，平均绝对百分比误差为5.1±4.6%，优于现有方法。

Conclusion: 该方法在胎儿出生体重估计中表现出色，接近资深医生的准确性。

Abstract: Accurate fetal birth weight (FBW) estimation is essential for optimizing
delivery decisions and reducing perinatal mortality. However, clinical methods
for FBW estimation are inefficient, operator-dependent, and challenging to
apply in cases of complex fetal anatomy. Existing deep learning methods are
based on 2D standard ultrasound (US) images or videos that lack spatial
information, limiting their prediction accuracy. In this study, we propose the
first method for directly estimating FBW from 3D fetal US volumes. Our approach
integrates a multi-scale feature fusion network (MFFN) and a synthetic
sample-based learning framework (SSLF). The MFFN effectively extracts and fuses
multi-scale features under sparse supervision by incorporating channel
attention, spatial attention, and a ranking-based loss function. SSLF generates
synthetic samples by simply combining fetal head and abdomen data from
different fetuses, utilizing semi-supervised learning to improve prediction
performance. Experimental results demonstrate that our method achieves superior
performance, with a mean absolute error of $166.4\pm155.9$ $g$ and a mean
absolute percentage error of $5.1\pm4.6$%, outperforming existing methods and
approaching the accuracy of a senior doctor. Code is available at:
https://github.com/Qioy-i/EFW.

</details>


### [126] [Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+](https://arxiv.org/abs/2507.00511)
*Sayandeep Kanrar,Raja Piyush,Qaiser Razi,Debanshi Chakraborty,Vikas Hassija,GSS Chalapathi*

Main category: eess.IV

TL;DR: VMSE U-Net和VM-Unet CBAM+模型通过集成SE和CBAM技术，显著提升了医学图像分割的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 提升传统VM U-Net在医学图像分割中的性能，包括精度、特征定位和计算效率。

Method: 在VM U-Net框架中集成Squeeze-and-Excitation (SE)和Convolutional Block Attention Module (CBAM)技术。

Result: VMSE-Unet在多个数据集上表现最优，精度、IoU、召回率等指标均优于基线模型，且计算效率更高。

Conclusion: VMSE-Unet是医学图像分析的有力工具，具有实际临床应用潜力，未来需进一步优化其性能和效率。

Abstract: In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two
cutting-edge deep learning architectures designed to enhance medical image
segmentation. Our approach integrates Squeeze-and-Excitation (SE) and
Convolutional Block Attention Module (CBAM) techniques into the traditional VM
U-Net framework, significantly improving segmentation accuracy, feature
localization, and computational efficiency. Both models show superior
performance compared to the baseline VM-Unet across multiple datasets. Notably,
VMSEUnet achieves the highest accuracy, IoU, precision, and recall while
maintaining low loss values. It also exhibits exceptional computational
efficiency with faster inference times and lower memory usage on both GPU and
CPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a
valuable tool for medical image analysis. These findings highlight its
potential for real-world clinical applications, emphasizing the importance of
further research to optimize accuracy, robustness, and computational
efficiency.

</details>


### [127] [Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models](https://arxiv.org/abs/2507.00582)
*Yi Zhang,Yidong Zhao,Qian Tao*

Main category: eess.IV

TL;DR: DEQReg提出了一种基于深度平衡模型的医学图像配准框架，解决了传统展开方法的内存和收敛问题。


<details>
  <summary>Details</summary>
Motivation: 传统展开方法缺乏理论收敛保证且内存消耗大，DEQReg旨在解决这些问题。

Method: 使用深度平衡模型（DEQ）将配准问题转化为平衡寻找问题，保持恒定内存使用。

Result: 在脑MRI和肺CT数据集上表现优异，内存消耗显著降低，且收敛稳定。

Conclusion: DEQReg在理论和实践上优于现有展开方法，弥合了传统优化与学习方法的差距。

Abstract: Deformable medical image registration is traditionally formulated as an
optimization problem. While classical methods solve this problem iteratively,
recent learning-based approaches use recurrent neural networks (RNNs) to mimic
this process by unrolling the prediction of deformation fields in a fixed
number of steps. However, classical methods typically converge after sufficient
iterations, but learning-based unrolling methods lack a theoretical convergence
guarantee and show instability empirically. In addition, unrolling methods have
a practical bottleneck at training time: GPU memory usage grows linearly with
the unrolling steps due to backpropagation through time (BPTT). To address both
theoretical and practical challenges, we propose DEQReg, a novel registration
framework based on Deep Equilibrium Models (DEQ), which formulates registration
as an equilibrium-seeking problem, establishing a natural connection between
classical optimization and learning-based unrolling methods. DEQReg maintains
constant memory usage, enabling theoretically unlimited iteration steps.
Through extensive evaluation on the public brain MRI and lung CT datasets, we
show that DEQReg can achieve competitive registration performance, while
substantially reducing memory consumption compared to state-of-the-art
unrolling methods. We also reveal an intriguing phenomenon: the performance of
existing unrolling methods first increases slightly then degrades irreversibly
when the inference steps go beyond the training configuration. In contrast,
DEQReg achieves stable convergence with its inbuilt equilibrium-seeking
mechanism, bridging the gap between classical optimization-based and modern
learning-based registration methods.

</details>


### [128] [MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound](https://arxiv.org/abs/2507.00660)
*Rusi Chen,Yuanting Yang,Jiezhi Yao,Hongning Song,Ji Zhang,Yongsong Zhou,Yuhao Huang,Ronghao Yang,Dan Jia,Yuhan Zhang,Xing Tao,Haoran Dou,Qing Zhou,Xin Yang,Dong Ni*

Main category: eess.IV

TL;DR: 提出了一种基于运动-拓扑引导的一致性网络（MTCNet），用于半监督学习中的4D二尖瓣超声分割，解决了现有方法中缺乏相位间依赖性的问题。


<details>
  <summary>Details</summary>
Motivation: 4D二尖瓣分析因相位标注有限、运动伪影严重和成像质量差而具有挑战性，现有方法缺乏相位间依赖性。

Method: 设计了跨相位运动引导一致性学习策略和拓扑引导相关性正则化，利用双向注意力记忆库传播时空特征。

Result: 在包含160名患者1408个相位的数据集上，MTCNet表现出色（Dice: 87.30%, HD: 1.75mm）。

Conclusion: MTCNet通过运动-拓扑引导策略有效提升了4D二尖瓣分割的准确性和一致性。

Abstract: Mitral regurgitation is one of the most prevalent cardiac disorders.
Four-dimensional (4D) ultrasound has emerged as the primary imaging modality
for assessing dynamic valvular morphology. However, 4D mitral valve (MV)
analysis remains challenging due to limited phase annotations, severe motion
artifacts, and poor imaging quality. Yet, the absence of inter-phase dependency
in existing methods hinders 4D MV analysis. To bridge this gap, we propose a
Motion-Topology guided consistency network (MTCNet) for accurate 4D MV
ultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only
sparse end-diastolic and end-systolic annotations. First, we design a
cross-phase motion-guided consistency learning strategy, utilizing a
bi-directional attention memory bank to propagate spatio-temporal features.
This enables MTCNet to achieve excellent performance both per- and inter-phase.
Second, we devise a novel topology-guided correlation regularization that
explores physical prior knowledge to maintain anatomically plausible.
Therefore, MTCNet can effectively leverage structural correspondence between
labeled and unlabeled phases. Extensive evaluations on the first largest 4D MV
dataset, with 1408 phases from 160 patients, show that MTCNet performs superior
cross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:
1.75mm). Both the code and the dataset are available at
https://github.com/crs524/MTCNet.

</details>


### [129] [Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions](https://arxiv.org/abs/2507.00670)
*Jan Nikolas Morshuis,Christian Schlarmann,Thomas Küstner,Christian F. Baumgartner,Matthias Hein*

Main category: eess.IV

TL;DR: 论文提出了一种名为“语义多样性重建”（SDR）的方法，用于解决现有深度学习MRI重建技术可能遗漏小病灶的问题，从而提高诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有技术在高加速因子下重建MRI图像时，可能遗漏小且罕见的病理信息，导致假阴性诊断。

Method: 提出SDR方法，通过生成语义多样性的重建图像，确保所有重建结果与测量数据一致，同时增强病理信息的可见性。

Result: SDR显著降低了假阴性诊断的概率（更高的召回率），并提高了平均精度。

Conclusion: SDR方法在保留临床相关信息方面优于现有技术，有助于提高诊断准确性。

Abstract: In recent years, accelerated MRI reconstruction based on deep learning has
led to significant improvements in image quality with impressive results for
high acceleration factors. However, from a clinical perspective image quality
is only secondary; much more important is that all clinically relevant
information is preserved in the reconstruction from heavily undersampled data.
In this paper, we show that existing techniques, even when considering
resampling for diffusion-based reconstruction, can fail to reconstruct small
and rare pathologies, thus leading to potentially wrong diagnosis decisions
(false negatives). To uncover the potentially missing clinical information we
propose ``Semantically Diverse Reconstructions'' (\SDR), a method which, given
an original reconstruction, generates novel reconstructions with enhanced
semantic variability while all of them are fully consistent with the measured
data. To evaluate \SDR automatically we train an object detector on the
fastMRI+ dataset. We show that \SDR significantly reduces the chance of
false-negative diagnoses (higher recall) and improves mean average precision
compared to the original reconstructions. The code is available on
https://github.com/NikolasMorshuis/SDR

</details>


### [130] [Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays](https://arxiv.org/abs/2507.00673)
*Abduz Zami,Shadman Sobhan,Rounaq Hossain,Md. Sawran Sorker,Mohiuddin Ahmed,Md. Redwan Hossain*

Main category: eess.IV

TL;DR: 提出了一种基于提示的交互式多器官和多疾病分割方法，专注于胸部X光片，并引入了一个轻量级模型Prompt2SegCXR。


<details>
  <summary>Details</summary>
Motivation: 传统分割模型局限于特定器官或疾病，缺乏灵活性。目前缺乏针对胸部X光片的基于提示的多器官和多疾病分割方法。

Method: 生成医学专家绘制的涂鸦提示数据集，包含23个类别（6个器官和17种疾病），并开发了轻量级模型Prompt2SegCXR，采用多阶段特征融合技术。

Result: 模型在基于提示的胸部X光片分割中表现优异，优于现有预训练模型。

Conclusion: Prompt2SegCXR为胸部X光片的多器官和多疾病分割提供了可靠且灵活的解决方案。

Abstract: Image segmentation plays a vital role in the medical field by isolating
organs or regions of interest from surrounding areas. Traditionally,
segmentation models are trained on a specific organ or a disease, limiting
their ability to handle other organs and diseases. At present, few advanced
models can perform multi-organ or multi-disease segmentation, offering greater
flexibility. Also, recently, prompt-based image segmentation has gained
attention as a more flexible approach. It allows models to segment areas based
on user-provided prompts. Despite these advances, there has been no dedicated
work on prompt-based interactive multi-organ and multi-disease segmentation,
especially for Chest X-rays. This work presents two main contributions: first,
generating doodle prompts by medical experts of a collection of datasets from
multiple sources with 23 classes, including 6 organs and 17 diseases,
specifically designed for prompt-based Chest X-ray segmentation. Second, we
introduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple
organs and diseases from Chest X-rays. The model incorporates multi-stage
feature fusion, enabling it to combine features from various network layers for
better spatial and semantic understanding, enhancing segmentation accuracy.
Compared to existing pre-trained models for prompt-based image segmentation,
our model scores well, providing a reliable solution for segmenting Chest
X-rays based on user prompts.

</details>


### [131] [Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery](https://arxiv.org/abs/2507.00743)
*An Le,Nehal Mehta,William Freeman,Ines Nagel,Melanie Tran,Anna Heinke,Akshay Agnihotri,Lingyun Cheng,Dirk-Uwe Bartsch,Hung Nguyen,Truong Nguyen,Cheolhong An*

Main category: eess.IV

TL;DR: 使用ResNet18 CNN和可调小波单元分类ERM手术类型，准确率最高达78%，优于人类评分者。


<details>
  <summary>Details</summary>
Motivation: 提高术后OCT扫描中ERM手术类型分类的准确性，辅助临床决策。

Method: 基于ResNet18 CNN，结合OrthLatt-UwU和PR-Relax-UwU可调小波单元，优化输入预处理。

Result: 预处理后准确率72%，可调小波单元提升至76%-78%，优于人类50%的准确率。

Conclusion: CNN模型结合可调小波单元能显著提升分类性能，具有临床应用潜力。

Abstract: In this study, we developed deep learning-based method to classify the type
of surgery performed for epiretinal membrane (ERM) removal, either internal
limiting membrane (ILM) removal or ERM-alone removal. Our model, based on the
ResNet18 convolutional neural network (CNN) architecture, utilizes
postoperative optical coherence tomography (OCT) center scans as inputs. We
evaluated the model using both original scans and scans preprocessed with
energy crop and wavelet denoising, achieving 72% accuracy on preprocessed
inputs, outperforming the 66% accuracy achieved on original scans. To further
improve accuracy, we integrated tunable wavelet units with two key adaptations:
Orthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect
Reconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units
allowed the model to automatically adjust filter coefficients during training
and were incorporated into downsampling, stride-two convolution, and pooling
layers, enhancing its ability to distinguish between ERM-ILM removal and
ERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU
increasing performance to 78%. Performance comparisons showed that our AI model
outperformed a trained human grader, who achieved only 50% accuracy in
classifying the removal surgery types from postoperative OCT scans. These
findings highlight the potential of CNN based models to improve clinical
decision-making by providing more accurate and reliable classifications. To the
best of our knowledge, this is the first work to employ tunable wavelets for
classifying different types of ERM removal surgery.

</details>


### [132] [Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n](https://arxiv.org/abs/2507.00780)
*Fei Yuhuan,Sun Xufei,Zang Ran,Wang Gengchen,Su Meng,Liu Fenghao*

Main category: eess.IV

TL;DR: 提出了一种基于改进YOLOv8n的轻量级高精度检测模型YOLO-KFG，用于糖尿病视网膜病变的早期检测，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变的微病变特征细微且易受背景干扰，现有检测方法在准确性和鲁棒性方面面临挑战。

Method: 设计了动态卷积KWConv和C2f-KW模块改进主干网络，提出特征聚焦扩散金字塔网络FDPN整合多尺度信息，并设计轻量级共享检测头GSDHead减少参数量。

Result: 改进模型参数量减少20.7%，mAP@0.5提升4.1%，召回率提高7.9%，优于主流单阶段算法。

Conclusion: YOLO-KFG在检测精度和效率上具有显著优势，适用于资源受限设备。

Abstract: Early detection and diagnosis of diabetic retinopathy is one of the current
research focuses in ophthalmology. However, due to the subtle features of
micro-lesions and their susceptibility to background interference, ex-isting
detection methods still face many challenges in terms of accuracy and
robustness. To address these issues, a lightweight and high-precision detection
model based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a
new dynamic convolution KWConv and C2f-KW module are designed to improve the
backbone network, enhancing the model's ability to perceive micro-lesions.
Secondly, a fea-ture-focused diffusion pyramid network FDPN is designed to
fully integrate multi-scale context information, further improving the model's
ability to perceive micro-lesions. Finally, a lightweight shared detection head
GSDHead is designed to reduce the model's parameter count, making it more
deployable on re-source-constrained devices. Experimental results show that
compared with the base model YOLOv8n, the improved model reduces the parameter
count by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by
7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and
YOLOv10n, YOLO-KFG demonstrates significant advantages in both detection
accuracy and efficiency.

</details>


### [133] [Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection](https://arxiv.org/abs/2507.00832)
*Jisoo Kim,Chu-Hsuan Lin,Alberto Ceballos-Arroyo,Ping Liu,Huaizu Jiang,Shrikanth Yadav,Qi Wan,Lei Qin,Geoffrey S Young*

Main category: eess.IV

TL;DR: 通过结合解剖学知识的后处理方法，显著降低了深度学习模型在颅内动脉瘤检测中的假阳性率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习模型在颅内动脉瘤检测中表现良好，但高假阳性率阻碍了其临床转化。研究旨在通过解剖学知识的后处理方法进一步减少假阳性。

Method: 使用两种深度学习模型（CPM-Net和3D-CNN-TR）训练开源CTA数据，并应用基于解剖学的后处理方法（如脑、动脉、静脉分割掩码）去除假阳性。

Result: 最佳后处理方法（方法5）显著降低了假阳性率（CPM-Net降低70.6%，3D-CNN-TR降低51.6%），同时未减少真阳性。

Conclusion: 基于解剖学的后处理方法可提升深度学习模型的性能，为动脉瘤检测模型的临床接受度提供了新思路。

Abstract: Introduction: Deep learning (DL) models can help detect intracranial
aneurysms on CTA, but high false positive (FP) rates remain a barrier to
clinical translation, despite improvement in model architectures and strategies
like detection threshold tuning. We employed an automated, anatomy-based,
heuristic-learning hybrid artery-vein segmentation post-processing method to
further reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D
convolutional neural network-transformer hybrid (3D-CNN-TR), were trained with
1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143
held-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and
cavernous venous sinus (CVS) segmentation masks were applied to remove possible
FPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)
vein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more
than artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79
false-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were
commonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;
3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular
(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing
CPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without
reducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from
1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable
post-processing can improve DL-based aneurysm detection model performance. More
broadly, automated, domain-informed, hybrid heuristic-learning processing holds
promise for improving the performance and clinical acceptance of aneurysm
detection models.

</details>


### [134] [Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection](https://arxiv.org/abs/2507.00903)
*Andreea Bianca Popescu,Andreas Seitz,Heiko Mahrholdt,Jens Wetzl,Athira Jacob,Lucian Mihai Itu,Constantin Suciu,Teodora Chitiboi*

Main category: eess.IV

TL;DR: 深度学习（DL）在心脏组织分割中达到与观察者间变异性相当的准确性，结合多特征的机器学习（ML）提高了疾病检测能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖平均松弛值和单一阈值，可能简化心肌复杂性，且手动分割存在观察者间变异性。

Method: 使用DL模型分割左心室血池和心肌，计算心肌像素的统计特征（如平均值、四分位数），并通过ML进行分类。

Result: DL分割的DICE系数为85.4%，优于观察者间一致性；随机森林结合多特征使F1-score提升至92.7%。

Conclusion: DL能有效分割T1/T2图，结合多特征的ML可显著提升疾病检测性能。

Abstract: Objectives Parametric tissue mapping enables quantitative cardiac tissue
characterization but is limited by inter-observer variability during manual
delineation. Traditional approaches relying on average relaxation values and
single cutoffs may oversimplify myocardial complexity. This study evaluates
whether deep learning (DL) can achieve segmentation accuracy comparable to
inter-observer variability, explores the utility of statistical features beyond
mean T1/T2 values, and assesses whether machine learning (ML) combining
multiple features enhances disease detection. Materials & Methods T1 and T2
maps were manually segmented. The test subset was independently annotated by
two observers, and inter-observer variability was assessed. A DL model was
trained to segment left ventricle blood pool and myocardium. Average (A), lower
quartile (LQ), median (M), and upper quartile (UQ) were computed for the
myocardial pixels and employed in classification by applying cutoffs or in ML.
Dice similarity coefficient (DICE) and mean absolute percentage error evaluated
segmentation performance. Bland-Altman plots assessed inter-user and
model-observer agreement. Receiver operating characteristic analysis determined
optimal cutoffs. Pearson correlation compared features from model and manual
segmentations. F1-score, precision, and recall evaluated classification
performance. Wilcoxon test assessed differences between classification methods,
with p < 0.05 considered statistically significant. Results 144 subjects were
split into training (100), validation (15) and evaluation (29) subsets.
Segmentation model achieved a DICE of 85.4%, surpassing inter-observer
agreement. Random forest applied to all features increased F1-score (92.7%, p <
0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining
multiple features with ML improves disease detection.

</details>


### [135] [DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images](https://arxiv.org/abs/2507.00983)
*Sara Yavari,Rahul Nitin Pandya,Jacob Furst*

Main category: eess.IV

TL;DR: 论文提出了一种基于扩散模型的脑肿瘤分割方法DMCIE，通过结合初始分割误差图和多模态MRI数据，显著提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的准确分割对临床诊断和治疗至关重要，扩散模型在图像生成和分割任务中表现出色。

Method: 使用3D U-Net生成初始分割掩码，生成误差图后与原始MRI数据结合，指导扩散模型优化分割。

Result: 在BraTS2020数据集上，DMCIE的Dice Score达到93.46，HD95为5.94 mm，优于现有方法。

Conclusion: DMCIE通过误差引导扩散模型，实现了高精度和可靠的脑肿瘤分割。

Abstract: Accurate segmentation of brain tumors in MRI scans is essential for reliable
clinical diagnosis and effective treatment planning. Recently, diffusion models
have demonstrated remarkable effectiveness in image generation and segmentation
tasks. This paper introduces a novel approach to corrective segmentation based
on diffusion models. We propose DMCIE (Diffusion Model with Concatenation of
Inputs and Errors), a novel framework for accurate brain tumor segmentation in
multi-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation
mask, from which an error map is generated by identifying the differences
between the prediction and the ground truth. The error map, concatenated with
the original MRI images, are used to guide a diffusion model. Using multimodal
MRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation
accuracy by focusing on misclassified regions, guided by the original inputs.
Evaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art
diffusion-based segmentation methods, achieving a Dice Score of 93.46 and an
HD95 of 5.94 mm. These results highlight the effectiveness of error-guided
diffusion in producing precise and reliable brain tumor segmentations.

</details>


### [136] [Advancing Lung Disease Diagnosis in 3D CT Scans](https://arxiv.org/abs/2507.00993)
*Qingqiu Li,Runtian Yuan,Junlin Hou,Jilan Xu,Yuejie Zhang,Rui Feng,Hao Chen*

Main category: eess.IV

TL;DR: 提出了一种简单有效的模型，用于提高胸部CT扫描中肺部疾病的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 通过分析3D CT扫描特征并去除非肺部区域，帮助模型专注于病变区域并降低计算成本。

Method: 采用ResNeSt50作为特征提取器，并使用加权交叉熵损失解决类别不平衡问题。

Result: 在Fair Disease Diagnosis Challenge验证集上，模型取得了0.80的Macro F1分数。

Conclusion: 该模型在区分不同肺部疾病方面表现出色。

Abstract: To enable more accurate diagnosis of lung disease in chest CT scans, we
propose a straightforward yet effective model. Firstly, we analyze the
characteristics of 3D CT scans and remove non-lung regions, which helps the
model focus on lesion-related areas and reduces computational cost. We adopt
ResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss
to mitigate class imbalance, especially for the underrepresented squamous cell
carcinoma category. Our model achieves a Macro F1 Score of 0.80 on the
validation set of the Fair Disease Diagnosis Challenge, demonstrating its
strong performance in distinguishing between different lung conditions.

</details>
