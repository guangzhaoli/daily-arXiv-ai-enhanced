<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 65]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.SD](#cs.SD) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.LG](#cs.LG) [Total: 4]
- [eess.IV](#eess.IV) [Total: 12]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [SemIRNet: A Semantic Irony Recognition Network for Multimodal Sarcasm Detection](https://arxiv.org/abs/2506.14791)
*Jingxuan Zhou,Yuehao Wu,Yibo Zhang,Yeyubei Zhang,Yunchong Liu,Bolin Huang,Chunhong Yuan*

Main category: cs.CV

TL;DR: 本文提出SemIRNet模型，通过引入ConceptNet知识库、设计跨模态语义相似性检测模块和对比学习损失函数，提升了多模态反讽检测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态反讽检测任务中图形隐含关联难以准确识别的问题。

Method: 1. 引入ConceptNet知识库增强常识推理能力；2. 设计词级和样本级跨模态语义相似性检测模块；3. 使用对比学习损失函数优化样本特征分布。

Result: 在公开数据集上，准确率和F1值分别提升1.64%和2.88%，达到88.87%和86.33%。

Conclusion: 知识融合和语义相似性检测对模型性能提升具有重要作用。

Abstract: Aiming at the problem of difficulty in accurately identifying graphical
implicit correlations in multimodal irony detection tasks, this paper proposes
a Semantic Irony Recognition Network (SemIRNet). The model contains three main
innovations: (1) The ConceptNet knowledge base is introduced for the first time
to acquire conceptual knowledge, which enhances the model's common-sense
reasoning ability; (2) Two cross-modal semantic similarity detection modules at
the word level and sample level are designed to model graphic-textual
correlations at different granularities; and (3) A contrastive learning loss
function is introduced to optimize the spatial distribution of the sample
features, which improves the separability of positive and negative samples.
Experiments on a publicly available multimodal irony detection benchmark
dataset show that the accuracy and F1 value of this model are improved by 1.64%
and 2.88% to 88.87% and 86.33%, respectively, compared with the existing
optimal methods. Further ablation experiments verify the important role of
knowledge fusion and semantic similarity detection in improving the model
performance.

</details>


### [2] [Argus Inspection: Do Multimodal Large Language Models Possess the Eye of Panoptes?](https://arxiv.org/abs/2506.14805)
*Yang Yao,Lingyu Li,Jiaxin Song,Chiyu Chen,Zhenqi He,Yixu Wang,Xin Wang,Tianle Gu,Jie Li,Yan Teng,Yingchun Wang*

Main category: cs.CV

TL;DR: 论文介绍了Argus Inspection多模态基准和Eye of Panoptes框架，用于评估MLLMs的视觉细粒度感知和常识因果推理能力，实验显示当前模型性能有限。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在视觉细粒度感知和常识因果推理方面的挑战。

Method: 提出Argus Inspection基准和Eye of Panoptes框架，结合Sigmoid度量进行更全面的评估。

Result: 实验显示主流MLLMs在视觉细粒度推理中的最高性能仅为0.46。

Conclusion: 研究为MLLMs的进一步改进提供了有价值的视角。

Abstract: As Multimodal Large Language Models (MLLMs) continue to evolve, their
cognitive and reasoning capabilities have seen remarkable progress. However,
challenges in visual fine-grained perception and commonsense causal inference
persist. This paper introduces Argus Inspection, a multimodal benchmark with
two levels of difficulty, emphasizing detailed visual recognition while
incorporating real-world commonsense understanding to evaluate causal reasoning
abilities. Expanding on it, we present the Eye of Panoptes framework, which
integrates a binary parametric Sigmoid metric with an indicator function,
enabling a more holistic evaluation of MLLMs' responses in opinion-based
reasoning tasks. Experiments conducted on 26 mainstream MLLMs reveal that the
highest performance in visual fine-grained reasoning reaches only 0.46,
highlighting considerable potential for enhancement. Our research offers
valuable perspectives for the continued refinement of MLLMs.

</details>


### [3] [A Hybrid ConvNeXt-EfficientNet AI Solution for Precise Falcon Disease Detection](https://arxiv.org/abs/2506.14816)
*Alavikunhu Panthakkan,Zubair Medammal,S M Anzar,Fatma Taher,Hussain Al-Ahmad*

Main category: cs.CV

TL;DR: 提出了一种结合ConvNeXt和EfficientNet的混合AI模型，用于准确分类猎鹰的疾病（正常、肝病和曲霉病），性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 猎鹰训练和狩猎中，健康监测至关重要，需要高精度方法确保猎鹰健康。

Method: 采用ConvNeXt和EfficientNet混合模型，通过大数据集训练和验证，关注准确率、精确率、召回率和F1分数。

Result: 混合模型在疾病检测中表现优于传统方法和单一模型。

Conclusion: 该模型为猎鹰疾病检测提供了高效工具，推动了AI在鸟类健康领域的应用。

Abstract: Falconry, a revered tradition involving the training and hunting with
falcons, requires meticulous health surveillance to ensure the health and
safety of these prized birds, particularly in hunting scenarios. This paper
presents an innovative method employing a hybrid of ConvNeXt and EfficientNet
AI models for the classification of falcon diseases. The study focuses on
accurately identifying three conditions: Normal, Liver Disease and
'Aspergillosis'. A substantial dataset was utilized for training and validating
the model, with an emphasis on key performance metrics such as accuracy,
precision, recall, and F1-score. Extensive testing and analysis have shown that
our concatenated AI model outperforms traditional diagnostic methods and
individual model architectures. The successful implementation of this hybrid AI
model marks a significant step forward in precise falcon disease detection and
paves the way for future developments in AI-powered avian healthcare solutions.

</details>


### [4] [ViLLa: A Neuro-Symbolic approach for Animal Monitoring](https://arxiv.org/abs/2506.14823)
*Harsha Koduri*

Main category: cs.CV

TL;DR: ViLLa是一个神经符号框架，用于可解释的动物监测，结合视觉检测、语言解析和符号推理。


<details>
  <summary>Details</summary>
Motivation: 自然环境中监测动物种群需要能够解释视觉数据和人类语言查询的系统。

Method: ViLLa包含三个核心模块：视觉检测、语言解析和符号推理层，通过逻辑推理回答问题。

Result: 系统在多种动物图像任务中表现良好，能够将视觉内容与结构化查询结合。

Conclusion: ViLLa通过模块化和透明性，提供了一种可解释的动物监测方法。

Abstract: Monitoring animal populations in natural environments requires systems that
can interpret both visual data and human language queries. This work introduces
ViLLa (Vision-Language-Logic Approach), a neuro-symbolic framework designed for
interpretable animal monitoring. ViLLa integrates three core components: a
visual detection module for identifying animals and their spatial locations in
images, a language parser for understanding natural language queries, and a
symbolic reasoning layer that applies logic-based inference to answer those
queries. Given an image and a question such as "How many dogs are in the
scene?" or "Where is the buffalo?", the system grounds visual detections into
symbolic facts and uses predefined rules to compute accurate answers related to
count, presence, and location. Unlike end-to-end black-box models, ViLLa
separates perception, understanding, and reasoning, offering modularity and
transparency. The system was evaluated on a range of animal imagery tasks and
demonstrates the ability to bridge visual content with structured,
human-interpretable queries.

</details>


### [5] [GraphGSOcc: Semantic and Geometric Graph Transformer for 3D Gaussian Splating-based Occupancy Prediction](https://arxiv.org/abs/2506.14825)
*Ke Song,Yunhe Wu,Chunchit Siu,Huiyuan Xiong*

Main category: cs.CV

TL;DR: 论文提出GraphGSOcc模型，通过双高斯图注意力机制解决3D语义占用预测中的特征聚合和边界模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法存在特征聚合忽略语义相关性和边界模糊问题，需改进。

Method: 采用几何和语义图Transformer，动态构建双图结构，结合多尺度图注意力框架。

Result: 在SurroundOcc数据集上mIoU达24.10%，GPU内存降至6.1GB，性能提升显著。

Conclusion: GraphGSOcc模型有效提升3D语义占用预测的精度和效率。

Abstract: Addressing the task of 3D semantic occupancy prediction for autonomous
driving, we tackle two key issues in existing 3D Gaussian Splating (3DGS)
methods: (1) unified feature aggregation neglecting semantic correlations among
similar categories and across regions, and (2) boundary ambiguities caused by
the lack of geometric constraints in MLP iterative optimization. We propose the
GraphGSOcc model, a novel framework that combines semantic and geometric graph
Transformer for 3D Gaussian Splating-based Occupancy Prediction. We propose the
Dual Gaussians Graph Attenntion, which dynamically constructs dual graph
structures: a geometric graph adaptively calculating KNN search radii based on
Gaussian poses, enabling large-scale Gaussians to aggregate features from
broader neighborhoods while compact Gaussians focus on local geometric
consistency; a semantic graph retaining top-M highly correlated nodes via
cosine similarity to explicitly encode semantic relationships within and across
instances. Coupled with the Multi-scale Graph Attention framework, fine-grained
attention at lower layers optimizes boundary details, while coarse-grained
attention at higher layers models object-level topology. Experiments on the
SurroundOcc dataset achieve an mIoU of 24.10%, reducing GPU memory to 6.1 GB,
demonstrating a 1.97% mIoU improvement and 13.7% memory reduction compared to
GaussianWorld

</details>


### [6] [DAVID-XR1: Detecting AI-Generated Videos with Explainable Reasoning](https://arxiv.org/abs/2506.14827)
*Yifeng Gao,Yifan Ding,Hongyu Su,Juncheng Li,Yunhan Zhao,Lin Luo,Zixing Chen,Li Wang,Xin Wang,Yixu Wang,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: DAVID-X和DAVID-XR1通过细粒度标注和可解释的视觉推理链，将AI生成视频检测从黑盒决策转变为透明可验证的诊断过程。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅将AI生成视频检测视为二分类任务，缺乏对检测结果的解释性，难以说服审计者和用户。

Method: 提出DAVID-X数据集，包含AI生成视频的缺陷级时空标注和书面解释；基于此开发DAVID-XR1模型，提供可解释的视觉推理链（缺陷分类、时空定位和自然语言解释）。

Result: 在紧凑数据集上微调的通用骨干模型，结合思维链蒸馏，展现出对多种生成器和生成模式的强泛化能力。

Conclusion: 可解释的检测方法为AI生成视频的可信识别提供了新方向。

Abstract: As AI-generated video becomes increasingly pervasive across media platforms,
the ability to reliably distinguish synthetic content from authentic footage
has become both urgent and essential. Existing approaches have primarily
treated this challenge as a binary classification task, offering limited
insight into where or why a model identifies a video as AI-generated. However,
the core challenge extends beyond simply detecting subtle artifacts; it
requires providing fine-grained, persuasive evidence that can convince auditors
and end-users alike. To address this critical gap, we introduce DAVID-X, the
first dataset to pair AI-generated videos with detailed defect-level,
temporal-spatial annotations and written rationales. Leveraging these rich
annotations, we present DAVID-XR1, a video-language model designed to deliver
an interpretable chain of visual reasoning-including defect categorization,
temporal-spatial localization, and natural language explanations. This approach
fundamentally transforms AI-generated video detection from an opaque black-box
decision into a transparent and verifiable diagnostic process. We demonstrate
that a general-purpose backbone, fine-tuned on our compact dataset and enhanced
with chain-of-thought distillation, achieves strong generalization across a
variety of generators and generation modes. Our results highlight the promise
of explainable detection methods for trustworthy identification of AI-generated
video content.

</details>


### [7] [Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review](https://arxiv.org/abs/2506.14831)
*Céline Finet,Stephane Da Silva Martins,Jean-Bernard Hayet,Ioannis Karamouzas,Javad Amirian,Sylvie Le Hégarat-Mascle,Julien Pettré,Emanuel Aldea*

Main category: cs.CV

TL;DR: 这篇综述回顾了2020至2024年间基于深度学习的多智能体轨迹预测的最新进展，重点分析了ETH/UCY基准测试中的模型，并探讨了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着数据驱动方法在人类轨迹预测中的发展，深入理解多智能体交互对自主导航和人群建模等领域具有重要意义。

Method: 根据架构设计、输入表示和预测策略对现有方法进行分类，特别关注ETH/UCY基准测试中的模型。

Result: 总结了多智能体轨迹预测领域的关键挑战和最新进展。

Conclusion: 指出了多智能体轨迹预测领域的未来研究方向。

Abstract: With the emergence of powerful data-driven methods in human trajectory
prediction (HTP), gaining a finer understanding of multi-agent interactions
lies within hand's reach, with important implications in areas such as
autonomous navigation and crowd modeling. This survey reviews some of the most
recent advancements in deep learning-based multi-agent trajectory prediction,
focusing on studies published between 2020 and 2024. We categorize the existing
methods based on their architectural design, their input representations, and
their overall prediction strategies, placing a particular emphasis on models
evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges
and future research directions in the field of multi-agent HTP.

</details>


### [8] [ArchShapeNet:An Interpretable 3D-CNN Framework for Evaluating Architectural Shapes](https://arxiv.org/abs/2506.14832)
*Jun Yin,Jing Zhong,Pengyu Zeng,Peilin Li,Zixuan Dai,Miao Zhang,Shuai Lu*

Main category: cs.CV

TL;DR: 论文构建了ArchForms-4000数据集和ArchShapeNet模型，用于区分人类设计与机器生成的3D建筑形式，模型表现优于人类专家。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对人类设计与机器生成3D建筑形式的客观分析，限制了对其各自优势的理解和生成工具的进步。

Method: 构建包含4000个3D形式的数据集（2000个人类设计，2000个机器生成），开发了专门用于分类和分析建筑形式的3D卷积神经网络ArchShapeNet，并进行了比较实验。

Result: 模型在区分形式来源上表现优异，准确率94.29%，精确率96.2%，召回率98.51%。

Conclusion: 研究揭示了人类设计在空间组织、比例和谐与细节优化上的优势，为未来生成设计工具的改进提供了宝贵见解。

Abstract: In contemporary architectural design, the growing complexity and diversity of
design demands have made generative plugin tools essential for quickly
producing initial concepts and exploring novel 3D forms. However, objectively
analyzing the differences between human-designed and machine-generated 3D forms
remains a challenge, limiting our understanding of their respective strengths
and hindering the advancement of generative tools.
  To address this, we built ArchForms-4000, a dataset containing 2,000
architect-designed and 2,000 Evomass-generated 3D forms; Proposed ArchShapeNet,
a 3D convolutional neural network tailored for classifying and analyzing
architectural forms, incorporating a saliency module to highlight key spatial
features aligned with architectural reasoning; And conducted comparative
experiments showing our model outperforms human experts in distinguishing form
origins, achieving 94.29% accuracy, 96.2% precision, and 98.51% recall.
  This study not only highlights the distinctive advantages of human-designed
forms in spatial organization, proportional harmony, and detail refinement but
also provides valuable insights for enhancing generative design tools in the
future.

</details>


### [9] [Real-Time, Low-Latency Surveillance Using Entropy-Based Adaptive Buffering and MobileNetV2 on Edge Devices](https://arxiv.org/abs/2506.14833)
*Poojashree Chandrashekar Pankaj M Sajjanar*

Main category: cs.CV

TL;DR: 提出了一种高性能、低延迟的视频监控系统，适用于资源受限环境，结合熵自适应帧缓冲算法和MobileNetV2，实现高吞吐量和低延迟。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限设备（如Raspberry Pi、NVIDIA Jetson Nano）在视频监控中的性能挑战，设计高效且低延迟的系统。

Method: 提出基于熵的自适应帧缓冲算法，并与MobileNetV2结合，优化处理流程。

Result: 系统在标准数据集上保持92%以上的检测准确率，端到端推理延迟低于50ms，且对光照、背景和速度变化具有鲁棒性。

Conclusion: 该系统可扩展、成本低，符合严格的数据隐私法规，适用于智能城市或嵌入式安全架构。

Abstract: This paper describes a high-performance, low-latency video surveillance
system designed for resource-constrained environments. We have proposed a
formal entropy-based adaptive frame buffering algorithm and integrated that
with MobileNetV2 to achieve high throughput with low latency. The system is
capable of processing live streams of video with sub-50ms end-to-end inference
latency on resource-constrained devices (embedding platforms) such as Raspberry
Pi, Amazon, and NVIDIA Jetson Nano. Our method maintains over 92% detection
accuracy on standard datasets focused on video surveillance and exhibits
robustness to varying lighting, backgrounds, and speeds. A number of
comparative and ablation experiments validate the effectiveness of our design.
Finally, our architecture is scalable, inexpensive, and compliant with stricter
data privacy regulations than common surveillance systems, so that the system
could coexist in a smart city or embedded security architecture.

</details>


### [10] [MonoVQD: Monocular 3D Object Detection with Variational Query Denoising and Self-Distillation](https://arxiv.org/abs/2506.14835)
*Kiet Dang Vu,Trung Thai Tran,Duc Dung Nguyen*

Main category: cs.CV

TL;DR: MonoVQD是一种新型的DETR框架，通过改进自注意力机制、引入变分查询去噪技术和自蒸馏策略，显著提升了单目3D检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决DETR在单目3D检测中的局限性，提升性能。

Method: 提出Mask Separated Self-Attention机制、Variational Query Denoising技术和自蒸馏策略。

Result: 在KITTI和nuScenes数据集上表现优异，具有强泛化能力。

Conclusion: MonoVQD通过创新方法显著提升了单目和多视角3D检测性能。

Abstract: Precisely localizing 3D objects from a single image constitutes a central
challenge in monocular 3D detection. While DETR-like architectures offer a
powerful paradigm, their direct application in this domain encounters inherent
limitations, preventing optimal performance. Our work addresses these
challenges by introducing MonoVQD, a novel framework designed to fundamentally
advance DETR-based monocular 3D detection. We propose three main contributions.
First, we propose the Mask Separated Self-Attention mechanism that enables the
integration of the denoising process into a DETR architecture. This improves
the stability of Hungarian matching to achieve a consistent optimization
objective. Second, we present the Variational Query Denoising technique to
address the gradient vanishing problem of conventional denoising methods, which
severely restricts the efficiency of the denoising process. This explicitly
introduces stochastic properties to mitigate this fundamental limitation and
unlock substantial performance gains. Finally, we introduce a sophisticated
self-distillation strategy, leveraging insights from later decoder layers to
synergistically improve query quality in earlier layers, thereby amplifying the
iterative refinement process. Rigorous experimentation demonstrates that
MonoVQD achieves superior performance on the challenging KITTI monocular
benchmark. Highlighting its broad applicability, MonoVQD's core components
seamlessly integrate into other architectures, delivering significant
performance gains even in multi-view 3D detection scenarios on the nuScenes
dataset and underscoring its robust generalization capabilities.

</details>


### [11] [Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction](https://arxiv.org/abs/2506.14837)
*Chengzhi Xu,Yuyang Wang,Lai Wei,Lichao Sun,Weiran Huang*

Main category: cs.CV

TL;DR: ChartIR是一种基于结构化指令的迭代优化方法，用于提升多模态大语言模型在图表到代码生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在图表到代码生成任务中表现不佳，需要同时具备精确的视觉理解和准确的代码翻译能力。

Method: 将任务分解为视觉理解和代码翻译两部分，设计描述和差异两种结构化指令，并采用初始代码生成和迭代优化的两阶段流程。

Result: 实验表明，ChartIR在开源模型Qwen2-VL和闭源模型GPT-4o上均优于其他方法。

Conclusion: ChartIR通过结构化指令和迭代优化有效提升了图表到代码生成的性能。

Abstract: Recently, multimodal large language models (MLLMs) have attracted increasing
research attention due to their powerful visual understanding capabilities.
While they have achieved impressive results on various vision tasks, their
performance on chart-to-code generation remains suboptimal. This task requires
MLLMs to generate executable code that can reproduce a given chart, demanding
not only precise visual understanding but also accurate translation of visual
elements into structured code. Directly prompting MLLMs to perform this complex
task often yields unsatisfactory results. To address this challenge, we propose
{ChartIR}, an iterative refinement method based on structured instruction.
First, we distinguish two tasks: visual understanding and code translation. To
accomplish the visual understanding component, we design two types of
structured instructions: description and difference. The description
instruction captures the visual elements of the reference chart, while the
difference instruction characterizes the discrepancies between the reference
chart and the generated chart. These instructions effectively transform visual
features into language representations, thereby facilitating the subsequent
code translation process. Second, we decompose the overall chart generation
pipeline into two stages: initial code generation and iterative refinement,
enabling progressive enhancement of the final output. Experimental results show
that, compared to other method, our method achieves superior performance on
both the open-source model Qwen2-VL and the closed-source model GPT-4o.

</details>


### [12] [PictSure: Pretraining Embeddings Matters for In-Context Learning Image Classifiers](https://arxiv.org/abs/2506.14842)
*Lukas Schiesser,Cornelius Wolff,Sophie Haas,Simon Pukrop*

Main category: cs.CV

TL;DR: PictSure是一个专注于图像嵌入模型的ICL框架，通过系统分析嵌入模型的架构、预训练和训练动态，显著提升了少样本图像分类的跨域性能。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺领域，构建图像分类模型困难，而现有ICL方法忽视了图像嵌入模型的关键作用。

Method: 提出PictSure框架，系统研究不同视觉编码器类型、预训练目标和微调策略对少样本分类性能的影响。

Result: 实验表明，嵌入模型的预训练方式显著影响训练成功率和跨域性能，PictSure在跨域任务中优于现有方法。

Conclusion: PictSure通过优化嵌入模型设计，提升了少样本图像分类的跨域泛化能力，同时保持域内任务性能。

Abstract: Building image classification models remains cumbersome in data-scarce
domains, where collecting large labeled datasets is impractical. In-context
learning (ICL) has emerged as a promising paradigm for few-shot image
classification (FSIC), enabling models to generalize across domains without
gradient-based adaptation. However, prior work has largely overlooked a
critical component of ICL-based FSIC pipelines: the role of image embeddings.
In this work, we present PictSure, an ICL framework that places the embedding
model -- its architecture, pretraining, and training dynamics -- at the center
of analysis. We systematically examine the effects of different visual encoder
types, pretraining objectives, and fine-tuning strategies on downstream FSIC
performance. Our experiments show that the training success and the
out-of-domain performance are highly dependent on how the embedding models are
pretrained. Consequently, PictSure manages to outperform existing ICL-based
FSIC models on out-of-domain benchmarks that differ significantly from the
training distribution, while maintaining comparable results on in-domain tasks.
Code can be found at https://github.com/PictSure/pictsure-library.

</details>


### [13] [Finding Optimal Kernel Size and Dimension in Convolutional Neural Networks An Architecture Optimization Approach](https://arxiv.org/abs/2506.14846)
*Shreyas Rajeev,B Sathish Babu*

Main category: cs.CV

TL;DR: 论文提出了一种名为BKSEF的框架，用于在CNN中动态选择最优的卷积核大小，显著提升了模型精度并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 卷积核大小在CNN设计中常被忽视，但其对感受野、特征提取、计算成本和模型精度有重要影响。

Method: BKSEF结合信息论、信号处理和学习理论，通过数学和实证方法确定每层的最优卷积核大小。

Result: 在多个数据集上，BKSEF指导的模型比传统3x3卷积核模型精度提升3.1%，计算量减少42.8%。

Conclusion: BKSEF为CNN设计提供了高效且应用感知的优化方法，适合集成到神经架构搜索和实时系统中。

Abstract: Kernel size selection in Convolutional Neural Networks (CNNs) is a critical
but often overlooked design decision that affects receptive field, feature
extraction, computational cost, and model accuracy. This paper proposes the
Best Kernel Size Estimation Function (BKSEF), a mathematically grounded and
empirically validated framework for optimal, layer-wise kernel size
determination. BKSEF balances information gain, computational efficiency, and
accuracy improvements by integrating principles from information theory, signal
processing, and learning theory. Extensive experiments on CIFAR-10, CIFAR-100,
ImageNet-lite, ChestX-ray14, and GTSRB datasets demonstrate that BKSEF-guided
architectures achieve up to 3.1 percent accuracy improvement and 42.8 percent
reduction in FLOPs compared to traditional models using uniform 3x3 kernels.
Two real-world case studies further validate the approach: one for medical
image classification in a cloud-based setup, and another for traffic sign
recognition on edge devices. The former achieved enhanced interpretability and
accuracy, while the latter reduced latency and model size significantly, with
minimal accuracy trade-off. These results show that kernel size can be an
active, optimizable parameter rather than a fixed heuristic. BKSEF provides
practical heuristics and theoretical support for researchers and developers
seeking efficient and application-aware CNN designs. It is suitable for
integration into neural architecture search pipelines and real-time systems,
offering a new perspective on CNN optimization.

</details>


### [14] [Efficient Retail Video Annotation: A Robust Key Frame Generation Approach for Product and Customer Interaction Analysis](https://arxiv.org/abs/2506.14854)
*Varun Mannam,Zhenyu Shi*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的零售视频自动标注方法，显著降低了人工标注成本。


<details>
  <summary>Details</summary>
Motivation: 传统零售视频标注依赖人工，耗时且成本高，需要更高效的自动化解决方案。

Method: 利用深度神经网络学习视频帧特征，结合零售环境定制的目标检测技术。

Result: 方法在保持标注质量的同时，节省了50%的成本，人工仅需验证5%的帧。

Conclusion: 该方法高效且经济，适用于多种零售场景，如顾客行为分析和产品交互检测。

Abstract: Accurate video annotation plays a vital role in modern retail applications,
including customer behavior analysis, product interaction detection, and
in-store activity recognition. However, conventional annotation methods heavily
rely on time-consuming manual labeling by human annotators, introducing
non-robust frame selection and increasing operational costs. To address these
challenges in the retail domain, we propose a deep learning-based approach that
automates key-frame identification in retail videos and provides automatic
annotations of products and customers. Our method leverages deep neural
networks to learn discriminative features by embedding video frames and
incorporating object detection-based techniques tailored for retail
environments. Experimental results showcase the superiority of our approach
over traditional methods, achieving accuracy comparable to human annotator
labeling while enhancing the overall efficiency of retail video annotation.
Remarkably, our approach leads to an average of 2 times cost savings in video
annotation. By allowing human annotators to verify/adjust less than 5% of
detected frames in the video dataset, while automating the annotation process
for the remaining frames without reducing annotation quality, retailers can
significantly reduce operational costs. The automation of key-frame detection
enables substantial time and effort savings in retail video labeling tasks,
proving highly valuable for diverse retail applications such as shopper journey
analysis, product interaction detection, and in-store security monitoring.

</details>


### [15] [Peering into the Unknown: Active View Selection with Neural Uncertainty Maps for 3D Reconstruction](https://arxiv.org/abs/2506.14856)
*Zhengquan Zhang,Feng Xu,Mengmi Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经不确定性预测的主动视角选择方法（UPNet），用于高效3D重建，显著减少计算开销并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决3D重建中如何选择最具信息量的视角以减少计算成本并提高效率的问题。

Method: 使用轻量级神经网络UPNet预测不确定性图，通过聚合不确定性图选择最优视角，训练3D神经渲染模型。

Result: 仅用一半视角即达到与上限相当的精度，计算速度提升400倍，资源消耗减少50%以上。

Conclusion: UPNet方法在高效性和泛化性上表现优异，适用于新物体类别且无需额外训练。

Abstract: Some perspectives naturally provide more information than others. How can an
AI system determine which viewpoint offers the most valuable insight for
accurate and efficient 3D object reconstruction? Active view selection (AVS)
for 3D reconstruction remains a fundamental challenge in computer vision. The
aim is to identify the minimal set of views that yields the most accurate 3D
reconstruction. Instead of learning radiance fields, like NeRF or 3D Gaussian
Splatting, from a current observation and computing uncertainty for each
candidate viewpoint, we introduce a novel AVS approach guided by neural
uncertainty maps predicted by a lightweight feedforward deep neural network,
named UPNet. UPNet takes a single input image of a 3D object and outputs a
predicted uncertainty map, representing uncertainty values across all possible
candidate viewpoints. By leveraging heuristics derived from observing many
natural objects and their associated uncertainty patterns, we train UPNet to
learn a direct mapping from viewpoint appearance to uncertainty in the
underlying volumetric representations. Next, our approach aggregates all
previously predicted neural uncertainty maps to suppress redundant candidate
viewpoints and effectively select the most informative one. Using these
selected viewpoints, we train 3D neural rendering models and evaluate the
quality of novel view synthesis against other competitive AVS methods.
Remarkably, despite using half of the viewpoints than the upper bound, our
method achieves comparable reconstruction accuracy. In addition, it
significantly reduces computational overhead during AVS, achieving up to a 400
times speedup along with over 50\% reductions in CPU, RAM, and GPU usage
compared to baseline methods. Notably, our approach generalizes effectively to
AVS tasks involving novel object categories, without requiring any additional
training.

</details>


### [16] [DETONATE: A Benchmark for Text-to-Image Alignment and Kernelized Direct Preference Optimization](https://arxiv.org/abs/2506.14903)
*Renjith Prasad,Abhilekh Borah,Hasnat Md Abdullah,Chathurangi Shyalika,Gurpreet Singh,Ritvik Garimella,Rajarshi Roy,Harshul Surana,Nasrin Imanpour,Suranjana Trivedy,Amit Sheth,Amitava Das*

Main category: cs.CV

TL;DR: 论文提出DPO-Kernels方法，通过混合损失、核化表示和散度选择增强文本到图像模型的对齐性，并引入DETONATE基准和AQI指标。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像模型在忠实用户意图、安全性和公平性方面的对齐问题。

Method: 结合混合损失、核化表示（RBF、多项式和小波核）和多种散度选择（Wasserstein、R'enyi）。

Result: 提出DETONATE基准和AQI指标，实验显示DPO-Kernels通过HT-SR保持强泛化能力。

Conclusion: DPO-Kernels在多维度对齐上表现优异，为文本到图像模型提供了更稳健的解决方案。

Abstract: Alignment is crucial for text-to-image (T2I) models to ensure that generated
images faithfully capture user intent while maintaining safety and fairness.
Direct Preference Optimization (DPO), prominent in large language models
(LLMs), is extending its influence to T2I systems. This paper introduces
DPO-Kernels for T2I models, a novel extension enhancing alignment across three
dimensions: (i) Hybrid Loss, integrating embedding-based objectives with
traditional probability-based loss for improved optimization; (ii) Kernelized
Representations, employing Radial Basis Function (RBF), Polynomial, and Wavelet
kernels for richer feature transformations and better separation between safe
and unsafe inputs; and (iii) Divergence Selection, expanding beyond DPO's
default Kullback-Leibler (KL) regularizer by incorporating Wasserstein and
R'enyi divergences for enhanced stability and robustness. We introduce
DETONATE, the first large-scale benchmark of its kind, comprising approximately
100K curated image pairs categorized as chosen and rejected. DETONATE
encapsulates three axes of social bias and discrimination: Race, Gender, and
Disability. Prompts are sourced from hate speech datasets, with images
generated by leading T2I models including Stable Diffusion 3.5 Large, Stable
Diffusion XL, and Midjourney. Additionally, we propose the Alignment Quality
Index (AQI), a novel geometric measure quantifying latent-space separability of
safe/unsafe image activations, revealing hidden vulnerabilities. Empirically,
we demonstrate that DPO-Kernels maintain strong generalization bounds via
Heavy-Tailed Self-Regularization (HT-SR). DETONATE and complete code are
publicly released.

</details>


### [17] [PeRL: Permutation-Enhanced Reinforcement Learning for Interleaved Vision-Language Reasoning](https://arxiv.org/abs/2506.14907)
*Yizhen Zhang,Yang Ding,Shuoshuo Zhang,Xinchen Zhang,Haoling Li,Zhong-zhi Li,Peijie Wang,Jie Wu,Lei Ji,Yelong Shen,Yujiu Yang,Yeyun Gong*

Main category: cs.CV

TL;DR: 本文提出了一种名为PeRL的强化学习方法，专注于多图像位置推理任务，通过多阶段策略和图像序列排列增强学习效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态强化学习方法主要局限于单图像空间推理，难以推广到涉及多图像位置关系的复杂场景。

Method: 提出PeRL方法，包括图像序列排列和多阶段策略，以及用于重采样的rollout过滤机制。

Result: 在5个多图像基准和3个单图像基准上，PeRL显著优于现有方法，在多图像任务上达到最优性能。

Conclusion: PeRL在多图像任务中表现出色，同时保持单图像任务的性能，为多模态推理任务提供了有效解决方案。

Abstract: Inspired by the impressive reasoning capabilities demonstrated by
reinforcement learning approaches like DeepSeek-R1, recent emerging research
has begun exploring the use of reinforcement learning (RL) to enhance
vision-language models (VLMs) for multimodal reasoning tasks. However, most
existing multimodal reinforcement learning approaches remain limited to spatial
reasoning within single-image contexts, yet still struggle to generalize to
more complex and real-world scenarios involving multi-image positional
reasoning, where understanding the relationships across images is crucial. To
address this challenge, we propose a general reinforcement learning approach
PeRL tailored for interleaved multimodal tasks, and a multi-stage strategy
designed to enhance the exploration-exploitation trade-off, thereby improving
learning efficiency and task performance. Specifically, we introduce
permutation of image sequences to simulate varied positional relationships to
explore more spatial and positional diversity. Furthermore, we design a rollout
filtering mechanism for resampling to focus on trajectories that contribute
most to learning optimal behaviors to exploit learned policies effectively. We
evaluate our model on 5 widely-used multi-image benchmarks and 3 single-image
benchmarks. Our experiments confirm that PeRL trained model consistently
surpasses R1-related and interleaved VLM baselines by a large margin, achieving
state-of-the-art performance on multi-image benchmarks, while preserving
comparable performance on single-image tasks.

</details>


### [18] [Frequency-Calibrated Membership Inference Attacks on Medical Image Diffusion Models](https://arxiv.org/abs/2506.14919)
*Xinkai Zhao,Yuta Tokuoka,Junichiro Iwasawa,Keita Oda*

Main category: cs.CV

TL;DR: 提出了一种针对医学图像扩散模型的频率校准重建误差（FCRE）方法，用于成员推断攻击（MIA），通过关注中频范围的重建误差，有效量化隐私风险。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在医学图像等敏感领域的应用引发了隐私担忧，现有MIA方法依赖重建误差，但受图像固有难度和高频细节重建困难的影响。

Method: 提出FCRE方法，分析反向扩散过程，计算中频重建误差和结构相似性指数，通过阈值比较确定成员关系。

Result: 在多个医学图像数据集上的实验表明，FCRE方法优于现有MIA方法。

Conclusion: FCRE方法通过频率选择性策略，有效解决了医学图像MIA中的挑战，为隐私风险评估提供了更可靠的工具。

Abstract: The increasing use of diffusion models for image generation, especially in
sensitive areas like medical imaging, has raised significant privacy concerns.
Membership Inference Attack (MIA) has emerged as a potential approach to
determine if a specific image was used to train a diffusion model, thus
quantifying privacy risks. Existing MIA methods often rely on diffusion
reconstruction errors, where member images are expected to have lower
reconstruction errors than non-member images. However, applying these methods
directly to medical images faces challenges. Reconstruction error is influenced
by inherent image difficulty, and diffusion models struggle with high-frequency
detail reconstruction. To address these issues, we propose a
Frequency-Calibrated Reconstruction Error (FCRE) method for MIAs on medical
image diffusion models. By focusing on reconstruction errors within a specific
mid-frequency range and excluding both high-frequency (difficult to
reconstruct) and low-frequency (less informative) regions, our
frequency-selective approach mitigates the confounding factor of inherent image
difficulty. Specifically, we analyze the reverse diffusion process, obtain the
mid-frequency reconstruction error, and compute the structural similarity index
score between the reconstructed and original images. Membership is determined
by comparing this score to a threshold. Experiments on several medical image
datasets demonstrate that our FCRE method outperforms existing MIA methods.

</details>


### [19] [Vision Transformers for End-to-End Quark-Gluon Jet Classification from Calorimeter Images](https://arxiv.org/abs/2506.14934)
*Md Abrar Jahin,Shahriar Soudeep,Arian Rahman Aditta,M. F. Mridha,Nafiz Fahad,Md. Jakir Hossen*

Main category: cs.CV

TL;DR: 论文系统评估了Vision Transformer (ViT)及其与CNN的混合模型在夸克-胶子喷注分类中的表现，发现ViT模型在多种指标上优于传统CNN。


<details>
  <summary>Details</summary>
Motivation: 区分夸克和胶子喷注是高能物理中的关键任务，但现有方法（如CNN）在全局信息建模方面存在不足，ViT的潜力尚未充分探索。

Method: 使用模拟的2012年CMS开放数据，构建多通道喷注图像（ECAL、HCAL和重建轨迹），并评估ViT和ViT-CNN混合模型的性能。

Result: ViT模型（尤其是ViT+MaxViT和ViT+ConvNeXt混合模型）在F1分数、ROC-AUC和准确率上显著优于CNN基线。

Conclusion: ViT架构在喷注分类中表现出色，为未来基于深度学习的喷注研究提供了系统框架和数据集。

Abstract: Distinguishing between quark- and gluon-initiated jets is a critical and
challenging task in high-energy physics, pivotal for improving new physics
searches and precision measurements at the Large Hadron Collider. While deep
learning, particularly Convolutional Neural Networks (CNNs), has advanced jet
tagging using image-based representations, the potential of Vision Transformer
(ViT) architectures, renowned for modeling global contextual information,
remains largely underexplored for direct calorimeter image analysis, especially
under realistic detector and pileup conditions. This paper presents a
systematic evaluation of ViTs and ViT-CNN hybrid models for quark-gluon jet
classification using simulated 2012 CMS Open Data. We construct multi-channel
jet-view images from detector-level energy deposits (ECAL, HCAL) and
reconstructed tracks, enabling an end-to-end learning approach. Our
comprehensive benchmarking demonstrates that ViT-based models, notably
ViT+MaxViT and ViT+ConvNeXt hybrids, consistently outperform established CNN
baselines in F1-score, ROC-AUC, and accuracy, highlighting the advantage of
capturing long-range spatial correlations within jet substructure. This work
establishes the first systematic framework and robust performance baselines for
applying ViT architectures to calorimeter image-based jet classification using
public collider data, alongside a structured dataset suitable for further deep
learning research in this domain.

</details>


### [20] [Advances in Compliance Detection: Novel Models Using Vision-Based Tactile Sensors](https://arxiv.org/abs/2506.14980)
*Ziteng Li,Malte Kuhlmann,Ilana Nisky,Nicolás Navarro-Guerrero*

Main category: cs.CV

TL;DR: 提出两种基于LRCN和Transformer的模型，利用GelSight触觉传感器的RGB图像预测物体顺应性，显著提升了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统顺应性检测方法缺乏便携性和扩展性，依赖昂贵设备，且现有神经网络方法精度不足。

Method: 采用LRCN和Transformer架构，结合GelSight传感器的RGB触觉图像和其他信息预测顺应性。

Result: 模型在多种指标下表现优异，显著优于基线方法，但较硬物体的顺应性估计更具挑战性。

Conclusion: 提出的模型能有效提升顺应性预测精度，但需进一步研究传感器与物体顺应性之间的相关性。

Abstract: Compliance is a critical parameter for describing objects in engineering,
agriculture, and biomedical applications. Traditional compliance detection
methods are limited by their lack of portability and scalability, rely on
specialized, often expensive equipment, and are unsuitable for robotic
applications. Moreover, existing neural network-based approaches using
vision-based tactile sensors still suffer from insufficient prediction
accuracy. In this paper, we propose two models based on Long-term Recurrent
Convolutional Networks (LRCNs) and Transformer architectures that leverage RGB
tactile images and other information captured by the vision-based sensor
GelSight to predict compliance metrics accurately. We validate the performance
of these models using multiple metrics and demonstrate their effectiveness in
accurately estimating compliance. The proposed models exhibit significant
performance improvement over the baseline. Additionally, we investigated the
correlation between sensor compliance and object compliance estimation, which
revealed that objects that are harder than the sensor are more challenging to
estimate.

</details>


### [21] [Hyper-Local Deformable Transformers for Text Spotting on Historical Maps](https://arxiv.org/abs/2506.15010)
*Yijun Lin,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: PALETTE是一种端到端的文本识别系统，专为历史地图设计，通过超局部采样模块和合成数据训练，显著提升了长文本和倾斜文本的识别效果。


<details>
  <summary>Details</summary>
Motivation: 历史地图中的文本信息具有重要价值，但现有方法难以处理其复杂背景和多样化的文本样式，亟需一种更灵活且精确的文本识别方法。

Method: 提出PALETTE系统，采用超局部采样模块学习文本边界点和字符的局部特征，并结合超局部位置嵌入增强空间关系建模；同时开发SynthMap+自动生成合成地图数据用于训练。

Result: PALETTE在历史地图基准数据集上优于现有方法，尤其在长文本和倾斜文本识别中表现突出，已成功处理6万张地图并生成1亿多个文本标签。

Conclusion: PALETTE通过创新的特征学习和合成数据方法，解决了历史地图文本识别的关键挑战，为大规模地图数字化提供了实用工具。

Abstract: Text on historical maps contains valuable information providing georeferenced
historical, political, and cultural contexts. However, text extraction from
historical maps is challenging due to the lack of (1) effective methods and (2)
training data. Previous approaches use ad-hoc steps tailored to only specific
map styles. Recent machine learning-based text spotters (e.g., for scene
images) have the potential to solve these challenges because of their
flexibility in supporting various types of text instances. However, these
methods remain challenges in extracting precise image features for predicting
every sub-component (boundary points and characters) in a text instance. This
is critical because map text can be lengthy and highly rotated with complex
backgrounds, posing difficulties in detecting relevant image features from a
rough text region. This paper proposes PALETTE, an end-to-end text spotter for
scanned historical maps of a wide variety. PALETTE introduces a novel
hyper-local sampling module to explicitly learn localized image features around
the target boundary points and characters of a text instance for detection and
recognition. PALETTE also enables hyper-local positional embeddings to learn
spatial interactions between boundary points and characters within and across
text instances. In addition, this paper presents a novel approach to
automatically generate synthetic map images, SynthMap+, for training text
spotters for historical maps. The experiment shows that PALETTE with SynthMap+
outperforms SOTA text spotters on two new benchmark datasets of historical
maps, particularly for long and angled text. We have deployed PALETTE with
SynthMap+ to process over 60,000 maps in the David Rumsey Historical Map
collection and generated over 100 million text labels to support map searching.
The project is released at
https://github.com/kartta-foundation/mapkurator-palette-doc.

</details>


### [22] [Break Stylistic Sophon: Are We Really Meant to Confine the Imagination in Style Transfer?](https://arxiv.org/abs/2506.15033)
*Gary Song Yan,Yusen Zhang,Jinyu Zhao,Hao Zhang,Zhangping Yang,Guanye Xiong,Yanfei Liu,Tao Zhang,Yujie He,Siyuan Tian,Yao Gou,Min Li*

Main category: cs.CV

TL;DR: StyleWallfacer是一个统一的训练和推理框架，解决了传统风格迁移中的问题，并支持艺术家级风格迁移和文本驱动风格化。


<details>
  <summary>Details</summary>
Motivation: 传统风格迁移方法存在多种问题，如风格注入效率低、语义对齐不足等，需要一种统一的框架来解决这些问题并支持多种任务。

Method: 1. 提出基于语义的风格注入方法，利用BLIP生成与风格图像语义严格对齐的文本描述；2. 设计基于人类反馈的数据增强策略；3. 提出无训练的三重扩散过程，通过替换自注意力层的键和值来注入风格。

Result: 实现了高质量的图像驱动风格迁移和文本驱动风格化，保留了原始图像内容，并首次在风格迁移过程中实现了图像颜色编辑。

Conclusion: StyleWallfacer框架在风格迁移领域取得了突破性进展，支持多种任务并提供了高质量的结果。

Abstract: In this pioneering study, we introduce StyleWallfacer, a groundbreaking
unified training and inference framework, which not only addresses various
issues encountered in the style transfer process of traditional methods but
also unifies the framework for different tasks. This framework is designed to
revolutionize the field by enabling artist level style transfer and text driven
stylization. First, we propose a semantic-based style injection method that
uses BLIP to generate text descriptions strictly aligned with the semantics of
the style image in CLIP space. By leveraging a large language model to remove
style-related descriptions from these descriptions, we create a semantic gap.
This gap is then used to fine-tune the model, enabling efficient and drift-free
injection of style knowledge. Second, we propose a data augmentation strategy
based on human feedback, incorporating high-quality samples generated early in
the fine-tuning process into the training set to facilitate progressive
learning and significantly reduce its overfitting. Finally, we design a
training-free triple diffusion process using the fine-tuned model, which
manipulates the features of self-attention layers in a manner similar to the
cross-attention mechanism. Specifically, in the generation process, the key and
value of the content-related process are replaced with those of the
style-related process to inject style while maintaining text control over the
model. We also introduce query preservation to mitigate disruptions to the
original content. Under such a design, we have achieved high-quality
image-driven style transfer and text-driven stylization, delivering
artist-level style transfer results while preserving the original image
content. Moreover, we achieve image color editing during the style transfer
process for the first time.

</details>


### [23] [Enhancing Vector Quantization with Distributional Matching: A Theoretical and Empirical Study](https://arxiv.org/abs/2506.15078)
*Xianghong Fang,Litao Guo,Hengchao Chen,Yuxuan Zhang,XiaofanXia,Dingjie Song,Yexin Liu,Hao Wang,Harry Yang,Yuan Yuan,Qiang Sun*

Main category: cs.CV

TL;DR: 提出了一种基于Wasserstein距离的向量量化方法，解决了训练不稳定和码本崩溃问题，显著提高了码本利用率和量化效果。


<details>
  <summary>Details</summary>
Motivation: 现有向量量化方法存在训练不稳定和码本崩溃问题，主要源于特征与码向量分布不匹配。

Method: 使用Wasserstein距离对齐特征与码向量分布，优化量化过程。

Result: 实现了接近100%的码本利用率，显著降低了量化误差。

Conclusion: 理论和实证分析验证了该方法的有效性。

Abstract: The success of autoregressive models largely depends on the effectiveness of
vector quantization, a technique that discretizes continuous features by
mapping them to the nearest code vectors within a learnable codebook. Two
critical issues in existing vector quantization methods are training
instability and codebook collapse. Training instability arises from the
gradient discrepancy introduced by the straight-through estimator, especially
in the presence of significant quantization errors, while codebook collapse
occurs when only a small subset of code vectors are utilized during training. A
closer examination of these issues reveals that they are primarily driven by a
mismatch between the distributions of the features and code vectors, leading to
unrepresentative code vectors and significant data information loss during
compression. To address this, we employ the Wasserstein distance to align these
two distributions, achieving near 100\% codebook utilization and significantly
reducing the quantization error. Both empirical and theoretical analyses
validate the effectiveness of the proposed approach.

</details>


### [24] [SynPo: Boosting Training-Free Few-Shot Medical Segmentation via High-Quality Negative Prompts](https://arxiv.org/abs/2506.15153)
*Yufei Liu,Haoke Xiao,Jiaxing Chai,Yongcun Zhang,Rong Wang,Zijie Meng,Zhiming Luo*

Main category: cs.CV

TL;DR: SynPo是一种基于LVMs（如SAM）的无训练少样本医学图像分割方法，通过改进负提示质量提升低对比度医学图像的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LVMs的无训练方法未能有效利用负提示，导致在低对比度医学图像上表现不佳。

Method: 设计了Confidence Map Synergy Module结合DINOv2和SAM的优势，选择高质量正负点提示，并通过K-means聚类优化提示集。

Result: 实验表明SynPo性能接近最先进的基于训练的方法。

Conclusion: SynPo通过优化提示质量，实现了无训练少样本医学图像分割的高性能。

Abstract: The advent of Large Vision Models (LVMs) offers new opportunities for
few-shot medical image segmentation. However, existing training-free methods
based on LVMs fail to effectively utilize negative prompts, leading to poor
performance on low-contrast medical images. To address this issue, we propose
SynPo, a training-free few-shot method based on LVMs (e.g., SAM), with the core
insight: improving the quality of negative prompts. To select point prompts in
a more reliable confidence map, we design a novel Confidence Map Synergy Module
by combining the strengths of DINOv2 and SAM. Based on the confidence map, we
select the top-k pixels as the positive points set and choose the negative
points set using a Gaussian distribution, followed by independent K-means
clustering for both sets. Then, these selected points are leveraged as
high-quality prompts for SAM to get the segmentation results. Extensive
experiments demonstrate that SynPo achieves performance comparable to
state-of-the-art training-based few-shot methods.

</details>


### [25] [Enhancing point cloud analysis via neighbor aggregation correction based on cross-stage structure correlation](https://arxiv.org/abs/2506.15160)
*Jiaqi Shi,Jin Xiao,Xiaoguang Hu,Boyang Song,Hao Jiang,Tianyou Chen,Baochang Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为PDSA的模块，通过高维空间相关性校正特征分布，提升点云分析的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决点云分析中局部坐标导致的无关点干扰和特征层次差距问题。

Method: 利用轻量级跨阶段结构描述符区分点相关性，并通过减少邻居特征矩阵方差和长距离建模增强结构同质性。

Result: 在语义分割和分类任务中验证了方法的泛化性，性能显著提升且参数成本更低。

Conclusion: PDSA模块有效解决了点云分析中的问题，并通过实验证明了其高效性和合理性。

Abstract: Point cloud analysis is the cornerstone of many downstream tasks, among which
aggregating local structures is the basis for understanding point cloud data.
While numerous works aggregate neighbor using three-dimensional relative
coordinates, there are irrelevant point interference and feature hierarchy gap
problems due to the limitation of local coordinates. Although some works
address this limitation by refining spatial description though explicit
modeling of cross-stage structure, these enhancement methods based on direct
geometric structure encoding have problems of high computational overhead and
noise sensitivity. To overcome these problems, we propose the Point
Distribution Set Abstraction module (PDSA) that utilizes the correlation in the
high-dimensional space to correct the feature distribution during aggregation,
which improves the computational efficiency and robustness. PDSA distinguishes
the point correlation based on a lightweight cross-stage structural descriptor,
and enhances structural homogeneity by reducing the variance of the neighbor
feature matrix and increasing classes separability though long-distance
modeling. Additionally, we introducing a key point mechanism to optimize the
computational overhead. The experimental result on semantic segmentation and
classification tasks based on different baselines verify the generalization of
the method we proposed, and achieve significant performance improvement with
less parameter cost. The corresponding ablation and visualization results
demonstrate the effectiveness and rationality of our method. The code and
training weight is available at: https://github.com/AGENT9717/PointDistribution

</details>


### [26] [Echo-DND: A dual noise diffusion model for robust and precise left ventricle segmentation in echocardiography](https://arxiv.org/abs/2506.15166)
*Abdur Rahman,Keerthiveena Balraj,Manojkumar Ramteke,Anurag Singh Rathore*

Main category: cs.CV

TL;DR: Echo-DND是一种新型双噪声扩散模型，用于超声心动图中左心室的高精度分割，结合高斯和伯努利噪声，并通过多尺度融合和空间一致性校准提升性能。


<details>
  <summary>Details</summary>
Motivation: 超声图像噪声多、对比度低且边界模糊，导致左心室分割困难，需要更精确的模型。

Method: 提出Echo-DND模型，结合高斯和伯努利噪声，采用多尺度融合条件模块和空间一致性校准技术。

Result: 在CAMUS和EchoNet-Dynamic数据集上表现优异，Dice分数分别达到0.962和0.939。

Conclusion: Echo-DND为超声心动图分割设定了新标准，并有望推广至其他医学影像任务。

Abstract: Recent advancements in diffusion probabilistic models (DPMs) have
revolutionized image processing, demonstrating significant potential in medical
applications. Accurate segmentation of the left ventricle (LV) in
echocardiograms is crucial for diagnostic procedures and necessary treatments.
However, ultrasound images are notoriously noisy with low contrast and
ambiguous LV boundaries, thereby complicating the segmentation process. To
address these challenges, this paper introduces Echo-DND, a novel dual-noise
diffusion model specifically designed for this task. Echo-DND leverages a
unique combination of Gaussian and Bernoulli noises. It also incorporates a
multi-scale fusion conditioning module to improve segmentation precision.
Furthermore, it utilizes spatial coherence calibration to maintain spatial
integrity in segmentation masks. The model's performance was rigorously
validated on the CAMUS and EchoNet-Dynamic datasets. Extensive evaluations
demonstrate that the proposed framework outperforms existing SOTA models. It
achieves high Dice scores of 0.962 and 0.939 on these datasets, respectively.
The proposed Echo-DND model establishes a new standard in echocardiogram
segmentation, and its architecture holds promise for broader applicability in
other medical imaging tasks, potentially improving diagnostic accuracy across
various medical domains. Project page: https://abdur75648.github.io/Echo-DND

</details>


### [27] [ReSeDis: A Dataset for Referring-based Object Search across Large-Scale Image Collections](https://arxiv.org/abs/2506.15180)
*Ziling Huang,Yidan Zhang,Shin'ichi Satoh*

Main category: cs.CV

TL;DR: ReSeDis任务统一了大规模检索与细粒度定位，提出了新的基准和评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有技术无法同时实现大规模检索和细粒度定位，ReSeDis填补了这一空白。

Method: 引入ReSeDis任务，设计基准和评估指标，提供零样本基线模型。

Result: 展示了ReSeDis任务的潜力，并揭示了未来研究的改进空间。

Conclusion: ReSeDis为构建下一代多模态搜索系统提供了现实的测试平台。

Abstract: Large-scale visual search engines are expected to solve a dual problem at
once: (i) locate every image that truly contains the object described by a
sentence and (ii) identify the object's bounding box or exact pixels within
each hit. Existing techniques address only one side of this challenge. Visual
grounding yields tight boxes and masks but rests on the unrealistic assumption
that the object is present in every test image, producing a flood of false
alarms when applied to web-scale collections. Text-to-image retrieval excels at
sifting through massive databases to rank relevant images, yet it stops at
whole-image matches and offers no fine-grained localization. We introduce
Referring Search and Discovery (ReSeDis), the first task that unifies
corpus-level retrieval with pixel-level grounding. Given a free-form
description, a ReSeDis model must decide whether the queried object appears in
each image and, if so, where it is, returning bounding boxes or segmentation
masks. To enable rigorous study, we curate a benchmark in which every
description maps uniquely to object instances scattered across a large, diverse
corpus, eliminating unintended matches. We further design a task-specific
metric that jointly scores retrieval recall and localization precision.
Finally, we provide a straightforward zero-shot baseline using a frozen
vision-language model, revealing significant headroom for future study. ReSeDis
offers a realistic, end-to-end testbed for building the next generation of
robust and scalable multimodal search systems.

</details>


### [28] [Conquering the Retina: Bringing Visual in-Context Learning to OCT](https://arxiv.org/abs/2506.15200)
*Alessio Negrini,Simon Reiß*

Main category: cs.CV

TL;DR: 论文探讨了如何通过视觉上下文学习（VICL）训练通用模型，用于视网膜光学相干断层扫描（OCT）领域，并提出了评估协议和基线结果。


<details>
  <summary>Details</summary>
Motivation: 现有专用模型性能优异但适用范围有限，通用模型能灵活适应新任务，无需特定开发。

Method: 使用视觉上下文学习（VICL）训练通用模型，基于推理时的少量示例实现跨任务泛化。

Result: 在多个视网膜OCT数据集上评估了先进的VICL方法，建立了首个基线，展示了其潜力与局限性。

Conclusion: VICL在OCT领域具有潜力，但需进一步研究以克服当前限制；代码已开源以促进研究。

Abstract: Recent advancements in medical image analysis have led to the development of
highly specialized models tailored to specific clinical tasks. These models
have demonstrated exceptional performance and remain a crucial research
direction. Yet, their applicability is limited to predefined tasks, requiring
expertise and extensive resources for development and adaptation. In contrast,
generalist models offer a different form of utility: allowing medical
practitioners to define tasks on the fly without the need for task-specific
model development. In this work, we explore how to train generalist models for
the domain of retinal optical coherence tomography using visual in-context
learning (VICL), i.e., training models to generalize across tasks based on a
few examples provided at inference time. To facilitate rigorous assessment, we
propose a broad evaluation protocol tailored to VICL in OCT. We extensively
evaluate a state-of-the-art medical VICL approach on multiple retinal OCT
datasets, establishing a first baseline to highlight the potential and current
limitations of in-context learning for OCT. To foster further research and
practical adoption, we openly release our code.

</details>


### [29] [Privacy-Shielded Image Compression: Defending Against Exploitation from Vision-Language Pretrained Models](https://arxiv.org/abs/2506.15201)
*Xuelin Shen,Jiayin Xu,Kangsheng Yin,Wenhan Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为PSIC的灵活编码方法，通过在图像压缩阶段实施防御，保护用户隐私，防止视觉语言预训练模型利用公开图像。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言预训练模型语义理解能力的提升，公开图像的隐私保护变得更具挑战性。本文旨在通过图像压缩阶段的防御措施保护用户隐私。

Method: 提出PSIC方法，生成具有多种解码选项的比特流，默认解码保留感知质量但阻止VLP模型解析。设计了CLTG模块和UAEO优化函数，并采用自适应多目标优化策略。

Result: 实验表明，该方法在多个下游任务中有效，既能保护隐私，又能保留图像压缩功能。

Conclusion: PSIC是一种即插即用的方案，可无缝集成到现有LIC模型中，同时提升加密性能和感知质量。

Abstract: The improved semantic understanding of vision-language pretrained (VLP)
models has made it increasingly difficult to protect publicly posted images
from being exploited by search engines and other similar tools. In this
context, this paper seeks to protect users' privacy by implementing defenses at
the image compression stage to prevent exploitation. Specifically, we propose a
flexible coding method, termed Privacy-Shielded Image Compression (PSIC), that
can produce bitstreams with multiple decoding options. By default, the
bitstream is decoded to preserve satisfactory perceptual quality while
preventing interpretation by VLP models. Our method also retains the original
image compression functionality. With a customizable input condition, the
proposed scheme can reconstruct the image that preserves its full semantic
information. A Conditional Latent Trigger Generation (CLTG) module is proposed
to produce bias information based on customizable conditions to guide the
decoding process into different reconstructed versions, and an
Uncertainty-Aware Encryption-Oriented (UAEO) optimization function is designed
to leverage the soft labels inferred from the target VLP model's uncertainty on
the training data. This paper further incorporates an adaptive multi-objective
optimization strategy to obtain improved encrypting performance and perceptual
quality simultaneously within a unified training process. The proposed scheme
is plug-and-play and can be seamlessly integrated into most existing Learned
Image Compression (LIC) models. Extensive experiments across multiple
downstream tasks have demonstrated the effectiveness of our design.

</details>


### [30] [DM-FNet: Unified multimodal medical image fusion via diffusion process-trained encoder-decoder](https://arxiv.org/abs/2506.15218)
*Dan He,Weisheng Li,Guofen Wang,Yuping Huang,Shiqiang Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于两阶段扩散模型的融合网络（DM-FNet），用于多模态医学图像融合，显著提升了融合图像的质量和信息密度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉细节特征和跨模态特征交互方面表现不足，导致融合图像质量不佳。

Method: 采用两阶段扩散模型：第一阶段训练UNet进行图像重建，第二阶段将噪声图像输入融合网络以增强特征识别能力，并结合三个关键融合模块。

Result: 实验结果表明，该方法在客观评价指标上表现优异，融合图像保留了适当的亮度、放射性示踪剂分布、丰富纹理和清晰边缘。

Conclusion: DM-FNet通过稳健的网络结构和混合损失函数，有效提升了多模态医学图像融合的质量和信息密度。

Abstract: Multimodal medical image fusion (MMIF) extracts the most meaningful
information from multiple source images, enabling a more comprehensive and
accurate diagnosis. Achieving high-quality fusion results requires a careful
balance of brightness, color, contrast, and detail; this ensures that the fused
images effectively display relevant anatomical structures and reflect the
functional status of the tissues. However, existing MMIF methods have limited
capacity to capture detailed features during conventional training and suffer
from insufficient cross-modal feature interaction, leading to suboptimal fused
image quality. To address these issues, this study proposes a two-stage
diffusion model-based fusion network (DM-FNet) to achieve unified MMIF. In
Stage I, a diffusion process trains UNet for image reconstruction. UNet
captures detailed information through progressive denoising and represents
multilevel data, providing a rich set of feature representations for the
subsequent fusion network. In Stage II, noisy images at various steps are input
into the fusion network to enhance the model's feature recognition capability.
Three key fusion modules are also integrated to process medical images from
different modalities adaptively. Ultimately, the robust network structure and a
hybrid loss function are integrated to harmonize the fused image's brightness,
color, contrast, and detail, enhancing its quality and information density. The
experimental results across various medical image types demonstrate that the
proposed method performs exceptionally well regarding objective evaluation
metrics. The fused image preserves appropriate brightness, a comprehensive
distribution of radioactive tracers, rich textures, and clear edges. The code
is available at https://github.com/HeDan-11/DM-FNet.

</details>


### [31] [video-SALMONN 2: Captioning-Enhanced Audio-Visual Large Language Models](https://arxiv.org/abs/2506.15220)
*Changli Tang,Yixuan Li,Yudong Yang,Jimin Zhuang,Guangzhi Sun,Wei Li,Zejun Ma,Chao Zhang*

Main category: cs.CV

TL;DR: 视频-SALMONN 2是一种先进的视听大语言模型，通过低秩适应（LoRA）和定向偏好优化（DPO）提升视频字幕生成能力，提出多轮DPO（MrDPO）方法显著提高准确性。


<details>
  <summary>Details</summary>
Motivation: 视频包含丰富信息，生成准确详细的自然语言描述是视频理解的关键。

Method: 提出多轮DPO（MrDPO）方法，定期更新DPO参考模型，合并并重新初始化LoRA模块，结合真实字幕指导以稳定训练。

Result: MrDPO将视频-SALMONN 2的字幕错误率降低28%，在7B参数规模下超越GPT-4o和Gemini-1.5-Pro。

Conclusion: 视频-SALMONN 2在视频字幕任务中表现优异，同时在类似规模的视频问答基准测试中保持竞争力。

Abstract: Videos contain a wealth of information, and generating detailed and accurate
descriptions in natural language is a key aspect of video understanding. In
this paper, we present video-SALMONN 2, an advanced audio-visual large language
model (LLM) with low-rank adaptation (LoRA) designed for enhanced video (with
paired audio) captioning through directed preference optimisation (DPO). We
propose new metrics to evaluate the completeness and accuracy of video
descriptions, which are optimised using DPO. To further improve training, we
propose a novel multi-round DPO (MrDPO) approach, which involves periodically
updating the DPO reference model, merging and re-initialising the LoRA module
as a proxy for parameter updates after each training round (1,000 steps), and
incorporating guidance from ground-truth video captions to stabilise the
process. Experimental results show that MrDPO significantly enhances
video-SALMONN 2's captioning accuracy, reducing the captioning error rates by
28\%. The final video-SALMONN 2 model, with just 7 billion parameters,
surpasses leading models such as GPT-4o and Gemini-1.5-Pro in video captioning
tasks, while maintaining highly competitive performance to the state-of-the-art
on widely used video question-answering benchmarks among models of similar
size. Codes are available at
\href{https://github.com/bytedance/video-SALMONN-2}{https://github.com/bytedance/video-SALMONN-2}.

</details>


### [32] [Convolutional Feature Enhancement and Attention Fusion BiFPN for Ship Detection in SAR Images](https://arxiv.org/abs/2506.15231)
*Liangjie Meng,Danxia Li,Jinrong He,Lili Ma,Zhixin Li*

Main category: cs.CV

TL;DR: 提出了一种名为C-AFBiFPN的特征增强与融合框架，用于解决SAR船舶检测中的多尺度变化、小目标与噪声混合以及复杂背景问题。


<details>
  <summary>Details</summary>
Motivation: SAR船舶检测面临多尺度变化、小目标与噪声混合以及复杂背景等挑战，需要更强大的特征表示和融合能力。

Method: C-AFBiFPN包含卷积特征增强模块（CFE）和创新的AFBiFPN网络，结合BiFormer注意力机制，提升局部细节和全局建模能力。

Result: 在SSDD数据集上，该方法显著提高了小目标检测精度、抗遮挡鲁棒性和多尺度特征适应性。

Conclusion: C-AFBiFPN有效解决了SAR船舶检测中的关键问题，提升了检测性能。

Abstract: Synthetic Aperture Radar (SAR) enables submeter-resolution imaging and
all-weather monitoring via active microwave and advanced signal processing.
Currently, SAR has found extensive applications in critical maritime domains
such as ship detection. However, SAR ship detection faces several challenges,
including significant scale variations among ships, the presence of small
offshore vessels mixed with noise, and complex backgrounds for large nearshore
ships. To address these issues, this paper proposes a novel feature enhancement
and fusion framework named C-AFBiFPN. C-AFBiFPN constructs a Convolutional
Feature Enhancement (CFE) module following the backbone network, aiming to
enrich feature representation and enhance the ability to capture and represent
local details and contextual information. Furthermore, C-AFBiFPN innovatively
integrates BiFormer attention within the fusion strategy of BiFPN, creating the
AFBiFPN network. AFBiFPN improves the global modeling capability of cross-scale
feature fusion and can adaptively focus on critical feature regions. The
experimental results on SAR Ship Detection Dataset (SSDD) indicate that the
proposed approach substantially enhances detection accuracy for small targets,
robustness against occlusions, and adaptability to multi-scale features.

</details>


### [33] [RA-NeRF: Robust Neural Radiance Field Reconstruction with Accurate Camera Pose Estimation under Complex Trajectories](https://arxiv.org/abs/2506.15242)
*Qingsong Yan,Qiang Wang,Kaiyong Zhao,Jie Chen,Bo Li,Xiaowen Chu,Fei Deng*

Main category: cs.CV

TL;DR: RA-NeRF是一种新方法，能够在复杂相机轨迹下预测高精度相机位姿，结合了NeRF和流驱动的位姿调节，实现了先进的相机位姿估计和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂相机轨迹下位姿估计精度不足，RA-NeRF旨在解决这一问题。

Method: 采用增量式流程，结合NeRF的光度一致性和流驱动的位姿调节，并使用隐式位姿滤波器消除噪声。

Result: 在Tanks&Temple和NeRFBuster数据集上，RA-NeRF在位姿估计和视觉质量上均达到最先进水平。

Conclusion: RA-NeRF在复杂相机轨迹下表现出高效性和鲁棒性，适用于场景重建任务。

Abstract: Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have emerged
as powerful tools for 3D reconstruction and SLAM tasks. However, their
performance depends heavily on accurate camera pose priors. Existing approaches
attempt to address this issue by introducing external constraints but fall
short of achieving satisfactory accuracy, particularly when camera trajectories
are complex. In this paper, we propose a novel method, RA-NeRF, capable of
predicting highly accurate camera poses even with complex camera trajectories.
Following the incremental pipeline, RA-NeRF reconstructs the scene using NeRF
with photometric consistency and incorporates flow-driven pose regulation to
enhance robustness during initialization and localization. Additionally,
RA-NeRF employs an implicit pose filter to capture the camera movement pattern
and eliminate the noise for pose estimation. To validate our method, we conduct
extensive experiments on the Tanks\&Temple dataset for standard evaluation, as
well as the NeRFBuster dataset, which presents challenging camera pose
trajectories. On both datasets, RA-NeRF achieves state-of-the-art results in
both camera pose estimation and visual quality, demonstrating its effectiveness
and robustness in scene reconstruction under complex pose trajectories.

</details>


### [34] [Retrospective Memory for Camouflaged Object Detection](https://arxiv.org/abs/2506.15244)
*Chenxi Zhang,Jiayun Wu,Qing Zhang,Yazhe Zhai,Youwei Pang*

Main category: cs.CV

TL;DR: 提出了一种名为RetroMem的回忆增强COD架构，通过整合历史知识动态调制伪装模式感知和推理，显著提升了模型对伪装场景的理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有COD方法缺乏获取历史背景的机制，限制了其在处理复杂伪装场景时的适应性和有效性。

Method: 采用两阶段训练范式（学习阶段和回忆阶段），设计了密集多尺度适配器（DMA）和动态记忆机制（DMM）及推理模式重建（IPR）。

Result: 在多个数据集上的实验表明，RetroMem显著优于现有最先进方法。

Conclusion: RetroMem通过动态整合历史知识，有效提升了伪装对象检测的性能。

Abstract: Camouflaged object detection (COD) primarily focuses on learning subtle yet
discriminative representations from complex scenes. Existing methods
predominantly follow the parametric feedforward architecture based on static
visual representation modeling. However, they lack explicit mechanisms for
acquiring historical context, limiting their adaptation and effectiveness in
handling challenging camouflage scenes. In this paper, we propose a
recall-augmented COD architecture, namely RetroMem, which dynamically modulates
camouflage pattern perception and inference by integrating relevant historical
knowledge into the process. Specifically, RetroMem employs a two-stage training
paradigm consisting of a learning stage and a recall stage to construct,
update, and utilize memory representations effectively. During the learning
stage, we design a dense multi-scale adapter (DMA) to improve the pretrained
encoder's capability to capture rich multi-scale visual information with very
few trainable parameters, thereby providing foundational inferences. In the
recall stage, we propose a dynamic memory mechanism (DMM) and an inference
pattern reconstruction (IPR). These components fully leverage the latent
relationships between learned knowledge and current sample context to
reconstruct the inference of camouflage patterns, thereby significantly
improving the model's understanding of camouflage scenes. Extensive experiments
on several widely used datasets demonstrate that our RetroMem significantly
outperforms existing state-of-the-art methods.

</details>


### [35] [Domain Adaptation for Image Classification of Defects in Semiconductor Manufacturing](https://arxiv.org/abs/2506.15260)
*Adrian Poniatowski,Natalie Gentner,Manuel Barusco,Davide Dalle Pezze,Samuele Salti,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 论文探讨了在半导体领域中使用领域自适应（DA）技术，特别是在半监督和无监督设置下，以减少重新标记和重新训练的需求，并提出了改进的DBACS方法。


<details>
  <summary>Details</summary>
Motivation: 由于半导体行业的高需求和激烈竞争，缩短上市时间和提高质量是关键。领域自适应技术能够利用源领域的知识适应目标领域，减少资源消耗。

Method: 提出了DBACS方法，这是一种受CycleGAN启发的模型，通过额外的损失项提升性能。在半监督和无监督设置下进行了实验验证。

Result: 在真实世界的电子显微镜图像上验证了方法的有效性，证明了其在半导体领域中推进DA技术的实用性。

Conclusion: DBACS方法在半导体领域的半监督和无监督设置中表现出色，为减少资源消耗和提高效率提供了有效解决方案。

Abstract: In the semiconductor sector, due to high demand but also strong and
increasing competition, time to market and quality are key factors in securing
significant market share in various application areas. Thanks to the success of
deep learning methods in recent years in the computer vision domain, Industry
4.0 and 5.0 applications, such as defect classification, have achieved
remarkable success. In particular, Domain Adaptation (DA) has proven highly
effective since it focuses on using the knowledge learned on a (source) domain
to adapt and perform effectively on a different but related (target) domain. By
improving robustness and scalability, DA minimizes the need for extensive
manual re-labeling or re-training of models. This not only reduces
computational and resource costs but also allows human experts to focus on
high-value tasks. Therefore, we tested the efficacy of DA techniques in
semi-supervised and unsupervised settings within the context of the
semiconductor field. Moreover, we propose the DBACS approach, a
CycleGAN-inspired model enhanced with additional loss terms to improve
performance. All the approaches are studied and validated on real-world
Electron Microscope images considering the unsupervised and semi-supervised
settings, proving the usefulness of our method in advancing DA techniques for
the semiconductor field.

</details>


### [36] [MSNeRV: Neural Video Representation with Multi-Scale Feature Fusion](https://arxiv.org/abs/2506.15276)
*Jun Zhu,Xinfeng Zhang,Lv Tang,JunHao Jiang*

Main category: cs.CV

TL;DR: MSNeRV是一种多尺度特征融合框架，用于神经视频表示，解决了现有INR方法在细节密集和快速变化视频内容中的不足，并在压缩效率上超越了VTM-23.7。


<details>
  <summary>Details</summary>
Motivation: 现有基于INR的视频压缩方法在细节密集和快速变化内容上表现不佳，主要原因是网络内部特征利用不足和缺乏视频特定设计。

Method: 提出MSNeRV框架，包括时间窗口增强时间一致性，GoP级网格表示背景，多尺度空间解码器和自适应损失函数，以及多尺度特征块。

Result: 在HEVC ClassB和UVG数据集上，MSNeRV在表示能力和压缩效率上优于现有INR方法和VTM-23.7。

Conclusion: MSNeRV通过多尺度特征融合和视频特定设计，显著提升了神经视频表示和压缩性能。

Abstract: Implicit Neural representations (INRs) have emerged as a promising approach
for video compression, and have achieved comparable performance to the
state-of-the-art codecs such as H.266/VVC. However, existing INR-based methods
struggle to effectively represent detail-intensive and fast-changing video
content. This limitation mainly stems from the underutilization of internal
network features and the absence of video-specific considerations in network
design. To address these challenges, we propose a multi-scale feature fusion
framework, MSNeRV, for neural video representation. In the encoding stage, we
enhance temporal consistency by employing temporal windows, and divide the
video into multiple Groups of Pictures (GoPs), where a GoP-level grid is used
for background representation. Additionally, we design a multi-scale spatial
decoder with a scale-adaptive loss function to integrate multi-resolution and
multi-frequency information. To further improve feature extraction, we
introduce a multi-scale feature block that fully leverages hidden features. We
evaluate MSNeRV on HEVC ClassB and UVG datasets for video representation and
compression. Experimental results demonstrate that our model exhibits superior
representation capability among INR-based approaches and surpasses VTM-23.7
(Random Access) in dynamic scenarios in terms of compression efficiency.

</details>


### [37] [BCRNet: Enhancing Landmark Detection in Laparoscopic Liver Surgery via Bezier Curve Refinement](https://arxiv.org/abs/2506.15279)
*Qian Li,Feng Liu,Shuojue Yang,Daiyun Shen,Yueming Jin*

Main category: cs.CV

TL;DR: BCRNet通过Bezier曲线细化策略显著提升腹腔镜肝脏手术中的关键解剖标志检测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜肝脏手术中准确识别解剖结构具有挑战性，AR系统通过2D-3D配准提供解决方案，但需精确检测曲线标志。

Method: 提出BCRNet框架，包括多模态特征提取（MFE）、自适应曲线提议初始化（ACPI）和分层曲线细化（HCR）机制。

Result: 在L3D和P2ILF数据集上，BCRNet表现优于现有方法，性能显著提升。

Conclusion: BCRNet为腹腔镜手术中的解剖标志检测提供了高效解决方案，代码将开源。

Abstract: Laparoscopic liver surgery, while minimally invasive, poses significant
challenges in accurately identifying critical anatomical structures. Augmented
reality (AR) systems, integrating MRI/CT with laparoscopic images based on
2D-3D registration, offer a promising solution for enhancing surgical
navigation. A vital aspect of the registration progress is the precise
detection of curvilinear anatomical landmarks in laparoscopic images. In this
paper, we propose BCRNet (Bezier Curve Refinement Net), a novel framework that
significantly enhances landmark detection in laparoscopic liver surgery
primarily via the Bezier curve refinement strategy. The framework starts with a
Multi-modal Feature Extraction (MFE) module designed to robustly capture
semantic features. Then we propose Adaptive Curve Proposal Initialization
(ACPI) to generate pixel-aligned Bezier curves and confidence scores for
reliable initial proposals. Additionally, we design the Hierarchical Curve
Refinement (HCR) mechanism to enhance these proposals iteratively through a
multi-stage process, capturing fine-grained contextual details from multi-scale
pixel-level features for precise Bezier curve adjustment. Extensive evaluations
on the L3D and P2ILF datasets demonstrate that BCRNet outperforms
state-of-the-art methods, achieving significant performance improvements. Code
will be available.

</details>


### [38] [AI-driven visual monitoring of industrial assembly tasks](https://arxiv.org/abs/2506.15285)
*Mattia Nardon,Stefano Messelodi,Antonio Granata,Fabio Poiesi,Alberto Danese,Davide Boscaini*

Main category: cs.CV

TL;DR: ViMAT是一种无需标记或固定工作环境的AI驱动系统，用于实时监控工业装配任务。


<details>
  <summary>Details</summary>
Motivation: 现有商业解决方案通常需要固定工作环境或视觉标记，限制了灵活性。

Method: ViMAT结合感知模块（从多视角视频流提取视觉信息）和推理模块（基于观察状态和任务知识推断动作）。

Result: 在LEGO组件更换和液压机模具重组任务中验证了其有效性。

Conclusion: ViMAT在部分和不确定视觉观察的挑战性场景中表现优异。

Abstract: Visual monitoring of industrial assembly tasks is critical for preventing
equipment damage due to procedural errors and ensuring worker safety. Although
commercial solutions exist, they typically require rigid workspace setups or
the application of visual markers to simplify the problem. We introduce ViMAT,
a novel AI-driven system for real-time visual monitoring of assembly tasks that
operates without these constraints. ViMAT combines a perception module that
extracts visual observations from multi-view video streams with a reasoning
module that infers the most likely action being performed based on the observed
assembly state and prior task knowledge. We validate ViMAT on two assembly
tasks, involving the replacement of LEGO components and the reconfiguration of
hydraulic press molds, demonstrating its effectiveness through quantitative and
qualitative analysis in challenging real-world scenarios characterized by
partial and uncertain visual observations. Project page:
https://tev-fbk.github.io/ViMAT

</details>


### [39] [MEGC2025: Micro-Expression Grand Challenge on Spot Then Recognize and Visual Question Answering](https://arxiv.org/abs/2506.15298)
*Xinqi Fan,Jingting Li,John See,Moi Hoon Yap,Wen-Huang Cheng,Xiaobai Li,Xiaopeng Hong,Su-Jing Wang,Adrian K. Davision*

Main category: cs.CV

TL;DR: MEGC 2025挑战赛提出两个新任务：ME-STR（微表情检测与识别一体化）和ME-VQA（基于视觉问答的微表情理解），旨在利用多模态大模型提升微表情分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法将微表情检测与识别分开处理，在长视频分析中效果不佳，而多模态大语言模型（MLLMs）和大型视觉语言模型（LVLMs）为微表情分析提供了新思路。

Method: MEGC 2025提出两个任务：1) ME-STR，将检测与识别整合为统一流程；2) ME-VQA，通过视觉问答探索微表情理解。

Result: 挑战赛要求参赛算法在测试集上运行并提交结果，以评估性能。

Conclusion: MEGC 2025旨在推动微表情分析领域的发展，尤其是结合多模态大模型的潜力。

Abstract: Facial micro-expressions (MEs) are involuntary movements of the face that
occur spontaneously when a person experiences an emotion but attempts to
suppress or repress the facial expression, typically found in a high-stakes
environment. In recent years, substantial advancements have been made in the
areas of ME recognition, spotting, and generation. However, conventional
approaches that treat spotting and recognition as separate tasks are
suboptimal, particularly for analyzing long-duration videos in realistic
settings. Concurrently, the emergence of multimodal large language models
(MLLMs) and large vision-language models (LVLMs) offers promising new avenues
for enhancing ME analysis through their powerful multimodal reasoning
capabilities. The ME grand challenge (MEGC) 2025 introduces two tasks that
reflect these evolving research directions: (1) ME spot-then-recognize
(ME-STR), which integrates ME spotting and subsequent recognition in a unified
sequential pipeline; and (2) ME visual question answering (ME-VQA), which
explores ME understanding through visual question answering, leveraging MLLMs
or LVLMs to address diverse question types related to MEs. All participating
algorithms are required to run on this test set and submit their results on a
leaderboard. More details are available at https://megc2025.github.io.

</details>


### [40] [MapFM: Foundation Model-Driven HD Mapping with Multi-Task Contextual Learning](https://arxiv.org/abs/2506.15313)
*Leonid Ivanov,Vasily Yuryev,Dmitry Yudin*

Main category: cs.CV

TL;DR: 本文提出了一种名为MapFM的端到端模型，用于在线生成矢量化的高精地图，通过结合强大的基础模型和辅助预测头，显著提升了特征表示质量和预测精度。


<details>
  <summary>Details</summary>
Motivation: 高精地图和鸟瞰图语义地图在自动驾驶中至关重要，但现有方法在特征表示和预测质量上仍有提升空间。

Method: 采用端到端模型MapFM，结合基础模型编码相机图像，并集成辅助预测头进行鸟瞰图语义分割，实现多任务学习。

Result: 模型显著提升了特征表示质量和预测精度，生成的矢量化高精地图质量更高。

Conclusion: MapFM通过多任务学习和上下文监督，提供了更全面的场景表示，适用于自动驾驶中的高精地图生成。

Abstract: In autonomous driving, high-definition (HD) maps and semantic maps in
bird's-eye view (BEV) are essential for accurate localization, planning, and
decision-making. This paper introduces an enhanced End-to-End model named MapFM
for online vectorized HD map generation. We show significantly boost feature
representation quality by incorporating powerful foundation model for encoding
camera images. To further enrich the model's understanding of the environment
and improve prediction quality, we integrate auxiliary prediction heads for
semantic segmentation in the BEV representation. This multi-task learning
approach provides richer contextual supervision, leading to a more
comprehensive scene representation and ultimately resulting in higher accuracy
and improved quality of the predicted vectorized HD maps. The source code is
available at https://github.com/LIvanoff/MapFM.

</details>


### [41] [OpenPath: Open-Set Active Learning for Pathology Image Classification via Pre-trained Vision-Language Models](https://arxiv.org/abs/2506.15318)
*Lanfeng Zhong,Xin Liao,Shichuan Zhang,Shaoting Zhang,Guotai Wang*

Main category: cs.CV

TL;DR: OpenPath是一种新颖的开放集主动学习方法，用于病理图像分类，通过预训练的视觉语言模型（VLM）和任务特定提示，有效选择分布内（ID）样本，避免选择分布外（OOD）样本。


<details>
  <summary>Details</summary>
Motivation: 在真实临床环境中，未标注数据常包含大量分布外（OOD）数据，传统主动学习方法效率低下且初始随机选择浪费标注成本。

Method: OpenPath结合任务特定提示和多样性信息ID采样（DIS），包括原型ID候选选择（PIS）和熵引导随机采样（EGSS），确保样本纯度和信息量。

Result: 在两个公共病理图像数据集上，OpenPath显著提升模型性能，优于现有开放集主动学习方法。

Conclusion: OpenPath通过高纯度的样本选择和多样性信息采样，有效解决了开放集主动学习中的挑战。

Abstract: Pathology image classification plays a crucial role in accurate medical
diagnosis and treatment planning. Training high-performance models for this
task typically requires large-scale annotated datasets, which are both
expensive and time-consuming to acquire. Active Learning (AL) offers a solution
by iteratively selecting the most informative samples for annotation, thereby
reducing the labeling effort. However, most AL methods are designed under the
assumption of a closed-set scenario, where all the unannotated images belong to
target classes. In real-world clinical environments, the unlabeled pool often
contains a substantial amount of Out-Of-Distribution (OOD) data, leading to low
efficiency of annotation in traditional AL methods. Furthermore, most existing
AL methods start with random selection in the first query round, leading to a
significant waste of labeling costs in open-set scenarios. To address these
challenges, we propose OpenPath, a novel open-set active learning approach for
pathological image classification leveraging a pre-trained Vision-Language
Model (VLM). In the first query, we propose task-specific prompts that combine
target and relevant non-target class prompts to effectively select
In-Distribution (ID) and informative samples from the unlabeled pool. In
subsequent queries, Diverse Informative ID Sampling (DIS) that includes
Prototype-based ID candidate Selection (PIS) and Entropy-Guided Stochastic
Sampling (EGSS) is proposed to ensure both purity and informativeness in a
query, avoiding the selection of OOD samples. Experiments on two public
pathology image datasets show that OpenPath significantly enhances the model's
performance due to its high purity of selected samples, and outperforms several
state-of-the-art open-set AL methods. The code is available at
\href{https://github.com/HiLab-git/OpenPath}{https://github.com/HiLab-git/OpenPath}..

</details>


### [42] [Open-World Object Counting in Videos](https://arxiv.org/abs/2506.15368)
*Niki Amini-Naieni,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出了一种新的开放世界视频对象计数任务，并介绍了CountVid模型和VideoCount数据集。


<details>
  <summary>Details</summary>
Motivation: 解决在拥挤场景中因遮挡和相似对象导致的重复计数和重新识别问题。

Method: 结合基于图像的计数模型和可提示的视频分割与跟踪模型。

Result: CountVid在VideoCount数据集上表现优于基线方法。

Conclusion: CountVid和VideoCount为开放世界视频对象计数提供了有效解决方案。

Abstract: We introduce a new task of open-world object counting in videos: given a text
description, or an image example, that specifies the target object, the
objective is to enumerate all the unique instances of the target objects in the
video. This task is especially challenging in crowded scenes with occlusions
and similar objects, where avoiding double counting and identifying
reappearances is crucial. To this end, we make the following contributions: we
introduce a model, CountVid, for this task. It leverages an image-based
counting model, and a promptable video segmentation and tracking model to
enable automated, open-world object counting across video frames. To evaluate
its performance, we introduce VideoCount, a new dataset for our novel task
built from the TAO and MOT20 tracking datasets, as well as from videos of
penguins and metal alloy crystallization captured by x-rays. Using this
dataset, we demonstrate that CountVid provides accurate object counts, and
significantly outperforms strong baselines. The VideoCount dataset, the
CountVid model, and all the code are available at
https://github.com/niki-amini-naieni/CountVid/.

</details>


### [43] [Unsupervised Pelage Pattern Unwrapping for Animal Re-identification](https://arxiv.org/abs/2506.15369)
*Aleksandr Algasov,Ekaterina Nepovinnykh,Fedor Zolotarev,Tuomas Eerola,Heikki Kälviäinen,Pavel Zemčík,Charles V. Stewart*

Main category: cs.CV

TL;DR: 提出了一种几何感知的纹理映射方法，用于解决动物皮毛或皮肤图案因身体运动和姿态变化导致的几何变形问题，提高了再识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理动物皮毛或皮肤图案的几何变形问题，尤其是在不同姿态和视角下。

Method: 通过表面法线估计将皮毛图案映射到规范的UV空间，保持几何一致性，并结合自监督训练。

Result: 在Saimaa环斑海豹和豹的数据集上，再识别准确率提高了5.4%。

Conclusion: 该方法无需标注真实UV数据，能有效提升复杂姿态下的再识别性能。

Abstract: Existing individual re-identification methods often struggle with the
deformable nature of animal fur or skin patterns which undergo geometric
distortions due to body movement and posture changes. In this paper, we propose
a geometry-aware texture mapping approach that unwarps pelage patterns, the
unique markings found on an animal's skin or fur, into a canonical UV space,
enabling more robust feature matching. Our method uses surface normal
estimation to guide the unwrapping process while preserving the geometric
consistency between the 3D surface and the 2D texture space. We focus on two
challenging species: Saimaa ringed seals (Pusa hispida saimensis) and leopards
(Panthera pardus). Both species have distinctive yet highly deformable fur
patterns. By integrating our pattern-preserving UV mapping with existing
re-identification techniques, we demonstrate improved accuracy across diverse
poses and viewing angles. Our framework does not require ground truth UV
annotations and can be trained in a self-supervised manner. Experiments on seal
and leopard datasets show up to a 5.4% improvement in re-identification
accuracy.

</details>


### [44] [When Model Knowledge meets Diffusion Model: Diffusion-assisted Data-free Image Synthesis with Alignment of Domain and Class](https://arxiv.org/abs/2506.15381)
*Yujin Kim,Hyunsoo Kim,Hyunwoo J. Kim,Suhyun Kim*

Main category: cs.CV

TL;DR: DDIS是一种新的数据自由图像合成方法，利用扩散模型作为图像先验，通过领域对齐指导和类对齐标记优化，生成更接近训练数据分布的图像。


<details>
  <summary>Details</summary>
Motivation: 现有DFIS方法因缺乏自然图像先验知识，生成的样本偏离训练数据分布，DDIS旨在解决这一问题。

Method: DDIS结合扩散模型作为图像先验，引入领域对齐指导（DAG）和优化类对齐标记（CAT）嵌入。

Result: 在PACS和ImageNet上，DDIS优于现有DFIS方法，生成更符合训练数据分布的图像。

Conclusion: DDIS通过扩散模型和领域对齐指导，显著提升了数据自由图像合成的质量。

Abstract: Open-source pre-trained models hold great potential for diverse applications,
but their utility declines when their training data is unavailable. Data-Free
Image Synthesis (DFIS) aims to generate images that approximate the learned
data distribution of a pre-trained model without accessing the original data.
However, existing DFIS meth ods produce samples that deviate from the training
data distribution due to the lack of prior knowl edge about natural images. To
overcome this limitation, we propose DDIS, the first Diffusion-assisted
Data-free Image Synthesis method that leverages a text-to-image diffusion model
as a powerful image prior, improving synthetic image quality. DDIS extracts
knowledge about the learned distribution from the given model and uses it to
guide the diffusion model, enabling the generation of images that accurately
align with the training data distribution. To achieve this, we introduce Domain
Alignment Guidance (DAG) that aligns the synthetic data domain with the
training data domain during the diffusion sampling process. Furthermore, we
optimize a single Class Alignment Token (CAT) embedding to effectively capture
class-specific attributes in the training dataset. Experiments on PACS and Ima
geNet demonstrate that DDIS outperforms prior DFIS methods by generating
samples that better reflect the training data distribution, achieving SOTA
performance in data-free applications.

</details>


### [45] [NERO: Explainable Out-of-Distribution Detection with Neuron-level Relevance](https://arxiv.org/abs/2506.15404)
*Anju Chhetri,Jari Korhonen,Prashnna Gyawali,Binod Bhattarai*

Main category: cs.CV

TL;DR: 提出了一种名为NERO的新型OOD评分机制，利用特征层的神经元级相关性来增强OOD样本的检测能力。


<details>
  <summary>Details</summary>
Motivation: 在医学影像领域，模型可靠性至关重要，而现有OOD检测方法可能无法完全捕捉OOD样本的多样性。

Method: 通过聚类神经元级相关性形成代表性中心点，并引入相关性距离度量来量化新样本与这些中心点的偏差。

Result: 在胃肠道影像基准测试中，NERO优于现有OOD检测方法。

Conclusion: NERO通过神经元级相关性和特征范数结合，提升了OOD检测性能，并支持可解释性检测。

Abstract: Ensuring reliability is paramount in deep learning, particularly within the
domain of medical imaging, where diagnostic decisions often hinge on model
outputs. The capacity to separate out-of-distribution (OOD) samples has proven
to be a valuable indicator of a model's reliability in research. In medical
imaging, this is especially critical, as identifying OOD inputs can help flag
potential anomalies that might otherwise go undetected. While many OOD
detection methods rely on feature or logit space representations, recent works
suggest these approaches may not fully capture OOD diversity. To address this,
we propose a novel OOD scoring mechanism, called NERO, that leverages
neuron-level relevance at the feature layer. Specifically, we cluster
neuron-level relevance for each in-distribution (ID) class to form
representative centroids and introduce a relevance distance metric to quantify
a new sample's deviation from these centroids, enhancing OOD separability.
Additionally, we refine performance by incorporating scaled relevance in the
bias term and combining feature norms. Our framework also enables explainable
OOD detection. We validate its effectiveness across multiple deep learning
architectures on the gastrointestinal imaging benchmarks Kvasir and
GastroVision, achieving improvements over state-of-the-art OOD detection
methods.

</details>


### [46] [Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material](https://arxiv.org/abs/2506.15442)
*Team Hunyuan3D,Shuhui Yang,Mingxin Yang,Yifei Feng,Xin Huang,Sheng Zhang,Zebin He,Di Luo,Haolin Liu,Yunfei Zhao,Qingxiang Lin,Zeqiang Lai,Xianghui Yang,Huiwen Shi,Zibo Zhao,Bowen Zhang,Hongyu Yan,Lifu Wang,Sicong Liu,Jihong Zhang,Meng Chen,Liang Dong,Yiwen Jia,Yulin Cai,Jiaao Yu,Yixuan Tang,Dongyuan Guo,Junlin Yu,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Shida Wei,Chao Zhang,Yonghao Tan,Yifu Sun,Lin Niu,Shirui Huang,Bojian Zheng,Shu Liu,Shilin Chen,Xiang Yuan,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Jingwei Huang,Chunchao Guo*

Main category: cs.CV

TL;DR: Hunyuan3D 2.1教程提供了处理3D数据、训练生成模型及评估性能的全面指南，适合游戏、VR和工业设计应用。


<details>
  <summary>Details</summary>
Motivation: 3D AIGC领域复杂且难以普及，Hunyuan3D 2.1旨在简化3D模型的生成流程。

Method: 系统包含Hunyuan3D-DiT（形状生成）和Hunyuan3D-Paint（纹理合成），涵盖数据准备、模型架构、训练策略等。

Result: 教程帮助用户掌握开发或微调3D生成模型的能力。

Conclusion: Hunyuan3D 2.1为3D生成提供了实用工具，降低了技术门槛。

Abstract: 3D AI-generated content (AIGC) is a passionate field that has significantly
accelerated the creation of 3D models in gaming, film, and design. Despite the
development of several groundbreaking models that have revolutionized 3D
generation, the field remains largely accessible only to researchers,
developers, and designers due to the complexities involved in collecting,
processing, and training 3D models. To address these challenges, we introduce
Hunyuan3D 2.1 as a case study in this tutorial. This tutorial offers a
comprehensive, step-by-step guide on processing 3D data, training a 3D
generative model, and evaluating its performance using Hunyuan3D 2.1, an
advanced system for producing high-resolution, textured 3D assets. The system
comprises two core components: the Hunyuan3D-DiT for shape generation and the
Hunyuan3D-Paint for texture synthesis. We will explore the entire workflow,
including data preparation, model architecture, training strategies, evaluation
metrics, and deployment. By the conclusion of this tutorial, you will have the
knowledge to finetune or develop a robust 3D generative model suitable for
applications in gaming, virtual reality, and industrial design.

</details>


### [47] [Multimodal Large Language Models for Medical Report Generation via Customized Prompt Tuning](https://arxiv.org/abs/2506.15477)
*Chunlei Li,Jingyang Hou,Yilei Shi,Jingliang Hu,Xiao Xiang Zhu,Lichao Mou*

Main category: cs.CV

TL;DR: MRG-LLM是一种新型多模态大语言模型，通过动态提示定制机制结合视觉编码器，在医学报告生成任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据生成报告在临床实践中具有挑战性，现有大语言模型与医学影像数据的结合需要深入探索。

Method: 提出MRG-LLM模型，结合冻结的LLM和可学习的视觉编码器，引入动态提示定制机制，通过视觉特征生成实例特定的提示。

Result: 在IU X-ray和MIMIC-CXR数据集上的实验表明，MRG-LLM在医学报告生成任务中表现最优。

Conclusion: MRG-LLM通过动态提示定制机制有效整合了医学影像数据与大语言模型，为临床实践提供了高效解决方案。

Abstract: Medical report generation from imaging data remains a challenging task in
clinical practice. While large language models (LLMs) show great promise in
addressing this challenge, their effective integration with medical imaging
data still deserves in-depth exploration. In this paper, we present MRG-LLM, a
novel multimodal large language model (MLLM) that combines a frozen LLM with a
learnable visual encoder and introduces a dynamic prompt customization
mechanism. Our key innovation lies in generating instance-specific prompts
tailored to individual medical images through conditional affine
transformations derived from visual features. We propose two implementations:
prompt-wise and promptbook-wise customization, enabling precise and targeted
report generation. Extensive experiments on IU X-ray and MIMIC-CXR datasets
demonstrate that MRG-LLM achieves state-of-the-art performance in medical
report generation. Our code will be made publicly available.

</details>


### [48] [GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects](https://arxiv.org/abs/2506.15483)
*Shujia Li,Haiyu Zhang,Xinyuan Chen,Yaohui Wang,Yutong Ban*

Main category: cs.CV

TL;DR: GenHOI框架通过两阶段方法实现未见物体的泛化和高质量4D人-物交互序列生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法在4D人-物交互（HOI）合成中面临大规模数据集不足的挑战。

Method: 使用Object-AnchorNet重建稀疏3D HOI关键帧，再通过ContactDM插值生成4D序列。

Result: 在OMOMO和3D-FUTURE数据集上达到SOTA，展示了对未见物体的强泛化能力。

Conclusion: GenHOI框架有效减少对大规模4D HOI数据集的依赖，并实现高质量4D HOI生成。

Abstract: While diffusion models and large-scale motion datasets have advanced
text-driven human motion synthesis, extending these advances to 4D human-object
interaction (HOI) remains challenging, mainly due to the limited availability
of large-scale 4D HOI datasets. In our study, we introduce GenHOI, a novel
two-stage framework aimed at achieving two key objectives: 1) generalization to
unseen objects and 2) the synthesis of high-fidelity 4D HOI sequences. In the
initial stage of our framework, we employ an Object-AnchorNet to reconstruct
sparse 3D HOI keyframes for unseen objects, learning solely from 3D HOI
datasets, thereby mitigating the dependence on large-scale 4D HOI datasets.
Subsequently, we introduce a Contact-Aware Diffusion Model (ContactDM) in the
second stage to seamlessly interpolate sparse 3D HOI keyframes into densely
temporally coherent 4D HOI sequences. To enhance the quality of generated 4D
HOI sequences, we propose a novel Contact-Aware Encoder within ContactDM to
extract human-object contact patterns and a novel Contact-Aware HOI Attention
to effectively integrate the contact signals into diffusion models.
Experimental results show that we achieve state-of-the-art results on the
publicly available OMOMO and 3D-FUTURE datasets, demonstrating strong
generalization abilities to unseen objects, while enabling high-fidelity 4D HOI
generation.

</details>


### [49] [NTIRE 2025 Image Shadow Removal Challenge Report](https://arxiv.org/abs/2506.15524)
*Florin-Alexandru Vasluianu,Tim Seizinger,Zhuyun Zhou,Cailian Chen,Zongwei Wu,Radu Timofte,Mingjia Li,Jin Hu,Hainuo Wang,Hengxing Liu,Jiarui Wang,Qiming Hu,Xiaojie Guo,Xin Lu,Jiarong Yang,Yuanfei Bao,Anya Hu,Zihao Fan,Kunyu Wang,Jie Xiao,Xi Wang,Xueyang Fu,Zheng-Jun Zha,Yu-Fan Lin,Chia-Ming Lee,Chih-Chung Hsu,Xingbo Wang,Dong Li,Yuxu Chen,Bin Chen,Yuanbo Zhou,Yuanbin Chen,Hongwei Wang,Jiannan Lin,Qinquan Gao,Tong Tong,Zhao Zhang,Yanyan Wei,Wei Dong,Han Zhou,Seyed Amirreza Mousavi,Jun Chen,Haobo Liang,Jiajie Jing,Junyu Li,Yan Yang,Seoyeon Lee,Chaewon Kim,Ziyu Feng,Shidi Chen,Bowen Luan,Zewen Chen,Vijayalaxmi Ashok Aralikatti,G Gyaneshwar Rao,Nikhil Akalwadi,Chaitra Desai,Ramesh Ashok Tabib,Uma Mudenagudi,Anas M. Ali,Bilel Benjdira,Wadii Boulila,Alexandru Brateanu,Cosmin Ancuti,Tanmay Chaturvedi,Manish Kumar,Anmol Srivastav,Daksh Trivedi,Shashwat Thakur,Kishor Upla,Zeyu Xiao,Zhuoyuan Li,Boda Zhou,Shashank Shekhar,Kele Xu,Qisheng Xu,Zijian Gao,Tianjiao Wan,Suiyi Zhao,Bo Wang,Yan Luo,Mingshen Wang,Yilin Zhang*

Main category: cs.CV

TL;DR: NTIRE 2025阴影去除挑战赛的结果分析，包括两个评估赛道：重建保真度和视觉感知。


<details>
  <summary>Details</summary>
Motivation: 研究阴影去除技术的最新进展，并通过挑战赛评估不同团队的表现。

Method: 使用WSRD+数据集，模拟自阴影和投射阴影的交互，评估17个团队的解决方案。

Result: 306名参与者注册，17个团队成功提交解决方案，分别在重建保真度和视觉感知赛道进行评估。

Conclusion: 挑战赛展示了阴影去除技术的多样性，并提供了评估标准的新视角。

Abstract: This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A
total of 306 participants have registered, with 17 teams successfully
submitting their solutions during the final evaluation phase. Following the
last two editions, this challenge had two evaluation tracks: one focusing on
reconstruction fidelity and the other on visual perception through a user
study. Both tracks were evaluated with images from the WSRD+ dataset,
simulating interactions between self- and cast-shadows with a large number of
diverse objects, textures, and materials.

</details>


### [50] [CLAIM: Clinically-Guided LGE Augmentation for Realistic and Diverse Myocardial Scar Synthesis and Segmentation](https://arxiv.org/abs/2506.15549)
*Farheen Ramzan,Yusuf Kiberu,Nikesh Jathanna,Shahnaz Jamil-Copley,Richard H. Clayton,Chen,Chen*

Main category: cs.CV

TL;DR: CLAIM框架通过临床引导的LGE增强和联合训练策略，实现了心肌疤痕的逼真合成和精确分割。


<details>
  <summary>Details</summary>
Motivation: 解决LGE图像稀缺和标签质量不一的问题，提升心肌疤痕分割模型的鲁棒性。

Method: 结合SMILE模块（基于临床知识的疤痕生成）和联合训练策略，生成解剖学一致的疤痕图像并优化分割网络。

Result: CLAIM生成的疤痕图案解剖学一致，与真实疤痕分布的Dice相似度高于基线模型。

Conclusion: CLAIM框架在心肌疤痕合成和分割任务中表现出色，对下游医学影像任务具有实用价值。

Abstract: Deep learning-based myocardial scar segmentation from late gadolinium
enhancement (LGE) cardiac MRI has shown great potential for accurate and timely
diagnosis and treatment planning for structural cardiac diseases. However, the
limited availability and variability of LGE images with high-quality scar
labels restrict the development of robust segmentation models. To address this,
we introduce CLAIM: \textbf{C}linically-Guided \textbf{L}GE
\textbf{A}ugmentation for Real\textbf{i}stic and Diverse \textbf{M}yocardial
Scar Synthesis and Segmentation framework, a framework for anatomically
grounded scar generation and segmentation. At its core is the SMILE module
(Scar Mask generation guided by cLinical knowledgE), which conditions a
diffusion-based generator on the clinically adopted AHA 17-segment model to
synthesize images with anatomically consistent and spatially diverse scar
patterns. In addition, CLAIM employs a joint training strategy in which the
scar segmentation network is optimized alongside the generator, aiming to
enhance both the realism of synthesized scars and the accuracy of the scar
segmentation performance. Experimental results show that CLAIM produces
anatomically coherent scar patterns and achieves higher Dice similarity with
real scar distributions compared to baseline models. Our approach enables
controllable and realistic myocardial scar synthesis and has demonstrated
utility for downstream medical imaging task.

</details>


### [51] [RaCalNet: Radar Calibration Network for Sparse-Supervised Metric Depth Estimation](https://arxiv.org/abs/2506.15560)
*Xingrui Qin,Wentao Zhao,Chuan Cao,Yihe Niu,Houcheng Jiang,Jingchuan Wang*

Main category: cs.CV

TL;DR: RaCalNet是一种无需密集监督的毫米波雷达深度估计框架，通过稀疏LiDAR监督实现高精度深度预测。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖密集LiDAR监督，成本高且数据密集，RaCalNet旨在解决这一问题。

Method: 通过稀疏雷达点重新校准和细化构建深度先验，引导单目深度预测。

Result: 在ZJU-4DRadarCam数据集上，RMSE分别降低35.30%和34.89%。

Conclusion: RaCalNet在稀疏监督下优于密集监督方法，生成具有清晰轮廓和细节的深度图。

Abstract: Dense metric depth estimation using millimeter-wave radar typically requires
dense LiDAR supervision, generated via multi-frame projection and
interpolation, to guide the learning of accurate depth from sparse radar
measurements and RGB images. However, this paradigm is both costly and
data-intensive. To address this, we propose RaCalNet, a novel framework that
eliminates the need for dense supervision by using sparse LiDAR to supervise
the learning of refined radar measurements, resulting in a supervision density
of merely around 1% compared to dense-supervised methods. Unlike previous
approaches that associate radar points with broad image regions and rely
heavily on dense labels, RaCalNet first recalibrates and refines sparse radar
points to construct accurate depth priors. These priors then serve as reliable
anchors to guide monocular depth prediction, enabling metric-scale estimation
without resorting to dense supervision. This design improves structural
consistency and preserves fine details. Despite relying solely on sparse
supervision, RaCalNet surpasses state-of-the-art dense-supervised methods,
producing depth maps with clear object contours and fine-grained textures.
Extensive experiments on the ZJU-4DRadarCam dataset and real-world deployment
scenarios demonstrate its effectiveness, reducing RMSE by 35.30% and 34.89%,
respectively.

</details>


### [52] [Control and Realism: Best of Both Worlds in Layout-to-Image without Training](https://arxiv.org/abs/2506.15563)
*Bonan Li,Yinhan Hu,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: WinWinLay是一种无需训练的布局到图像生成方法，通过非局部注意力能量函数和自适应更新策略，提升了控制精度和图像真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在布局到图像生成中存在定位不精确和伪影问题，WinWinLay旨在解决这些问题。

Method: 提出非局部注意力能量函数和基于Langevin动力学的自适应更新方案。

Result: 实验表明WinWinLay在元素布局控制和图像真实性上优于现有方法。

Conclusion: WinWinLay通过理论分析和创新策略，显著提升了布局到图像生成的质量。

Abstract: Layout-to-Image generation aims to create complex scenes with precise control
over the placement and arrangement of subjects. Existing works have
demonstrated that pre-trained Text-to-Image diffusion models can achieve this
goal without training on any specific data; however, they often face challenges
with imprecise localization and unrealistic artifacts. Focusing on these
drawbacks, we propose a novel training-free method, WinWinLay. At its core,
WinWinLay presents two key strategies, Non-local Attention Energy Function and
Adaptive Update, that collaboratively enhance control precision and realism. On
one hand, we theoretically demonstrate that the commonly used attention energy
function introduces inherent spatial distribution biases, hindering objects
from being uniformly aligned with layout instructions. To overcome this issue,
non-local attention prior is explored to redistribute attention scores,
facilitating objects to better conform to the specified spatial conditions. On
the other hand, we identify that the vanilla backpropagation update rule can
cause deviations from the pre-trained domain, leading to out-of-distribution
artifacts. We accordingly introduce a Langevin dynamics-based adaptive update
scheme as a remedy that promotes in-domain updating while respecting layout
constraints. Extensive experiments demonstrate that WinWinLay excels in
controlling element placement and achieving photorealistic visual fidelity,
outperforming the current state-of-the-art methods.

</details>


### [53] [Show-o2: Improved Native Unified Multimodal Models](https://arxiv.org/abs/2506.15564)
*Jinheng Xie,Zhenheng Yang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: Show-o2是一种改进的统一多模态模型，结合自回归建模和流匹配技术，支持图像和视频的多模态理解与生成。


<details>
  <summary>Details</summary>
Motivation: 旨在构建一个统一的多模态模型，能够同时处理文本、图像和视频等多种模态的任务。

Method: 基于3D因果变分自编码器空间，通过双路径时空融合构建统一视觉表示，并采用两阶段训练方法。

Result: Show-o2模型在多模态理解和生成任务中表现出色，支持多种模态的灵活处理。

Conclusion: Show-o2展示了统一多模态模型的潜力，为多模态任务提供了高效的解决方案。

Abstract: This paper presents improved native unified multimodal models, \emph{i.e.,}
Show-o2, that leverage autoregressive modeling and flow matching. Built upon a
3D causal variational autoencoder space, unified visual representations are
constructed through a dual-path of spatial (-temporal) fusion, enabling
scalability across image and video modalities while ensuring effective
multimodal understanding and generation. Based on a language model,
autoregressive modeling and flow matching are natively applied to the language
head and flow head, respectively, to facilitate text token prediction and
image/video generation. A two-stage training recipe is designed to effectively
learn and scale to larger models. The resulting Show-o2 models demonstrate
versatility in handling a wide range of multimodal understanding and generation
tasks across diverse modalities, including text, images, and videos. Code and
models are released at https://github.com/showlab/Show-o.

</details>


### [54] [Baltimore Atlas: FreqWeaver Adapter for Semi-supervised Ultra-high Spatial Resolution Land Cover Classification](https://arxiv.org/abs/2506.15565)
*Junhao Wu,Aboagye-Ntow Stephen,Chuyuan Wang,Gang Chen,Xin Huang*

Main category: cs.CV

TL;DR: 提出了一种参数高效的半监督分割框架，用于0.3米空间分辨率影像，结合SAM2知识和遥感专用FreqWeaver Adapter，在仅5.96%参数量的情况下提升细粒度细节建模能力。


<details>
  <summary>Details</summary>
Motivation: 超高空间分辨率土地覆盖分类在细粒度分析中至关重要，但面临像素级标注成本高、尺度变化大和大规模视觉模型适应性有限等挑战。现有方法多依赖标注数据且适用于1米分辨率影像，而实际应用常需在弱监督下处理更高分辨率影像。

Method: 提出参数高效的半监督分割框架，结合SAM2知识并引入遥感专用FreqWeaver Adapter，增强细粒度细节建模，同时保持轻量设计（仅5.96%参数量）。

Result: 通过有效利用未标注数据并保持低参数量，该方法在结构一致性上表现优异，比现有参数高效调优策略提升1.78%，比最先进高分辨率遥感分割方法提升3.44%。

Conclusion: 该方法在超高分辨率土地覆盖分类中实现了高效且鲁棒的分割结果，为实际应用提供了可行的解决方案。

Abstract: Ultra-high Spatial Resolution Land Cover Classification is essential for
fine-grained land cover analysis, yet it remains challenging due to the high
cost of pixel-level annotations, significant scale variation, and the limited
adaptability of large-scale vision models. Existing methods typically focus on
1-meter spatial resolution imagery and rely heavily on annotated data, whereas
practical applications often require processing higher-resolution imagery under
weak supervision. To address this, we propose a parameter-efficient
semi-supervised segmentation framework for 0.3 m spatial resolution imagery,
which leverages the knowledge of SAM2 and introduces a remote sensing-specific
FreqWeaver Adapter to enhance fine-grained detail modeling while maintaining a
lightweight design at only 5.96% of the total model parameters. By effectively
leveraging unlabeled data and maintaining minimal parameter overhead, the
proposed method delivers robust segmentation results with superior structural
consistency, achieving a 1.78% improvement over existing parameter-efficient
tuning strategies and a 3.44% gain compared to state-of-the-art high-resolution
remote sensing segmentation approaches.

</details>


### [55] [A Unified Graph-based Framework for Scalable 3D Tree Reconstruction and Non-Destructive Biomass Estimation from Point Clouds](https://arxiv.org/abs/2506.15577)
*Di Wang,Shi Li*

Main category: cs.CV

TL;DR: 提出了一种基于图的新型统一框架，用于大规模点云的端到端处理，显著提高了森林地上生物量（AGB）估计的可行性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前定量结构模型（QSM）方法存在局限性，如依赖高质量点云数据、需要多个预处理步骤，难以扩展和实际部署。

Method: 通过创新的基于图的流程，无缝集成树分割、叶木分离和3D骨架重建，利用路径和抽象等图操作进行树拓扑推理。

Result: 在多种条件下（如叶覆盖、空间尺度和数据源）验证，表现出色，特别是在叶覆盖场景（约20%相对误差）和低密度ULS数据集（约30%相对误差）。

Conclusion: 该框架为大规模、非破坏性AGB估计提供了稳健且可扩展的解决方案，减少了对专业预处理工具的依赖，并确立了ULS作为TLS的可行替代方案。

Abstract: Estimating forest above-ground biomass (AGB) is crucial for assessing carbon
storage and supporting sustainable forest management. Quantitative Structural
Model (QSM) offers a non-destructive approach to AGB estimation through 3D tree
structural reconstruction. However, current QSM methods face significant
limitations, as they are primarily designed for individual trees,depend on
high-quality point cloud data from terrestrial laser scanning (TLS), and also
require multiple pre-processing steps that hinder scalability and practical
deployment. This study presents a novel unified framework that enables
end-to-end processing of large-scale point clouds using an innovative
graph-based pipeline. The proposed approach seamlessly integrates tree
segmentation,leaf-wood separation and 3D skeletal reconstruction through
dedicated graph operations including pathing and abstracting for tree topology
reasoning. Comprehensive validation was conducted on datasets with varying leaf
conditions (leaf-on and leaf-off), spatial scales (tree- and plot-level), and
data sources (TLS and UAV-based laser scanning, ULS). Experimental results
demonstrate strong performance under challenging conditions, particularly in
leaf-on scenarios (~20% relative error) and low-density ULS datasets with
partial coverage (~30% relative error). These findings indicate that the
proposed framework provides a robust and scalable solution for large-scale,
non-destructive AGB estimation. It significantly reduces dependency on
specialized pre-processing tools and establishes ULS as a viable alternative to
TLS. To our knowledge, this is the first method capable of enabling seamless,
end-to-end 3D tree reconstruction at operational scales. This advancement
substantially improves the feasibility of QSM-based AGB estimation, paving the
way for broader applications in forest inventory and climate change research.

</details>


### [56] [One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution](https://arxiv.org/abs/2506.15591)
*Yujing Sun,Lingchen Sun,Shuaizheng Liu,Rongyuan Wu,Zhengqiang Zhang,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为DLoRAL的双LoRA学习范式，用于在视频超分辨率任务中同时实现细节增强和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于稳定扩散的Real-VSR方法在保持时间一致性时牺牲了空间细节，导致视觉质量不佳。

Method: 通过交叉帧检索模块和一致性-LoRA学习时间表示，再训练细节-LoRA增强细节，交替优化。

Result: 实验表明DLoRAL在精度和速度上均表现优异。

Conclusion: DLoRAL能够高效且高质量地实现视频恢复，同时保持细节和时间一致性。

Abstract: It is a challenging problem to reproduce rich spatial details while
maintaining temporal consistency in real-world video super-resolution
(Real-VSR), especially when we leverage pre-trained generative models such as
stable diffusion (SD) for realistic details synthesis. Existing SD-based
Real-VSR methods often compromise spatial details for temporal coherence,
resulting in suboptimal visual quality. We argue that the key lies in how to
effectively extract the degradation-robust temporal consistency priors from the
low-quality (LQ) input video and enhance the video details while maintaining
the extracted consistency priors. To achieve this, we propose a Dual LoRA
Learning (DLoRAL) paradigm to train an effective SD-based one-step diffusion
model, achieving realistic frame details and temporal consistency
simultaneously. Specifically, we introduce a Cross-Frame Retrieval (CFR) module
to aggregate complementary information across frames, and train a
Consistency-LoRA (C-LoRA) to learn robust temporal representations from
degraded inputs. After consistency learning, we fix the CFR and C-LoRA modules
and train a Detail-LoRA (D-LoRA) to enhance spatial details while aligning with
the temporal space defined by C-LoRA to keep temporal coherence. The two phases
alternate iteratively for optimization, collaboratively delivering consistent
and detail-rich outputs. During inference, the two LoRA branches are merged
into the SD model, allowing efficient and high-quality video restoration in a
single diffusion step. Experiments show that DLoRAL achieves strong performance
in both accuracy and speed. Code and models are available at
https://github.com/yjsunnn/DLoRAL.

</details>


### [57] [Mono-Modalizing Extremely Heterogeneous Multi-Modal Medical Image Registration](https://arxiv.org/abs/2506.15596)
*Kyobin Choo,Hyunkyung Han,Jinyeong Kim,Chanyong Yoon,Seong Jae Hwang*

Main category: cs.CV

TL;DR: 提出了一种名为M2M-Reg的新框架，用于解决多模态图像配准中因模态差异大导致的配准困难问题，通过单模态相似性训练多模态配准模型，并引入GradCyCon正则化器提升配准效果。


<details>
  <summary>Details</summary>
Motivation: 由于功能成像模态（如PET、FA）与结构参考（如MRI、CT）差异极大，传统无监督配准方法难以学习可靠的空间映射，导致图像失真。

Method: 提出M2M-Reg框架，利用单模态相似性训练多模态配准模型，并引入GradCyCon正则化器促进微分同胚性。框架还支持半监督学习，无需真实变换或分割掩码。

Result: 在ADNI数据集上，M2M-Reg在PET-MRI和FA-MRI配准中DSC指标比现有方法提升高达2倍。

Conclusion: M2M-Reg能有效处理高度异质的多模态图像配准问题，且易于集成到现有模型中。

Abstract: In clinical practice, imaging modalities with functional characteristics,
such as positron emission tomography (PET) and fractional anisotropy (FA), are
often aligned with a structural reference (e.g., MRI, CT) for accurate
interpretation or group analysis, necessitating multi-modal deformable image
registration (DIR). However, due to the extreme heterogeneity of these
modalities compared to standard structural scans, conventional unsupervised DIR
methods struggle to learn reliable spatial mappings and often distort images.
We find that the similarity metrics guiding these models fail to capture
alignment between highly disparate modalities. To address this, we propose
M2M-Reg (Multi-to-Mono Registration), a novel framework that trains multi-modal
DIR models using only mono-modal similarity while preserving the established
architectural paradigm for seamless integration into existing models. We also
introduce GradCyCon, a regularizer that leverages M2M-Reg's cyclic training
scheme to promote diffeomorphism. Furthermore, our framework naturally extends
to a semi-supervised setting, integrating pre-aligned and unaligned pairs only,
without requiring ground-truth transformations or segmentation masks.
Experiments on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset
demonstrate that M2M-Reg achieves up to 2x higher DSC than prior methods for
PET-MRI and FA-MRI registration, highlighting its effectiveness in handling
highly heterogeneous multi-modal DIR. Our code is available at
https://github.com/MICV-yonsei/M2M-Reg.

</details>


### [58] [BoxFusion: Reconstruction-Free Open-Vocabulary 3D Object Detection via Real-Time Multi-View Box Fusion](https://arxiv.org/abs/2506.15610)
*Yuqing Lan,Chenyang Zhu,Zhirui Gao,Jiazhao Zhang,Yihan Cao,Renjiao Yi,Yijie Wang,Kai Xu*

Main category: cs.CV

TL;DR: 提出了一种无需重建的实时3D物体检测框架，结合预训练视觉模型和CLIP，通过多视图关联和优化模块实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有3D检测方法依赖密集点云重建，计算和内存开销大，难以实时部署。

Method: 利用Cubify Anything和CLIP进行单视图检测，通过关联和优化模块融合多视图结果。

Result: 在ScanNetV2和CA-1M数据集上达到SOTA性能，支持1000平方米以上环境的实时感知。

Conclusion: 无需重建的框架在实时性和泛化能力上表现优异，适用于大规模场景。

Abstract: Open-vocabulary 3D object detection has gained significant interest due to
its critical applications in autonomous driving and embodied AI. Existing
detection methods, whether offline or online, typically rely on dense point
cloud reconstruction, which imposes substantial computational overhead and
memory constraints, hindering real-time deployment in downstream tasks. To
address this, we propose a novel reconstruction-free online framework tailored
for memory-efficient and real-time 3D detection. Specifically, given streaming
posed RGB-D video input, we leverage Cubify Anything as a pre-trained visual
foundation model (VFM) for single-view 3D object detection by bounding boxes,
coupled with CLIP to capture open-vocabulary semantics of detected objects. To
fuse all detected bounding boxes across different views into a unified one, we
employ an association module for correspondences of multi-views and an
optimization module to fuse the 3D bounding boxes of the same instance
predicted in multi-views. The association module utilizes 3D Non-Maximum
Suppression (NMS) and a box correspondence matching module, while the
optimization module uses an IoU-guided efficient random optimization technique
based on particle filtering to enforce multi-view consistency of the 3D
bounding boxes while minimizing computational complexity. Extensive experiments
on ScanNetV2 and CA-1M datasets demonstrate that our method achieves
state-of-the-art performance among online methods. Benefiting from this novel
reconstruction-free paradigm for 3D object detection, our method exhibits great
generalization abilities in various scenarios, enabling real-time perception
even in environments exceeding 1000 square meters.

</details>


### [59] [HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization](https://arxiv.org/abs/2506.15625)
*Roey Ron,Guy Tevet,Haim Sawdayee,Amit H. Bermano*

Main category: cs.CV

TL;DR: HOIDiNi是一个基于文本驱动的扩散框架，用于生成真实且合理的人-物交互（HOI），通过优化噪声空间实现高接触精度和自然运动。


<details>
  <summary>Details</summary>
Motivation: 解决人-物交互生成中现实性与物理正确性之间的权衡问题，提出一种既能保证接触精度又能保持运动自然性的方法。

Method: 采用扩散噪声优化（DNO）在预训练扩散模型的噪声空间中直接优化，将问题分为物体中心阶段（选择手-物接触位置）和人体中心阶段（细化全身运动）。

Result: 在GRAB数据集上的定量、定性和主观评估表明，HOIDiNi在接触精度、物理有效性和整体质量上优于现有方法。

Conclusion: HOIDiNi能够生成复杂且可控的交互动作，仅通过文本提示驱动，展示了其在人-物交互生成中的潜力。

Abstract: We present HOIDiNi, a text-driven diffusion framework for synthesizing
realistic and plausible human-object interaction (HOI). HOI generation is
extremely challenging since it induces strict contact accuracies alongside a
diverse motion manifold. While current literature trades off between realism
and physical correctness, HOIDiNi optimizes directly in the noise space of a
pretrained diffusion model using Diffusion Noise Optimization (DNO), achieving
both. This is made feasible thanks to our observation that the problem can be
separated into two phases: an object-centric phase, primarily making discrete
choices of hand-object contact locations, and a human-centric phase that
refines the full-body motion to realize this blueprint. This structured
approach allows for precise hand-object contact without compromising motion
naturalness. Quantitative, qualitative, and subjective evaluations on the GRAB
dataset alone clearly indicate HOIDiNi outperforms prior works and baselines in
contact accuracy, physical validity, and overall quality. Our results
demonstrate the ability to generate complex, controllable interactions,
including grasping, placing, and full-body coordination, driven solely by
textual prompts. https://hoidini.github.io.

</details>


### [60] [FindingDory: A Benchmark to Evaluate Memory in Embodied Agents](https://arxiv.org/abs/2506.15635)
*Karmesh Yadav,Yusuf Ali,Gunshi Gupta,Yarin Gal,Zsolt Kira*

Main category: cs.CV

TL;DR: 论文提出了一个新的基准测试，用于评估大型视觉语言模型在长期记忆和推理任务中的表现，特别是在机器人控制任务中。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉语言模型在处理长期记忆和大量图像时存在局限性，且现有基准测试未能充分体现机器人控制任务中的低层技能和细粒度推理需求。

Method: 在Habitat模拟器中引入了一个新的基准测试，包含60个需要长期记忆和上下文感知的任务，并提出了结合先进视觉语言模型和低层导航策略的基线方法。

Result: 基准测试能够评估模型在长期记忆任务中的表现，并展示了现有模型的改进空间。

Conclusion: 该基准测试为评估和改进大型视觉语言模型在机器人控制任务中的长期记忆和推理能力提供了重要工具。

Abstract: Large vision-language models have recently demonstrated impressive
performance in planning and control tasks, driving interest in their
application to real-world robotics. However, deploying these models for
reasoning in embodied contexts is limited by their ability to incorporate
long-term experience collected across multiple days and represented by vast
collections of images. Current VLMs typically struggle to process more than a
few hundred images concurrently, highlighting the need for more efficient
mechanisms to handle long-term memory in embodied settings. To effectively
evaluate these models for long-horizon control, a benchmark must specifically
target scenarios where memory is crucial for success. Existing long-video QA
benchmarks overlook embodied challenges like object manipulation and
navigation, which demand low-level skills and fine-grained reasoning over past
interactions. Moreover, effective memory integration in embodied agents
involves both recalling relevant historical information and executing actions
based on that information, making it essential to study these aspects together
rather than in isolation. In this work, we introduce a new benchmark for
long-range embodied tasks in the Habitat simulator. This benchmark evaluates
memory-based capabilities across 60 tasks requiring sustained engagement and
contextual awareness in an environment. The tasks can also be procedurally
extended to longer and more challenging versions, enabling scalable evaluation
of memory and reasoning. We also present baselines that integrate
state-of-the-art VLMs with low level navigation policies, assessing their
performance on these memory-intensive tasks and highlight areas for
improvement.

</details>


### [61] [Demystifying the Visual Quality Paradox in Multimodal Large Language Models](https://arxiv.org/abs/2506.15645)
*Shuo Xing,Lanqing Guo,Hongyuan Hua,Seoyoung Lee,Peiran Li,Yufei Wang,Zhangyang Wang,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 研究发现，多模态大语言模型（MLLMs）在视觉质量与性能之间存在矛盾，高质量图像未必带来更好表现。提出轻量级适配模块VQ-TTT，动态调整输入图像以提升性能。


<details>
  <summary>Details</summary>
Motivation: 探索视觉输入质量对MLLMs性能的影响，发现传统高质量图像未必最优，需适配模型偏好。

Method: 引入VQ-TTT模块，通过可学习低秩核和LoRA微调浅层视觉编码器，动态调整输入图像。

Result: VQ-TTT显著提升所有数据集和MLLMs的平均准确率，无需额外数据或模型。

Conclusion: MLLMs需要适应性而非普遍“干净”的图像，VQ-TTT为这一需求提供了高效解决方案。

Abstract: Recent Multimodal Large Language Models (MLLMs) excel on benchmark
vision-language tasks, yet little is known about how input visual quality
shapes their responses. Does higher perceptual quality of images already
translate to better MLLM understanding? We conduct the first systematic study
spanning leading MLLMs and a suite of vision-language benchmarks, applying
controlled degradations and stylistic shifts to each image. Surprisingly, we
uncover a visual-quality paradox: model, task, and even individual-instance
performance can improve when images deviate from human-perceived fidelity.
Off-the-shelf restoration pipelines fail to reconcile these idiosyncratic
preferences. To close the gap, we introduce Visual-Quality Test-Time Tuning
(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,
low-rank kernel before the frozen vision encoder to modulate frequency content;
and (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT
dynamically adjusts each input image in a single forward pass, aligning it with
task-specific model preferences. Across the evaluated MLLMs and all datasets,
VQ-TTT lifts significant average accuracy, with no external models, cached
features, or extra training data. These findings redefine ``better'' visual
inputs for MLLMs and highlight the need for adaptive, rather than universally
``clean'', imagery, in the new era of AI being the main data customer.

</details>


### [62] [Dual-Stage Value-Guided Inference with Margin-Based Reward Adjustment for Fast and Faithful VLM Captioning](https://arxiv.org/abs/2506.15649)
*Ankan Deria,Adinath Madhavrao Dukre,Feilong Tang,Sara Atito,Sudipta Roy,Muhammad Awais,Muhammad Haris Khan,Imran Razzak*

Main category: cs.CV

TL;DR: ViMaR是一种两阶段推理框架，通过结合时间差分价值模型和边缘感知奖励调整，显著提高了视觉语言模型的推理效率和输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理时搜索视觉语言模型时存在计算成本高和低置信度生成的问题，导致幻觉现象。

Method: ViMaR采用两阶段方法：第一阶段选择最高价值的候选描述，第二阶段选择性优化视觉基础薄弱的片段，并通过边缘惩罚抑制低置信度生成。

Result: ViMaR生成的描述更可靠、准确且详细，速度提升4倍以上，并能跨模型泛化，提升自训练效果。

Conclusion: ViMaR是一种高效、可扩展的推理策略，显著提升了视觉语言模型的性能和泛化能力。

Abstract: Despite significant advances in inference-time search for vision-language
models (VLMs), existing approaches remain both computationally expensive and
prone to unpenalized, low-confidence generations which often lead to persistent
hallucinations. We introduce \textbf{Value-guided Inference with Margin-based
Reward (ViMaR)}, a two-stage inference framework that improves both efficiency
and output fidelity by combining a temporal-difference value model with a
margin-aware reward adjustment. In the first stage, we perform a single pass to
identify the highest-value caption among diverse candidates. In the second
stage, we selectively refine only those segments that were overlooked or
exhibit weak visual grounding, thereby eliminating frequently rewarded
evaluations. A calibrated margin-based penalty discourages low-confidence
continuations while preserving descriptive richness. Extensive experiments
across multiple VLM architectures demonstrate that ViMaR generates captions
that are significantly more reliable, factually accurate, detailed, and
explanatory, while achieving over 4$\times$ speedup compared to existing
value-guided methods. Specifically, we show that ViMaR trained solely on LLaVA
Mistral-7B, \textit{generalizes effectively to guide decoding in a stronger
unseen model}. To further validate this, we adapt the ViMaR to steer generation
in LLaVA-OneVision-Qwen2-7B, leading to consistent improvements in caption
quality and demonstrating robust cross-model guidance. This cross-model
generalization highlights ViMaR's flexibility and modularity, positioning it as
a scalable and transferable inference-time decoding strategy. Furthermore, when
ViMaR-generated captions are used for self-training, the underlying models
achieve substantial gains across a broad suite of visual comprehension
benchmarks, underscoring the potential of fast, accurate, and self-improving
VLM pipelines.

</details>


### [63] [UniRelight: Learning Joint Decomposition and Synthesis for Video Relighting](https://arxiv.org/abs/2506.15673)
*Kai He,Ruofan Liang,Jacob Munkberg,Jon Hasselgren,Nandita Vijaykumar,Alexander Keller,Sanja Fidler,Igor Gilitschenski,Zan Gojcic,Zian Wang*

Main category: cs.CV

TL;DR: 提出了一种联合估计反照率并合成重光照输出的单步方法，利用视频扩散模型的生成能力，提升了场景理解和光照效果的真实性。


<details>
  <summary>Details</summary>
Motivation: 现有端到端重光照模型受限于配对多光照数据的稀缺性，难以泛化到多样场景；而两阶段方法易产生误差累积，难以处理复杂光照或材质。

Method: 通过视频扩散模型联合估计反照率和合成重光照输出，利用合成多光照数据和自动标注的真实视频进行训练。

Result: 模型在多样领域表现出强泛化能力，在视觉保真度和时间一致性上超越先前方法。

Conclusion: 该方法通过单步联合优化，显著提升了重光照任务的效果和泛化能力。

Abstract: We address the challenge of relighting a single image or video, a task that
demands precise scene intrinsic understanding and high-quality light transport
synthesis. Existing end-to-end relighting models are often limited by the
scarcity of paired multi-illumination data, restricting their ability to
generalize across diverse scenes. Conversely, two-stage pipelines that combine
inverse and forward rendering can mitigate data requirements but are
susceptible to error accumulation and often fail to produce realistic outputs
under complex lighting conditions or with sophisticated materials. In this
work, we introduce a general-purpose approach that jointly estimates albedo and
synthesizes relit outputs in a single pass, harnessing the generative
capabilities of video diffusion models. This joint formulation enhances
implicit scene comprehension and facilitates the creation of realistic lighting
effects and intricate material interactions, such as shadows, reflections, and
transparency. Trained on synthetic multi-illumination data and extensive
automatically labeled real-world videos, our model demonstrates strong
generalization across diverse domains and surpasses previous methods in both
visual fidelity and temporal consistency.

</details>


### [64] [Sekai: A Video Dataset towards World Exploration](https://arxiv.org/abs/2506.15675)
*Zhen Li,Chuanhao Li,Xiaofeng Mao,Shaoheng Lin,Ming Li,Shitian Zhao,Zhaopan Xu,Xinyue Li,Yukang Feng,Jianwen Sun,Zizhen Li,Fanrui Zhang,Jiaxin Ai,Zhixiang Wang,Yuwei Wu,Tong He,Jiangmiao Pang,Yu Qiao,Yunde Jia,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Sekai是一个高质量的第一人称视角全球视频数据集，用于世界探索训练，包含5000多小时视频和丰富标注。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成数据集不适合世界探索训练，存在地点有限、时长短、场景静态和缺乏标注等问题。

Method: 开发高效工具箱收集、预处理和标注视频，包含位置、场景、天气等信息。

Result: 实验验证数据集质量，并训练了交互式视频世界探索模型YUME。

Conclusion: Sekai将推动视频生成和世界探索领域发展，激发有价值应用。

Abstract: Video generation techniques have made remarkable progress, promising to be
the foundation of interactive world exploration. However, existing video
generation datasets are not well-suited for world exploration training as they
suffer from some limitations: limited locations, short duration, static scenes,
and a lack of annotations about exploration and the world. In this paper, we
introduce Sekai (meaning ``world'' in Japanese), a high-quality first-person
view worldwide video dataset with rich annotations for world exploration. It
consists of over 5,000 hours of walking or drone view (FPV and UVA) videos from
over 100 countries and regions across 750 cities. We develop an efficient and
effective toolbox to collect, pre-process and annotate videos with location,
scene, weather, crowd density, captions, and camera trajectories. Experiments
demonstrate the quality of the dataset. And, we use a subset to train an
interactive video world exploration model, named YUME (meaning ``dream'' in
Japanese). We believe Sekai will benefit the area of video generation and world
exploration, and motivate valuable applications.

</details>


### [65] [Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model](https://arxiv.org/abs/2506.15682)
*Anirud Aggarwal,Abhinav Shrivastava,Matthew Gwilliam*

Main category: cs.CV

TL;DR: ECAD是一种基于遗传算法的缓存调度方法，显著加速扩散模型的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型推理速度慢且计算成本高，现有方法依赖固定启发式规则，效果有限。

Method: 提出ECAD，通过遗传算法学习高效的缓存调度策略，无需修改模型参数。

Result: 在多个模型和基准测试中，ECAD在加速推理的同时提升生成质量，优于现有方法。

Conclusion: ECAD是一种可扩展且通用的扩散模型加速方法。

Abstract: Diffusion-based image generation models excel at producing high-quality
synthetic content, but suffer from slow and computationally expensive
inference. Prior work has attempted to mitigate this by caching and reusing
features within diffusion transformers across inference steps. These methods,
however, often rely on rigid heuristics that result in limited acceleration or
poor generalization across architectures. We propose Evolutionary Caching to
Accelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,
per-model, caching schedules forming a Pareto frontier, using only a small set
of calibration prompts. ECAD requires no modifications to network parameters or
reference images. It offers significant inference speedups, enables
fine-grained control over the quality-latency trade-off, and adapts seamlessly
to different diffusion models. Notably, ECAD's learned schedules can generalize
effectively to resolutions and model variants not seen during calibration. We
evaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple
metrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,
PartiPrompts), demonstrating consistent improvements over previous approaches.
On PixArt-alpha, ECAD identifies a schedule that outperforms the previous
state-of-the-art method by 4.47 COCO FID while increasing inference speedup
from 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable
approach for accelerating diffusion inference. Our project website is available
at https://aniaggarwal.github.io/ecad and our code is available at
https://github.com/aniaggarwal/ecad.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [66] [Omnidirectional Video Super-Resolution using Deep Learning](https://arxiv.org/abs/2506.14803)
*Arbind Agrahari Baniya,Tsz-Kwan Lee,Peter W. Eklund,Sunil Aryal*

Main category: cs.MM

TL;DR: 本文提出了一种针对360度视频的超分辨率方法S3PO，并创建了新的数据集360VDS，解决了传统VSR技术在360度视频中的失真问题。


<details>
  <summary>Details</summary>
Motivation: 360度视频在VR中广泛应用，但其空间分辨率有限，传统VSR技术无法解决其投影失真问题，且缺乏相关数据集。

Method: 提出S3PO模型，采用循环建模和注意力机制，设计了针对球形失真的特征提取器和损失函数。

Result: S3PO在360度视频数据集上优于传统VSR模型和特定超分辨率模型。

Conclusion: S3PO有效解决了360度视频的超分辨率问题，并通过消融研究验证了其设计有效性。

Abstract: Omnidirectional Videos (or 360{\deg} videos) are widely used in Virtual
Reality (VR) to facilitate immersive and interactive viewing experiences.
However, the limited spatial resolution in 360{\deg} videos does not allow for
each degree of view to be represented with adequate pixels, limiting the visual
quality offered in the immersive experience. Deep learning Video
Super-Resolution (VSR) techniques used for conventional videos could provide a
promising software-based solution; however, these techniques do not tackle the
distortion present in equirectangular projections of 360{\deg} video signals.
An additional obstacle is the limited availability of 360{\deg} video datasets
for study. To address these issues, this paper creates a novel 360{\deg} Video
Dataset (360VDS) with a study of the extensibility of conventional VSR models
to 360{\deg} videos. This paper further proposes a novel deep learning model
for 360{\deg} Video Super-Resolution (360{\deg} VSR), called Spherical Signal
Super-resolution with a Proportioned Optimisation (S3PO). S3PO adopts recurrent
modelling with an attention mechanism, unbound from conventional VSR techniques
like alignment. With a purpose-built feature extractor and a novel loss
function addressing spherical distortion, S3PO outperforms most
state-of-the-art conventional VSR models and 360{\deg}~specific
super-resolution models on 360{\deg} video datasets. A step-wise ablation study
is presented to understand and demonstrate the impact of the chosen
architectural sub-components, targeted training and optimisation.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [67] [An Empirical Study of Bugs in Data Visualization Libraries](https://arxiv.org/abs/2506.15084)
*Weiqi Lu,Yongqiang Tian,Xiaohan Zhong,Haoyang Ma,Zhenyang Xu,Shing-Chi Cheung,Chengnian Sun*

Main category: cs.SE

TL;DR: 该研究首次对数据可视化库中的错误进行了全面分析，总结了症状和根本原因，并提出了详细的分类法。研究发现，错误/不准确的绘图普遍存在，且图形计算错误是主要原因。此外，研究还探索了视觉语言模型（VLMs）在检测错误绘图中的可行性。


<details>
  <summary>Details</summary>
Motivation: 数据可视化库的准确性对用户体验和信息传达至关重要，但其中的视觉错误可能导致误导性决策。因此，理解这些错误的特性对检测和修复至关重要。

Method: 研究分析了来自五个广泛使用的数据可视化库的564个错误，系统性地研究了其症状和根本原因，并提出了分类法。此外，还探索了VLMs在检测错误绘图中的应用。

Result: 研究发现错误/不准确的绘图普遍存在，图形计算错误是主要原因。VLMs在检测错误绘图中的有效性在29%到57%之间，且提示信息的增加并不一定能提高效果。

Conclusion: 该研究为数据可视化库的错误检测和修复提供了重要见解，并指出了未来自动化测试技术的研究方向。

Abstract: Data visualization (DataViz) libraries play a crucial role in presentation,
data analysis, and application development, underscoring the importance of
their accuracy in transforming data into visual representations. Incorrect
visualizations can adversely impact user experience, distort information
conveyance, and influence user perception and decision-making processes. Visual
bugs in these libraries can be particularly insidious as they may not cause
obvious errors like crashes, but instead mislead users of the underlying data
graphically, resulting in wrong decision making. Consequently, a good
understanding of the unique characteristics of bugs in DataViz libraries is
essential for researchers and developers to detect and fix bugs in DataViz
libraries.
  This study presents the first comprehensive analysis of bugs in DataViz
libraries, examining 564 bugs collected from five widely-used libraries. Our
study systematically analyzes their symptoms and root causes, and provides a
detailed taxonomy. We found that incorrect/inaccurate plots are pervasive in
DataViz libraries and incorrect graphic computation is the major root cause,
which necessitates further automated testing methods for DataViz libraries.
Moreover, we identified eight key steps to trigger such bugs and two test
oracles specific to DataViz libraries, which may inspire future research in
designing effective automated testing techniques. Furthermore, with the recent
advancements in Vision Language Models (VLMs), we explored the feasibility of
applying these models to detect incorrect/inaccurate plots. The results show
that the effectiveness of VLMs in bug detection varies from 29% to 57%,
depending on the prompts, and adding more information in prompts does not
necessarily increase the effectiveness. More findings can be found in our
manuscript.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [68] [Human Motion Capture from Loose and Sparse Inertial Sensors with Garment-aware Diffusion Models](https://arxiv.org/abs/2506.15290)
*Andela Ilic,Jiaxi Jiang,Paul Streli,Xintong Liu,Christian Holz*

Main category: cs.GR

TL;DR: 提出了一种基于稀疏、松散附着IMU传感器的全身姿态估计新任务，通过模拟数据和扩散模型解决。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设IMU传感器紧密附着于人体，但实际场景中常不成立，需解决松散附着问题。

Method: 利用服装感知数据集模拟松散IMU数据，开发基于Transformer的扩散模型进行姿态估计。

Result: 实验表明，该方法在模拟和合成数据上优于现有技术。

Conclusion: 结合服装参数训练模型，提升了表达能力，为未来研究开辟了新方向。

Abstract: Motion capture using sparse inertial sensors has shown great promise due to
its portability and lack of occlusion issues compared to camera-based tracking.
Existing approaches typically assume that IMU sensors are tightly attached to
the human body. However, this assumption often does not hold in real-world
scenarios. In this paper, we present a new task of full-body human pose
estimation using sparse, loosely attached IMU sensors. To solve this task, we
simulate IMU recordings from an existing garment-aware human motion dataset. We
developed transformer-based diffusion models to synthesize loose IMU data and
estimate human poses based on this challenging loose IMU data. In addition, we
show that incorporating garment-related parameters while training the model on
simulated loose data effectively maintains expressiveness and enhances the
ability to capture variations introduced by looser or tighter garments.
Experiments show that our proposed diffusion methods trained on simulated and
synthetic data outperformed the state-of-the-art methods quantitatively and
qualitatively, opening up a promising direction for future research.

</details>


### [69] [One-shot Face Sketch Synthesis in the Wild via Generative Diffusion Prior and Instruction Tuning](https://arxiv.org/abs/2506.15312)
*Han Wu,Junyao Li,Kangbo Zhao,Sen Zhang,Yukai Shi,Liang Lin*

Main category: cs.GR

TL;DR: 提出了一种基于扩散模型的单次人脸素描合成方法，通过优化文本指令实现高效生成，并引入新基准数据集OS-Sketch。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖大量训练数据、数据稀缺和高人力成本的问题。

Method: 利用扩散模型优化文本指令，通过梯度优化生成指令进行推理。

Result: 在单次训练下生成逼真且一致的素描，优于其他方法。

Conclusion: 该方法更便捷且适用性更广，数据集将公开。

Abstract: Face sketch synthesis is a technique aimed at converting face photos into
sketches. Existing face sketch synthesis research mainly relies on training
with numerous photo-sketch sample pairs from existing datasets. However, these
large-scale discriminative learning methods will have to face problems such as
data scarcity and high human labor costs. Once the training data becomes
scarce, their generative performance significantly degrades. In this paper, we
propose a one-shot face sketch synthesis method based on diffusion models. We
optimize text instructions on a diffusion model using face photo-sketch image
pairs. Then, the instructions derived through gradient-based optimization are
used for inference. To simulate real-world scenarios more accurately and
evaluate method effectiveness more comprehensively, we introduce a new
benchmark named One-shot Face Sketch Dataset (OS-Sketch). The benchmark
consists of 400 pairs of face photo-sketch images, including sketches with
different styles and photos with different backgrounds, ages, sexes,
expressions, illumination, etc. For a solid out-of-distribution evaluation, we
select only one pair of images for training at each time, with the rest used
for inference. Extensive experiments demonstrate that the proposed method can
convert various photos into realistic and highly consistent sketches in a
one-shot context. Compared to other methods, our approach offers greater
convenience and broader applicability. The dataset will be available at:
https://github.com/HanWu3125/OS-Sketch

</details>


### [70] [Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards](https://arxiv.org/abs/2506.15684)
*Qingming Liu,Zhen Liu,Dinghuai Zhang,Kui Jia*

Main category: cs.GR

TL;DR: Nabla-R2D3是一个基于强化学习的对齐框架，用于优化3D扩散模型，仅需2D奖励信号即可高效生成高质量的3D内容。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型（如扩散模型）在遵循指令、对齐人类偏好或生成逼真纹理和几何体方面表现不足，需要改进。

Method: 基于Nabla-GFlowNet方法，Nabla-R2D3通过2D奖励信号对3D扩散模型进行高效微调。

Result: 实验表明，Nabla-R2D3在少量微调步骤内能获得更高奖励并减少先验遗忘，优于基线方法。

Conclusion: Nabla-R2D3为3D生成提供了一种高效且样本利用率高的对齐解决方案。

Abstract: Generating high-quality and photorealistic 3D assets remains a longstanding
challenge in 3D vision and computer graphics. Although state-of-the-art
generative models, such as diffusion models, have made significant progress in
3D generation, they often fall short of human-designed content due to limited
ability to follow instructions, align with human preferences, or produce
realistic textures, geometries, and physical attributes. In this paper, we
introduce Nabla-R2D3, a highly effective and sample-efficient reinforcement
learning alignment framework for 3D-native diffusion models using 2D rewards.
Built upon the recently proposed Nabla-GFlowNet method, which matches the score
function to reward gradients in a principled manner for reward finetuning, our
Nabla-R2D3 enables effective adaptation of 3D diffusion models using only 2D
reward signals. Extensive experiments show that, unlike vanilla finetuning
baselines which either struggle to converge or suffer from reward hacking,
Nabla-R2D3 consistently achieves higher rewards and reduced prior forgetting
within a few finetuning steps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [71] [Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence](https://arxiv.org/abs/2506.15677)
*Yining Hong,Rui Sun,Bingxuan Li,Xingcheng Yao,Maxine Wu,Alexander Chien,Da Yin,Ying Nian Wu,Zhecan James Wang,Kai-Wei Chang*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI代理范式——Embodied Web Agents，旨在融合物理世界交互与网络规模推理能力，并开发了统一仿真平台和基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理在物理世界交互和网络规模推理之间存在割裂，限制了其在需要跨领域智能的任务中的应用。

Method: 开发了Embodied Web Agents任务环境和基准测试，整合了3D仿真环境与功能性网络接口。

Result: 实验显示现有AI系统与人类能力之间存在显著差距，揭示了跨领域智能的挑战与机遇。

Conclusion: 论文为融合物理与数字智能提供了新方向，并公开了相关数据集和代码。

Abstract: AI agents today are mostly siloed - they either retrieve and reason over vast
amount of digital information and knowledge obtained online; or interact with
the physical world through embodied perception, planning and action - but
rarely both. This separation limits their ability to solve tasks that require
integrated physical and digital intelligence, such as cooking from online
recipes, navigating with dynamic map data, or interpreting real-world landmarks
using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI
agents that fluidly bridge embodiment and web-scale reasoning. To
operationalize this concept, we first develop the Embodied Web Agents task
environments, a unified simulation platform that tightly integrates realistic
3D indoor and outdoor environments with functional web interfaces. Building
upon this platform, we construct and release the Embodied Web Agents Benchmark,
which encompasses a diverse suite of tasks including cooking, navigation,
shopping, tourism, and geolocation - all requiring coordinated reasoning across
physical and digital realms for systematic assessment of cross-domain
intelligence. Experimental results reveal significant performance gaps between
state-of-the-art AI systems and human capabilities, establishing both
challenges and opportunities at the intersection of embodied cognition and
web-scale knowledge access. All datasets, codes and websites are publicly
available at our project page https://embodied-web-agent.github.io/.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [72] [pycnet-audio: A Python package to support bioacoustics data processing](https://arxiv.org/abs/2506.14864)
*Zachary J. Ruff,Damon B. Lesmeister*

Main category: cs.SD

TL;DR: 论文介绍了一种基于被动声学监测的野生动物研究方法，利用自动化录音设备（ARUs）收集数据，并通过pycnet-audio工具进行自动化处理。


<details>
  <summary>Details</summary>
Motivation: 解决大规模声学数据手动处理不切实际的问题，提供高效的自动化检测方法。

Method: 使用PNW-Cnet模型扩展版，检测约80种森林野生动物及环境噪音的声学信号。

Result: 开发了pycnet-audio工具，支持高效处理声学数据。

Conclusion: 该方法为野生动物研究提供了实用的自动化解决方案。

Abstract: Passive acoustic monitoring is an emerging approach in wildlife research that
leverages recent improvements in purpose-made automated recording units (ARUs).
The general approach is to deploy ARUs in the field to record on a programmed
schedule for extended periods (weeks or months), after which the audio data are
retrieved. These data must then be processed, typically either by measuring or
analyzing characteristics of the audio itself (e.g. calculating acoustic
indices), or by searching for some signal of interest within the recordings,
e.g. vocalizations or other sounds produced by some target species,
anthropogenic or environmental noise, etc. In the latter case, some method is
required to locate the signal(s) of interest within the audio. While very small
datasets can simply be searched manually, even modest projects can produce
audio datasets on the order of 105 hours of recordings, making manual review
impractical and necessitating some form of automated detection. pycnet-audio
(Ruff 2024) is intended to provide a practical processing workflow for acoustic
data, built around the PNW-Cnet model, which was initially developed by the
U.S. Forest Service to support population monitoring of northern spotted owls
(Strix occidentalis caurina) and other forest owls (Lesmeister and Jenkins
2022; Ruff et al. 2020). PNW-Cnet has been expanded to detect vocalizations of
ca. 80 forest wildlife species and numerous forms of anthropogenic and
environmental noise (Ruff et al. 2021, 2023).

</details>


### [73] [An accurate and revised version of optical character recognition-based speech synthesis using LabVIEW](https://arxiv.org/abs/2506.15029)
*Prateek Mehta,Anasuya Patil*

Main category: cs.SD

TL;DR: 开发了一种基于OCR的语音合成系统，帮助视障人士通过声音获取知识。


<details>
  <summary>Details</summary>
Motivation: 视障人士通常依赖盲文书籍和音频录音，但这些方式有限，无法自由选择书籍。语音是更有效的沟通方式。

Method: 使用LabVIEW实现OCR技术，构建准确、可靠、经济且用户友好的语音合成系统。

Result: 系统能够将文本转换为语音，帮助视障人士更便捷地获取信息。

Conclusion: 该系统为视障人士提供了一种有效的知识获取方式，具有实用性和成本效益。

Abstract: Knowledge extraction through sound is a distinctive property. Visually
impaired individuals often rely solely on Braille books and audio recordings
provided by NGOs. Due to limitations in these approaches, blind individuals
often cannot access books of their choice. Speech is a more effective mode of
communication than text for blind and visually impaired persons, as they can
easily respond to sounds. This paper presents the development of an accurate,
reliable, cost-effective, and user-friendly optical character recognition
(OCR)-based speech synthesis system. The OCR-based system has been implemented
using Laboratory Virtual Instrument Engineering Workbench (LabVIEW).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [74] [Towards Perception-based Collision Avoidance for UAVs when Guiding the Visually Impaired](https://arxiv.org/abs/2506.14857)
*Suman Raj,Swapnil Padhi,Ruchi Bhoot,Prince Modi,Yogesh Simmhan*

Main category: cs.RO

TL;DR: 无人机通过机载传感器结合机器学习和计算机视觉算法进行自主导航，应用于农业、物流和灾害管理等领域。本文探讨了无人机辅助视障人士在户外城市环境中导航的可行性。


<details>
  <summary>Details</summary>
Motivation: 研究无人机如何帮助视障人士在复杂城市环境中安全导航，解决传统导航工具的局限性。

Method: 提出了一种基于感知的局部路径规划系统，结合基于GPS和地图的全局规划器，采用几何问题表示和多DNN框架进行障碍物避障。

Result: 在校园环境中进行了无人机-人类系统测试，验证了算法在三种场景（人行道行走、停车区附近、拥挤街道）中的可行性。

Conclusion: 研究表明，无人机辅助视障人士导航的算法在实际环境中具有可行性，为未来应用提供了基础。

Abstract: Autonomous navigation by drones using onboard sensors combined with machine
learning and computer vision algorithms is impacting a number of domains,
including agriculture, logistics, and disaster management. In this paper, we
examine the use of drones for assisting visually impaired people (VIPs) in
navigating through outdoor urban environments. Specifically, we present a
perception-based path planning system for local planning around the
neighborhood of the VIP, integrated with a global planner based on GPS and maps
for coarse planning. We represent the problem using a geometric formulation and
propose a multi DNN based framework for obstacle avoidance of the UAV as well
as the VIP. Our evaluations conducted on a drone human system in a university
campus environment verifies the feasibility of our algorithms in three
scenarios; when the VIP walks on a footpath, near parked vehicles, and in a
crowded street.

</details>


### [75] [Robust Instant Policy: Leveraging Student's t-Regression Model for Robust In-context Imitation Learning of Robot Manipulation](https://arxiv.org/abs/2506.15157)
*Hanbit Oh,Andrea M. Salcedo-Vázquez,Ixchel G. Ramirez-Alpizar,Yukiyasu Domae*

Main category: cs.RO

TL;DR: 提出了一种名为RIP的鲁棒上下文模仿学习算法，通过使用Student's t回归模型来减少LLM生成的幻觉轨迹，提高任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决现有In-Context IL方法中LLM生成的幻觉轨迹问题，提高机器人任务执行的可靠性。

Method: 利用Student's t分布聚合LLM生成的多个候选轨迹，忽略异常值（幻觉轨迹），生成鲁棒轨迹。

Result: 在模拟和真实环境中，RIP显著优于现有IL方法，任务成功率至少提高26%。

Conclusion: RIP通过鲁棒聚合方法有效减少了幻觉轨迹的影响，提升了机器人任务执行的可靠性。

Abstract: Imitation learning (IL) aims to enable robots to perform tasks autonomously
by observing a few human demonstrations. Recently, a variant of IL, called
In-Context IL, utilized off-the-shelf large language models (LLMs) as instant
policies that understand the context from a few given demonstrations to perform
a new task, rather than explicitly updating network models with large-scale
demonstrations. However, its reliability in the robotics domain is undermined
by hallucination issues such as LLM-based instant policy, which occasionally
generates poor trajectories that deviate from the given demonstrations. To
alleviate this problem, we propose a new robust in-context imitation learning
algorithm called the robust instant policy (RIP), which utilizes a Student's
t-regression model to be robust against the hallucinated trajectories of
instant policies to allow reliable trajectory generation. Specifically, RIP
generates several candidate robot trajectories to complete a given task from an
LLM and aggregates them using the Student's t-distribution, which is beneficial
for ignoring outliers (i.e., hallucinations); thereby, a robust trajectory
against hallucinations is generated. Our experiments, conducted in both
simulated and real-world environments, show that RIP significantly outperforms
state-of-the-art IL methods, with at least $26\%$ improvement in task success
rates, particularly in low-data scenarios for everyday tasks. Video results
available at https://sites.google.com/view/robustinstantpolicy.

</details>


### [76] [MCOO-SLAM: A Multi-Camera Omnidirectional Object SLAM System](https://arxiv.org/abs/2506.15402)
*Miaoxin Pan,Jinnan Li,Yaowen Zhang,Yi Yang,Yufeng Yue*

Main category: cs.RO

TL;DR: MCOO-SLAM是一种多相机全景物体SLAM系统，利用环绕视角相机配置在复杂户外场景中实现鲁棒、一致且语义丰富的建图。


<details>
  <summary>Details</summary>
Motivation: 现有SLAM方法依赖RGB-D或单目相机，视野窄、易受遮挡且深度感知有限，导致物体建模不准确和数据关联不可靠。

Method: 结合点特征和物体级地标，引入语义-几何-时间融合策略，设计全景闭环模块，构建分层3D场景图。

Result: 在真实场景中验证，MCOO-SLAM实现了精确定位和可扩展的物体级建图，对遮挡、姿态变化和环境复杂性具有更强鲁棒性。

Conclusion: MCOO-SLAM通过多相机全景配置和语义增强，显著提升了复杂户外场景中的SLAM性能。

Abstract: Object-level SLAM offers structured and semantically meaningful environment
representations, making it more interpretable and suitable for high-level
robotic tasks. However, most existing approaches rely on RGB-D sensors or
monocular views, which suffer from narrow fields of view, occlusion
sensitivity, and limited depth perception-especially in large-scale or outdoor
environments. These limitations often restrict the system to observing only
partial views of objects from limited perspectives, leading to inaccurate
object modeling and unreliable data association. In this work, we propose
MCOO-SLAM, a novel Multi-Camera Omnidirectional Object SLAM system that fully
leverages surround-view camera configurations to achieve robust, consistent,
and semantically enriched mapping in complex outdoor scenarios. Our approach
integrates point features and object-level landmarks enhanced with
open-vocabulary semantics. A semantic-geometric-temporal fusion strategy is
introduced for robust object association across multiple views, leading to
improved consistency and accurate object modeling, and an omnidirectional loop
closure module is designed to enable viewpoint-invariant place recognition
using scene-level descriptors. Furthermore, the constructed map is abstracted
into a hierarchical 3D scene graph to support downstream reasoning tasks.
Extensive experiments in real-world demonstrate that MCOO-SLAM achieves
accurate localization and scalable object-level mapping with improved
robustness to occlusion, pose variation, and environmental complexity.

</details>


### [77] [Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos](https://arxiv.org/abs/2506.15680)
*Kaifeng Zhang,Baoyu Li,Kris Hauser,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种结合粒子与空间网格的神经动力学框架，用于建模可变形物体的动态行为，并通过实验验证其在稀疏视角下的优越性能。


<details>
  <summary>Details</summary>
Motivation: 解决可变形物体动态建模的挑战，尤其是从有限视觉信息中估计状态的问题。

Method: 采用粒子-网格混合表示，结合高斯渲染技术，实现基于学习的数字孪生模型。

Result: 模型在稀疏视角RGB-D记录中学习多种物体的动态行为，并在类别级别泛化到未见实例，性能优于现有方法。

Conclusion: 该框架在可变形物体建模和任务规划中表现出色，适用于机器人-物体交互场景。

Abstract: Modeling the dynamics of deformable objects is challenging due to their
diverse physical properties and the difficulty of estimating states from
limited visual information. We address these challenges with a neural dynamics
framework that combines object particles and spatial grids in a hybrid
representation. Our particle-grid model captures global shape and motion
information while predicting dense particle movements, enabling the modeling of
objects with varied shapes and materials. Particles represent object shapes,
while the spatial grid discretizes the 3D space to ensure spatial continuity
and enhance learning efficiency. Coupled with Gaussian Splattings for visual
rendering, our framework achieves a fully learning-based digital twin of
deformable objects and generates 3D action-conditioned videos. Through
experiments, we demonstrate that our model learns the dynamics of diverse
objects -- such as ropes, cloths, stuffed animals, and paper bags -- from
sparse-view RGB-D recordings of robot-object interactions, while also
generalizing at the category level to unseen instances. Our approach
outperforms state-of-the-art learning-based and physics-based simulators,
particularly in scenarios with limited camera views. Furthermore, we showcase
the utility of our learned models in model-based planning, enabling
goal-conditioned object manipulation across a range of tasks. The project page
is available at https://kywind.github.io/pgnd .

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [PIPE: Physics-Informed Position Encoding for Alignment of Satellite Images and Time Series](https://arxiv.org/abs/2506.14786)
*Haobo Li,Eunseo Jung,Zixin Chen,Zhaowei Wang,Yueya Wang,Huamin Qu,Alexis Kai Hon Lau*

Main category: cs.LG

TL;DR: 提出了一种名为PIPE的轻量级方法，通过物理信息位置编码提升多模态时间序列预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要利用文本数据，忽略了视觉数据中的物理信息，如卫星图像的时间和地理空间上下文。

Method: PIPE通过物理信息位置索引和变频率位置编码机制，将物理信息嵌入视觉语言模型。

Result: 在最大开源卫星图像数据集上，PIPE在深度学习和气候领域方法中均达到最优性能，台风强度预测提升12%。

Conclusion: PIPE有效提升了多模态对齐和预测准确性，尤其在物理信息丰富的场景中表现卓越。

Abstract: Multimodal time series forecasting is foundational in various fields, such as
utilizing satellite imagery and numerical data for predicting typhoons in
climate science. However, existing multimodal approaches primarily focus on
utilizing text data to help time series forecasting, leaving the visual data in
existing time series datasets untouched. Furthermore, it is challenging for
models to effectively capture the physical information embedded in visual data,
such as satellite imagery's temporal and geospatial context, which extends
beyond images themselves. To address this gap, we propose physics-informed
positional encoding (PIPE), a lightweight method that embeds physical
information into vision language models (VLMs). PIPE introduces two key
innovations: (1) a physics-informed positional indexing scheme for mapping
physics to positional IDs, and (2) a variant-frequency positional encoding
mechanism for encoding frequency information of physical variables and
sequential order of tokens within the embedding space. By preserving both the
physical information and sequential order information, PIPE significantly
improves multimodal alignment and forecasting accuracy. Through the experiments
on the most representative and the largest open-sourced satellite image
dataset, PIPE achieves state-of-the-art performance in both deep learning
forecasting and climate domain methods, demonstrating superiority across
benchmarks, including a 12% improvement in typhoon intensity forecasting over
prior works. Our code is provided in the supplementary material.

</details>


### [79] [Reinforcing VLMs to Use Tools for Detailed Visual Reasoning Under Resource Constraints](https://arxiv.org/abs/2506.14821)
*Sunil Kumar,Bowen Zhao,Leo Dirac,Paulina Varshavskaya*

Main category: cs.LG

TL;DR: 小规模视觉语言模型通过GRPO学习和外部工具（如缩放）提升视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在有限计算资源下详细视觉推理能力不足的问题。

Method: 采用GRPO学习、简单奖励结构、简化工具调用接口、分配额外标记给工具调用结果，并过采样视觉困难样本。

Result: 在部分视觉问答任务上表现优于基线模型。

Conclusion: 结合GRPO和外部工具能有效提升小规模模型的视觉推理能力。

Abstract: Despite tremendous recent advances in large model reasoning ability,
vision-language models (VLMs) still struggle with detailed visual reasoning,
especially when compute resources are limited. To address this challenge, we
draw inspiration from methods like Deepseek-r1 for VLMs and train smaller-scale
models with Group Relative Policy Optimization (GRPO) to use external tools
such as zoom. The greatest benefit is obtained with a combination of GRPO
learning, a simple reward structure, a simplified tool-calling interface,
allocating additional tokens to the result of the tool call, and a training
data mix that over-represents visually difficult examples. Compared to
similarly-sized baseline models, our method achieves better performance on some
visual question-answering (VQA) tasks, thanks to the detailed visual
information gathered from the external tool.

</details>


### [80] [CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration](https://arxiv.org/abs/2506.14843)
*Luca Gherardini,Imre Lengyel,Tunde Peto,Caroline C. W. Klaverd,Magda A. Meester-Smoord,Johanna Maria Colijnd,EYE-RISK Consortium,E3 Consortium,Jose Sousa*

Main category: cs.LG

TL;DR: CACTUS是一种用于改进年龄相关性黄斑变性（AMD）分期的分类工具，提供可解释性和灵活性，优于标准机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 医疗数据通常有限或不完整，影响机器学习模型性能，且缺乏透明度。AMD早期诊断至关重要，但缺乏有效治疗方法。

Method: CACTUS结合遗传、饮食、临床和人口统计学因素进行分类，通过消除不相关或有偏见的数据提高模型性能。

Result: CACTUS在AMD分期分类中表现优于标准模型，提供关键特征和结果可信度。

Conclusion: CACTUS通过提高透明度和减少偏见，为临床决策提供支持，并与现有医学知识一致。

Abstract: Machine Learning (ML) is used to tackle various tasks, such as disease
classification and prediction. The effectiveness of ML models relies heavily on
having large amounts of complete data. However, healthcare data is often
limited or incomplete, which can hinder model performance. Additionally, issues
like the trustworthiness of solutions vary with the datasets used. The lack of
transparency in some ML models further complicates their understanding and use.
In healthcare, particularly in the case of Age-related Macular Degeneration
(AMD), which affects millions of older adults, early diagnosis is crucial due
to the absence of effective treatments for reversing progression. Diagnosing
AMD involves assessing retinal images along with patients' symptom reports.
There is a need for classification approaches that consider genetic, dietary,
clinical, and demographic factors. Recently, we introduced the -Comprehensive
Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed
at improving AMD stage classification. CACTUS offers explainability and
flexibility, outperforming standard ML models. It enhances decision-making by
identifying key factors and providing confidence in its results. The important
features identified by CACTUS allow us to compare with existing medical
knowledge. By eliminating less relevant or biased data, we created a clinical
scenario for clinicians to offer feedback and address biases.

</details>


### [81] [Pixel-level Certified Explanations via Randomized Smoothing](https://arxiv.org/abs/2506.15499)
*Alaa Anani,Tobias Lorenz,Mario Fritz,Bernt Schiele*

Main category: cs.LG

TL;DR: 提出首个基于随机平滑的像素级归因认证框架，确保黑盒归因方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有归因方法对微小输入扰动高度敏感，影响其可信度，亟需鲁棒性保证。

Method: 通过稀疏化和平滑化归因图，将其转化为分割问题，并认证每个像素的重要性。

Result: 在12种归因方法和5个ImageNet模型上的评估显示，认证归因具有鲁棒性、可解释性和忠实性。

Conclusion: 该框架为归因方法提供了可靠的鲁棒性保证，适用于下游任务。

Abstract: Post-hoc attribution methods aim to explain deep learning predictions by
highlighting influential input pixels. However, these explanations are highly
non-robust: small, imperceptible input perturbations can drastically alter the
attribution map while maintaining the same prediction. This vulnerability
undermines their trustworthiness and calls for rigorous robustness guarantees
of pixel-level attribution scores. We introduce the first certification
framework that guarantees pixel-level robustness for any black-box attribution
method using randomized smoothing. By sparsifying and smoothing attribution
maps, we reformulate the task as a segmentation problem and certify each
pixel's importance against $\ell_2$-bounded perturbations. We further propose
three evaluation metrics to assess certified robustness, localization, and
faithfulness. An extensive evaluation of 12 attribution methods across 5
ImageNet models shows that our certified attributions are robust,
interpretable, and faithful, enabling reliable use in downstream tasks. Our
code is at https://github.com/AlaaAnani/certified-attributions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [82] [Empirical Studies of Large Scale Environment Scanning by Consumer Electronics](https://arxiv.org/abs/2506.14771)
*Mengyuan Wang,Yang Liu,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: eess.IV

TL;DR: 本文对Matterport Pro3消费级3D扫描设备在大规模环境重建中的性能进行了实证评估，展示了其在高密度点云生成和高精度对齐方面的优势。


<details>
  <summary>Details</summary>
Motivation: 评估Matterport Pro3在大规模环境重建中的有效性、局限性和性能提升，探索解决扫描过程中遇到的挑战。

Method: 通过1,099个扫描点对六层建筑（17,567平方米）进行详细扫描，并与iPhone进行对比分析。

Result: Pro3生成的点云密度更高（1,877,324点 vs. iPhone的506,961点），对齐精度更高（RMSE 0.0118米），C2C平均距离误差为0.0408米。

Conclusion: Matterport Pro3适合大规模应用，能够生成高质量3D模型，得益于LiDAR和先进对齐技术。

Abstract: This paper presents an empirical evaluation of the Matterport Pro3, a
consumer-grade 3D scanning device, for large-scale environment reconstruction.
We conduct detailed scanning (1,099 scanning points) of a six-floor building
(17,567 square meters) and assess the device's effectiveness, limitations, and
performance enhancements in diverse scenarios. Challenges encountered during
the scanning are addressed through proposed solutions, while we also explore
advanced methods to overcome them more effectively. Comparative analysis with
another consumer-grade device (iPhone) highlights the Pro3's balance between
cost-effectiveness and performance. The Matterport Pro3 achieves a denser point
cloud with 1,877,324 points compared to the iPhone's 506,961 points and higher
alignment accuracy with an RMSE of 0.0118 meters. The cloud-to-cloud (C2C)
average distance error between the two point cloud models is 0.0408 meters,
with a standard deviation of 0.0715 meters. The study demonstrates the Pro3's
ability to generate high-quality 3D models suitable for large-scale
applications, leveraging features such as LiDAR and advanced alignment
techniques.

</details>


### [83] [Deploying and Evaluating Multiple Deep Learning Models on Edge Devices for Diabetic Retinopathy Detection](https://arxiv.org/abs/2506.14834)
*Akwasi Asare,Dennis Agyemanh Nana Gookyi,Derrick Boateng,Fortunatus Aabangbio Wulnye*

Main category: eess.IV

TL;DR: 该研究利用Edge Impulse部署深度学习模型，实现实时糖尿病视网膜病变（DR）检测，适用于边缘设备，提供了一种高效、低成本的解决方案。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是全球糖尿病患者视力受损的主要原因，传统诊断方法耗时且资源密集，亟需高效、低成本的解决方案。

Method: 研究使用TensorFlow设计了多种卷积神经网络（如MobileNet、ShuffleNet等），并通过预处理和量化技术优化模型，部署到边缘设备。

Result: MobileNet准确率达96.45%，SqueezeNet模型仅176KB，延迟17ms，适合实时检测；其他模型在资源效率上表现优异。

Conclusion: 边缘AI技术为DR早期检测提供了可扩展、低成本的解决方案，尤其适用于资源有限的医疗环境。

Abstract: Diabetic Retinopathy (DR), a leading cause of vision impairment in
individuals with diabetes, affects approximately 34.6% of diabetes patients
globally, with the number of cases projected to reach 242 million by 2045.
Traditional DR diagnosis relies on the manual examination of retinal fundus
images, which is both time-consuming and resource intensive. This study
presents a novel solution using Edge Impulse to deploy multiple deep learning
models for real-time DR detection on edge devices. A robust dataset of over
3,662 retinal fundus images, sourced from the Kaggle EyePACS dataset, was
curated, and enhanced through preprocessing techniques, including augmentation
and normalization. Using TensorFlow, various Convolutional Neural Networks
(CNNs), such as MobileNet, ShuffleNet, SqueezeNet, and a custom Deep Neural
Network (DNN), were designed, trained, and optimized for edge deployment. The
models were converted to TensorFlowLite and quantized to 8-bit integers to
reduce their size and enhance inference speed, with minimal trade-offs in
accuracy. Performance evaluations across different edge hardware platforms,
including smartphones and microcontrollers, highlighted key metrics such as
inference speed, accuracy, precision, and resource utilization. MobileNet
achieved an accuracy of 96.45%, while SqueezeNet demonstrated strong real-time
performance with a small model size of 176 KB and latency of just 17 ms on GPU.
ShuffleNet and the custom DNN achieved moderate accuracy but excelled in
resource efficiency, making them suitable for lower-end devices. This
integration of edge AI technology into healthcare presents a scalable,
cost-effective solution for early DR detection, providing timely and accurate
diagnosis, especially in resource-constrained and remote healthcare settings.

</details>


### [84] [Improving Prostate Gland Segmenting Using Transformer based Architectures](https://arxiv.org/abs/2506.14844)
*Shatha Abudalou*

Main category: eess.IV

TL;DR: 研究探讨了Transformer模型（UNETR和SwinUNETR）在T2加权MRI图像中前列腺分割的鲁棒性，相比传统3D UNet模型，SwinUNETR在跨域和标签噪声下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决前列腺MRI分割中因读者差异和跨站点域偏移导致的精度下降问题。

Method: 比较UNETR和SwinUNETR与3D UNet的性能，采用三种训练策略（单队列、5折交叉验证混合队列、腺体大小分组）并通过Optuna调参。

Result: SwinUNETR在多种训练策略下表现最佳，Dice分数最高提升5分，尤其在腺体大小分组中表现突出。

Conclusion: SwinUNETR的全局和移位窗口自注意力机制有效降低标签噪声和类别不平衡敏感性，适合临床部署。

Abstract: Inter reader variability and cross site domain shift challenge the automatic
segmentation of prostate anatomy using T2 weighted MRI images. This study
investigates whether transformer models can retain precision amid such
heterogeneity. We compare the performance of UNETR and SwinUNETR in prostate
gland segmentation against our previous 3D UNet model [1], based on 546 MRI
(T2weighted) volumes annotated by two independent experts. Three training
strategies were analyzed: single cohort dataset, 5 fold cross validated mixed
cohort, and gland size based dataset. Hyperparameters were tuned by Optuna. The
test set, from an independent population of readers, served as the evaluation
endpoint (Dice Similarity Coefficient). In single reader training, SwinUNETR
achieved an average dice score of 0.816 for Reader#1 and 0.860 for Reader#2,
while UNETR scored 0.8 and 0.833 for Readers #1 and #2, respectively, compared
to the baseline UNets 0.825 for Reader #1 and 0.851 for Reader #2. SwinUNETR
had an average dice score of 0.8583 for Reader#1 and 0.867 for Reader#2 in
cross-validated mixed training. For the gland size-based dataset, SwinUNETR
achieved an average dice score of 0.902 for Reader#1 subset and 0.894 for
Reader#2, using the five-fold mixed training strategy (Reader#1, n=53;
Reader#2, n=87) at larger gland size-based subsets, where UNETR performed
poorly. Our findings demonstrate that global and shifted-window self-attention
effectively reduces label noise and class imbalance sensitivity, resulting in
improvements in the Dice score over CNNs by up to five points while maintaining
computational efficiency. This contributes to the high robustness of SwinUNETR
for clinical deployment.

</details>


### [85] [Foundation Artificial Intelligence Models for Health Recognition Using Face Photographs (FAHR-Face)](https://arxiv.org/abs/2506.14909)
*Fridolin Haugg,Grace Lee,John He,Leonard Nürnberg,Dennis Bontempi,Danielle S. Bitterman,Paul Catalano,Vasco Prudente,Dmitrii Glubokov,Andrew Warrington,Suraj Pai,Dirk De Ruysscher,Christian Guthier,Benjamin H. Kann,Vadim N. Gladyshev,Hugo JWL Aerts,Raymond H. Mak*

Main category: eess.IV

TL;DR: FAHR-Face是一个基于面部图像的基础模型，用于生物年龄估计和生存风险预测，表现出色且具有临床实用性。


<details>
  <summary>Details</summary>
Motivation: 通过非侵入性的面部外观分析，提供健康和疾病风险的生物标志物。

Method: FAHR-FaceAge和FAHR-FaceSurvival分别通过两阶段微调和癌症患者数据训练，测试了模型鲁棒性和独立性。

Result: FAHR-FaceAge在年龄估计中误差最低（5.1年），FAHR-FaceSurvival能稳健预测死亡率，两者结合提高预后准确性。

Conclusion: 基础模型能生成廉价、可扩展的面部生物标志物，适用于生物衰老和疾病风险预测。

Abstract: Background: Facial appearance offers a noninvasive window into health. We
built FAHR-Face, a foundation model trained on >40 million facial images and
fine-tuned it for two distinct tasks: biological age estimation (FAHR-FaceAge)
and survival risk prediction (FAHR-FaceSurvival).
  Methods: FAHR-FaceAge underwent a two-stage, age-balanced fine-tuning on
749,935 public images; FAHR-FaceSurvival was fine-tuned on 34,389 photos of
cancer patients. Model robustness (cosmetic surgery, makeup, pose, lighting)
and independence (saliency mapping) was tested extensively. Both models were
clinically tested in two independent cancer patient datasets with survival
analyzed by multivariable Cox models and adjusted for clinical prognostic
factors.
  Findings: For age estimation, FAHR-FaceAge had the lowest mean absolute error
of 5.1 years on public datasets, outperforming benchmark models and maintaining
accuracy across the full human lifespan. In cancer patients, FAHR-FaceAge
outperformed a prior facial age estimation model in survival prognostication.
FAHR-FaceSurvival demonstrated robust prediction of mortality, and the
highest-risk quartile had more than triple the mortality of the lowest
(adjusted hazard ratio 3.22; P<0.001). These findings were validated in the
independent cohort and both models showed generalizability across age, sex,
race and cancer subgroups. The two algorithms provided distinct, complementary
prognostic information; saliency mapping revealed each model relied on distinct
facial regions. The combination of FAHR-FaceAge and FAHR-FaceSurvival improved
prognostic accuracy.
  Interpretation: A single foundation model can generate inexpensive, scalable
facial biomarkers that capture both biological ageing and disease-related
mortality risk. The foundation model enabled effective training using
relatively small clinical datasets.

</details>


### [86] [Recursive Variational Autoencoders for 3D Blood Vessel Generative Modeling](https://arxiv.org/abs/2506.14914)
*Paula Feldman,Miguel Fainstein,Viviana Siless,Claudio Delrieux,Emmanuel Iarussi*

Main category: eess.IV

TL;DR: 提出了一种递归变分神经网络（RvNN）来生成复杂且多样化的血管结构，解决了现有规则方法无法捕捉真实解剖数据多样性的问题。


<details>
  <summary>Details</summary>
Motivation: 解剖树在临床诊断和治疗规划中至关重要，但现有方法难以准确表示其复杂的拓扑和几何结构。

Method: 使用RvNN学习血管的分层组织，编码分支连接性和几何特征，生成新的血管几何模型。

Result: 生成的3D血管模型在半径、长度和弯曲度等方面与真实数据高度相似，适用于医疗培训和模拟。

Conclusion: RvNN首次用于血管合成，生成的模型准确且多样，具有广泛的应用潜力。

Abstract: Anatomical trees play an important role in clinical diagnosis and treatment
planning. Yet, accurately representing these structures poses significant
challenges owing to their intricate and varied topology and geometry. Most
existing methods to synthesize vasculature are rule based, and despite
providing some degree of control and variation in the structures produced, they
fail to capture the diversity and complexity of actual anatomical data. We
developed a Recursive variational Neural Network (RvNN) that fully exploits the
hierarchical organization of the vessel and learns a low-dimensional manifold
encoding branch connectivity along with geometry features describing the target
surface. After training, the RvNN latent space can be sampled to generate new
vessel geometries. By leveraging the power of generative neural networks, we
generate 3D models of blood vessels that are both accurate and diverse, which
is crucial for medical and surgical training, hemodynamic simulations, and many
other purposes. These results closely resemble real data, achieving high
similarity in vessel radii, length, and tortuosity across various datasets,
including those with aneurysms. To the best of our knowledge, this work is the
first to utilize this technique for synthesizing blood vessels.

</details>


### [87] [NeuroMoE: A Transformer-Based Mixture-of-Experts Framework for Multi-Modal Neurological Disorder Classification](https://arxiv.org/abs/2506.14970)
*Wajih Hassan Raza,Aamir Bader Shah,Yu Wen,Yidan Shen,Juan Diego Martinez Lemus,Mya Caryn Schiess,Timothy Michael Ellmore,Renjie Hu,Xin Fu*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的混合专家框架，用于整合多模态MRI和临床数据，显著提高了神经退行性疾病的诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在多模态MRI和临床数据的整合上表现不佳，影响了神经退行性疾病的诊断效果。

Method: 使用Transformer编码器捕捉MRI数据的空间关系，并通过模态特定的专家进行特征提取，采用自适应融合门控机制动态整合专家输出。

Result: 验证准确率达到82.47%，比基线方法高出10%以上。

Conclusion: 该框架通过多模态学习显著提升了神经退行性疾病的诊断性能，具有临床应用潜力。

Abstract: The integration of multi-modal Magnetic Resonance Imaging (MRI) and clinical
data holds great promise for enhancing the diagnosis of neurological disorders
(NDs) in real-world clinical settings. Deep Learning (DL) has recently emerged
as a powerful tool for extracting meaningful patterns from medical data to aid
in diagnosis. However, existing DL approaches struggle to effectively leverage
multi-modal MRI and clinical data, leading to suboptimal performance.
  To address this challenge, we utilize a unique, proprietary multi-modal
clinical dataset curated for ND research. Based on this dataset, we propose a
novel transformer-based Mixture-of-Experts (MoE) framework for ND
classification, leveraging multiple MRI modalities-anatomical (aMRI), Diffusion
Tensor Imaging (DTI), and functional (fMRI)-alongside clinical assessments. Our
framework employs transformer encoders to capture spatial relationships within
volumetric MRI data while utilizing modality-specific experts for targeted
feature extraction. A gating mechanism with adaptive fusion dynamically
integrates expert outputs, ensuring optimal predictive performance.
Comprehensive experiments and comparisons with multiple baselines demonstrate
that our multi-modal approach significantly enhances diagnostic accuracy,
particularly in distinguishing overlapping disease states. Our framework
achieves a validation accuracy of 82.47\%, outperforming baseline methods by
over 10\%, highlighting its potential to improve ND diagnosis by applying
multi-modal learning to real-world clinical data.

</details>


### [88] [Classification of Multi-Parametric Body MRI Series Using Deep Learning](https://arxiv.org/abs/2506.15182)
*Boah Kim,Tejas Sudharshan Mathai,Kimberly Helm,Peter A. Pinto,Ronald M. Summers*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的分类模型，用于准确分类8种不同的MRI序列类型，以提高放射科医生的工作效率。


<details>
  <summary>Details</summary>
Motivation: 由于DICOM头部信息常因协议多样性和人为错误而不准确，需要一种自动分类方法。

Method: 使用ResNet、EfficientNet和DenseNet等深度学习模型训练分类器，并比较性能。

Result: DenseNet-121模型表现最佳，内部数据集F1-score和准确率分别为0.966和0.972，外部数据集也表现良好。

Conclusion: DenseNet-121模型在分类MRI序列任务中表现出高准确性和鲁棒性。

Abstract: Multi-parametric magnetic resonance imaging (mpMRI) exams have various series
types acquired with different imaging protocols. The DICOM headers of these
series often have incorrect information due to the sheer diversity of protocols
and occasional technologist errors. To address this, we present a deep
learning-based classification model to classify 8 different body mpMRI series
types so that radiologists read the exams efficiently. Using mpMRI data from
various institutions, multiple deep learning-based classifiers of ResNet,
EfficientNet, and DenseNet are trained to classify 8 different MRI series, and
their performance is compared. Then, the best-performing classifier is
identified, and its classification capability under the setting of different
training data quantities is studied. Also, the model is evaluated on the
out-of-training-distribution datasets. Moreover, the model is trained using
mpMRI exams obtained from different scanners in two training strategies, and
its performance is tested. Experimental results show that the DenseNet-121
model achieves the highest F1-score and accuracy of 0.966 and 0.972 over the
other classification models with p-value$<$0.05. The model shows greater than
0.95 accuracy when trained with over 729 studies of the training data, whose
performance improves as the training data quantities grew larger. On the
external data with the DLDS and CPTAC-UCEC datasets, the model yields 0.872 and
0.810 accuracy for each. These results indicate that in both the internal and
external datasets, the DenseNet-121 model attains high accuracy for the task of
classifying 8 body MRI series types.

</details>


### [89] [Privacy-Preserving Chest X-ray Classification in Latent Space with Homomorphically Encrypted Neural Inference](https://arxiv.org/abs/2506.15258)
*Jonghun Kim,Gyeongdeok Jo,Shinyoung Ra,Hyunjin Park*

Main category: eess.IV

TL;DR: 提出了一种基于VQGAN的HE推理框架，用于医学图像分类，通过压缩图像和近似激活函数降低计算负担。


<details>
  <summary>Details</summary>
Motivation: 医学图像数据隐私保护需求高，HE推理计算成本高，需优化。

Method: 使用VQGAN压缩图像，近似激活函数，调整下采样因子，并改进SE模块。

Result: 在胸部X光数据集上测试，性能接近未加密推理，计算负担显著降低。

Conclusion: 该方法在医学图像HE推理中具有实用潜力，尽管仍存在速度问题。

Abstract: Medical imaging data contain sensitive patient information requiring strong
privacy protection. Many analytical setups require data to be sent to a server
for inference purposes. Homomorphic encryption (HE) provides a solution by
allowing computations to be performed on encrypted data without revealing the
original information. However, HE inference is computationally expensive,
particularly for large images (e.g., chest X-rays). In this study, we propose
an HE inference framework for medical images that uses VQGAN to compress images
into latent representations, thereby significantly reducing the computational
burden while preserving image quality. We approximate the activation functions
with lower-degree polynomials to balance the accuracy and efficiency in
compliance with HE requirements. We observed that a downsampling factor of
eight for compression achieved an optimal balance between performance and
computational cost. We further adapted the squeeze and excitation module, which
is known to improve traditional CNNs, to enhance the HE framework. Our method
was tested on two chest X-ray datasets for multi-label classification tasks
using vanilla CNN backbones. Although HE inference remains relatively slow and
introduces minor performance differences compared with unencrypted inference,
our approach shows strong potential for practical use in medical images

</details>


### [90] [FedWSIDD: Federated Whole Slide Image Classification via Dataset Distillation](https://arxiv.org/abs/2506.15365)
*Haolong Jin,Shenglin Liu,Cong Cong,Qingmin Feng,Yongzhi Liu,Lina Huang,Yingzi Hu*

Main category: eess.IV

TL;DR: FedWSIDD是一种新的联邦学习范式，通过数据集蒸馏生成合成切片，解决WSI分类中的计算资源异构性和隐私问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在WSI分类中面临的异构计算资源和隐私问题。

Method: 提出FedWSIDD，利用数据集蒸馏生成合成切片，并在客户端引入新的蒸馏算法结合染色归一化。

Result: 在CAMELYON16和CAMELYON17等任务中，FedWSIDD提升了分类性能并保护隐私。

Conclusion: FedWSIDD是WSI分类任务的高效解决方案。

Abstract: Federated learning (FL) has emerged as a promising approach for collaborative
medical image analysis, enabling multiple institutions to build robust
predictive models while preserving sensitive patient data. In the context of
Whole Slide Image (WSI) classification, FL faces significant challenges,
including heterogeneous computational resources across participating medical
institutes and privacy concerns. To address these challenges, we propose
FedWSIDD, a novel FL paradigm that leverages dataset distillation (DD) to learn
and transmit synthetic slides. On the server side, FedWSIDD aggregates
synthetic slides from participating centres and distributes them across all
centres. On the client side, we introduce a novel DD algorithm tailored to
histopathology datasets which incorporates stain normalisation into the
distillation process to generate a compact set of highly informative synthetic
slides. These synthetic slides, rather than model parameters, are transmitted
to the server. After communication, the received synthetic slides are combined
with original slides for local tasks. Extensive experiments on multiple WSI
classification tasks, including CAMELYON16 and CAMELYON17, demonstrate that
FedWSIDD offers flexibility for heterogeneous local models, enhances local WSI
classification performance, and preserves patient privacy. This makes it a
highly effective solution for complex WSI classification tasks. The code is
available at FedWSIDD.

</details>


### [91] [A Real-time Endoscopic Image Denoising System](https://arxiv.org/abs/2506.15395)
*Yu Xing,Shishi Huang,Meng Lv,Guo Chen,Huailiang Wang,Lingzhi Sui*

Main category: eess.IV

TL;DR: 论文提出了一种针对医疗内窥镜中模拟图像传感器的混合去噪系统，有效解决了噪声问题，同时保持图像细节和实时性能。


<details>
  <summary>Details</summary>
Motivation: 小型化内窥镜传感器因光敏区域有限导致噪声问题，影响图像质量，需要一种高效的去噪方法。

Method: 开发了噪声模型，结合传统图像处理算法和基于学习的技术，提出混合去噪系统。

Result: 实验显示，系统显著降低噪声，PSNR从21.16提升至33.05，且实时运行于FPGA平台。

Conclusion: 混合去噪系统在医疗内窥镜图像处理中表现出色，平衡了去噪效果与细节保留。

Abstract: Endoscopes featuring a miniaturized design have significantly enhanced
operational flexibility, portability, and diagnostic capability while
substantially reducing the invasiveness of medical procedures. Recently,
single-use endoscopes equipped with an ultra-compact analogue image sensor
measuring less than 1mm x 1mm bring revolutionary advancements to medical
diagnosis. They reduce the structural redundancy and large capital expenditures
associated with reusable devices, eliminate the risk of patient infections
caused by inadequate disinfection, and alleviate patient suffering. However,
the limited photosensitive area results in reduced photon capture per pixel,
requiring higher photon sensitivity settings to maintain adequate brightness.
In high-contrast medical imaging scenarios, the small-sized sensor exhibits a
constrained dynamic range, making it difficult to simultaneously capture
details in both highlights and shadows, and additional localized digital gain
is required to compensate. Moreover, the simplified circuit design and analog
signal transmission introduce additional noise sources. These factors
collectively contribute to significant noise issues in processed endoscopic
images. In this work, we developed a comprehensive noise model for analog image
sensors in medical endoscopes, addressing three primary noise types:
fixed-pattern noise, periodic banding noise, and mixed Poisson-Gaussian noise.
Building on this analysis, we propose a hybrid denoising system that
synergistically combines traditional image processing algorithms with advanced
learning-based techniques for captured raw frames from sensors. Experiments
demonstrate that our approach effectively reduces image noise without fine
detail loss or color distortion, while achieving real-time performance on FPGA
platforms and an average PSNR improvement from 21.16 to 33.05 on our test
dataset.

</details>


### [92] [Advanced cervical cancer classification: enhancing pap smear images with hybrid PMD Filter-CLAHE](https://arxiv.org/abs/2506.15489)
*Ach Khozaimi,Isnani Darti,Syaiful Anam,Wuryansari Muharini Kusumawinahyu*

Main category: eess.IV

TL;DR: 该论文研究了图像预处理技术对CNN在宫颈癌分类中的性能影响，提出了一种混合PMD滤波器和CLAHE的方法，显著提升了分类效果。


<details>
  <summary>Details</summary>
Motivation: 宫颈癌早期检测至关重要，但CNN性能受图像质量影响，因此研究预处理技术对性能的提升。

Method: 评估了PMD滤波、CLAHE及混合PMD-CLAHE三种预处理技术，并在多个预训练CNN模型上进行测试。

Result: 混合PMD-CLAHE方法在准确率、精确率、召回率和F1分数上分别提升了13.62%、10.04%、13.08%和14.34%。

Conclusion: 混合PMD-CLAHE技术为提升宫颈癌分类性能提供了新思路。

Abstract: Cervical cancer remains a significant health problem, especially in
developing countries. Early detection is critical for effective treatment.
Convolutional neural networks (CNN) have shown promise in automated cervical
cancer screening, but their performance depends on Pap smear image quality.
This study investigates the impact of various image preprocessing techniques on
CNN performance for cervical cancer classification using the SIPaKMeD dataset.
Three preprocessing techniques were evaluated: perona-malik diffusion (PMD)
filter for noise reduction, contrast-limited adaptive histogram equalization
(CLAHE) for image contrast enhancement, and the proposed hybrid PMD
filter-CLAHE approach. The enhanced image datasets were evaluated on pretrained
models, such as ResNet-34, ResNet-50, SqueezeNet-1.0, MobileNet-V2,
EfficientNet-B0, EfficientNet-B1, DenseNet-121, and DenseNet-201. The results
show that hybrid preprocessing PMD filter-CLAHE can improve the Pap smear image
quality and CNN architecture performance compared to the original images. The
maximum metric improvements are 13.62% for accuracy, 10.04% for precision,
13.08% for recall, and 14.34% for F1-score. The proposed hybrid PMD
filter-CLAHE technique offers a new perspective in improving cervical cancer
classification performance using CNN architectures.

</details>


### [93] [Automated MRI Tumor Segmentation using hybrid U-Net with Transformer and Efficient Attention](https://arxiv.org/abs/2506.15562)
*Syed Haider Ali,Asrar Ahmad,Muhammad Ali,Asifullah Khan,Muhammad Shahban,Nadeem Shaukat*

Main category: eess.IV

TL;DR: 该研究提出了一种混合UNet-Transformer模型，用于在本地MRI数据集上高效准确地分割肿瘤，以支持临床放疗计划。


<details>
  <summary>Details</summary>
Motivation: 现有AI分割模型通常基于大型公共数据集训练，缺乏对本地患者群体异质性的考虑，因此需要开发适用于本地数据的模型以优化临床治疗。

Method: 研究采用混合UNet-Transformer架构，结合多种注意力模块（如SE块、CBAM等），并通过数据增强和预训练权重初始化提升模型性能。

Result: 在本地MRI数据集上，模型取得了Dice系数0.764和IoU 0.736的竞争性表现。

Conclusion: 研究表明，针对特定临床场景的模型开发对实现高效准确的肿瘤分割至关重要。

Abstract: Cancer is an abnormal growth with potential to invade locally and metastasize
to distant organs. Accurate auto-segmentation of the tumor and surrounding
normal tissues is required for radiotherapy treatment plan optimization. Recent
AI-based segmentation models are generally trained on large public datasets,
which lack the heterogeneity of local patient populations. While these studies
advance AI-based medical image segmentation, research on local datasets is
necessary to develop and integrate AI tumor segmentation models directly into
hospital software for efficient and accurate oncology treatment planning and
execution. This study enhances tumor segmentation using computationally
efficient hybrid UNet-Transformer models on magnetic resonance imaging (MRI)
datasets acquired from a local hospital under strict privacy protection. We
developed a robust data pipeline for seamless DICOM extraction and
preprocessing, followed by extensive image augmentation to ensure model
generalization across diverse clinical settings, resulting in a total dataset
of 6080 images for training. Our novel architecture integrates UNet-based
convolutional neural networks with a transformer bottleneck and complementary
attention modules, including efficient attention, Squeeze-and-Excitation (SE)
blocks, Convolutional Block Attention Module (CBAM), and ResNeXt blocks. To
accelerate convergence and reduce computational demands, we used a maximum
batch size of 8 and initialized the encoder with pretrained ImageNet weights,
training the model on dual NVIDIA T4 GPUs via checkpointing to overcome
Kaggle's runtime limits. Quantitative evaluation on the local MRI dataset
yielded a Dice similarity coefficient of 0.764 and an Intersection over Union
(IoU) of 0.736, demonstrating competitive performance despite limited data and
underscoring the importance of site-specific model development for clinical
deployment.

</details>
