<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 44]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards Efficient General Feature Prediction in Masked Skeleton Modeling](https://arxiv.org/abs/2509.03609)
*Shengkai Sun,Zefan Zhang,Jianfeng Dong,Zhiyong Cheng,Xiaojun Chang,Meng Wang*

Main category: cs.CV

TL;DR: 提出GFP框架，用高级特征预测替代传统低级重建，实现高效骨架动作识别，训练速度快6.2倍且性能优越


<details>
  <summary>Details</summary>
Motivation: 现有MAE方法重建目标局限于原始关节坐标，导致计算冗余和语义表示有限

Method: 引入协作学习框架，轻量级目标生成网络动态产生多样化监督信号，采用约束优化确保特征多样性

Result: 在NTU RGB+D 60/120和PKU-MMD数据集上实现SOTA性能，训练速度比标准方法快6.2倍

Conclusion: GFP框架通过高级特征预测有效解决了传统骨架建模的计算冗余问题，在效率和性能上均有显著提升

Abstract: Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.

</details>


### [2] [Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge](https://arxiv.org/abs/2509.03614)
*Seungho Choe,Xiaoli Qin,Abubakr Shafique,Amanda Dy,Dimitri Androutsos,Susan Done,April Khademi*

Main category: cs.CV

TL;DR: 提出了一种基于UNet分割骨干网络和师生策略的方法，用于有丝分裂检测和非典型有丝分裂分类，解决了医学图像分析中的领域偏移和数据不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统病理学家计数有丝分裂图像耗时且存在观察者间差异，AI工具虽然能自动检测但易受领域偏移和数据不平衡问题影响。

Method: 使用UNet分割框架集成对比表示学习和领域对抗训练，采用师生策略生成像素级伪掩码，并引入多尺度CNN分类器进行多任务学习。

Result: 在初步测试集上，Track 1获得F1分数0.7660，Track 2获得平衡准确率0.8414。

Conclusion: 将基于分割的检测和分类整合到统一框架中，能够有效提高有丝分裂分析的鲁棒性和准确性。

Abstract: Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.

</details>


### [3] [Multi Attribute Bias Mitigation via Representation Learning](https://arxiv.org/abs/2509.03616)
*Rajeev Ranjan Dwivedi,Ankur Kumar,Vinod K Kurmi*

Main category: cs.CV

TL;DR: GMBM是一个两阶段框架，通过识别已知偏差并抑制梯度方向来解决多重偏差问题，同时提出了新的偏差度量方法SBA


<details>
  <summary>Details</summary>
Motivation: 现实世界图像存在多重重叠偏差，这些偏差会损害视觉模型的性能和公平性，而单独处理每个偏差往往会导致其他偏差加剧

Method: 两阶段框架：1) Adaptive Bias Integrated Learning (ABIL) 训练每个属性的编码器并整合到主干网络中；2) Gradient Suppression Fine Tuning 从梯度中剪除偏差方向

Result: 在FB CMNIST、CelebA和COCO数据集上提高了最差组准确率，将多属性偏差放大减半，并在SBA指标上创下新低

Conclusion: GMBM是第一个实用的端到端多重偏差解决方案，能够有效应对偏差复杂性和分布偏移的挑战

Abstract: Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/

</details>


### [4] [Lightweight image segmentation for echocardiography](https://arxiv.org/abs/2509.03631)
*Anders Kjelsrud,Lasse Løvstakken,Erik Smistad,Håvard Dalen,Gilles Van De Vyver*

Main category: cs.CV

TL;DR: 开发了一个轻量级U-Net模型，参数量从33M减少到2M，在保持与nnU-Net相当分割性能的同时，实现了16倍模型压缩和4倍推理速度提升。


<details>
  <summary>Details</summary>
Motivation: nnU-Net在心脏超声左心室分割中表现良好但模型过大且推理速度慢，限制了实时应用，需要通过分析nnU-Net的关键组件来开发更轻量的替代方案。

Method: 通过消融实验系统评估数据增强方案、架构修改、损失函数和后处理技术，识别出简单仿射增强和深度监督是关键有效组件，而复杂增强和大模型容量收益递减。

Result: 在CAMUS数据集(N=500)上达到与nnU-Net统计等效的性能(Dice分数：LV 0.93 vs 0.93, MYO 0.85 vs 0.86, LA 0.89 vs 0.89)，模型大小减少16倍(2M vs 33M参数)，推理速度提升4倍(1.35ms vs 5.40ms每帧)。

Conclusion: 通过针对性优化nnU-Net的关键组件，成功开发出轻量高效的U-Net模型，在保持分割精度的同时显著提升了实时应用潜力，为临床实时超声分析提供了可行解决方案。

Abstract: Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.

</details>


### [5] [treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds](https://arxiv.org/abs/2509.03633)
*Josafat-Mattias Burmeister,Andreas Tockner,Stefan Reder,Markus Engel,Rico Richter,Jan-Peter Mund,Jürgen Döllner*

Main category: cs.CV

TL;DR: 提出了treeX算法的修订版本，用于森林激光扫描数据的树木实例分割，无需标注数据和大量计算资源，在多个数据集上表现优于原算法和其他开源方法。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习树木分割方法需要大量标注数据和计算资源，需要一种资源效率更高的替代方案。

Method: 改进的treeX算法，结合基于聚类的树干检测和区域生长进行树冠划分，提供地面激光扫描和无人机激光扫描两种参数预设。

Result: 修订版相比原算法减少运行时间并提高准确率，地面数据F1分数提升0.11-0.49，无人机数据F1分数达到0.58（原算法为0），性能与现有开源深度学习方法相当。

Conclusion: 该算法可作为深度学习的资源高效替代方案，或用于半自动生成深度学习标签，已开源在pointtree包中。

Abstract: Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.

</details>


### [6] [Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding](https://arxiv.org/abs/2509.03635)
*Hongpei Zheng,Lintao Xiang,Qijun Yang,Qian Lin,Hujun Yin*

Main category: cs.CV

TL;DR: Reg3D是一个创新的3D几何指令调优框架，通过重建几何结构而非仅描述来提升多模态模型的3D空间理解能力


<details>
  <summary>Details</summary>
Motivation: 现有大型多模态模型在2D视觉理解方面取得显著进展，但在3D场景理解方面仍面临挑战，主要依赖纯文本监督，缺乏几何约束来学习鲁棒的3D空间表示

Method: 采用双重监督范式，在双编码器架构中设计互补的对象级和帧级重建任务，同时利用3D几何信息作为输入和显式学习目标，强制几何一致性

Result: 在ScanQA、Scan2Cap、ScanRefer和SQA3D等多个基准测试中表现出显著的性能提升

Conclusion: Reg3D为空间感知多模态模型建立了新的训练范式，通过几何重建任务有效提升了3D空间理解能力

Abstract: The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.

</details>


### [7] [QuantV2X: A Fully Quantized Multi-Agent System for Cooperative Perception](https://arxiv.org/abs/2509.03704)
*Seth Z. Zhao,Huizhi Zhang,Zhaowei Li,Juntong Peng,Anthony Chui,Zewei Zhou,Zonglin Meng,Hao Xiang,Zhiyu Huang,Fujia Wang,Ran Tian,Chenfeng Xu,Bolei Zhou,Jiaqi Ma*

Main category: cs.CV

TL;DR: QuantV2X是首个完全量化的多模态多智能体V2X协同感知系统，通过统一的端到端量化策略，在保持精度的同时显著降低了计算负载和传输带宽，实现了3.2倍延迟降低和9.5 mAP30提升。


<details>
  <summary>Details</summary>
Motivation: 现有V2X协同感知研究主要关注精度指标，忽视了效率、延迟和实际部署等系统级考量，且大多使用计算和传输成本高昂的全精度模型，难以在资源受限环境中实时运行。

Method: 提出统一的端到端量化策略，同时对神经网络模型和传输消息表示进行量化，降低计算负载和传输带宽。

Result: 在低比特约束下实现与全精度系统相当的精度，系统级延迟降低3.2倍，mAP30提升9.5，能更有效地扩展以适应严格内存预算。

Conclusion: 完全量化的多智能体中间融合系统在实际部署中具有可行性，该系统将公开发布以推动该领域研究。

Abstract: Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.

</details>


### [8] [Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns](https://arxiv.org/abs/2509.03729)
*Bandita Bharadwaj,Ankur Mishra,Saurav Bharadwaj*

Main category: cs.CV

TL;DR: 评估ResNet50、MobileNetV2和EfficientNetB0三种深度学习架构在基于叶脉模式的植物物种自动分类中的效果，发现EfficientNetB0表现最佳，测试准确率达到94.67%


<details>
  <summary>Details</summary>
Motivation: 叶脉模式是植物分类的重要形态特征，本研究旨在探索深度学习在基于叶脉特征的自动化植物分类中的应用潜力

Method: 使用瑞典叶片数据集（15个物种，1125张图像），对ResNet50、MobileNetV2和EfficientNetB0三种模型进行训练和测试，采用标准性能指标进行评估

Result: ResNet50训练准确率94.11%但存在过拟合（测试准确率88.45%）；MobileNetV2测试准确率93.34%，泛化能力较好；EfficientNetB0表现最优，测试准确率94.67%，各项指标均超过94.6%

Conclusion: 深度学习特别是EfficientNetB0在基于叶脉特征的自动化植物分类中具有巨大潜力，可用于开发可扩展且准确的植物分类工具

Abstract: This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.

</details>


### [9] [LayoutGKN: Graph Similarity Learning of Floor Plans](https://arxiv.org/abs/2509.03737)
*Casper van Engelenburg,Jan van Gemert,Seyran Khademi*

Main category: cs.CV

TL;DR: LayoutGKN是一种高效的平面图相似性计算方法，通过延迟跨图节点交互并利用可微分图核，在保持相似性计算精度的同时显著提升推理速度


<details>
  <summary>Details</summary>
Motivation: 现有图匹配网络依赖昂贵的跨图节点级交互，导致推理时间缓慢，需要更高效的平面图比较方法

Method: 推迟跨图节点级交互到联合嵌入架构的末端，使用可微分图核作为最终学习节点嵌入的距离函数

Result: LayoutGKN在相似性计算上与图匹配网络相当或更好，同时显著提高了速度

Conclusion: LayoutGKN提供了一种既高效又准确的平面图相似性计算方法，适用于搜索、聚类和数据可视化等应用

Abstract: Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.

</details>


### [10] [Singular Value Few-shot Adaptation of Vision-Language Models](https://arxiv.org/abs/2509.03740)
*Taha Koleilat,Hassan Rivaz,Yiming Xiao*

Main category: cs.CV

TL;DR: CLIP-SVD是一种新颖的多模态参数高效适应技术，通过奇异值分解(SVD)修改CLIP内部参数空间，仅需调整0.04%参数即可实现领域适应，在11个自然和10个生物医学数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs适应方法依赖提示工程和完整模型微调，成本高且可能限制适应质量、破坏模型稳定性并损害预训练知识。

Method: 利用奇异值分解仅微调CLIP参数矩阵的奇异值，重新缩放基向量进行领域适应，同时保留预训练模型。

Result: 在少样本设置下，在11个自然数据集和10个生物医学数据集上实现了最先进的分类结果，在准确性和泛化能力方面均优于先前方法。

Conclusion: CLIP-SVD提供了一种参数高效、可解释的CLIP适应方法，能够保持模型泛化能力的同时显著提升领域适应性能。

Abstract: Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.

</details>


### [11] [STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight Plant Disease Classification](https://arxiv.org/abs/2509.03754)
*Zongsen Qiu*

Main category: cs.CV

TL;DR: 提出STA-Net轻量网络，通过形状-纹理注意力模块(STAM)和解耦注意力机制，在边缘设备上实现高效的植物病害识别


<details>
  <summary>Details</summary>
Motivation: 现有轻量网络使用的通用注意力机制难以捕捉植物病害的细微病理特征（如不规则病斑形状和复杂纹理），需要针对农业领域特点设计专用模型

Method: 1) 使用无训练神经架构搜索(DeepMAD)构建高效网络骨干；2) 提出STAM模块，将注意力分为两个分支：可变形卷积(DCNv4)分支负责形状感知，Gabor滤波器分支负责纹理感知

Result: 在CCMT植物病害数据集上，STA-Net(401K参数，51.1M FLOPs)达到89.00%准确率和88.96% F1分数，显著优于基线模型

Conclusion: 通过解耦注意力机制整合领域知识，为边缘部署的精准农业AI提供了有前景的技术路径

Abstract: Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.

</details>


### [12] [SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object Detection](https://arxiv.org/abs/2509.03786)
*Xinxin Wang,Han Sun,Ningzhong Liu,Huiyu Zhou,Yinan Yao*

Main category: cs.CV

TL;DR: 提出了水下伪装目标检测任务(UCOD)，创建了DeepCamo数据集和SLENet框架，通过Gamma-非对称增强和定位引导分支提升检测性能，在多个数据集上验证了优越性


<details>
  <summary>Details</summary>
Motivation: 水下伪装目标检测对海洋生态学至关重要，但受光学失真、水体浑浊和海洋生物复杂特性影响，该任务研究不足且检测精度受限

Method: 提出SLENet框架，包含Gamma-非对称增强(GAE)模块和定位引导分支(LGB)，通过多尺度特征增强和全局语义信息定位图指导多尺度监督解码器(MSSD)

Result: 在DeepCamo数据集和三个基准COD数据集上的实验表明，SLENet优于当前最先进方法，并展现出对更广泛COD任务的高泛化能力

Conclusion: 该研究填补了水下伪装目标检测领域的空白，提出的SLENet框架有效解决了水下环境的特殊挑战，为海洋生态研究和保护提供了重要技术支撑

Abstract: Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.

</details>


### [13] [Fitting Image Diffusion Models on Video Datasets](https://arxiv.org/abs/2509.03794)
*Juhun Lee,Simon S. Woo*

Main category: cs.CV

TL;DR: 提出利用视频时序信息改进扩散模型训练的方法，无需修改架构即可集成到标准训练流程中，在HandCo数据集上实现2倍加速收敛和更好的生成质量


<details>
  <summary>Details</summary>
Motivation: 传统扩散模型基于独立采样的静态图像训练，这种设计存在信息缺陷，导致收敛慢、分布覆盖有限和泛化能力不足的问题

Method: 利用连续视频帧中的时序归纳偏置来改进扩散训练，通过正则化方法降低梯度方差

Result: 在HandCo数据集上收敛速度提升2倍以上，训练和验证分布上的FID指标均有所降低，生成多样性得到改善

Conclusion: 该方法能有效利用时序信息提升扩散模型性能，加速收敛并提高生成质量

Abstract: Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.

</details>


### [14] [MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting](https://arxiv.org/abs/2509.03800)
*Yuheng Li,Yenho Chen,Yuxiang Lai,Jike Zhong,Vanessa Wildman,Xiaofeng Yang*

Main category: cs.CV

TL;DR: MedVista3D是一个用于3D CT分析的多尺度语义增强视觉语言预训练框架，通过局部和全局图像文本对齐实现疾病检测和整体解读，解决了现有模型在空间推理和报告变异性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 放射学诊断错误（漏读错误、注意力盲区和沟通失败）在临床实践中普遍存在，特别是在3D成像中需要检查数百个切片。现有3D视觉语言模型无法同时满足精确局部检测、全局体积级推理和语义一致的自然语言报告需求。

Method: 采用多尺度语义增强视觉语言预训练框架，进行局部和全局图像文本对齐以实现细粒度表示学习；使用语言模型重写和引入放射学语义匹配库来解决报告变异性问题。

Result: 在零样本疾病分类、报告检索和医学视觉问答方面达到最先进性能，同时在器官分割和预后预测任务上表现出良好的迁移能力。

Conclusion: MedVista3D成功解决了3D CT分析中的关键挑战，为放射学诊断提供了更准确和全面的解决方案，代码和数据集将公开发布。

Abstract: Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.

</details>


### [15] [Causality-guided Prompt Learning for Vision-language Models via Visual Granulation](https://arxiv.org/abs/2509.03803)
*Mengyu Gao,Qiulei Dong*

Main category: cs.CV

TL;DR: CaPL是一种基于因果推理的视觉粒度化文本提示学习方法，通过属性解耦和粒度学习模块来提升CLIP模型在细粒度数据集上的性能表现


<details>
  <summary>Details</summary>
Motivation: 现有的CLIP提示学习方法在细粒度数据集上表现有限，无法有效捕捉细粒度类别间的细微差异

Method: 提出属性解耦模块（使用布朗桥扩散模型分解视觉特征）和粒度学习模块（通过因果推理策略构建视觉粒度），实现更具判别性的文本提示学习

Result: 在15个数据集上的实验表明，CaPL方法显著优于最先进的提示学习方法，特别是在细粒度数据集上

Conclusion: 通过视觉粒度化和因果推理的结合，CaPL方法成功提升了CLIP模型在细粒度识别任务中的性能

Abstract: Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.

</details>


### [16] [EGTM: Event-guided Efficient Turbulence Mitigation](https://arxiv.org/abs/2509.03808)
*Huanan Li,Rui Fan,Juntao Guan,Weidong Hao,Lai Rui,Tong Wu,Yikai Wang,Lin Gu*

Main category: cs.CV

TL;DR: 提出基于事件相机的湍流去除方法，通过事件流的时空分布特性提取像素级可靠引导，实现高效湍流修复，在模型大小、推理延迟和复杂度上大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法需要从有限帧率的同步帧中学习粗粒度湍流动态，计算和存储效率低下。事件相机具有微秒级时间分辨率和稀疏异步成像机制，有望从根本上解决这一瓶颈。

Method: 提出事件幸运洞察理论，揭示湍流扭曲与事件流逆时空分布的关联；构建EGTM框架，从噪声湍流事件中提取像素级可靠无湍流引导进行时间幸运融合；建立首个真实世界事件驱动湍流数据集。

Result: 方法在模型大小、推理延迟和模型复杂度上分别超越现有SOTA方法710倍、214倍和224倍，在修复质量上达到SOTA（PSNR提升0.94，SSIM提升0.08）。

Conclusion: 将事件模态引入湍流去除任务具有显著效率优势，事件相机为湍流修复提供了新的高效解决方案。

Abstract: Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.

</details>


### [17] [Focus Through Motion: RGB-Event Collaborative Token Sparsification for Efficient Object Detection](https://arxiv.org/abs/2509.03872)
*Nan Yang,Yang Wang,Zhanwen Liu,Yuchao Dai,Yang Liu,Xiangmo Zhao*

Main category: cs.CV

TL;DR: FocusMamba提出自适应多模态特征稀疏化方法，通过事件引导的模态稀疏化策略和跨模态融合模块，在RGB-事件检测中实现精度和效率的平衡


<details>
  <summary>Details</summary>
Motivation: 现有RGB-事件检测方法对所有区域进行统一处理，计算成本高且性能不佳；现有token稀疏化方法使用固定阈值，无法适应不同复杂度样本

Method: 提出Event-Guided Multimodal Sparsification (EGMS)策略利用事件相机感知的场景变化识别并丢弃低信息区域；设计Cross-Modality Focus Fusion (CMFF)模块有效整合互补特征

Result: 在DSEC-Det和PKU-DAVIS-SOD数据集上相比现有方法在精度和效率方面均取得优越性能

Conclusion: FocusMamba通过自适应多模态特征稀疏化和高效融合，成功解决了RGB-事件检测中的计算冗余问题，实现了更好的精度-效率权衡

Abstract: Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.

</details>


### [18] [SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition](https://arxiv.org/abs/2509.03873)
*Jiajun Song,Xiaoou Liu*

Main category: cs.CV

TL;DR: 提出了组合零样本食物识别任务CZSFR，针对背景冗余、主食配菜角色混淆和语义偏见三大挑战，开发了SalientFusion方法，在新建的基准数据集上取得SOTA效果


<details>
  <summary>Details</summary>
Motivation: 食物识别受到广泛关注，但新菜品的快速涌现需要识别未见过的食物类别，这推动了零样本食物学习的需求

Method: 提出SalientFusion方法，包含SalientFormer（去除背景冗余并利用深度特征解决角色混淆）和DebiasAT（通过对齐提示与视觉特征减少语义偏见）

Result: 在新建的CZSFood-90和CZSFood-164基准数据集以及通用CZSL数据集上均取得了最先进的性能

Conclusion: 该方法有效解决了组合零样本食物识别中的关键挑战，为食物识别领域提供了新的解决方案

Abstract: Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.

</details>


### [19] [Human Motion Video Generation: A Survey](https://arxiv.org/abs/2509.03883)
*Haiwei Xue,Xiangyang Luo,Zhanghao Hu,Xin Zhang,Xunzhi Xiang,Yuqin Dai,Jianzhuang Liu,Zhensong Zhang,Minglei Li,Jian Yang,Fei Ma,Zhiyong Wu,Changpeng Yang,Zonghong Dai,Fei Richard Yu*

Main category: cs.CV

TL;DR: 这篇论文提供了人类运动视频生成的全面综述，涵盖10多个子任务和5个关键生成阶段，首次讨论了大型语言模型在该领域的潜力，并回顾了视觉、文本和音频三种模态的最新发展。


<details>
  <summary>Details</summary>
Motivation: 现有的人类运动视频生成综述主要关注个别方法，缺乏对整个生成过程的全面概述，本文旨在填补这一空白。

Method: 通过深入调查200多篇论文，详细分析了人类运动视频生成的五个关键阶段：输入、运动规划、运动视频生成、精炼和输出，并覆盖视觉、文本和音频三种主要模态。

Result: 提供了该领域的全面技术趋势和发展概述，识别了推动技术突破的里程碑工作，并创建了包含所有研究模型的完整资源库。

Conclusion: 本综述旨在揭示人类运动视频生成的前景，为推进数字人类的综合应用提供宝贵资源，特别强调了大型语言模型在该领域的潜在应用价值。

Abstract: Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.

</details>


### [20] [OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction](https://arxiv.org/abs/2509.03887)
*Bu Jin,Songen Gu,Xiaotao Hu,Yupeng Zheng,Xiaoyang Guo,Qian Zhang,Xiaoxiao Long,Wei Yin*

Main category: cs.CV

TL;DR: OccTENS是一个生成式占用世界模型，通过时间多尺度预测(TENS)方法解决了长期占用生成中的效率、时间退化问题和可控性不足，在保持计算效率的同时实现了高质量可控的长期3D场景生成。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归方法在预测车辆运动和未来占用场景时存在效率低下、长期生成时间退化严重以及缺乏可控性等问题，需要一种新的方法来同时解决这些挑战。

Method: 将占用世界模型重新表述为时间多尺度预测(TENS)任务，将时间序列建模分解为空间尺度级生成和时间场景级预测，使用TensFormer管理时间因果关系和空间关系，并提出整体姿态聚合策略统一建模占用和自运动。

Result: 实验表明OccTENS在占用质量和推理时间方面均优于现有最先进方法，实现了更高的生成质量和更快的推理速度。

Conclusion: OccTENS通过创新的时间多尺度预测框架和统一的序列建模方法，成功解决了长期占用生成中的关键挑战，为3D场景的动态演化建模提供了有效的解决方案。

Abstract: In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.

</details>


### [21] [Weakly-Supervised Learning of Dense Functional Correspondences](https://arxiv.org/abs/2509.03893)
*Stefan Stojanov,Linan Zhao,Yunzhi Zhang,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: 提出了一种弱监督学习范式，利用视觉语言模型为多视角图像伪标注功能部件，结合密集对比学习来建立密集功能对应关系


<details>
  <summary>Details</summary>
Motivation: 在不同类别物体间建立密集对应关系时，物体功能可以指导对应关系的建立，因为实现特定功能的物体部件通常在形状和外观上具有相似性

Method: 利用视觉语言模型对多视角图像进行伪标注获取功能部件，结合密集对比学习从像素对应关系中提取功能和空间知识，训练新模型建立密集功能对应

Result: 在合成和真实评估数据集上，该方法优于现有的自监督图像表示和基于视觉语言模型的基线解决方案

Conclusion: 通过功能指导的密集对应关系建立方法有效，视觉语言模型和密集对比学习的结合能够成功提取功能语义和空间信息

Abstract: Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.

</details>


### [22] [Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of Vision-Language Model](https://arxiv.org/abs/2509.03895)
*Phuoc-Nguyen Bui,Khanh-Binh Nguyen,Hyunseung Choo*

Main category: cs.CV

TL;DR: Attn-Adapter是一种新颖的在线少样本学习框架，通过双注意力机制增强CLIP的适应性，无需重新训练基础模型即可实现动态适应。


<details>
  <summary>Details</summary>
Motivation: 对比视觉语言模型在零样本图像识别方面表现出色，但在少样本场景中面临挑战，主要因为计算密集的离线微调和提示学习容易导致过拟合。

Method: 提出双注意力机制：Memory Attn-Adapter利用支持样本细化类别嵌入，Local-Global Attn-Adapter通过整合局部和全局特征来丰富图像嵌入。

Result: Attn-Adapter在跨类别和跨数据集泛化方面优于最先进方法，保持高效推理能力，并能在不同CLIP骨干网络上扩展。

Conclusion: 该框架成功解决了少样本学习中的过拟合问题，实现了动态适应，为视觉语言模型的少样本学习提供了有效的解决方案。

Abstract: Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.

</details>


### [23] [SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation](https://arxiv.org/abs/2509.03897)
*Xiaofu Chen,Israfel Salazar,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: SPECS是一个新的无参考图像描述评估指标，通过改进CLIP来强调特异性，在保持与人类判断强相关性的同时大幅提升了计算效率


<details>
  <summary>Details</summary>
Motivation: 现有评估指标存在缺陷：n-gram指标无法捕捉语义正确性，基于表示的相似性指标计算成本高且与人类判断相关性低，而LLM指标虽然相关性强但计算成本过高，不适合模型开发中的迭代使用

Method: 改进CLIP模型，引入新的目标函数来强调特异性：奖励正确细节并惩罚错误细节，从而构建SPECS指标

Result: SPECS在相关性方面与开源LLM指标相当，但计算效率显著更高

Conclusion: SPECS为图像描述模型开发提供了一个实用的迭代评估替代方案，平衡了相关性准确性和计算效率

Abstract: As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.

</details>


### [24] [A Generative Foundation Model for Chest Radiography](https://arxiv.org/abs/2509.03903)
*Yuanfeng Ji,Dan Lin,Xiyue Wang,Lu Zhang,Wenhui Zhou,Chongjian Ge,Ruihang Chu,Xiaoli Yang,Junhan Zhao,Junsong Chen,Xiangde Luo,Sen Yang,Jin Fang,Ping Luo,Ruijiang Li*

Main category: cs.CV

TL;DR: ChexGen是一个生成式视觉-语言基础模型，通过文本、掩码和边界框引导生成胸部X光片，解决了医疗图像标注数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 医疗领域缺乏标注良好的多样化医学图像数据，这阻碍了可靠AI模型的开发。生成式基础模型在自然图像领域已取得重大进展，但在医学图像生成方面仍有挑战。

Method: 基于潜在扩散变换器架构，在迄今为止最大的胸部X射线数据集（96万张X光片-报告对）上进行预训练，开发了统一的文本、掩码和边界框引导合成框架。

Result: 通过专家评估和定量指标验证了准确的X光片合成能力，在数据增强和监督预训练中表现出色，使用少量训练数据就能提升疾病分类、检测和分割任务的性能，并能创建多样化患者队列以增强模型公平性。

Conclusion: 生成式基础模型在构建更准确、数据高效和公平的医疗AI系统中具有变革性作用。

Abstract: The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.

</details>


### [25] [LMVC: An End-to-End Learned Multiview Video Coding Framework](https://arxiv.org/abs/2509.03922)
*Xihua Sheng,Yingwen Zhang,Long Xu,Shiqi Wang*

Main category: cs.CV

TL;DR: 提出端到端学习型多视角视频编码框架LMVC，通过利用独立视角的运动和内容信息来增强依赖视角的压缩效率，显著超越传统MV-HEVC标准


<details>
  <summary>Details</summary>
Motivation: 多视角视频是体视频的关键数据源，但数据量巨大带来存储和传输挑战。现有深度学习视频编码主要关注单视角或立体视频，多视角场景研究不足

Method: 提出基于特征的视角间运动矢量预测方法，利用解码的独立视角运动特征来条件化依赖视角运动编码；提出无视差的视角间上下文预测模块，从解码的独立视角内容特征预测视角间上下文；配合视角间运动熵模型和视角间上下文熵模型

Result: 实验结果表明LMVC框架大幅超越传统MV-HEVC标准的参考软件

Conclusion: 该框架为多视角视频编码领域建立了强大的基准，支持随机访问和向后兼容性

Abstract: Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.

</details>


### [26] [TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained Tubular Shapes](https://arxiv.org/abs/2509.03938)
*Minghui Zhang,Yaoyu Liu,Junyang Wu,Xin You,Hanxiao Zhang,Junjun He,Yun Gu*

Main category: cs.CV

TL;DR: 提出TopoSculpt框架，用于三维管状结构的拓扑细化，通过整体建模、拓扑完整性约束和课程细化方案，显著提升几何和拓扑准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖体素重叠度量，无法保证拓扑正确性和完整性，需要全局拓扑保持和几何误差校正。

Method: 采用整体区域建模策略、拓扑完整性Betti约束和基于持续同调的课程细化方案，从粗到细逐步校正误差。

Result: 在气道和Willis环数据集上，β₀误差分别从69.00降至3.40和从1.65降至0.30，树长度检测和分支检测率提升近10%。

Conclusion: TopoSculpt能有效校正关键拓扑错误，推进复杂三维管状解剖结构的高保真建模。

Abstract: Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.

</details>


### [27] [Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture](https://arxiv.org/abs/2509.03950)
*Alvaro Aranibar Roque,Helga Sebastian*

Main category: cs.CV

TL;DR: 提出基于U-Net和EfficientNet-B4编码器的深度学习管道，用于自动分割气胸区域，在独立测试集上达到IoU 0.7008和Dice分数0.8241的准确率。


<details>
  <summary>Details</summary>
Motivation: 气胸是一种可能危及生命的疾病，胸部X光是首选诊断工具，但小范围气胸可能难以识别，需要自动化辅助诊断系统。

Method: 使用U-Net架构配合EfficientNet-B4编码器，在SIIM-ACR数据集上进行训练，采用数据增强技术和二元交叉熵加Dice损失函数的组合。

Result: 在独立PTX-498测试集上，模型取得了IoU 0.7008和Dice分数0.8241的优秀性能。

Conclusion: 该模型能够准确定位气胸区域，为放射科医生提供有效的辅助诊断支持。

Abstract: Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.

</details>


### [28] [ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD Detection](https://arxiv.org/abs/2509.03951)
*Zhu Wenjie,Zhang Yabin,Xin Jin,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出ANT方法，利用多模态大语言模型构建自适应负文本空间，提升OOD检测性能，在ImageNet上FPR95降低4.2%，达到新SOTA


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对OOD图像的理解，难以构建准确的负空间，且假负标签会显著降低近OOD检测性能

Method: 利用MLLM的描述和推理能力，生成表达性负句子精确表征OOD分布，为视觉相似的ID类别生成定制化负标签，设计自适应加权分数平衡两种负文本空间

Result: 在ImageNet基准上显著降低FPR95达4.2%，建立新的最先进水平，方法无需训练且零样本，具有高可扩展性

Conclusion: ANT方法通过MLLM构建自适应负文本空间，有效解决了OOD检测中的负空间构建和假负标签问题，在近OOD和远OOD设置下都表现出色

Abstract: The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.

</details>


### [29] [Multimodal Feature Fusion Network with Text Difference Enhancement for Remote Sensing Change Detection](https://arxiv.org/abs/2509.03961)
*Yijun Zhou,Yikui Zhai,Zilu Ying,Tingfeng Xian,Wenlve Zhou,Zhiheng Zhou,Xiaolin Tian,Xudong Jia,Hongsheng Zhang,C. L. Philip Chen*

Main category: cs.CV

TL;DR: MMChange是一个多模态遥感变化检测方法，结合图像和文本模态，通过图像特征精炼、视觉语言模型生成语义描述、文本差异增强和图像文本特征融合模块，显著提升了变化检测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要依赖单一图像模态，在特征表示、变化模式建模和泛化能力方面存在局限，特别是在光照和噪声干扰下表现不佳。

Method: 提出MMChange多模态框架：1）图像特征精炼模块突出关键区域并抑制环境噪声；2）使用视觉语言模型生成双时相图像的语义描述；3）文本差异增强模块捕获细粒度语义变化；4）图像文本特征融合模块实现跨模态深度整合。

Result: 在LEVIRCD、WHUCD和SYSUCD数据集上的大量实验表明，MMChange在多个指标上一致超越最先进方法，验证了其有效性。

Conclusion: 多模态方法能够有效提升遥感变化检测的性能，图像和文本模态的结合为变化检测提供了更丰富和鲁棒的特征表示。

Abstract: Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.

</details>


### [30] [SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for Histopathology Whole Slide Image Classification](https://arxiv.org/abs/2509.03973)
*Yu Bai,Zitong Yu,Haowen Tian,Xijing Wang,Shuo Yan,Lin Wang,Honglin Li,Xitong Ling,Bo Zhang,Zheng Zhang,Wufan Wang,Hui Gao,Xiangyang Gong,Wendong Wang*

Main category: cs.CV

TL;DR: SAC-MIL是一种用于WSI分类的空间感知相关多实例学习方法，通过位置编码模块和SAC块实现全实例相关性计算，在多个数据集上达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 解决WSI分类中空间关系编码和序列长度外推问题，提供比基于Transformer方法更易于部署的解决方案

Method: 使用基于坐标的位置编码模块处理空间关系，采用MLP基础的SAC块进行线性时间复杂度的全实例相关性计算

Result: 在CAMELYON-16、TCGA-LUNG和TCGA-BRAC数据集上实现了最先进的性能

Conclusion: SAC-MIL提供了一种高效且易于部署的WSI分类方法，解决了传统方法的空间编码和计算复杂度问题

Abstract: We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.

</details>


### [31] [Improving Vessel Segmentation with Multi-Task Learning and Auxiliary Data Available Only During Model Training](https://arxiv.org/abs/2509.03975)
*Daniel Sobotka,Alexander Herold,Matthias Perkonigg,Lucian Beer,Nina Bastati,Alina Sablatnig,Ahmed Ba-Ssalamah,Georg Langs*

Main category: cs.CV

TL;DR: 提出多任务学习框架，利用训练时辅助的对比增强MRI数据来改善无对比剂肝脏MRI血管分割，减少对标注数据的需求


<details>
  <summary>Details</summary>
Motivation: 解决无对比剂MRI血管分割的挑战，因为对比增强成像序列并非普遍获取，而无对比剂图像更常见但分割困难且需要大量标注数据

Method: 多任务学习框架，利用训练时可用的配对原生和对比增强MRI数据（有/无血管标注）进行联合训练，通过共享任务结构提升特征表示

Result: 辅助数据提高了血管分割准确性，即使推理时不可用；在标注数据少时效果最显著；在脑肿瘤分割验证中证实了跨领域有效性

Conclusion: 辅助信息成像模态可以在仅训练时可用的情况下增强专家标注，为医学图像分割提供有效的数据增强策略

Abstract: Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.

</details>


### [32] [Promptception: How Sensitive Are Large Multimodal Models to Prompts?](https://arxiv.org/abs/2509.03986)
*Mohamed Insaf Ismithdeen,Muhammad Uzair Khattak,Salman Khan*

Main category: cs.CV

TL;DR: 大型多模态模型在多项选择题回答中，提示词设计的微小变化会导致准确性偏差高达15%，Promptception框架系统评估提示词敏感性，发现专有模型对提示词更敏感，开源模型更稳定但处理复杂提示困难。


<details>
  <summary>Details</summary>
Motivation: 大型多模态模型在多项选择题回答中的提示词设计缺乏系统研究，现有评估存在不透明和不公平问题，因为模型通常使用精心挑选的提示词来报告最佳性能。

Method: 提出Promptception框架，包含61种提示类型、15个类别和6个超类别，用于评估10个大型多模态模型在3个多项选择题基准测试上的表现。

Result: 专有模型对提示词措辞更敏感，反映出与指令语义更紧密的对齐；开源模型更稳定但在处理细致和复杂措辞时表现较差。

Conclusion: 基于分析结果，提出了针对专有和开源大型多模态模型的提示原则，以实现更稳健和公平的模型评估。

Abstract: Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.

</details>


### [33] [SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy Representation](https://arxiv.org/abs/2509.03999)
*Han Huang,Han Sun,Ningzhong Liu,Huiyu Zhou,Jiaquan Shen*

Main category: cs.CV

TL;DR: 提出SliceSemOcc框架，通过垂直切片和SEAttention3D模块解决3D语义占据预测中高度轴信息忽略的问题，显著提升小物体类别的检测性能


<details>
  <summary>Details</summary>
Motivation: 现有3D语义占据预测方法在处理体素特征时忽视高度轴信息，传统通道注意力机制对所有高度层赋予相同权重，限制了不同高度特征的表征能力

Method: 1) 使用全局和局部垂直切片提取高度轴体素特征；2) 全局局部融合模块协调细粒度空间细节与整体上下文信息；3) SEAttention3D模块通过平均池化保持高度分辨率并为每个高度层分配动态通道注意力权重

Result: 在nuScenes-SurroundOcc和nuScenes-OpenOccupancy数据集上的大量实验表明，该方法显著提升了平均IoU，特别是在大多数小物体类别上取得了明显增益

Conclusion: 提出的SliceSemOcc框架通过有效的垂直切片处理和高度感知注意力机制，成功解决了3D语义占据预测中的高度信息利用问题，消融研究进一步验证了该框架的有效性

Abstract: Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.

</details>


### [34] [Detecting Regional Spurious Correlations in Vision Transformers via Token Discarding](https://arxiv.org/abs/2509.04009)
*Solha Kang,Esla Timothy Anzaku,Wesley De Neve,Arnout Van Messem,Joris Vankerschaver,Francois Rameau,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文提出了一种检测视觉Transformer中虚假相关性的新方法，通过在ImageNet数据集上进行大规模实验，证明了该方法能有效识别虚假相关性，并发现训练方法对模型依赖虚假相关性的影响。


<details>
  <summary>Details</summary>
Motivation: 神经网络视觉模型可能利用数据中的无意模式进行预测，这些虚假相关性会导致模型基于错误信号做出正确预测，影响模型的可信度和泛化能力。

Method: 使用监督和自监督训练的视觉Transformer模型，在ImageNet数据集上进行大规模实验，提出检测虚假相关性的新方法。

Result: 方法能有效识别虚假相关性，发现训练方法对模型依赖虚假相关性有显著影响，识别出ImageNet中易被模型检测的虚假信号类别。

Conclusion: 提供了包含虚假信号的图像列表，呼吁在研究中使用时保持谨慎，并通过侵入性乳腺肿块分类的案例研究验证了方法的实际应用价值。

Abstract: Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.

</details>


### [35] [Learning from Majority Label: A Novel Problem in Multi-class Multiple-Instance Learning](https://arxiv.org/abs/2509.04023)
*Shiku Kaito,Shinnosuke Matsuo,Daiki Suehiro,Ryoma Bise*

Main category: cs.CV

TL;DR: 提出LML问题：从多数标签学习，使用计数网络和多数比例增强模块来提升多类多示例学习性能


<details>
  <summary>Details</summary>
Motivation: 解决多类多示例学习中基于多数标签的分类问题，在病理图像分割、政治投票预测、客户情感分析和环境监测等应用中具有重要价值

Method: 提出计数网络来生成包级多数标签，并通过多数比例增强模块移除包中少数类实例来增加多数类比例

Result: 在四个数据集上相比传统MIL方法表现出优越性，消融研究证实了各模块的有效性

Conclusion: LML是一个有价值的新问题，所提出的计数网络和MPEM模块能有效提升多类多示例学习性能

Abstract: The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.

</details>


### [36] [Millisecond-Response Tracking and Gazing System for UAVs: A Domestic Solution Based on "Phytium + Cambricon"](https://arxiv.org/abs/2509.04043)
*Yuchen Zhu,Longxiang Yin,Kai Zhao*

Main category: cs.CV

TL;DR: 基于飞腾处理器和寒武纪加速卡的异构计算架构，实现毫秒级响应的无人机跟踪凝视系统


<details>
  <summary>Details</summary>
Motivation: 传统摄像头系统在动态场景中响应延迟超过200ms，无法满足复杂场景的实时性要求，需要解决深度特征提取能力不足和计算架构效率瓶颈问题

Method: 硬件采用飞腾FT-2000/4处理器和MLU220加速卡的多卡并行协同计算架构，软件集成轻量化YOLOv5s检测网络与DeepSORT级联跟踪算法，形成"检测-跟踪-反馈"闭环控制链

Result: 系统在1920*1080分辨率视频流处理中实现50-100ms的稳定单帧综合处理延迟，多尺度目标识别准确率超过98.5%，兼具低延迟和高精度特性

Conclusion: 该研究为无人机监控和国产芯片应用提供了创新解决方案

Abstract: In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.

</details>


### [37] [A Re-ranking Method using K-nearest Weighted Fusion for Person Re-identification](https://arxiv.org/abs/2509.04050)
*Quang-Huy Che,Le-Chuong Nguyen,Gia-Nghia Tran,Dinh-Duy Phan,Vinh-Tiep Nguyen*

Main category: cs.CV

TL;DR: 提出了一种基于K近邻加权融合的无监督重排序方法，通过聚合多视角特征来减少视角偏差，显著提升了行人重识别的Rank@1和mAP指标，同时具有较高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统重排序方法主要依赖单视角图像特征，容易受到姿态变化、视角变化和遮挡等问题的影响，导致视角偏差。使用多视角特征可以更好地表示行人身份，减少这种偏差。

Method: 采用K近邻加权融合(KWF)方法，在无监督情况下选择K个最邻近的特征来生成多视角特征。研究特征聚合过程中的权重选择策略，找到有效的策略。该方法不需要模型微调或额外标注。

Result: 在Market1501、MSMT17和Occluded-DukeMTMC数据集上验证，相比初始结果，在MSMT17和Occluded-DukeMTMC数据集上Rank@1分别提升了9.8%和22.0%，mAP也有显著提升，且计算效率优于其他重排序方法。

Conclusion: 提出的无监督多视角特征重排序方法有效解决了视角偏差问题，显著提升了行人重识别的性能，同时具有较好的计算效率，适用于大规模数据集。

Abstract: In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.

</details>


### [38] [TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph](https://arxiv.org/abs/2509.04086)
*Yaru Chen,Faegheh Sardari,Peiliang Zhang,Ruohao Guo,Yang Xiang,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出结合双向文本融合和类别感知时序图的新方法，在弱监督音频-视觉视频解析任务中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有方法要么将噪声段级伪标签作为可靠监督，要么让无差别注意力将所有帧传播错误，导致训练过程中错误被反复放大

Method: 结合BiT模块进行语义注入和动态校准来定位和净化语义线索，使用CATS模块进行语义传播和连接以实现精确的语义信息跨时间传播

Result: 在LLP和UnAV-100两个基准数据集上的多个关键指标上达到最先进性能

Conclusion: 通过整合两种研究方向的优势并互补，有效解决了弱监督AVVP任务中的错误放大问题

Abstract: Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.

</details>


### [39] [TriLiteNet: Lightweight Model for Multi-Task Visual Perception](https://arxiv.org/abs/2509.04092)
*Quang-Huy Che,Duc-Khai Lam*

Main category: cs.CV

TL;DR: TriLiteNet是一个高效的多任务全景驾驶感知模型，在BDD100k数据集上实现车辆检测、可行驶区域分割和车道线分割三个任务，参数量仅2.35M，计算量7.72 GFLOPs，在嵌入式设备上具有低延迟和合理功耗。


<details>
  <summary>Details</summary>
Motivation: 高级驾驶辅助系统(ADAS)需要高效的感知模型来实现实时处理和响应，确保在真实环境中的安全性和有效性。

Method: 提出TriLiteNet模型，设计用于同时处理多个全景驾驶感知任务，优化性能的同时保持低计算成本，包括基础版和微型配置(仅0.14M参数)。

Result: 在BDD100k数据集上，TriLiteNet_base实现车辆检测召回率85.6%、可行驶区域分割mIoU 92.4%、车道线分割准确率82.3%，在嵌入式设备上表现出低延迟和合理功耗。

Conclusion: TriLiteNet通过平衡性能、计算效率和可扩展性，为现实世界自动驾驶应用提供了实用且可部署的解决方案。

Abstract: Efficient perception models are essential for Advanced Driver Assistance
Systems (ADAS), as these applications require rapid processing and response to
ensure safety and effectiveness in real-world environments. To address the
real-time execution needs of such perception models, this study introduces the
TriLiteNet model. This model can simultaneously manage multiple tasks related
to panoramic driving perception. TriLiteNet is designed to optimize performance
while maintaining low computational costs. Experimental results on the BDD100k
dataset demonstrate that the model achieves competitive performance across
three key tasks: vehicle detection, drivable area segmentation, and lane line
segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of
85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for
drivable area segmentation, and an Acc of 82.3% for lane line segmentation with
only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed
model includes a tiny configuration with just 0.14M parameters, which provides
a multi-task solution with minimal computational demand. Evaluated for latency
and power consumption on embedded devices, TriLiteNet in both configurations
shows low latency and reasonable power during inference. By balancing
performance, computational efficiency, and scalability, TriLiteNet offers a
practical and deployable solution for real-world autonomous driving
applications. Code is available at https://github.com/chequanghuy/TriLiteNet.

</details>


### [40] [DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset](https://arxiv.org/abs/2509.04117)
*Mustafa Sakhai,Kaung Sithu,Min Khant Soe Oke,Maciej Wielgosz*

Main category: cs.CV

TL;DR: DVS-PedX是一个用于行人检测和过马路意图分析的神经形态数据集，包含合成和真实世界的事件流数据，支持在正常和恶劣天气条件下进行事件相机研究。


<details>
  <summary>Details</summary>
Motivation: 为了解决基于事件相机的行人安全检测和意图预测问题，特别是在不同天气条件下的鲁棒性需求，需要专门的数据集来推动神经形态感知研究。

Method: 通过两种互补数据源构建数据集：(1) CARLA模拟器生成合成事件流，控制天气和光照条件；(2) 使用v2e工具将真实JAAD行车记录仪视频转换为事件流。提供RGB帧、事件帧、标签和原始事件文件。

Result: 数据集包含配对的多模态数据，基线SNN实验展示了数据集的可用性，并揭示了模拟到现实的差距，为领域适应和多模态融合提供了动机。

Conclusion: DVS-PedX数据集旨在加速基于事件的行人安全、意图预测和神经形态感知研究，为相关领域提供重要的数据资源。

Abstract: Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.

</details>


### [41] [TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering](https://arxiv.org/abs/2509.04123)
*Ayan Banerjee,Josep Lladós,Umapada Pal,Anjan Dutta*

Main category: cs.CV

TL;DR: TaleDiffusion是一个用于生成多角色故事可视化内容的新框架，通过迭代过程保持角色一致性并准确分配对话


<details>
  <summary>Details</summary>
Motivation: 现有方法在角色一致性和对话渲染方面存在困难，导致artifact生成和不连贯的叙事

Method: 使用预训练LLM生成每帧描述、角色细节和对话，采用有界注意力掩码技术控制角色交互，使用身份一致性自注意力机制和区域感知交叉注意力确保角色一致性和精确对象放置

Result: 实验结果表明TaleDiffusion在一致性、降噪和对话渲染方面优于现有方法

Conclusion: 该框架成功解决了多角色故事可视化中的关键挑战，提供了更连贯和一致的故事叙述体验

Abstract: Text-to-story visualization is challenging due to the need for consistent
interaction among multiple characters across frames. Existing methods struggle
with character consistency, leading to artifact generation and inaccurate
dialogue rendering, which results in disjointed storytelling. In response, we
introduce TaleDiffusion, a novel framework for generating multi-character
stories with an iterative process, maintaining character consistency, and
accurate dialogue assignment via postprocessing. Given a story, we use a
pre-trained LLM to generate per-frame descriptions, character details, and
dialogues via in-context learning, followed by a bounded attention-based
per-box mask technique to control character interactions and minimize
artifacts. We then apply an identity-consistent self-attention mechanism to
ensure character consistency across frames and region-aware cross-attention for
precise object placement. Dialogues are also rendered as bubbles and assigned
to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion
outperforms existing methods in consistency, noise reduction, and dialogue
rendering.

</details>


### [42] [MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image Generation](https://arxiv.org/abs/2509.04126)
*Yuan Zhao,Liu Lin*

Main category: cs.CV

TL;DR: 提出了MEPG框架，通过位置风格感知LLM和多专家扩散模块，显著提升多元素提示的图像生成质量和风格多样性


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像扩散模型在处理复杂多元素提示时表现不佳，以及风格多样性有限的问题

Method: 使用监督微调的LLM分解提示为空间坐标和风格指令，采用注意力门控机制动态选择专家模型进行区域生成

Result: 在图像质量和风格多样性方面显著优于同骨干网络的基线模型

Conclusion: MEPG框架有效解决了多元素提示生成问题，提供了可扩展的专家模型集成方案和实时编辑能力

Abstract: Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.

</details>


### [43] [Revisiting Simple Baselines for In-The-Wild Deepfake Detection](https://arxiv.org/abs/2509.04150)
*Orlando Castaneda,Kevin So-Tang,Kshitij Gurung*

Main category: cs.CV

TL;DR: 通过更好的超参数调优，简单的预训练视觉骨干网络方法在Deepfake-Eval-2024基准上达到了81%的准确率，与商业检测器性能相当


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测器在受控数据集上表现良好，但在真实场景中性能显著下降，需要更实用的检测方法

Method: 重新评估Ojha等人提出的方法，使用预训练视觉骨干网络进行适配，并通过改进的超参数调优策略

Result: 在Deepfake-Eval-2024基准上达到81%准确率，比之前报告的基线方法提升18%，与领先商业检测器82%的准确率相当

Conclusion: 简单的预训练模型适配方法经过适当调优后，能够在真实场景中达到与商业检测器竞争的性能，具有实际部署的潜力

Abstract: The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.

</details>


### [44] [YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind Turbine Components](https://arxiv.org/abs/2509.04156)
*Serhii Svystun,Pavlo Radiuk,Oleksandr Melnychenko,Oleg Savenko,Anatoliy Sachenko*

Main category: cs.CV

TL;DR: 开发基于YOLO的集成深度学习模型，融合可见光和热成像数据，提高风力发电厂无人机检测缺陷的准确率


<details>
  <summary>Details</summary>
Motivation: 无人机配备先进传感器为风力发电厂监测提供了新机会，但可靠缺陷检测需要高分辨率数据和高效的多光谱图像处理方法

Method: 提出集成方法，将通用YOLOv8模型与专用热成像模型结合，使用复杂的边界框融合算法整合预测结果

Result: 实验显示该方法达到mAP@.5为0.93，F1分数为0.90，优于单独YOLOv8模型的0.91 mAP@.5

Conclusion: 结合多个YOLO架构和融合多光谱数据提供了更可靠的解决方案，改善了视觉和热缺陷的检测能力

Abstract: Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.

</details>
