<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 123]
- [cs.HC](#cs.HC) [Total: 1]
- [quant-ph](#quant-ph) [Total: 2]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.GR](#cs.GR) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.LG](#cs.LG) [Total: 5]
- [cs.RO](#cs.RO) [Total: 8]
- [eess.IV](#eess.IV) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [PolypSeg-GradCAM: Towards Explainable Computer-Aided Gastrointestinal Disease Detection Using U-Net Based Segmentation and Grad-CAM Visualization on the Kvasir Dataset](https://arxiv.org/abs/2509.18159)
*Akwasi Asare,Ulas Bagci*

Main category: cs.CV

TL;DR: 提出PolypSeg-GradCAM框架，结合U-Net和Grad-CAM实现可解释的息肉分割，在Kvasir-SEG数据集上达到高精度分割性能。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌是全球主要癌症死因，胃肠道息肉是重要前兆。现有深度学习分割方法缺乏可解释性，阻碍临床应用。

Method: 集成U-Net架构和梯度加权类激活映射(Grad-CAM)，在1000张标注内窥镜图像数据集上进行训练和评估。

Result: 测试集平均IoU达0.9257，训练和验证集Dice系数均高于0.96，Grad-CAM可视化确认预测基于临床相关区域。

Conclusion: 该框架将高分割精度与可解释性结合，为可靠AI辅助结肠镜检查提供了可行方案，有助于改善结直肠癌早期预防。

Abstract: Colorectal cancer (CRC) remains one of the leading causes of cancer-related
morbidity and mortality worldwide, with gastrointestinal (GI) polyps serving as
critical precursors according to the World Health Organization (WHO). Early and
accurate segmentation of polyps during colonoscopy is essential for reducing
CRC progression, yet manual delineation is labor-intensive and prone to
observer variability. Deep learning methods have demonstrated strong potential
for automated polyp analysis, but their limited interpretability remains a
barrier to clinical adoption. In this study, we present PolypSeg-GradCAM, an
explainable deep learning framework that integrates the U-Net architecture with
Gradient-weighted Class Activation Mapping (Grad-CAM) for transparent polyp
segmentation. The model was trained and evaluated on the Kvasir-SEG dataset of
1000 annotated endoscopic images. Experimental results demonstrate robust
segmentation performance, achieving a mean Intersection over Union (IoU) of
0.9257 on the test set and consistently high Dice coefficients (F-score > 0.96)
on training and validation sets. Grad-CAM visualizations further confirmed that
predictions were guided by clinically relevant regions, enhancing transparency
and trust in the model's decisions. By coupling high segmentation accuracy with
interpretability, PolypSeg-GradCAM represents a step toward reliable,
trustworthy AI-assisted colonoscopy and improved early colorectal cancer
prevention.

</details>


### [2] [PerceptronCARE: A Deep Learning-Based Intelligent Teleopthalmology Application for Diabetic Retinopathy Diagnosis](https://arxiv.org/abs/2509.18160)
*Akwasi Asare,Isaac Baffour Senkyire,Emmanuel Freeman,Simon Hilary Ayinedenaba Aluze-Ele,Kelvin Kwao*

Main category: cs.CV

TL;DR: 提出PerceptronCARE深度学习系统，用于糖尿病视网膜病变自动检测，在临床和远程医疗中实现实时筛查，准确率达85.4%。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变是导致成人视力丧失的主要原因，尤其在医疗资源匮乏地区，需要高效、可扩展的筛查解决方案。

Method: 使用多种卷积神经网络（ResNet-18、EfficientNet-B0、SqueezeNet）开发并评估系统，平衡准确性和计算效率。

Result: 最终模型分类准确率达到85.4%，具备云可扩展性、安全数据管理和多用户框架，支持实时筛查。

Conclusion: AI驱动的远程医疗解决方案可有效扩展糖尿病视网膜病变筛查的覆盖范围，特别是在偏远和资源受限环境中。

Abstract: Diabetic retinopathy is a leading cause of vision loss among adults and a
major global health challenge, particularly in underserved regions. This study
presents PerceptronCARE, a deep learning-based teleophthalmology application
designed for automated diabetic retinopathy detection using retinal images. The
system was developed and evaluated using multiple convolutional neural
networks, including ResNet-18, EfficientNet-B0, and SqueezeNet, to determine
the optimal balance between accuracy and computational efficiency. The final
model classifies disease severity with an accuracy of 85.4%, enabling real-time
screening in clinical and telemedicine settings. PerceptronCARE integrates
cloud-based scalability, secure patient data management, and a multi-user
framework, facilitating early diagnosis, improving doctor-patient interactions,
and reducing healthcare costs. This study highlights the potential of AI-driven
telemedicine solutions in expanding access to diabetic retinopathy screening,
particularly in remote and resource-constrained environments.

</details>


### [3] [Self Identity Mapping](https://arxiv.org/abs/2509.18165)
*Xiuding Cai,Yaoyao Zhu,Linjie Fu,Dong Miao,Yu Yao*

Main category: cs.CV

TL;DR: 提出了SIM（自身份映射）正则化框架，通过逆向映射机制增强表示学习，降低前向传播中的信息损失并促进梯度流动。还提出了ρSIM版本以解决计算效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统正则化技术通常依赖启发式方法，在不同设置下效果不稳定。需要一种更可靠、有效的数据内在正则化方法。

Method: SIM框架通过重构输入来从变换后的输出中恢复原始信息，使用逆向映射机制。ρSIM通过补丁级特征采样和基于投影的方法重构潜在特征来降低复杂度。

Result: 在图像分类、少样本提示学习和领域泛化等任务上均优于基线方法，能够有效保留语义信息并提升密集到密集任务的性能。

Conclusion: SIM是一种模型无关、任务无关的正则化器，可无缝集成作为即插即用模块，适用于不同网络架构和任务，且与现有正则化方法正交，能提升其效果。

Abstract: Regularization is essential in deep learning to enhance generalization and
mitigate overfitting. However, conventional techniques often rely on
heuristics, making them less reliable or effective across diverse settings. We
propose Self Identity Mapping (SIM), a simple yet effective, data-intrinsic
regularization framework that leverages an inverse mapping mechanism to enhance
representation learning. By reconstructing the input from its transformed
output, SIM reduces information loss during forward propagation and facilitates
smoother gradient flow. To address computational inefficiencies, We instantiate
SIM as $ \rho\text{SIM} $ by incorporating patch-level feature sampling and
projection-based method to reconstruct latent features, effectively lowering
complexity. As a model-agnostic, task-agnostic regularizer, SIM can be
seamlessly integrated as a plug-and-play module, making it applicable to
different network architectures and tasks.
  We extensively evaluate $\rho\text{SIM}$ across three tasks: image
classification, few-shot prompt learning, and domain generalization.
Experimental results show consistent improvements over baseline methods,
highlighting $\rho\text{SIM}$'s ability to enhance representation learning
across various tasks. We also demonstrate that $\rho\text{SIM}$ is orthogonal
to existing regularization methods, boosting their effectiveness. Moreover, our
results confirm that $\rho\text{SIM}$ effectively preserves semantic
information and enhances performance in dense-to-dense tasks, such as semantic
segmentation and image translation, as well as in non-visual domains including
audio classification and time series anomaly detection. The code is publicly
available at https://github.com/XiudingCai/SIM-pytorch.

</details>


### [4] [MAGIA: Sensing Per-Image Signals from Single-Round Averaged Gradients for Label-Inference-Free Gradient Inversion](https://arxiv.org/abs/2509.18170)
*Zhanting Zhou,Jinbo Wang,Zeqin Wu,Fengli Zhang*

Main category: cs.CV

TL;DR: MAGIA是一种基于动量的自适应梯度反演攻击方法，通过组合重缩放和动量混合损失，在单轮平均梯度场景下实现高效的多图像重建。


<details>
  <summary>Details</summary>
Motivation: 解决在单轮平均梯度(SAG)机制下，由于每个样本的梯度线索被纠缠在批量平均梯度中，导致传统梯度反演攻击难以重建单个图像的问题。

Method: 1. 组合重缩放：通过闭式组合重缩放创建更紧的优化边界；2. 动量混合：将整个批次和子集损失进行动量混合以确保重建鲁棒性；3. 随机子集探测：通过探测随机数据子集来感知潜在的每图像信号。

Result: 在大批量场景下，MAGIA显著优于现有先进方法，实现了高保真度的多图像重建，且计算开销与标准求解器相当，无需任何辅助信息。

Conclusion: MAGIA框架在挑战性的单轮平均梯度场景下有效解决了梯度反演问题，为隐私保护研究提供了重要参考。

Abstract: We study gradient inversion in the challenging single round averaged gradient
SAG regime where per sample cues are entangled within a single batch mean
gradient. We introduce MAGIA a momentum based adaptive correction on gradient
inversion attack a novel label inference free framework that senses latent per
image signals by probing random data subsets. MAGIA objective integrates two
core innovations 1 a closed form combinatorial rescaling that creates a
provably tighter optimization bound and 2 a momentum based mixing of whole
batch and subset losses to ensure reconstruction robustness. Extensive
experiments demonstrate that MAGIA significantly outperforms advanced methods
achieving high fidelity multi image reconstruction in large batch scenarios
where prior works fail. This is all accomplished with a computational footprint
comparable to standard solvers and without requiring any auxiliary information.

</details>


### [5] [Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR](https://arxiv.org/abs/2509.18174)
*Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan*

Main category: cs.CV

TL;DR: Baseer是一个专门针对阿拉伯语文档OCR的视觉语言模型，通过领域特定微调在阿拉伯语OCR任务上实现了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语文档OCR具有挑战性，因为阿拉伯语的草书字体、多样字体、变音符号和从右到左的书写方向。现有的多模态大语言模型在阿拉伯语上的性能有限。

Method: 使用结合合成和真实世界文档的大规模数据集，采用仅解码器微调策略来适应预训练的MLLM，同时保留通用视觉特征。

Result: Baseer显著优于现有的开源和商业解决方案，实现了0.25的WER，在阿拉伯语文档OCR领域建立了新的最先进水平。

Conclusion: 研究结果强调了通用MLLM的领域特定适应的优势，并为像阿拉伯语这样的形态丰富语言的高精度OCR建立了强大的基准。

Abstract: Arabic document OCR remains a challenging task due to the language's cursive
script, diverse fonts, diacritics, and right-to-left orientation. While modern
Multimodal Large Language Models (MLLMs) have advanced document understanding
for high-resource languages, their performance on Arabic remains limited. In
this work, we introduce Baseer, a vision-language model fine- tuned
specifically for Arabic document OCR. Leveraging a large-scale dataset
combining synthetic and real-world documents, Baseer is trained using a
decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving
general visual features. We also present Misraj-DocOCR, a high-quality,
expert-verified benchmark designed for rigorous evaluation of Arabic OCR
systems. Our experiments show that Baseer significantly outperforms existing
open-source and commercial solutions, achieving a WER of 0.25 and establishing
a new state-of-the-art in the domain of Arabic document OCR. Our results
highlight the benefits of domain-specific adaptation of general-purpose MLLMs
and establish a strong baseline for high-accuracy OCR on morphologically rich
languages like Arabic.

</details>


### [6] [A Deep Learning Approach for Spatio-Temporal Forecasting of InSAR Ground Deformation in Eastern Ireland](https://arxiv.org/abs/2509.18176)
*Wendong Yao,Saeed Azadnejad,Binhua Huang,Shane Donohue,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种基于CNN-LSTM混合深度学习框架的InSAR时序数据预测方法，将稀疏点测量转换为密集时空张量，实现了更准确的地面形变预测。


<details>
  <summary>Details</summary>
Motivation: 解决从稀疏InSAR时序数据预测未来地面形变的挑战，传统方法难以捕捉复杂的时空动态特征。

Method: 设计并实现了CNN-LSTM混合模型，同时学习空间模式和时间依赖关系，将稀疏点测量转换为密集时空张量。

Result: 与LightGBM和LASSO回归等基准模型相比，提出的架构提供了更准确和空间一致的预测，建立了新的性能基准。

Conclusion: 验证了时空深度学习在高分辨率形变预测中的有效性和潜力，表明集成时空方法对于捕捉地面形变复杂动态的必要性。

Abstract: Monitoring ground displacement is crucial for urban infrastructure stability
and mitigating geological hazards. However, forecasting future deformation from
sparse Interferometric Synthetic Aperture Radar (InSAR) time-series data
remains a significant challenge. This paper introduces a novel deep learning
framework that transforms these sparse point measurements into a dense
spatio-temporal tensor. This methodological shift allows, for the first time,
the direct application of advanced computer vision architectures to this
forecasting problem. We design and implement a hybrid Convolutional Neural
Network and Long-Short Term Memory (CNN-LSTM) model, specifically engineered to
simultaneously learn spatial patterns and temporal dependencies from the
generated data tensor. The model's performance is benchmarked against powerful
machine learning baselines, Light Gradient Boosting Machine and LASSO
regression, using Sentinel-1 data from eastern Ireland. Results demonstrate
that the proposed architecture provides significantly more accurate and
spatially coherent forecasts, establishing a new performance benchmark for this
task. Furthermore, an interpretability analysis reveals that baseline models
often default to simplistic persistence patterns, highlighting the necessity of
our integrated spatio-temporal approach to capture the complex dynamics of
ground deformation. Our findings confirm the efficacy and potential of
spatio-temporal deep learning for high-resolution deformation forecasting.

</details>


### [7] [A Framework for Generating Artificial Datasets to Validate Absolute and Relative Position Concepts](https://arxiv.org/abs/2509.18177)
*George Corrêa de Araújo,Helena de Almeida Maia,Helio Pedrini*

Main category: cs.CV

TL;DR: Scrapbook框架是一种生成大规模数据集的新方法，用于评估AI模型对基本概念（如物体识别、位置关系、属性识别）的理解能力。实验发现当前模型在物体识别方面表现良好，但在位置理解和约束性问题方面存在困难。


<details>
  <summary>Details</summary>
Motivation: 开发一个系统性的方法来评估AI模型对基本概念的理解能力，为后续复杂任务奠定基础。通过生成多样化的问题数据集来验证模型对基础元素的掌握程度。

Method: 提出Scrapbook框架，生成包含大量关于单个概念问题和广泛语言变体的数据集。重点关注物体识别、绝对和相对位置、属性识别等基本概念。

Result: 实验显示当代模型在物体识别和枚举方面表现熟练，但在理解位置信息和处理约束性问题时遇到挑战。MobileVLM-V2模型出现显著答案分歧和错误答案，其他模型则表现出肯定答案偏见，在几何形状和位置信息问题上表现不佳。

Conclusion: Scrapbook框架为生成多样化和全面数据集提供了有价值的工具，可用于系统评估和提升AI模型性能，特别是在基础概念理解方面需要进一步改进。

Abstract: In this paper, we present the Scrapbook framework, a novel methodology
designed to generate extensive datasets for probing the learned concepts of
artificial intelligence (AI) models. The framework focuses on fundamental
concepts such as object recognition, absolute and relative positions, and
attribute identification. By generating datasets with a large number of
questions about individual concepts and a wide linguistic variation, the
Scrapbook framework aims to validate the model's understanding of these basic
elements before tackling more complex tasks. Our experimental findings reveal
that, while contemporary models demonstrate proficiency in recognizing and
enumerating objects, they encounter challenges in comprehending positional
information and addressing inquiries with additional constraints. Specifically,
the MobileVLM-V2 model showed significant answer disagreements and plausible
wrong answers, while other models exhibited a bias toward affirmative answers
and struggled with questions involving geometric shapes and positional
information, indicating areas for improvement in understanding and consistency.
The proposed framework offers a valuable instrument for generating diverse and
comprehensive datasets, which can be utilized to systematically assess and
enhance the performance of AI models.

</details>


### [8] [The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes](https://arxiv.org/abs/2509.18179)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CV

TL;DR: 论文通过实证分析量化了视觉-语言-视觉管道中描述-生成瓶颈造成的信息损失，发现99.3%的样本存在显著感知退化，91.5%存在显著结构信息损失。


<details>
  <summary>Details</summary>
Motivation: 随着多模态AI系统在创意工作流中的集成日益增多，理解视觉-语言-视觉管道中的信息损失对于评估系统局限性变得重要，但目前通过文本中介传递视觉内容时的退化程度尚未得到充分量化。

Method: 生成150对通过描述-生成管道的图像对，应用现有指标（LPIPS、SSIM和颜色距离）来测量感知、结构和色彩维度上的信息保存情况。

Result: 评估显示99.3%的样本表现出显著的感知退化，91.5%的样本表现出显著的结构信息损失。

Conclusion: 描述-生成瓶颈代表了当代多模态系统中可测量且一致的限制，提供了该瓶颈造成信息损失的实证证据。

Abstract: With the increasing integration of multimodal AI systems in creative
workflows, understanding information loss in vision-language-vision pipelines
has become important for evaluating system limitations. However, the
degradation that occurs when visual content passes through textual
intermediation remains poorly quantified. In this work, we provide empirical
analysis of the describe-then-generate bottleneck, where natural language
serves as an intermediate representation for visual information. We generated
150 image pairs through the describe-then-generate pipeline and applied
existing metrics (LPIPS, SSIM, and color distance) to measure information
preservation across perceptual, structural, and chromatic dimensions. Our
evaluation reveals that 99.3% of samples exhibit substantial perceptual
degradation and 91.5% demonstrate significant structural information loss,
providing empirical evidence that the describe-then-generate bottleneck
represents a measurable and consistent limitation in contemporary multimodal
systems.

</details>


### [9] [AI-Derived Structural Building Intelligence for Urban Resilience: An Application in Saint Vincent and the Grenadines](https://arxiv.org/abs/2509.18182)
*Isabelle Tingzon,Yoji Toriumi,Caroline Gevaert*

Main category: cs.CV

TL;DR: AI驱动的工作流从高分辨率卫星图像自动推断屋顶属性，用于小岛屿发展中国家（SIDS）的建筑信息收集，以支持灾害风险评估和城市韧性规划。


<details>
  <summary>Details</summary>
Motivation: 许多气候脆弱地区的小岛屿发展中国家缺乏详细的建筑结构信息，这些信息对于评估飓风、洪水和山体滑坡等灾害的潜在损害至关重要。

Method: 比较了地理空间基础模型结合浅层分类器与微调深度学习模型在屋顶分类中的效果，并评估了整合邻近SIDS额外训练数据对模型性能的影响。

Result: 最佳模型在屋顶坡度和屋顶材料分类上分别达到了0.88和0.83的F1分数。

Conclusion: 结合本地能力建设，该工作旨在为SIDS提供利用AI和地球观测数据的新能力，实现更高效、基于证据的城市治理。

Abstract: Detailed structural building information is used to estimate potential damage
from hazard events like cyclones, floods, and landslides, making them critical
for urban resilience planning and disaster risk reduction. However, such
information is often unavailable in many small island developing states (SIDS)
in climate-vulnerable regions like the Caribbean. To address this data gap, we
present an AI-driven workflow to automatically infer rooftop attributes from
high-resolution satellite imagery, with Saint Vincent and the Grenadines as our
case study. Here, we compare the utility of geospatial foundation models
combined with shallow classifiers against fine-tuned deep learning models for
rooftop classification. Furthermore, we assess the impact of incorporating
additional training data from neighboring SIDS to improve model performance.
Our best models achieve F1 scores of 0.88 and 0.83 for roof pitch and roof
material classification, respectively. Combined with local capacity building,
our work aims to provide SIDS with novel capabilities to harness AI and Earth
Observation (EO) data to enable more efficient, evidence-based urban
governance.

</details>


### [10] [VLA-LPAF: Lightweight Perspective-Adaptive Fusion for Vision-Language-Action to Enable More Unconstrained Robotic Manipulation](https://arxiv.org/abs/2509.18183)
*Jinyue Bian,Zhaoxing Zhang,Zhengyu Liang,Shiwei Zheng,Shengtao Zhang,Rong Shen,Chen Yang,Anzhou Hou*

Main category: cs.CV

TL;DR: 提出了VLA-LPAF轻量级模块来解决VLA模型在跨视角环境中的泛化问题，通过单视角图像微调并在潜空间融合多视角观察，显著提升了任务成功率。


<details>
  <summary>Details</summary>
Motivation: VLA模型在处理不同视角和数量的视觉观察时存在视角异质性限制，这影响了模型的泛化能力。

Method: 提出VLA-LPAF轻量级模块，使用单视角图像进行微调，在潜空间融合多视角观察，构建RoboFlamingo-LPAF框架。

Result: 在CALVIN上平均提升8%任务成功率，LIBERO上提升15%，定制仿真基准上提升30%，并在真实世界任务中验证了视角自适应特性。

Conclusion: VLA-LPAF有效解决了VLA模型的视角不一致问题，显著提升了跨视角环境的适应性和任务性能。

Abstract: The Visual-Language-Action (VLA) models can follow text instructions
according to visual observations of the surrounding environment. This ability
to map multimodal inputs to actions is derived from the training of the VLA
model on extensive standard demonstrations. These visual observations captured
by third-personal global and in-wrist local cameras are inevitably varied in
number and perspective across different environments, resulting in significant
differences in the visual features. This perspective heterogeneity constrains
the generality of VLA models. In light of this, we first propose the
lightweight module VLA-LPAF to foster the perspective adaptivity of VLA models
using only 2D data. VLA-LPAF is finetuned using images from a single view and
fuses other multiview observations in the latent space, which effectively and
efficiently bridge the gap caused by perspective inconsistency. We instantiate
our VLA-LPAF framework with the VLA model RoboFlamingo to construct
RoboFlamingo-LPAF. Experiments show that RoboFlamingo-LPAF averagely achieves
around 8% task success rate improvement on CALVIN, 15% on LIBERO, and 30% on a
customized simulation benchmark. We also demonstrate the developed viewadaptive
characteristics of the proposed RoboFlamingo-LPAF through real-world tasks.

</details>


### [11] [URNet: Uncertainty-aware Refinement Network for Event-based Stereo Depth Estimation](https://arxiv.org/abs/2509.18184)
*Yifeng Cheng,Alois Knoll,Hu Cao*

Main category: cs.CV

TL;DR: 提出了一种名为URNet的不确定性感知细化网络，用于基于事件的立体深度估计，通过局部-全局细化模块和KL散度不确定性建模方法，在DSEC数据集上优于现有最先进方法。


<details>
  <summary>Details</summary>
Motivation: 事件相机具有高时间分辨率、高动态范围和低延迟等优势，但现有的基于事件的立体深度估计方法在精度和可靠性方面仍有提升空间。

Method: URNet网络包含局部-全局细化模块，能够有效捕捉细粒度局部细节和长距离全局上下文，并引入基于KL散度的不确定性建模方法来增强预测可靠性。

Result: 在DSEC数据集上的大量实验表明，URNet在定性和定量评估中都一致优于最先进的方法。

Conclusion: URNet通过创新的局部-全局细化架构和不确定性建模，显著提升了基于事件相机的立体深度估计性能。

Abstract: Event cameras provide high temporal resolution, high dynamic range, and low
latency, offering significant advantages over conventional frame-based cameras.
In this work, we introduce an uncertainty-aware refinement network called URNet
for event-based stereo depth estimation. Our approach features a local-global
refinement module that effectively captures fine-grained local details and
long-range global context. Additionally, we introduce a Kullback-Leibler (KL)
divergence-based uncertainty modeling method to enhance prediction reliability.
Extensive experiments on the DSEC dataset demonstrate that URNet consistently
outperforms state-of-the-art (SOTA) methods in both qualitative and
quantitative evaluations.

</details>


### [12] [Visionerves: Automatic and Reproducible Hybrid AI for Peripheral Nervous System Recognition Applied to Endometriosis Cases](https://arxiv.org/abs/2509.18185)
*Giammarco La Barbera,Enzo Bonnot,Thomas Isla,Juan Pablo de la Plata,Joy-Rose Dunoyer de Segonzac,Jennifer Attali,Cécile Lozach,Alexandre Bellucci,Louis Marcellin,Laure Fournier,Sabine Sarnacki,Pietro Gori,Isabelle Bloch*

Main category: cs.CV

TL;DR: Visionerves是一种新型混合AI框架，用于从多梯度DWI和形态学MRI数据中识别周围神经系统。该方法通过模糊空间关系编码解剖知识，无需手动选择ROI，在10名子宫内膜异位症患者中相比标准纤维束成像显著提高了识别准确性。


<details>
  <summary>Details</summary>
Motivation: 子宫内膜异位症常导致慢性盆腔疼痛和可能的神经受累，但周围神经的成像仍然是一个挑战。需要开发能够自动识别周围神经系统的非侵入性诊断方法。

Method: 该框架包含两个阶段：(A)使用深度学习模型自动分割解剖结构，(B)通过符号空间推理进行纤维束成像和神经识别。该方法利用模糊空间关系编码解剖知识，避免了传统方法需要手动选择感兴趣区域的问题。

Result: 在10名子宫内膜异位症患者的腰骶丛神经应用中，Visionerves相比标准纤维束成像显示出显著改进，Dice分数提高高达25%，空间误差减少到小于5毫米。

Conclusion: 这种自动且可重复的方法能够进行详细的神经分析，为子宫内膜异位症相关神经病变以及其他涉及神经的疾病的非侵入性诊断开辟了新途径。

Abstract: Endometriosis often leads to chronic pelvic pain and possible nerve
involvement, yet imaging the peripheral nerves remains a challenge. We
introduce Visionerves, a novel hybrid AI framework for peripheral nervous
system recognition from multi-gradient DWI and morphological MRI data. Unlike
conventional tractography, Visionerves encodes anatomical knowledge through
fuzzy spatial relationships, removing the need for selection of manual ROIs.
The pipeline comprises two phases: (A) automatic segmentation of anatomical
structures using a deep learning model, and (B) tractography and nerve
recognition by symbolic spatial reasoning. Applied to the lumbosacral plexus in
10 women with (confirmed or suspected) endometriosis, Visionerves demonstrated
substantial improvements over standard tractography, with Dice score
improvements of up to 25% and spatial errors reduced to less than 5 mm. This
automatic and reproducible approach enables detailed nerve analysis and paves
the way for non-invasive diagnosis of endometriosis-related neuropathy, as well
as other conditions with nerve involvement.

</details>


### [13] [V-SenseDrive: A Privacy-Preserving Road Video and In-Vehicle Sensor Fusion Framework for Road Safety & Driver Behaviour Modelling](https://arxiv.org/abs/2509.18187)
*Muhammad Naveed,Nazia Perwaiz,Sidra Sultana,Mohaira Ahmad,Muhammad Moazam Fraz*

Main category: cs.CV

TL;DR: V-SenseDrive是首个在巴基斯坦驾驶环境中收集的隐私保护多模态驾驶员行为数据集，结合智能手机传感器数据和道路视频，用于检测不安全驾驶行为。


<details>
  <summary>Details</summary>
Motivation: 现有数据集主要来自发达国家，缺乏新兴经济体的行为多样性代表性，且驾驶员面部记录侵犯隐私保护。巴基斯坦等国的异质道路条件和混合交通流需要更可靠的驾驶行为检测方法。

Method: 使用定制Android应用收集高频率加速度计、陀螺仪和GPS数据，并与同步的道路视频结合，记录三种目标驾驶行为（正常、攻击性、危险）在不同类型道路上。

Result: 创建了结构化的多模态数据集，包含原始、处理和语义层，为驾驶员行为分类、交通安全分析和ADAS开发提供数据基础。

Conclusion: V-SenseDrive填补了全球驾驶员行为数据集的空白，为情境感知智能交通解决方案奠定了基础，特别适用于新兴经济体的驾驶环境。

Abstract: Road traffic accidents remain a major public health challenge, particularly
in countries with heterogeneous road conditions, mixed traffic flow, and
variable driving discipline, such as Pakistan. Reliable detection of unsafe
driving behaviours is a prerequisite for improving road safety, enabling
advanced driver assistance systems (ADAS), and supporting data driven decisions
in insurance and fleet management. Most of existing datasets originate from the
developed countries with limited representation of the behavioural diversity
observed in emerging economies and the driver's face recording voilates the
privacy preservation. We present V-SenseDrive, the first privacy-preserving
multimodal driver behaviour dataset collected entirely within the Pakistani
driving environment. V-SenseDrive combines smartphone based inertial and GPS
sensor data with synchronized road facing video to record three target driving
behaviours (normal, aggressive, and risky) on multiple types of roads,
including urban arterials, secondary roads, and motorways. Data was gathered
using a custom Android application designed to capture high frequency
accelerometer, gyroscope, and GPS streams alongside continuous video, with all
sources precisely time aligned to enable multimodal analysis. The focus of this
work is on the data acquisition process, covering participant selection,
driving scenarios, environmental considerations, and sensor video
synchronization techniques. The dataset is structured into raw, processed, and
semantic layers, ensuring adaptability for future research in driver behaviour
classification, traffic safety analysis, and ADAS development. By representing
real world driving in Pakistan, V-SenseDrive fills a critical gap in the global
landscape of driver behaviour datasets and lays the groundwork for context
aware intelligent transportation solutions.

</details>


### [14] [Qianfan-VL: Domain-Enhanced Universal Vision-Language Models](https://arxiv.org/abs/2509.18189)
*Daxiang Dong,Mingming Zheng,Dong Xu,Bairong Zhuang,Wenyu Zhang,Chunhua Luo,Haoran Wang,Zijian Zhao,Jie Li,Yuxuan Li,Hanjun Zhong,Mengyue Liu,Jieting Chen,Shupeng Li,Lun Tian,Yaping Feng,Xin Li,Donggang Jiang,Yong Chen,Yehua Xu,Duohao Qin,Chen Feng,Dan Wang,Henghua Zhang,Jingjing Ha,Jinhui He,Yanfeng Zhai,Chengxin Zheng,Jiayi Mao,Jiacheng Chen,Ruchang Yao,Ziye Yuan,Jianmin Wu,Guangjun Xie,Dou Shen*

Main category: cs.CV

TL;DR: 千帆-VL是一个多模态大语言模型系列，参数量从30亿到700亿，通过创新的领域增强技术实现了最先进的性能表现。


<details>
  <summary>Details</summary>
Motivation: 开发能够在保持强大通用性能的同时，通过领域增强技术提升特定领域能力的多模态大模型，为多样化企业部署场景提供有效解决方案。

Method: 采用多阶段渐进式训练和高精度数据合成流水线，在百度昆仑P800芯片上进行大规模训练，实现了超过90%的5000芯片单任务扩展效率。

Result: 在通用基准测试中与领先开源模型相当，在CCBench、SEEDBench IMG、ScienceQA和MMStar等基准测试中达到最先进水平。在OCR和文档理解方面表现突出（OCRBench 873分，DocVQA 94.75%），数学推理（MathVista 78.6%）和逻辑推理任务也展现卓越性能。

Conclusion: 该工作建立了一个有效的领域增强多模态模型开发方法学，验证了大规模AI基础设施训练SOTA级多模态模型的能力，为多样化企业部署场景提供了可靠的技术方案。

Abstract: We present Qianfan-VL, a series of multimodal large language models ranging
from 3B to 70B parameters, achieving state-of-the-art performance through
innovative domain enhancement techniques. Our approach employs multi-stage
progressive training and high-precision data synthesis pipelines, which prove
to be critical technologies for enhancing domain-specific capabilities while
maintaining strong general performance. Qianfan-VL achieves comparable results
to leading open-source models on general benchmarks, with state-of-the-art
performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and
MMStar. The domain enhancement strategy delivers significant advantages in OCR
and document understanding, validated on both public benchmarks (OCRBench 873,
DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B
variants incorporate long chain-of-thought capabilities, demonstrating superior
performance on mathematical reasoning (MathVista 78.6%) and logical inference
tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating
the capability of large-scale AI infrastructure to train SOTA-level multimodal
models with over 90% scaling efficiency on 5000 chips for a single task. This
work establishes an effective methodology for developing domain-enhanced
multimodal models suitable for diverse enterprise deployment scenarios.

</details>


### [15] [HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing](https://arxiv.org/abs/2509.18190)
*Junseong Shin,Seungwoo Chung,Yunjeong Yang,Tae Hyun Kim*

Main category: cs.CV

TL;DR: HazeFlow是一个基于ODE的新框架，将大气散射模型重新表述为常微分方程，通过单步推理实现高效去雾，并引入马尔可夫链布朗运动生成非均匀雾霾数据来增强模型在真实场景中的适应性。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习方法在真实世界去雾任务中因缺乏配对训练数据和领域差距导致的泛化能力不足问题，以及传统基于大气散射模型的物理方法难以处理真实世界复杂雾霾模式的局限性。

Method: 提出HazeFlow框架：1）将大气散射模型重新表述为ODE，学习从雾霾图像到清晰图像的最优ODE轨迹；2）引入基于马尔可夫链布朗运动的非均匀雾霾生成方法，模拟真实雾霾模式；3）仅需单步推理即可实现高效去雾。

Result: 在多个真实世界去雾基准数据集上实现了最先进的性能表现。

Conclusion: HazeFlow通过ODE框架和物理启发的学习方法，有效解决了真实世界去雾任务中的数据稀缺和泛化问题，展示了在复杂真实场景中的优越性能。

Abstract: Dehazing involves removing haze or fog from images to restore clarity and
improve visibility by estimating atmospheric scattering effects. While deep
learning methods show promise, the lack of paired real-world training data and
the resulting domain gap hinder generalization to real-world scenarios. In this
context, physics-grounded learning becomes crucial; however, traditional
methods based on the Atmospheric Scattering Model (ASM) often fall short in
handling real-world complexities and diverse haze patterns. To solve this
problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM
as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF),
HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones,
enhancing real-world dehazing performance with only a single inference step.
Additionally, we introduce a non-homogeneous haze generation method using
Markov Chain Brownian Motion (MCBM) to address the scarcity of paired
real-world data. By simulating realistic haze patterns through MCBM, we enhance
the adaptability of HazeFlow to diverse real-world scenarios. Through extensive
experiments, we demonstrate that HazeFlow achieves state-of-the-art performance
across various real-world dehazing benchmark datasets.

</details>


### [16] [TinyEcoWeedNet: Edge Efficient Real-Time Aerial Agricultural Weed Detection](https://arxiv.org/abs/2509.18193)
*Omar H. Khater,Abdul Jabbar Siddiqui,Aiman El-Maleh,M. Shamim Hossain*

Main category: cs.CV

TL;DR: 本文提出了一种压缩版的EcoWeedNet模型，通过结构化通道剪枝、量化感知训练和TensorRT加速，在Jetson Orin Nano边缘设备上实现了68.5%的模型压缩和184 FPS的推理速度，在棉花杂草检测任务中表现优于YOLO11n和YOLO12n。


<details>
  <summary>Details</summary>
Motivation: 农业领域部署深度学习模型面临边缘设备资源有限的挑战，需要开发高效的轻量化模型来满足实时检测需求。

Method: 采用结构化通道剪枝技术处理复杂架构（包含残差连接、注意力机制、连接操作和CSP模块），结合量化感知训练，并使用NVIDIA TensorRT在Jetson Orin Nano上进行加速优化。

Result: 模型大小减少68.5%，计算量减少3.2 GFLOPs，推理速度达到184 FPS（FP16精度），比基线快28.7%。在CottonWeedDet12数据集上，39.5%剪枝率的EcoWeedNet达到83.7%精确率、77.5%召回率和85.9% mAP50，优于YOLO11n和YOLO12n。

Conclusion: 该方法证明了在保持高精度的同时显著提升模型效率的可行性，为精准农业提供了既高效又有效的解决方案。

Abstract: Deploying deep learning models in agriculture is difficult because edge
devices have limited resources, but this work presents a compressed version of
EcoWeedNet using structured channel pruning, quantization-aware training (QAT),
and acceleration with NVIDIA's TensorRT on the Jetson Orin Nano. Despite the
challenges of pruning complex architectures with residual shortcuts, attention
mechanisms, concatenations, and CSP blocks, the model size was reduced by up to
68.5% and computations by 3.2 GFLOPs, while inference speed reached 184 FPS at
FP16, 28.7% faster than the baseline. On the CottonWeedDet12 dataset, the
pruned EcoWeedNet with a 39.5% pruning ratio outperformed YOLO11n and YOLO12n
(with only 20% pruning), achieving 83.7% precision, 77.5% recall, and 85.9%
mAP50, proving it to be both efficient and effective for precision agriculture.

</details>


### [17] [Learning Contrastive Multimodal Fusion with Improved Modality Dropout for Disease Detection and Prediction](https://arxiv.org/abs/2509.18284)
*Yi Gu,Kuniaki Saito,Jiaxin Ma*

Main category: cs.CV

TL;DR: 提出了一种新的多模态学习框架，通过增强模态丢弃和对比学习来解决模态不平衡和缺失问题，在医疗诊断任务中实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 医疗诊断越来越多地使用多模态数据，但实际应用中存在模态不平衡和缺失的问题，需要开发能够有效融合异构信息并保持对缺失模态鲁棒性的模型

Method: 引入可学习的模态令牌来改进缺失感知的模态融合，将传统的单模态对比目标与融合的多模态表示相结合，结合增强模态丢弃技术

Result: 在大规模临床数据集上的实验表明，该方法在疾病检测和预测任务中达到最先进性能，特别是在只有单一模态可用的实际场景中表现优异

Conclusion: 该方法具有有效性、高效性和通用性，为多模态学习提供了可扩展、低成本的解决方案，在真实临床应用中具有显著潜力

Abstract: As medical diagnoses increasingly leverage multimodal data, machine learning
models are expected to effectively fuse heterogeneous information while
remaining robust to missing modalities. In this work, we propose a novel
multimodal learning framework that integrates enhanced modalities dropout and
contrastive learning to address real-world limitations such as modality
imbalance and missingness. Our approach introduces learnable modality tokens
for improving missingness-aware fusion of modalities and augments conventional
unimodal contrastive objectives with fused multimodal representations. We
validate our framework on large-scale clinical datasets for disease detection
and prediction tasks, encompassing both visual and tabular modalities.
Experimental results demonstrate that our method achieves state-of-the-art
performance, particularly in challenging and practical scenarios where only a
single modality is available. Furthermore, we show its adaptability through
successful integration with a recent CT foundation model. Our findings
highlight the effectiveness, efficiency, and generalizability of our approach
for multimodal learning, offering a scalable, low-cost solution with
significant potential for real-world clinical applications. The code is
available at https://github.com/omron-sinicx/medical-modality-dropout.

</details>


### [18] [Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model](https://arxiv.org/abs/2509.18308)
*Yixin Zhang,Ryan Chamberlain,Lawrance Ngo,Kevin Kramer,Maciej A. Mazurowski*

Main category: cs.CV

TL;DR: 该研究通过系统评估9种分割架构在490个CTPA扫描上的表现，发现3D U-Net+ResNet在肺栓塞分割中表现最佳，CNN模型优于ViT模型，且分类预训练可能对分割性能产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 由于肺栓塞(PE)分割任务缺乏系统性的架构比较研究，作者旨在通过统一测试框架评估不同分割架构的性能，为临床实践提供指导。

Method: 使用490个CTPA扫描构建内部数据集，系统评估9种CNN和ViT分割架构（包括预训练和随机初始化权重），采用统一测试框架进行性能审计。

Result: 最佳模型达到0.7131的平均Dice分数，在60个测试扫描中检测到181个栓子（49个假阳性，28个假阴性），并在公共数据集上验证了泛化能力。

Conclusion: 3D CNN模型在PE分割中表现优异，远端栓子分割仍具挑战性，分类和分割任务可能依赖不同的判别特征。

Abstract: In this study, we curated a densely annotated in-house dataset comprising 490
CTPA scans. Using this dataset, we systematically evaluated nine widely used
segmentation architectures from both the CNN and Vision Transformer (ViT)
families, initialized with either pretrained or random weights, under a unified
testing framework as a performance audit. Our study leads to several important
observations: (1) 3D U-Net with a ResNet encoder remains a highly effective
architecture for PE segmentation; (2) 3D models are particularly well-suited to
this task given the morphological characteristics of emboli; (3) CNN-based
models generally yield superior performance compared to their ViT-based
counterparts in PE segmentation; (4) classification-based pretraining, even on
large PE datasets, can adversely impact segmentation performance compared to
training from scratch, suggesting that PE classification and segmentation may
rely on different sets of discriminative features; (5) different model
architectures show a highly consistent pattern of segmentation performance when
trained on the same data; and (6) while central and large emboli can be
segmented with satisfactory accuracy, distal emboli remain challenging due to
both task complexity and the scarcity of high-quality datasets. Besides these
findings, our best-performing model achieves a mean Dice score of 0.7131 for
segmentation. It detects 181 emboli with 49 false positives and 28 false
negatives from 60 in-house testing scans. Its generalizability is further
validated on public datasets.

</details>


### [19] [Improving Handshape Representations for Sign Language Processing: A Graph Neural Network Approach](https://arxiv.org/abs/2509.18309)
*Alessa Carbo,Eric Nalisnick*

Main category: cs.CV

TL;DR: 提出了一种新颖的图神经网络，将时间动态与静态手形配置分离，用于手语手形识别，在37个手形类别上达到46%的准确率。


<details>
  <summary>Details</summary>
Motivation: 手形在手语中具有基础音系学作用，但计算方法很少明确建模手形，限制了识别准确性和语言分析。

Method: 结合解剖学启发的图结构和对比学习，分离时间动态与静态手形配置，解决手形识别中的类间细微差异和时间变化挑战。

Result: 建立了手语序列中结构化手形识别的首个基准，在37个手形类别上达到46%的准确率（基线方法为25%）。

Conclusion: 该方法显著提升了手形识别性能，为手语计算分析提供了更准确的手形建模工具。

Abstract: Handshapes serve a fundamental phonological role in signed languages, with
American Sign Language employing approximately 50 distinct shapes.
However,computational approaches rarely model handshapes explicitly, limiting
both recognition accuracy and linguistic analysis.We introduce a novel graph
neural network that separates temporal dynamics from static handshape
configurations. Our approach combines anatomically-informed graph structures
with contrastive learning to address key challenges in handshape recognition,
including subtle interclass distinctions and temporal variations. We establish
the first benchmark for structured handshape recognition in signing sequences,
achieving 46% accuracy across 37 handshape classes (with baseline methods
achieving 25%).

</details>


### [20] [Influence of Classification Task and Distribution Shift Type on OOD Detection in Fetal Ultrasound](https://arxiv.org/abs/2509.18326)
*Chun Kit Wong,Anders N. Christensen,Cosmin I. Bercea,Julia A. Schnabel,Martin G. Tolsgaard,Aasa Feragen*

Main category: cs.CV

TL;DR: 本文研究了分类任务对胎儿超声图像中离群分布检测性能的影响，发现任务选择对OOD检测效果有显著影响，且最佳任务取决于ID-OOD标准类型。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注不确定性量化方法，但忽视了分类任务本身对OOD检测的影响。在胎儿超声图像存在异质性和多样化临床设置的背景下，可靠的OOD检测对深度学习模型的安全部署至关重要。

Method: 在四个分类任务上测试了八种不确定性量化方法，分析任务选择对OOD检测性能的影响，特别关注图像特征偏移和解剖特征偏移两种ID-OOD标准。

Result: OOD检测性能随任务选择显著变化，最佳任务取决于ID-OOD标准类型。同时发现优异的OOD检测性能并不保证最优的弃权预测效果。

Conclusion: 在医学图像分析中，需要根据具体下游应用来协调任务选择和不确定性策略，强调任务选择与不确定性策略的协调重要性。

Abstract: Reliable out-of-distribution (OOD) detection is important for safe deployment
of deep learning models in fetal ultrasound amidst heterogeneous image
characteristics and clinical settings. OOD detection relies on estimating a
classification model's uncertainty, which should increase for OOD samples.
While existing research has largely focused on uncertainty quantification
methods, this work investigates the impact of the classification task itself.
Through experiments with eight uncertainty quantification methods across four
classification tasks, we demonstrate that OOD detection performance
significantly varies with the task, and that the best task depends on the
defined ID-OOD criteria; specifically, whether the OOD sample is due to: i) an
image characteristic shift or ii) an anatomical feature shift. Furthermore, we
reveal that superior OOD detection does not guarantee optimal abstained
prediction, underscoring the necessity to align task selection and uncertainty
strategies with the specific downstream application in medical image analysis.

</details>


### [21] [OrthoLoC: UAV 6-DoF Localization and Calibration Using Orthographic Geodata](https://arxiv.org/abs/2509.18350)
*Oussema Dhaouadi,Riccardo Marin,Johannes Meier,Jacques Kaiser,Daniel Cremers*

Main category: cs.CV

TL;DR: 提出了OrthoLoC数据集，这是首个大规模无人机图像与正射影像配对的视觉定位数据集，包含16,425张图像，并提出了AdHoP改进技术提升特征匹配性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉定位系统在资源受限环境下（如无网络或GPS）难以使用大型图像数据库或重型3D模型，而正射地理数据作为轻量级替代方案未被充分利用。

Method: 创建包含德国和美国无人机图像的多模态数据集，采用配对结构分离图像检索和特征匹配评估，并提出AdHoP改进技术集成到任意特征匹配器中。

Result: AdHoP技术将匹配性能提升高达95%，平移误差降低高达63%，数据集支持公平基准测试和域偏移分析。

Conclusion: OrthoLoC数据集填补了正射地理数据在视觉定位中的空白，AdHoP技术显著提升了定位精度，为资源受限环境下的高精度定位提供了可行方案。

Abstract: Accurate visual localization from aerial views is a fundamental problem with
applications in mapping, large-area inspection, and search-and-rescue
operations. In many scenarios, these systems require high-precision
localization while operating with limited resources (e.g., no internet
connection or GNSS/GPS support), making large image databases or heavy 3D
models impractical. Surprisingly, little attention has been given to leveraging
orthographic geodata as an alternative paradigm, which is lightweight and
increasingly available through free releases by governmental authorities (e.g.,
the European Union). To fill this gap, we propose OrthoLoC, the first
large-scale dataset comprising 16,425 UAV images from Germany and the United
States with multiple modalities. The dataset addresses domain shifts between
UAV imagery and geospatial data. Its paired structure enables fair benchmarking
of existing solutions by decoupling image retrieval from feature matching,
allowing isolated evaluation of localization and calibration performance.
Through comprehensive evaluation, we examine the impact of domain shifts, data
resolutions, and covisibility on localization accuracy. Finally, we introduce a
refinement technique called AdHoP, which can be integrated with any feature
matcher, improving matching by up to 95% and reducing translation error by up
to 63%. The dataset and code are available at:
https://deepscenario.github.io/OrthoLoC.

</details>


### [22] [A Single Image Is All You Need: Zero-Shot Anomaly Localization Without Training Data](https://arxiv.org/abs/2509.18354)
*Mehrdad Moradi,Shengzhe Chen,Hao Yan,Kamran Paynabar*

Main category: cs.CV

TL;DR: 提出了一种无需训练数据的单图像异常检测方法SSDnet，利用卷积神经网络的归纳偏置进行自重建，通过掩码、打乱和噪声避免恒等映射，在MVTec-AD和fabric数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决现实场景中缺乏训练数据和参考样本的零样本异常检测问题，仅使用测试图像本身进行异常定位。

Method: 基于Deep Image Prior思想，设计patch-based自重建框架，输入图像直接进入网络重建，使用掩码、patch打乱和小高斯噪声防止恒等映射，采用基于内积相似度的感知损失。

Result: 在MVTec-AD上达到0.99 AUROC和0.60 AUPRC，在fabric数据集上达到0.98 AUROC和0.67 AUPRC，优于现有最佳方法。

Conclusion: SSDnet无需外部训练数据、标签或参考样本，对噪声和缺失像素具有鲁棒性，在零样本异常检测任务中表现优异。

Abstract: Anomaly detection in images is typically addressed by learning from
collections of training data or relying on reference samples. In many
real-world scenarios, however, such training data may be unavailable, and only
the test image itself is provided. We address this zero-shot setting by
proposing a single-image anomaly localization method that leverages the
inductive bias of convolutional neural networks, inspired by Deep Image Prior
(DIP). Our method is named Single Shot Decomposition Network (SSDnet). Our key
assumption is that natural images often exhibit unified textures and patterns,
and that anomalies manifest as localized deviations from these repetitive or
stochastic patterns. To learn the deep image prior, we design a patch-based
training framework where the input image is fed directly into the network for
self-reconstruction, rather than mapping random noise to the image as done in
DIP. To avoid the model simply learning an identity mapping, we apply masking,
patch shuffling, and small Gaussian noise. In addition, we use a perceptual
loss based on inner-product similarity to capture structure beyond pixel
fidelity. Our approach needs no external training data, labels, or references,
and remains robust in the presence of noise or missing pixels. SSDnet achieves
0.99 AUROC and 0.60 AUPRC on MVTec-AD and 0.98 AUROC and 0.67 AUPRC on the
fabric dataset, outperforming state-of-the-art methods. The implementation code
will be released at https://github.com/mehrdadmoradi124/SSDnet

</details>


### [23] [Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning](https://arxiv.org/abs/2509.18369)
*Riad Ahmed Anonto,Sardar Md. Saffat Zabin,M. Saifur Rahman*

Main category: cs.CV

TL;DR: 提出了一种针对低资源语言（孟加拉语）的视觉-语言模型，通过三重损失目标（PAL+InfoNCE+OT）解决跨语言对齐问题，在Flickr30k和MSCOCO数据集上取得了显著提升


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言视觉-语言模型中的对齐挑战，包括配对数据稀缺、翻译对齐断裂和英语中心预训练忽略目标语言语义的问题

Method: 使用LaBSE验证的英-孟配对数据和11万双语提示合成图像训练管道，包含冻结的MaxViT视觉编码器、孟加拉语原生mBART-50解码器和轻量级桥接模块，核心创新是三重损失目标：PAL对齐真实和合成图像块描述符，InfoNCE强制全局分离，Sinkhorn OT确保细粒度对应

Result: 在Flickr30k-1k上BLEU-4达到12.29，METEOR 27.98，BERTScore-F1 71.20；在MSCOCO-1k上BLEU-4 12.00，METEOR 28.14，BERTScore-F1 75.40，优于强基线，真实-合成质心差距缩小41%

Conclusion: 提出的三重损失目标有效提升了低资源语言的视觉-语言模型性能，显著改善了跨语言对齐和图像描述质量

Abstract: Grounding vision--language models in low-resource languages remains
challenging, as they often produce fluent text about the wrong objects. This
stems from scarce paired data, translation pivots that break alignment, and
English-centric pretraining that ignores target-language semantics. We address
this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified
EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT
yields stable visual patches, a Bengali-native mBART-50 decodes, and a
lightweight bridge links the modalities. Our core novelty is a tri-loss
objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch
descriptors using decoder cross-attention, InfoNCE enforces global
real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained
patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces
spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR
27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14,
BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the
real--synthetic centroid gap by 41%.

</details>


### [24] [TinyBEV: Cross Modal Knowledge Distillation for Efficient Multi Task Bird's Eye View Perception and Planning](https://arxiv.org/abs/2509.18372)
*Reeshad Khan,John Gauch*

Main category: cs.CV

TL;DR: TinyBEV是一个统一的纯摄像头BEV框架，通过知识蒸馏将大型规划导向教师模型(UniAD)的能力压缩到紧凑的实时学生模型中，在保持完整自动驾驶堆栈功能的同时大幅减少参数和提升速度。


<details>
  <summary>Details</summary>
Motivation: 解决现有高效纯摄像头基线模型功能有限的问题，将大规模多模态感知规划模型的完整能力移植到资源受限的实时部署环境中。

Method: 采用模型无关的多阶段蒸馏策略，结合特征级、输出级和自适应区域感知监督，将高容量多模态知识转移到轻量级BEV表示中。

Result: 在nuScenes数据集上，TinyBEV实现39.0 mAP检测精度、1.08 minADE运动预测精度和0.32碰撞率，运行速度提升5倍(11 FPS)，参数减少78%。

Conclusion: 证明了在资源受限环境下仍能保留完整的驾驶智能，弥合了大规模多模态感知规划模型与部署就绪实时自动驾驶系统之间的差距。

Abstract: We present TinyBEV, a unified, camera only Bird's Eye View (BEV) framework
that distills the full-stack capabilities of a large planning-oriented teacher
(UniAD [19]) into a compact, real-time student model. Unlike prior efficient
camera only baselines such as VAD[23] and VADv2[7], TinyBEV supports the
complete autonomy stack 3D detection, HD-map segmentation, motion forecasting,
occupancy prediction, and goal-directed planning within a streamlined
28M-parameter backbone, achieving a 78% reduction in parameters over UniAD
[19]. Our model-agnostic, multi-stage distillation strategy combines
feature-level, output-level, and adaptive region-aware supervision to
effectively transfer high-capacity multi-modal knowledge to a lightweight BEV
representation. On nuScenes[4], Tiny-BEV achieves 39.0 mAP for detection, 1.08
minADE for motion forecasting, and a 0.32 collision rate, while running 5x
faster (11 FPS) and requiring only camera input. These results demonstrate that
full-stack driving intelligence can be retained in resource-constrained
settings, bridging the gap between large-scale, multi-modal perception-planning
models and deployment-ready real-time autonomy.

</details>


### [25] [BlurBall: Joint Ball and Motion Blur Estimation for Table Tennis Ball Tracking](https://arxiv.org/abs/2509.18387)
*Thomas Gossard,Filip Radovic,Andreas Ziegler,Andrea Zell*

Main category: cs.CV

TL;DR: 本文提出了一种新的运动模糊球体标注方法，将球标注在模糊条纹的中心而非前沿，并显式标注模糊属性，显著提升了球体检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有标注方法将球标记在模糊条纹的前沿，存在不对称性且忽略了与速度相关的运动线索，这限制了检测系统的性能。

Method: 引入新的标注策略，将球置于模糊条纹中心并标注模糊属性；开发BlurBall模型，通过多帧输入的注意力机制联合估计球位置和运动模糊属性。

Result: 新标注方法在各种模型上一致提升检测性能，BlurBall模型在球体检测方面达到最先进结果，并改善了轨迹预测可靠性。

Conclusion: 利用运动模糊信息不仅能提高检测精度，还能实现更可靠的轨迹预测，有益于实时体育分析应用。

Abstract: Motion blur reduces the clarity of fast-moving objects, posing challenges for
detection systems, especially in racket sports, where balls often appear as
streaks rather than distinct points. Existing labeling conventions mark the
ball at the leading edge of the blur, introducing asymmetry and ignoring
valuable motion cues correlated with velocity. This paper introduces a new
labeling strategy that places the ball at the center of the blur streak and
explicitly annotates blur attributes. Using this convention, we release a new
table tennis ball detection dataset. We demonstrate that this labeling approach
consistently enhances detection performance across various models. Furthermore,
we introduce BlurBall, a model that jointly estimates ball position and motion
blur attributes. By incorporating attention mechanisms such as
Squeeze-and-Excitation over multi-frame inputs, we achieve state-of-the-art
results in ball detection. Leveraging blur not only improves detection accuracy
but also enables more reliable trajectory prediction, benefiting real-time
sports analytics.

</details>


### [26] [MVP: Motion Vector Propagation for Zero-Shot Video Object Detection](https://arxiv.org/abs/2509.18388)
*Binhua Huang,Ni Wang,Wendong Yao,Soumyabrata Dev*

Main category: cs.CV

TL;DR: 提出了一种基于压缩域运动向量的无训练视频目标检测方法MVP，通过在关键帧上运行OWLv2检测器，利用运动向量将检测结果传播到中间帧，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决大规模开放词汇检测器在视频帧上逐帧运行计算成本高的问题，旨在保持检测精度的同时大幅降低计算负担。

Method: 使用固定间隔的关键帧策略，在关键帧上运行OWLv2检测器，通过压缩域运动向量（3x3网格聚合）进行检测结果传播，包含面积增长检查和可选的单类别切换。

Result: 在ILSVRC2015-VID数据集上达到mAP@0.5=0.609，在宽松IoU阈值下接近逐帧检测性能，优于基于跟踪器的传播方法，且无需标注训练数据。

Conclusion: 压缩域传播是一种实用的方法，可以在保持强大零样本覆盖能力的同时显著减少检测器调用次数。

Abstract: Running a large open-vocabulary (Open-vocab) detector on every video frame is
accurate but expensive. We introduce a training-free pipeline that invokes
OWLv2 only on fixed-interval keyframes and propagates detections to
intermediate frames using compressed-domain motion vectors (MV). A simple 3x3
grid aggregation of motion vectors provides translation and uniform-scale
updates, augmented with an area-growth check and an optional single-class
switch. The method requires no labels, no fine-tuning, and uses the same prompt
list for all open-vocabulary methods. On ILSVRC2015-VID (validation dataset),
our approach (MVP) attains mAP@0.5=0.609 and mAP@[0.5:0.95]=0.316. At loose
intersection-over-union (IoU) thresholds it remains close to framewise
OWLv2-Large (0.747/0.721 at 0.2/0.3 versus 0.784/0.780), reflecting that coarse
localization is largely preserved. Under the same keyframe schedule, MVP
outperforms tracker-based propagation (MOSSE, KCF, CSRT) at mAP@0.5. A
supervised reference (YOLOv12x) reaches 0.631 at mAP@0.5 but requires labeled
training, whereas our method remains label-free and open-vocabulary. These
results indicate that compressed-domain propagation is a practical way to
reduce detector invocations while keeping strong zero-shot coverage in videos.
Our code and models are available at https://github.com/microa/MVP.

</details>


### [27] [Improving the color accuracy of lighting estimation models](https://arxiv.org/abs/2509.18390)
*Zitian Zhang,Joshua Urban Davis,Jeanne Phuong Anh Vu,Jiangtao Kuang,Jean-François Lalonde*

Main category: cs.CV

TL;DR: 该论文研究了高动态范围（HDR）光照估计方法的颜色鲁棒性，发现通过预训练的白平衡网络预处理输入图像可以显著提高现有模型的颜色准确性，且无需重新训练光照估计模型。


<details>
  <summary>Details</summary>
Motivation: 现有的单图像HDR光照估计方法在颜色准确性方面存在不足，这会影响增强现实应用中虚拟物体渲染的真实感。论文旨在解决这一被忽视但关键的问题。

Method: 通过使用包含不同光照颜色的新型HDR数据集，系统评估了多种适应策略，重点测试了预训练白平衡网络预处理输入图像的方法。

Result: 实验结果表明，白平衡预处理方法在所有测试场景中都优于其他策略，能够有效提高光照估计的颜色鲁棒性。

Conclusion: 简单的白平衡预处理技术可以显著提升现有光照估计模型的颜色准确性，这一发现具有通用性，已在三种最先进的方法上得到验证。

Abstract: Advances in high dynamic range (HDR) lighting estimation from a single image
have opened new possibilities for augmented reality (AR) applications.
Predicting complex lighting environments from a single input image allows for
the realistic rendering and compositing of virtual objects. In this work, we
investigate the color robustness of such methods -- an often overlooked yet
critical factor for achieving visual realism. While most evaluations conflate
color with other lighting attributes (e.g., intensity, direction), we isolate
color as the primary variable of interest. Rather than introducing a new
lighting estimation algorithm, we explore whether simple adaptation techniques
can enhance the color accuracy of existing models. Using a novel HDR dataset
featuring diverse lighting colors, we systematically evaluate several
adaptation strategies. Our results show that preprocessing the input image with
a pre-trained white balance network improves color robustness, outperforming
other strategies across all tested scenarios. Notably, this approach requires
no retraining of the lighting estimation model. We further validate the
generality of this finding by applying the technique to three state-of-the-art
lighting estimation methods from recent literature.

</details>


### [28] [Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models](https://arxiv.org/abs/2509.18405)
*Sourav Halder,Jinjun Tong,Xinyu Wu*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型和多模态大语言模型的无需训练支票字段检测框架，实现零样本检测，显著降低金融场景部署门槛。


<details>
  <summary>Details</summary>
Motivation: 支票在金融系统中仍被广泛使用但易受欺诈，传统检测方法依赖大量标注数据，而这类数据因隐私和专有性问题难以获取。

Method: 利用视觉语言模型(VLM)和多模态大语言模型(MLLM)构建无需训练的检测框架，实现支票关键字段的零样本识别和定位。

Result: 在包含110张不同格式和布局支票的手工标注数据集上验证，显示出强大的性能和泛化能力。

Conclusion: 该框架不仅实现了高效的支票字段检测，还可作为生成高质量标注数据的引导机制，支持开发专门的实时目标检测模型。

Abstract: Checks remain a foundational instrument in the financial ecosystem,
facilitating substantial transaction volumes across institutions. However,
their continued use also renders them a persistent target for fraud,
underscoring the importance of robust check fraud detection mechanisms. At the
core of such systems lies the accurate identification and localization of
critical fields, such as the signature, magnetic ink character recognition
(MICR) line, courtesy amount, legal amount, payee, and payer, which are
essential for subsequent verification against reference checks belonging to the
same customer. This field-level detection is traditionally dependent on object
detection models trained on large, diverse, and meticulously labeled datasets,
a resource that is scarce due to proprietary and privacy concerns. In this
paper, we introduce a novel, training-free framework for automated check field
detection, leveraging the power of a vision language model (VLM) in conjunction
with a multimodal large language model (MLLM). Our approach enables zero-shot
detection of check components, significantly lowering the barrier to deployment
in real-world financial settings. Quantitative evaluation of our model on a
hand-curated dataset of 110 checks spanning multiple formats and layouts
demonstrates strong performance and generalization capability. Furthermore,
this framework can serve as a bootstrap mechanism for generating high-quality
labeled datasets, enabling the development of specialized real-time object
detection models tailored to institutional needs.

</details>


### [29] [Losing the Plot: How VLM responses degrade on imperfect charts](https://arxiv.org/abs/2509.18425)
*Philip Wootaek Shin,Jack Sampson,Vijaykrishnan Narayanan,Andres Marquez,Mahantesh Halappanavar*

Main category: cs.CV

TL;DR: 论文评估了视觉语言模型在图表理解中的表现，发现现有基准假设图表干净且查询基于事实，而真实世界图表常包含失真和需要推理。研究引入CHART NOISe数据集，结合图表损坏、遮挡和考试风格问题，揭示了模型在退化设置下的性能下降和幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有图表理解基准假设图表干净且查询基于事实，但真实世界图表常包含失真和需要推理。为评估模型在真实场景下的鲁棒性，研究旨在暴露视觉语言模型在图表推理中的系统性漏洞。

Method: 研究评估了ChatGPT 4o、Claude Sonnet 4和Gemini 2.5 Pro等先进视觉语言模型，引入CHART NOISe数据集，该数据集结合图表损坏、遮挡和考试风格多项选择题。关键创新是提示反向不一致性测试，即模型在确认和否认同一陈述时自相矛盾。

Result: 研究发现模型在图表损坏或遮挡下性能急剧下降，幻觉问题（如数值捏造、趋势误解和实体混淆）更频繁。模型在退化设置下仍过度自信，生成看似合理但无支持的解释。

Conclusion: 研究建立了图表理解的严格测试平台，揭示了当前视觉语言模型在鲁棒性和可靠性方面的不足。提出了质量过滤和遮挡检测等基线缓解策略，为提升图表理解的稳健性奠定了基础。

Abstract: Vision language models (VLMs) show strong results on chart understanding, yet
existing benchmarks assume clean figures and fact based queries. Real world
charts often contain distortions and demand reasoning beyond simple matching.
We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp
performance drops under corruption or occlusion, with hallucinations such as
value fabrication, trend misinterpretation, and entity confusion becoming more
frequent. Models remain overconfident in degraded settings, generating
plausible but unsupported explanations.
  To address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers,
and Reasoning Testing on Noisy and Occluded Input Selections), a dataset
combining chart corruptions, occlusions, and exam style multiple choice
questions inspired by Korea's CSAT English section. A key innovation is prompt
reverse inconsistency, where models contradict themselves when asked to confirm
versus deny the same statement. Our contributions are threefold: (1)
benchmarking state of the art VLMs, exposing systematic vulnerabilities in
chart reasoning; (2) releasing CHART NOISe, the first dataset unifying
corruption, occlusion, and reverse inconsistency; and (3) proposing baseline
mitigation strategies such as quality filtering and occlusion detection.
Together, these efforts establish a rigorous testbed for advancing robustness
and reliability in chart understanding.

</details>


### [30] [CPT-4DMR: Continuous sPatial-Temporal Representation for 4D-MRI Reconstruction](https://arxiv.org/abs/2509.18427)
*Xinyang Wu,Muheng Li,Xia Li,Orso Pusterla,Sairos Safai,Philippe C. Cattin,Antony J. Lomax,Ye Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于神经表示的4D-MRI重建框架，通过两个协同网络（SAN和TMN）替代传统的离散排序方法，能够高效准确地重建任意呼吸状态下的3D图像。


<details>
  <summary>Details</summary>
Motivation: 传统4D-MRI重建方法存在难以捕捉时间变异性、工作流程复杂和计算负担重的问题，需要一种更高效准确的重建技术。

Method: 使用两个协同网络：空间解剖网络（SAN）编码连续3D解剖表示，时间运动网络（TMN）基于Transformer提取的呼吸信号生成时间一致的变形场。

Result: 在19名志愿者的自由呼吸数据集上验证，该方法能准确捕捉规则和不规则呼吸模式，处理时间从5小时缩短到15分钟训练时间，每帧3D体积推断时间小于1秒。

Conclusion: 该框架在4D放射治疗规划和实时自适应治疗中具有强大应用潜力，相比传统方法具有显著优势。

Abstract: Four-dimensional MRI (4D-MRI) is an promising technique for capturing
respiratory-induced motion in radiation therapy planning and delivery.
Conventional 4D reconstruction methods, which typically rely on phase binning
or separate template scans, struggle to capture temporal variability,
complicate workflows, and impose heavy computational loads. We introduce a
neural representation framework that considers respiratory motion as a smooth,
continuous deformation steered by a 1D surrogate signal, completely replacing
the conventional discrete sorting approach. The new method fuses motion
modeling with image reconstruction through two synergistic networks: the
Spatial Anatomy Network (SAN) encodes a continuous 3D anatomical
representation, while a Temporal Motion Network (TMN), guided by
Transformer-derived respiratory signals, produces temporally consistent
deformation fields. Evaluation using a free-breathing dataset of 19 volunteers
demonstrates that our template- and phase-free method accurately captures both
regular and irregular respiratory patterns, while preserving vessel and
bronchial continuity with high anatomical fidelity. The proposed method
significantly improves efficiency, reducing the total processing time from
approximately five hours required by conventional discrete sorting methods to
just 15 minutes of training. Furthermore, it enables inference of each 3D
volume in under one second. The framework accurately reconstructs 3D images at
any respiratory state, achieves superior performance compared to conventional
methods, and demonstrates strong potential for application in 4D radiation
therapy planning and real-time adaptive treatment.

</details>


### [31] [An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects](https://arxiv.org/abs/2509.18451)
*Prithvi Raj Singh,Raju Gottumukkala,Anthony Maida*

Main category: cs.CV

TL;DR: 评估五种基于卡尔曼滤波的跟踪方法在快速移动小物体（如壁球）跟踪中的性能，发现DeepOCSORT跟踪误差最小但所有方法都存在显著跟踪漂移，误差比标准目标跟踪基准高3-4倍。


<details>
  <summary>Details</summary>
Motivation: 快速移动小物体的不可预测运动模式和小视觉标记使得精确跟踪成为计算机视觉中的挑战性问题，特别是在体育机器人应用中需要轻量级准确的跟踪系统。

Method: 使用包含10,000个标注壁球帧的自定义数据集，评估OCSORT、DeepOCSORT、ByteTrack、BoTSORT和StrongSORT五种卡尔曼滤波跟踪方法，分析推理速度和每图像更新频率对跟踪精度的影响。

Result: DeepOCSORT达到最低跟踪误差（平均ADE 31.15像素），ByteTrack处理速度最快（平均推理时间26.6ms），但所有跟踪器都表现出显著跟踪漂移，空间误差范围3-11cm（ADE值31-114像素）。

Conclusion: 当前跟踪方法在处理快速移动小物体的不可预测运动模式方面存在根本局限性，需要专门的方法论改进，误差率比标准目标跟踪基准高3-4倍。

Abstract: Unpredictable movement patterns and small visual mark make precise tracking
of fast-moving tiny objects like a racquetball one of the challenging problems
in computer vision. This challenge is particularly relevant for sport robotics
applications, where lightweight and accurate tracking systems can improve robot
perception and planning capabilities. While Kalman filter-based tracking
methods have shown success in general object tracking scenarios, their
performance degrades substantially when dealing with rapidly moving objects
that exhibit irregular bouncing behavior. In this study, we evaluate the
performance of five state-of-the-art Kalman filter-based tracking
methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom
dataset containing 10,000 annotated racquetball frames captured at 720p-1280p
resolution. We focus our analysis on two critical performance factors:
inference speed and update frequency per image, examining how these parameters
affect tracking accuracy and reliability for fast-moving tiny objects. Our
experimental evaluation across four distinct scenarios reveals that DeepOCSORT
achieves the lowest tracking error with an average ADE of 31.15 pixels compared
to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest
processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms.
However, our results show that all Kalman filter-based trackers exhibit
significant tracking drift with spatial errors ranging from 3-11cm (ADE values:
31-114 pixels), indicating fundamental limitations in handling the
unpredictable motion patterns of fast-moving tiny objects like racquetballs.
Our analysis demonstrates that current tracking approaches require substantial
improvements, with error rates 3-4x higher than standard object tracking
benchmarks, highlighting the need for specialized methodologies for fast-moving
tiny object tracking applications.

</details>


### [32] [MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition](https://arxiv.org/abs/2509.18473)
*Binhua Huang,Wendong Yao,Shaowu Chen,Guoxin Wang,Qingyuan Wang,Soumyabrata Dev*

Main category: cs.CV

TL;DR: MoCrop是一个用于压缩域视频动作识别的高效运动感知自适应裁剪模块，利用H.264视频中的运动向量定位运动密集区域，无需训练即可提升识别精度或降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有视频动作识别方法在压缩域中效率较低，需要处理大量数据。MoCrop旨在利用压缩视频中已有的运动信息，实现高效且无需额外训练的动作识别。

Method: MoCrop包含去噪合并(DM)、蒙特卡洛采样(MCS)和自适应裁剪(AC)三个轻量级组件，通过运动密度子矩阵搜索生成稳健的裁剪区域，适用于各种骨干网络。

Result: 在UCF101数据集上，MoCrop将ResNet-50的Top-1准确率提升3.5%（相同计算量）或提升2.4%同时减少26.5%计算量。在CoViAR上达到89.2%准确率，同时将计算量从11.6GFLOPs降至8.5GFLOPs。

Conclusion: MoCrop在多种骨干网络上均表现出良好的泛化能力，无需训练、不增加参数，适合压缩域实时部署，为高效视频动作识别提供了实用解决方案。

Abstract: We introduce MoCrop, a motion-aware adaptive cropping module for efficient
video action recognition in the compressed domain. MoCrop uses motion vectors
that are available in H.264 video to locate motion-dense regions and produces a
single clip-level crop that is applied to all I-frames at inference. The module
is training free, adds no parameters, and can be plugged into diverse
backbones. A lightweight pipeline that includes denoising & merge (DM), Monte
Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix
search yields robust crops with negligible overhead. On UCF101, MoCrop improves
accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy
at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer
FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy
at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6
to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B
indicate strong generality and make MoCrop practical for real-time deployment
in the compressed domain. Our code and models are available at
https://github.com/microa/MoCrop.

</details>


### [33] [Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems](https://arxiv.org/abs/2509.18481)
*Xinyu Wang,Zikun Zhou,Yingjian Li,Xin An,Hongpeng Wang*

Main category: cs.CV

TL;DR: 提出CAFC-SE框架，通过码本自适应特征压缩和语义增强，在低比特率条件下提升机器视觉分析性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在低比特率条件下表现不佳，要么保留过多冗余细节，要么学习过度集中的符号分布，导致分析性能下降

Method: 使用向量量化将连续视觉特征映射到离散码本索引，选择性传输到云端，通过投影到最近视觉基元来保留更多信息性视觉模式

Result: 大量实验证明该方法在比特率和准确率方面具有优越性

Conclusion: CAFC-SE框架对低比特率条件更具鲁棒性，能够有效提升边缘-云系统的分析性能

Abstract: Coding images for machines with minimal bitrate and strong analysis
performance is key to effective edge-cloud systems. Several approaches deploy
an image codec and perform analysis on the reconstructed image. Other methods
compress intermediate features using entropy models and subsequently perform
analysis on the decoded features. Nevertheless, these methods both perform
poorly under low-bitrate conditions, as they retain many redundant details or
learn over-concentrated symbol distributions. In this paper, we propose a
Codebook-based Adaptive Feature Compression framework with Semantic
Enhancement, named CAFC-SE. It maps continuous visual features to discrete
indices with a codebook at the edge via Vector Quantization (VQ) and
selectively transmits them to the cloud. The VQ operation that projects feature
vectors onto the nearest visual primitives enables us to preserve more
informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is
less vulnerable to low-bitrate conditions. Extensive experiments demonstrate
the superiority of our method in terms of rate and accuracy.

</details>


### [34] [MK-UNet: Multi-kernel Lightweight CNN for Medical Image Segmentation](https://arxiv.org/abs/2509.18493)
*Md Mostafijur Rahman,Radu Marculescu*

Main category: cs.CV

TL;DR: MK-UNet是一种超轻量级的多核U形CNN架构，专为医学图像分割设计，通过多核深度卷积块和注意力机制，在极低计算成本下实现优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学图像分割模型计算复杂度高、参数量大的问题，为资源受限环境（如床旁设备）提供实时、高精度的分割解决方案。

Method: 设计了多核深度卷积块（MKDC）处理多分辨率空间关系，结合通道、空间和分组门控注意力机制强调图像显著特征，构建超轻量级U形网络架构。

Result: 仅需0.316M参数和0.314G FLOPs，在六个医学图像基准测试中超越TransUNet、UNeXt等SOTA方法，DICE分数显著提升，计算资源大幅降低。

Conclusion: MK-UNet在保持极低计算成本的同时实现了卓越的分割性能，为资源受限环境下的实时医学诊断提供了无与伦比的解决方案。

Abstract: In this paper, we introduce MK-UNet, a paradigm shift towards
ultra-lightweight, multi-kernel U-shaped CNNs tailored for medical image
segmentation. Central to MK-UNet is the multi-kernel depth-wise convolution
block (MKDC) we design to adeptly process images through multiple kernels,
while capturing complex multi-resolution spatial relationships. MK-UNet also
emphasizes the images salient features through sophisticated attention
mechanisms, including channel, spatial, and grouped gated attention. Our
MK-UNet network, with a modest computational footprint of only 0.316M
parameters and 0.314G FLOPs, represents not only a remarkably lightweight, but
also significantly improved segmentation solution that provides higher accuracy
over state-of-the-art (SOTA) methods across six binary medical imaging
benchmarks. Specifically, MK-UNet outperforms TransUNet in DICE score with
nearly 333$\times$ and 123$\times$ fewer parameters and FLOPs, respectively.
Similarly, when compared against UNeXt, MK-UNet exhibits superior segmentation
performance, improving the DICE score up to 6.7% margins while operating with
4.7$\times$ fewer #Params. Our MK-UNet also outperforms other recent
lightweight networks, such as MedT, CMUNeXt, EGE-UNet, and Rolling-UNet, with
much lower computational resources. This leap in performance, coupled with
drastic computational gains, positions MK-UNet as an unparalleled solution for
real-time, high-fidelity medical diagnostics in resource-limited settings, such
as point-of-care devices. Our implementation is available at
https://github.com/SLDGroup/MK-UNet.

</details>


### [35] [BridgeSplat: Bidirectionally Coupled CT and Non-Rigid Gaussian Splatting for Deformable Intraoperative Surgical Navigation](https://arxiv.org/abs/2509.18501)
*Maximilian Fehrentz,Alexander Winkler,Thomas Heiliger,Nazim Haouchine,Christian Heiliger,Nassir Navab*

Main category: cs.CV

TL;DR: BridgeSplat是一种新颖的可变形手术导航方法，通过将术中3D重建与术前CT数据耦合，弥合手术视频和体积患者数据之间的差距。


<details>
  <summary>Details</summary>
Motivation: 解决手术视频与术前CT数据之间的不对齐问题，实现更精确的手术导航。

Method: 将3D高斯函数绑定到CT网格上，通过光度监督联合优化高斯参数和网格变形，参数化每个高斯相对于其父网格三角形。

Result: 在猪内脏手术和人类肝脏模拟数据上验证了方法的有效性，能够在单目RGB数据上实现合理的术前CT变形。

Conclusion: BridgeSplat通过耦合3D重建和CT数据，实现了有效的可变形手术导航，代码和数据已开源。

Abstract: We introduce BridgeSplat, a novel approach for deformable surgical navigation
that couples intraoperative 3D reconstruction with preoperative CT data to
bridge the gap between surgical video and volumetric patient data. Our method
rigs 3D Gaussians to a CT mesh, enabling joint optimization of Gaussian
parameters and mesh deformation through photometric supervision. By
parametrizing each Gaussian relative to its parent mesh triangle, we enforce
alignment between Gaussians and mesh and obtain deformations that can be
propagated back to update the CT. We demonstrate BridgeSplat's effectiveness on
visceral pig surgeries and synthetic data of a human liver under simulation,
showing sensible deformations of the preoperative CT on monocular RGB data.
Code, data, and additional resources can be found at
https://maxfehrentz.github.io/ct-informed-splatting/ .

</details>


### [36] [Source-Free Domain Adaptive Semantic Segmentation of Remote Sensing Images with Diffusion-Guided Label Enrichment](https://arxiv.org/abs/2509.18502)
*Wenjie Liu,Hongmin Liu,Lixin Zhang,Bin Fan*

Main category: cs.CV

TL;DR: 提出了Diffusion-Guided Label Enrichment (DGLE)框架，用于源自由域适应中的语义分割，通过扩散模型从少量高质量伪标签生成完整的高质量伪标签集。


<details>
  <summary>Details</summary>
Motivation: 现有源自由域适应方法通常需要优化整个伪标签集，但伪标签往往包含大量噪声，同时优化所有标签具有挑战性，限制了自训练的效果。

Method: 首先通过置信度过滤和超分辨率增强获得少量高质量伪标签作为初始种子，然后利用扩散模型传播这些不完整且分布不规则的种子伪标签，生成完整的高质量伪标签。

Result: 该方法有效避免了直接优化完整伪标签集的困难，显著提高了伪标签质量，从而提升了模型在目标域的性能。

Conclusion: DGLE框架通过扩散模型引导的标签富集策略，为源自由域适应中的语义分割任务提供了一种有效的伪标签优化解决方案。

Abstract: Research on unsupervised domain adaptation (UDA) for semantic segmentation of
remote sensing images has been extensively conducted. However, research on how
to achieve domain adaptation in practical scenarios where source domain data is
inaccessible namely, source-free domain adaptation (SFDA) remains limited.
Self-training has been widely used in SFDA, which requires obtaining as many
high-quality pseudo-labels as possible to train models on target domain data.
Most existing methods optimize the entire pseudo-label set to obtain more
supervisory information. However, as pseudo-label sets often contain
substantial noise, simultaneously optimizing all labels is challenging. This
limitation undermines the effectiveness of optimization approaches and thus
restricts the performance of self-training. To address this, we propose a novel
pseudo-label optimization framework called Diffusion-Guided Label Enrichment
(DGLE), which starts from a few easily obtained high-quality pseudo-labels and
propagates them to a complete set of pseudo-labels while ensuring the quality
of newly generated labels. Firstly, a pseudo-label fusion method based on
confidence filtering and super-resolution enhancement is proposed, which
utilizes cross-validation of details and contextual information to obtain a
small number of high-quality pseudo-labels as initial seeds. Then, we leverage
the diffusion model to propagate incomplete seed pseudo-labels with irregular
distributions due to its strong denoising capability for randomly distributed
noise and powerful modeling capacity for complex distributions, thereby
generating complete and high-quality pseudo-labels. This method effectively
avoids the difficulty of directly optimizing the complete set of pseudo-labels,
significantly improves the quality of pseudo-labels, and thus enhances the
model's performance in the target domain.

</details>


### [37] [Hyperbolic Coarse-to-Fine Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.18504)
*Jiaxin Dai,Xiang Xiang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: In the field of machine learning, hyperbolic space demonstrates superior
representation capabilities for hierarchical data compared to conventional
Euclidean space. This work focuses on the Coarse-To-Fine Few-Shot
Class-Incremental Learning (C2FSCIL) task. Our study follows the Knowe
approach, which contrastively learns coarse class labels and subsequently
normalizes and freezes the classifier weights of learned fine classes in the
embedding space. To better interpret the "coarse-to-fine" paradigm, we propose
embedding the feature extractor into hyperbolic space. Specifically, we employ
the Poincar\'e ball model of hyperbolic space, enabling the feature extractor
to transform input images into feature vectors within the Poincar\'e ball
instead of Euclidean space. We further introduce hyperbolic contrastive loss
and hyperbolic fully-connected layers to facilitate model optimization and
classification in hyperbolic space. Additionally, to enhance performance under
few-shot conditions, we implement maximum entropy distribution in hyperbolic
space to estimate the probability distribution of fine-class feature vectors.
This allows generation of augmented features from the distribution to mitigate
overfitting during training with limited samples. Experiments on C2FSCIL
benchmarks show that our method effectively improves both coarse and fine class
accuracies.

</details>


### [38] [GeoRemover: Removing Objects and Their Causal Visual Artifacts](https://arxiv.org/abs/2509.18538)
*Zixin Zhu,Haoxiang Li,Xuelu Feng,He Wu,Chunming Qiao,Junsong Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种几何感知的两阶段框架，通过解耦几何移除和外观渲染来解决对象移除中的因果视觉伪影问题，实现了对物体及其相关阴影、反射等因果效应的有效去除。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑方法在对象移除时存在局限性：严格掩码对齐的方法无法去除未明确掩码的因果视觉伪影（如阴影、反射），而松散掩码对齐的方法缺乏可控性且可能过度擦除其他对象。这些限制源于忽略了物体几何存在与其视觉效应之间的因果关系。

Method: 提出几何感知的两阶段框架：1）几何移除阶段：使用严格掩码对齐监督直接从几何（如深度）中移除对象，实现具有强几何约束的结构感知编辑；2）外观渲染阶段：基于更新后的几何条件渲染逼真的RGB图像，其中因果视觉效应作为修改3D几何的结果被隐式考虑。引入基于正负样本对的偏好驱动目标来指导几何移除阶段的学习。

Result: 在两个流行基准测试上的广泛实验表明，该方法在移除物体及其相关伪影方面达到了最先进的性能。

Conclusion: 通过解耦几何移除和外观渲染，并利用几何-外观因果关系，该方法能够有效去除目标物体及其因果视觉伪影，同时保持编辑的可控性和准确性。

Abstract: Towards intelligent image editing, object removal should eliminate both the
target object and its causal visual artifacts, such as shadows and reflections.
However, existing image appearance-based methods either follow strictly
mask-aligned training and fail to remove these causal effects which are not
explicitly masked, or adopt loosely mask-aligned strategies that lack
controllability and may unintentionally over-erase other objects. We identify
that these limitations stem from ignoring the causal relationship between an
object's geometry presence and its visual effects. To address this limitation,
we propose a geometry-aware two-stage framework that decouples object removal
into (1) geometry removal and (2) appearance rendering. In the first stage, we
remove the object directly from the geometry (e.g., depth) using strictly
mask-aligned supervision, enabling structure-aware editing with strong
geometric constraints. In the second stage, we render a photorealistic RGB
image conditioned on the updated geometry, where causal visual effects are
considered implicitly as a result of the modified 3D geometry. To guide
learning in the geometry removal stage, we introduce a preference-driven
objective based on positive and negative sample pairs, encouraging the model to
remove objects as well as their causal visual artifacts while avoiding new
structural insertions. Extensive experiments demonstrate that our method
achieves state-of-the-art performance in removing both objects and their
associated artifacts on two popular benchmarks. The code is available at
https://github.com/buxiangzhiren/GeoRemover.

</details>


### [39] [SEGA: A Transferable Signed Ensemble Gaussian Black-Box Attack against No-Reference Image Quality Assessment Models](https://arxiv.org/abs/2509.18546)
*Yujia Liu,Dingquan Li,Tiejun Huang*

Main category: cs.CV

TL;DR: 本文提出了SEGA方法，通过高斯平滑和集成梯度来解决NR-IQA模型黑盒攻击中迁移性差的问题


<details>
  <summary>Details</summary>
Motivation: 现有的NR-IQA模型白盒攻击方法在更现实的无法访问目标模型的黑盒场景下迁移性较差，需要提升攻击的迁移能力

Method: 提出SEGA方法：1）对源模型应用高斯平滑并集成其平滑梯度来近似目标模型梯度；2）使用专门设计的扰动过滤掩码去除不合适的扰动以保证扰动不可感知

Result: 在CLIVE数据集上的实验结果表明SEGA具有优越的迁移性，能够成功实现对NR-IQA模型的基于迁移的黑盒攻击

Conclusion: SEGA是首个有效解决NR-IQA模型攻击迁移性挑战的方法，为揭示模型脆弱性和指导鲁棒系统设计提供了有价值的工具

Abstract: No-Reference Image Quality Assessment (NR-IQA) models play an important role
in various real-world applications. Recently, adversarial attacks against
NR-IQA models have attracted increasing attention, as they provide valuable
insights for revealing model vulnerabilities and guiding robust system design.
Some effective attacks have been proposed against NR-IQA models in white-box
settings, where the attacker has full access to the target model. However,
these attacks often suffer from poor transferability to unknown target models
in more realistic black-box scenarios, where the target model is inaccessible.
This work makes the first attempt to address the challenge of low
transferability in attacking NR-IQA models by proposing a transferable Signed
Ensemble Gaussian black-box Attack (SEGA). The main idea is to approximate the
gradient of the target model by applying Gaussian smoothing to source models
and ensembling their smoothed gradients. To ensure the imperceptibility of
adversarial perturbations, SEGA further removes inappropriate perturbations
using a specially designed perturbation filter mask. Experimental results on
the CLIVE dataset demonstrate the superior transferability of SEGA, validating
its effectiveness in enabling successful transfer-based black-box attacks
against NR-IQA models.

</details>


### [40] [HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles](https://arxiv.org/abs/2509.18550)
*Mohammad Junayed Hasan,Nabeel Mohammed,Shafin Rahman,Philipp Koehn*

Main category: cs.CV

TL;DR: HadaSmileNet是一个新的特征融合框架，通过参数自由的乘法交互直接整合基于Transformer的表征和生理学基础的D-Marker特征，在微笑情感识别任务中实现了最先进的性能，同时显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有多任务学习框架在微笑情感识别中的计算效率低下问题，这些方法由于辅助任务监督和复杂的损失平衡需求而效率不高。

Method: 提出了HadaSmileNet框架，系统评估了15种融合策略，发现Hadamard乘法融合能够实现直接特征交互同时保持计算效率。

Result: 在四个基准数据集上取得了最先进的结果：UvA-NEMO (88.7%, +0.8)、MMI (99.7%)、SPOS (98.5%, +0.7)和BBC (100%, +5.0)。参数减少26%，训练过程简化。

Conclusion: 该框架的效率和有效性使其特别适合需要实时情感计算能力的多媒体数据挖掘应用的实际部署。

Abstract: The distinction between genuine and posed emotions represents a fundamental
pattern recognition challenge with significant implications for data mining
applications in social sciences, healthcare, and human-computer interaction.
While recent multi-task learning frameworks have shown promise in combining
deep learning architectures with handcrafted D-Marker features for smile facial
emotion recognition, these approaches exhibit computational inefficiencies due
to auxiliary task supervision and complex loss balancing requirements. This
paper introduces HadaSmileNet, a novel feature fusion framework that directly
integrates transformer-based representations with physiologically grounded
D-Markers through parameter-free multiplicative interactions. Through
systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard
multiplicative fusion achieves optimal performance by enabling direct feature
interactions while maintaining computational efficiency. The proposed approach
establishes new state-of-the-art results for deep learning methods across four
benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS
(98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational
analysis reveals 26 percent parameter reduction and simplified training
compared to multi-task alternatives, while feature visualization demonstrates
enhanced discriminative power through direct domain knowledge integration. The
framework's efficiency and effectiveness make it particularly suitable for
practical deployment in multimedia data mining applications that require
real-time affective computing capabilities.

</details>


### [41] [Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction](https://arxiv.org/abs/2509.18566)
*Xiaoting Yin,Hao Shi,Kailun Yang,Jiajun Zhai,Shangwei Guo,Lin Wang,Kaiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种基于事件相机和3D高斯泼溅的单目视频动态人体与静态场景重建框架，通过事件引导的损失函数解决快速运动下的运动模糊问题。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在快速运动时会产生运动模糊，影响动态人体重建质量。事件相机具有微秒级时间分辨率，更适合捕捉快速运动。

Method: 使用统一的3D高斯集合，其中可学习语义属性区分人体和场景高斯；人体高斯进行变形动画，场景高斯保持静态；提出事件引导损失函数匹配渲染亮度变化与事件流。

Result: 在ZJU-MoCap-Blur和MMHPSD-Blur数据集上实现最先进的重建效果，PSNR/SSIM指标显著提升，LPIPS降低，特别对高速运动主体效果更好。

Conclusion: 该方法无需外部人体掩码，简化了高斯集管理，有效解决了快速运动下的重建问题，为动态人体场景重建提供了新思路。

Abstract: Reconstructing dynamic humans together with static scenes from monocular
videos remains difficult, especially under fast motion, where RGB frames suffer
from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond
temporal resolution, making them a superior sensing choice for dynamic human
reconstruction. Accordingly, we present a novel event-guided human-scene
reconstruction framework that jointly models human and scene from a single
monocular event camera via 3D Gaussian Splatting. Specifically, a unified set
of 3D Gaussians carries a learnable semantic attribute; only Gaussians
classified as human undergo deformation for animation, while scene Gaussians
stay static. To combat blur, we propose an event-guided loss that matches
simulated brightness changes between consecutive renderings with the event
stream, improving local fidelity in fast-moving regions. Our approach removes
the need for external human masks and simplifies managing separate Gaussian
sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers
state-of-the-art human-scene reconstruction, with notable gains over strong
baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.

</details>


### [42] [Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought](https://arxiv.org/abs/2509.18571)
*Yuhan Wang,Cheng Liu,Zihan Zhao,Weichao Wu*

Main category: cs.CV

TL;DR: Live-E2T是一个实时威胁监控框架，通过结构化语义元组、在线事件去重和基于大语言模型的推理，同时实现了实时性能和决策可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足实时威胁监控对性能和可解释性的双重需求，需要一种统一解决方案。

Method: 1. 将视频帧解构为结构化的人-物-交互-地点语义元组；2. 提出高效的在线事件去重和更新机制；3. 使用思维链策略微调大语言模型进行透明推理。

Result: 在XD-Violence和UCF-Crime基准数据集上的实验表明，Live-E2T在威胁检测准确率、实时效率和可解释性方面显著优于现有最优方法。

Conclusion: Live-E2T成功解决了实时威胁监控中性能与可解释性的平衡问题，为实际应用提供了有效解决方案。

Abstract: Real-time threat monitoring identifies threatening behaviors in video streams
and provides reasoning and assessment of threat events through explanatory
text. However, prevailing methodologies, whether based on supervised learning
or generative models, struggle to concurrently satisfy the demanding
requirements of real-time performance and decision explainability. To bridge
this gap, we introduce Live-E2T, a novel framework that unifies these two
objectives through three synergistic mechanisms. First, we deconstruct video
frames into structured Human-Object-Interaction-Place semantic tuples. This
approach creates a compact, semantically focused representation, circumventing
the information degradation common in conventional feature compression. Second,
an efficient online event deduplication and updating mechanism is proposed to
filter spatio-temporal redundancies, ensuring the system's real time
responsiveness. Finally, we fine-tune a Large Language Model using a
Chain-of-Thought strategy, endow it with the capability for transparent and
logical reasoning over event sequences to produce coherent threat assessment
reports. Extensive experiments on benchmark datasets, including XD-Violence and
UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art
methods in terms of threat detection accuracy, real-time efficiency, and the
crucial dimension of explainability.

</details>


### [43] [The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers](https://arxiv.org/abs/2509.18582)
*Daiqing Qi,Handong Zhao,Jing Shi,Simon Jenni,Yifei Fan,Franck Dernoncourt,Scott Cohen,Sheng Li*

Main category: cs.CV

TL;DR: 论文提出了一种增强多模态大语言模型（MLLMs）美学理解能力的新方法，包括PhotoCritique数据集、PhotoEye模型和PhotoBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在美学视觉理解方面的不足，特别是专业摄影场景中需要摄影技术、前后期处理知识等专业知识的挑战。

Method: 1. 构建大规模专业摄影讨论数据集PhotoCritique；2. 提出语言引导的多视角视觉融合机制PhotoEye模型；3. 建立全面专业的美学视觉理解基准PhotoBench。

Result: 在现有基准和PhotoBench上，PhotoEye模型相比现有模型展现出明显优势。

Conclusion: 通过专业数据集、创新模型和全面基准的协同设计，显著提升了MLLMs的美学视觉理解能力。

Abstract: While editing directly from life, photographers have found it too difficult
to see simultaneously both the blue and the sky. Photographer and curator,
Szarkowski insightfully revealed one of the notable gaps between general and
aesthetic visual understanding: while the former focuses on identifying the
factual element in an image (sky), the latter transcends such object
identification, viewing it instead as an aesthetic component--a pure color
block (blue). Such fundamental distinctions between general (detection,
localization, etc.) and aesthetic (color, lighting, composition, etc.) visual
understanding present a significant challenge for Multimodal Large Language
Models (MLLMs). Although some recent works have made initial explorations, they
are often limited to general and basic aesthetic commonsense. As a result, they
frequently fall short in real-world scenarios (Fig. 1), which require extensive
expertise--including photographic techniques, photo pre/post-processing
knowledge, and more, to provide a detailed analysis and description. To
fundamentally enhance the aesthetics understanding of MLLMs, we first introduce
a novel dataset, PhotoCritique, derived from extensive discussions among
professional photographers and enthusiasts, and characterized by the large
scale, expertise, and diversity. Then, to better learn visual aesthetics from
PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a
languageguided multi-view vision fusion mechanism to understand image
aesthetics from multiple perspectives. Finally, we present a novel benchmark,
PhotoBench, a comprehensive and professional benchmark for aesthetic visual
understanding. On existing benchmarks and PhotoBench, our model demonstrates
clear advantages over existing models.

</details>


### [44] [Enhancing Video Object Segmentation in TrackRAD Using XMem Memory Network](https://arxiv.org/abs/2509.18591)
*Pengchao Deng,Shengqi Chen*

Main category: cs.CV

TL;DR: 提出基于XMem模型的实时MRI引导放疗肿瘤分割框架，用于TrackRAD2025挑战赛，通过记忆增强架构在长序列cine-MRI中跟踪肿瘤运动


<details>
  <summary>Details</summary>
Motivation: 提高MRI引导放疗中肿瘤跟踪的精度，这对于提升癌症治疗的准确性和安全性至关重要

Method: 利用XMem记忆增强架构，集成记忆机制来实时跟踪肿瘤运动，在有限标注数据条件下实现高效分割

Result: 实验记录丢失无法报告精确量化结果，但初步开发印象显示XMem框架具有合理分割性能并满足临床实时要求

Conclusion: 该工作有助于改善MRI引导放疗期间的肿瘤跟踪精度，为癌症治疗提供更准确安全的解决方案

Abstract: This paper presents an advanced tumor segmentation framework for real-time
MRI-guided radiotherapy, designed for the TrackRAD2025 challenge. Our method
leverages the XMem model, a memory-augmented architecture, to segment tumors
across long cine-MRI sequences. The proposed system efficiently integrates
memory mechanisms to track tumor motion in real-time, achieving high
segmentation accuracy even under challenging conditions with limited annotated
data. Unfortunately, the detailed experimental records have been lost,
preventing us from reporting precise quantitative results at this stage.
Nevertheless, From our preliminary impressions during development, the
XMem-based framework demonstrated reasonable segmentation performance and
satisfied the clinical real-time requirement. Our work contributes to improving
the precision of tumor tracking during MRI-guided radiotherapy, which is
crucial for enhancing the accuracy and safety of cancer treatments.

</details>


### [45] [SSCM: A Spatial-Semantic Consistent Model for Multi-Contrast MRI Super-Resolution](https://arxiv.org/abs/2509.18593)
*Xiaoman Wu,Lubin Gan,Siying Wu,Jing Zhang,Yunwei Ou,Xiaoyan Sun*

Main category: cs.CV

TL;DR: 提出SSCM模型解决多对比度MRI超分辨率中的空间-语义一致性问题，通过动态空间扭曲、语义感知令牌聚合和空间-频率融合实现更好的对齐和细节恢复


<details>
  <summary>Details</summary>
Motivation: 传统方法在空间-语义一致性建模不足，且未充分利用频域信息，导致细粒度对齐差和高频细节恢复不足

Method: SSCM模型包含三个核心模块：动态空间扭曲模块用于跨对比度空间对齐，语义感知令牌聚合块用于长程语义一致性，空间-频率融合块用于精细结构恢复

Result: 在公开和私有数据集上的实验表明，SSCM以更少的参数实现了最先进的性能，同时确保空间和语义一致的重建

Conclusion: SSCM模型有效解决了多对比度MRI超分辨率中的关键挑战，为医学图像处理提供了更高效的解决方案

Abstract: Multi-contrast Magnetic Resonance Imaging super-resolution (MC-MRI SR) aims
to enhance low-resolution (LR) contrasts leveraging high-resolution (HR)
references, shortening acquisition time and improving imaging efficiency while
preserving anatomical details. The main challenge lies in maintaining
spatial-semantic consistency, ensuring anatomical structures remain
well-aligned and coherent despite structural discrepancies and motion between
the target and reference images. Conventional methods insufficiently model
spatial-semantic consistency and underuse frequency-domain information, which
leads to poor fine-grained alignment and inadequate recovery of high-frequency
details. In this paper, we propose the Spatial-Semantic Consistent Model
(SSCM), which integrates a Dynamic Spatial Warping Module for inter-contrast
spatial alignment, a Semantic-Aware Token Aggregation Block for long-range
semantic consistency, and a Spatial-Frequency Fusion Block for fine structure
restoration. Experiments on public and private datasets show that SSCM achieves
state-of-the-art performance with fewer parameters while ensuring spatially and
semantically consistent reconstructions.

</details>


### [46] [OraPO: Oracle-educated Reinforcement Learning for Data-efficient and Factual Radiology Report Generation](https://arxiv.org/abs/2509.18600)
*Zhuoxiao Chen,Hongyang Yu,Ying Xu,Yadan Luo,Long Duong,Yuan-Fang Li*

Main category: cs.CV

TL;DR: 提出OraPO方法和FactS奖励机制，在有限计算资源下实现高效放射学报告生成，显著减少训练数据需求并达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 解决现有放射学报告生成方法对大规模数据和计算资源的高度依赖问题，为资源受限环境提供高效解决方案

Method: 使用Oracle-educated GRPO (OraPO)结合FactScore-based奖励机制，通过单阶段强化学习训练，将失败的探索转化为直接偏好监督

Result: 在CheXpert Plus数据集上达到0.341的F1分数，仅需2-3个数量级更少的训练数据，使用小型基础VLM在普通硬件上实现

Conclusion: OraPO和FactS共同构建了一个紧凑而强大的框架，显著提高了临床挑战性病例的学习效率，为资源受限环境下的放射学报告生成提供了新标准

Abstract: Radiology report generation (RRG) aims to automatically produce clinically
faithful reports from chest X-ray images. Prevailing work typically follows a
scale-driven paradigm, by multi-stage training over large paired corpora and
oversized backbones, making pipelines highly data- and compute-intensive. In
this paper, we propose Oracle-educated GRPO {OraPO) with a FactScore-based
reward (FactS) to tackle the RRG task under constrained budgets. OraPO enables
single-stage, RL-only training by converting failed GRPO explorations on rare
or difficult studies into direct preference supervision via a lightweight
oracle step. FactS grounds learning in diagnostic evidence by extracting atomic
clinical facts and checking entailment against ground-truth labels, yielding
dense, interpretable sentence-level rewards. Together, OraPO and FactS create a
compact and powerful framework that significantly improves learning efficiency
on clinically challenging cases, setting the new SOTA performance on the
CheXpert Plus dataset (0.341 in F1) with 2--3 orders of magnitude less training
data using a small base VLM on modest hardware.

</details>


### [47] [Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation](https://arxiv.org/abs/2509.18602)
*Xu Liu,Yibo Lu,Xinxian Wang,Xinyu Wu*

Main category: cs.CV

TL;DR: AMSF是一个无需训练的参考式框架，通过语义令牌分解和相似性感知重加权模块，实现多风格图像在扩散模型中的可控融合。


<details>
  <summary>Details</summary>
Motivation: 现有参考式方法只能处理单一风格图像，无法实现混合美学效果，且缺乏平衡多种风格影响的原则性机制。

Method: 使用语义令牌分解模块编码所有风格图像和文本提示，自适应注入到冻结扩散模型的交叉注意力层；相似性感知重加权模块在每个去噪步骤重新校准对各风格分量的注意力分配。

Result: 定性和定量评估表明AMSF在多风格融合结果上持续优于最先进方法，且融合设计可无缝扩展到两种或更多风格。

Conclusion: AMSF是扩散模型中实现表达性多风格生成的实用步骤，无需微调或外部适配器即可实现平衡且用户可控的风格混合。

Abstract: We propose Adaptive Multi-Style Fusion (AMSF), a reference-based
training-free framework that enables controllable fusion of multiple reference
styles in diffusion models. Most of the existing reference-based methods are
limited by (a) acceptance of only one style image, thus prohibiting hybrid
aesthetics and scalability to more styles, and (b) lack of a principled
mechanism to balance several stylistic influences. AMSF mitigates these
challenges by encoding all style images and textual hints with a semantic token
decomposition module that is adaptively injected into every cross-attention
layer of an frozen diffusion model. A similarity-aware re-weighting module then
recalibrates, at each denoising step, the attention allocated to every style
component, yielding balanced and user-controllable blends without any
fine-tuning or external adapters. Both qualitative and quantitative evaluations
show that AMSF produces multi-style fusion results that consistently outperform
the state-of-the-art approaches, while its fusion design scales seamlessly to
two or more styles. These capabilities position AMSF as a practical step toward
expressive multi-style generation in diffusion models.

</details>


### [48] [MLF-4DRCNet: Multi-Level Fusion with 4D Radar and Camera for 3D Object Detection in Autonomous Driving](https://arxiv.org/abs/2509.18613)
*Yuzhi Wu,Li Xiao,Jun Liu,Guangfeng Jiang,XiangGen Xia*

Main category: cs.CV

TL;DR: MLF-4DRCNet是一种用于4D雷达和相机图像多级融合的两阶段3D目标检测框架，通过点级、场景级和提议级融合克服雷达点云稀疏性问题，在VoD和TJ4DRadSet数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的4D雷达-相机融合方法大多采用为LiDAR设计的BEV融合范式，忽视了雷达点云的稀疏性和几何不完整性，且仅限于粗粒度的场景级融合，限制了检测性能。

Method: 提出三阶段融合框架：1) ERPE模块通过三重注意力体素特征编码器增强雷达点云；2) HSFP模块使用可变形注意力动态融合多尺度体素特征与2D图像特征；3) PLFE模块通过融合图像特征精炼区域提议。

Result: 在View-of-Delft和TJ4DRadSet数据集上的实验表明，MLF-4DRCNet实现了最先进的性能，在VoD数据集上达到与基于LiDAR模型相当的性能。

Conclusion: 多级融合策略能有效解决4D雷达点云稀疏性问题，为自动驾驶提供了一种成本效益高且鲁棒的3D目标检测方案。

Abstract: The emerging 4D millimeter-wave radar, measuring the range, azimuth,
elevation, and Doppler velocity of objects, is recognized for its
cost-effectiveness and robustness in autonomous driving. Nevertheless, its
point clouds exhibit significant sparsity and noise, restricting its standalone
application in 3D object detection. Recent 4D radar-camera fusion methods have
provided effective perception. Most existing approaches, however, adopt
explicit Bird's-Eye-View fusion paradigms originally designed for LiDAR-camera
fusion, neglecting radar's inherent drawbacks. Specifically, they overlook the
sparse and incomplete geometry of radar point clouds and restrict fusion to
coarse scene-level integration. To address these problems, we propose
MLF-4DRCNet, a novel two-stage framework for 3D object detection via
multi-level fusion of 4D radar and camera images. Our model incorporates the
point-, scene-, and proposal-level multi-modal information, enabling
comprehensive feature representation. It comprises three crucial components:
the Enhanced Radar Point Encoder (ERPE) module, the Hierarchical Scene Fusion
Pooling (HSFP) module, and the Proposal-Level Fusion Enhancement (PLFE) module.
Operating at the point-level, ERPE densities radar point clouds with 2D image
instances and encodes them into voxels via the proposed Triple-Attention Voxel
Feature Encoder. HSFP dynamically integrates multi-scale voxel features with 2D
image features using deformable attention to capture scene context and adopts
pooling to the fused features. PLFE refines region proposals by fusing image
features, and further integrates with the pooled features from HSFP.
Experimental results on the View-of-Delft (VoD) and TJ4DRadSet datasets
demonstrate that MLF-4DRCNet achieves the state-of-the-art performance.
Notably, it attains performance comparable to LiDAR-based models on the VoD
dataset.

</details>


### [49] [Prompt-Guided Dual Latent Steering for Inversion Problems](https://arxiv.org/abs/2509.18619)
*Yichen Wu,Xu Liu,Chenxuan Zhao,Xinyu Wu*

Main category: cs.CV

TL;DR: PDLS是一种基于Rectified Flow模型的训练免费框架，通过双流潜在空间引导解决图像逆扩散中的语义漂移问题，在保持结构完整性的同时提升语义准确性。


<details>
  <summary>Details</summary>
Motivation: 当前基于单潜在向量的图像逆扩散方法难以平衡结构保真度和语义准确性，导致重建图像出现语义漂移（如细节模糊或属性错误）。

Method: 提出Prompt-Guided Dual Latent Steering (PDLS)框架，将逆扩散过程分解为结构路径和语义路径，通过最优控制问题和线性二次调节器(LQR)实现动态轨迹引导。

Result: 在FFHQ-1K和ImageNet-1K数据集上的多种逆扩散任务（高斯去模糊、运动去模糊、超分辨率和自由形式修复）中，PDLS相比单潜在基线方法能产生更忠实于原图且语义对齐更好的重建结果。

Conclusion: PDLS通过双流潜在引导机制有效解决了逆扩散中的语义漂移问题，无需昂贵的逐图像优化即可实现高质量重建。

Abstract: Inverting corrupted images into the latent space of diffusion models is
challenging. Current methods, which encode an image into a single latent
vector, struggle to balance structural fidelity with semantic accuracy, leading
to reconstructions with semantic drift, such as blurred details or incorrect
attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering
(PDLS), a novel, training-free framework built upon Rectified Flow models for
their stable inversion paths. PDLS decomposes the inversion process into two
complementary streams: a structural path to preserve source integrity and a
semantic path guided by a prompt. We formulate this dual guidance as an optimal
control problem and derive a closed-form solution via a Linear Quadratic
Regulator (LQR). This controller dynamically steers the generative trajectory
at each step, preventing semantic drift while ensuring the preservation of fine
detail without costly, per-image optimization. Extensive experiments on FFHQ-1K
and ImageNet-1K under various inversion tasks, including Gaussian deblurring,
motion deblurring, super-resolution and freeform inpainting, demonstrate that
PDLS produces reconstructions that are both more faithful to the original image
and better aligned with the semantic information than single-latent baselines.

</details>


### [50] [Learning neuroimaging models from health system-scale data](https://arxiv.org/abs/2509.18638)
*Yiwei Lyu,Samir Harake,Asadur Chowdury,Soumyanil Banerjee,Rachel Gologorsky,Shixuan Liu,Anna-Katharina Meissner,Akshay Rao,Chenhui Zhao,Akhil Kondepudi,Cheng Jiang,Xinhai Hou,Rushikesh S. Joshi,Volker Neuschmelting,Ashok Srinivasan,Dawn Kleindorfer,Brian Athey,Vikas Gulani,Aditya Pandey,Honglak Lee,Todd Hollon*

Main category: cs.CV

TL;DR: 开发了Prima，首个基于视觉语言模型的神经影像AI基础模型，用于临床MRI分析，在52种神经疾病诊断中达到92.0的AUC值，优于现有先进模型。


<details>
  <summary>Details</summary>
Motivation: 全球MRI需求持续增长给医疗系统带来压力，特别是在资源匮乏地区，需要AI技术来改善诊断效率和公平性。

Method: 利用大型学术医疗系统作为数据引擎，训练包含22万例MRI研究的视觉语言模型，采用分层视觉架构提供通用可迁移的MRI特征。

Result: 在包含3万例MRI研究的1年系统测试中，Prima在主要神经疾病诊断中表现优异，并提供可解释的鉴别诊断、工作列表优先级和临床转诊建议。

Conclusion: Prima展示了医疗系统规模视觉语言模型的变革潜力，能够推动AI驱动的医疗保健发展，特别是在解决医疗系统偏见方面。

Abstract: Neuroimaging is a ubiquitous tool for evaluating patients with neurological
diseases. The global demand for magnetic resonance imaging (MRI) studies has
risen steadily, placing significant strain on health systems, prolonging
turnaround times, and intensifying physician burnout \cite{Chen2017-bt,
Rula2024-qp-1}. These challenges disproportionately impact patients in
low-resource and rural settings. Here, we utilized a large academic health
system as a data engine to develop Prima, the first vision language model (VLM)
serving as an AI foundation for neuroimaging that supports real-world, clinical
MRI studies as input. Trained on over 220,000 MRI studies, Prima uses a
hierarchical vision architecture that provides general and transferable MRI
features. Prima was tested in a 1-year health system-wide study that included
30K MRI studies. Across 52 radiologic diagnoses from the major neurologic
disorders, including neoplastic, inflammatory, infectious, and developmental
lesions, Prima achieved a mean diagnostic area under the ROC curve of 92.0,
outperforming other state-of-the-art general and medical AI models. Prima
offers explainable differential diagnoses, worklist priority for radiologists,
and clinical referral recommendations across diverse patient demographics and
MRI systems. Prima demonstrates algorithmic fairness across sensitive groups
and can help mitigate health system biases, such as prolonged turnaround times
for low-resource populations. These findings highlight the transformative
potential of health system-scale VLMs and Prima's role in advancing AI-driven
healthcare.

</details>


### [51] [Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation](https://arxiv.org/abs/2509.18639)
*Yuanhuiyi Lyu,Chi Kit Wong,Chenfei Liao,Lutao Jiang,Xu Zheng,Zexin Lu,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: 提出Understanding-in-Generation (UiG)框架，通过统一模型的强大理解能力来增强图像生成性能，解决了现有方法将理解和生成过程分离的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于Chain-of-Thought的文本到图像生成方法将理解和生成过程分离，限制了其指导统一模型推理的能力，无法有效弥补生成能力的不足。

Method: 引入"图像编辑"作为桥梁，在推理过程中集成生成指导。首先验证生成图像并将统一模型的理解融入编辑指令，然后逐步增强生成图像，将理解逐步注入生成过程。

Result: 在TIIF基准测试的长提示设置上实现了3.92%的性能提升，显著优于现有的文本到图像推理方法。

Conclusion: UiG框架通过将理解能力融入生成过程，有效提升了统一模型在文本到图像生成任务中的性能表现。

Abstract: Recent works have made notable advancements in enhancing unified models for
text-to-image generation through the Chain-of-Thought (CoT). However, these
reasoning methods separate the processes of understanding and generation, which
limits their ability to guide the reasoning of unified models in addressing the
deficiencies of their generative capabilities. To this end, we propose a novel
reasoning framework for unified models, Understanding-in-Generation (UiG),
which harnesses the robust understanding capabilities of unified models to
reinforce their performance in image generation. The core insight of our UiG is
to integrate generative guidance by the strong understanding capabilities
during the reasoning process, thereby mitigating the limitations of generative
abilities. To achieve this, we introduce "Image Editing" as a bridge to infuse
understanding into the generation process. Initially, we verify the generated
image and incorporate the understanding of unified models into the editing
instructions. Subsequently, we enhance the generated image step by step,
gradually infusing the understanding into the generation process. Our UiG
framework demonstrates a significant performance improvement in text-to-image
generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on
the long prompt setting of the TIIF benchmark. The project code:
https://github.com/QC-LY/UiG

</details>


### [52] [Zero-shot Monocular Metric Depth for Endoscopic Images](https://arxiv.org/abs/2509.18642)
*Nicolas Toussaint,Emanuele Colleoni,Ricardo Sanchez-Matilla,Joshua Sutcliffe,Vanessa Thompson,Muhammad Asad,Imanol Luengo,Danail Stoyanov*

Main category: cs.CV

TL;DR: 该论文提出了一个用于内窥镜图像深度估计的综合基准测试和新颖合成数据集EndoSynth，通过微调深度基础模型显著提升了在真实数据上的准确性。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像深度估计领域缺乏鲁棒的基准测试和高质量数据集，限制了该领域的发展和应用。

Method: 创建了包含真实内窥镜图像的基准测试，并开发了EndoSynth合成数据集，该数据集包含内窥镜手术器械及其对应的真实深度和分割掩码，用于微调深度基础模型。

Result: 使用EndoSynth数据集微调深度基础模型后，在大多数未见过的真实数据上的准确性显著提升。

Conclusion: 该工作通过提供基准测试和合成数据集，推动了内窥镜图像深度估计领域的发展，为未来研究提供了重要资源。

Abstract: Monocular relative and metric depth estimation has seen a tremendous boost in
the last few years due to the sharp advancements in foundation models and in
particular transformer based networks. As we start to see applications to the
domain of endoscopic images, there is still a lack of robust benchmarks and
high-quality datasets in that area. This paper addresses these limitations by
presenting a comprehensive benchmark of state-of-the-art (metric and relative)
depth estimation models evaluated on real, unseen endoscopic images, providing
critical insights into their generalisation and performance in clinical
scenarios. Additionally, we introduce and publish a novel synthetic dataset
(EndoSynth) of endoscopic surgical instruments paired with ground truth metric
depth and segmentation masks, designed to bridge the gap between synthetic and
real-world data. We demonstrate that fine-tuning depth foundation models using
our synthetic dataset boosts accuracy on most unseen real data by a significant
margin. By providing both a benchmark and a synthetic dataset, this work
advances the field of depth estimation for endoscopic images and serves as an
important resource for future research. Project page, EndoSynth dataset and
trained weights are available at https://github.com/TouchSurgery/EndoSynth.

</details>


### [53] [LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection](https://arxiv.org/abs/2509.18683)
*Lanhu Wu,Zilin Gao,Hao Fei,Mong-Li Lee,Wynne Hsu*

Main category: cs.CV

TL;DR: 提出LEAF-Mamba模型，通过局部强调状态空间模块和自适应融合模块，解决RGB-D显著性目标检测中局部语义缺失和跨模态融合不足的问题，在性能和效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有RGB-D显著性目标检测方法要么受限于CNN的局部感受野，要么受限于Vision Transformer的二次复杂度，难以平衡性能与计算效率。状态空间模型(SSM)虽然能线性复杂度建模长距离依赖，但直接应用于RGB-D SOD会导致局部语义缺失和跨模态融合不足。

Method: 提出LEAF-Mamba模型，包含两个核心组件：1)局部强调状态空间模块(LE-SSM)用于捕获多尺度局部依赖；2)基于SSM的自适应融合模块(AFM)实现互补的跨模态交互和可靠的跨模态集成。

Result: 在RGB-D SOD任务上超越16个最先进方法，在RGB-T SOD任务上也表现出色，证明了强大的泛化能力。

Conclusion: LEAF-Mamba在RGB-D显著性目标检测中实现了性能与效率的良好平衡，通过创新的局部强调和自适应融合机制有效解决了现有方法的局限性。

Abstract: RGB-D salient object detection (SOD) aims to identify the most conspicuous
objects in a scene with the incorporation of depth cues. Existing methods
mainly rely on CNNs, limited by the local receptive fields, or Vision
Transformers that suffer from the cost of quadratic complexity, posing a
challenge in balancing performance and computational efficiency. Recently,
state space models (SSM), Mamba, have shown great potential for modeling
long-range dependency with linear complexity. However, directly applying SSM to
RGB-D SOD may lead to deficient local semantics as well as the inadequate
cross-modality fusion. To address these issues, we propose a Local Emphatic and
Adaptive Fusion state space model (LEAF-Mamba) that contains two novel
components: 1) a local emphatic state space module (LE-SSM) to capture
multi-scale local dependencies for both modalities. 2) an SSM-based adaptive
fusion module (AFM) for complementary cross-modality interaction and reliable
cross-modality integration. Extensive experiments demonstrate that the
LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in
both efficacy and efficiency. Moreover, our method can achieve excellent
performance on the RGB-T SOD task, proving a powerful generalization ability.

</details>


### [54] [Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification](https://arxiv.org/abs/2509.18692)
*Xinle Gao,Linghui Ye,Zhiyong Xiao*

Main category: cs.CV

TL;DR: 提出了一种轻量级食品图像分类算法，结合窗口多头注意力机制和空间注意力机制，在降低计算复杂度的同时保持高分类精度。


<details>
  <summary>Details</summary>
Motivation: 随着食品工业对生产质量和效率要求的提高，需要高效的自动化质量控制系统。但Vision Transformer模型参数多、计算复杂，难以在资源受限环境中部署。

Method: 提出WMHAM窗口多头注意力机制通过窗口划分降低计算成本，同时结合SAM空间注意力机制自适应强调关键空间区域，提升特征表示能力。

Result: 在Food-101和Vireo Food-172数据集上分别达到95.24%和94.33%的准确率，同时显著减少了参数和计算量。

Conclusion: 该方法在计算效率和分类性能之间取得了良好平衡，适合在资源受限环境中部署。

Abstract: With the rapid development of society and continuous advances in science and
technology, the food industry increasingly demands higher production quality
and efficiency. Food image classification plays a vital role in enabling
automated quality control on production lines, supporting food safety
supervision, and promoting intelligent agricultural production. However, this
task faces challenges due to the large number of parameters and high
computational complexity of Vision Transformer models. To address these issues,
we propose a lightweight food image classification algorithm that integrates a
Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism
(SAM). The WMHAM reduces computational cost by capturing local and global
contextual features through efficient window partitioning, while the SAM
adaptively emphasizes key spatial regions to improve discriminative feature
representation. Experiments conducted on the Food-101 and Vireo Food-172
datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%,
respectively, while significantly reducing parameters and FLOPs compared with
baseline methods. These results confirm that the proposed approach achieves an
effective balance between computational efficiency and classification
performance, making it well-suited for deployment in resource-constrained
environments.

</details>


### [55] [OSDA: A Framework for Open-Set Discovery and Automatic Interpretation of Land-cover in Remote Sensing Imagery](https://arxiv.org/abs/2509.18693)
*Siyi Chen,Kai Wang,Weicong Pang,Ruiming Yang,Ziru Chen,Renjun Gao,Alexis Kai Hon Lau,Dasa Gu,Chenchen Zhang,Cheng Li*

Main category: cs.CV

TL;DR: OSDA是一个用于遥感开放集土地覆盖分析的三阶段框架，无需标注即可实现发现、分割和描述


<details>
  <summary>Details</summary>
Motivation: 解决遥感中开放集土地覆盖分析的关键挑战，需要同时实现细粒度空间定位和语义开放分类，检测和分割无类别监督的新对象，并通过多模态推理赋予可解释的语义标签

Method: 三阶段流程：1) 使用可提示的微调分割模型(SAM)进行精确发现和掩码提取；2) 通过两阶段微调的多模态大语言模型(MLLM)进行语义归因和上下文描述；3) 使用LLM作为裁判和人工评分来评估MLLM

Result: 该框架结合像素级精度与高级语义理解，支持在多样化卫星图像上进行鲁棒评估，无需人工标注

Conclusion: OSDA为动态土地覆盖监测提供了可扩展和可解释的解决方案，在自动化制图更新和大规模地球观测分析方面显示出强大潜力

Abstract: Open-set land-cover analysis in remote sensing requires the ability to
achieve fine-grained spatial localization and semantically open categorization.
This involves not only detecting and segmenting novel objects without
categorical supervision but also assigning them interpretable semantic labels
through multimodal reasoning. In this study, we introduce OSDA, an integrated
three-stage framework for annotation-free open-set land-cover discovery,
segmentation, and description. The pipeline consists of: (1) precise discovery
and mask extraction with a promptable fine-tuned segmentation model (SAM), (2)
semantic attribution and contextual description via a two-phase fine-tuned
multimodal large language model (MLLM), and (3) LLM-as-judge and manual scoring
of the MLLMs evaluation. By combining pixel-level accuracy with high-level
semantic understanding, OSDA addresses key challenges in open-world remote
sensing interpretation. Designed to be architecture-agnostic and label-free,
the framework supports robust evaluation across diverse satellite imagery
without requiring manual annotation. Our work provides a scalable and
interpretable solution for dynamic land-cover monitoring, showing strong
potential for automated cartographic updating and large-scale earth observation
analysis.

</details>


### [56] [Overview of PlantCLEF 2021: cross-domain plant identification](https://arxiv.org/abs/2509.18697)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: PlantCLEF 2021挑战赛旨在评估如何利用植物标本馆数据改进在数据稀缺地区（如热带地区）的植物自动识别，通过跨域分类任务结合标本馆样本和野外照片进行训练和测试。


<details>
  <summary>Details</summary>
Motivation: 当前植物自动识别技术主要基于北美和西欧的数据，而生物多样性最丰富的热带地区数据稀缺。植物标本馆积累了大量的标本数据，可用于改进这些地区的识别能力。

Method: 使用包含约1000种南美洲圭亚那地盾地区植物的数据集，训练集包含数十万标本馆样本和数千张野外照片，测试集仅包含野外照片。除了常规元数据外，还包含5种形态和功能性状数据。

Result: 挑战赛评估了跨域分类任务的效果，参与者开发了多种系统来学习标本馆和野外照片之间的对应关系，以改进数据稀缺地区的植物识别。

Conclusion: 利用植物标本馆的数字化记录可以有效提升在生物多样性丰富但数据稀缺地区的植物自动识别能力，为全球植物保护和研究提供技术支持。

Abstract: Automated plant identification has improved considerably thanks to recent
advances in deep learning and the availability of training data with more and
more field photos. However, this profusion of data concerns only a few tens of
thousands of species, mainly located in North America and Western Europe, much
less in the richest regions in terms of biodiversity such as tropical
countries. On the other hand, for several centuries, botanists have
systematically collected, catalogued and stored plant specimens in herbaria,
especially in tropical regions, and recent efforts by the biodiversity
informatics community have made it possible to put millions of digitised
records online. The LifeCLEF 2021 plant identification challenge (or "PlantCLEF
2021") was designed to assess the extent to which automated identification of
flora in data-poor regions can be improved by using herbarium collections. It
is based on a dataset of about 1,000 species mainly focused on the Guiana
Shield of South America, a region known to have one of the highest plant
diversities in the world. The challenge was evaluated as a cross-domain
classification task where the training set consisted of several hundred
thousand herbarium sheets and a few thousand photos to allow learning a
correspondence between the two domains. In addition to the usual metadata
(location, date, author, taxonomy), the training data also includes the values
of 5 morphological and functional traits for each species. The test set
consisted exclusively of photos taken in the field. This article presents the
resources and evaluations of the assessment carried out, summarises the
approaches and systems used by the participating research groups and provides
an analysis of the main results.

</details>


### [57] [AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping](https://arxiv.org/abs/2509.18699)
*Zedong Zhang,Ying Tai,Jianjun Qian,Jian Yang,Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGSwap的文本到图像生成方法，用于融合跨类别对象，并引入了COF基准数据集。该方法通过组嵌入交换和自适应组更新机制，有效解决了现有方法在对象融合时产生的偏差、视觉混乱和语义不一致问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成方法在融合跨类别对象时经常产生有偏见的、视觉混乱或语义不一致的结果，且该领域缺乏全面的基准数据集。

Method: AGSwap方法包含两个关键组件：(1) 组嵌入交换，通过特征操作融合不同概念的语义属性；(2) 自适应组更新，基于平衡评估分数的动态优化机制。此外还构建了COF数据集，包含95个超类和10个子类，共451,250个独特的融合对。

Result: 大量实验表明，AGSwap在简单和复杂提示下均优于现有的最先进组合T2I方法，包括GPT-Image-1。

Conclusion: AGSwap是一种简单而高效的跨类别对象融合方法，结合COF基准数据集，显著提升了文本到图像生成中对象融合的质量和一致性。

Abstract: Fusing cross-category objects to a single coherent object has gained
increasing attention in text-to-image (T2I) generation due to its broad
applications in virtual reality, digital media, film, and gaming. However,
existing methods often produce biased, visually chaotic, or semantically
inconsistent results due to overlapping artifacts and poor integration.
Moreover, progress in this field has been limited by the absence of a
comprehensive benchmark dataset. To address these problems, we propose
\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective
approach comprising two key components: (1) Group-wise Embedding Swapping,
which fuses semantic attributes from different concepts through feature
manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism
guided by a balance evaluation score to ensure coherent synthesis.
Additionally, we introduce \textbf{Cross-category Object Fusion (COF)}, a
large-scale, hierarchically structured dataset built upon ImageNet-1K and
WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling
451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap
outperforms state-of-the-art compositional T2I methods, including GPT-Image-1
using simple and complex prompts.

</details>


### [58] [Overview of LifeCLEF Plant Identification task 2019: diving into data deficient tropical countries](https://arxiv.org/abs/2509.18705)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: 该论文介绍了LifeCLEF 2019植物识别挑战赛，旨在评估在数据稀缺地区（如圭亚那地盾和亚马逊雨林）的植物自动识别性能，并与热带植物专家的表现进行比较。


<details>
  <summary>Details</summary>
Motivation: 虽然深度学习在植物识别方面取得了进展，但现有数据仅覆盖数万种植物，而全球有近36.9万种植物。该挑战赛旨在解决数据稀缺地区的植物识别问题。

Method: 基于包含1万种主要来自圭亚那地盾和亚马逊雨林地区的植物数据集，组织挑战赛并比较各参与研究组的自动识别系统性能。

Result: 论文总结了各参与团队的方法和系统，并提供了主要结果的分析。

Conclusion: 该挑战赛为数据稀缺地区的植物自动识别提供了评估平台，并展示了当前技术在该领域的应用潜力。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data.
However, this profusion of data only concerns a few tens of thousands of
species, while the planet has nearly 369K. The LifeCLEF 2019 Plant
Identification challenge (or "PlantCLEF 2019") was designed to evaluate
automated identification on the flora of data deficient regions. It is based on
a dataset of 10K species mainly focused on the Guiana shield and the Northern
Amazon rainforest, an area known to have one of the greatest diversity of
plants and animals in the world. As in the previous edition, a comparison of
the performance of the systems evaluated with the best tropical flora experts
was carried out. This paper presents the resources and assessments of the
challenge, summarizes the approaches and systems employed by the participating
research groups, and provides an analysis of the main outcomes.

</details>


### [59] [RSVG-ZeroOV: Exploring a Training-Free Framework for Zero-Shot Open-Vocabulary Visual Grounding in Remote Sensing Images](https://arxiv.org/abs/2509.18711)
*Ke Li,Di Wang,Ting Wang,Fuyu Dong,Yiming Zhang,Luyao Zhang,Xiangyu Wang,Shaofeng Li,Quan Wang*

Main category: cs.CV

TL;DR: RSVG-ZeroOV是一个无需训练的零样本开放词汇遥感视觉定位框架，利用冻结的通用基础模型，通过三阶段方法实现高效定位。


<details>
  <summary>Details</summary>
Motivation: 现有遥感视觉定位方法受限于封闭词汇集，且依赖高质量数据集和微调，难以适应开放世界场景。

Method: 采用三阶段框架：Overview阶段使用VLM获取跨注意力图；Focus阶段利用DM补充结构信息；Evolve阶段通过注意力进化模块净化分割掩码。

Result: 实验表明该框架在弱监督和零样本方法中表现优异。

Conclusion: RSVG-ZeroOV提供了一个无需任务特定训练的高效可扩展解决方案。

Abstract: Remote sensing visual grounding (RSVG) aims to localize objects in remote
sensing images based on free-form natural language expressions. Existing
approaches are typically constrained to closed-set vocabularies, limiting their
applicability in open-world scenarios. While recent attempts to leverage
generic foundation models for open-vocabulary RSVG, they overly rely on
expensive high-quality datasets and time-consuming fine-tuning. To address
these limitations, we propose \textbf{RSVG-ZeroOV}, a training-free framework
that aims to explore the potential of frozen generic foundation models for
zero-shot open-vocabulary RSVG. Specifically, RSVG-ZeroOV comprises three key
stages: (i) Overview: We utilize a vision-language model (VLM) to obtain
cross-attention\footnote[1]{In this paper, although decoder-only VLMs use
self-attention over all tokens, we refer to the image-text interaction part as
cross-attention to distinguish it from pure visual self-attention.}maps that
capture semantic correlations between text queries and visual regions. (ii)
Focus: By leveraging the fine-grained modeling priors of a diffusion model
(DM), we fill in gaps in structural and shape information of objects, which are
often overlooked by VLM. (iii) Evolve: A simple yet effective attention
evolution module is introduced to suppress irrelevant activations, yielding
purified segmentation masks over the referred objects. Without cumbersome
task-specific training, RSVG-ZeroOV offers an efficient and scalable solution.
Extensive experiments demonstrate that the proposed framework consistently
outperforms existing weakly-supervised and zero-shot methods.

</details>


### [60] [What Makes You Unique? Attribute Prompt Composition for Object Re-Identification](https://arxiv.org/abs/2509.18715)
*Yingquan Wang,Pingping Zhang,Chong Sun,Dong Wang,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出属性提示组合框架（APC），通过文本语义联合增强对象重识别（ReID）的判别性和泛化能力，解决单域过拟合和跨域泛化不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有ReID模型受限于单域或跨域场景，单域模型易过拟合域特定特征，跨域模型可能抑制身份判别线索。需要平衡判别性和泛化性。

Method: 设计属性提示生成器（APG）包含语义属性字典（SAD）和提示组合模块（PCM），结合快慢训练策略（FSTS）平衡ReID特定判别和预训练视觉语言模型（VLM）的泛化知识。

Result: 在传统和领域泛化（DG）ReID数据集上超越现有最优方法，在判别性和泛化性方面均表现优异。

Conclusion: APC框架有效利用文本语义提升ReID性能，通过属性提示和双流训练实现判别与泛化的平衡，具有实际应用价值。

Abstract: Object Re-IDentification (ReID) aims to recognize individuals across
non-overlapping camera views. While recent advances have achieved remarkable
progress, most existing models are constrained to either single-domain or
cross-domain scenarios, limiting their real-world applicability. Single-domain
models tend to overfit to domain-specific features, whereas cross-domain models
often rely on diverse normalization strategies that may inadvertently suppress
identity-specific discriminative cues. To address these limitations, we propose
an Attribute Prompt Composition (APC) framework, which exploits textual
semantics to jointly enhance discrimination and generalization. Specifically,
we design an Attribute Prompt Generator (APG) consisting of a Semantic
Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an
over-complete attribute dictionary to provide rich semantic descriptions, while
PCM adaptively composes relevant attributes from SAD to generate discriminative
attribute-aware features. In addition, motivated by the strong generalization
ability of Vision-Language Models (VLM), we propose a Fast-Slow Training
Strategy (FSTS) to balance ReID-specific discrimination and generalizable
representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS)
to rapidly acquire ReID-specific discriminative knowledge and a Slow Update
Stream (SUS) to retain the generalizable knowledge inherited from the
pre-trained VLM. Through a mutual interaction, the framework effectively
focuses on ReID-relevant features while mitigating overfitting. Extensive
experiments on both conventional and Domain Generalized (DG) ReID datasets
demonstrate that our framework surpasses state-of-the-art methods, exhibiting
superior performances in terms of both discrimination and generalization. The
source code is available at https://github.com/AWangYQ/APC.

</details>


### [61] [Pre-training CLIP against Data Poisoning with Optimal Transport-based Matching and Alignment](https://arxiv.org/abs/2509.18717)
*Tong Zhang,Kuofeng Gao,Jiawang Bai,Leo Yu Zhang,Xin Yin,Zonghui Wang,Shouling Ji,Wenzhi Chen*

Main category: cs.CV

TL;DR: 提出基于最优传输的OTCCLIP框架，通过细粒度视觉-文本特征对齐来防御CLIP模型的数据中毒攻击，相比现有方法显著提升模型性能


<details>
  <summary>Details</summary>
Motivation: 现有防御方法仅依赖全局特征匹配图像-文本对，忽视了细粒度特征，可能引入错误配对并损害CLIP预训练效果

Method: 设计基于最优传输的细粒度特征距离度量，重新分配图像-文本对，并采用最优传输目标函数促进跨模态和模态内细粒度对齐

Result: OTCCLIP成功降低中毒攻击成功率，在中毒数据集上显著提升CLIP的零样本和线性探测性能

Conclusion: 最优传输框架能有效重建图像-文本对，通过细粒度特征对齐提升CLIP模型对数据中毒攻击的鲁棒性

Abstract: Recent studies have shown that Contrastive Language-Image Pre-training (CLIP)
models are threatened by targeted data poisoning and backdoor attacks due to
massive training image-caption pairs crawled from the Internet. Previous
defense methods correct poisoned image-caption pairs by matching a new caption
for each image. However, the matching process relies solely on the global
representations of images and captions, overlooking fine-grained features of
visual and textual features. It may introduce incorrect image-caption pairs and
harm the CLIP pre-training. To address their limitations, we propose an Optimal
Transport-based framework to reconstruct image-caption pairs, named OTCCLIP. We
propose a new optimal transport-based distance measure between fine-grained
visual and textual feature sets and re-assign new captions based on the
proposed optimal transport distance. Additionally, to further reduce the
negative impact of mismatched pairs, we encourage the inter- and intra-modality
fine-grained alignment by employing optimal transport-based objective
functions. Our experiments demonstrate that OTCCLIP can successfully decrease
the attack success rates of poisoning attacks. Also, compared to previous
methods, OTCCLIP significantly improves CLIP's zero-shot and linear probing
performance trained on poisoned datasets.

</details>


### [62] [Knowledge Transfer from Interaction Learning](https://arxiv.org/abs/2509.18733)
*Yilin Gao,Kangyi Chen,Zhongxing Peng,Hengjie Lu,Shugong Xu*

Main category: cs.CV

TL;DR: 提出了LFI框架，通过模拟视觉语言模型中的交互过程来改进视觉基础模型的知识迁移能力


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型主要采用结果导向范式，忽略了视觉语言模型中跨模态交互过程的知识迁移，导致泛化能力受限

Method: LFI框架包含两个技术创新：交互查询（维护跨网络层的持久关系结构）和基于交互的监督（从VLMs的跨模态注意力机制中提取）

Result: 在多个基准测试中取得显著提升：TinyImageNet分类提升3.3mAP，COCO检测/分割提升1.6mAP/2.4AP，跨域设置中PACS和VLCS分别提升2.4和9.3零样本性能

Conclusion: LFI框架通过模拟交互过程实现了更有效的知识迁移，在参数开销最小的情况下获得更快收敛和更好的认知对齐

Abstract: Current visual foundation models (VFMs) face a fundamental limitation in
transferring knowledge from vision language models (VLMs), while VLMs excel at
modeling cross-modal interactions through unified representation spaces,
existing VFMs predominantly adopt result-oriented paradigms that neglect the
underlying interaction processes. This representational discrepancy hinders
effective knowledge transfer and limits generalization across diverse vision
tasks. We propose Learning from Interactions (LFI), a cognitive-inspired
framework that addresses this gap by explicitly modeling visual understanding
as an interactive process. Our key insight is that capturing the dynamic
interaction patterns encoded in pre-trained VLMs enables more faithful and
efficient knowledge transfer to VFMs. The approach centers on two technical
innovations, Interaction Queries, which maintain persistent relational
structures across network layers, and interaction-based supervision, derived
from the cross-modal attention mechanisms of VLMs. Comprehensive experiments
demonstrate consistent improvements across multiple benchmarks, achieving 3.3
and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO
detection/segmentation respectively, with minimal parameter overhead and faster
convergence. The framework particularly excels in cross-domain settings,
delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human
evaluations further confirm its cognitive alignment, outperforming
result-oriented methods by 2.7 times in semantic consistency metrics.

</details>


### [63] [HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection](https://arxiv.org/abs/2509.18738)
*Ruichao Hou,Xingyuan Li,Tongwei Ren,Dongming Zhou,Gangshan Wu,Jinde Cao*

Main category: cs.CV

TL;DR: 提出了HyPSAM模型，利用SAM的零样本泛化能力进行RGB-热成像显著目标检测，通过动态融合网络和可插拔优化网络实现高质量显著图生成和优化


<details>
  <summary>Details</summary>
Motivation: 解决RGB-热成像显著目标检测中由于内在特征融合不足和外在数据稀缺导致的边界不精确和对象不完整问题

Method: 1. 动态融合网络(DFNet)生成高质量初始显著图作为视觉提示；2. 可插拔优化网络(P2RNet)使用混合提示引导SAM优化显著图

Result: 在三个公开数据集上实现最先进性能，具有显著通用性，可与不同RGB-T SOD方法无缝集成

Conclusion: HyPSAM展示了提示工程在RGB-T SOD领域的潜力，为多模态显著目标检测提供了有效解决方案

Abstract: RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent
objects by integrating complementary information from RGB and thermal
modalities. However, learning the precise boundaries and complete objects
remains challenging due to the intrinsic insufficient feature fusion and the
extrinsic limitations of data scarcity. In this paper, we propose a novel
hybrid prompt-driven segment anything model (HyPSAM), which leverages the
zero-shot generalization capabilities of the segment anything model (SAM) for
RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that
generates high-quality initial saliency maps as visual prompts. DFNet employs
dynamic convolution and multi-branch decoding to facilitate adaptive
cross-modality interaction, overcoming the limitations of fixed-parameter
kernels and enhancing multi-modal feature representation. Moreover, we propose
a plug-and-play refinement network (P2RNet), which serves as a general
optimization strategy to guide SAM in refining saliency maps by using hybrid
prompts. The text prompt ensures reliable modality input, while the mask and
box prompts enable precise salient object localization. Extensive experiments
on three public datasets demonstrate that our method achieves state-of-the-art
performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating
with different RGB-T SOD methods to achieve significant performance gains,
thereby highlighting the potential of prompt engineering in this field. The
code and results of our method are available at:
https://github.com/milotic233/HyPSAM.

</details>


### [64] [TriFusion-AE: Language-Guided Depth and LiDAR Fusion for Robust Point Cloud Processing](https://arxiv.org/abs/2509.18743)
*Susmit Neogi*

Main category: cs.CV

TL;DR: TriFusion-AE是一个多模态交叉注意力自编码器，通过融合文本先验、单目深度图和LiDAR点云来提高点云去噪和重建的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶和机器人感知中容易受到噪声、遮挡和对抗性攻击的影响，现有自编码器在真实世界挑战性条件下性能下降。

Method: 提出TriFusion-AE模型，整合文本语义线索、图像几何特征和LiDAR空间结构，通过多模态交叉注意力机制进行联合表示学习。

Result: 在nuScenes-mini数据集上评估，模型在强对抗攻击和重噪声条件下显著优于CNN自编码器，在轻度扰动下提升有限。

Conclusion: 多模态融合框架是模型无关的，可与任何CNN点云自编码器集成，提高在挑战性条件下的鲁棒性。

Abstract: LiDAR-based perception is central to autonomous driving and robotics, yet raw
point clouds remain highly vulnerable to noise, occlusion, and adversarial
corruptions. Autoencoders offer a natural framework for denoising and
reconstruction, but their performance degrades under challenging real-world
conditions. In this work, we propose TriFusion-AE, a multimodal cross-attention
autoencoder that integrates textual priors, monocular depth maps from
multi-view images, and LiDAR point clouds to improve robustness. By aligning
semantic cues from text, geometric (depth) features from images, and spatial
structure from LiDAR, TriFusion-AE learns representations that are resilient to
stochastic noise and adversarial perturbations. Interestingly, while showing
limited gains under mild perturbations, our model achieves significantly more
robust reconstruction under strong adversarial attacks and heavy noise, where
CNN-based autoencoders collapse. We evaluate on the nuScenes-mini dataset to
reflect realistic low-data deployment scenarios. Our multimodal fusion
framework is designed to be model-agnostic, enabling seamless integration with
any CNN-based point cloud autoencoder for joint representation learning.

</details>


### [65] [COLT: Enhancing Video Large Language Models with Continual Tool Usage](https://arxiv.org/abs/2509.18754)
*Yuyang Liu,Xinyuan Shi,Bang Yang,Peilin Zhou,Jiahua Dong,Long Chen,Ian Reid,Xiaondan Liang*

Main category: cs.CV

TL;DR: 提出COLT方法，通过可学习的工具代码本增强开源视频大语言模型的持续工具使用能力，避免灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频LLM方法假设工具库固定，难以适应现实世界中工具数据持续演化和流式输入的环境。

Method: COLT方法包含可学习的工具代码本作为工具特定记忆系统，基于用户指令与代码本中工具特征的相似度动态选择相关工具，并构建VideoToolBench数据集进行指令调优。

Result: 在现有视频LLM基准和VideoToolBench数据集上的实验表明COLT达到最先进性能。

Conclusion: COLT方法成功解决了视频LLM在持续工具流环境中的工具使用泛化问题。

Abstract: The success of Large Language Models (LLMs) has significantly propelled the
research of video understanding. To harvest the benefits of well-trained expert
models (i.e., tools), video LLMs prioritize the exploration of tool usage
capabilities. Existing methods either prompt closed-source LLMs or employ the
instruction tuning paradigm for tool-use fine-tuning. These methods, however,
assume an established repository of fixed tools and struggle to generalize to
real-world environments where tool data is perpetually evolving and streaming
in. To this end, we propose to enhance open-source video LLMs with COntinuaL
Tool usage (termed COLT), which automatically acquires tool-use ability in a
successive tool stream without suffering 'catastrophic forgetting' of the past
learned tools. Specifically, our COLT incorporates a learnable tool codebook as
a tool-specific memory system. Then relevant tools are dynamically selected
based on the similarity between user instruction and tool features within the
codebook. To unleash the tool usage potential of video LLMs, we collect a
video-centric tool-use instruction tuning dataset VideoToolBench. Extensive
experiments on both previous video LLM benchmarks and the tool-use-specific
VideoToolBench dataset demonstrate the state-of-the-art performance of our
proposed COLT.

</details>


### [66] [FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation](https://arxiv.org/abs/2509.18759)
*Zhaorui Wang,Yi Gu,Deming Zhou,Renjing Xu*

Main category: cs.CV

TL;DR: FixingGS是一种无需训练的方法，利用现有扩散模型增强稀疏视角3D高斯泼溅重建，通过改进的蒸馏方法提供更准确的多视图一致先验，实现有效的伪影去除和补全。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏视角3D重建中由于视觉信息不足导致的伪影问题，现有生成先验方法虽然有效但难以保证多视图一致性，导致结构模糊和细节不真实。

Method: 提出FixingGS方法，核心是改进的蒸馏方法提供更准确和跨视图一致的扩散先验，同时采用自适应渐进增强方案进一步细化欠约束区域的重建。

Result: 大量实验表明FixingGS在视觉质量和重建性能上优于现有最先进方法。

Conclusion: FixingGS通过充分利用现有扩散模型的能力，成功解决了稀疏视角3DGS重建中的伪影和多视图一致性问题，取得了优异的性能。

Abstract: Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in
3D reconstruction and novel view synthesis. However, reconstructing 3D scenes
from sparse viewpoints remains highly challenging due to insufficient visual
information, which results in noticeable artifacts persisting across the 3D
representation. To address this limitation, recent methods have resorted to
generative priors to remove artifacts and complete missing content in
under-constrained areas. Despite their effectiveness, these approaches struggle
to ensure multi-view consistency, resulting in blurred structures and
implausible details. In this work, we propose FixingGS, a training-free method
that fully exploits the capabilities of the existing diffusion model for
sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our
distillation approach, which delivers more accurate and cross-view coherent
diffusion priors, thereby enabling effective artifact removal and inpainting.
In addition, we propose an adaptive progressive enhancement scheme that further
refines reconstructions in under-constrained regions. Extensive experiments
demonstrate that FixingGS surpasses existing state-of-the-art methods with
superior visual quality and reconstruction performance. Our code will be
released publicly.

</details>


### [67] [Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models](https://arxiv.org/abs/2509.18763)
*Xijun Wang,Junyun Huang,Rayyan Abdalla,Chengyuan Zhang,Ruiqi Xian,Dinesh Manocha*

Main category: cs.CV

TL;DR: Bi-VLM提出了一种基于高斯分位数的非均匀权重分离方法，通过显著性感知混合量化算法，在超低比特权重精度（≤2位）下显著提升视觉语言模型的效率。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型的计算需求和内存要求很高，限制了其在硬件受限环境中的应用。需要解决计算需求与超低比特权重精度之间的关键差距。

Method: 基于高斯分位数非均匀分离模型权重，将权重分为异常值（显著）和多个正常值（非显著）子集，提出显著性感知混合量化算法，对缩放器和二进制矩阵施加不同约束。

Result: 在VLM的语言模型部分，Bi-VLM在视觉问答任务上比SOTA提升3%-47%；整体VLM性能提升4%-45%。量化模型存在90%-99%的图像令牌冗余，可进一步剪枝提升效率。

Conclusion: Bi-VLM在超低比特量化下显著提升了VLMs的效率，同时保持了模型性能，为硬件受限环境中的VLMs应用提供了可行方案。

Abstract: We address the critical gap between the computational demands of
vision-language models and the possible ultra-low-bit weight precision
(bitwidth $\leq2$ bits) we can use for higher efficiency. Our work is motivated
by the substantial computational cost and memory requirements of VLMs, which
restrict their applicability in hardware-constrained environments. We propose
Bi-VLM, which separates model weights non-uniformly based on the Gaussian
quantiles. Our formulation groups the model weights into outlier (salient) and
multiple inlier (unsalient) subsets, ensuring that each subset contains a
proportion of weights corresponding to its quantile in the distribution. We
propose a saliency-aware hybrid quantization algorithm and use it to quantize
weights by imposing different constraints on the scaler and binary matrices
based on the saliency metric and compression objective. We have evaluated our
approach on different VLMs. For the language model part of the VLM, our Bi-VLM
outperforms the SOTA by 3%-47% on the visual question answering task in terms
of four different benchmarks and three different models. For the overall VLM,
our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the
quantized models and observe that there is redundancy of image tokens 90% - 99%
in the quantized models. This helps us to further prune the visual tokens to
improve efficiency.

</details>


### [68] [DiSSECT: Structuring Transfer-Ready Medical Image Representations through Discrete Self-Supervision](https://arxiv.org/abs/2509.18765)
*Azad Singh,Deepak Mishra*

Main category: cs.CV

TL;DR: DiSSECT是一个用于医学图像表示学习的自监督学习框架，通过引入多尺度向量量化来创建离散表示瓶颈，从而提高特征的可迁移性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督学习方法在医学图像中容易受到捷径学习的影响，特别是在胸部X光等解剖结构相似、病理特征细微的模态中。这些方法通常依赖复杂架构、解剖学先验或精心调优的数据增强，限制了其可扩展性和泛化能力。

Method: DiSSECT将多尺度向量量化整合到自监督学习流程中，强制模型学习可重复、结构感知的特征，同时抑制视图特定或低效用模式。这种离散表示瓶颈约束了特征学习过程。

Result: DiSSECT在分类和分割任务上均表现出色，需要极少或无需微调，在低标签场景下具有特别高的标签效率。在多个公共医学影像数据集上的验证表明，该方法相比现有最先进方法具有更好的鲁棒性和泛化能力。

Conclusion: DiSSECT通过离散表示学习有效解决了医学图像自监督学习中的捷径学习问题，提供了一种高效、可迁移的表示学习方法，特别适用于标签数据有限的临床场景。

Abstract: Self-supervised learning (SSL) has emerged as a powerful paradigm for medical
image representation learning, particularly in settings with limited labeled
data. However, existing SSL methods often rely on complex architectures,
anatomy-specific priors, or heavily tuned augmentations, which limit their
scalability and generalizability. More critically, these models are prone to
shortcut learning, especially in modalities like chest X-rays, where anatomical
similarity is high and pathology is subtle. In this work, we introduce DiSSECT
-- Discrete Self-Supervision for Efficient Clinical Transferable
Representations, a framework that integrates multi-scale vector quantization
into the SSL pipeline to impose a discrete representational bottleneck. This
constrains the model to learn repeatable, structure-aware features while
suppressing view-specific or low-utility patterns, improving representation
transfer across tasks and domains. DiSSECT achieves strong performance on both
classification and segmentation tasks, requiring minimal or no fine-tuning, and
shows particularly high label efficiency in low-label regimes. We validate
DiSSECT across multiple public medical imaging datasets, demonstrating its
robustness and generalizability compared to existing state-of-the-art
approaches.

</details>


### [69] [Real-time Deer Detection and Warning in Connected Vehicles via Thermal Sensing and Deep Learning](https://arxiv.org/abs/2509.18779)
*Hemanth Puppala,Wayne Sarasua,Srinivas Biyaguda,Farhad Farzinpour,Mashrur Chowdhury*

Main category: cs.CV

TL;DR: 本文提出了一种结合热成像、深度学习和车联网通信的实时检测和驾驶员预警系统，旨在减少鹿车碰撞事故。


<details>
  <summary>Details</summary>
Motivation: 美国每年发生约210万起鹿车碰撞事故，导致约440人死亡、5.9万人受伤和100亿美元经济损失，这些事故还导致鹿群数量下降。

Method: 系统使用热成像技术和深度学习算法，在超过12,000张热成像鹿图像数据集上进行训练和验证，并通过车联网设备实现传感器数据共享。

Result: 实验结果显示系统性能优异：平均精度98.84%，精确率95.44%，召回率95.96%。在恶劣天气条件下，热成像检测准确率保持在88-92%，而传统可见光摄像头效果低于60%。系统端到端延迟始终低于100毫秒。

Conclusion: 该系统通过热成像和联网车辆技术，为减少鹿车碰撞事故提供了可行的技术路径。

Abstract: Deer-vehicle collisions represent a critical safety challenge in the United
States, causing nearly 2.1 million incidents annually and resulting in
approximately 440 fatalities, 59,000 injuries, and 10 billion USD in economic
damages. These collisions also contribute significantly to declining deer
populations. This paper presents a real-time detection and driver warning
system that integrates thermal imaging, deep learning, and
vehicle-to-everything communication to help mitigate deer-vehicle collisions.
Our system was trained and validated on a custom dataset of over 12,000 thermal
deer images collected in Mars Hill, North Carolina. Experimental evaluation
demonstrates exceptional performance with 98.84 percent mean average precision,
95.44 percent precision, and 95.96 percent recall. The system was field tested
during a follow-up visit to Mars Hill and readily sensed deer providing the
driver with advanced warning. Field testing validates robust operation across
diverse weather conditions, with thermal imaging maintaining between 88 and 92
percent detection accuracy in challenging scenarios where conventional visible
light based cameras achieve less than 60 percent effectiveness. When a high
probability threshold is reached sensor data sharing messages are broadcast to
surrounding vehicles and roadside units via cellular vehicle to everything
(CV2X) communication devices. Overall, our system achieves end to end latency
consistently under 100 milliseconds from detection to driver alert. This
research establishes a viable technological pathway for reducing deer-vehicle
collisions through thermal imaging and connected vehicles.

</details>


### [70] [Towards Application Aligned Synthetic Surgical Image Synthesis](https://arxiv.org/abs/2509.18796)
*Danush Kumar Venkatesh,Stefanie Speidel*

Main category: cs.CV

TL;DR: SAADi框架通过偏好对齐扩散模型解决手术数据稀缺问题，提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: 手术标注数据稀缺限制了深度学习系统在计算机辅助干预中的应用，现有扩散模型存在数据记忆问题导致样本不一致且缺乏多样性，影响下游任务性能

Method: 提出SAADi框架，构建偏好和非偏好合成图像对，通过轻量级微调扩散模型实现与下游目标的对齐

Result: 在三个手术数据集上，分类任务提升7-9%，分割任务提升2-10%，尤其对代表性不足类别改善显著，迭代细化可进一步提升4-10%性能

Conclusion: SAADi方法克服了样本退化问题，确立了任务感知对齐作为缓解数据稀缺和推进手术视觉应用的关键原则

Abstract: The scarcity of annotated surgical data poses a significant challenge for
developing deep learning systems in computer-assisted interventions. While
diffusion models can synthesize realistic images, they often suffer from data
memorization, resulting in inconsistent or non-diverse samples that may fail to
improve, or even harm, downstream performance. We introduce \emph{Surgical
Application-Aligned Diffusion} (SAADi), a new framework that aligns diffusion
models with samples preferred by downstream models. Our method constructs pairs
of \emph{preferred} and \emph{non-preferred} synthetic images and employs
lightweight fine-tuning of diffusion models to align the image generation
process with downstream objectives explicitly. Experiments on three surgical
datasets demonstrate consistent gains of $7$--$9\%$ in classification and
$2$--$10\%$ in segmentation tasks, with the considerable improvements observed
for underrepresented classes. Iterative refinement of synthetic samples further
boosts performance by $4$--$10\%$. Unlike baseline approaches, our method
overcomes sample degradation and establishes task-aware alignment as a key
principle for mitigating data scarcity and advancing surgical vision
applications.

</details>


### [71] [A Kernel Space-based Multidimensional Sparse Model for Dynamic PET Image Denoising](https://arxiv.org/abs/2509.18801)
*Kuang Xiaodong,Li Bingxuan,Li Yuan,Rao Fan,Ma Gege,Xie Qingguo,Mok Greta S P,Liu Huafeng,Zhu Wentao*

Main category: cs.CV

TL;DR: 提出了一种基于模型神经网络的动态PET图像去噪方法，通过结合帧间空间相关性和帧内结构一致性，构建了核空间多维稀疏模型，并用神经网络替代参数估计，形成了端到端的KMDS-Net网络。


<details>
  <summary>Details</summary>
Motivation: 动态PET中由于时间帧统计量有限，特别是短时间帧，导致图像质量难以保证。深度学习在医学图像去噪领域已显示出广泛应用前景。

Method: 建立基于核空间的多维稀疏模型，利用动态PET的帧间空间相关性和帧内结构一致性，然后用神经网络替代传统参数估计方法，实现自适应参数优化。

Result: 模拟和真实数据的实验结果表明，KMDS-Net在动态PET去噪方面表现出强大的性能，优于之前的基线方法。

Conclusion: 该方法可以有效实现动态PET的高时间和空间分辨率，代码已开源。

Abstract: Achieving high image quality for temporal frames in dynamic positron emission
tomography (PET) is challenging due to the limited statistic especially for the
short frames. Recent studies have shown that deep learning (DL) is useful in a
wide range of medical image denoising tasks. In this paper, we propose a
model-based neural network for dynamic PET image denoising. The inter-frame
spatial correlation and intra-frame structural consistency in dynamic PET are
used to establish the kernel space-based multidimensional sparse (KMDS) model.
We then substitute the inherent forms of the parameter estimation with neural
networks to enable adaptive parameters optimization, forming the end-to-end
neural KMDS-Net. Extensive experimental results from simulated and real data
demonstrate that the neural KMDS-Net exhibits strong denoising performance for
dynamic PET, outperforming previous baseline methods. The proposed method may
be used to effectively achieve high temporal and spatial resolution for dynamic
PET. Our source code is available at
https://github.com/Kuangxd/Neural-KMDS-Net/tree/main.

</details>


### [72] [Surgical Video Understanding with Label Interpolation](https://arxiv.org/abs/2509.18802)
*Garam Kim,Tae Kyeong Jeong,Juyoun Park*

Main category: cs.CV

TL;DR: 提出了一种结合光流标签插值和多任务学习的新框架，用于解决机器人辅助手术中视觉数据理解不足的问题，特别是针对时空标注不平衡的挑战。


<details>
  <summary>Details</summary>
Motivation: 机器人辅助手术需要精确理解视觉数据，但现有方法多为单任务学习，无法处理复杂的手术场景动态。同时，像素级分割标注成本高，导致长短期标注存在时空不平衡问题。

Method: 使用光流估计从标注关键帧向相邻未标注帧传播标签，通过标签插值丰富稀疏的空间监督信息，结合多任务学习平衡时空信息进行训练。

Result: 该框架提高了手术场景理解的准确性和效率，增强了机器人辅助手术的实用性。

Conclusion: 提出的光流标签插值与多任务学习结合的方法有效解决了手术场景中的时空标注不平衡问题，为机器人辅助手术的视觉理解提供了更有效的解决方案。

Abstract: Robot-assisted surgery (RAS) has become a critical paradigm in modern
surgery, promoting patient recovery and reducing the burden on surgeons through
minimally invasive approaches. To fully realize its potential, however, a
precise understanding of the visual data generated during surgical procedures
is essential. Previous studies have predominantly focused on single-task
approaches, but real surgical scenes involve complex temporal dynamics and
diverse instrument interactions that limit comprehensive understanding.
Moreover, the effective application of multi-task learning (MTL) requires
sufficient pixel-level segmentation data, which are difficult to obtain due to
the high cost and expertise required for annotation. In particular, long-term
annotations such as phases and steps are available for every frame, whereas
short-term annotations such as surgical instrument segmentation and action
detection are provided only for key frames, resulting in a significant
temporal-spatial imbalance. To address these challenges, we propose a novel
framework that combines optical flow-based segmentation label interpolation
with multi-task learning. optical flow estimated from annotated key frames is
used to propagate labels to adjacent unlabeled frames, thereby enriching sparse
spatial supervision and balancing temporal and spatial information for
training. This integration improves both the accuracy and efficiency of
surgical scene understanding and, in turn, enhances the utility of RAS.

</details>


### [73] [Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation](https://arxiv.org/abs/2509.18824)
*Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao*

Main category: cs.CV

TL;DR: Hyper-Bagel是一个统一加速框架，通过推测解码和多阶段蒸馏技术，显著提升多模态理解和生成任务的效率，实现2倍以上的速度提升，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 随着多模态模型中交错令牌数量的增加，扩散去噪和自回归解码的迭代过程带来了显著的计算开销，需要高效的加速解决方案。

Method: 采用分治策略，使用推测解码进行下一个令牌预测，并通过多阶段蒸馏过程加速扩散去噪。结合对抗蒸馏和人类反馈学习开发高效模型。

Result: 多模态理解任务实现2倍以上加速；文本到图像生成实现16.67倍加速；图像编辑实现22倍加速；开发的高效1-NFE模型支持近实时交互式编辑和生成。

Conclusion: Hyper-Bagel框架通过先进的加速技术，在保持高质量输出的同时显著提升了多模态任务的效率，使复杂的多模态交互变得无缝和即时。

Abstract: Unified multimodal models have recently attracted considerable attention for
their remarkable abilities in jointly understanding and generating diverse
content. However, as contexts integrate increasingly numerous interleaved
multimodal tokens, the iterative processes of diffusion denoising and
autoregressive decoding impose significant computational overhead. To address
this, we propose Hyper-Bagel, a unified acceleration framework designed to
simultaneously speed up both multimodal understanding and generation tasks. Our
approach uses a divide-and-conquer strategy, employing speculative decoding for
next-token prediction and a multi-stage distillation process for diffusion
denoising. The framework delivers substantial performance gains, achieving over
a 2x speedup in multimodal understanding. For generative tasks, our resulting
lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a
22x speedup in image editing, all while preserving the high-quality output of
the original model. We further develop a highly efficient 1-NFE model that
enables near real-time interactive editing and generation. By combining
advanced adversarial distillation with human feedback learning, this model
achieves ultimate cost-effectiveness and responsiveness, making complex
multimodal interactions seamless and instantaneous.

</details>


### [74] [Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography](https://arxiv.org/abs/2509.18839)
*Gianmarco Spinaci,Lukas Klic,Giovanni Colavizza*

Main category: cs.CV

TL;DR: 评估多模态大语言模型在基督教圣像学单标签分类任务中的表现，发现GPT-4o和Gemini-2.5 Pro优于ResNet50基线，提示优化和上下文信息能提升性能，支持LLMs在文化遗产领域的应用。


<details>
  <summary>Details</summary>
Motivation: 评估通用多模态LLMs和VLMs在传统上由监督分类器处理的基督教圣像学分类任务中的能力，探索其在数字人文学科中作为元数据管理工具的潜力。

Method: 使用ArtDL、ICONCLASS和Wikidata三个数据集，对CLIP、SigLIP、GPT-4o和Gemini-2.5等模型在三种条件下进行基准测试：(1)仅类别标签分类，(2)带Iconclass描述分类，(3)五样本少样本学习。

Result: Gemini-2.5 Pro和GPT-4o优于ResNet50基线；在Wikidata数据集上准确率显著下降，Siglip表现最佳；添加类别描述提升零样本性能，少样本学习效果有限。

Conclusion: 通用多模态LLMs能够处理视觉复杂的文化遗产分类任务，支持其在数字人文学科工作流中作为元数据管理工具的应用，未来需研究提示优化和扩展分类策略。

Abstract: This study evaluates the capabilities of Multimodal Large Language Models
(LLMs) and Vision Language Models (VLMs) in the task of single-label
classification of Christian Iconography. The goal was to assess whether
general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5,
can interpret the Iconography, typically addressed by supervised classifiers,
and evaluate their performance. Two research questions guided the analysis:
(RQ1) How do multimodal LLMs perform on image classification of Christian
saints? And (RQ2), how does performance vary when enriching input with
contextual information or few-shot exemplars? We conducted a benchmarking study
using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and
Wikidata, filtered to include the top 10 most frequent classes. Models were
tested under three conditions: (1) classification using class labels, (2)
classification with Iconclass descriptions, and (3) few-shot learning with five
exemplars. Results were compared against ResNet50 baselines fine-tuned on the
same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed
the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset,
where Siglip reached the highest accuracy score, suggesting model sensitivity
to image size and metadata alignment. Enriching prompts with class descriptions
generally improved zero-shot performance, while few-shot learning produced
lower results, with only occasional and minimal increments in accuracy. We
conclude that general-purpose multimodal LLMs are capable of classification in
visually complex cultural heritage domains. These results support the
application of LLMs as metadata curation tools in digital humanities workflows,
suggesting future research on prompt optimization and the expansion of the
study to other classification strategies and models.

</details>


### [75] [ViG-LRGC: Vision Graph Neural Networks with Learnable Reparameterized Graph Construction](https://arxiv.org/abs/2509.18840)
*Ismael Elsharkawi,Hossam Sharara,Ahmed Rafea*

Main category: cs.CV

TL;DR: 提出了可学习重参数化图构建（LRGC）方法，用于视觉图神经网络，通过可学习的注意力机制和软阈值重参数化来自动构建图结构，无需超参数搜索。


<details>
  <summary>Details</summary>
Motivation: 传统ViG模型使用非参数化统计方法构建图结构，无法选择最佳邻域且依赖超参数搜索。需要一种可学习的、无需超参数的图构建方法。

Method: LRGC在每对节点间应用键-查询注意力，然后使用软阈值重参数化进行边选择，通过可学习参数自动调整阈值，实现端到端训练。

Result: 在ImageNet-1k基准数据集上，ViG-LRGC方法优于同等规模的state-of-the-art ViG模型。

Conclusion: LRGC提供了一种可学习的、无需超参数的图构建方法，能够自动适应训练数据，显著提升ViG模型性能。

Abstract: Image Representation Learning is an important problem in Computer Vision.
Traditionally, images were processed as grids, using Convolutional Neural
Networks or as a sequence of visual tokens, using Vision Transformers.
Recently, Vision Graph Neural Networks (ViG) have proposed the treatment of
images as a graph of nodes; which provides a more intuitive image
representation. The challenge is to construct a graph of nodes in each layer
that best represents the relations between nodes and does not need a
hyper-parameter search. ViG models in the literature depend on
non-parameterized and non-learnable statistical methods that operate on the
latent features of nodes to create a graph. This might not select the best
neighborhood for each node. Starting from k-NN graph construction to HyperGraph
Construction and Similarity-Thresholded graph construction, these methods lack
the ability to provide a learnable hyper-parameter-free graph construction
method. To overcome those challenges, we present the Learnable Reparameterized
Graph Construction (LRGC) for Vision Graph Neural Networks. LRGC applies
key-query attention between every pair of nodes; then uses soft-threshold
reparameterization for edge selection, which allows the use of a differentiable
mathematical model for training. Using learnable parameters to select the
neighborhood removes the bias that is induced by any clustering or thresholding
methods previously introduced in the literature. In addition, LRGC allows
tuning the threshold in each layer to the training data since the thresholds
are learnable through training and are not provided as hyper-parameters to the
model. We demonstrate that the proposed ViG-LRGC approach outperforms
state-of-the-art ViG models of similar sizes on the ImageNet-1k benchmark
dataset.

</details>


### [76] [Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions](https://arxiv.org/abs/2509.18847)
*Junhao Su,Yuanliang Wan,Junwei Yang,Hengyu Shi,Tianyang Han,Junfeng Luo,Yurui Qiu*

Main category: cs.CV

TL;DR: 提出结构化反思方法，将错误修复路径转化为明确可控的训练动作，通过优化反思-调用-最终的三步策略，显著提升多轮工具调用的成功率和错误恢复能力


<details>
  <summary>Details</summary>
Motivation: 当前工具增强大语言模型通常使用监督模仿或粗粒度强化学习训练，自我反思依赖启发式提示或单向推理，在多轮交互中脆弱且容易重复错误

Method: 结合DAPO和GSPO目标与工具使用定制奖励方案，优化Reflect-Call-Final三步策略，引入Tool-Reflection-Bench基准程序化检查结构有效性、可执行性、参数正确性和结果一致性

Result: 在BFCL v3和Tool-Reflection-Bench上的实验显示多轮工具调用成功率和错误恢复能力大幅提升，冗余调用减少

Conclusion: 使反思明确化并直接优化可提高工具交互的可靠性，为智能体从失败中学习提供可复现路径

Abstract: Tool-augmented large language models (LLMs) are usually trained with
supervised imitation or coarse-grained reinforcement learning that optimizes
single tool calls. Current self-reflection practices rely on heuristic prompts
or one-way reasoning: the model is urged to 'think more' instead of learning
error diagnosis and repair. This is fragile in multi-turn interactions; after a
failure the model often repeats the same mistake. We propose structured
reflection, which turns the path from error to repair into an explicit,
controllable, and trainable action. The agent produces a short yet precise
reflection: it diagnoses the failure using evidence from the previous step and
then proposes a correct, executable follow-up call. For training we combine
DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing
the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce
Tool-Reflection-Bench, a lightweight benchmark that programmatically checks
structural validity, executability, parameter correctness, and result
consistency. Tasks are built as mini trajectories of erroneous call,
reflection, and corrected call, with disjoint train and test splits.
Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn
tool-call success and error recovery, and a reduction of redundant calls. These
results indicate that making reflection explicit and optimizing it directly
improves the reliability of tool interaction and offers a reproducible path for
agents to learn from failure.

</details>


### [77] [Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model](https://arxiv.org/abs/2509.18891)
*Xueyu Liu,Xiaoyi Zhang,Guangze Shi,Meilin Liu,Yexin Lai,Yongfei Wu,Mingqiang Wei*

Main category: cs.CV

TL;DR: 提出了Point Prompt Defender框架，通过对抗性强化学习自动优化SAM的点提示，采用攻击-防御范式提升分割性能


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖启发式或手动设计的提示，限制了可扩展性和泛化能力，需要自动化的提示优化方案

Method: 构建双空间图表示图像块，使用DQN训练攻击者（最大化破坏分割）和防御者（恢复分割精度）的对抗智能体

Result: 实验表明该方法有效提升SAM的鲁棒性和泛化能力，建立灵活可解释的即插即用框架

Conclusion: Point Prompt Defender为基于提示的分割提供了有效的自动化优化方案，无需重新训练即可提升性能

Abstract: Prompt quality plays a critical role in the performance of the Segment
Anything Model (SAM), yet existing approaches often rely on heuristic or
manually crafted prompts, limiting scalability and generalization. In this
paper, we propose Point Prompt Defender, an adversarial reinforcement learning
framework that adopts an attack-for-defense paradigm to automatically optimize
point prompts. We construct a task-agnostic point prompt environment by
representing image patches as nodes in a dual-space graph, where edges encode
both physical and semantic distances. Within this environment, an attacker
agent learns to activate a subset of prompts that maximally degrade SAM's
segmentation performance, while a defender agent learns to suppress these
disruptive prompts and restore accuracy. Both agents are trained using Deep
Q-Networks with a reward signal based on segmentation quality variation. During
inference, only the defender is deployed to refine arbitrary coarse prompt
sets, enabling enhanced SAM segmentation performance across diverse tasks
without retraining. Extensive experiments show that Point Prompt Defender
effectively improves SAM's robustness and generalization, establishing a
flexible, interpretable, and plug-and-play framework for prompt-based
segmentation.

</details>


### [78] [SmartWilds: Multimodal Wildlife Monitoring Dataset](https://arxiv.org/abs/2509.18894)
*Jenna Kline,Anirudh Potlapally,Bharath Pillai,Tanishka Wani,Rugved Katole,Vedant Patil,Penelope Covey,Hari Subramoni,Tanya Berger-Wolf,Christopher Stewart*

Main category: cs.CV

TL;DR: SmartWilds是首个多模态野生动物监测数据集，包含无人机图像、相机陷阱照片视频和生物声学记录，用于支持多模态AI研究的环境监测。


<details>
  <summary>Details</summary>
Motivation: 解决濒危物种研究、保护生态学和栖息地管理中多模态AI研究的迫切需求，建立可复制的多模态野生动物监测协议。

Method: 在俄亥俄州野生动物园220英亩牧场进行四天同步监测，收集三种模态数据，包括无人机图像、相机陷阱和生物声学记录。

Result: 提供了传感器模态性能的比较分析，展示了在土地利用模式、物种检测、行为分析和栖息地监测方面的互补优势。

Conclusion: 这项工作为多模态野生动物监测建立了可复制的协议，并为保护计算机视觉研究贡献了开放数据集，未来将扩展包括GPS追踪数据和更长时间覆盖。

Abstract: We present the first release of SmartWilds, a multimodal wildlife monitoring
dataset. SmartWilds is a synchronized collection of drone imagery, camera trap
photographs and videos, and bioacoustic recordings collected during summer 2025
at The Wilds safari park in Ohio. This dataset supports multimodal AI research
for comprehensive environmental monitoring, addressing critical needs in
endangered species research, conservation ecology, and habitat management. Our
pilot deployment captured four days of synchronized monitoring across three
modalities in a 220-acre pasture containing Pere David's deer, Sichuan takin,
Przewalski's horses, as well as species native to Ohio, including bald eagles,
white-tailed deer, and coyotes. We provide a comparative analysis of sensor
modality performance, demonstrating complementary strengths for landuse
patterns, species detection, behavioral analysis, and habitat monitoring. This
work establishes reproducible protocols for multimodal wildlife monitoring
while contributing open datasets to advance conservation computer vision
research. Future releases will include synchronized GPS tracking data from
tagged individuals, citizen science data, and expanded temporal coverage across
multiple seasons.

</details>


### [79] [RS3DBench: A Comprehensive Benchmark for 3D Spatial Perception in Remote Sensing](https://arxiv.org/abs/2509.18897)
*Jiayu Wang,Ruizhi Wang,Jie Song,Haofei Zhang,Mingli Song,Zunlei Feng,Li Sun*

Main category: cs.CV

TL;DR: 提出了一个名为RS3DBench的新型基准数据集，用于推动遥感图像的大规模3D视觉模型发展。该数据集包含54,951对遥感图像和像素级对齐的深度图，并配有文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有遥感数据集缺乏全面的深度信息或深度数据与遥感图像之间的精确对齐，这限制了3D视觉模型在遥感领域的发展。

Method: 构建RS3DBench数据集，包含大量配对的遥感图像和深度图；基于稳定扩散模型开发遥感深度估计模型，利用其多模态融合能力。

Result: 提出的深度估计模型在RS3DBench数据集上实现了最先进的性能。

Conclusion: RS3DBench数据集和相应模型将推动遥感领域3D视觉感知模型和地理人工智能的发展，相关资源将在https://rs3dbench.github.io上公开。

Abstract: In this paper, we introduce a novel benchmark designed to propel the
advancement of general-purpose, large-scale 3D vision models for remote sensing
imagery. While several datasets have been proposed within the realm of remote
sensing, many existing collections either lack comprehensive depth information
or fail to establish precise alignment between depth data and remote sensing
images. To address this deficiency, we present a visual Benchmark for 3D
understanding of Remotely Sensed images, dubbed RS3DBench. This dataset
encompasses 54,951 pairs of remote sensing images and pixel-level aligned depth
maps, accompanied by corresponding textual descriptions, spanning a broad array
of geographical contexts. It serves as a tool for training and assessing 3D
visual perception models within remote sensing image spatial understanding
tasks. Furthermore, we introduce a remotely sensed depth estimation model
derived from stable diffusion, harnessing its multimodal fusion capabilities,
thereby delivering state-of-the-art performance on our dataset. Our endeavor
seeks to make a profound contribution to the evolution of 3D visual perception
models and the advancement of geographic artificial intelligence within the
remote sensing domain. The dataset, models and code will be accessed on the
https://rs3dbench.github.io.

</details>


### [80] [DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring](https://arxiv.org/abs/2509.18898)
*Pengteng Li,Yunfan Lu,Pinhao Song,Weiyu Guo,Huizai Yao,F. Richard Yu,Hui Xiong*

Main category: cs.CV

TL;DR: 提出了首个无需SfM的基于事件相机的去模糊3D高斯泼溅方法DeblurSplat，通过利用预训练的密集立体模块直接获取初始点云，并引入事件流进行精细监督，实现了高效高质量的去模糊3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决传统基于SfM的方法在运动模糊场景中由于相机姿态估计不准确导致的累积误差问题，以及需要更有效的监督信号来优化场景重建。

Method: 1. 使用预训练的DUSt3R密集立体模块直接从模糊图像获取准确初始点云，避免相机姿态估计的中间步骤；2. 引入事件流，通过解码事件流和模糊图像中的潜在清晰图像，为场景重建优化提供细粒度监督信号。

Result: 在多种场景下的实验表明，DeblurSplat不仅能生成高保真度的新视角图像，而且在去模糊3D-GS任务中相比现有技术实现了显著的渲染效率提升。

Conclusion: DeblurSplat方法通过绕过SfM流程和利用事件流特性，有效解决了运动模糊下的3D重建问题，在质量和效率方面均优于现有方法。

Abstract: In this paper, we propose the first Structure-from-Motion (SfM)-free
deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat.
We address the motion-deblurring problem in two ways. First, we leverage the
pretrained capability of the dense stereo module (DUSt3R) to directly obtain
accurate initial point clouds from blurred images. Without calculating camera
poses as an intermediate result, we avoid the cumulative errors transfer from
inaccurate camera poses to the initial point clouds' positions. Second, we
introduce the event stream into the deblur pipeline for its high sensitivity to
dynamic change. By decoding the latent sharp images from the event stream and
blurred images, we can provide a fine-grained supervision signal for scene
reconstruction optimization. Extensive experiments across a range of scenes
demonstrate that DeblurSplat not only excels in generating high-fidelity novel
views but also achieves significant rendering efficiency compared to the SOTAs
in deblur 3D-GS.

</details>


### [81] [MoiréNet: A Compact Dual-Domain Network for Image Demoiréing](https://arxiv.org/abs/2509.18910)
*Shuwei Guo,Simin Luan,Yan Ke,Zeyd Boukhers,John See,Cong Yang*

Main category: cs.CV

TL;DR: MoiréNet是一个基于U-Net的卷积神经网络框架，通过融合频域和空域特征来有效去除数字图像中的摩尔纹伪影，具有参数效率高的特点。


<details>
  <summary>Details</summary>
Motivation: 摩尔纹是由显示像素网格和相机传感器网格之间的频谱混叠引起的各向异性、多尺度伪影，对数字图像去摩尔纹处理构成重大挑战。

Method: 提出MoiréNet框架，包含两个关键组件：方向频率-空间编码器（DFSE）通过方向差分卷积识别摩尔纹方向，以及频率-空间自适应选择器（FSAS）实现精确的特征自适应抑制。

Result: 在公共和实际使用数据集上的广泛实验表明，MoiréNet实现了最先进的性能，仅需5.513M参数，比ESDNet-L减少48%参数。

Conclusion: MoiréNet将卓越的恢复质量与参数效率相结合，非常适合资源受限的应用场景，如智能手机摄影、工业成像和增强现实。

Abstract: Moir\'e patterns arise from spectral aliasing between display pixel lattices
and camera sensor grids, manifesting as anisotropic, multi-scale artifacts that
pose significant challenges for digital image demoir\'eing. We propose
Moir\'eNet, a convolutional neural U-Net-based framework that synergistically
integrates frequency and spatial domain features for effective artifact
removal. Moir\'eNet introduces two key components: a Directional
Frequency-Spatial Encoder (DFSE) that discerns moir\'e orientation via
directional difference convolution, and a Frequency-Spatial Adaptive Selector
(FSAS) that enables precise, feature-adaptive suppression. Extensive
experiments demonstrate that Moir\'eNet achieves state-of-the-art performance
on public and actively used datasets while being highly parameter-efficient.
With only 5.513M parameters, representing a 48% reduction compared to ESDNet-L,
Moir\'eNet combines superior restoration quality with parameter efficiency,
making it well-suited for resource-constrained applications including
smartphone photography, industrial imaging, and augmented reality.

</details>


### [82] [Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation](https://arxiv.org/abs/2509.18912)
*Yunzhe Shen,Kai Peng,Leiye Liu,Wei Ji,Jingjing Li,Miao Zhang,Yongri Piao,Huchuan Lu*

Main category: cs.CV

TL;DR: 提出FAVS框架，通过频率域分解和重组解决音频-视觉分割中的模态差异问题，在三个基准数据集上达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有AVS方法忽略了音频和视觉模态在频率域上的固有矛盾——音频高频信号中的干扰噪声与视觉高频信号中的丰富结构细节，导致性能不佳

Method: 提出频率感知的音频-视觉分割框架FAVS，包含频率域增强分解器模块和协同跨模态一致性模块，分别负责模态特征分解和语义一致性保持

Result: 在三个基准数据集上实现最先进性能，定性可视化验证了FDED和SCMC模块的有效性

Conclusion: 将AVS任务重新定义为频率域分解和重组问题，提出的FAVS框架能够有效处理模态间频率差异，提升分割性能

Abstract: Audio-visual segmentation (AVS) plays a critical role in multimodal machine
learning by effectively integrating audio and visual cues to precisely segment
objects or regions within visual scenes. Recent AVS methods have demonstrated
significant improvements. However, they overlook the inherent frequency-domain
contradictions between audio and visual modalities--the pervasively interfering
noise in audio high-frequency signals vs. the structurally rich details in
visual high-frequency signals. Ignoring these differences can result in
suboptimal performance. In this paper, we rethink the AVS task from a deeper
perspective by reformulating AVS task as a frequency-domain decomposition and
recomposition problem. To this end, we introduce a novel Frequency-Aware
Audio-Visual Segmentation (FAVS) framework consisting of two key modules:
Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal
Consistency (SCMC) module. FDED module employs a residual-based iterative
frequency decomposition to discriminate modality-specific semantics and
structural features, and SCMC module leverages a mixture-of-experts
architecture to reinforce semantic consistency and modality-specific feature
preservation through dynamic expert routing. Extensive experiments demonstrate
that our FAVS framework achieves state-of-the-art performance on three
benchmark datasets, and abundant qualitative visualizations further verify the
effectiveness of the proposed FDED and SCMC modules. The code will be released
as open source upon acceptance of the paper.

</details>


### [83] [xAI-CV: An Overview of Explainable Artificial Intelligence in Computer Vision](https://arxiv.org/abs/2509.18913)
*Nguyen Van Tu,Pham Nguyen Hai Long,Vo Hoai Viet*

Main category: cs.CV

TL;DR: 这篇论文综述了可解释人工智能（xAI）在视觉感知任务中的四种代表性方法：显著性图、概念瓶颈模型、基于原型的方法和混合方法，分析了它们的机制、优缺点及评估指标，为未来研究提供指导。


<details>
  <summary>Details</summary>
Motivation: 深度学习在图像分析任务中表现出色，但其"黑盒"特性使得决策过程难以解释，这在关键应用中引发可靠性担忧。为了让人工智能模型的决策过程对人类可理解，可解释AI领域应运而生。

Method: 本文采用综述研究方法，系统性地分析和比较了四种xAI方法：Saliency Maps、Concept Bottleneck Models (CBM)、Prototype-based methods和Hybrid approaches。

Result: 论文提供了对四种xAI方法的全面分析，包括它们的底层机制、优势、局限性以及评估指标，为研究者和实践者提供了清晰的指导框架。

Conclusion: xAI对于提高深度学习模型在关键应用中的可靠性和可信度至关重要。本文的综述为未来可解释AI研究提供了有价值的参考，并强调了在视觉感知任务中实现模型透明性的重要性。

Abstract: Deep learning has become the de facto standard and dominant paradigm in image
analysis tasks, achieving state-of-the-art performance. However, this approach
often results in "black-box" models, whose decision-making processes are
difficult to interpret, raising concerns about reliability in critical
applications. To address this challenge and provide human a method to
understand how AI model process and make decision, the field of xAI has
emerged. This paper surveys four representative approaches in xAI for visual
perception tasks: (i) Saliency Maps, (ii) Concept Bottleneck Models (CBM),
(iii) Prototype-based methods, and (iv) Hybrid approaches. We analyze their
underlying mechanisms, strengths and limitations, as well as evaluation
metrics, thereby providing a comprehensive overview to guide future research
and applications.

</details>


### [84] [LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2509.18917)
*Amirhesam Aghanouri,Cristina Olaverri-Monreal*

Main category: cs.CV

TL;DR: 本文提出了一种基于DDPM的增强方法，通过改进噪声调度和时间步嵌入技术来生成高质量合成LiDAR数据，用于自动驾驶视觉系统的数据增强。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖LiDAR进行3D环境感知，但真实LiDAR数据采集耗时且易受噪声和稀疏性影响。需要生成高质量合成数据来提升感知性能。

Method: 使用去噪扩散概率模型（DDPM），结合新颖的噪声调度和时间步嵌入技术，生成逼真的点云数据。

Result: 在IAMCV和KITTI-360数据集上的评估显示，该方法在多种性能指标上优于现有基线方法，能有效缓解噪声和稀疏数据问题。

Conclusion: 所提出的方法能够生成具有丰富空间关系和结构细节的多样化点云，显著提升自动驾驶感知系统的性能。

Abstract: Autonomous vehicles (AVs) are expected to revolutionize transportation by
improving efficiency and safety. Their success relies on 3D vision systems that
effectively sense the environment and detect traffic agents. Among sensors AVs
use to create a comprehensive view of surroundings, LiDAR provides
high-resolution depth data enabling accurate object detection, safe navigation,
and collision avoidance. However, collecting real-world LiDAR data is
time-consuming and often affected by noise and sparsity due to adverse weather
or sensor limitations. This work applies a denoising diffusion probabilistic
model (DDPM), enhanced with novel noise scheduling and time-step embedding
techniques to generate high-quality synthetic data for augmentation, thereby
improving performance across a range of computer vision tasks, particularly in
AV perception. These modifications impact the denoising process and the model's
temporal awareness, allowing it to produce more realistic point clouds based on
the projection. The proposed method was extensively evaluated under various
configurations using the IAMCV and KITTI-360 datasets, with four performance
metrics compared against state-of-the-art (SOTA) methods. The results
demonstrate the model's superior performance over most existing baselines and
its effectiveness in mitigating the effects of noisy and sparse LiDAR data,
producing diverse point clouds with rich spatial relationships and structural
detail.

</details>


### [85] [Advancing Metallic Surface Defect Detection via Anomaly-Guided Pretraining on a Large Industrial Dataset](https://arxiv.org/abs/2509.18919)
*Chuni Liu,Hongjie Li,Jiaqi Du,Yangyang Hou,Qian Sun,Lei Jin,Ke Xu*

Main category: cs.CV

TL;DR: AGSSP是一种新的预训练范式，通过异常先验指导表示学习，解决金属表面缺陷检测中预训练微调范式的困境。


<details>
  <summary>Details</summary>
Motivation: 传统预训练方法面临域差距问题（如ImageNet），而简单的自监督预训练在工业数据上效果不佳，无法区分细微缺陷模式和复杂背景噪声。

Method: 采用两阶段框架：1）通过从异常图中蒸馏知识预训练骨干网络，捕获缺陷显著特征；2）使用伪缺陷框预训练检测器，与定位任务对齐。开发了知识增强方法生成高质量异常图，并收集了12万张图像的大规模工业数据集。

Result: 实验表明AGSSP在各种设置下持续提升性能，相比基于ImageNet的模型，mAP@0.5提升达10%，mAP@0.5:0.95提升11.4%。

Conclusion: AGSSP有效解决了金属表面缺陷检测中的预训练困境，通过异常引导的自监督预训练显著提升了检测性能。

Abstract: The pretraining-finetuning paradigm is a crucial strategy in metallic surface
defect detection for mitigating the challenges posed by data scarcity. However,
its implementation presents a critical dilemma. Pretraining on natural image
datasets such as ImageNet, faces a significant domain gap. Meanwhile, naive
self-supervised pretraining on in-domain industrial data is often ineffective
due to the inability of existing learning objectives to distinguish subtle
defect patterns from complex background noise and textures. To resolve this, we
introduce Anomaly-Guided Self-Supervised Pretraining (AGSSP), a novel paradigm
that explicitly guides representation learning through anomaly priors. AGSSP
employs a two-stage framework: (1) it first pretrains the model's backbone by
distilling knowledge from anomaly maps, encouraging the network to capture
defect-salient features; (2) it then pretrains the detector using pseudo-defect
boxes derived from these maps, aligning it with localization tasks. To enable
this, we develop a knowledge-enhanced method to generate high-quality anomaly
maps and collect a large-scale industrial dataset of 120,000 images.
Additionally, we present two small-scale, pixel-level labeled metallic surface
defect datasets for validation. Extensive experiments demonstrate that AGSSP
consistently enhances performance across various settings, achieving up to a
10\% improvement in mAP@0.5 and 11.4\% in mAP@0.5:0.95 compared to
ImageNet-based models. All code, pretrained models, and datasets are publicly
available at https://clovermini.github.io/AGSSP-Dev/.

</details>


### [86] [Audio-Driven Universal Gaussian Head Avatars](https://arxiv.org/abs/2509.18924)
*Kartik Teotia,Helge Rhodin,Mohit Mendiratta,Hyeongwoo Kim,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出首个音频驱动的通用逼真头像合成方法，结合人无关的语音模型和新的通用头部头像先验（UHAP），能够同时处理几何和外观变化，在唇同步准确性和感知真实性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要将音频特征映射到几何变形，而忽略了音频相关的外观变化，无法实现高质量的逼真头像合成。需要开发能够同时处理几何和外观变化的通用音频驱动头像模型。

Method: 1. 训练跨身份多视角视频的通用头部头像先验（UHAP）；2. 使用人无关的语音模型将原始音频输入映射到UHAP潜在表达空间；3. 采用单目编码器进行高效个性化；4. 通过UHAP解码音频驱动的表达代码生成逼真头像。

Result: 该方法不仅首个实现可泛化的音频驱动头像模型，能够进行详细的外观建模和渲染，而且在唇同步准确性、定量图像质量和感知真实性方面优于竞争方法。

Conclusion: 提出的UHAP框架成功实现了高质量的音频驱动头像合成，能够同时处理几何和外观变化，生成具有精确唇同步和细腻表情细节的逼真头像。

Abstract: We introduce the first method for audio-driven universal photorealistic
avatar synthesis, combining a person-agnostic speech model with our novel
Universal Head Avatar Prior (UHAP). UHAP is trained on cross-identity
multi-view videos. In particular, our UHAP is supervised with neutral scan
data, enabling it to capture the identity-specific details at high fidelity. In
contrast to previous approaches, which predominantly map audio features to
geometric deformations only while ignoring audio-dependent appearance
variations, our universal speech model directly maps raw audio inputs into the
UHAP latent expression space. This expression space inherently encodes, both,
geometric and appearance variations. For efficient personalization to new
subjects, we employ a monocular encoder, which enables lightweight regression
of dynamic expression variations across video frames. By accounting for these
expression-dependent changes, it enables the subsequent model fine-tuning stage
to focus exclusively on capturing the subject's global appearance and geometry.
Decoding these audio-driven expression codes via UHAP generates highly
realistic avatars with precise lip synchronization and nuanced expressive
details, such as eyebrow movement, gaze shifts, and realistic mouth interior
appearance as well as motion. Extensive evaluations demonstrate that our method
is not only the first generalizable audio-driven avatar model that can account
for detailed appearance modeling and rendering, but it also outperforms
competing (geometry-only) methods across metrics measuring lip-sync accuracy,
quantitative image quality, and perceptual realism.

</details>


### [87] [SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines](https://arxiv.org/abs/2509.18926)
*Pamela Osuna-Vargas,Altug Kamacioglu,Dominik F. Aschauer,Petros E. Vlachos,Sercan Alipek,Jochen Triesch,Simon Rumpel,Matthias Kaschube*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的模块化管道，用于自动化检测、跟踪和提取3D+时间显微镜数据中树突棘的特征，解决了大规模分析树突棘结构动态的挑战。


<details>
  <summary>Details</summary>
Motivation: 树突棘是大脑中兴奋性突触的关键结构组成部分，其大小可作为突触效能的代理。然而，大规模分析3D+时间显微镜数据中树突棘的结构动态仍然具有挑战性且劳动密集。

Method: 采用模块化机器学习管道，结合基于transformer的检测模块、集成空间特征的深度跟踪组件、利用空间一致性的时间跟踪模块以及量化生物学相关脊柱特性的特征提取单元。

Result: 在开源标记脊柱数据和两个新发布的注释数据集上验证了该方法，建立了可扩展的端到端树突棘动态分析基准。

Conclusion: 该方法为树突棘动态的大规模分析提供了自动化解决方案，并发布了数据、代码和预训练权重以促进未来研究。

Abstract: Dendritic spines are key structural components of excitatory synapses in the
brain. Given the size of dendritic spines provides a proxy for synaptic
efficacy, their detection and tracking across time is important for studies of
the neural basis of learning and memory. Despite their relevance, large-scale
analyses of the structural dynamics of dendritic spines in 3D+time microscopy
data remain challenging and labor-intense. Here, we present a modular machine
learning-based pipeline designed to automate the detection, time-tracking, and
feature extraction of dendritic spines in volumes chronically recorded with
two-photon microscopy. Our approach tackles the challenges posed by biological
data by combining a transformer-based detection module, a depth-tracking
component that integrates spatial features, a time-tracking module to associate
3D spines across time by leveraging spatial consistency, and a feature
extraction unit that quantifies biologically relevant spine properties. We
validate our method on open-source labeled spine data, and on two complementary
annotated datasets that we publish alongside this work: one for detection and
depth-tracking, and one for time-tracking, which, to the best of our knowledge,
is the first data of this kind. To encourage future research, we release our
data, code, and pre-trained weights at
https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable,
end-to-end analysis of dendritic spine dynamics.

</details>


### [88] [No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning](https://arxiv.org/abs/2509.18938)
*Matheus Vinícius Todescato,Joel Luís Carbonera*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型和预训练视觉模型的自学习零样本图像分类框架，无需标注数据，仅需类别名称即可实现动态适应分类。


<details>
  <summary>Details</summary>
Motivation: 深度学习通常依赖大量标注数据，但在实际场景中标注数据稀缺。视觉语言模型和预训练视觉模型为解决这一问题提供了可能。

Method: 使用基于置信度的伪标签策略，在测试数据上直接训练轻量级分类器。VLM识别高置信度样本，预训练视觉模型增强其特征表示，通过迭代训练捕获互补的语义和视觉线索。

Result: 在十个不同数据集上的实验表明，该方法优于基线零样本方法。

Conclusion: 该方法无需VLM微调和大语言模型，仅依赖视觉模型减少对语义表示的依赖，实现了有效的零样本分类。

Abstract: While deep learning, including Convolutional Neural Networks (CNNs) and
Vision Transformers (ViTs), has significantly advanced classification
performance, its typical reliance on extensive annotated datasets presents a
major obstacle in many practical scenarios where such data is scarce.
Vision-language models (VLMs) and transfer learning with pre-trained visual
models appear as promising techniques to deal with this problem. This paper
proposes a novel zero-shot image classification framework that combines a VLM
and a pre-trained visual model within a self-learning cycle. Requiring only the
set of class names and no labeled training data, our method utilizes a
confidence-based pseudo-labeling strategy to train a lightweight classifier
directly on the test data, enabling dynamic adaptation. The VLM identifies
high-confidence samples, and the pre-trained visual model enhances their visual
representations. These enhanced features then iteratively train the classifier,
allowing the system to capture complementary semantic and visual cues without
supervision. Notably, our approach avoids VLM fine-tuning and the use of large
language models, relying on the visual-only model to reduce the dependence on
semantic representation. Experimental evaluations on ten diverse datasets
demonstrate that our approach outperforms the baseline zero-shot method.

</details>


### [89] [Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting](https://arxiv.org/abs/2509.18956)
*Zijing Guo,Yunyang Zhao,Lin Wang*

Main category: cs.CV

TL;DR: 本文提出了MirrorScene3D数据集和ReflectiveGS方法，用于解决含镜面环境中3D重建和新视角合成的挑战，通过利用镜面反射作为补充视角来提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 含镜面环境对3D重建和新视角合成带来独特挑战，现有方法如NeRF和3DGS在镜面存在时性能下降，且现有解决方案主要关注镜面处理而忽略了镜面反射所携带的丰富信息。

Method: 提出ReflectiveGS方法，基于3D高斯泼溅技术扩展，将镜面反射视为补充视角而非简单对称伪影，利用反射信息增强场景几何和恢复缺失细节。

Result: 在MirrorScene3D数据集上的实验表明，ReflectiveGS在SSIM、PSNR、LPIPS指标和训练速度上均优于现有方法。

Conclusion: ReflectiveGS为含镜面环境的3D重建设立了新的基准，证明了利用镜面反射作为补充视角的有效性。

Abstract: Mirror-containing environments pose unique challenges for 3D reconstruction
and novel view synthesis (NVS), as reflective surfaces introduce view-dependent
distortions and inconsistencies. While cutting-edge methods such as Neural
Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical
scenes, their performance deteriorates in the presence of mirrors. Existing
solutions mainly focus on handling mirror surfaces through symmetry mapping but
often overlook the rich information carried by mirror reflections. These
reflections offer complementary perspectives that can fill in absent details
and significantly enhance reconstruction quality. To advance 3D reconstruction
in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset
featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror
masks, providing a benchmark for evaluating reconstruction methods in
reflective settings. Building on this, we propose ReflectiveGS, an extension of
3D Gaussian Splatting that utilizes mirror reflections as complementary
viewpoints rather than simple symmetry artifacts, enhancing scene geometry and
recovering absent details. Experiments on MirrorScene3D show that
ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and
training speed, setting a new benchmark for 3D reconstruction in mirror-rich
environments.

</details>


### [90] [Generative data augmentation for biliary tract detection on intraoperative images](https://arxiv.org/abs/2509.18958)
*Cristina Iacono,Mariarosaria Meola,Federica Conte,Laura Mecozzi,Umberto Bracale,Pietro Falco,Fanny Ficuciello*

Main category: cs.CV

TL;DR: 使用深度学习方法来定位胆囊切除术中的胆道结构，以降低胆管损伤风险。通过构建图像数据库训练Yolo检测算法，并采用GAN生成合成训练数据。


<details>
  <summary>Details</summary>
Motivation: 腹腔镜胆囊切除术虽然恢复快、美容效果好，但存在较高的胆管损伤风险，严重影响患者生活质量和生存率。需要改进术中胆道可视化来避免损伤。

Method: 构建并标注图像数据库训练Yolo检测算法，使用经典数据增强技术和GAN生成合成训练数据。

Result: 实验结果表明该方法能够有效定位胆道结构，讨论了相关伦理考量。

Conclusion: 深度学习技术可用于提高胆囊切除术中胆道结构的可视化，有助于降低胆管损伤风险。

Abstract: Cholecystectomy is one of the most frequently performed procedures in
gastrointestinal surgery, and the laparoscopic approach is the gold standard
for symptomatic cholecystolithiasis and acute cholecystitis. In addition to the
advantages of a significantly faster recovery and better cosmetic results, the
laparoscopic approach bears a higher risk of bile duct injury, which has a
significant impact on quality of life and survival. To avoid bile duct injury,
it is essential to improve the intraoperative visualization of the bile duct.
This work aims to address this problem by leveraging a deep-learning approach
for the localization of the biliary tract from white-light images acquired
during the surgical procedures. To this end, the construction and annotation of
an image database to train the Yolo detection algorithm has been employed.
Besides classical data augmentation techniques, the paper proposes Generative
Adversarial Network (GAN) for the generation of a synthetic portion of the
training dataset. Experimental results have been discussed along with ethical
considerations.

</details>


### [91] [Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images](https://arxiv.org/abs/2509.18973)
*Jiabao Chen,Shan Xiong,Jialin Peng*

Main category: cs.CV

TL;DR: 提出Prompt-DAS框架，通过可提示的多任务学习实现电子显微镜图像中细胞器实例的自适应分割，支持无监督、弱监督和交互式分割。


<details>
  <summary>Details</summary>
Motivation: 解决大规模电子显微镜图像中细胞器实例分割需要大量标注的问题，利用SAM的提示机制实现标注高效的学习。

Method: 基于SAM构建可提示多任务框架，结合辅助中心点检测任务和提示引导对比学习，支持多种提示配置（全点、稀疏点、无点）。

Result: 在多个挑战性基准测试中优于现有的UDA、WDA和基于SAM的方法。

Conclusion: Prompt-DAS是一个灵活有效的框架，能够通过不同的提示配置实现多种自适应分割任务，显著提升分割性能。

Abstract: Domain adaptive segmentation (DAS) of numerous organelle instances from
large-scale electron microscopy (EM) is a promising way to enable
annotation-efficient learning. Inspired by SAM, we propose a promptable
multitask framework, namely Prompt-DAS, which is flexible enough to utilize any
number of point prompts during the adaptation training stage and testing stage.
Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised
domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well
as interactive segmentation during testing. Unlike the foundation model SAM,
which necessitates a prompt for each individual object instance, Prompt-DAS is
only trained on a small dataset and can utilize full points on all instances,
sparse points on partial instances, or even no points at all, facilitated by
the incorporation of an auxiliary center-point detection task. Moreover, a
novel prompt-guided contrastive learning is proposed to enhance discriminative
feature learning. Comprehensive experiments conducted on challenging benchmarks
demonstrate the effectiveness of the proposed approach over existing UDA, WDA,
and SAM-based approaches.

</details>


### [92] [VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction](https://arxiv.org/abs/2509.19002)
*Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara*

Main category: cs.CV

TL;DR: 提出了VIR-Bench基准测试，用于评估多模态大语言模型在长距离旅行视频理解方面的能力，发现现有模型表现不佳，并开发了改进的旅行规划代理。


<details>
  <summary>Details</summary>
Motivation: 当前视频基准测试主要关注室内场景或短距离户外活动，缺乏对长距离旅行视频理解能力的评估，而掌握扩展的地理时空轨迹对于下一代MLLMs至关重要。

Method: 构建包含200个旅行视频的VIR-Bench基准，将行程重建作为核心任务，并开发原型旅行规划代理来验证基准的有效性。

Result: 实验表明，包括专有模型在内的最先进MLLMs在VIR-Bench上得分较低，证明处理长时空跨度视频具有挑战性。开发的旅行规划代理在行程推荐方面有明显改进。

Conclusion: VIR-Bench不仅能有效评估模型性能，还能转化为实际应用中的性能提升，为下一代MLLMs的发展提供了重要基准。

Abstract: Recent advances in multimodal large language models (MLLMs) have
significantly enhanced video understanding capabilities, opening new
possibilities for practical applications. Yet current video benchmarks focus
largely on indoor scenes or short-range outdoor activities, leaving the
challenges associated with long-distance travel largely unexplored. Mastering
extended geospatial-temporal trajectories is critical for next-generation
MLLMs, underpinning real-world tasks such as embodied-AI planning and
navigation. To bridge this gap, we present VIR-Bench, a novel benchmark
consisting of 200 travel videos that frames itinerary reconstruction as a
challenging task designed to evaluate and push forward MLLMs'
geospatial-temporal intelligence. Experimental results reveal that
state-of-the-art MLLMs, including proprietary ones, struggle to achieve high
scores, underscoring the difficulty of handling videos that span extended
spatial and temporal scales. Moreover, we conduct an in-depth case study in
which we develop a prototype travel-planning agent that leverages the insights
gained from VIR-Bench. The agent's markedly improved itinerary recommendations
verify that our evaluation protocol not only benchmarks models effectively but
also translates into concrete performance gains in user-facing applications.

</details>


### [93] [Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards](https://arxiv.org/abs/2509.19003)
*Honghao Chen,Xingzhou Lou,Xiaokun Feng,Kaiqi Huang,Xinlong Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对视觉语言模型的细粒度推理框架，通过步骤级推理数据、过程奖励模型和强化学习训练，实现了对中间推理质量的准确评估和有效强化。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理在视觉语言推理中面临挑战，粗粒度的推理链难以进行细粒度结构化推理，且难以评估中间推理的质量和奖励。

Method: 提出了一个简单有效的透明框架，包括步骤级推理数据、过程奖励模型（PRM）和强化学习训练，实现对推理步骤质量的准确评估。

Result: 在具有挑战性的视觉语言基准测试中取得了持续改进，建立了强基线，并通过实证分析揭示了各组件的影响和推理时缩放的特性。

Conclusion: 该工作为视觉语言模型提供了基准，并为更复杂的多模态推理提供了见解，相关数据集、PRM和代码将开源。

Abstract: Chain of thought reasoning has demonstrated remarkable success in large
language models, yet its adaptation to vision-language reasoning remains an
open challenge with unclear best practices. Existing attempts typically employ
reasoning chains at a coarse-grained level, which struggles to perform
fine-grained structured reasoning and, more importantly, are difficult to
evaluate the reward and quality of intermediate reasoning. In this work, we
delve into chain of step reasoning for vision-language models, enabling
assessing reasoning step quality accurately and leading to effective
reinforcement learning and inference-time scaling with fine-grained rewards. We
present a simple, effective, and fully transparent framework, including the
step-level reasoning data, process reward model (PRM), and reinforcement
learning training. With the proposed approaches, our models set strong
baselines with consistent improvements on challenging vision-language
benchmarks. More importantly, we conduct a thorough empirical analysis and
ablation study, unveiling the impact of each component and several intriguing
properties of inference-time scaling. We believe this paper serves as a
baseline for vision-language models and offers insights into more complex
multimodal reasoning. Our dataset, PRM, and code will be available at
https://github.com/baaivision/CoS.

</details>


### [94] [Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model](https://arxiv.org/abs/2509.19028)
*Ioannis Sarafis,Alexandros Papadopoulos,Anastasios Delopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于SAM和ViT的弱监督食物图像语义分割方法，仅需图像级标注即可生成高质量分割掩码


<details>
  <summary>Details</summary>
Motivation: 解决食物图像分割需要大量像素级标注的问题，利用SAM的零样本能力和ViT的注意力机制来降低标注成本

Method: 使用Swin Transformer生成类激活图作为SAM的提示，结合图像预处理技术和单掩码/多掩码生成策略

Result: 在FoodSeg103数据集上，多掩码场景下达到0.54的mIoU，平均每张图像生成2.4个掩码（不包括背景）

Conclusion: 该方法可作为加速食物图像标注任务的工具，或集成到食物营养追踪应用中

Abstract: In this paper, we propose a weakly supervised semantic segmentation approach
for food images which takes advantage of the zero-shot capabilities and
promptability of the Segment Anything Model (SAM) along with the attention
mechanisms of Vision Transformers (ViTs). Specifically, we use class activation
maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable
for food image segmentation. The ViT model, a Swin Transformer, is trained
exclusively using image-level annotations, eliminating the need for pixel-level
annotations during training. Additionally, to enhance the quality of the
SAM-generated masks, we examine the use of image preprocessing techniques in
combination with single-mask and multi-mask SAM generation strategies. The
methodology is evaluated on the FoodSeg103 dataset, generating an average of
2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for
the multi-mask scenario. We envision the proposed approach as a tool to
accelerate food image annotation tasks or as an integrated component in food
and nutrition tracking applications.

</details>


### [95] [A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation](https://arxiv.org/abs/2509.19052)
*Jierui Qu,Jianchun Zhao*

Main category: cs.CV

TL;DR: DyL-UNet是一种基于动态学习的超声心动图分割架构，通过构建回声动态图和心脏相位动态注意力机制，在保持分割精度的同时提升时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 超声心动图容易变形和产生散斑噪声，导致帧间分割抖动，即使单帧分割准确，时间不稳定性也会削弱功能评估和临床可解释性。

Method: 提出DyL-UNet架构，构建回声动态图(EDG)提取动态信息，使用多个Swin-Transformer编码器-解码器分支处理单帧图像，在跳跃连接处引入心脏相位动态注意力(CPDA)机制。

Result: 在CAMUS和EchoNet-Dynamic数据集上的实验表明，DyL-UNet在保持与现有方法相当的分割精度的同时，实现了更优的时间一致性。

Conclusion: DyL-UNet为自动化临床超声心动图提供了可靠解决方案，能够实现时间稳定且精确的分割。

Abstract: Accurate segmentation of cardiac anatomy in echocardiography is essential for
cardiovascular diagnosis and treatment. Yet echocardiography is prone to
deformation and speckle noise, causing frame-to-frame segmentation jitter. Even
with high accuracy in single-frame segmentation, temporal instability can
weaken functional estimates and impair clinical interpretability. To address
these issues, we propose DyL-UNet, a dynamic learning-based temporal
consistency U-Net segmentation architecture designed to achieve temporally
stable and precise echocardiographic segmentation. The framework constructs an
Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic
information from videos. DyL-UNet incorporates multiple Swin-Transformer-based
encoder-decoder branches for processing single-frame images. It further
introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections,
which uses EDG-encoded dynamic features and cardiac-phase cues to enforce
temporal consistency during segmentation. Extensive experiments on the CAMUS
and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation
accuracy comparable to existing methods while achieving superior temporal
consistency, providing a reliable solution for automated clinical
echocardiography.

</details>


### [96] [ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?](https://arxiv.org/abs/2509.19070)
*Zijian Ling,Han Zhang,Yazhuo Zhou,Jiahao Cui*

Main category: cs.CV

TL;DR: 提出了ColorBlindnessEval基准，用于评估视觉语言模型在色盲测试类对抗场景中的鲁棒性，包含500张石原式图像，测试VLMs在复杂视觉模式中识别数字的能力。


<details>
  <summary>Details</summary>
Motivation: 评估VLMs在视觉对抗场景（受石原色盲测试启发）中的鲁棒性，发现模型在复杂视觉环境中解释数字的局限性。

Method: 创建包含500张石原式图像的数据集，数字范围0-99，使用是/否和开放式提示评估9个VLMs，并与人类参与者表现对比。

Result: 实验显示VLMs在对抗性上下文中解释数字存在局限性，普遍存在幻觉问题，模型性能不如人类。

Conclusion: ColorBlindnessEval可作为评估和改进VLMs在关键应用场景中可靠性的有效工具，突显了提升VLMs在复杂视觉环境中鲁棒性的必要性。

Abstract: This paper presents ColorBlindnessEval, a novel benchmark designed to
evaluate the robustness of Vision-Language Models (VLMs) in visually
adversarial scenarios inspired by the Ishihara color blindness test. Our
dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with
varying color combinations, challenging VLMs to accurately recognize numerical
information embedded in complex visual patterns. We assess 9 VLMs using Yes/No
and open-ended prompts and compare their performance with human participants.
Our experiments reveal limitations in the models' ability to interpret numbers
in adversarial contexts, highlighting prevalent hallucination issues. These
findings underscore the need to improve the robustness of VLMs in complex
visual environments. ColorBlindnessEval serves as a valuable tool for
benchmarking and improving the reliability of VLMs in real-world applications
where accuracy is critical.

</details>


### [97] [WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction](https://arxiv.org/abs/2509.19073)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: WaveletGaussian是一个用于稀疏视图3D高斯对象重建的高效框架，通过在小波域应用扩散模型来减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射在稀疏视图设置下性能显著下降，现有方法使用扩散模型修复损坏的渲染图但计算成本高昂。

Method: 将扩散模型转移到小波域：仅对低分辨率LL子带应用扩散，高频子带使用轻量级网络细化；提出在线随机掩码策略替代低效的留一法。

Result: 在两个基准数据集上的实验表明，WaveletGaussian在保持竞争性渲染质量的同时大幅减少了训练时间。

Conclusion: WaveletGaussian通过小波域扩散和高效训练策略，实现了稀疏视图3D高斯重建的高效优化。

Abstract: 3D Gaussian Splatting (3DGS) has become a powerful representation for
image-based object reconstruction, yet its performance drops sharply in
sparse-view settings. Prior works address this limitation by employing
diffusion models to repair corrupted renders, subsequently using them as pseudo
ground truths for later optimization. While effective, such approaches incur
heavy computation from the diffusion fine-tuning and repair steps. We present
WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object
reconstruction. Our key idea is to shift diffusion into the wavelet domain:
diffusion is applied only to the low-resolution LL subband, while
high-frequency subbands are refined with a lightweight network. We further
propose an efficient online random masking strategy to curate training pairs
for diffusion fine-tuning, replacing the commonly used, but inefficient,
leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360
and OmniObject3D, show WaveletGaussian achieves competitive rendering quality
while substantially reducing training time.

</details>


### [98] [3rd Place Report of LSVOS 2025 MeViS Track: Sa2VA-i: Improving Sa2VA Results with Consistent Training and Inference](https://arxiv.org/abs/2509.19082)
*Alexey Nekrasov,Ali Athar,Daan de Geus,Alexander Hermans,Bastian Leibe*

Main category: cs.CV

TL;DR: Sa2VA-i是对Sa2VA模型的改进版本，通过修正训练和推理过程中的不一致性问题，显著提升了在视频对象分割任务上的性能表现。


<details>
  <summary>Details</summary>
Motivation: 发现Sa2VA模型在视频对象分割任务中未能充分发挥潜力，主要原因是训练和推理过程存在不一致性。

Method: 提出Sa2VA-i模型，通过修正训练和推理过程中的不一致性问题来改进原Sa2VA模型。

Result: Sa2VA-i在多个视频基准测试中创下新纪录：MeViS提升+11.6 J&F，Ref-YT-VOS提升+1.4，Ref-DAVIS提升+3.3，ReVOS提升+4.1。Sa2VA-i-1B模型在MeViS基准上与原始Sa2VA-26B模型性能相当。

Conclusion: 这项工作强调了看似微不足道的实现细节的重要性，为视频分割领域提供了有价值的见解，代码和更新模型已开源。

Abstract: Sa2VA is a recent model for language-guided dense grounding in images and
video that achieves state-of-the-art results on multiple segmentation
benchmarks and that has become widely popular. However, we found that Sa2VA
does not perform according to its full potential for referring video object
segmentation tasks. We identify inconsistencies between training and inference
procedures as the key factor holding it back. To mitigate this issue, we
propose an improved version of Sa2VA, Sa2VA-i, that rectifies these issues and
improves the results. In fact, Sa2VA-i sets a new state of the art for multiple
video benchmarks and achieves improvements of up to +11.6 J&F on MeViS, +1.4 on
Ref-YT-VOS, +3.3 on Ref-DAVIS and +4.1 on ReVOS using the same Sa2VA
checkpoints. With our fixes, the Sa2VA-i-1B model even performs on par with the
original Sa2VA-26B model on the MeViS benchmark. We hope that this work will
show the importance of seemingly trivial implementation details and that it
will provide valuable insights for the referring video segmentation field. We
provide the code and updated models at https://github.com/kumuji/sa2va-i

</details>


### [99] [Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications](https://arxiv.org/abs/2509.19087)
*Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，将多光谱数据以零样本方式输入到仅训练于RGB数据的通用多模态模型中，通过适配输入空间和注入领域特定信息，在遥感分类任务中实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 多光谱图像在遥感应用中具有重要价值，但现有机器学习模型需要专门训练且成本高昂，而强大的通用多模态模型无法处理多光谱信号。

Method: 采用训练免费的方法，将多光谱数据适配到多模态模型的视觉空间，并将领域特定信息作为指令注入模型，以Gemini2.5模型为例进行验证。

Result: 在土地覆盖和土地利用分类的流行遥感基准测试中，该方法实现了强大的零样本性能提升，展示了Gemini2.5对新输入的易适应性。

Conclusion: 该方法使地理空间专业人员能够轻松利用强大的多模态模型（如Gemini2.5）来加速工作，受益于其丰富的推理和上下文能力。

Abstract: Multi-spectral imagery plays a crucial role in diverse Remote Sensing
applications including land-use classification, environmental monitoring and
urban planning. These images are widely adopted because their additional
spectral bands correlate strongly with physical materials on the ground, such
as ice, water, and vegetation. This allows for more accurate identification,
and their public availability from missions, such as Sentinel-2 and Landsat,
only adds to their value. Currently, the automatic analysis of such data is
predominantly managed through machine learning models specifically trained for
multi-spectral input, which are costly to train and support. Furthermore,
although providing a lot of utility for Remote Sensing, such additional inputs
cannot be used with powerful generalist large multimodal models, which are
capable of solving many visual problems, but are not able to understand
specialized multi-spectral signals.
  To address this, we propose a training-free approach which introduces new
multi-spectral data in a Zero-Shot-only mode, as inputs to generalist
multimodal models, trained on RGB-only inputs. Our approach leverages the
multimodal models' understanding of the visual space, and proposes to adapt to
inputs to that space, and to inject domain-specific information as instructions
into the model. We exemplify this idea with the Gemini2.5 model and observe
strong Zero-Shot performance gains of the approach on popular Remote Sensing
benchmarks for land cover and land use classification and demonstrate the easy
adaptability of Gemini2.5 to new inputs. These results highlight the potential
for geospatial professionals, working with non-standard specialized inputs, to
easily leverage powerful multimodal models, such as Gemini2.5, to accelerate
their work, benefiting from their rich reasoning and contextual capabilities,
grounded in the specialized sensor data.

</details>


### [100] [Citrus-V: Advancing Medical Foundation Models with Unified Medical Image Grounding for Clinical Reasoning](https://arxiv.org/abs/2509.19090)
*Guoxin Wang,Jun Zhao,Xinyi Liu,Yanbo Liu,Xuyang Cao,Chao Li,Zhuoyun Liu,Qintian Sun,Fangru Zhou,Haoqiang Xing,Zhenhong Yang*

Main category: cs.CV

TL;DR: Citrus-V是一个多模态医学基础模型，结合图像分析和文本推理，统一了检测、分割和多模态思维链推理，在单一框架中实现像素级病灶定位、结构化报告生成和医生级诊断推理。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像模型大多功能单一且需要多个专用网络，限制了泛化能力。临床应用需要精确的视觉定位、多模态集成和思维链推理能力。

Method: 提出新颖的多模态训练方法，集成检测、分割和多模态思维链推理，发布涵盖推理、检测、分割和文档理解任务的开放源码数据集。

Result: 评估显示Citrus-V在多个基准测试中优于现有开源医学模型和专家级影像系统，支持精确病灶量化、自动报告生成和可靠第二意见。

Conclusion: Citrus-V提供了从视觉定位到临床推理的统一流程，为医学影像分析提供了更全面和高效的解决方案。

Abstract: Medical imaging provides critical evidence for clinical diagnosis, treatment
planning, and surgical decisions, yet most existing imaging models are narrowly
focused and require multiple specialized networks, limiting their
generalization. Although large-scale language and multimodal models exhibit
strong reasoning and multi-task capabilities, real-world clinical applications
demand precise visual grounding, multimodal integration, and chain-of-thought
reasoning. We introduce Citrus-V, a multimodal medical foundation model that
combines image analysis with textual reasoning. The model integrates detection,
segmentation, and multimodal chain-of-thought reasoning, enabling pixel-level
lesion localization, structured report generation, and physician-like
diagnostic inference in a single framework. We propose a novel multimodal
training approach and release a curated open-source data suite covering
reasoning, detection, segmentation, and document understanding tasks.
Evaluations demonstrate that Citrus-V outperforms existing open-source medical
models and expert-level imaging systems across multiple benchmarks, delivering
a unified pipeline from visual grounding to clinical reasoning and supporting
precise lesion quantification, automated reporting, and reliable second
opinions.

</details>


### [101] [Investigating Traffic Accident Detection Using Multimodal Large Language Models](https://arxiv.org/abs/2509.19096)
*Ilhan Skender,Kailin Tong,Selim Solmaz,Daniel Watzenig*

Main category: cs.CV

TL;DR: 该研究评估了多模态大语言模型在交通事故检测中的零样本能力，通过整合YOLO、Deep SORT和SAM等视觉分析技术提升模型性能，发现Pixtral模型表现最佳，F1分数达0.71。


<details>
  <summary>Details</summary>
Motivation: 交通事故检测对公共安全至关重要，但缺乏多样化的基础设施事故数据。研究旨在利用MLLMs的零样本能力，减少对大量标注数据的依赖，实现实时自动化监控。

Method: 使用CARLA模拟的DeepAccident数据集，比较Gemini 1.5/2.0、Gemma 3和Pixtral模型的零样本性能，并整合YOLO目标检测、Deep SORT多目标跟踪和SAM实例分割技术来增强提示效果。

Result: Pixtral模型表现最佳（F1-score 0.71，召回率83%），Gemini模型通过增强提示精度提升至90%但F1和召回率下降，Gemma 3性能最均衡。

Conclusion: MLLMs与先进视觉分析技术结合在自动化交通监控系统中具有巨大应用潜力，能够提高检测准确性和可解释性。

Abstract: Traffic safety remains a critical global concern, with timely and accurate
accident detection essential for hazard reduction and rapid emergency response.
Infrastructure-based vision sensors offer scalable and efficient solutions for
continuous real-time monitoring, facilitating automated detection of acci-
dents directly from captured images. This research investigates the zero-shot
capabilities of multimodal large language models (MLLMs) for detecting and
describing traffic accidents using images from infrastructure cameras, thus
minimizing reliance on extensive labeled datasets. Main contributions include:
(1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA,
explicitly addressing the scarcity of diverse, realistic, infrastructure-based
accident data through controlled simulations; (2) Comparative performance
analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent
identification and descriptive capabilities without prior fine-tuning; and (3)
Integration of advanced visual analytics, specifically YOLO for object
detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for
instance segmentation, into enhanced prompts to improve model accuracy and
explainability. Key numerical results show Pixtral as the top performer with an
F1-score of 0.71 and 83% recall, while Gemini models gained precision with
enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and
recall losses. Gemma 3 offered the most balanced performance with minimal
metric fluctuation. These findings demonstrate the substantial potential of
integrating MLLMs with advanced visual analytics techniques, enhancing their
applicability in real-world automated traffic monitoring systems.

</details>


### [102] [Track-On2: Enhancing Online Point Tracking with Memory](https://arxiv.org/abs/2509.19115)
*Görkay Aydemir,Weidi Xie,Fatma Güney*

Main category: cs.CV

TL;DR: Track-On2是一个基于Transformer的在线长时点跟踪模型，通过架构优化、内存机制改进和合成训练策略提升性能，在多个基准测试中达到最先进水平


<details>
  <summary>Details</summary>
Motivation: 解决长时点跟踪问题，需要在不依赖未来帧的在线设置下处理显著外观变化、运动和遮挡，实现实时和流式应用

Method: 采用因果处理框架，通过内存机制保持时间一致性，先进行粗粒度补丁级分类后进行细化，并系统研究合成训练对内存行为的影响

Result: 在五个合成和真实世界基准测试中取得最先进结果，超越先前的在线跟踪器甚至利用双向上下文的强离线方法

Conclusion: 基于因果、内存的架构和纯合成数据训练是解决真实世界点跟踪问题的可扩展有效方案

Abstract: In this paper, we consider the problem of long-term point tracking, which
requires consistent identification of points across video frames under
significant appearance changes, motion, and occlusion. We target the online
setting, i.e. tracking points frame-by-frame, making it suitable for real-time
and streaming applications. We extend our prior model Track-On into Track-On2,
a simple and efficient transformer-based model for online long-term tracking.
Track-On2 improves both performance and efficiency through architectural
refinements, more effective use of memory, and improved synthetic training
strategies. Unlike prior approaches that rely on full-sequence access or
iterative updates, our model processes frames causally and maintains temporal
coherence via a memory mechanism, which is key to handling drift and occlusions
without requiring future frames. At inference, we perform coarse patch-level
classification followed by refinement. Beyond architecture, we systematically
study synthetic training setups and their impact on memory behavior, showing
how they shape temporal robustness over long sequences. Through comprehensive
experiments, Track-On2 achieves state-of-the-art results across five synthetic
and real-world benchmarks, surpassing prior online trackers and even strong
offline methods that exploit bidirectional context. These results highlight the
effectiveness of causal, memory-based architectures trained purely on synthetic
data as scalable solutions for real-world point tracking. Project page:
https://kuis-ai.github.io/track_on2

</details>


### [103] [KAMERA: Enhancing Aerial Surveys of Ice-associated Seals in Arctic Environments](https://arxiv.org/abs/2509.19129)
*Adam Romlein,Benjamin X. Hou,Yuval Boss,Cynthia L. Christman,Stacie Koslovsky,Erin E. Moreland,Jason Parham,Anthony Hoogs*

Main category: cs.CV

TL;DR: KAMERA是一个用于多摄像头、多光谱同步和实时检测海豹与北极熊的综合系统，能够将航空调查的数据处理时间减少80%。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够高效处理极地海洋生物调查数据的系统，以支持对阿拉斯加周边海域冰相关海豹的监测工作。

Method: 采用多摄像头和多光谱技术，通过严格的校准和硬件同步实现目标检测，所有数据都带有元数据注释并映射到世界平面上。

Result: 系统在伯令海、楚科奇海和波弗特海的航空调查中应用成功，数据处理时间比之前方法减少80%。

Conclusion: KAMERA系统为科学界的测绘和检测工作提供了有力工具，所有软件、模型和原理图都已开源，有望推动相关研究发展。

Abstract: We introduce KAMERA: a comprehensive system for multi-camera, multi-spectral
synchronization and real-time detection of seals and polar bears. Utilized in
aerial surveys for ice-associated seals in the Bering, Chukchi, and Beaufort
seas around Alaska, KAMERA provides up to an 80% reduction in dataset
processing time over previous methods. Our rigorous calibration and hardware
synchronization enable using multiple spectra for object detection. All
collected data are annotated with metadata so they can be easily referenced
later. All imagery and animal detections from a survey are mapped onto a world
plane for accurate surveyed area estimates and quick assessment of survey
results. We hope KAMERA will inspire other mapping and detection efforts in the
scientific community, with all software, models, and schematics fully
open-sourced.

</details>


### [104] [NeuCODEX: Edge-Cloud Co-Inference with Spike-Driven Compression and Dynamic Early-Exit](https://arxiv.org/abs/2509.19156)
*Maurf Hassan,Steven Davy,Muhammad Zawish,Owais Bin Zuber,Nouman Ashraf*

Main category: cs.CV

TL;DR: NeuCODEX是一个神经形态协同推理架构，通过联合优化空间和时间冗余，显著降低边缘计算中的数据传输和能耗，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在边缘计算中由于固定高时间步开销导致的延迟和能耗问题，以及边缘-云协同推理系统中高延迟和特征传输成本的问题。

Method: 引入学习型脉冲驱动压缩模块减少数据传输，采用动态早退机制基于输出置信度自适应终止推理。

Result: 在静态图像和神经形态事件流数据集上验证，数据传输减少高达2048倍，边缘能耗降低超过90%，端到端延迟降低3倍，精度损失小于2%。

Conclusion: NeuCODEX实现了在资源受限环境中实用且高性能的SNN部署。

Abstract: Spiking Neural Networks (SNNs) offer significant potential for enabling
energy-efficient intelligence at the edge. However, performing full SNN
inference at the edge can be challenging due to the latency and energy
constraints arising from fixed and high timestep overheads. Edge-cloud
co-inference systems present a promising solution, but their deployment is
often hindered by high latency and feature transmission costs. To address these
issues, we introduce NeuCODEX, a neuromorphic co-inference architecture that
jointly optimizes both spatial and temporal redundancy. NeuCODEX incorporates a
learned spike-driven compression module to reduce data transmission and employs
a dynamic early-exit mechanism to adaptively terminate inference based on
output confidence. We evaluated NeuCODEX on both static images (CIFAR10 and
Caltech) and neuromorphic event streams (CIFAR10-DVS and N-Caltech). To
demonstrate practicality, we prototyped NeuCODEX on ResNet-18 and VGG-16
backbones in a real edge-to-cloud testbed. Our proposed system reduces data
transfer by up to 2048x and edge energy consumption by over 90%, while reducing
end-to-end latency by up to 3x compared to edge-only inference, all with a
negligible accuracy drop of less than 2%. In doing so, NeuCODEX enables
practical, high-performance SNN deployment in resource-constrained
environments.

</details>


### [105] [RoSe: Robust Self-supervised Stereo Matching under Adverse Weather Conditions](https://arxiv.org/abs/2509.19165)
*Yun Wang,Junjie Hu,Junhui Hou,Chenghao Zhang,Renwei Yang,Dapeng Oliver Wu*

Main category: cs.CV

TL;DR: 提出了一种针对恶劣天气条件的鲁棒自监督立体匹配方法，通过引入视觉基础模型的先验知识和场景对应性先验来改进特征表示和监督信号


<details>
  <summary>Details</summary>
Motivation: 现有自监督立体匹配方法在恶劣天气条件下性能显著下降，主要原因是CNN特征提取器在退化区域表现不佳，以及光度一致性假设在恶劣天气下失效

Method: 1. 从视觉基础模型注入鲁棒先验到CNN特征提取器；2. 构建包含清晰和恶劣天气图像对的合成立体数据集；3. 提出鲁棒自监督场景对应性学习和恶劣天气蒸馏的两步训练范式

Result: 在恶劣天气条件下显著优于现有最先进的自监督方法，实验证明方法的有效性和通用性

Conclusion: 该方法通过引入鲁棒先验和场景对应性学习，有效解决了恶劣天气下立体匹配的性能退化问题，为自监督立体匹配在真实世界应用提供了可行方案

Abstract: Recent self-supervised stereo matching methods have made significant
progress, but their performance significantly degrades under adverse weather
conditions such as night, rain, and fog. We identify two primary weaknesses
contributing to this performance degradation. First, adverse weather introduces
noise and reduces visibility, making CNN-based feature extractors struggle with
degraded regions like reflective and textureless areas. Second, these degraded
regions can disrupt accurate pixel correspondences, leading to ineffective
supervision based on the photometric consistency assumption. To address these
challenges, we propose injecting robust priors derived from the visual
foundation model into the CNN-based feature extractor to improve feature
representation under adverse weather conditions. We then introduce scene
correspondence priors to construct robust supervisory signals rather than
relying solely on the photometric consistency assumption. Specifically, we
create synthetic stereo datasets with realistic weather degradations. These
datasets feature clear and adverse image pairs that maintain the same semantic
context and disparity, preserving the scene correspondence property. With this
knowledge, we propose a robust self-supervised training paradigm, consisting of
two key steps: robust self-supervised scene correspondence learning and adverse
weather distillation. Both steps aim to align underlying scene results from
clean and adverse image pairs, thus improving model disparity estimation under
adverse weather effects. Extensive experiments demonstrate the effectiveness
and versatility of our proposed solution, which outperforms existing
state-of-the-art self-supervised methods. Codes are available at
\textcolor{blue}{https://github.com/cocowy1/RoSe-Robust-Self-supervised-Stereo-Matching-under-Adverse-Weather-Conditions}.

</details>


### [106] [YOLO-LAN: Precise Polyp Detection via Optimized Loss, Augmentations and Negatives](https://arxiv.org/abs/2509.19166)
*Siddharth Gupta,Jitin Singla*

Main category: cs.CV

TL;DR: 提出YOLO-LAN息肉检测管道，基于YOLO架构，在Kvasir-seg和BKAI-IGH NeoPolyp数据集上表现优异，mAP50达到0.9619，mAP50:95达到0.8599，展示了在AI辅助结直肠癌筛查中的临床价值。


<details>
  <summary>Details</summary>
Motivation: 结直肠癌早期表现为息肉，结肠镜检查是标准检测方法，但人工检测存在不一致性和漏检问题。基于深度学习的对象检测可以提供更准确、实时的诊断方案。

Method: 提出YOLO-LAN息肉检测管道，使用M2IoU损失函数、多样化数据增强和负样本数据来模拟真实临床场景，基于YOLOv12和YOLOv8架构进行训练。

Result: 在Kvasir-seg数据集上，YOLOv12达到mAP50 0.9619、mAP50:95 0.8599；YOLOv8达到mAP50 0.9540、mAP50:95 0.8487。在息肉大小和精确定位检测方面表现出鲁棒性。

Conclusion: YOLO-LAN管道在息肉检测精度上有显著提升，特别是在mAP50:95分数上的显著增加，证明了其在AI辅助结直肠筛查中的临床相关性。

Abstract: Colorectal cancer (CRC), a lethal disease, begins with the growth of abnormal
mucosal cell proliferation called polyps in the inner wall of the colon. When
left undetected, polyps can become malignant tumors. Colonoscopy is the
standard procedure for detecting polyps, as it enables direct visualization and
removal of suspicious lesions. Manual detection by colonoscopy can be
inconsistent and is subject to oversight. Therefore, object detection based on
deep learning offers a better solution for a more accurate and real-time
diagnosis during colonoscopy. In this work, we propose YOLO-LAN, a YOLO-based
polyp detection pipeline, trained using M2IoU loss, versatile data
augmentations and negative data to replicate real clinical situations. Our
pipeline outperformed existing methods for the Kvasir-seg and BKAI-IGH NeoPolyp
datasets, achieving mAP$_{50}$ of 0.9619, mAP$_{50:95}$ of 0.8599 with YOLOv12
and mAP$_{50}$ of 0.9540, mAP$_{50:95}$ of 0.8487 with YOLOv8 on the Kvasir-seg
dataset. The significant increase is achieved in mAP$_{50:95}$ score, showing
the precision of polyp detection. We show robustness based on polyp size and
precise location detection, making it clinically relevant in AI-assisted
colorectal screening.

</details>


### [107] [The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC](https://arxiv.org/abs/2509.19183)
*Mingqi Gao,Jingkun Chen,Yunqi Miao,Gengshen Wu,Zhijin Qin,Jungong Han*

Main category: cs.CV

TL;DR: 本文提出基于SeC增强SAM-2框架的解决方案，在LSVOS挑战赛MOSEv2赛道中取得第一名，JF分数达39.89%。


<details>
  <summary>Details</summary>
Motivation: 针对复杂半监督视频对象分割任务，研究如何利用长期记忆和概念感知记忆来提升分割性能。

Method: 分析和改进SeC框架，重点研究其长期记忆机制（保持时间连续性）和概念感知记忆（提供语义先验），这些特性直接应对MOSEv2的核心挑战。

Result: 在LSVOS挑战赛MOSEv2测试集上获得39.89%的JF分数，排名第一。

Conclusion: 长期记忆在遮挡和重现情况下保持时间连续性，概念感知记忆通过语义先验抑制干扰物，两者结合有效提升了半监督视频对象分割性能。

Abstract: This technical report explores the MOSEv2 track of the LSVOS Challenge, which
targets complex semi-supervised video object segmentation. By analysing and
adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its
long-term memory and concept-aware memory, showing that long-term memory
preserves temporal continuity under occlusion and reappearance, while
concept-aware memory supplies semantic priors that suppress distractors;
together, these traits directly benefit several MOSEv2's core challenges. Our
solution achieves a JF score of 39.89% on the test set, ranking 1st in the
MOSEv2 track of the LSVOS Challenge.

</details>


### [108] [Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models](https://arxiv.org/abs/2509.19191)
*Yueyan Li,Chenggong Zhao,Zeyuan Zang,Caixia Yuan,Xiaojie Wang*

Main category: cs.CV

TL;DR: 该论文通过借鉴人类视觉的双流假说，将视觉语言模型（VLM）的视觉处理分解为物体识别和空间感知两个独立研究部分，揭示了VLM的内部工作机制，并提出了提升解码效率和空间推理能力的方法。


<details>
  <summary>Details</summary>
Motivation: 现有VLM通过序列化图像处理视觉信息，这与人类视觉的并行处理方式存在差异，且其不透明的内部机制阻碍了深入理解和架构创新。

Method: 1）将图像转换为文本标记图研究物体识别的两阶段过程；2）理论推导和实证验证VLM中位置表示的几何结构；3）提出基于即插即用视觉解码器的指令无关标记压缩算法和RoPE缩放技术。

Result: 验证了VLM内部处理机制的分析结果，包括物体识别从浅层到深层的两阶段过程，以及位置表示的几何结构特性。

Conclusion: 该研究为理解VLM内部机制提供了更深入的见解，并为设计更强大的未来架构提供了明确的设计原则。

Abstract: Vision-Language Models (VLMs) have demonstrated remarkable performance across
a variety of real-world tasks. However, existing VLMs typically process visual
information by serializing images, a method that diverges significantly from
the parallel nature of human vision. Moreover, their opaque internal mechanisms
hinder both deeper understanding and architectural innovation. Inspired by the
dual-stream hypothesis of human vision, which distinguishes the "what" and
"where" pathways, we deconstruct the visual processing in VLMs into object
recognition and spatial perception for separate study. For object recognition,
we convert images into text token maps and find that the model's perception of
image content unfolds as a two-stage process from shallow to deep layers,
beginning with attribute recognition and culminating in semantic
disambiguation. For spatial perception, we theoretically derive and empirically
verify the geometric structure underlying the positional representation in
VLMs. Based on these findings, we introduce an instruction-agnostic token
compression algorithm based on a plug-and-play visual decoder to improve
decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning.
Through rigorous experiments, our work validates these analyses, offering a
deeper understanding of VLM internals and providing clear principles for
designing more capable future architectures.

</details>


### [109] [Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions](https://arxiv.org/abs/2509.19203)
*Ioanna Ntinou,Alexandros Xenos,Yassine Ouali,Adrian Bulat,Georgios Tzimiropoulos*

Main category: cs.CV

TL;DR: 本文提出了一种无需视觉编码器的文本到文本检索方法，通过VLLM生成结构化图像描述，取代传统的文本到图像检索范式，显著减少模态差距并提升检索性能。


<details>
  <summary>Details</summary>
Motivation: 解决对比学习的视觉语言模型中存在的语言理解浅层化、模态差距大、计算成本高和隐私问题等局限性。

Method: 采用视觉无关的单编码器检索管道，将图像转换为结构化文本描述，实现文本到文本的检索范式迁移。

Result: 该方法在多个检索和组合性基准测试中实现了最先进的零样本性能，甚至超过传统多模态模型，且仅需少量GPU计算资源。

Conclusion: 视觉无关的检索方法不仅性能优越，还提供了更隐私友好的替代方案，挑战了检索任务中视觉编码器的必要性。

Abstract: Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have
become the standard approach for learning discriminative vision-language
representations. However, these models often exhibit shallow language
understanding, manifesting bag-of-words behaviour. These limitations are
reinforced by their dual-encoder design, which induces a modality gap.
Additionally, the reliance on vast web-collected data corpora for training
makes the process computationally expensive and introduces significant privacy
concerns. To address these limitations, in this work, we challenge the
necessity of vision encoders for retrieval tasks by introducing a vision-free,
single-encoder retrieval pipeline. Departing from the traditional text-to-image
retrieval paradigm, we migrate to a text-to-text paradigm with the assistance
of VLLM-generated structured image descriptions. We demonstrate that this
paradigm shift has significant advantages, including a substantial reduction of
the modality gap, improved compositionality, and better performance on short
and long caption queries, all attainable with only a few hours of calibration
on two GPUs. Additionally, substituting raw images with textual descriptions
introduces a more privacy-friendly alternative for retrieval. To further assess
generalisation and address some of the shortcomings of prior compositionality
benchmarks, we release two benchmarks derived from Flickr30k and COCO,
containing diverse compositional queries made of short captions, which we coin
subFlickr and subCOCO. Our vision-free retriever matches and often surpasses
traditional multimodal models. Importantly, our approach achieves
state-of-the-art zero-shot performance on multiple retrieval and
compositionality benchmarks, with models as small as 0.3B parameters. Code is
available at: https://github.com/IoannaNti/LexiCLIP

</details>


### [110] [Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs](https://arxiv.org/abs/2509.19207)
*Israfel Salazar,Desmond Elliott,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 论文研究了对比视觉语言模型在理解长密集描述时的挑战，发现组合性（对象-属性绑定和对象间关系推理能力）与长描述理解之间存在双向促进关系。


<details>
  <summary>Details</summary>
Motivation: 当前对比视觉语言模型在绑定视觉和文本信息方面取得显著进展，但理解长密集描述仍然是一个开放挑战。作者假设组合性是理解长描述的关键。

Method: 训练和评估了一系列针对组合性和长描述理解能力的模型，研究两种能力之间的相互作用。

Result: 发现组合性训练能提高长描述检索性能，长描述训练也能促进组合性，但这些增益对数据质量和模型设计敏感。高质量长描述数据训练可获得两项任务的强性能。

Conclusion: 组合性理解和长描述理解是相互交织的能力，可以通过训练在密集、接地气的描述上共同学习，为改进VLM泛化提供实用指导。

Abstract: Contrastive vision-language models (VLMs) have made significant progress in
binding visual and textual information, but understanding long, dense captions
remains an open challenge. We hypothesize that compositionality, the capacity
to reason about object-attribute bindings and inter-object relationships, is
key to understanding longer captions. In this paper, we investigate the
interaction between compositionality and long-caption understanding, asking
whether training for one property enhances the other. We train and evaluate a
range of models that target each of these capabilities. Our results reveal a
bidirectional relationship: compositional training improves performance on
long-caption retrieval, and training on long captions promotes
compositionality. However, these gains are sensitive to data quality and model
design. We find that training on poorly structured captions, or with limited
parameter updates, fails to support generalization. Likewise, strategies that
aim at retaining general alignment, such as freezing positional embeddings, do
not improve compositional understanding. Overall, we find that compositional
understanding and long-caption understanding are intertwined capabilities that
can be jointly learned through training on dense, grounded descriptions.
Despite these challenges, we show that models trained on high-quality,
long-caption data can achieve strong performance in both tasks, offering
practical guidance for improving VLM generalization.

</details>


### [111] [Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data](https://arxiv.org/abs/2509.19208)
*Earl Ranario,Ismael Mayanja,Heesup Yun,Brian N. Bailey,J. Mason Earles*

Main category: cs.CV

TL;DR: 提出了一种结合合成RGB图像、少量真实标注和GAN跨模态对齐的框架，用于提升热图像中的植物语义分割性能，在复杂田间环境中显著提高了分割准确率。


<details>
  <summary>Details</summary>
Motivation: 热图像中的植物分割在户外高通量表型分析中面临挑战，主要是植物与杂草对比度低以及频繁遮挡影响性能。

Method: 使用1,128张合成图像训练模型生成作物和杂草分割掩码，结合少量真实标注图像，通过CycleGAN-turbo实现RGB到热图像的跨模态对齐。

Result: 与全真实数据基线相比，杂草类别相对改进22%，植物类别相对改进17%，证明了合成数据与有限手动标注结合的有效性。

Conclusion: 通过生成模型的跨域翻译，结合合成数据和有限手动标注，可以显著提升复杂田间环境中多模态图像的分割性能。

Abstract: Accurate plant segmentation in thermal imagery remains a significant
challenge for high throughput field phenotyping, particularly in outdoor
environments where low contrast between plants and weeds and frequent
occlusions hinder performance. To address this, we present a framework that
leverages synthetic RGB imagery, a limited set of real annotations, and
GAN-based cross-modality alignment to enhance semantic segmentation in thermal
images. We trained models on 1,128 synthetic images containing complex mixtures
of crop and weed plants in order to generate image segmentation masks for crop
and weed plants. We additionally evaluated the benefit of integrating as few as
five real, manually segmented field images within the training process using
various sampling strategies. When combining all the synthetic images with a few
labeled real images, we observed a maximum relative improvement of 22% for the
weed class and 17% for the plant class compared to the full real-data baseline.
Cross-modal alignment was enabled by translating RGB to thermal using
CycleGAN-turbo, allowing robust template matching without calibration. Results
demonstrated that combining synthetic data with limited manual annotations and
cross-domain translation via generative models can significantly boost
segmentation performance in complex field environments for multi-model imagery.

</details>


### [112] [HyKid: An Open MRI Dataset with Expert-Annotated Multi-Structure and Choroid Plexus in Pediatric Hydrocephalus](https://arxiv.org/abs/2509.19218)
*Yunzhi Xu,Yushuang Ding,Hu Sun,Hongxi Zhang,Li Zhao*

Main category: cs.CV

TL;DR: HyKid是一个开源数据集，包含48名儿童脑积水患者的3D MRI数据和手动校正的脑组织分割，特别关注脉络丛分割，为脑积水评估提供了高质量基准。


<details>
  <summary>Details</summary>
Motivation: 儿童脑积水评估具有挑战性，现有研究缺乏公开可用的专家标注数据集，特别是包含脉络丛分割的数据集。

Method: 使用48名儿科脑积水患者的3D MRI数据，通过切片到体积算法从常规低分辨率图像重建1mm各向同性分辨率图像，由经验丰富的神经学家手动校正脑组织分割，并使用检索增强生成框架从临床放射学报告中提取结构化数据。

Result: 脉络丛体积与总脑脊液体积之间存在强相关性，在预测模型中表现出优异性能（AUC = 0.87），为脑积水评估提供了潜在生物标志物。

Conclusion: HyKid数据集为神经影像算法开发提供了高质量基准，揭示了脉络丛相关特征在脑积水评估中的重要性，数据集已在https://www.synapse.org/Synapse:syn68544889公开可用。

Abstract: Evaluation of hydrocephalus in children is challenging, and the related
research is limited by a lack of publicly available, expert-annotated datasets,
particularly those with segmentation of the choroid plexus. To address this, we
present HyKid, an open-source dataset from 48 pediatric patients with
hydrocephalus. 3D MRIs were provided with 1mm isotropic resolution, which was
reconstructed from routine low-resolution images using a slice-to-volume
algorithm. Manually corrected segmentations of brain tissues, including white
matter, grey matter, lateral ventricle, external CSF, and the choroid plexus,
were provided by an experienced neurologist. Additionally, structured data was
extracted from clinical radiology reports using a Retrieval-Augmented
Generation framework. The strong correlation between choroid plexus volume and
total CSF volume provided a potential biomarker for hydrocephalus evaluation,
achieving excellent performance in a predictive model (AUC = 0.87). The
proposed HyKid dataset provided a high-quality benchmark for neuroimaging
algorithms development, and it revealed the choroid plexus-related features in
hydrocephalus assessments. Our datasets are publicly available at
https://www.synapse.org/Synapse:syn68544889.

</details>


### [113] [MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation](https://arxiv.org/abs/2509.19227)
*Tongshuai Wu,Chao Lu,Ze Song,Yunlong Lin,Sizhe Fan,Xuemei Chen*

Main category: cs.CV

TL;DR: 提出多尺度特征交互网络MsFIN用于车载摄像头视频的早期事故预测，通过多尺度特征聚合、时序特征处理和多尺度特征后融合解决交通参与者特征交互建模和复杂异步多时序行为捕捉的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着行车记录仪的广泛部署和计算机视觉的发展，从行车记录仪视角开发事故预测模型对于主动安全干预变得至关重要。但存在两个关键挑战：建模交通参与者之间的特征级交互（在行车记录仪视图中经常被遮挡）和捕捉事故前复杂异步的多时序行为线索。

Method: MsFIN包含三层：多尺度特征聚合层使用多尺度模块提取短期、中期和长期时间尺度的场景表示，利用Transformer架构促进全面特征交互；时序特征处理层在因果约束下捕捉场景和对象特征的序列演化；多尺度特征后融合层融合多个时间尺度的场景和对象特征生成全面风险表示。

Result: 在DAD和DADA数据集上的实验表明，MsFIN在预测准确性和早期性方面显著优于单尺度特征提取的最先进模型。消融研究验证了MsFIN中每个模块的有效性。

Conclusion: MsFIN通过多尺度特征融合和上下文交互建模实现了优越性能，证明了多尺度特征交互网络在早期事故预测中的有效性。

Abstract: With the widespread deployment of dashcams and advancements in computer
vision, developing accident prediction models from the dashcam perspective has
become critical for proactive safety interventions. However, two key challenges
persist: modeling feature-level interactions among traffic participants (often
occluded in dashcam views) and capturing complex, asynchronous multi-temporal
behavioral cues preceding accidents. To deal with these two challenges, a
Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage
accident anticipation from dashcam videos. MsFIN has three layers for
multi-scale feature aggregation, temporal feature processing and multi-scale
feature post fusion, respectively. For multi-scale feature aggregation, a
Multi-scale Module is designed to extract scene representations at short-term,
mid-term and long-term temporal scales. Meanwhile, the Transformer architecture
is leveraged to facilitate comprehensive feature interactions. Temporal feature
processing captures the sequential evolution of scene and object features under
causal constraints. In the multi-scale feature post fusion stage, the network
fuses scene and object features across multiple temporal scales to generate a
comprehensive risk representation. Experiments on DAD and DADA datasets show
that MsFIN significantly outperforms state-of-the-art models with single-scale
feature extraction in both prediction correctness and earliness. Ablation
studies validate the effectiveness of each module in MsFIN, highlighting how
the network achieves superior performance through multi-scale feature fusion
and contextual interaction modeling.

</details>


### [114] [DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces](https://arxiv.org/abs/2509.19230)
*Tianshuo Zhang,Li Gao,Siran Peng,Xiangyu Zhu,Zhen Lei*

Main category: cs.CV

TL;DR: 该论文提出了一种基于持续学习的面部伪造检测方法，通过开发性混合专家架构（MoE）来解决伪造技术快速演变带来的检测挑战。


<details>
  <summary>Details</summary>
Motivation: 随着数字人脸生成和篡改技术的快速发展，现有检测模型难以跟上不断演变的伪造技术。需要一种能够快速适应新伪造类型、同时避免遗忘已学习知识的检测方法。

Method: 采用开发性混合专家架构，使用LoRA模型作为专家：Real-LoRA学习真实人脸知识，多个Fake-LoRA捕获不同伪造类型的增量信息。通过正交梯度和正交损失防止梯度干扰和灾难性遗忘。

Result: 在数据集和操作类型增量协议下的实验结果表明该方法有效，能够持续学习新伪造类型而不遗忘旧知识。

Conclusion: 将面部伪造检测框架为持续学习问题，并通过开发性MoE架构成功实现了对不断演变伪造技术的有效检测。

Abstract: The rise of realistic digital face generation and manipulation poses
significant social risks. The primary challenge lies in the rapid and diverse
evolution of generation techniques, which often outstrip the detection
capabilities of existing models. To defend against the ever-evolving new types
of forgery, we need to enable our model to quickly adapt to new domains with
limited computation and data while avoiding forgetting previously learned
forgery types. In this work, we posit that genuine facial samples are abundant
and relatively stable in acquisition methods, while forgery faces continuously
evolve with the iteration of manipulation techniques. Given the practical
infeasibility of exhaustively collecting all forgery variants, we frame face
forgery detection as a continual learning problem and allow the model to
develop as new forgery types emerge. Specifically, we employ a Developmental
Mixture of Experts (MoE) architecture that uses LoRA models as its individual
experts. These experts are organized into two groups: a Real-LoRA to learn and
refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental
information from different forgery types. To prevent catastrophic forgetting,
we ensure that the learning direction of Fake-LoRAs is orthogonal to the
established subspace. Moreover, we integrate orthogonal gradients into the
orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the
training process of each task. Experimental results under both the datasets and
manipulation types incremental protocols demonstrate the effectiveness of our
method.

</details>


### [115] [Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation](https://arxiv.org/abs/2509.19244)
*Shufan Li,Jiuxiang Gu,Kangning Liu,Zhe Lin,Zijun Wei,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: Lavida-O是一个统一的多模态掩码扩散模型，支持图像理解与生成任务，具备物体定位、图像编辑和高分辨率合成等新能力，通过规划和自反思机制提升生成质量，在多项基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态扩散语言模型如MMaDa和Muddit仅支持简单的图像级理解任务和低分辨率图像生成，缺乏高级功能如物体定位和高质量图像编辑。

Method: 采用弹性混合Transformer架构、通用文本条件化和分层采样等新技术，通过掩码扩散模型实现理解与生成的统一，利用理解能力通过规划和迭代自反思改进生成结果。

Result: 在RefCOCO物体定位、GenEval文本到图像生成和ImgEdit图像编辑等基准测试中超越Qwen2.5-VL和FluxKontext-dev等自回归和连续扩散模型，推理速度显著提升。

Conclusion: Lavida-O首次实现了理解能力与生成能力的统一，通过创新架构和技术在多项任务中达到最优性能，为多模态AI模型发展提供了新方向。

Abstract: We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM)
capable of image understanding and generation tasks. Unlike existing multimodal
diffsion language models such as MMaDa and Muddit which only support simple
image-level understanding tasks and low-resolution image generation, Lavida-O
exhibits many new capabilities such as object grounding, image-editing, and
high-resolution (1024px) image synthesis. It is also the first unified MDM that
uses its understanding capabilities to improve image generation and editing
results through planning and iterative self-reflection. To allow effective and
efficient training and sampling, Lavida-O ntroduces many novel techniques such
as Elastic Mixture-of-Transformer architecture, universal text conditioning,
and stratified sampling. \ours~achieves state-of-the-art performance on a wide
range of benchmarks such as RefCOCO object grounding, GenEval text-to-image
generation, and ImgEdit image editing, outperforming existing autoregressive
and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while
offering considerable speedup at inference.

</details>


### [116] [ConViS-Bench: Estimating Video Similarity Through Semantic Concepts](https://arxiv.org/abs/2509.19245)
*Benedetta Liberatori,Alessandro Conti,Lorenzo Vaquero,Yiming Wang,Elisa Ricci,Paolo Rota*

Main category: cs.CV

TL;DR: 论文提出了基于概念的视频相似度估计任务(ConViS)，通过预定义的关键语义概念来计算视频对的相似度，并建立了ConViS-Bench基准数据集来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频相似度评估通常依赖全局相似度分数，无法像人类那样从不同方面（如动作、地点等）进行多维度比较。大型多模态模型为利用自然语言进行视频比较任务提供了新机会。

Method: 引入ConViS任务，通过预定义的关键语义概念计算视频对的相似度分数；构建ConViS-Bench基准数据集，包含多个领域的视频对，每个视频对都有概念级相似度分数和文本描述；对多个先进模型进行基准测试。

Result: 不同模型在ConViS任务上表现出显著性能差异，某些概念对视频相似度估计更具挑战性。ConViS-Bench为语言驱动的视频理解研究提供了有价值的资源。

Conclusion: ConViS任务能够实现人类般的视频相似度推理，支持概念条件视频检索等新应用。该研究为多维度视频比较提供了新的研究方向。

Abstract: What does it mean for two videos to be similar? Videos may appear similar
when judged by the actions they depict, yet entirely different if evaluated
based on the locations where they were filmed. While humans naturally compare
videos by taking different aspects into account, this ability has not been
thoroughly studied and presents a challenge for models that often depend on
broad global similarity scores. Large Multimodal Models (LMMs) with video
understanding capabilities open new opportunities for leveraging natural
language in comparative video tasks. We introduce Concept-based Video
Similarity estimation (ConViS), a novel task that compares pairs of videos by
computing interpretable similarity scores across a predefined set of key
semantic concepts. ConViS allows for human-like reasoning about video
similarity and enables new applications such as concept-conditioned video
retrieval. To support this task, we also introduce ConViS-Bench, a new
benchmark comprising carefully annotated video pairs spanning multiple domains.
Each pair comes with concept-level similarity scores and textual descriptions
of both differences and similarities. Additionally, we benchmark several
state-of-the-art models on ConViS, providing insights into their alignment with
human judgments. Our results reveal significant performance differences on
ConViS, indicating that some concepts present greater challenges for estimating
video similarity. We believe that ConViS-Bench will serve as a valuable
resource for advancing research in language-driven video understanding.

</details>


### [117] [Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps](https://arxiv.org/abs/2509.19252)
*Gabriel Maldonado,Narges Rashvand,Armin Danesh Pazho,Ghazal Alinezhad Noghre,Vinit Katariya,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 提出了一种基于对抗性精炼的VQ-GAN框架，通过密集运动标记化来压缩时空热图，同时保持人体运动的细粒度轨迹。该方法在CMU Panoptic数据集上显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 人体运动理解在计算机视觉中具有高维度和固有冗余性的挑战，需要高效的压缩和表示方法来分析复杂运动动态。

Method: 结合密集运动标记化和对抗性精炼的VQ-GAN框架，消除重建伪影如运动模糊和时间错位。

Result: 在CMU Panoptic数据集上，SSIM指标比dVAE基线提高9.31%，时间不稳定性降低37.1%。发现2D运动可用128标记词汇表表示，3D运动需要1024标记码本。

Conclusion: 该方法为各种运动分析应用建立了实际部署可行性，证明了密集标记化策略的有效性。

Abstract: Continuous human motion understanding remains a core challenge in computer
vision due to its high dimensionality and inherent redundancy. Efficient
compression and representation are crucial for analyzing complex motion
dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework
with dense motion tokenization for compressing spatio-temporal heatmaps while
preserving the fine-grained traces of human motion. Our approach combines dense
motion tokenization with adversarial refinement, which eliminates
reconstruction artifacts like motion smearing and temporal misalignment
observed in non-adversarial baselines. Our experiments on the CMU Panoptic
dataset provide conclusive evidence of our method's superiority, outperforming
the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%.
Furthermore, our dense tokenization strategy enables a novel analysis of motion
complexity, revealing that 2D motion can be optimally represented with a
compact 128-token vocabulary, while 3D motion's complexity demands a much
larger 1024-token codebook for faithful reconstruction. These results establish
practical deployment feasibility across diverse motion analysis applications.
The code base for this work is available at
https://github.com/TeCSAR-UNCC/Pose-Quantization.

</details>


### [118] [Graph-Radiomic Learning (GrRAiL) Descriptor to Characterize Imaging Heterogeneity in Confounding Tumor Pathologies](https://arxiv.org/abs/2509.19258)
*Dheerendranath Battalapalli,Apoorva Safai,Maria Jaramillo,Hyemin Um,Gustavo Adalfo Pineda Ortiz,Ulas Bagci,Manmeet Singh Ahluwalia,Marwa Ismail,Pallavi Tiwari*

Main category: cs.CV

TL;DR: 提出了一种新的Graph-Radiomic Learning（GrRAiL）描述符，通过图论方法量化肿瘤内部异质性，在区分恶性肿瘤与混淆病理方面显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 解决常规影像学中可靠区分恶性肿瘤与混淆病理的挑战，传统影像组学方法忽略了复杂的空间关系

Method: GrRAiL方法：(1)使用逐体素影像组学测量识别亚区域聚类，(2)计算图论指标量化聚类间的空间关联，生成加权图编码高阶空间关系

Result: 在多中心研究中，GrRAiL在三个临床应用场景中均显著优于基线方法：胶质母细胞瘤（测试准确率78%，提升>10%）、脑转移瘤（测试准确率74%，提升>13%）、胰腺IPMN风险分层（测试准确率75%，提升>10%）

Conclusion: GrRAiL能够可靠捕捉肿瘤内部异质性，有效区分恶性肿瘤与混淆病理，在临床应用中具有重要价值

Abstract: A significant challenge in solid tumors is reliably distinguishing
confounding pathologies from malignant neoplasms on routine imaging. While
radiomics methods seek surrogate markers of lesion heterogeneity on CT/MRI,
many aggregate features across the region of interest (ROI) and miss complex
spatial relationships among varying intensity compositions. We present a new
Graph-Radiomic Learning (GrRAiL) descriptor for characterizing intralesional
heterogeneity (ILH) on clinical MRI scans. GrRAiL (1) identifies clusters of
sub-regions using per-voxel radiomic measurements, then (2) computes
graph-theoretic metrics to quantify spatial associations among clusters. The
resulting weighted graphs encode higher-order spatial relationships within the
ROI, aiming to reliably capture ILH and disambiguate confounding pathologies
from malignancy. To assess efficacy and clinical feasibility, GrRAiL was
evaluated in n=947 subjects spanning three use cases: differentiating tumor
recurrence from radiation effects in glioblastoma (GBM; n=106) and brain
metastasis (n=233), and stratifying pancreatic intraductal papillary mucinous
neoplasms (IPMNs) into no+low vs high risk (n=608). In a multi-institutional
setting, GrRAiL consistently outperformed state-of-the-art baselines - Graph
Neural Networks (GNNs), textural radiomics, and intensity-graph analysis. In
GBM, cross-validation (CV) and test accuracies for recurrence vs
pseudo-progression were 89% and 78% with >10% test-accuracy gains over
comparators. In brain metastasis, CV and test accuracies for recurrence vs
radiation necrosis were 84% and 74% (>13% improvement). For IPMN risk
stratification, CV and test accuracies were 84% and 75%, showing >10%
improvement.

</details>


### [119] [Moving by Looking: Towards Vision-Driven Avatar Motion Generation](https://arxiv.org/abs/2509.19259)
*Markos Diomataris,Berat Mert Albaba,Giorgio Becherini,Partha Ghosh,Omid Taheri,Michael J. Black*

Main category: cs.CV

TL;DR: CLOPS是第一个仅使用自我中心视觉感知环境和导航的人类化身，通过分离学习低级运动技能和高级视觉控制来实现人类化运动特性。


<details>
  <summary>Details</summary>
Motivation: 当前人类运动生成方法忽视了感知与运动的相互依赖关系，使用的任务特定感知与人类感知差异很大。生成人类化化身行为需要人类化感知。

Method: 1. 在大型运动捕捉数据集上训练运动先验模型；2. 使用Q-learning训练策略，将自我中心视觉输入映射到运动先验的高级控制命令。

Result: 实验证明自我中心视觉能在化身中产生人类化运动特性，如避开视觉场中的障碍物行走。

Conclusion: 为化身配备人类化传感器（特别是自我中心视觉）有望训练出行为像人类的化身。

Abstract: The way we perceive the world fundamentally shapes how we move, whether it is
how we navigate in a room or how we interact with other humans. Current human
motion generation methods, neglect this interdependency and use task-specific
``perception'' that differs radically from that of humans. We argue that the
generation of human-like avatar behavior requires human-like perception.
Consequently, in this work we present CLOPS, the first human avatar that solely
uses egocentric vision to perceive its surroundings and navigate. Using vision
as the primary driver of motion however, gives rise to a significant challenge
for training avatars: existing datasets have either isolated human motion,
without the context of a scene, or lack scale. We overcome this challenge by
decoupling the learning of low-level motion skills from learning of high-level
control that maps visual input to motion. First, we train a motion prior model
on a large motion capture dataset. Then, a policy is trained using Q-learning
to map egocentric visual inputs to high-level control commands for the motion
prior. Our experiments empirically demonstrate that egocentric vision can give
rise to human-like motion characteristics in our avatars. For example, the
avatars walk such that they avoid obstacles present in their visual field.
These findings suggest that equipping avatars with human-like sensors,
particularly egocentric vision, holds promise for training avatars that behave
like humans.

</details>


### [120] [OverLayBench: A Benchmark for Layout-to-Image Generation with Dense Overlaps](https://arxiv.org/abs/2509.19282)
*Bingnan Li,Chen-Yu Wang,Haiyang Xu,Xiang Zhang,Ethan Armand,Divyansh Srivastava,Xiaojun Shan,Zeyuan Chen,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 该论文针对布局到图像生成中边界框重叠问题，提出了OverLayScore指标和OverLayBench基准，并开发了CreatiLayout-AM模型来解决复杂重叠场景的生成挑战。


<details>
  <summary>Details</summary>
Motivation: 当前布局到图像生成方法在处理包含显著重叠边界框的布局时表现不佳，特别是在大重叠区域和语义区分度小的重叠实例上存在困难。

Method: 提出了OverLayScore指标量化重叠边界框复杂度，构建了OverLayBench基准数据集，并开发了基于精心整理的amodal掩码数据集微调的CreatiLayout-AM模型。

Result: 分析发现现有基准偏向低OverLayScore的简单案例，新基准提供了高质量标注和平衡的OverLayScore分布，模型在复杂重叠场景下表现更优。

Conclusion: 该研究为在现实和挑战性场景下实现更鲁棒的布局到图像生成奠定了基础。

Abstract: Despite steady progress in layout-to-image generation, current methods still
struggle with layouts containing significant overlap between bounding boxes. We
identify two primary challenges: (1) large overlapping regions and (2)
overlapping instances with minimal semantic distinction. Through both
qualitative examples and quantitative analysis, we demonstrate how these
factors degrade generation quality. To systematically assess this issue, we
introduce OverLayScore, a novel metric that quantifies the complexity of
overlapping bounding boxes. Our analysis reveals that existing benchmarks are
biased toward simpler cases with low OverLayScore values, limiting their
effectiveness in evaluating model performance under more challenging
conditions. To bridge this gap, we present OverLayBench, a new benchmark
featuring high-quality annotations and a balanced distribution across different
levels of OverLayScore. As an initial step toward improving performance on
complex overlaps, we also propose CreatiLayout-AM, a model fine-tuned on a
curated amodal mask dataset. Together, our contributions lay the groundwork for
more robust layout-to-image generation under realistic and challenging
scenarios. Project link: https://mlpc-ucsd.github.io/OverLayBench.

</details>


### [121] [Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation](https://arxiv.org/abs/2509.19296)
*Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren*

Main category: cs.CV

TL;DR: 提出了一种自蒸馏框架，将视频扩散模型中的隐式3D知识蒸馏到显式的3D高斯泼溅表示中，无需多视图训练数据即可生成3D场景。


<details>
  <summary>Details</summary>
Motivation: 当前基于学习的3D重建方法依赖于真实世界多视图数据，但这些数据并不总是容易获取。视频扩散模型具有强大的想象力，但其2D特性限制了在机器人导航等需要3D交互的应用。

Method: 在典型的RGB解码器基础上增加3D高斯泼溅解码器，通过RGB解码器的输出进行监督训练。该框架可以仅使用视频扩散模型生成的合成数据进行训练，支持从文本提示或单张图像实时渲染3D场景，并可扩展到从单目视频生成动态3D场景。

Result: 实验结果表明，该方法在静态和动态3D场景生成方面达到了最先进的性能。

Conclusion: 该自蒸馏框架成功地将视频扩散模型的2D想象力转化为实用的3D表示，为虚拟环境生成提供了新的解决方案。

Abstract: The ability to generate virtual environments is crucial for applications
ranging from gaming to physical AI domains such as robotics, autonomous
driving, and industrial AI. Current learning-based 3D reconstruction methods
rely on the availability of captured real-world multi-view data, which is not
always readily available. Recent advancements in video diffusion models have
shown remarkable imagination capabilities, yet their 2D nature limits the
applications to simulation where a robot needs to navigate and interact with
the environment. In this paper, we propose a self-distillation framework that
aims to distill the implicit 3D knowledge in the video diffusion models into an
explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for
multi-view training data. Specifically, we augment the typical RGB decoder with
a 3DGS decoder, which is supervised by the output of the RGB decoder. In this
approach, the 3DGS decoder can be purely trained with synthetic data generated
by video diffusion models. At inference time, our model can synthesize 3D
scenes from either a text prompt or a single image for real-time rendering. Our
framework further extends to dynamic 3D scene generation from a monocular input
video. Experimental results show that our framework achieves state-of-the-art
performance in static and dynamic 3D scene generation.

</details>


### [122] [VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction](https://arxiv.org/abs/2509.19297)
*Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang*

Main category: cs.CV

TL;DR: VolSplat提出了一种新的多视图前馈3D高斯泼溅方法，用体素对齐的高斯分布替代像素对齐方法，解决了现有方法对输入视图数量的依赖、视角偏差密度分布和对齐误差等问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于像素对齐高斯预测的3DGS方法存在三个主要问题：重建的3D模型严重依赖输入视图数量、导致视角偏差密度分布、在源视图存在遮挡或低纹理时引入对齐误差。

Method: VolSplat采用体素对齐高斯分布，直接从预测的3D体素网格预测高斯分布，避免了像素对齐方法对容易出错的2D特征匹配的依赖，确保稳健的多视图一致性。

Result: 在RealEstate10K和ScanNet等基准测试中，VolSplat实现了最先进的性能，产生更可信和视角一致的高斯重建，具有更好的几何一致性和新颖视图渲染质量。

Conclusion: VolSplat不仅提供了更优越的结果，还建立了一个更可扩展的前馈3D重建框架，为更广泛社区的研究铺平了道路。

Abstract: Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective
solution for novel view synthesis. Existing methods predominantly rely on a
pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a
3D Gaussian. We rethink this widely adopted formulation and identify several
inherent limitations: it renders the reconstructed 3D models heavily dependent
on the number of input views, leads to view-biased density distributions, and
introduces alignment errors, particularly when source views contain occlusions
or low texture. To address these challenges, we introduce VolSplat, a new
multi-view feed-forward paradigm that replaces pixel alignment with
voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D
voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature
matching, ensuring robust multi-view consistency. Furthermore, it enables
adaptive control over Gaussian density based on 3D scene complexity, yielding
more faithful Gaussian point clouds, improved geometric consistency, and
enhanced novel-view rendering quality. Experiments on widely used benchmarks
including RealEstate10K and ScanNet demonstrate that VolSplat achieves
state-of-the-art performance while producing more plausible and view-consistent
Gaussian reconstructions. In addition to superior results, our approach
establishes a more scalable framework for feed-forward 3D reconstruction with
denser and more robust representations, paving the way for further research in
wider communities. The video results, code and trained models are available on
our project page: https://lhmd.top/volsplat.

</details>


### [123] [CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching](https://arxiv.org/abs/2509.19300)
*Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing*

Main category: cs.CV

TL;DR: CAR-Flow是一种条件感知重参数化方法，通过在流匹配中学习轻量级的条件偏移来缩短概率路径，提高训练效率。在ImageNet-256上，将SiT-XL/2模型的FID从2.07降低到1.68，仅增加不到0.6%的参数。


<details>
  <summary>Details</summary>
Motivation: 现有的条件生成模型需要同时学习质量传输和条件注入，这给模型带来了较大负担。为了减轻模型的学习需求，需要一种更高效的方法来简化条件生成过程。

Method: 提出CAR-Flow方法，通过学习轻量级的条件偏移来重新参数化源分布、目标分布或两者。这种方法通过重新定位这些分布来缩短模型必须学习的概率路径。

Result: 在低维合成数据上可视化和量化了CAR的效果。在ImageNet-256数据集上，CAR-Flow将SiT-XL/2模型的FID从2.07显著降低到1.68，同时仅引入不到0.6%的额外参数。

Conclusion: CAR-Flow通过条件感知重参数化有效简化了条件生成任务，显著提高了模型性能，同时保持了参数效率，为条件生成建模提供了更高效的解决方案。

Abstract: Conditional generative modeling aims to learn a conditional data distribution
from samples containing data-condition pairs. For this, diffusion and
flow-based methods have attained compelling results. These methods use a
learned (flow) model to transport an initial standard Gaussian noise that
ignores the condition to the conditional data distribution. The model is hence
required to learn both mass transport and conditional injection. To ease the
demand on the model, we propose Condition-Aware Reparameterization for Flow
Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source,
the target, or both distributions. By relocating these distributions, CAR-Flow
shortens the probability path the model must learn, leading to faster training
in practice. On low-dimensional synthetic data, we visualize and quantify the
effects of CAR. On higher-dimensional natural image data (ImageNet-256),
equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while
introducing less than 0.6% additional parameters.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [124] [Does Embodiment Matter to Biomechanics and Function? A Comparative Analysis of Head-Mounted and Hand-Held Assistive Devices for Individuals with Blindness and Low Vision](https://arxiv.org/abs/2509.18391)
*Gaurav Seth,Hoa Pham,Giles Hamilton-Fletcher,Charles Leclercq,John-Ross Rizzo*

Main category: cs.HC

TL;DR: 本研究比较了手持智能手机和头戴式AR系统两种视觉辅助技术设备对盲人或低视力人群的影响，发现头戴式系统能减少上身运动和时间，手持系统在扫描小/弯曲文本时成功率更高。


<details>
  <summary>Details</summary>
Motivation: 当前视觉辅助技术（如Microsoft Seeing AI）虽能帮助盲人或低视力人群获取环境信息，但不同设备形态（手持vs头戴）对用户身体功能和运动的影响尚不明确。

Method: 让11名盲人或低视力参与者使用手持智能手机和头戴式ARx Vision系统执行6项日常生活活动，通过Xsens运动捕捉系统记录运动数据，分析功能结果（任务时间、成功率、尝试次数）和生物力学指标（关节活动范围、角路径长度、工作体积、运动平滑度）。

Result: 头戴式系统总体上减少了上身运动和任务时间（尤其在文档扫描类任务中），而手持系统在处理小或弯曲文本的任务中成功率更高。

Conclusion: 两种设备形态都可行，但在身体需求和易用性方面存在差异。将生物力学测量纳入辅助技术评估有助于优化用户体验，平衡功能效率、身体可持续性和直观交互。

Abstract: Visual assistive technologies, such as Microsoft Seeing AI, can improve
access to environmental information for persons with blindness or low vision
(pBLV). Yet, the physical and functional implications of different device
embodiments remain unclear. In this study, 11 pBLV participants used Seeing AI
on a hand-held smartphone and on a head-mounted ARx Vision system to perform
six activities of daily living, while their movements were captured with Xsens
motion capture. Functional outcomes included task time, success rate, and
number of attempts, and biomechanical measures included joint range of motion,
angular path length, working volume, and movement smoothness. The head-mounted
system generally reduced upper-body movement and task time, especially for
document-scanning style tasks, whereas the hand-held system yielded higher
success rates for tasks involving small or curved text. These findings indicate
that both embodiments are viable, but they differ in terms of physical demands
and ease of use. Incorporating biomechanical measures into assistive technology
evaluations can inform designs that optimise user experience by balancing
functional efficiency, physical sustainability, and intuitive interaction.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [125] [Machine learning approach to single-shot multiparameter estimation for the non-linear Schrödinger equation](https://arxiv.org/abs/2509.18479)
*Louis Rossignol,Tangui Aladjidi,Myrann Baker-Rasooli,Quentin Glorieux*

Main category: quant-ph

TL;DR: 该论文提出了一种基于神经网络的非线性薛定谔方程参数估计方法，通过结合快速数值求解器和ConvNeXt架构，从单次场图像中准确估计非线性系数、饱和强度和线性吸收系数。


<details>
  <summary>Details</summary>
Motivation: 非线性薛定谔方程是描述非线性介质中波动力学的基本模型，但从单次测量中准确估计其强相关参数仍然是一个重大挑战。

Method: 将参数估计视为逆问题，训练神经网络来反转NLSE映射。结合快速数值求解器和基于ConvNeXt架构的机器学习方法，使用多元高斯负对数似然损失函数。

Result: 在10万张模拟图像上训练后，模型在12,500个未见测试样本上实现了3.22%的平均绝对误差，表现出强大的泛化能力和与真实值的密切一致性。

Conclusion: 该方法为表征非线性系统提供了一条有效途径，在纳入现实噪声时有潜力连接理论建模和实验数据。

Abstract: The nonlinear Schr\"odinger equation (NLSE) is a fundamental model for wave
dynamics in nonlinear media ranging from optical fibers to Bose-Einstein
condensates. Accurately estimating its parameters, which are often strongly
correlated, from a single measurement remains a significant challenge. We
address this problem by treating parameter estimation as an inverse problem and
training a neural network to invert the NLSE mapping. We combine a fast
numerical solver with a machine learning approach based on the ConvNeXt
architecture and a multivariate Gaussian negative log-likelihood loss function.
From single-shot field (density and phase) images, our model estimates three
key parameters: the nonlinear coefficient $n_2$, the saturation intensity
$I_{sat}$, and the linear absorption coefficient $\alpha$. Trained on 100,000
simulated images, the model achieves a mean absolute error of $3.22\%$ on
12,500 unseen test samples, demonstrating strong generalization and close
agreement with ground-truth values. This approach provides an efficient route
for characterizing nonlinear systems and has the potential to bridge
theoretical modeling and experimental data when realistic noise is
incorporated.

</details>


### [126] [Quantum Random Synthetic Skyrmion Texture Generation, a Qiskit Simulation](https://arxiv.org/abs/2509.18947)
*Hillol Biswas*

Main category: quant-ph

TL;DR: 该论文研究利用量子计算合成斯格明子纹理的可能性，生成数百种不同纹理并进行比较，为基于量子随机性的斯格明子研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 斯格明子作为拓扑非平凡的磁自旋结构，具有拓扑保护的特性，可以作为量子比特。但能否利用量子计算合成斯格明子纹理是一个值得探索的问题。

Method: 通过量子计算生成数百种不同的斯格明子纹理，并对各种类型进行样本比较分析。

Result: 成功生成了多种斯格明子纹理，比较结果表明量子计算可以有效地合成斯格明子结构。

Conclusion: 量子计算为斯格明子研究提供了新的合成途径，基于量子随机性的方法为斯格明子基础研究开辟了创新方向。

Abstract: An integer winding, i.e., topological charge, is a characteristic of
skyrmions, which are topologically nontrivial spin patterns in magnets. They
emerge when smooth two-dimensional spin configurations are stabilized by
conflicting interactions such as exchange, anisotropy, the
Dzyaloshinskii-Moriya interaction, or geometric frustration. These nanoscale
textures, which are typically a few to tens of nanometers in size, are strong
'particle-like' excitations because they are shielded by energy barriers
connected to their topology. By exploiting their helicity, i.e., spin rotation
angle or associated internal modes, as a two-level system, skyrmions can
function as quantum bits or qubits. Two quantized helicity states of a
nanometer-scale skyrmion encode the logical value states in a 'skyrmion qubit.'
Interestingly, skyrmion qubits are topologically protected and macroscopic,
i.e., they involve a large number of spins; however, external influences can
still affect them. When the texture is tiny and disconnected, the helicity
angle of the skyrmion becomes quantized. A qubit basis is made up of the lowest
two energy eigenstates, i.e., symmetric or antisymmetric superpositions of
opposite helicity, for example. Therefore, Skyrmion textures can provide
valuable insights for different purposes. However, is it possible to
synthetically generate skyrmion textures using quantum computing? This paper
investigates the possibility and generates a few hundred different textures,
producing sample comparisons from various types, which indicate a novel
direction for skyrmion-based research based on quantum randomness and other
criteria.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [127] [Reconstruction of Optical Coherence Tomography Images from Wavelength-space Using Deep-learning](https://arxiv.org/abs/2509.18783)
*Maryam Viqar,Erdem Sahin,Elena Stoykova,Violeta Madjarova*

Main category: physics.optics

TL;DR: 提出基于深度学习的OCT图像重建方法，直接从波长域重建去噪图像，降低计算复杂度


<details>
  <summary>Details</summary>
Motivation: 传统FD-OCT系统依赖重采样到波数域，需要额外硬件或增加计算复杂度，且OCT图像存在散斑噪声问题

Method: 使用两个编码器-解码器网络SD-CNN和FD-CNN顺序处理，SD-CNN从傅里叶变换域重建形态结构并抑制噪声，FD-CNN在傅里叶域进一步优化图像质量

Result: 定量和视觉上证明了该方法获得高质量OCT图像的有效性，并展示了计算复杂度的降低

Conclusion: 这项工作为OCT图像重建领域的进一步创新奠定了基础

Abstract: Conventional Fourier-domain Optical Coherence Tomography (FD-OCT) systems
depend on resampling into wavenumber (k) domain to extract the depth profile.
This either necessitates additional hardware resources or amplifies the
existing computational complexity. Moreover, the OCT images also suffer from
speckle noise, due to systemic reliance on low coherence interferometry. We
propose a streamlined and computationally efficient approach based on
Deep-Learning (DL) which enables reconstructing speckle-reduced OCT images
directly from the wavelength domain. For reconstruction, two encoder-decoder
styled networks namely Spatial Domain Convolution Neural Network (SD-CNN) and
Fourier Domain CNN (FD-CNN) are used sequentially. The SD-CNN exploits the
highly degraded images obtained by Fourier transforming the domain fringes to
reconstruct the deteriorated morphological structures along with suppression of
unwanted noise. The FD-CNN leverages this output to enhance the image quality
further by optimization in Fourier domain (FD). We quantitatively and visually
demonstrate the efficacy of the method in obtaining high-quality OCT images.
Furthermore, we illustrate the computational complexity reduction by harnessing
the power of DL models. We believe that this work lays the framework for
further innovations in the realm of OCT image reconstruction.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [128] [Zero-Shot Visual Deepfake Detection: Can AI Predict and Prevent Fake Content Before It's Created?](https://arxiv.org/abs/2509.18461)
*Ayan Sar,Sampurna Roy,Tanupriya Choudhury,Ajith Abraham*

Main category: cs.GR

TL;DR: 本文探讨了零样本深度伪造检测技术，结合生成模型指纹识别、元学习等方法，并提出了AI驱动的预防策略，包括对抗性扰动、数字水印等技术。同时指出了当前面临的挑战和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着GAN和扩散模型推动深度伪造技术快速发展，其对数字安全、媒体完整性和公众信任的威胁日益严重，需要开发能够在未见过的深度伪造变体上进行检测的零样本方法。

Method: 研究了自监督学习、基于Transformer的零样本分类器、生成模型指纹识别和元学习技术，并提出了对抗性扰动、数字水印、实时AI监控和基于区块链的内容验证等预防策略。

Result: 开发了结合零样本学习和预防机制的集成防御框架，但面临对抗攻击、可扩展性限制、伦理困境和缺乏标准化评估基准等挑战。

Conclusion: 需要AI研究人员、网络安全专家和政策制定者之间的跨学科合作，建立针对深度伪造攻击的弹性防御体系，未来研究方向包括可解释AI、多模态融合、量子AI和联邦学习等。

Abstract: Generative adversarial networks (GANs) and diffusion models have dramatically
advanced deepfake technology, and its threats to digital security, media
integrity, and public trust have increased rapidly. This research explored
zero-shot deepfake detection, an emerging method even when the models have
never seen a particular deepfake variation. In this work, we studied
self-supervised learning, transformer-based zero-shot classifier, generative
model fingerprinting, and meta-learning techniques that better adapt to the
ever-evolving deepfake threat. In addition, we suggested AI-driven prevention
strategies that mitigated the underlying generation pipeline of the deepfakes
before they occurred. They consisted of adversarial perturbations for creating
deepfake generators, digital watermarking for content authenticity
verification, real-time AI monitoring for content creation pipelines, and
blockchain-based content verification frameworks. Despite these advancements,
zero-shot detection and prevention faced critical challenges such as
adversarial attacks, scalability constraints, ethical dilemmas, and the absence
of standardized evaluation benchmarks. These limitations were addressed by
discussing future research directions on explainable AI for deepfake detection,
multimodal fusion based on image, audio, and text analysis, quantum AI for
enhanced security, and federated learning for privacy-preserving deepfake
detection. This further highlighted the need for an integrated defense
framework for digital authenticity that utilized zero-shot learning in
combination with preventive deepfake mechanisms. Finally, we highlighted the
important role of interdisciplinary collaboration between AI researchers,
cybersecurity experts, and policymakers to create resilient defenses against
the rising tide of deepfake attacks.

</details>


### [129] [Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction](https://arxiv.org/abs/2509.18497)
*Kaiwen Jiang,Jia-Mu Sun,Zilu Li,Dan Wang,Tzu-Mao Li,Ravi Ramamoorthi*

Main category: cs.GR

TL;DR: 提出了一种基于高斯面元的高效可微分光传输框架，通过球谐系数空间实现全局光照，支持漫反射和镜面反射材料，在推理时实现视角无关渲染。


<details>
  <summary>Details</summary>
Motivation: 解决辐射场方法在材质反射属性和光照条件建模方面的不足，特别是几何模糊性和无法轻松重光照的问题，同时克服传统基于物理渲染方法计算成本过高的问题。

Method: 采用高斯面元作为基元，基于经典辐射度理论构建可微分光传输框架，在球谐系数空间中操作，扩展辐射度方法支持非二元可见性和半透明基元，提出高效求解器和梯度优化方法。

Result: 在几何重建、视角合成和重光照方面优于现有逆渲染基线，在已知或未知光照条件的稀疏数据集上表现优异，推理时达到数百FPS的全局光照效果。

Conclusion: 该方法成功解决了辐射场方法的局限性，实现了高效准确的全局光照计算，为逆渲染任务提供了有效的解决方案。

Abstract: Radiance fields have gained tremendous success with applications ranging from
novel view synthesis to geometry reconstruction, especially with the advent of
Gaussian splatting. However, they sacrifice modeling of material reflective
properties and lighting conditions, leading to significant geometric
ambiguities and the inability to easily perform relighting. One way to address
these limitations is to incorporate physically-based rendering, but it has been
prohibitively expensive to include full global illumination within the inner
loop of the optimization. Therefore, previous works adopt simplifications that
make the whole optimization with global illumination effects efficient but less
accurate. In this work, we adopt Gaussian surfels as the primitives and build
an efficient framework for differentiable light transport, inspired from the
classic radiosity theory. The whole framework operates in the coefficient space
of spherical harmonics, enabling both diffuse and specular materials. We extend
the classic radiosity into non-binary visibility and semi-opaque primitives,
propose novel solvers to efficiently solve the light transport, and derive the
backward pass for gradient optimizations, which is more efficient than
auto-differentiation. During inference, we achieve view-independent rendering
where light transport need not be recomputed under viewpoint changes, enabling
hundreds of FPS for global illumination effects, including view-dependent
reflections using a spherical harmonics representation. Through extensive
qualitative and quantitative experiments, we demonstrate superior geometry
reconstruction, view synthesis and relighting than previous inverse rendering
baselines, or data-driven baselines given relatively sparse datasets with known
or unknown lighting conditions.

</details>


### [130] [Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters](https://arxiv.org/abs/2509.18831)
*Pin-Yen Chiu,I-Sheng Fang,Jun-Cheng Chen*

Main category: cs.GR

TL;DR: Text Slider是一个轻量级、高效的即插即用框架，通过在预训练文本编码器中识别低秩方向来实现对视觉概念的连续控制，显著减少训练时间、GPU内存消耗和可训练参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的概念控制方法需要大量训练时间和GPU内存来学习滑块或嵌入，且需要为不同的扩散骨干网络重新训练，限制了其可扩展性和适应性。

Method: 提出Text Slider框架，在预训练文本编码器中识别低秩方向，实现连续控制。支持多概念组合和连续控制，可在图像和视频合成中进行细粒度灵活操作。

Result: Text Slider实现了平滑连续的概念属性调制，同时保持输入的空间布局和结构。训练速度比Concept Slider快5倍，比Attribute Control快47倍，GPU内存使用分别减少近2倍和4倍。

Conclusion: Text Slider是一个高效、轻量级的概念控制解决方案，显著提升了扩散模型在概念控制方面的效率和实用性。

Abstract: Recent advances in diffusion models have significantly improved image and
video synthesis. In addition, several concept control methods have been
proposed to enable fine-grained, continuous, and flexible control over
free-form text prompts. However, these methods not only require intensive
training time and GPU memory usage to learn the sliders or embeddings but also
need to be retrained for different diffusion backbones, limiting their
scalability and adaptability. To address these limitations, we introduce Text
Slider, a lightweight, efficient and plug-and-play framework that identifies
low-rank directions within a pre-trained text encoder, enabling continuous
control of visual concepts while significantly reducing training time, GPU
memory consumption, and the number of trainable parameters. Furthermore, Text
Slider supports multi-concept composition and continuous control, enabling
fine-grained and flexible manipulation in both image and video synthesis. We
show that Text Slider enables smooth and continuous modulation of specific
attributes while preserving the original spatial layout and structure of the
input. Text Slider achieves significantly better efficiency: 5$\times$ faster
training than Concept Slider and 47$\times$ faster than Attribute Control,
while reducing GPU memory usage by nearly 2$\times$ and 4$\times$,
respectively.

</details>


### [131] [One-shot Embroidery Customization via Contrastive LoRA Modulation](https://arxiv.org/abs/2509.18948)
*Jun Ma,Qian He,Gaofeng He,Huang Chen,Chen Liu,Xiaogang Jin,Huamin Wang*

Main category: cs.GR

TL;DR: 提出了一种基于对比学习的框架，用于从单张参考图像中解耦细粒度风格和内容特征，特别针对刺绣等复杂纹理的定制化风格迁移任务。


<details>
  <summary>Details</summary>
Motivation: 刺绣等纺织品艺术形式具有复杂的针法图案和材质特性，现有风格迁移方法难以处理这种细粒度特征。需要探索能够进行精细视觉特征迁移的定制化方法。

Method: 采用两阶段对比学习LoRA调制技术：第一阶段迭代更新整个LoRA和选定的风格块来初步分离风格与内容；第二阶段通过自知识蒸馏的对比学习策略进一步解耦风格和内容。构建了基于预训练扩散模型的推理管道。

Result: 在刺绣定制化基准测试中超越了现有方法，并在艺术风格迁移、素描上色和外观迁移三个额外领域展示了强大的泛化能力。

Conclusion: 该方法能够有效处理细粒度风格迁移任务，特别是在刺绣等复杂纹理的定制化应用中表现出色，具有广泛的适用性。

Abstract: Diffusion models have significantly advanced image manipulation techniques,
and their ability to generate photorealistic images is beginning to transform
retail workflows, particularly in presale visualization. Beyond artistic style
transfer, the capability to perform fine-grained visual feature transfer is
becoming increasingly important. Embroidery is a textile art form characterized
by intricate interplay of diverse stitch patterns and material properties,
which poses unique challenges for existing style transfer methods. To explore
the customization for such fine-grained features, we propose a novel
contrastive learning framework that disentangles fine-grained style and content
features with a single reference image, building on the classic concept of
image analogy. We first construct an image pair to define the target style, and
then adopt a similarity metric based on the decoupled representations of
pretrained diffusion models for style-content separation. Subsequently, we
propose a two-stage contrastive LoRA modulation technique to capture
fine-grained style features. In the first stage, we iteratively update the
whole LoRA and the selected style blocks to initially separate style from
content. In the second stage, we design a contrastive learning strategy to
further decouple style and content through self-knowledge distillation.
Finally, we build an inference pipeline to handle image or text inputs with
only the style blocks. To evaluate our method on fine-grained style transfer,
we build a benchmark for embroidery customization. Our approach surpasses prior
methods on this task and further demonstrates strong generalization to three
additional domains: artistic style transfer, sketch colorization, and
appearance transfer.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [132] [Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data](https://arxiv.org/abs/2509.18507)
*Mohammad Hosseini,Maryam M. Shanechi*

Main category: q-bio.NC

TL;DR: SBIND是一个新颖的数据驱动深度学习框架，用于建模神经图像中的时空依赖性，并将行为相关动态与其他神经动态分离。


<details>
  <summary>Details</summary>
Motivation: 高维神经成像技术（如宽场钙成像和功能性超声成像）为理解大脑活动与行为关系提供了丰富信息，但现有动态模型常通过预处理步骤获得低维表示，这可能会丢弃行为相关信息并错过时空结构。

Method: 提出SBIND框架，通过深度学习建模神经图像中的时空依赖性，分离行为相关动态。在宽场成像数据集上验证，并扩展到功能性超声成像。

Result: SBIND有效识别大脑中局部和长程空间依赖性，同时分离行为相关神经动态，在神经行为预测方面优于现有模型。

Conclusion: SBIND为使用成像模态研究行为背后的神经机制提供了一个多功能工具。

Abstract: High-dimensional imaging of neural activity, such as widefield calcium and
functional ultrasound imaging, provide a rich source of information for
understanding the relationship between brain activity and behavior. Accurately
modeling neural dynamics in these modalities is crucial for understanding this
relationship but is hindered by the high-dimensionality, complex spatiotemporal
dependencies, and prevalent behaviorally irrelevant dynamics in these
modalities. Existing dynamical models often employ preprocessing steps to
obtain low-dimensional representations from neural image modalities. However,
this process can discard behaviorally relevant information and miss
spatiotemporal structure. We propose SBIND, a novel data-driven deep learning
framework to model spatiotemporal dependencies in neural images and disentangle
their behaviorally relevant dynamics from other neural dynamics. We validate
SBIND on widefield imaging datasets, and show its extension to functional
ultrasound imaging, a recent modality whose dynamical modeling has largely
remained unexplored. We find that our model effectively identifies both local
and long-range spatial dependencies across the brain while also dissociating
behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing
models in neural-behavioral prediction. Overall, SBIND provides a versatile
tool for investigating the neural mechanisms underlying behavior using imaging
modalities.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [133] [Neural Network-Driven Direct CBCT-Based Dose Calculation for Head-and-Neck Proton Treatment Planning](https://arxiv.org/abs/2509.18378)
*Muheng Li,Evangelia Choulilitsa,Lisa Fankhauser,Francesca Albertini,Antony Lomax,Ye Zhang*

Main category: physics.med-ph

TL;DR: 本研究开发并验证了一种基于xLSTM神经网络的深度学习算法，能够直接从CBCT图像进行质子剂量计算，无需传统复杂的校正流程，在3分钟内即可完成蒙特卡洛级别的精确计算。


<details>
  <summary>Details</summary>
Motivation: 传统基于CBCT的剂量计算受限于图像质量问题，需要复杂的校正流程。现代质子治疗计划工作流程中，特别是在考虑分次间解剖结构变化的自适应治疗场景下，需要准确的CBCT剂量计算。

Method: 使用40例头颈癌患者的配对计划CT和治疗CBCT图像数据集，训练基于xLSTM的神经网络（CBCT-NN）。架构包含能量标记编码和射束视角序列建模，以捕捉质子剂量沉积模式的空间依赖性。训练使用了82,500个配对射束配置和蒙特卡洛生成的基准剂量。

Result: CBCT-NN在5例独立患者验证中，使用2mm/2%标准的伽马通过率为95.1±2.7%。高剂量区域（>90%最大剂量）的平均百分比剂量误差为2.6±1.4%，全局误差为5.9±1.9%。剂量体积直方图分析显示目标覆盖指标和危及器官约束得到良好保持。计算时间在3分钟内。

Conclusion: 本研究证明了使用xLSTM神经网络进行直接CBCT质子剂量计算的可行性，该方法消除了传统校正流程，同时实现了适用于自适应协议的准确性和计算效率。

Abstract: Accurate dose calculation on cone beam computed tomography (CBCT) images is
essential for modern proton treatment planning workflows, particularly when
accounting for inter-fractional anatomical changes in adaptive treatment
scenarios. Traditional CBCT-based dose calculation suffers from image quality
limitations, requiring complex correction workflows. This study develops and
validates a deep learning approach for direct proton dose calculation from CBCT
images using extended Long Short-Term Memory (xLSTM) neural networks. A
retrospective dataset of 40 head-and-neck cancer patients with paired planning
CT and treatment CBCT images was used to train an xLSTM-based neural network
(CBCT-NN). The architecture incorporates energy token encoding and
beam's-eye-view sequence modelling to capture spatial dependencies in proton
dose deposition patterns. Training utilized 82,500 paired beam configurations
with Monte Carlo-generated ground truth doses. Validation was performed on 5
independent patients using gamma analysis, mean percentage dose error
assessment, and dose-volume histogram comparison. The CBCT-NN achieved gamma
pass rates of 95.1 $\pm$ 2.7% using 2mm/2% criteria. Mean percentage dose
errors were 2.6 $\pm$ 1.4% in high-dose regions ($>$90% of max dose) and 5.9
$\pm$ 1.9% globally. Dose-volume histogram analysis showed excellent
preservation of target coverage metrics (Clinical Target Volume V95%
difference: -0.6 $\pm$ 1.1%) and organ-at-risk constraints (parotid mean dose
difference: -0.5 $\pm$ 1.5%). Computation time is under 3 minutes without
sacrificing Monte Carlo-level accuracy. This study demonstrates the
proof-of-principle of direct CBCT-based proton dose calculation using xLSTM
neural networks. The approach eliminates traditional correction workflows while
achieving comparable accuracy and computational efficiency suitable for
adaptive protocols.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [134] [Localized PCA-Net Neural Operators for Scalable Solution Reconstruction of Elliptic PDEs](https://arxiv.org/abs/2509.18110)
*Mrigank Dhingra,Romit Maulik,Adil Rasheed,Omer San*

Main category: cs.LG

TL;DR: 提出基于分块的PCA-Net框架，通过将高维解场分解为小块并在每个块内应用PCA，显著降低计算复杂度，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 解决在高维解场上应用主成分分析（PCA）时计算开销过大的问题，提高神经算子学习在偏微分方程求解中的效率。

Method: 提出两种分块PCA方法：局部到全局分块PCA和局部到局部分块PCA，并探索重叠分块平滑滤波和CNN细化两种优化策略。

Result: 分块PCA将端到端处理时间减少3.7-4倍，相比全局PCA显著降低计算复杂度，同时保持高精度。

Conclusion: 基于分块的PCA-Net是PDE系统中高效算子学习的有前景技术，在计算效率和精度之间取得了良好平衡。

Abstract: Neural operator learning has emerged as a powerful approach for solving
partial differential equations (PDEs) in a data-driven manner. However,
applying principal component analysis (PCA) to high-dimensional solution fields
incurs significant computational overhead. To address this, we propose a
patch-based PCA-Net framework that decomposes the solution fields into smaller
patches, applies PCA within each patch, and trains a neural operator in the
reduced PCA space. We investigate two different patch-based approaches that
balance computational efficiency and reconstruction accuracy: (1)
local-to-global patch PCA, and (2) local-to-local patch PCA. The trade-off
between computational cost and accuracy is analyzed, highlighting the
advantages and limitations of each approach. Furthermore, within each approach,
we explore two refinements for the most computationally efficient method: (i)
introducing overlapping patches with a smoothing filter and (ii) employing a
two-step process with a convolutional neural network (CNN) for refinement. Our
results demonstrate that patch-based PCA significantly reduces computational
complexity while maintaining high accuracy, reducing end-to-end pipeline
processing time by a factor of 3.7 to 4 times compared to global PCA, thefore
making it a promising technique for efficient operator learning in PDE-based
systems.

</details>


### [135] [Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection](https://arxiv.org/abs/2509.18111)
*Faizul Rakib Sayem,Shahana Ibrahim*

Main category: cs.LG

TL;DR: 提出了一种基于上下文优化的新框架，通过将子空间表示学习与提示调优相结合，利用视觉语言模型的嵌入特征来改进OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示学习的OOD检测方法仅依赖softmax概率，忽略了视觉语言模型在数百万样本上学习到的丰富特征嵌入的判别潜力。

Method: 提出CoOp框架，通过将ID特征投影到提示向量张成的子空间，同时将ID无关特征投影到正交零空间，结合端到端学习准则训练OOD检测框架。

Result: 在真实世界数据集上的实验证明了该方法的有效性。

Conclusion: 该方法通过充分利用视觉语言模型的嵌入特征，显著提高了ID-OOD的可分离性和OOD检测性能。

Abstract: The reliability of artificial intelligence (AI) systems in open-world
settings depends heavily on their ability to flag out-of-distribution (OOD)
inputs unseen during training. Recent advances in large-scale vision-language
models (VLMs) have enabled promising few-shot OOD detection frameworks using
only a handful of in-distribution (ID) samples. However, existing prompt
learning-based OOD methods rely solely on softmax probabilities, overlooking
the rich discriminative potential of the feature embeddings learned by VLMs
trained on millions of samples. To address this limitation, we propose a novel
context optimization (CoOp)-based framework that integrates subspace
representation learning with prompt tuning. Our approach improves ID-OOD
separability by projecting the ID features into a subspace spanned by prompt
vectors, while projecting ID-irrelevant features into an orthogonal null space.
To train such OOD detection framework, we design an easy-to-handle end-to-end
learning criterion that ensures strong OOD detection performance as well as
high ID classification accuracy. Experiments on real-world datasets showcase
the effectiveness of our approach.

</details>


### [136] [KM-GPT: An Automated Pipeline for Reconstructing Individual Patient Data from Kaplan-Meier Plots](https://arxiv.org/abs/2509.18141)
*Yao Zhao,Haoyue Sun,Yantian Ding,Yanxun Xu*

Main category: cs.LG

TL;DR: KM-GPT是一个全自动AI管道，用于从Kaplan-Meier图中高精度重建个体患者数据，解决了传统手动数字化方法的局限。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手动数字化，容易出错且缺乏可扩展性，需要自动化解决方案来提高临床研究中的证据合成效率。

Method: 整合高级图像预处理、GPT-5驱动的多模态推理和迭代重建算法，采用混合推理架构自动将非结构化信息转换为结构化数据流。

Result: 在合成和真实数据集上严格评估，始终表现出卓越的准确性，成功应用于胃癌免疫治疗试验的荟萃分析。

Conclusion: KM-GPT通过自动化传统手动流程，为临床研究提供可扩展的基于Web的解决方案，支持基于证据的决策制定。

Abstract: Reconstructing individual patient data (IPD) from Kaplan-Meier (KM) plots
provides valuable insights for evidence synthesis in clinical research.
However, existing approaches often rely on manual digitization, which is
error-prone and lacks scalability. To address these limitations, we develop
KM-GPT, the first fully automated, AI-powered pipeline for reconstructing IPD
directly from KM plots with high accuracy, robustness, and reproducibility.
KM-GPT integrates advanced image preprocessing, multi-modal reasoning powered
by GPT-5, and iterative reconstruction algorithms to generate high-quality IPD
without manual input or intervention. Its hybrid reasoning architecture
automates the conversion of unstructured information into structured data flows
and validates data extraction from complex KM plots. To improve accessibility,
KM-GPT is equipped with a user-friendly web interface and an integrated AI
assistant, enabling researchers to reconstruct IPD without requiring
programming expertise. KM-GPT was rigorously evaluated on synthetic and
real-world datasets, consistently demonstrating superior accuracy. To
illustrate its utility, we applied KM-GPT to a meta-analysis of gastric cancer
immunotherapy trials, reconstructing IPD to facilitate evidence synthesis and
biomarker-based subgroup analyses. By automating traditionally manual processes
and providing a scalable, web-based solution, KM-GPT transforms clinical
research by leveraging reconstructed IPD to enable more informed downstream
analyses, supporting evidence-based decision-making.

</details>


### [137] [MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe](https://arxiv.org/abs/2509.18154)
*Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: MiniCPM-V 4.5是一个8B参数的高效多模态大语言模型，在模型架构、数据策略和训练方法上进行了三项核心改进，在OpenCompass评估中超越了GPT-4o-latest等专有模型和更大的开源模型，同时在GPU内存和推理时间上具有显著效率优势。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型训练和推理效率低下的核心瓶颈问题，使其更具可访问性和可扩展性。

Method: 1. 统一的3D-Resampler模型架构实现图像和视频的高度紧凑编码；2. 统一的学习范式，无需大量数据工程即可学习文档知识和文本识别；3. 混合强化学习策略，在短推理和长推理模式中均表现熟练。

Result: 在OpenCompass评估中超越GPT-4o-latest和Qwen2.5-VL 72B等模型。在VideoMME基准测试中，在30B以下模型中达到最先进性能，仅使用Qwen2.5-VL 7B的46.7% GPU内存和8.7%推理时间。

Conclusion: MiniCPM-V 4.5在保持强性能的同时实现了显著的高效率，为多模态大语言模型的普及和扩展提供了可行的解决方案。

Abstract: Multimodal Large Language Models (MLLMs) are undergoing rapid progress and
represent the frontier of AI development. However, their training and inference
efficiency have emerged as a core bottleneck in making MLLMs more accessible
and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B
parameter model designed for high efficiency and strong performance. We
introduce three core improvements in model architecture, data strategy and
training method: a unified 3D-Resampler model architecture for highly compact
encoding over images and videos, a unified learning paradigm for document
knowledge and text recognition without heavy data engineering, and a hybrid
reinforcement learning strategy for proficiency in both short and long
reasoning modes. Comprehensive experimental results in OpenCompass evaluation
show that MiniCPM-V 4.5 surpasses widely used proprietary models such as
GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL
72B. Notably, the strong performance is achieved with remarkable efficiency.
For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves
state-of-the-art performance among models under 30B size, using just 46.7\% GPU
memory cost and 8.7\% inference time of Qwen2.5-VL 7B.

</details>


### [138] [Latent Danger Zone: Distilling Unified Attention for Cross-Architecture Black-box Attacks](https://arxiv.org/abs/2509.19044)
*Yang Li,Chenyu Wang,Tingrui Wang,Yongwei Wang,Haonan Li,Zhunga Liu,Quan Pan*

Main category: cs.LG

TL;DR: JAD是一种基于潜在扩散模型的黑盒对抗攻击框架，通过联合注意力蒸馏策略实现跨架构的对抗样本生成，具有更好的泛化性和效率


<details>
  <summary>Details</summary>
Motivation: 解决现有黑盒对抗攻击方法在跨架构迁移性和查询成本方面的局限性

Method: 利用从CNN和ViT模型中提取的注意力图指导潜在扩散模型生成对抗样本，通过关注不同架构共同敏感的图像区域实现跨架构攻击

Result: JAD在攻击泛化性、生成效率和跨架构迁移性方面优于现有方法

Conclusion: JAD为黑盒对抗攻击提供了一个有前景的有效范式

Abstract: Black-box adversarial attacks remain challenging due to limited access to
model internals. Existing methods often depend on specific network
architectures or require numerous queries, resulting in limited
cross-architecture transferability and high query costs. To address these
limitations, we propose JAD, a latent diffusion model framework for black-box
adversarial attacks. JAD generates adversarial examples by leveraging a latent
diffusion model guided by attention maps distilled from both a convolutional
neural network (CNN) and a Vision Transformer (ViT) models. By focusing on
image regions that are commonly sensitive across architectures, this approach
crafts adversarial perturbations that transfer effectively between different
model types. This joint attention distillation strategy enables JAD to be
architecture-agnostic, achieving superior attack generalization across diverse
models. Moreover, the generative nature of the diffusion framework yields high
adversarial sample generation efficiency by reducing reliance on iterative
queries. Experiments demonstrate that JAD offers improved attack
generalization, generation efficiency, and cross-architecture transferability
compared to existing methods, providing a promising and effective paradigm for
black-box adversarial attacks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [139] [Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation](https://arxiv.org/abs/2509.18342)
*Rajitha de Silva,Jonathan Cox,James R. Heselden,Marija Popovic,Cesar Cadena,Riccardo Polvara*

Main category: cs.RO

TL;DR: 提出了一种语义粒子滤波器，结合稳定的物体级检测（如葡萄藤树干和支撑杆）来改善结构化户外环境（如葡萄园）中的机器人定位问题，解决LiDAR方法因重复行几何和感知混淆而失效的问题。


<details>
  <summary>Details</summary>
Motivation: 在结构化户外环境（如葡萄园）中，由于重复的行几何和感知混淆，基于LiDAR的定位方法经常失败。需要一种能够利用稳定语义信息来提高定位准确性的方法。

Method: 使用语义粒子滤波器，将检测到的地标（葡萄藤树干和支撑杆）投影到鸟瞰图并与LiDAR扫描融合生成语义观测。关键创新是使用语义墙将相邻地标连接成伪结构约束以减少行混淆。在语义稀疏的区域引入噪声GPS先验以保持全局一致性。

Result: 在真实葡萄园中的实验表明，该方法能够保持在正确行内定位，从AMCL失败的偏差中恢复，并且优于基于视觉的SLAM方法如RTAB-Map。

Conclusion: 所提出的语义粒子滤波器通过结合语义信息和自适应GPS先验，有效解决了葡萄园环境中的定位挑战，提高了定位的鲁棒性和准确性。

Abstract: Accurate localisation is critical for mobile robots in structured outdoor
environments, yet LiDAR-based methods often fail in vineyards due to repetitive
row geometry and perceptual aliasing. We propose a semantic particle filter
that incorporates stable object-level detections, specifically vine trunks and
support poles into the likelihood estimation process. Detected landmarks are
projected into a birds eye view and fused with LiDAR scans to generate semantic
observations. A key innovation is the use of semantic walls, which connect
adjacent landmarks into pseudo-structural constraints that mitigate row
aliasing. To maintain global consistency in headland regions where semantics
are sparse, we introduce a noisy GPS prior that adaptively supports the filter.
Experiments in a real vineyard demonstrate that our approach maintains
localisation within the correct row, recovers from deviations where AMCL fails,
and outperforms vision-based SLAM methods such as RTAB-Map.

</details>


### [140] [Latent Action Pretraining Through World Modeling](https://arxiv.org/abs/2509.18428)
*Bahey Tharwat,Yara Nasser,Ali Abouzeid,Ian Reid*

Main category: cs.RO

TL;DR: LAWM是一个模型无关的框架，通过世界建模从无标签视频数据中学习潜在动作表示，实现自监督预训练，在机器人模仿学习中表现出色且更高效实用


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型需要大规模人工标注的动作数据集，且模型尺寸大难以在真实世界部署，需要更高效实用的预训练方法

Method: 提出LAWM框架，通过世界建模从无标签视频（机器人记录或人类日常动作视频）中学习潜在动作表示，实现自监督预训练，支持跨任务、环境和实现的迁移

Result: 在LIBERO基准测试和真实世界设置中，LAWM超越了使用真实机器人动作训练的模型和类似预训练方法，同时显著更高效实用

Conclusion: LAWM框架为机器人模仿学习提供了一种更高效实用的自监督预训练方案，能够从无标签视频中学习有效的潜在动作表示

Abstract: Vision-Language-Action (VLA) models have gained popularity for learning
robotic manipulation tasks that follow language instructions. State-of-the-art
VLAs, such as OpenVLA and $\pi_{0}$, were trained on large-scale, manually
labeled action datasets collected through teleoperation. More recent
approaches, including LAPA and villa-X, introduce latent action representations
that enable unsupervised pretraining on unlabeled datasets by modeling abstract
visual changes between frames. Although these methods have shown strong
results, their large model sizes make deployment in real-world settings
challenging. In this work, we propose LAWM, a model-agnostic framework to
pretrain imitation learning models in a self-supervised way, by learning latent
action representations from unlabeled video data through world modeling. These
videos can be sourced from robot recordings or videos of humans performing
actions with everyday objects. Our framework is designed to be effective for
transferring across tasks, environments, and embodiments. It outperforms models
trained with ground-truth robotics actions and similar pretraining methods on
the LIBERO benchmark and real-world setup, while being significantly more
efficient and practical for real-world settings.

</details>


### [141] [VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation](https://arxiv.org/abs/2509.18592)
*Neel P. Bhatt,Yunhao Yang,Rohan Siva,Pranay Samineni,Daniel Milan,Zhangyang Wang,Ufuk Topcu*

Main category: cs.RO

TL;DR: VLN-Zero是一个两阶段视觉语言导航框架，通过视觉语言模型构建符号场景图，实现零样本神经符号导航，在未知环境中实现快速适应和高效导航。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在未知环境中适应性差、计算效率低和泛化能力不足的问题，实现可扩展的自主导航。

Method: 采用两阶段框架：探索阶段使用结构化提示引导VLM搜索构建紧凑场景图；部署阶段使用神经符号规划器在场景图上进行推理生成可执行计划，并利用缓存机制加速适应。

Result: 相比最先进的零样本模型成功率提高2倍，超越大多数微调基线，到达目标位置时间减半，VLM调用减少55%。

Conclusion: VLN-Zero通过结合快速探索、符号推理和缓存执行，克服了现有视觉语言导航方法的计算低效和泛化能力差的问题，实现了在未知环境中的鲁棒和可扩展决策。

Abstract: Rapid adaptation in unseen environments is essential for scalable real-world
autonomy, yet existing approaches rely on exhaustive exploration or rigid
navigation policies that fail to generalize. We present VLN-Zero, a two-phase
vision-language navigation framework that leverages vision-language models to
efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic
navigation. In the exploration phase, structured prompts guide VLM-based search
toward informative and diverse trajectories, yielding compact scene graph
representations. In the deployment phase, a neurosymbolic planner reasons over
the scene graph and environmental observations to generate executable plans,
while a cache-enabled execution module accelerates adaptation by reusing
previously computed task-location trajectories. By combining rapid exploration,
symbolic reasoning, and cache-enabled execution, the proposed framework
overcomes the computational inefficiency and poor generalization of prior
vision-language navigation methods, enabling robust and scalable
decision-making in unseen environments. VLN-Zero achieves 2x higher success
rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned
baselines, and reaches goal locations in half the time with 55% fewer VLM calls
on average compared to state-of-the-art models across diverse environments.
Codebase, datasets, and videos for VLN-Zero are available at:
https://vln-zero.github.io/.

</details>


### [142] [Human-Interpretable Uncertainty Explanations for Point Cloud Registration](https://arxiv.org/abs/2509.18786)
*Johannes A. Gaus,Loris Schneider,Yitian Shi,Jongseok Lee,Rania Rayyes,Rudolph Triebel*

Main category: cs.RO

TL;DR: 提出了一种名为GP-CA的新方法，用于解决点云配准中的不确定性量化与解释问题，通过主动学习发现新的不确定性来源，并在多个数据集和真实机器人实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如ICP在传感器噪声、位姿估计误差和部分重叠等不确定性条件下容易失败，需要一种能够量化和解释配准不确定性的方法。

Method: 开发了高斯过程概念归因（GP-CA）方法，利用主动学习来查询信息丰富的实例，从而发现新的不确定性来源，并对配准不确定性进行归因分析。

Result: 在三个公开数据集和真实机器人实验中验证了GP-CA的有效性，其在运行时间、样本效率和准确性方面均优于现有最先进方法，并能实现有效的故障恢复行为。

Conclusion: GP-CA方法不仅能够量化配准不确定性，还能解释其来源，提高了机器人感知的鲁棒性，具有实际应用价值。

Abstract: In this paper, we address the point cloud registration problem, where
well-known methods like ICP fail under uncertainty arising from sensor noise,
pose-estimation errors, and partial overlap due to occlusion. We develop a
novel approach, Gaussian Process Concept Attribution (GP-CA), which not only
quantifies registration uncertainty but also explains it by attributing
uncertainty to well-known sources of errors in registration problems. Our
approach leverages active learning to discover new uncertainty sources in the
wild by querying informative instances. We validate GP-CA on three publicly
available datasets and in our real-world robot experiment. Extensive ablations
substantiate our design choices. Our approach outperforms other
state-of-the-art methods in terms of runtime, high sample-efficiency with
active learning, and high accuracy. Our real-world experiment clearly
demonstrates its applicability. Our video also demonstrates that GP-CA enables
effective failure-recovery behaviors, yielding more robust robotic perception.

</details>


### [143] [DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation](https://arxiv.org/abs/2509.18830)
*Suzannah Wistreich,Baiyu Shi,Stephen Tian,Samuel Clarke,Michael Nath,Chengyi Xu,Zhenan Bao,Jiajun Wu*

Main category: cs.RO

TL;DR: DexSkin是一种柔软、可适应的电容式电子皮肤，能够在机器人手指表面实现灵敏、局部化和可校准的触觉感知，用于学习接触密集的灵巧操作任务。


<details>
  <summary>Details</summary>
Motivation: 人类皮肤具有丰富的触觉感知能力，能够在大型曲面区域定位有意和无意的接触事件。复制这些触觉感知能力对于灵巧机器人操作系统仍是一个长期挑战。

Method: 开发了DexSkin电子皮肤系统，将其安装在平行夹爪手指上，覆盖几乎整个手指表面。通过模仿学习框架和在线强化学习评估其在接触密集操作任务中的性能。

Result: DexSkin在需要整个手指表面感知覆盖的挑战性操作任务中表现出色，如物体重定向和弹性带缠绕任务。系统可校准以实现传感器实例间的模型迁移。

Conclusion: DexSkin证明了对学习真实世界接触密集操作的适用性和实用性，为机器人系统提供了类似人类皮肤的触觉感知能力。

Abstract: Human skin provides a rich tactile sensing stream, localizing intentional and
unintentional contact events over a large and contoured region. Replicating
these tactile sensing capabilities for dexterous robotic manipulation systems
remains a longstanding challenge. In this work, we take a step towards this
goal by introducing DexSkin. DexSkin is a soft, conformable capacitive
electronic skin that enables sensitive, localized, and calibratable tactile
sensing, and can be tailored to varying geometries. We demonstrate its efficacy
for learning downstream robotic manipulation by sensorizing a pair of parallel
jaw gripper fingers, providing tactile coverage across almost the entire finger
surfaces. We empirically evaluate DexSkin's capabilities in learning
challenging manipulation tasks that require sensing coverage across the entire
surface of the fingers, such as reorienting objects in hand and wrapping
elastic bands around boxes, in a learning-from-demonstration framework. We then
show that, critically for data-driven approaches, DexSkin can be calibrated to
enable model transfer across sensor instances, and demonstrate its
applicability to online reinforcement learning on real robots. Our results
highlight DexSkin's suitability and practicality for learning real-world,
contact-rich manipulation. Please see our project webpage for videos and
visualizations: https://dex-skin.github.io/.

</details>


### [144] [Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation](https://arxiv.org/abs/2509.18954)
*Minoo Dolatabadi,Fardin Ayar,Ehsan Javanmardi,Manabu Tsukada,Mahdi Javanmardi*

Main category: cs.RO

TL;DR: 提出了一种基于深度学习的框架，用于在ICP匹配前估计注册误差协方差，无需参考地图即可为LiDAR扫描提供可靠的6-DoF误差协方差估计，从而提高定位精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: ICP算法在特征稀少环境和动态场景中容易产生误差，现有方法难以准确预测ICP的不确定性，且深度学习方法要么依赖预建地图，要么仅提供二元分类无法正确建模不确定性。

Method: 使用数据驱动的深度学习框架，在ICP匹配前估计注册误差协方差，即使没有参考地图也能为每个LiDAR扫描提供6-DoF误差协方差估计，便于与卡尔曼滤波集成。

Result: 在KITTI数据集上的实验表明，该方法能准确预测协方差，在基于预建地图的定位或SLAM应用中减少了定位误差并提高了鲁棒性。

Conclusion: 该框架有效解决了ICP不确定性估计的挑战，通过深度学习实现了准确的协方差预测，显著提升了LiDAR定位系统的性能。

Abstract: LiDAR-based localization and SLAM often rely on iterative matching
algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align
sensor data with pre-existing maps or previous scans. However, ICP is prone to
errors in featureless environments and dynamic scenes, leading to inaccurate
pose estimation. Accurately predicting the uncertainty associated with ICP is
crucial for robust state estimation but remains challenging, as existing
approaches often rely on handcrafted models or simplified assumptions.
Moreover, a few deep learning-based methods for localizability estimation
either depend on a pre-built map, which may not always be available, or provide
a binary classification of localizable versus non-localizable, which fails to
properly model uncertainty. In this work, we propose a data-driven framework
that leverages deep learning to estimate the registration error covariance of
ICP before matching, even in the absence of a reference map. By associating
each LiDAR scan with a reliable 6-DoF error covariance estimate, our method
enables seamless integration of ICP within Kalman filtering, enhancing
localization accuracy and robustness. Extensive experiments on the KITTI
dataset demonstrate the effectiveness of our approach, showing that it
accurately predicts covariance and, when applied to localization using a
pre-built map or SLAM, reduces localization errors and improves robustness.

</details>


### [145] [Category-Level Object Shape and Pose Estimation in Less Than a Millisecond](https://arxiv.org/abs/2509.18979)
*Lorenzo Shaikewitz,Tim Nguyen,Luca Carlone*

Main category: cs.RO

TL;DR: 提出了一种快速局部求解器，用于物体形状和姿态估计，仅需类别级先验知识，并能提供高效的全局最优性证明。


<details>
  <summary>Details</summary>
Motivation: 物体形状和姿态估计是机器人学的基础问题，支持从操作到场景理解和导航等任务。需要一种快速且能保证最优性的方法。

Method: 使用学习的前端检测稀疏的类别级语义关键点，通过线性活动形状模型表示未知形状，并将问题表述为最大后验优化问题，使用自洽场迭代求解。

Result: 该方法在合成数据和真实世界场景（包括两个公共数据集和无人机跟踪场景）上进行了测试，每次迭代运行时间约为100微秒，能快速剔除异常值。

Conclusion: 提出的方法能够高效解决形状和姿态估计问题，并提供简单的全局最优性证明，代码已开源。

Abstract: Object shape and pose estimation is a foundational robotics problem,
supporting tasks from manipulation to scene understanding and navigation. We
present a fast local solver for shape and pose estimation which requires only
category-level object priors and admits an efficient certificate of global
optimality. Given an RGB-D image of an object, we use a learned front-end to
detect sparse, category-level semantic keypoints on the target object. We
represent the target object's unknown shape using a linear active shape model
and pose a maximum a posteriori optimization problem to solve for position,
orientation, and shape simultaneously. Expressed in unit quaternions, this
problem admits first-order optimality conditions in the form of an eigenvalue
problem with eigenvector nonlinearities. Our primary contribution is to solve
this problem efficiently with self-consistent field iteration, which only
requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector
pair at each iterate. Solving a linear system for the corresponding Lagrange
multipliers gives a simple global optimality certificate. One iteration of our
solver runs in about 100 microseconds, enabling fast outlier rejection. We test
our method on synthetic data and a variety of real-world settings, including
two public datasets and a drone tracking scenario. Code is released at
https://github.com/MIT-SPARK/Fast-ShapeAndPose.

</details>


### [146] [FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation](https://arxiv.org/abs/2509.19102)
*Hongli Xu,Lei Zhang,Xiaoyue Hu,Boyang Zhong,Kaixin Bai,Zoltán-Csaba Márton,Zhenshan Bing,Zhaopeng Chen,Alois Christian Knoll,Jianwei Zhang*

Main category: cs.RO

TL;DR: FunCanon框架将长程操作任务分解为动作块序列，通过功能对齐和轨迹迁移实现策略的泛化能力


<details>
  <summary>Details</summary>
Motivation: 解决端到端演示方法产生的任务特定策略无法泛化到训练分布之外的问题

Method: 使用功能对象规范化进行功能对齐，利用大视觉语言模型的affordance线索，将对象映射到共享功能框架中，训练对象中心和动作中心的扩散策略FuncDiffuser

Result: 在模拟和真实世界基准测试中展示了类别级泛化、跨任务行为重用和稳健的sim2real部署能力

Conclusion: 功能规范化为复杂操作领域中的可扩展模仿学习提供了强归纳偏置

Abstract: General-purpose robotic skills from end-to-end demonstrations often leads to
task-specific policies that fail to generalize beyond the training
distribution. Therefore, we introduce FunCanon, a framework that converts
long-horizon manipulation tasks into sequences of action chunks, each defined
by an actor, verb, and object. These chunks focus policy learning on the
actions themselves, rather than isolated tasks, enabling compositionality and
reuse. To make policies pose-aware and category-general, we perform functional
object canonicalization for functional alignment and automatic manipulation
trajectory transfer, mapping objects into shared functional frames using
affordance cues from large vision language models. An object centric and action
centric diffusion policy FuncDiffuser trained on this aligned data naturally
respects object affordances and poses, simplifying learning and improving
generalization ability. Experiments on simulated and real-world benchmarks
demonstrate category-level generalization, cross-task behavior reuse, and
robust sim2real deployment, showing that functional canonicalization provides a
strong inductive bias for scalable imitation learning in complex manipulation
domains. Details of the demo and supplemental material are available on our
project website https://sites.google.com/view/funcanon.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [147] [Efficient Breast and Ovarian Cancer Classification via ViT-Based Preprocessing and Transfer Learning](https://arxiv.org/abs/2509.18553)
*Richa Rawat,Faisal Ahmed*

Main category: eess.IV

TL;DR: 提出了一种基于Vision Transformer (ViT)的新方法，用于乳腺癌和卵巢癌的检测与分类，通过预训练模型微调和高效预处理流程，在二元和五分类任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统癌症检测方法依赖人工检查医学影像，耗时耗力且需要专业病理学家。为提高早期癌症检测效率，需要自动化、准确的AI辅助诊断系统。

Method: 使用预训练的ViT-Base-Patch16-224模型，通过预处理管道将组织病理学图像转换为标准化PyTorch张量，在BreakHis数据集进行二元分类，在UBC-OCEAN数据集进行五分类，不使用数据增强。

Result: 在二元分类任务中超越现有CNN、ViT和拓扑数据分析方法；在多分类任务中相比最新拓扑方法表现出更优性能。

Conclusion: 研究表明Vision Transformer迁移学习结合高效预处理在肿瘤学诊断中具有显著效果，为癌症早期检测提供了有效的AI解决方案。

Abstract: Cancer is one of the leading health challenges for women, specifically breast
and ovarian cancer. Early detection can help improve the survival rate through
timely intervention and treatment. Traditional methods of detecting cancer
involve manually examining mammograms, CT scans, ultrasounds, and other imaging
types. However, this makes the process labor-intensive and requires the
expertise of trained pathologists. Hence, making it both time-consuming and
resource-intensive. In this paper, we introduce a novel vision transformer
(ViT)-based method for detecting and classifying breast and ovarian cancer. We
use a pre-trained ViT-Base-Patch16-224 model, which is fine-tuned for both
binary and multi-class classification tasks using publicly available
histopathological image datasets. Further, we use a preprocessing pipeline that
converts raw histophological images into standardized PyTorch tensors, which
are compatible with the ViT architecture and also help improve the model
performance. We evaluated the performance of our model on two benchmark
datasets: the BreakHis dataset for binary classification and the UBC-OCEAN
dataset for five-class classification without any data augmentation. Our model
surpasses existing CNN, ViT, and topological data analysis-based approaches in
binary classification. For multi-class classification, it is evaluated against
recent topological methods and demonstrates superior performance. Our study
highlights the effectiveness of Vision Transformer-based transfer learning
combined with efficient preprocessing in oncological diagnostics.

</details>


### [148] [MOIS-SAM2: Exemplar-based Segment Anything Model 2 for multilesion interactive segmentation of neurobromas in whole-body MRI](https://arxiv.org/abs/2509.19277)
*Georgii Kolokolnikov,Marie-Lena Schmalhofer,Sophie Götz,Lennart Well,Said Farschtschi,Victor-Felix Mautner,Inka Ristow,Rene Werner*

Main category: eess.IV

TL;DR: MOIS-SAM2是一种新型多目标交互式分割模型，专门用于神经纤维瘤病1型患者的全身MRI中神经纤维瘤的精确分割，在保持高精度的同时实现了对数百个病灶的可扩展分割。


<details>
  <summary>Details</summary>
Motivation: 神经纤维瘤病1型患者体内会发展大量神经纤维瘤，现有交互式分割方法无法同时实现高病灶精度和扩展到数百个病灶的可扩展性。

Method: MOIS-SAM2基于最先进的Segment Anything Model 2（SAM2），通过基于范例的语义传播进行扩展，在119个WB-MRI扫描数据上进行训练和评估。

Result: 在域内测试集上，MOIS-SAM2的DSC达到0.60，优于基线方法；在不同域偏移场景下保持良好性能；模型与专家的一致性（DSC: 0.62-0.68）与专家间一致性相当。

Conclusion: MOIS-SAM2能够以最小用户输入实现高效可扩展的神经纤维瘤分割，具有强大的泛化能力，支持临床工作流程的集成。

Abstract: Background and Objectives: Neurofibromatosis type 1 is a genetic disorder
characterized by the development of numerous neurofibromas (NFs) throughout the
body. Whole-body MRI (WB-MRI) is the clinical standard for detection and
longitudinal surveillance of NF tumor growth. Existing interactive segmentation
methods fail to combine high lesion-wise precision with scalability to hundreds
of lesions. This study proposes a novel interactive segmentation model tailored
to this challenge.
  Methods: We introduce MOIS-SAM2, a multi-object interactive segmentation
model that extends the state-of-the-art, transformer-based, promptable Segment
Anything Model 2 (SAM2) with exemplar-based semantic propagation. MOIS-SAM2 was
trained and evaluated on 119 WB-MRI scans from 84 NF1 patients acquired using
T2-weighted fat-suppressed sequences. The dataset was split at the patient
level into a training set and four test sets (one in-domain and three
reflecting different domain shift scenarios, e.g., MRI field strength
variation, low tumor burden, differences in clinical site and scanner vendor).
  Results: On the in-domain test set, MOIS-SAM2 achieved a scan-wise DSC of
0.60 against expert manual annotations, outperforming baseline 3D nnU-Net (DSC:
0.54) and SAM2 (DSC: 0.35). Performance of the proposed model was maintained
under MRI field strength shift (DSC: 0.53) and scanner vendor variation (DSC:
0.50), and improved in low tumor burden cases (DSC: 0.61). Lesion detection F1
scores ranged from 0.62 to 0.78 across test sets. Preliminary inter-reader
variability analysis showed model-to-expert agreement (DSC: 0.62-0.68),
comparable to inter-expert agreement (DSC: 0.57-0.69).
  Conclusions: The proposed MOIS-SAM2 enables efficient and scalable
interactive segmentation of NFs in WB-MRI with minimal user input and strong
generalization, supporting integration into clinical workflows.

</details>
