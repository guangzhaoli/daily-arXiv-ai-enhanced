<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.RO](#cs.RO) [Total: 6]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [q-bio.PE](#q-bio.PE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]
- [cs.AI](#cs.AI) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Proximity-Based Evidence Retrieval for Uncertainty-Aware Neural Networks](https://arxiv.org/abs/2509.13338)
*Hassan Gharoun,Mohammad Sadegh Khorshidi,Kasra Ranjbarigderi,Fang Chen,Amir H. Gandomi*

Main category: cs.CV

TL;DR: 提出基于证据检索的不确定性感知决策机制，用实例自适应的证据条件准则替代全局固定阈值，通过Dempster-Shafer理论融合近邻样本预测分布，实现更可靠和可解释的决策。


<details>
  <summary>Details</summary>
Motivation: 传统基于预测熵的全局固定阈值方法存在置信错误预测较多且决策不透明的问题，需要一种能够提供明确支持证据、可审计且实例自适应的不确定性感知决策方案。

Method: 为每个测试实例在嵌入空间中检索近邻样本，使用Dempster-Shafer理论融合这些近邻样本的预测分布，生成每个实例特定的阈值标准，决策过程基于明确的支撑证据。

Result: 在CIFAR-10/100数据集上，使用BiT和ViT骨干网络，相比预测熵阈值方法，取得了相当或更好的不确定性感知性能，显著减少了置信错误预测，维持了可持续的审核负载，且仅需少量证据即可实现这些优势。

Conclusion: 证据条件标记为操作不确定性感知决策提供了比固定预测熵阈值更可靠和可解释的替代方案，支持透明和可审计的决策过程。

Abstract: This work proposes an evidence-retrieval mechanism for uncertainty-aware
decision-making that replaces a single global cutoff with an
evidence-conditioned, instance-adaptive criterion. For each test instance,
proximal exemplars are retrieved in an embedding space; their predictive
distributions are fused via Dempster-Shafer theory. The resulting fused belief
acts as a per-instance thresholding mechanism. Because the supporting evidences
are explicit, decisions are transparent and auditable. Experiments on
CIFAR-10/100 with BiT and ViT backbones show higher or comparable
uncertainty-aware performance with materially fewer confidently incorrect
outcomes and a sustainable review load compared with applying threshold on
prediction entropy. Notably, only a few evidences are sufficient to realize
these gains; increasing the evidence set yields only modest changes. These
results indicate that evidence-conditioned tagging provides a more reliable and
interpretable alternative to fixed prediction entropy thresholds for
operational uncertainty-aware decision-making.

</details>


### [2] [Hybrid Quantum-Classical Model for Image Classification](https://arxiv.org/abs/2509.13353)
*Muhammad Adnan Shahzad*

Main category: cs.CV

TL;DR: 混合量子-经典神经网络在准确性、训练效率和参数可扩展性方面优于纯经典模型，特别是在复杂视觉任务中表现更佳


<details>
  <summary>Details</summary>
Motivation: 系统比较混合量子-经典神经网络与纯经典模型在性能、效率和鲁棒性方面的差异，评估量子计算在深度学习中的实际价值

Method: 在三个基准数据集(MNIST、CIFAR100、STL10)上进行了50个训练周期的实验，将参数化量子电路与经典深度学习架构集成，并与传统CNN模型进行对比

Result: 混合模型在所有数据集上都获得了更高的验证准确率(分别达到99.38%、41.69%、74.05%)，训练速度快5-12倍，参数减少6-32%，内存和CPU使用率更低，在简单数据集上对抗攻击鲁棒性更强

Conclusion: 混合量子-经典架构在准确性、训练效率和参数可扩展性方面具有显著优势，特别适用于复杂视觉任务，但在复杂数据集上的对抗鲁棒性仍需改进

Abstract: This study presents a systematic comparison between hybrid quantum-classical
neural networks and purely classical models across three benchmark datasets
(MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and
robustness. The hybrid models integrate parameterized quantum circuits with
classical deep learning architectures, while the classical counterparts use
conventional convolutional neural networks (CNNs). Experiments were conducted
over 50 training epochs for each dataset, with evaluations on validation
accuracy, test accuracy, training time, computational resource usage, and
adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings
demonstrate that hybrid models consistently outperform classical models in
final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\%
(STL10) validation accuracy, compared to classical benchmarks of 98.21\%,
32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with
dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%)
and STL10 (+10.29\%). Hybrid models also train 5--12$\times$ faster (e.g.,
21.23s vs. 108.44s per epoch on MNIST) and use 6--32\% fewer parameters} while
maintaining superior generalization to unseen test data.Adversarial robustness
tests reveal that hybrid models are significantly more resilient on simpler
datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but
show comparable fragility on complex datasets like CIFAR100 ($\sim$1\%
robustness for both). Resource efficiency analyses indicate that hybrid models
consume less memory (4--5GB vs. 5--6GB for classical) and lower CPU utilization
(9.5\% vs. 23.2\% on average).These results suggest that hybrid
quantum-classical architectures offer compelling advantages in accuracy,
training efficiency, and parameter scalability, particularly for complex vision
tasks.

</details>


### [3] [Research on Expressway Congestion Warning Technology Based on YOLOv11-DIoU and GRU-Attention](https://arxiv.org/abs/2509.13361)
*Tong Yulin,Liang Xuechen*

Main category: cs.CV

TL;DR: 提出了一个高速公路交通拥堵综合技术框架，通过优化车辆感知算法和构建GRU-Attention预警模型，解决了遮挡条件下感知精度低和长序列依赖丢失问题，实现了高精度的拥堵预警。


<details>
  <summary>Details</summary>
Motivation: 高速公路交通拥堵严重降低出行效率，现有'检测-预测'系统存在遮挡条件下车辆感知精度低和拥堵预测中长序列依赖丢失的关键缺陷。

Method: 1) 优化YOLOv11为YOLOv11-DIoU（用DIoU Loss替换GIoU Loss）；2) 改进DeepSort（融合马氏距离和余弦距离）；3) 构建GRU-Attention模型捕捉拥堵前兆特征

Result: YOLOv11-DIoU达到95.7% mAP（比基线高6.5%），遮挡漏检率5.3%；DeepSort达到93.8% MOTA（比SORT高11.3%）；GRU-Attention模型测试精度99.7%（比传统GRU高7-9%）；10分钟提前预警时间误差≤1分钟

Conclusion: 该框架为高速公路拥堵控制提供了量化支持，在智能交通应用中具有良好前景，能够实现高精度的拥堵预警和稳定的高性能表现

Abstract: Expressway traffic congestion severely reduces travel efficiency and hinders
regional connectivity. Existing "detection-prediction" systems have critical
flaws: low vehicle perception accuracy under occlusion and loss of
long-sequence dependencies in congestion forecasting. This study proposes an
integrated technical framework to resolve these issues.For traffic flow
perception, two baseline algorithms were optimized. Traditional YOLOv11 was
upgraded to YOLOv11-DIoU by replacing GIoU Loss with DIoU Loss, and DeepSort
was improved by fusing Mahalanobis (motion) and cosine (appearance) distances.
Experiments on Chang-Shen Expressway videos showed YOLOv11-DIoU achieved 95.7\%
mAP (6.5 percentage points higher than baseline) with 5.3\% occlusion miss
rate. DeepSort reached 93.8\% MOTA (11.3 percentage points higher than SORT)
with only 4 ID switches. Using the Greenberg model (for 10-15 vehicles/km
high-density scenarios), speed and density showed a strong negative correlation
(r=-0.97), conforming to traffic flow theory. For congestion warning, a
GRU-Attention model was built to capture congestion precursors. Trained 300
epochs with flow, density, and speed, it achieved 99.7\% test accuracy (7-9
percentage points higher than traditional GRU). In 10-minute advance warnings
for 30-minute congestion, time error was $\leq$ 1 minute. Validation with an
independent video showed 95\% warning accuracy, over 90\% spatial overlap of
congestion points, and stable performance in high-flow ($>$5 vehicles/second)
scenarios.This framework provides quantitative support for expressway
congestion control, with promising intelligent transportation applications.

</details>


### [4] [Parking Space Ground Truth Test Automation by Artificial Intelligence Using Convolutional Neural Networks](https://arxiv.org/abs/2509.13366)
*Tony Rohe,Martin Margreiter,Markus Moertl*

Main category: cs.CV

TL;DR: 该研究通过应用卷积神经网络和图像模式识别技术，实现了基于车载传感器数据的实时路边停车服务分析流程的自动化，将人工工作量减少了99.58%。


<details>
  <summary>Details</summary>
Motivation: 优化现有实时路边停车服务的质量，通过自动化地面实况测试流程来替代人工工程工作，提高分析效率和准确性。

Method: 应用机器学习方法，特别是图像模式识别和卷积神经网络，对众包的车载超声波传感器检测数据进行分类和分析，实现分析过程的自动化。

Result: 实现了高水平的自动化，人工资源时间减少了99.58%，显著提高了停车服务的分析效率和质量。

Conclusion: 研究成功实现了停车服务分析流程的自动化，大幅减少了人工工作量，为未来发展和潜在应用提供了良好的基础。

Abstract: This research is part of a study of a real-time, cloud-based on-street
parking service using crowd-sourced in-vehicle fleet data. The service provides
real-time information about available parking spots by classifying
crowd-sourced detections observed via ultrasonic sensors. The goal of this
research is to optimize the current parking service quality by analyzing the
automation of the existing test process for ground truth tests. Therefore,
methods from the field of machine learning, especially image pattern
recognition, are applied to enrich the database and substitute human
engineering work in major areas of the analysis process. After an introduction
into the related areas of machine learning, this paper explains the methods and
implementations made to achieve a high level of automation, applying
convolutional neural networks. Finally, predefined metrics present the
performance level achieved, showing a time reduction of human resources up to
99.58 %. The overall improvements are discussed, summarized, and followed by an
outlook for future development and potential application of the analysis
automation tool.

</details>


### [5] [An Empirical Analysis of VLM-based OOD Detection: Mechanisms, Advantages, and Sensitivity](https://arxiv.org/abs/2509.13375)
*Yuxiao Lee,Xiaofeng Cao,Wei Ye,Jiangchao Yao,Jingkuan Song,Heng Tao Shen*

Main category: cs.CV

TL;DR: 该论文系统分析了基于视觉语言模型(VLM)的零样本分布外检测机制，揭示了其工作原理、相对于单模态方法的优势，以及存在提示词敏感性这一关键脆弱性


<details>
  <summary>Details</summary>
Motivation: 尽管视觉语言模型在零样本分布外检测方面表现出色，但研究界对其为何有效、相比单模态方法的优势以及行为鲁棒性等方面的理解仍不完整

Method: 使用分布内和分布外提示进行系统实证分析，包括：(1)形式化VLM嵌入空间的关键操作属性；(2)实证量化VLM相对于单模态方法的优势；(3)分析VLM对图像噪声和提示词措辞的敏感性

Result: 发现VLM的优势源于其利用丰富语义新颖性的能力；揭示了一个重要的不对称鲁棒性特征：对常见图像噪声具有韧性，但对提示词措辞高度敏感

Conclusion: 研究结果提供了对VLM基于分布外检测的优势和关键脆弱性的结构化理解，为开发更鲁棒可靠的未来设计提供了基于实证的关键指导

Abstract: Vision-Language Models (VLMs), such as CLIP, have demonstrated remarkable
zero-shot out-of-distribution (OOD) detection capabilities, vital for reliable
AI systems. Despite this promising capability, a comprehensive understanding of
(1) why they work so effectively, (2) what advantages do they have over
single-modal methods, and (3) how is their behavioral robustness -- remains
notably incomplete within the research community. This paper presents a
systematic empirical analysis of VLM-based OOD detection using in-distribution
(ID) and OOD prompts. (1) Mechanisms: We systematically characterize and
formalize key operational properties within the VLM embedding space that
facilitate zero-shot OOD detection. (2) Advantages: We empirically quantify the
superiority of these models over established single-modal approaches,
attributing this distinct advantage to the VLM's capacity to leverage rich
semantic novelty. (3) Sensitivity: We uncovers a significant and previously
under-explored asymmetry in their robustness profile: while exhibiting
resilience to common image noise, these VLM-based methods are highly sensitive
to prompt phrasing. Our findings contribute a more structured understanding of
the strengths and critical vulnerabilities inherent in VLM-based OOD detection,
offering crucial, empirically-grounded guidance for developing more robust and
reliable future designs.

</details>


### [6] [Curvature as a tool for evaluating dimensionality reduction and estimating intrinsic dimension](https://arxiv.org/abs/2509.13385)
*Charlotte Beylier,Parvaneh Joharinad,Jürgen Jost,Nahid Torbati*

Main category: cs.CV

TL;DR: 提出基于截面曲率概念构建离散度量空间几何轮廓的方法，用于评估数据表示效果和估计数据集内在维度


<details>
  <summary>Details</summary>
Motivation: 开发一种基于曲率的几何分析方法来量化评估数据表示（如降维技术）的有效性，并探索经验网络的大规模几何特性

Method: 利用新发展的截面曲率抽象概念，构建离散度量空间的曲率几何轮廓，捕捉点三元组与其他点之间的度量关系

Result: 实验证明该曲率分析可用于估计数据集的内在维度，评估降维技术的有效性，并探索经验网络的大规模几何结构

Conclusion: 曲率几何轮廓为离散度量空间提供了有效的定量分析工具，能够评估数据表示质量和揭示数据集的内在几何特性

Abstract: Utilizing recently developed abstract notions of sectional curvature, we
introduce a method for constructing a curvature-based geometric profile of
discrete metric spaces. The curvature concept that we use here captures the
metric relations between triples of points and other points. More
significantly, based on this curvature profile, we introduce a quantitative
measure to evaluate the effectiveness of data representations, such as those
produced by dimensionality reduction techniques. Furthermore, Our experiments
demonstrate that this curvature-based analysis can be employed to estimate the
intrinsic dimensionality of datasets. We use this to explore the large-scale
geometry of empirical networks and to evaluate the effectiveness of
dimensionality reduction techniques.

</details>


### [7] [Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji](https://arxiv.org/abs/2509.13388)
*Yadvendra Gurjar,Ruoni Wan,Ehsan Farahbakhsh,Rohitash Chandra*

Main category: cs.CV

TL;DR: 本研究使用机器学习和遥感技术分析斐济纳迪地区2013-2024年的土地利用变化，通过Landsat-8卫星影像、Google Earth Engine和CNN分类方法监测城市化进程。


<details>
  <summary>Details</summary>
Motivation: 斐济作为发展中国家正经历快速城市化，需要技术支持来监测土地利用变化，为城市规划和发展项目提供科学依据。

Method: 使用Landsat-8卫星影像，结合Google Earth Engine平台，采用k-means聚类无监督学习和卷积神经网络(CNN)进行监督分类，生成土地利用图并进行变化检测。

Result: 成功生成了纳迪地区的土地利用变化可视化结果，突出了城市区域随时间的变化情况。

Conclusion: 该研究为土地利用建模和变化检测提供了有效的技术框架，能够有效监测城市化进程中的土地利用变化。

Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible
in the massive development projects that include housing, roads, and civil
works. In this study, we present machine learning and remote sensing frameworks
to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The
ultimate goal of this study is to provide technical support in land cover/land
use modelling and change detection. We used Landsat-8 satellite image for the
study region and created our training dataset with labels for supervised
machine learning. We used Google Earth Engine and unsupervised machine learning
via k-means clustering to generate the land cover map. We used convolutional
neural networks to classify the selected regions' land cover types. We present
a visualisation of change detection, highlighting urban area changes over time
to monitor changes in the map.

</details>


### [8] [Real-Time Detection and Tracking of Foreign Object Intrusions in Power Systems via Feature-Based Edge Intelligence](https://arxiv.org/abs/2509.13396)
*Xinan Wang,Di Shi,Fengyu Wang*

Main category: cs.CV

TL;DR: 提出了一种用于电力传输系统实时异物入侵检测与跟踪的三阶段框架，结合YOLOv7分割、ConvNeXt特征提取和特征辅助IoU跟踪，支持边缘设备部署和增量学习。


<details>
  <summary>Details</summary>
Motivation: 电力传输系统中异物入侵检测对电网安全至关重要，需要实时、准确且能在边缘设备上运行的解决方案，同时要处理遮挡、运动等复杂场景并支持新物体的增量学习。

Method: 三阶段框架：1) YOLOv7分割模型进行快速目标定位；2) ConvNeXt特征提取器配合三元组损失生成判别性嵌入；3) 特征辅助IoU跟踪器处理遮挡和多目标跟踪。采用混合精度推理优化边缘部署。

Result: 在真实监控和无人机视频数据集上的实验表明，该框架在多种异物入侵场景下具有高准确性和鲁棒性。在NVIDIA Jetson设备上的硬件基准测试证实了其在实际边缘应用中的实用性和可扩展性。

Conclusion: 该框架为电力传输系统提供了一种高效、实用的实时异物入侵检测与跟踪解决方案，特别适合边缘计算环境，并支持无需重新训练的增量更新能力。

Abstract: This paper presents a novel three-stage framework for real-time foreign
object intrusion (FOI) detection and tracking in power transmission systems.
The framework integrates: (1) a YOLOv7 segmentation model for fast and robust
object localization, (2) a ConvNeXt-based feature extractor trained with
triplet loss to generate discriminative embeddings, and (3) a feature-assisted
IoU tracker that ensures resilient multi-object tracking under occlusion and
motion. To enable scalable field deployment, the pipeline is optimized for
deployment on low-cost edge hardware using mixed-precision inference. The
system supports incremental updates by adding embeddings from previously unseen
objects into a reference database without requiring model retraining. Extensive
experiments on real-world surveillance and drone video datasets demonstrate the
framework's high accuracy and robustness across diverse FOI scenarios. In
addition, hardware benchmarks on NVIDIA Jetson devices confirm the framework's
practicality and scalability for real-world edge applications.

</details>


### [9] [EdiVal-Agent: An Object-Centric Framework for Automated, Scalable, Fine-Grained Evaluation of Multi-Turn Editing](https://arxiv.org/abs/2509.13399)
*Tianyu Chen,Yasi Zhang,Zhi Zhang,Peiyu Yu,Shu Wang,Zhendong Wang,Kevin Lin,Xiaofei Wang,Zhengyuan Yang,Linjie Li,Chung-Ching Lin,Jianwen Xie,Oscar Leong,Lijuan Wang,Ying Nian Wu,Mingyuan Zhou*

Main category: cs.CV

TL;DR: EdiVal-Agent是一个自动化、可扩展的细粒度评估框架，用于基于多轮指令的图像编辑，通过对象中心视角和专家工具套件来解决当前评估方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑评估方法存在局限性：(i)依赖配对参考图像，导致覆盖范围有限且继承先前生成模型的偏见；(ii)仅依赖零样本视觉语言模型(VLMs)，其基于提示的评估往往不精确。

Method: EdiVal-Agent首先将图像分解为语义上有意义的对象，然后合成多样化的上下文感知编辑指令。评估时，它结合VLMs和开放词汇对象检测器来评估指令遵循，使用语义级特征提取器评估内容一致性，并利用人类偏好模型判断视觉质量。

Result: 研究表明，将VLMs与对象检测器结合在指令遵循评估中比单独使用VLMs和CLIP-based指标与人类判断有更强的一致性。模块化设计允许未来工具无缝集成，提高评估准确性。

Conclusion: 该框架构建了EdiVal-Bench基准，覆盖9种指令类型和11种最先进的编辑模型，能够识别现有失败模式，为下一代编辑模型的开发提供信息。

Abstract: Instruction-based image editing has advanced rapidly, yet reliable and
interpretable evaluation remains a bottleneck. Current protocols either (i)
depend on paired reference images -- resulting in limited coverage and
inheriting biases from prior generative models -- or (ii) rely solely on
zero-shot vision-language models (VLMs), whose prompt-based assessments of
instruction following, content consistency, and visual quality are often
imprecise.
  To address this, we introduce EdiVal-Agent, an automated, scalable, and
fine-grained evaluation framework for multi-turn instruction-based editing from
an object-centric perspective, supported by a suite of expert tools. Given an
image, EdiVal-Agent first decomposes it into semantically meaningful objects,
then synthesizes diverse, context-aware editing instructions. For evaluation,
it integrates VLMs with open-vocabulary object detectors to assess instruction
following, uses semantic-level feature extractors to evaluate content
consistency, and leverages human preference models to judge visual quality. We
show that combining VLMs with object detectors yields stronger agreement with
human judgments in instruction-following evaluation compared to using VLMs
alone and CLIP-based metrics. Furthermore, the pipeline's modular design allows
future tools to be seamlessly integrated, enhancing evaluation accuracy over
time.
  Instantiating this pipeline, we build EdiVal-Bench, a multi-turn editing
benchmark covering 9 instruction types and 11 state-of-the-art editing models
spanning autoregressive (AR) (including Nano Banana, GPT-Image-1),
flow-matching, and diffusion paradigms. We demonstrate that EdiVal-Agent can be
used to identify existing failure modes, thereby informing the development of
the next generation of editing models. Project page:
https://tianyucodings.github.io/EdiVAL-page/.

</details>


### [10] [MapAnything: Universal Feed-Forward Metric 3D Reconstruction](https://arxiv.org/abs/2509.13414)
*Nikhil Keetha,Norman Müller,Johannes Schönberger,Lorenzo Porzi,Yuchen Zhang,Tobias Fischer,Arno Knapitsch,Duncan Zauss,Ethan Weber,Nelson Antunes,Jonathon Luiten,Manuel Lopez-Antequera,Samuel Rota Bulò,Christian Richardt,Deva Ramanan,Sebastian Scherer,Peter Kontschieder*

Main category: cs.CV

TL;DR: MapAnything是一个基于Transformer的统一前馈模型，能够处理多种输入（图像、相机参数、深度等）并直接回归出度量3D场景几何和相机参数，在多个3D视觉任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了解决3D视觉任务中不同方法需要专门模型的问题，希望开发一个统一的模型来处理多种3D重建任务，实现全局一致的度量重建。

Method: 使用基于Transformer的前馈架构，采用分解的多视角场景几何表示（深度图、局部射线图、相机位姿、度量尺度因子），通过标准化监督和灵活输入增强进行训练。

Result: 在无标定SfM、标定多视角立体、单目深度估计、相机定位、深度补全等任务上，MapAnything超越或匹配专门的前馈模型，并展现出更高效的联合训练性能。

Conclusion: MapAnything为通用3D重建骨干网络开辟了道路，展示了单一模型处理多种3D视觉任务的潜力。

Abstract: We introduce MapAnything, a unified transformer-based feed-forward model that
ingests one or more images along with optional geometric inputs such as camera
intrinsics, poses, depth, or partial reconstructions, and then directly
regresses the metric 3D scene geometry and cameras. MapAnything leverages a
factored representation of multi-view scene geometry, i.e., a collection of
depth maps, local ray maps, camera poses, and a metric scale factor that
effectively upgrades local reconstructions into a globally consistent metric
frame. Standardizing the supervision and training across diverse datasets,
along with flexible input augmentation, enables MapAnything to address a broad
range of 3D vision tasks in a single feed-forward pass, including uncalibrated
structure-from-motion, calibrated multi-view stereo, monocular depth
estimation, camera localization, depth completion, and more. We provide
extensive experimental analyses and model ablations demonstrating that
MapAnything outperforms or matches specialist feed-forward models while
offering more efficient joint training behavior, thus paving the way toward a
universal 3D reconstruction backbone.

</details>


### [11] [Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization](https://arxiv.org/abs/2509.13474)
*Yujia Lin,Nicholas Evans*

Main category: cs.CV

TL;DR: SCM-PR是一种结合语义信息的跨模态地点识别框架，通过RGB图像和LiDAR地图的语义几何匹配，在复杂场景中实现鲁棒的机器人定位。


<details>
  <summary>Details</summary>
Motivation: 解决RGB视觉地点识别对光照、天气和季节变化敏感的问题，以及现有跨模态方法在复杂场景、细粒度匹配和视角变化下的性能不足。

Method: 提出VMamba主干网络提取RGB特征，语义感知特征融合模块，结合语义和几何的LiDAR描述符，跨模态语义注意力机制，多视角语义几何匹配和语义一致性损失。

Result: 在KITTI和KITTI-360数据集上实现了最先进的性能，优于其他跨模态地点识别方法。

Conclusion: 通过引入语义信息，SCM-PR显著提升了跨模态地点识别的鲁棒性和准确性，特别是在复杂环境和视角变化条件下。

Abstract: Ensuring accurate localization of robots in environments without GPS
capability is a challenging task. Visual Place Recognition (VPR) techniques can
potentially achieve this goal, but existing RGB-based methods are sensitive to
changes in illumination, weather, and other seasonal changes. Existing
cross-modal localization methods leverage the geometric properties of RGB
images and 3D LiDAR maps to reduce the sensitivity issues highlighted above.
Currently, state-of-the-art methods struggle in complex scenes, fine-grained or
high-resolution matching, and situations where changes can occur in viewpoint.
In this work, we introduce a framework we call Semantic-Enhanced Cross-Modal
Place Recognition (SCM-PR) that combines high-level semantics utilizing RGB
images for robust localization in LiDAR maps. Our proposed method introduces: a
VMamba backbone for feature extraction of RGB images; a Semantic-Aware Feature
Fusion (SAFF) module for using both place descriptors and segmentation masks;
LiDAR descriptors that incorporate both semantics and geometry; and a
cross-modal semantic attention mechanism in NetVLAD to improve matching.
Incorporating the semantic information also was instrumental in designing a
Multi-View Semantic-Geometric Matching and a Semantic Consistency Loss, both in
a contrastive learning framework. Our experimental work on the KITTI and
KITTI-360 datasets show that SCM-PR achieves state-of-the-art performance
compared to other cross-modal place recognition methods.

</details>


### [12] [Improving 3D Gaussian Splatting Compression by Scene-Adaptive Lattice Vector Quantization](https://arxiv.org/abs/2509.13482)
*Hao Xu,Xiaolin Wu,Xi Zhang*

Main category: cs.CV

TL;DR: 提出了一种场景自适应晶格向量量化(SALVQ)方法，用于改进3D高斯泼溅(3DGS)数据的压缩性能，替代传统的均匀标量量化(USQ)，在保持低复杂度的同时提升率失真效率。


<details>
  <summary>Details</summary>
Motivation: 3DGS技术虽然渲染质量高且实时性好，但生成数据量巨大，需要高效压缩。现有基于锚点的神经压缩方法都使用简单的USQ，但更复杂的量化器可能带来更好的压缩效果。

Method: 使用晶格向量量化(LVQ)替代USQ，并为每个场景优化晶格基，实现场景自适应的LVQ(SALVQ)。通过缩放晶格基向量动态调整晶格密度，支持多码率目标。

Result: SALVQ在保持低复杂度的同时，显著提升了率失真性能，可以无缝集成到现有3DGS压缩架构中，只需最小修改和计算开销。

Conclusion: SALVQ方法成功平衡了向量量化的率失真效率和USQ的低复杂度，为3DGS压缩提供了灵活高效的解决方案，无需为不同压缩级别训练单独模型。

Abstract: 3D Gaussian Splatting (3DGS) is rapidly gaining popularity for its
photorealistic rendering quality and real-time performance, but it generates
massive amounts of data. Hence compressing 3DGS data is necessary for the cost
effectiveness of 3DGS models. Recently, several anchor-based neural compression
methods have been proposed, achieving good 3DGS compression performance.
However, they all rely on uniform scalar quantization (USQ) due to its
simplicity. A tantalizing question is whether more sophisticated quantizers can
improve the current 3DGS compression methods with very little extra overhead
and minimal change to the system. The answer is yes by replacing USQ with
lattice vector quantization (LVQ). To better capture scene-specific
characteristics, we optimize the lattice basis for each scene, improving LVQ's
adaptability and R-D efficiency. This scene-adaptive LVQ (SALVQ) strikes a
balance between the R-D efficiency of vector quantization and the low
complexity of USQ. SALVQ can be seamlessly integrated into existing 3DGS
compression architectures, enhancing their R-D performance with minimal
modifications and computational overhead. Moreover, by scaling the lattice
basis vectors, SALVQ can dynamically adjust lattice density, enabling a single
model to accommodate multiple bit rate targets. This flexibility eliminates the
need to train separate models for different compression levels, significantly
reducing training time and memory consumption.

</details>


### [13] [MINGLE: VLMs for Semantically Complex Region Detection in Urban Scenes](https://arxiv.org/abs/2509.13484)
*Liu Liu,Alexandra Kudaeva,Marco Cipriano,Fatimeh Al Ghannam,Freya Tan,Gerard de Melo,Andres Sevtsuk*

Main category: cs.CV

TL;DR: 提出了MINGLE方法，用于从街景图像中检测和定位社交群体区域，通过三阶段流程整合人员检测、深度估计、VLM推理和空间聚合算法。


<details>
  <summary>Details</summary>
Motivation: 理解公共场所的群体社交互动对城市规划至关重要，但传统目标检测方法难以捕捉基于人际关系的复杂语义信号。

Method: 三阶段模块化流程：1) 现成的人员检测和深度估计；2) 基于VLM的成对社交关系分类；3) 轻量级空间聚合算法定位社交连接群体。

Result: 构建了包含10万张街景图像的新数据集，标注了个人和社交群体的边界框和标签，结合人工标注和MINGLE输出确保语义丰富性。

Conclusion: MINGLE方法有效解决了社交群体区域检测任务，为城市规划提供了实用的计算机视觉工具，并发布了高质量数据集推动未来研究。

Abstract: Understanding group-level social interactions in public spaces is crucial for
urban planning, informing the design of socially vibrant and inclusive
environments. Detecting such interactions from images involves interpreting
subtle visual cues such as relations, proximity, and co-movement - semantically
complex signals that go beyond traditional object detection. To address this
challenge, we introduce a social group region detection task, which requires
inferring and spatially grounding visual regions defined by abstract
interpersonal relations. We propose MINGLE (Modeling INterpersonal Group-Level
Engagement), a modular three-stage pipeline that integrates: (1) off-the-shelf
human detection and depth estimation, (2) VLM-based reasoning to classify
pairwise social affiliation, and (3) a lightweight spatial aggregation
algorithm to localize socially connected groups. To support this task and
encourage future research, we present a new dataset of 100K urban street-view
images annotated with bounding boxes and labels for both individuals and
socially interacting groups. The annotations combine human-created labels and
outputs from the MINGLE pipeline, ensuring semantic richness and broad coverage
of real-world scenarios.

</details>


### [14] [BiasMap: Leveraging Cross-Attentions to Discover and Mitigate Hidden Social Biases in Text-to-Image Generation](https://arxiv.org/abs/2509.13496)
*Rajatsubhra Chakraborty,Xujun Che,Depeng Xu,Cori Faklaris,Xi Niu,Shuhan Yuan*

Main category: cs.CV

TL;DR: BiasMap是一个模型无关的框架，用于发现稳定扩散模型中的潜在概念级表示偏见，通过交叉注意力归因图揭示人口统计学与语义概念之间的结构纠缠，并提出基于能量引导扩散采样的偏见缓解方法。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注输出层面的人口分布偏见，但无法保证偏见缓解后概念表示的解耦。需要更深入地揭示生成过程中的表示偏见。

Method: 利用交叉注意力归因图量化人口统计学与语义概念的空间纠缠（通过IoU指标），并采用能量引导扩散采样方法在去噪过程中最小化预期SoftIoU来缓解概念纠缠。

Result: 研究发现现有公平性干预可能减少输出分布差距，但往往无法解耦概念级耦合，而BiasMap的缓解方法能够在图像生成中缓解概念纠缠，同时补充分布偏见缓解。

Conclusion: BiasMap提供了一个揭示隐藏偏见的新视角，能够有效发现和缓解稳定扩散模型中的概念级表示偏见，超越了传统分布层面的偏见分析方法。

Abstract: Bias discovery is critical for black-box generative models, especiall
text-to-image (TTI) models. Existing works predominantly focus on output-level
demographic distributions, which do not necessarily guarantee concept
representations to be disentangled post-mitigation. We propose BiasMap, a
model-agnostic framework for uncovering latent concept-level representational
biases in stable diffusion models. BiasMap leverages cross-attention
attribution maps to reveal structural entanglements between demographics (e.g.,
gender, race) and semantics (e.g., professions), going deeper into
representational bias during the image generation. Using attribution maps of
these concepts, we quantify the spatial demographics-semantics concept
entanglement via Intersection over Union (IoU), offering a lens into bias that
remains hidden in existing fairness discovery approaches. In addition, we
further utilize BiasMap for bias mitigation through energy-guided diffusion
sampling that directly modifies latent noise space and minimizes the expected
SoftIoU during the denoising process. Our findings show that existing fairness
interventions may reduce the output distributional gap but often fail to
disentangle concept-level coupling, whereas our mitigation method can mitigate
concept entanglement in image generation while complementing distributional
bias mitigation.

</details>


### [15] [LivePyxel: Accelerating image annotations with a Python-integrated webcam live streaming](https://arxiv.org/abs/2509.13504)
*Uriel Garcilazo-Cruz,Joseph O. Okeme,Rodrigo A. Vargas--Hernández*

Main category: cs.CV

TL;DR: LivePixel是一个基于Python的实时图像标注工具，可与显微镜等成像系统集成，支持实时图像采集和标注，加速AI模型开发。


<details>
  <summary>Details</summary>
Motivation: 现有图像标注软件需要用户上传预收集的数据集，限制了按需流水线的支持，在实验室环境中实时数据采集日益普遍的情况下尤为不便。

Method: 开发基于Python的图形用户界面，集成OpenCV和Numpy等高性能库，支持贝塞尔曲线、二值掩码和非破坏性图层等专业标注工具，与各种视频设备兼容。

Result: 实现了实时图像标注功能，能够直接从成像设备获取图像并进行精确标注，简化了数据收集和标注流程。

Conclusion: LivePixel解决了科学领域中AI模型部署的标注工具限制问题，为实验工作流程中的AI模型开发提供了便利，软件已在GitHub上开源提供。

Abstract: The lack of flexible annotation tools has hindered the deployment of AI
models in some scientific areas. Most existing image annotation software
requires users to upload a precollected dataset, which limits support for
on-demand pipelines and introduces unnecessary steps to acquire images. This
constraint is particularly problematic in laboratory environments, where
real-time data acquisition from instruments such as microscopes is increasingly
common. In this work, we introduce \texttt{LivePixel}, a Python-based graphical
user interface that integrates with imaging systems, such as webcams,
microscopes, and others, to enable real-time image annotation. LivePyxel is
designed to be easy to use through a simple interface that allows users to
precisely delimit areas for annotation using tools commonly found in commercial
graphics editing software. Of particular interest is the availability of
B\'ezier splines and binary masks, and the software's capacity to work with
non-destructive layers that enable high-performance editing. LivePyxel also
integrates a wide compatibility across video devices, and it's optimized for
object detection operations via the use of OpenCV in combination with
high-performance libraries designed to handle matrix and linear algebra
operations via Numpy effectively. LivePyxel facilitates seamless data
collection and labeling, accelerating the development of AI models in
experimental workflows. LivePyxel freely available at
https://github.com/UGarCil/LivePyxel

</details>


### [16] [DEFT-VTON: Efficient Virtual Try-On with Consistent Generalised H-Transform](https://arxiv.org/abs/2509.13506)
*Xingzi Xu,Qi Li,Shuwen Qiu,Julien Han,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 提出DEFT-VTON方法，通过Doob's h-transform高效微调和自适应一致性损失，用仅1.42%的参数量实现高质量虚拟试穿，推理步骤减少到15步


<details>
  <summary>Details</summary>
Motivation: 解决现有虚拟试穿方法需要大量端到端训练和推理资源的问题，满足实际应用中对有限训练和部署预算的需求

Method: 使用DEFT冻结预训练模型参数，训练小型h-transform网络学习条件变换；结合自适应一致性损失和去噪得分匹配损失进行低成本微调

Result: 在虚拟试穿任务上达到state-of-the-art性能，仅需15步去噪步骤，参数量仅为传统PEFT方法的1.42%

Conclusion: DEFT-VTON方法实现了高效且高性能的虚拟试穿，显著降低了计算成本和推理时间，具有很好的实际应用价值

Abstract: Diffusion models enable high-quality virtual try-on (VTO) with their
established image synthesis abilities. Despite the extensive end-to-end
training of large pre-trained models involved in current VTO methods,
real-world applications often prioritize limited training and inference,
serving, and deployment budgets for VTO. To solve this obstacle, we apply
Doob's h-transform efficient fine-tuning (DEFT) for adapting large pre-trained
unconditional models for downstream image-conditioned VTO abilities. DEFT
freezes the pre-trained model's parameters and trains a small h-transform
network to learn a conditional h-transform. The h-transform network allows
training only 1.42 percent of the frozen parameters, compared to a baseline of
5.52 percent in traditional parameter-efficient fine-tuning (PEFT).
  To further improve DEFT's performance and decrease existing models' inference
time, we additionally propose an adaptive consistency loss. Consistency
training distills slow but high-performing diffusion models into a fast one
while retaining performance by enforcing consistencies along the inference
path. Inspired by constrained optimization, instead of distillation, we combine
the consistency loss and the denoising score matching loss in a data-adaptive
manner for fine-tuning existing VTO models at a low cost. Empirical results
show the proposed DEFT-VTON method achieves state-of-the-art performance on VTO
tasks, with as few as 15 denoising steps, while maintaining competitive
results.

</details>


### [17] [Adversarial Appearance Learning in Augmented Cityscapes for Pedestrian Recognition in Autonomous Driving](https://arxiv.org/abs/2509.13507)
*Artem Savkin,Thomas Lapotre,Kevin Strauss,Uzair Akbar,Federico Tombari*

Main category: cs.CV

TL;DR: 使用数据增强和对抗学习生成逼真的合成交通场景数据，提高行人识别性能


<details>
  <summary>Details</summary>
Motivation: 自动驾驶领域需要合成数据覆盖特定交通场景，但合成数据与真实数据存在域差距问题

Method: 开发数据增强流程，在Cityscapes数据集中添加虚拟行人，并提出新颖的生成对抗网络架构学习数据集光照条件

Result: 在语义分割和实例分割任务上评估了该方法

Conclusion: 该方法能够生成更逼真的合成数据，改善自动驾驶系统的行人识别能力

Abstract: In the autonomous driving area synthetic data is crucial for cover specific
traffic scenarios which autonomous vehicle must handle. This data commonly
introduces domain gap between synthetic and real domains. In this paper we
deploy data augmentation to generate custom traffic scenarios with VRUs in
order to improve pedestrian recognition. We provide a pipeline for augmentation
of the Cityscapes dataset with virtual pedestrians. In order to improve
augmentation realism of the pipeline we reveal a novel generative network
architecture for adversarial learning of the data-set lighting conditions. We
also evaluate our approach on the tasks of semantic and instance segmentation.

</details>


### [18] [FunKAN: Functional Kolmogorov-Arnold Network for Medical Image Enhancement and Segmentation](https://arxiv.org/abs/2509.13508)
*Maksim Penkin,Andrey Krylov*

Main category: cs.CV

TL;DR: 提出了FunKAN和U-FunKAN模型，将Kolmogorov-Arnold网络扩展到功能空间，在医学图像增强和分割任务中表现优异，超越了其他KAN基模型。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法在医学图像处理中缺乏可解释性，而现有KAN网络会破坏图像的空间结构特征，需要一种既能保持可解释性又能保留空间结构的解决方案。

Method: 提出了Functional Kolmogorov-Arnold Network (FunKAN)，通过在功能空间上泛化Kolmogorov-Arnold表示定理，使用傅里叶分解和Hermite函数基学习内部函数。

Result: 在IXI数据集上的MRI吉布斯伪影抑制任务，以及在BUSI、GlaS、CVC-ClinicDB三个医学数据集上的分割任务中，均取得了优于其他KAN基模型的性能（PSNR、TV、IoU、F1指标）。

Conclusion: 该工作成功地将理论函数逼近与医学图像分析相结合，为临床应用提供了强大且可解释的解决方案。

Abstract: Medical image enhancement and segmentation are critical yet challenging tasks
in modern clinical practice, constrained by artifacts and complex anatomical
variations. Traditional deep learning approaches often rely on complex
architectures with limited interpretability. While Kolmogorov-Arnold networks
offer interpretable solutions, their reliance on flattened feature
representations fundamentally disrupts the intrinsic spatial structure of
imaging data. To address this issue we propose a Functional Kolmogorov-Arnold
Network (FunKAN) -- a novel interpretable neural framework, designed
specifically for image processing, that formally generalizes the
Kolmogorov-Arnold representation theorem onto functional spaces and learns
inner functions using Fourier decomposition over the basis Hermite functions.
We explore FunKAN on several medical image processing tasks, including Gibbs
ringing suppression in magnetic resonance images, benchmarking on IXI dataset.
We also propose U-FunKAN as state-of-the-art binary medical segmentation model
with benchmarks on three medical datasets: BUSI (ultrasound images), GlaS
(histological structures) and CVC-ClinicDB (colonoscopy videos), detecting
breast cancer, glands and polyps, respectively. Experiments on those diverse
datasets demonstrate that our approach outperforms other KAN-based backbones in
both medical image enhancement (PSNR, TV) and segmentation (IoU, F1). Our work
bridges the gap between theoretical function approximation and medical image
analysis, offering a robust, interpretable solution for clinical applications.

</details>


### [19] [Multimodal Hate Detection Using Dual-Stream Graph Neural Networks](https://arxiv.org/abs/2509.13515)
*Jiangbei Yue,Shuonan Yang,Tailin Chen,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 提出一种新颖的多模态双流图神经网络模型，通过分离视频为多个实例并构建实例图来提取特征，利用互补权重图突出仇恨内容，在仇恨视频分类任务上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法通常忽视即使最少的仇恨内容也决定了视频类别，且无法系统捕获视频中的结构化信息，限制了多模态融合效果。

Method: 构建实例图分离视频为多个实例提取特征，使用互补权重图为特征分配重要性权重以突出仇恨实例，结合权重和特征生成视频标签，采用基于图的框架系统建模模态内和模态间结构化关系。

Result: 在公开数据集上的广泛实验表明，该模型在仇恨视频分类任务上达到最先进水平，并具有很强的可解释性。

Conclusion: 提出的多模态双流图神经网络模型有效解决了现有方法的局限性，在仇恨视频检测方面表现出色且具有解释性优势。

Abstract: Hateful videos present serious risks to online safety and real-world
well-being, necessitating effective detection methods. Although multimodal
classification approaches integrating information from several modalities
outperform unimodal ones, they typically neglect that even minimal hateful
content defines a video's category. Specifically, they generally treat all
content uniformly, instead of emphasizing the hateful components. Additionally,
existing multimodal methods cannot systematically capture structured
information in videos, limiting the effectiveness of multimodal fusion. To
address these limitations, we propose a novel multimodal dual-stream graph
neural network model. It constructs an instance graph by separating the given
video into several instances to extract instance-level features. Then, a
complementary weight graph assigns importance weights to these features,
highlighting hateful instances. Importance weights and instance features are
combined to generate video labels. Our model employs a graph-based framework to
systematically model structured relationships within and across modalities.
Extensive experiments on public datasets show that our model is
state-of-the-art in hateful video classification and has strong explainability.
Code is available:
https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.

</details>


### [20] [ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors](https://arxiv.org/abs/2509.13525)
*Romain Hardy,Tyler Berzin,Pranav Rajpurkar*

Main category: cs.CV

TL;DR: ColonCrafter是一个基于扩散模型的深度估计方法，从单目结肠镜视频生成时间一致的深度图，在C3VD数据集上实现最先进的零样本性能


<details>
  <summary>Details</summary>
Motivation: 结肠镜三维场景理解需要准确的深度估计，但现有内窥镜深度估计模型在视频序列中缺乏时间一致性，限制了3D重建的应用

Method: 使用基于扩散的深度估计模型，从合成结肠镜序列学习几何先验，并引入风格迁移技术将真实临床视频适配到合成训练域

Result: 在C3VD数据集上实现最先进的零样本性能，优于通用和内窥镜专用方法，能够生成3D点云和进行表面覆盖评估

Conclusion: 虽然完整轨迹3D重建仍是挑战，但ColonCrafter展示了在临床相关应用中的价值，包括3D点云生成和表面覆盖评估

Abstract: Three-dimensional (3D) scene understanding in colonoscopy presents
significant challenges that necessitate automated methods for accurate depth
estimation. However, existing depth estimation models for endoscopy struggle
with temporal consistency across video sequences, limiting their applicability
for 3D reconstruction. We present ColonCrafter, a diffusion-based depth
estimation model that generates temporally consistent depth maps from monocular
colonoscopy videos. Our approach learns robust geometric priors from synthetic
colonoscopy sequences to generate temporally consistent depth maps. We also
introduce a style transfer technique that preserves geometric structure while
adapting real clinical videos to match our synthetic training domain.
ColonCrafter achieves state-of-the-art zero-shot performance on the C3VD
dataset, outperforming both general-purpose and endoscopy-specific approaches.
Although full trajectory 3D reconstruction remains a challenge, we demonstrate
clinically relevant applications of ColonCrafter, including 3D point cloud
generation and surface coverage assessment.

</details>


### [21] [MemGS: Memory-Efficient Gaussian Splatting for Real-Time SLAM](https://arxiv.org/abs/2509.13536)
*Yinlong Bai,Hongxin Zhang,Sheng Zhong,Junkai Niu,Hai Li,Yijia He,Yi Zhou*

Main category: cs.CV

TL;DR: 提出了一种针对嵌入式平台的3D高斯泼溅优化方法，通过体素空间合并和补丁网格采样来减少GPU内存使用并提高渲染质量


<details>
  <summary>Details</summary>
Motivation: 当前3DGS研究主要关注高性能桌面GPU，忽略了嵌入式平台（如微型飞行器）的计算资源和内存限制，需要在系统性能和重建质量之间进行权衡

Method: 1. 在SLAM中基于几何相似性在体素空间合并冗余的3D高斯基元；2. 通过补丁网格点采样初始化3D高斯基元来更精确地建模整个场景

Result: 在公开数据集上的定量和定性评估证明了改进的有效性，减少了GPU内存使用且不影响系统运行时性能

Conclusion: 该方法成功解决了嵌入式平台在3D高斯泼溅应用中的内存和性能挑战，在保持实时性能的同时提升了渲染质量

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have made a significant
impact on rendering and reconstruction techniques. Current research
predominantly focuses on improving rendering performance and reconstruction
quality using high-performance desktop GPUs, largely overlooking applications
for embedded platforms like micro air vehicles (MAVs). These devices, with
their limited computational resources and memory, often face a trade-off
between system performance and reconstruction quality. In this paper, we
improve existing methods in terms of GPU memory usage while enhancing rendering
quality. Specifically, to address redundant 3D Gaussian primitives in SLAM, we
propose merging them in voxel space based on geometric similarity. This reduces
GPU memory usage without impacting system runtime performance. Furthermore,
rendering quality is improved by initializing 3D Gaussian primitives via
Patch-Grid (PG) point sampling, enabling more accurate modeling of the entire
scene. Quantitative and qualitative evaluations on publicly available datasets
demonstrate the effectiveness of our improvements.

</details>


### [22] [Dynamic Aware: Adaptive Multi-Mode Out-of-Distribution Detection for Trajectory Prediction in Autonomous Vehicles](https://arxiv.org/abs/2509.13577)
*Tongfei Guo,Lili Su*

Main category: cs.CV

TL;DR: 提出了一个自适应轨迹级OOD检测框架，通过显式建模预测误差的模式依赖性，在检测延迟和误报率方面显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆在部署中面临训练数据与现实条件之间的分布偏移，现有OOD检测研究主要集中在计算机视觉任务，轨迹级OOD检测研究不足

Method: 基于快速变化检测(QCD)任务构建新框架，引入自适应机制，显式建模预测误差的模式依赖性分布及其随时间演化的特性

Result: 在多个真实数据集上的实验表明，该方法在检测延迟和误报率方面取得显著改进，在准确性和计算效率上显著优于现有UQ和基于视觉的OOD方法

Conclusion: 该框架为实现可靠、驾驶感知的自主性提供了实用路径，能够有效处理复杂驾驶环境中的分布偏移问题

Abstract: Trajectory prediction is central to the safe and seamless operation of
autonomous vehicles (AVs). In deployment, however, prediction models inevitably
face distribution shifts between training data and real-world conditions, where
rare or underrepresented traffic scenarios induce out-of-distribution (OOD)
cases. While most prior OOD detection research in AVs has concentrated on
computer vision tasks such as object detection and segmentation,
trajectory-level OOD detection remains largely underexplored. A recent study
formulated this problem as a quickest change detection (QCD) task, providing
formal guarantees on the trade-off between detection delay and false alarms
[1]. Building on this foundation, we propose a new framework that introduces
adaptive mechanisms to achieve robust detection in complex driving
environments. Empirical analysis across multiple real-world datasets reveals
that prediction errors -- even on in-distribution samples -- exhibit
mode-dependent distributions that evolve over time with dataset-specific
dynamics. By explicitly modeling these error modes, our method achieves
substantial improvements in both detection delay and false alarm rates.
Comprehensive experiments on established trajectory prediction benchmarks show
that our framework significantly outperforms prior UQ- and vision-based OOD
approaches in both accuracy and computational efficiency, offering a practical
path toward reliable, driving-aware autonomy.

</details>


### [23] [Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection](https://arxiv.org/abs/2509.13586)
*Nathalie Neptune,Josiane Mothe*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的方法，利用卫星图像对检测亚马逊雨林砍伐，并通过视觉语义模型自动标注检测到的变化。


<details>
  <summary>Details</summary>
Motivation: 亚马逊雨林是全球重要的生态系统，森林砍伐对碳排放和生物多样性有重大影响，需要有效的监测方法。

Method: 使用深度学习技术比较不同时间点的卫星图像对，检测森林覆盖变化，并提出视觉语义模型从科学文献中提取关键词自动标注变化。

Result: 在亚马逊图像对数据集上验证了方法的有效性，能够准确检测砍伐并生成相关标注。

Conclusion: 该方法为监测和研究亚马逊砍伐影响提供了有用工具，且具有通用性可应用于其他领域。

Abstract: The Amazon rain forest is a vital ecosystem that plays a crucial role in
regulating the Earth's climate and providing habitat for countless species.
Deforestation in the Amazon is a major concern as it has a significant impact
on global carbon emissions and biodiversity. In this paper, we present a method
for detecting deforestation in the Amazon using image pairs from Earth
observation satellites. Our method leverages deep learning techniques to
compare the images of the same area at different dates and identify changes in
the forest cover. We also propose a visual semantic model that automatically
annotates the detected changes with relevant keywords. The candidate annotation
for images are extracted from scientific documents related to the Amazon
region. We evaluate our approach on a dataset of Amazon image pairs and
demonstrate its effectiveness in detecting deforestation and generating
relevant annotations. Our method provides a useful tool for monitoring and
studying the impact of deforestation in the Amazon. While we focus on
environment applications of our work by using images of deforestation in the
Amazon rain forest to demonstrate the effectiveness of our proposed approach,
it is generic enough to be applied to other domains.

</details>


### [24] [Intelligent Healthcare Imaging Platform An VLM-Based Framework for Automated Medical Image Analysis and Clinical Report Generation](https://arxiv.org/abs/2509.13590)
*Samer Al-Hamadani*

Main category: cs.CV

TL;DR: 基于Google Gemini 2.5 Flash的多模态医疗影像分析框架，整合视觉和语言处理实现自动化肿瘤检测和临床报告生成，支持多种影像模态并具备零样本学习能力。


<details>
  <summary>Details</summary>
Motivation: 利用AI技术提升医疗影像诊断效率和准确性，解决传统诊断方法对大量标注数据的依赖问题，改善放射科工作流程。

Method: 采用Vision-Language Models整合视觉特征提取和自然语言处理，结合坐标验证机制和高斯概率建模进行异常分布分析，使用多层可视化技术生成医学图示。

Result: 在多种影像模态上实现高性能异常检测，位置测量平均偏差80像素，通过精确提示工程提取结构化临床信息，具备零样本学习能力。

Conclusion: 该框架在自动化诊断支持和放射科工作流程效率方面取得显著进展，但需要进行临床验证和多中心评估才能广泛采用。

Abstract: The rapid advancement of artificial intelligence (AI) in healthcare imaging
has revolutionized diagnostic medicine and clinical decision-making processes.
This work presents an intelligent multimodal framework for medical image
analysis that leverages Vision-Language Models (VLMs) in healthcare
diagnostics. The framework integrates Google Gemini 2.5 Flash for automated
tumor detection and clinical report generation across multiple imaging
modalities including CT, MRI, X-ray, and Ultrasound. The system combines visual
feature extraction with natural language processing to enable contextual image
interpretation, incorporating coordinate verification mechanisms and
probabilistic Gaussian modeling for anomaly distribution. Multi-layered
visualization techniques generate detailed medical illustrations, overlay
comparisons, and statistical representations to enhance clinical confidence,
with location measurement achieving 80 pixels average deviation. Result
processing utilizes precise prompt engineering and textual analysis to extract
structured clinical information while maintaining interpretability.
Experimental evaluations demonstrated high performance in anomaly detection
across multiple modalities. The system features a user-friendly Gradio
interface for clinical workflow integration and demonstrates zero-shot learning
capabilities to reduce dependence on large datasets. This framework represents
a significant advancement in automated diagnostic support and radiological
workflow efficiency, though clinical validation and multi-center evaluation are
necessary prior to widespread adoption.

</details>


### [25] [A Generalization of CLAP from 3D Localization to Image Processing, A Connection With RANSAC & Hough Transforms](https://arxiv.org/abs/2509.13605)
*Ruochen Hou,Gabriel I. Fernandez,Alex Xu,Dennis W. Hong*

Main category: cs.CV

TL;DR: CLAP算法从2D定位扩展到3D定位和图像拼接的通用框架，展示了与RANSAC和霍夫变换的关系，为处理噪声和不确定性提供新工具


<details>
  <summary>Details</summary>
Motivation: 扩展CLAP算法到更广泛的应用领域，建立与传统方法（RANSAC、霍夫变换）的理论联系，提供更通用的噪声处理解决方案

Method: 将基于聚类的CLAP算法从2D定位推广到3D定位和图像拼接，通过聚类抑制噪声和错误特征匹配

Result: 成功将CLAP算法扩展到3D领域，建立了与RANSAC和霍夫变换的理论联系，证明了其在多领域的适用性

Conclusion: CLAP的通用化框架为处理各种噪声和不确定性提供了有效工具，在计算机视觉和机器人领域具有广泛应用前景

Abstract: In previous work, we introduced a 2D localization algorithm called CLAP,
Clustering to Localize Across $n$ Possibilities, which was used during our
championship win in RoboCup 2024, an international autonomous humanoid soccer
competition. CLAP is particularly recognized for its robustness against
outliers, where clustering is employed to suppress noise and mitigate against
erroneous feature matches. This clustering-based strategy provides an
alternative to traditional outlier rejection schemes such as RANSAC, in which
candidates are validated by reprojection error across all data points. In this
paper, CLAP is extended to a more general framework beyond 2D localization,
specifically to 3D localization and image stitching. We also show how CLAP,
RANSAC, and Hough transforms are related. The generalization of CLAP is widely
applicable to many different fields and can be a useful tool to deal with noise
and uncertainty.

</details>


### [26] [SAMIR, an efficient registration framework via robust feature learning from SAM](https://arxiv.org/abs/2509.13629)
*Yue He,Min Liu,Qinghao Liu,Jiazheng Wang,Yaonan Wang,Hang Zhang,Xiang Chen*

Main category: cs.CV

TL;DR: SAMIR是一个利用Segment Anything Model (SAM)增强特征提取的医学图像配准框架，通过在SAM编码器基础上设计任务特定的适应管道和轻量级3D头部，显著提升了心脏和腹部CT图像的配准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的弱监督方法需要解剖学先验信息（如分割掩码或标志点），但这些弱标签通常难以获取，限制了实际应用。受视觉基础模型强大表示学习能力的启发，作者希望利用预训练的SAM模型来改进医学图像配准中的特征提取。

Method: 设计了基于SAM图像编码器的任务特定适应管道来提取结构感知的特征嵌入，使用轻量级3D头部在嵌入空间中细化特征以适应局部变形，并引入了分层特征一致性损失来指导从粗到细的特征匹配。

Result: 在基准数据集上的广泛实验表明，SAMIR在心脏图像配准上性能提升2.68%（ACDC数据集），在腹部CT图像配准上提升6.44%，显著优于现有最先进方法。

Conclusion: SAMIR框架通过有效利用预训练的视觉基础模型SAM，在没有额外弱监督标签的情况下实现了医学图像配准的显著性能提升，证明了基础模型在医学图像分析任务中的巨大潜力。

Abstract: Image registration is a fundamental task in medical image analysis.
Deformations are often closely related to the morphological characteristics of
tissues, making accurate feature extraction crucial. Recent weakly supervised
methods improve registration by incorporating anatomical priors such as
segmentation masks or landmarks, either as inputs or in the loss function.
However, such weak labels are often not readily available, limiting their
practical use. Motivated by the strong representation learning ability of
visual foundation models, this paper introduces SAMIR, an efficient medical
image registration framework that utilizes the Segment Anything Model (SAM) to
enhance feature extraction. SAM is pretrained on large-scale natural image
datasets and can learn robust, general-purpose visual representations. Rather
than using raw input images, we design a task-specific adaptation pipeline
using SAM's image encoder to extract structure-aware feature embeddings,
enabling more accurate modeling of anatomical consistency and deformation
patterns. We further design a lightweight 3D head to refine features within the
embedding space, adapting to local deformations in medical images.
Additionally, we introduce a Hierarchical Feature Consistency Loss to guide
coarse-to-fine feature matching and improve anatomical alignment. Extensive
experiments demonstrate that SAMIR significantly outperforms state-of-the-art
methods on benchmark datasets for both intra-subject cardiac image registration
and inter-subject abdomen CT image registration, achieving performance
improvements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source code
will be publicly available on GitHub following the acceptance of this paper.

</details>


### [27] [Federated Learning for Deforestation Detection: A Distributed Approach with Satellite Imagery](https://arxiv.org/abs/2509.13631)
*Yuvraj Dutta,Aaditya Sikder,Basabdatta Palit*

Main category: cs.CV

TL;DR: 本文提出了一种基于联邦学习(FL)的分布式方法，利用卫星图像识别和定位森林砍伐，通过FLOWER和RAY框架实现分布式训练，在保护数据隐私的同时完成目标检测任务。


<details>
  <summary>Details</summary>
Motivation: 传统集中式训练方法需要合并数据，会危及客户端数据安全。卫星图像处理需要分布式协作，同时保护各地卫星中心的数据隐私和安全。

Method: 使用联邦学习框架，结合FLOWER和RAY框架进行分布式学习。采用YOLOS-small、Faster R-CNN with ResNet50和Faster R-CNN with MobileNetV3三种模型，在公开数据集上进行训练和测试。

Result: 开发了一个能够识别和定位森林砍伐的分布式系统，实现了数据隐私保护下的协作训练，为卫星图像分割任务提供了新的解决方案视角。

Conclusion: 联邦学习为卫星图像分析提供了一种有效的分布式学习方法，在保护数据安全的同时实现了准确的森林砍伐检测，为类似的地理空间分析任务提供了可行的技术路径。

Abstract: Accurate identification of deforestation from satellite images is essential
in order to understand the geographical situation of an area. This paper
introduces a new distributed approach to identify as well as locate
deforestation across different clients using Federated Learning (FL). Federated
Learning enables distributed network clients to collaboratively train a model
while maintaining data privacy and security of the active users. In our
framework, a client corresponds to an edge satellite center responsible for
local data processing. Moreover, FL provides an advantage over centralized
training method which requires combining data, thereby compromising with data
security of the clients. Our framework leverages the FLOWER framework with RAY
framework to execute the distributed learning workload. Furthermore, efficient
client spawning is ensured by RAY as it can select definite amount of users to
create an emulation environment. Our FL framework uses YOLOS-small (a Vision
Transformer variant), Faster R-CNN with a ResNet50 backbone, and Faster R-CNN
with a MobileNetV3 backbone models trained and tested on publicly available
datasets. Our approach provides us a different view for image
segmentation-based tasks on satellite imagery.

</details>


### [28] [Gaussian Alignment for Relative Camera Pose Estimation via Single-View Reconstruction](https://arxiv.org/abs/2509.13652)
*Yumin Li,Dylan Campbell*

Main category: cs.CV

TL;DR: GARPS是一种无需训练的两视图相对位姿估计框架，通过直接对齐两个独立重建的3D高斯混合模型来实现度量尺度的相机位姿估计


<details>
  <summary>Details</summary>
Motivation: 解决传统两视图位姿估计方法无法恢复度量尺度、在宽基线和纹理缺失区域表现不佳的问题

Method: 使用度量单目深度估计器和高斯场景重建器为每张图像构建度量3D GMM，然后通过优化可微分的GMM对齐目标来精化初始位姿

Result: 在Real-Estate10K数据集上超越经典方法和最先进的学习方法，包括MASt3R

Conclusion: 将单视图感知与多视图几何相结合，能够实现鲁棒且度量的相对位姿估计

Abstract: Estimating metric relative camera pose from a pair of images is of great
importance for 3D reconstruction and localisation. However, conventional
two-view pose estimation methods are not metric, with camera translation known
only up to a scale, and struggle with wide baselines and textureless or
reflective surfaces. This paper introduces GARPS, a training-free framework
that casts this problem as the direct alignment of two independently
reconstructed 3D scenes. GARPS leverages a metric monocular depth estimator and
a Gaussian scene reconstructor to obtain a metric 3D Gaussian Mixture Model
(GMM) for each image. It then refines an initial pose from a feed-forward
two-view pose estimator by optimising a differentiable GMM alignment objective.
This objective jointly considers geometric structure, view-independent colour,
anisotropic covariance, and semantic feature consistency, and is robust to
occlusions and texture-poor regions without requiring explicit 2D
correspondences. Extensive experiments on the Real\-Estate10K dataset
demonstrate that GARPS outperforms both classical and state-of-the-art
learning-based methods, including MASt3R. These results highlight the potential
of bridging single-view perception with multi-view geometry to achieve robust
and metric relative pose estimation.

</details>


### [29] [Deep Lookup Network](https://arxiv.org/abs/2509.13662)
*Yulan Guo,Longguang Wang,Wendong Mao,Xiaoyu Dong,Yingqian Wang,Li Liu,Wei An*

Main category: cs.CV

TL;DR: 提出一种基于查找表的高效神经网络操作，用查找操作替代计算密集的乘法运算，在保持性能的同时显著提升能效和推理速度


<details>
  <summary>Details</summary>
Motivation: 卷积神经网络中的乘法运算计算复杂度高、能耗大，阻碍了在移动设备上的部署。资源受限的边缘设备通常使用查找表来降低计算成本

Method: 构建可微分的查找表，提出多种训练策略实现端到端优化，用查找操作替代权重和激活值的乘法运算

Result: 在图像分类、超分辨率和点云分类任务中，查找网络在能耗和推理速度方面效率更高，同时保持与原始卷积网络相当的性能

Conclusion: 查找网络在不同任务（分类和回归）和数据类型（图像和点云）上均能实现最先进的性能，为移动设备部署提供了高效解决方案

Abstract: Convolutional neural networks are constructed with massive operations with
different types and are highly computationally intensive. Among these
operations, multiplication operation is higher in computational complexity and
usually requires {more} energy consumption with longer inference time than
other operations, which hinders the deployment of convolutional neural networks
on mobile devices. In many resource-limited edge devices, complicated
operations can be calculated via lookup tables to reduce computational cost.
Motivated by this, in this paper, we introduce a generic and efficient lookup
operation which can be used as a basic operation for the construction of neural
networks. Instead of calculating the multiplication of weights and activation
values, simple yet efficient lookup operations are adopted to compute their
responses. To enable end-to-end optimization of the lookup operation, we
construct the lookup tables in a differentiable manner and propose several
training strategies to promote their convergence. By replacing computationally
expensive multiplication operations with our lookup operations, we develop
lookup networks for the image classification, image super-resolution, and point
cloud classification tasks. It is demonstrated that our lookup networks can
benefit from the lookup operations to achieve higher efficiency in terms of
energy consumption and inference speed while maintaining competitive
performance to vanilla convolutional networks. Extensive experiments show that
our lookup networks produce state-of-the-art performance on different tasks
(both classification and regression tasks) and different data types (both
images and point clouds).

</details>


### [30] [Re-purposing SAM into Efficient Visual Projectors for MLLM-Based Referring Image Segmentation](https://arxiv.org/abs/2509.13676)
*Xiaobo Yang,Xiaojin Gong*

Main category: cs.CV

TL;DR: 提出基于SAM语义超像素的视觉投影器，将视觉token减少93%而不损失性能，显著加速MLLM训练和推理


<details>
  <summary>Details</summary>
Motivation: 传统patch-wise视觉投影器在减少视觉token数量和保持语义清晰度之间难以平衡，往往需要保留过长的token序列以避免性能下降

Method: 利用SAM生成的语义超像素识别"视觉词汇"，通过压缩和投影语义超像素作为视觉token，并引入语义超像素位置嵌入和聚合器来保持细节和全局上下文

Result: 实验显示该方法将视觉token减少93%且不损害性能，显著加速MLLM训练和推理，在RIS任务上优于现有压缩视觉投影器

Conclusion: 基于语义超像素的视觉投影方法能有效解决视觉token冗余问题，在保持性能的同时大幅提升效率

Abstract: Recently, Referring Image Segmentation (RIS) frameworks that pair the
Multimodal Large Language Model (MLLM) with the Segment Anything Model (SAM)
have achieved impressive results. However, adapting MLLM to segmentation is
computationally intensive, primarily due to visual token redundancy. We observe
that traditional patch-wise visual projectors struggle to strike a balance
between reducing the number of visual tokens and preserving semantic clarity,
often retaining overly long token sequences to avoid performance drops.
Inspired by text tokenizers, we propose a novel semantic visual projector that
leverages semantic superpixels generated by SAM to identify "visual words" in
an image. By compressing and projecting semantic superpixels as visual tokens,
our approach adaptively shortens the token sequence according to scene
complexity while minimizing semantic loss in compression. To mitigate loss of
information, we propose a semantic superpixel positional embedding to
strengthen MLLM's awareness of superpixel geometry and position, alongside a
semantic superpixel aggregator to preserve both fine-grained details inside
superpixels and global context outside. Experiments show that our method cuts
visual tokens by 93% without compromising performance, notably speeding up MLLM
training and inference, and outperforming existing compressive visual
projectors on RIS.

</details>


### [31] [FishBEV: Distortion-Resilient Bird's Eye View Segmentation with Surround-View Fisheye Cameras](https://arxiv.org/abs/2509.13681)
*Hang Li,Dianmo Sheng,Qiankun Dong,Zichun Wang,Zhiwei Xu,Tao Li*

Main category: cs.CV

TL;DR: FishBEV是一个专门针对鱼眼相机设计的BEV分割框架，通过三个创新模块解决鱼眼相机的几何畸变、多视图对应模糊和时间动态不稳定问题，在Synwoodscapes数据集上表现优于现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 现有的BEV分割方法主要针对针孔相机设计，难以直接应用于存在严重几何畸变、多视图对应关系模糊和时间动态不稳定的鱼眼相机，这些因素显著降低了BEV分割性能。

Method: 提出FishBEV框架，包含三个创新模块：1) 抗畸变多尺度提取(DRME)主干网络，在畸变下学习鲁棒特征并保持尺度一致性；2) 不确定性感知空间交叉注意力(U-SCA)机制，利用不确定性估计实现可靠的跨视图对齐；3) 距离感知时间自注意力(D-TSA)模块，自适应平衡近场细节和远场上下文以确保时间一致性。

Result: 在Synwoodscapes数据集上的大量实验表明，FishBEV在环视鱼眼BEV分割任务上持续优于现有的最优基线方法。

Conclusion: FishBEV通过专门针对鱼眼相机特性设计的三个互补创新模块，有效解决了鱼眼相机BEV分割中的关键挑战，实现了优异的性能表现。

Abstract: As a cornerstone technique for autonomous driving, Bird's Eye View (BEV)
segmentation has recently achieved remarkable progress with pinhole cameras.
However, it is non-trivial to extend the existing methods to fisheye cameras
with severe geometric distortion, ambiguous multi-view correspondences and
unstable temporal dynamics, all of which significantly degrade BEV performance.
To address these challenges, we propose FishBEV, a novel BEV segmentation
framework specifically tailored for fisheye cameras. This framework introduces
three complementary innovations, including a Distortion-Resilient Multi-scale
Extraction (DRME) backbone that learns robust features under distortion while
preserving scale consistency, an Uncertainty-aware Spatial Cross-Attention
(U-SCA) mechanism that leverages uncertainty estimation for reliable cross-view
alignment, a Distance-aware Temporal Self-Attention (D-TSA) module that
adaptively balances near field details and far field context to ensure temporal
coherence. Extensive experiments on the Synwoodscapes dataset demonstrate that
FishBEV consistently outperforms SOTA baselines, regarding the performance
evaluation of FishBEV on the surround-view fisheye BEV segmentation tasks.

</details>


### [32] [Taylor-Series Expanded Kolmogorov-Arnold Network for Medical Imaging Classification](https://arxiv.org/abs/2509.13687)
*Kaniz Fatema,Emad A. Mohammed,Sukhjit Singh Sehra*

Main category: cs.CV

TL;DR: 该研究提出基于样条的KAN网络用于医学图像分类，在有限数据集下实现了高精度分类，参数量远少于传统CNN，具有轻量化和可解释性优势。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分类在资源受限临床环境中的挑战，特别是在数据集有限且多样的情况下，需要轻量、可解释且泛化能力强的模型。

Method: 开发了三种基于样条的KAN变体：SBTAYLOR-KAN（B样条+泰勒级数）、SBRBF-KAN（B样条+径向基函数）、SBWAVELET-KAN（B样条+Morlet小波变换），利用样条函数逼近捕捉局部和全局非线性特征。

Result: SBTAYLOR-KAN在多个医学图像数据集上达到98.93%准确率，仅用2872个参数即可媲美ResNet50（2418万参数）的性能，在仅使用30%训练数据时仍保持86%以上准确率，并在类别不平衡情况下表现优异。

Conclusion: 该框架为医学图像分类提供了轻量、可解释且泛化性强的解决方案，特别适合数据稀缺的临床AI应用场景。

Abstract: Effective and interpretable classification of medical images is a challenge
in computer-aided diagnosis, especially in resource-limited clinical settings.
This study introduces spline-based Kolmogorov-Arnold Networks (KANs) for
accurate medical image classification with limited, diverse datasets. The
models include SBTAYLOR-KAN, integrating B-splines with Taylor series;
SBRBF-KAN, combining B-splines with Radial Basis Functions; and SBWAVELET-KAN,
embedding B-splines in Morlet wavelet transforms. These approaches leverage
spline-based function approximation to capture both local and global
nonlinearities. The models were evaluated on brain MRI, chest X-rays,
tuberculosis X-rays, and skin lesion images without preprocessing,
demonstrating the ability to learn directly from raw data. Extensive
experiments, including cross-dataset validation and data reduction analysis,
showed strong generalization and stability. SBTAYLOR-KAN achieved up to 98.93%
accuracy, with a balanced F1-score, maintaining over 86% accuracy using only
30% of the training data across three datasets. Despite class imbalance in the
skin cancer dataset, experiments on both imbalanced and balanced versions
showed SBTAYLOR-KAN outperforming other models, achieving 68.22% accuracy.
Unlike traditional CNNs, which require millions of parameters (e.g., ResNet50
with 24.18M), SBTAYLOR-KAN achieves comparable performance with just 2,872
trainable parameters, making it more suitable for constrained medical
environments. Gradient-weighted Class Activation Mapping (Grad-CAM) was used
for interpretability, highlighting relevant regions in medical images. This
framework provides a lightweight, interpretable, and generalizable solution for
medical image classification, addressing the challenges of limited datasets and
data-scarce scenarios in clinical AI applications.

</details>


### [33] [StyleProtect: Safeguarding Artistic Identity in Fine-tuned Diffusion Models](https://arxiv.org/abs/2509.13711)
*Qiuyu Tang,Joshua Krinsky,Aparna Bharati*

Main category: cs.CV

TL;DR: 提出了StyleProtect方法，通过选择性地更新扩散模型中敏感的艺术风格相关交叉注意力层，有效防御针对艺术作品的风格模仿攻击。


<details>
  <summary>Details</summary>
Motivation: 生成模型特别是扩散模型的快速发展，使得恶意使用者能够低成本地复制艺术家的独特风格，侵犯艺术家的创作劳动和个人视觉表达，因此需要保护艺术作品免受风格模仿。

Method: 识别扩散模型中与艺术风格高度相关的交叉注意力层，通过测量注意力层对风格和内容表示的激活强度，以及它们与外部模型提取特征的相关性来确定敏感性。基于此设计轻量级的StyleProtect保护策略，仅更新选定的交叉注意力层。

Result: 在基于WikiArt的30位艺术家作品数据集和Anita卡通动画数据集上的实验表明，该方法能有效保护独特艺术风格和动画风格免受恶意扩散模型定制攻击，同时保持良好的不可感知性。

Conclusion: StyleProtect提供了一种高效轻量的解决方案，通过针对性地修改扩散模型的关键注意力层，成功实现了对艺术作品风格模仿的有效防御。

Abstract: The rapid advancement of generative models, particularly diffusion-based
approaches, has inadvertently facilitated their potential for misuse. Such
models enable malicious exploiters to replicate artistic styles that capture an
artist's creative labor, personal vision, and years of dedication in an
inexpensive manner. This has led to a rise in the need and exploration of
methods for protecting artworks against style mimicry. Although generic
diffusion models can easily mimic an artistic style, finetuning amplifies this
capability, enabling the model to internalize and reproduce the style with
higher fidelity and control. We hypothesize that certain cross-attention layers
exhibit heightened sensitivity to artistic styles. Sensitivity is measured
through activation strengths of attention layers in response to style and
content representations, and assessing their correlations with features
extracted from external models. Based on our findings, we introduce an
efficient and lightweight protection strategy, StyleProtect, that achieves
effective style defense against fine-tuned diffusion models by updating only
selected cross-attention layers. Our experiments utilize a carefully curated
artwork dataset based on WikiArt, comprising representative works from 30
artists known for their distinctive and influential styles and cartoon
animations from the Anita dataset. The proposed method demonstrates promising
performance in safeguarding unique styles of artworks and anime from malicious
diffusion customization, while maintaining competitive imperceptibility.

</details>


### [34] [UM-Depth : Uncertainty Masked Self-Supervised Monocular Depth Estimation with Visual Odometry](https://arxiv.org/abs/2509.13713)
*Tae-Wook Um,Ki-Hyeon Kim,Hyun-Duck Choi,Hyo-Sung Ahn*

Main category: cs.CV

TL;DR: UM-Depth是一个自监督单目深度估计框架，通过运动感知和不确定性感知的细化方法，在动态物体边界和纹理缺失区域提升深度估计精度，无需额外标注和运行时开销。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督单目深度估计方法在处理低纹理或动态区域时存在不确定性，导致深度精度下降，需要解决这些挑战。

Method: 提出教师-学生训练策略，将不确定性估计嵌入训练流程和网络架构中，仅使用光流在教师网络中进行训练，无需额外标注和运行时成本。

Result: 在KITTI和Cityscapes数据集上的广泛实验表明，该方法在不确定性感知细化方面有效，在KITTI数据集上实现了自监督深度和姿态估计的最先进结果。

Conclusion: UM-Depth框架通过结合运动感知和不确定性感知的细化，有效提升了自监督单目深度估计在挑战性场景下的性能，且不增加推理时的计算开销。

Abstract: Monocular depth estimation has been increasingly adopted in robotics and
autonomous driving for its ability to infer scene geometry from a single
camera. In self-supervised monocular depth estimation frameworks, the network
jointly generates and exploits depth and pose estimates during training,
thereby eliminating the need for depth labels. However, these methods remain
challenged by uncertainty in the input data, such as low-texture or dynamic
regions, which can cause reduced depth accuracy. To address this, we introduce
UM-Depth, a framework that combines motion- and uncertainty-aware refinement to
enhance depth accuracy at dynamic object boundaries and in textureless regions.
Specifically, we develop a teacherstudent training strategy that embeds
uncertainty estimation into both the training pipeline and network
architecture, thereby strengthening supervision where photometric signals are
weak. Unlike prior motion-aware approaches that incur inference-time overhead
and rely on additional labels or auxiliary networks for real-time generation,
our method uses optical flow exclusively within the teacher network during
training, which eliminating extra labeling demands and any runtime cost.
Extensive experiments on the KITTI and Cityscapes datasets demonstrate the
effectiveness of our uncertainty-aware refinement. Overall, UM-Depth achieves
state-of-the-art results in both self-supervised depth and pose estimation on
the KITTI datasets.

</details>


### [35] [Mitigating Query Selection Bias in Referring Video Object Segmentation](https://arxiv.org/abs/2509.13722)
*Dingwei Zhang,Dong Zhang,Jinhui Tang*

Main category: cs.CV

TL;DR: 提出了Triple Query Former (TQF)方法，通过将参考查询分解为三个专门组件来解决现有方法中的查询选择偏差问题，显著提升了Referring Video Object Segmentation的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于查询的RVOS方法使用静态文本查询进行跨模态对齐，容易被外观或运动相似的干扰物误导，导致查询选择偏差问题。

Method: 将参考查询分解为三个专门组件：外观查询（静态属性）、帧内交互查询（空间关系）和帧间运动查询（时间关联）；动态构建查询整合语言线索和视觉引导；引入两个运动感知聚合模块增强对象标记表示。

Result: 在多个RVOS基准测试上的广泛实验证明了TQF的优势以及结构化查询设计和运动感知聚合模块的有效性。

Conclusion: TQF通过结构化查询分解和运动感知聚合机制有效解决了查询选择偏差问题，为RVOS任务提供了更准确和鲁棒的解决方案。

Abstract: Recently, query-based methods have achieved remarkable performance in
Referring Video Object Segmentation (RVOS) by using textual static object
queries to drive cross-modal alignment. However, these static queries are
easily misled by distractors with similar appearance or motion, resulting in
\emph{query selection bias}. To address this issue, we propose Triple Query
Former (TQF), which factorizes the referring query into three specialized
components: an appearance query for static attributes, an intra-frame
interaction query for spatial relations, and an inter-frame motion query for
temporal association. Instead of relying solely on textual embeddings, our
queries are dynamically constructed by integrating both linguistic cues and
visual guidance. Furthermore, we introduce two motion-aware aggregation modules
that enhance object token representations: Intra-frame Interaction Aggregation
incorporates position-aware interactions among objects within a single frame,
while Inter-frame Motion Aggregation leverages trajectory-guided alignment
across frames to ensure temporal coherence. Extensive experiments on multiple
RVOS benchmarks demonstrate the advantages of TQF and the effectiveness of our
structured query design and motion-aware aggregation modules.

</details>


### [36] [Improving Generalized Visual Grounding with Instance-aware Joint Learning](https://arxiv.org/abs/2509.13747)
*Ming Dai,Wenxuan Cheng,Jiang-Jiang Liu,Lingfeng Yang,Zhenhua Feng,Wankou Yang,Jingdong Wang*

Main category: cs.CV

TL;DR: InstanceVG是一个多任务广义视觉定位框架，首次同时处理GREC和GRES任务，通过实例查询统一实例级边界框和掩码的一致性预测，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将广义视觉定位任务（GREC和GRES）独立处理，忽略了联合训练的好处，且当前GRES方法缺乏实例感知能力和实例级预测一致性。

Method: 提出InstanceVG框架，利用实例查询统一实例级边界框和掩码的联合一致性预测，为每个实例查询分配先验参考点作为目标匹配的额外基础。

Result: 在10个数据集、4个任务上的广泛实验表明，InstanceVG在各项评估指标上显著超越现有方法，达到最先进性能。

Conclusion: InstanceVG是首个同时处理GREC和GRES并融入实例感知能力的广义视觉定位框架，通过一致性多粒度预测实现了优异性能。

Abstract: Generalized visual grounding tasks, including Generalized Referring
Expression Comprehension (GREC) and Segmentation (GRES), extend the classical
visual grounding paradigm by accommodating multi-target and non-target
scenarios. Specifically, GREC focuses on accurately identifying all referential
objects at the coarse bounding box level, while GRES aims for achieve
fine-grained pixel-level perception. However, existing approaches typically
treat these tasks independently, overlooking the benefits of jointly training
GREC and GRES to ensure consistent multi-granularity predictions and streamline
the overall process. Moreover, current methods often treat GRES as a semantic
segmentation task, neglecting the crucial role of instance-aware capabilities
and the necessity of ensuring consistent predictions between instance-level
boxes and masks. To address these limitations, we propose InstanceVG, a
multi-task generalized visual grounding framework equipped with instance-aware
capabilities, which leverages instance queries to unify the joint and
consistency predictions of instance-level boxes and masks. To the best of our
knowledge, InstanceVG is the first framework to simultaneously tackle both GREC
and GRES while incorporating instance-aware capabilities into generalized
visual grounding. To instantiate the framework, we assign each instance query a
prior reference point, which also serves as an additional basis for target
matching. This design facilitates consistent predictions of points, boxes, and
masks for the same instance. Extensive experiments obtained on ten datasets
across four tasks demonstrate that InstanceVG achieves state-of-the-art
performance, significantly surpassing the existing methods in various
evaluation metrics. The code and model will be publicly available at
https://github.com/Dmmm1997/InstanceVG.

</details>


### [37] [Cross-modal Full-mode Fine-grained Alignment for Text-to-Image Person Retrieval](https://arxiv.org/abs/2509.13754)
*Hao Yin,Xin Man,Feiyu Chen,Jie Shao,Heng Tao Shen*

Main category: cs.CV

TL;DR: FMFA框架通过显式细粒度对齐和自适应相似度分布匹配，解决了文本-图像行人检索中的跨模态对齐问题，实现了全局匹配的优化。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖注意力机制进行隐式局部对齐，但无法验证所有局部特征是否正确对齐，且主要关注困难负样本而忽略错误匹配的正样本对。

Method: 提出FMFA框架，包含自适应相似度分布匹配(A-SDM)模块来修正未匹配的正样本对，以及显式细粒度对齐(EFA)模块通过稀疏化相似度矩阵和硬编码方法加强显式跨模态细粒度交互。

Result: 在三个公开数据集上评估，在所有全局匹配方法中达到了最先进的性能。

Conclusion: FMFA通过全模式细粒度对齐，在不增加额外监督的情况下，有效提升了文本-图像行人检索的跨模态对齐效果。

Abstract: Text-to-Image Person Retrieval (TIPR) is a cross-modal matching task that
aims to retrieve the most relevant person images based on a given text query.
The key challenge in TIPR lies in achieving effective alignment between textual
and visual modalities within a common latent space. To address this challenge,
prior approaches incorporate attention mechanisms for implicit cross-modal
local alignment. However, they lack the ability to verify whether all local
features are correctly aligned. Moreover, existing methods primarily focus on
hard negative samples during model updates, with the goal of refining
distinctions between positive and negative pairs, often neglecting incorrectly
matched positive pairs. To alleviate these issues, we propose FMFA, a
cross-modal Full-Mode Fine-grained Alignment framework, which enhances global
matching through explicit fine-grained alignment and existing implicit
relational reasoning -- hence the term ``full-mode" -- without requiring
additional supervision. Specifically, we design an Adaptive Similarity
Distribution Matching (A-SDM) module to rectify unmatched positive sample
pairs. A-SDM adaptively pulls the unmatched positive pairs closer in the joint
embedding space, thereby achieving more precise global alignment. Additionally,
we introduce an Explicit Fine-grained Alignment (EFA) module, which makes up
for the lack of verification capability of implicit relational reasoning. EFA
strengthens explicit cross-modal fine-grained interactions by sparsifying the
similarity matrix and employs a hard coding method for local alignment. Our
proposed method is evaluated on three public datasets, achieving
state-of-the-art performance among all global matching methods. Our code is
available at https://github.com/yinhao1102/FMFA.

</details>


### [38] [Controllable-Continuous Color Editing in Diffusion Model via Color Mapping](https://arxiv.org/abs/2509.13756)
*Yuqi Yang,Dongliang Chang,Yuanchen Fang,Yi-Zhe SonG,Zhanyu Ma,Jun Guo*

Main category: cs.CV

TL;DR: 提出颜色映射模块来解决文本驱动图像编辑中颜色控制不精确和连续性差的问题，通过建立文本嵌入空间与RGB值的对应关系实现精确可控的颜色编辑。


<details>
  <summary>Details</summary>
Motivation: 文本驱动的图像编辑在颜色编辑方面存在精度不足和难以实现连续控制的问题，现有方法无法精确控制颜色变化范围和插值系数与图像颜色的关系。

Method: 引入颜色映射模块，显式建模文本嵌入空间与图像RGB值的对应关系，根据给定RGB值预测对应的嵌入向量，实现精确的颜色控制和语义一致性保持。

Result: 实验结果表明，该方法在颜色连续性和可控性方面表现良好，能够生成指定RGB范围内的连续颜色变化图像。

Conclusion: 该方法实现了更细粒度、连续且可控的颜色编辑，解决了文本驱动图像编辑中颜色控制的难题。

Abstract: In recent years, text-driven image editing has made significant progress.
However, due to the inherent ambiguity and discreteness of natural language,
color editing still faces challenges such as insufficient precision and
difficulty in achieving continuous control. Although linearly interpolating the
embedding vectors of different textual descriptions can guide the model to
generate a sequence of images with varying colors, this approach lacks precise
control over the range of color changes in the output images. Moreover, the
relationship between the interpolation coefficient and the resulting image
color is unknown and uncontrollable. To address these issues, we introduce a
color mapping module that explicitly models the correspondence between the text
embedding space and image RGB values. This module predicts the corresponding
embedding vector based on a given RGB value, enabling precise color control of
the generated images while maintaining semantic consistency. Users can specify
a target RGB range to generate images with continuous color variations within
the desired range, thereby achieving finer-grained, continuous, and
controllable color editing. Experimental results demonstrate that our method
performs well in terms of color continuity and controllability.

</details>


### [39] [Iterative Prompt Refinement for Safer Text-to-Image Generation](https://arxiv.org/abs/2509.13760)
*Jinwoo Jeon,JunHyeok Oh,Hayeong Lee,Byung-Jun Lee*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型的迭代提示词优化算法，通过分析生成图像来提升文本到图像模型的安全性，同时保持用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的提示词安全优化方法仅关注文本，忽略了生成的图像，可能导致不安全输出或对已安全提示词进行不必要修改。

Method: 使用视觉语言模型分析输入提示词和生成图像，通过视觉反馈迭代优化提示词，并构建了包含文本和视觉安全信号的新数据集用于监督微调。

Result: 实验结果表明该方法能生成更安全的输出，同时保持与用户意图的一致性，性能与现有基于大语言模型的方法相当。

Conclusion: 该方法为生成更安全的文本到图像内容提供了实用解决方案，通过视觉反馈有效提升了模型安全性。

Abstract: Text-to-Image (T2I) models have made remarkable progress in generating images
from text prompts, but their output quality and safety still depend heavily on
how prompts are phrased. Existing safety methods typically refine prompts using
large language models (LLMs), but they overlook the images produced, which can
result in unsafe outputs or unnecessary changes to already safe prompts. To
address this, we propose an iterative prompt refinement algorithm that uses
Vision Language Models (VLMs) to analyze both the input prompts and the
generated images. By leveraging visual feedback, our method refines prompts
more effectively, improving safety while maintaining user intent and
reliability comparable to existing LLM-based approaches. Additionally, we
introduce a new dataset labeled with both textual and visual safety signals
using off-the-shelf multi-modal LLM, enabling supervised fine-tuning.
Experimental results demonstrate that our approach produces safer outputs
without compromising alignment with user intent, offering a practical solution
for generating safer T2I content. Our code is available at
https://github.com/ku-dmlab/IPR. \textbf{\textcolor{red}WARNING: This paper
contains examples of harmful or inappropriate images generated by models.

</details>


### [40] [Task-Aware Image Signal Processor for Advanced Visual Perception](https://arxiv.org/abs/2509.13762)
*Kai Chen,Jin Xiao,Leheng Zhang,Kexuan Shi,Shuhang Gu*

Main category: cs.CV

TL;DR: TA-ISP是一个轻量级的RAW到RGB转换框架，通过多尺度调制算子为预训练视觉模型生成任务导向的表示，显著提高下游任务性能同时减少计算开销


<details>
  <summary>Details</summary>
Motivation: 现有RAW数据处理方法面临两大问题：大规模ISP网络计算开销大，基于传统ISP流水线的方法表示能力有限。需要一种既能保持丰富信息又计算高效的解决方案

Method: 提出任务感知图像信号处理(TA-ISP)框架，使用轻量级多尺度调制算子（全局、区域、像素级别）来重塑图像统计信息，替代传统的密集卷积流水线

Result: 在多个RAW域检测和分割基准测试中，TA-ISP在白天和夜间条件下均能持续提高下游任务准确率，同时显著减少参数数量和推理时间

Conclusion: TA-ISP非常适合在资源受限设备上部署，通过分解控制显著扩展了空间变化变换的表示范围，同时严格控制内存使用、计算和延迟

Abstract: In recent years, there has been a growing trend in computer vision towards
exploiting RAW sensor data, which preserves richer information compared to
conventional low-bit RGB images. Early studies mainly focused on enhancing
visual quality, while more recent efforts aim to leverage the abundant
information in RAW data to improve the performance of visual perception tasks
such as object detection and segmentation. However, existing approaches still
face two key limitations: large-scale ISP networks impose heavy computational
overhead, while methods based on tuning traditional ISP pipelines are
restricted by limited representational capacity.To address these issues, we
propose Task-Aware Image Signal Processing (TA-ISP), a compact RAW-to-RGB
framework that produces task-oriented representations for pretrained vision
models. Instead of heavy dense convolutional pipelines, TA-ISP predicts a small
set of lightweight, multi-scale modulation operators that act at global,
regional, and pixel scales to reshape image statistics across different spatial
extents. This factorized control significantly expands the range of spatially
varying transforms that can be represented while keeping memory usage,
computation, and latency tightly constrained. Evaluated on several RAW-domain
detection and segmentation benchmarks under both daytime and nighttime
conditions, TA-ISP consistently improves downstream accuracy while markedly
reducing parameter count and inference time, making it well suited for
deployment on resource-constrained devices.

</details>


### [41] [NDLPNet: A Location-Aware Nighttime Deraining Network and a Real-World Benchmark Dataset](https://arxiv.org/abs/2509.13766)
*Huichun Liu,Xiaosong Li,Yang Liu,Xiaoqi Cheng,Haishu Tan*

Main category: cs.CV

TL;DR: 提出NDLPNet网络解决夜间低光条件下的图像去雨问题，通过位置感知模块捕获雨纹空间信息，并构建了新的夜间雨景数据集NSR


<details>
  <summary>Details</summary>
Motivation: 现有去雨技术主要针对白天条件，在夜间低光环境下性能不佳，因为雨纹分布具有空间异质性和光依赖的条纹可见性

Method: 提出NDLPNet网络，包含位置感知模块(PPM)来捕获空间上下文信息，识别和重新校准不同特征通道的重要性

Result: 在现有数据集和新建的NSR数据集上，该方法在定性和定量评估中均优于最先进方法，能有效去除雨纹并保留背景信息

Conclusion: 该方法成功解决了夜间去雨问题，提供了新的数据集和基准，代码和数据集已开源

Abstract: Visual degradation caused by rain streak artifacts in low-light conditions
significantly hampers the performance of nighttime surveillance and autonomous
navigation. Existing image deraining techniques are primarily designed for
daytime conditions and perform poorly under nighttime illumination due to the
spatial heterogeneity of rain distribution and the impact of light-dependent
stripe visibility. In this paper, we propose a novel Nighttime Deraining
Location-enhanced Perceptual Network(NDLPNet) that effectively captures the
spatial positional information and density distribution of rain streaks in
low-light environments. Specifically, we introduce a Position Perception Module
(PPM) to capture and leverage spatial contextual information from input data,
enhancing the model's capability to identify and recalibrate the importance of
different feature channels. The proposed nighttime deraining network can
effectively remove the rain streaks as well as preserve the crucial background
information. Furthermore, We construct a night scene rainy (NSR) dataset
comprising 900 image pairs, all based on real-world nighttime scenes, providing
a new benchmark for nighttime deraining task research. Extensive qualitative
and quantitative experimental evaluations on both existing datasets and the NSR
dataset consistently demonstrate our method outperform the state-of-the-art
(SOTA) methods in nighttime deraining tasks. The source code and dataset is
available at https://github.com/Feecuin/NDLPNet.

</details>


### [42] [VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI](https://arxiv.org/abs/2509.13767)
*Daiqi Liu,Tomás Arias-Vergara,Johannes Enk,Fangxu Xing,Maureen Stone,Jerry L. Prince,Jana Hutter,Andreas Maier,Jonghye Woo,Paula Andrea Pérez-Toro*

Main category: cs.CV

TL;DR: VocSegMRI是一个多模态框架，整合视频、音频和音韵信号，通过交叉注意力融合和对比学习，显著提高了实时MRI中发音结构分割的准确性


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖视觉线索分割发音结构，但同步的声学和音韵信号能提供互补信息，丰富视觉信息并提高分割精度

Method: 提出VocSegMRI多模态框架，通过交叉注意力融合实现动态特征对齐，并加入对比学习目标增强跨模态表示，即使在推理时音频不可用也能保持性能

Result: 在USC-75 rtMRI数据集子集上达到最先进性能：Dice分数0.95，95% Hausdorff距离4.20mm，优于单模态和多模态基线

Conclusion: 交叉注意力和对比学习对分割精度和鲁棒性有重要贡献，集成多模态建模对准确声道分析具有重要价值

Abstract: Accurately segmenting articulatory structures in real-time magnetic resonance
imaging (rtMRI) remains challenging, as most existing methods rely almost
entirely on visual cues. Yet synchronized acoustic and phonological signals
provide complementary context that can enrich visual information and improve
precision. In this paper, we introduce VocSegMRI, a multimodal framework that
integrates video, audio, and phonological inputs through cross-attention fusion
for dynamic feature alignment. To further enhance cross-modal representation,
we incorporate a contrastive learning objective that improves segmentation
performance even when the audio modality is unavailable at inference. Evaluated
on a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-art
performance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance
(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.
Ablation studies confirm the contributions of cross-attention and contrastive
learning to segmentation precision and robustness. These results highlight the
value of integrative multimodal modeling for accurate vocal tract analysis.

</details>


### [43] [Generative Image Coding with Diffusion Prior](https://arxiv.org/abs/2509.13768)
*Jianhui Chang*

Main category: cs.CV

TL;DR: 提出基于扩散先验的生成式编码框架，在低码率下显著提升压缩性能和视觉保真度，相比H.266/VVC压缩性能提升高达79%


<details>
  <summary>Details</summary>
Motivation: 随着生成技术的发展，视觉内容变得复杂多样，传统编解码器和学习方法难以在高压缩比下保持主观质量，现有生成方法在视觉保真度和泛化性方面存在挑战

Method: 采用预优化编码器生成广义压缩域表示，通过轻量级适配器和注意力融合模块与预训练模型内部特征集成，并引入分布重归一化方法提升重建保真度

Result: 方法在低码率下视觉保真度优于现有方法，压缩性能比H.266/VVC提升高达79%，为AI生成内容提供高效解决方案且可适应更广泛内容类型

Conclusion: 该框架有效利用现有预训练扩散模型，能以最小重训练成本适应不同预训练模型满足新需求，在低码率压缩领域表现出色

Abstract: As generative technologies advance, visual content has evolved into a complex
mix of natural and AI-generated images, driving the need for more efficient
coding techniques that prioritize perceptual quality. Traditional codecs and
learned methods struggle to maintain subjective quality at high compression
ratios, while existing generative approaches face challenges in visual fidelity
and generalization. To this end, we propose a novel generative coding framework
leveraging diffusion priors to enhance compression performance at low bitrates.
Our approach employs a pre-optimized encoder to generate generalized
compressed-domain representations, integrated with the pretrained model's
internal features via a lightweight adapter and an attentive fusion module.
This framework effectively leverages existing pretrained diffusion models and
enables efficient adaptation to different pretrained models for new
requirements with minimal retraining costs. We also introduce a distribution
renormalization method to further enhance reconstruction fidelity. Extensive
experiments show that our method (1) outperforms existing methods in visual
fidelity across low bitrates, (2) improves compression performance by up to 79%
over H.266/VVC, and (3) offers an efficient solution for AI-generated content
while being adaptable to broader content types.

</details>


### [44] [AdaThinkDrive: Adaptive Thinking via Reinforcement Learning for Autonomous Driving](https://arxiv.org/abs/2509.13769)
*Yuechen Luo,Fang Li,Shaoqing Xu,Zhiyi Lai,Lei Yang,Qimao Chen,Ziang Luo,Zixun Xie,Shengyin Jiang,Jiaxin Liu,Long Chen,Bing Wang,Zhi-xin Yang*

Main category: cs.CV

TL;DR: AdaThinkDrive是一个新颖的视觉语言动作框架，采用双模式推理机制（快速和慢速思维），通过自适应选择是否使用思维链推理来平衡自动驾驶决策的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的思维链推理在视觉语言动作模型中虽然展现出潜力，但在简单场景中往往引入不必要的计算开销而不提升决策质量，需要一种更智能的推理机制。

Method: 提出AdaThinkDrive框架，通过大规模自动驾驶场景预训练获取世界知识和驾驶常识，在监督微调阶段使用双模式数据集（快速回答和慢速思维），并结合自适应思维奖励策略和组相对策略优化来选择性应用思维链推理。

Result: 在Navsim基准测试中达到90.3的PDMS分数，比最佳纯视觉基线提高1.7分，比始终使用思维链推理基线减少14%推理时间，在准确性和效率之间取得了良好平衡。

Conclusion: AdaThinkDrive通过自适应推理机制有效解决了思维链推理在简单场景中的计算效率问题，在保持高性能的同时显著提升了推理效率，为自动驾驶决策系统提供了新的解决方案。

Abstract: While reasoning technology like Chain of Thought (CoT) has been widely
adopted in Vision Language Action (VLA) models, it demonstrates promising
capabilities in end to end autonomous driving. However, recent efforts to
integrate CoT reasoning often fall short in simple scenarios, introducing
unnecessary computational overhead without improving decision quality. To
address this, we propose AdaThinkDrive, a novel VLA framework with a dual mode
reasoning mechanism inspired by fast and slow thinking. First, our framework is
pretrained on large scale autonomous driving (AD) scenarios using both question
answering (QA) and trajectory datasets to acquire world knowledge and driving
commonsense. During supervised fine tuning (SFT), we introduce a two mode
dataset, fast answering (w/o CoT) and slow thinking (with CoT), enabling the
model to distinguish between scenarios that require reasoning. Furthermore, an
Adaptive Think Reward strategy is proposed in conjunction with the Group
Relative Policy Optimization (GRPO), which rewards the model for selectively
applying CoT by comparing trajectory quality across different reasoning modes.
Extensive experiments on the Navsim benchmark show that AdaThinkDrive achieves
a PDMS of 90.3, surpassing the best vision only baseline by 1.7 points.
Moreover, ablations show that AdaThinkDrive surpasses both the never Think and
always Think baselines, improving PDMS by 2.0 and 1.4, respectively. It also
reduces inference time by 14% compared to the always Think baseline,
demonstrating its ability to balance accuracy and efficiency through adaptive
reasoning.

</details>


### [45] [Morphology-optimized Multi-Scale Fusion: Combining Local Artifacts and Mesoscopic Semantics for Deepfake Detection and Localization](https://arxiv.org/abs/2509.13776)
*Chao Shuai,Gaojian Wang,Kun Pan,Tong Wu,Fanli Jin,Haohan Tan,Mengxiang Li,Zhenguang Liu,Feng Lin,Kui Ren*

Main category: cs.CV

TL;DR: 提出一种新的深度伪造区域定位方法，通过独立使用局部和全局视角预测篡改区域，并采用形态学操作融合输出，有效抑制噪声并增强空间一致性。


<details>
  <summary>Details</summary>
Motivation: 当前深度伪造检测虽然分类准确率显著提升，但精确定位篡改区域仍面临挑战。现有方法往往忽略局部细节与全局语义的互补性，且融合策略简单导致噪声放大和性能下降。

Method: 独立使用局部和全局视角预测篡改区域，采用形态学操作融合两个分支的输出，抑制噪声同时增强空间连贯性。

Result: 大量实验验证了各模块在提高伪造定位准确性和鲁棒性方面的有效性。

Conclusion: 该方法通过创新的局部-全局独立预测和形态学融合策略，显著提升了深度伪造区域定位的性能和可靠性。

Abstract: While the pursuit of higher accuracy in deepfake detection remains a central
goal, there is an increasing demand for precise localization of manipulated
regions. Despite the remarkable progress made in classification-based
detection, accurately localizing forged areas remains a significant challenge.
A common strategy is to incorporate forged region annotations during model
training alongside manipulated images. However, such approaches often neglect
the complementary nature of local detail and global semantic context, resulting
in suboptimal localization performance. Moreover, an often-overlooked aspect is
the fusion strategy between local and global predictions. Naively combining the
outputs from both branches can amplify noise and errors, thereby undermining
the effectiveness of the localization.
  To address these issues, we propose a novel approach that independently
predicts manipulated regions using both local and global perspectives. We
employ morphological operations to fuse the outputs, effectively suppressing
noise while enhancing spatial coherence. Extensive experiments reveal the
effectiveness of each module in improving the accuracy and robustness of
forgery localization.

</details>


### [46] [CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling](https://arxiv.org/abs/2509.13784)
*Hanfang Liang,Bing Wang,Shizhen Zhang,Wen Jiang,Yizhuo Yang,Weixiang Guo,Shenghai Yuan*

Main category: cs.CV

TL;DR: 提出Variable-Rate Spatial Event Mamba架构，直接处理原始事件流，无需中间表示，通过自适应速率控制实现低延迟高效处理


<details>
  <summary>Details</summary>
Motivation: 现有方法需要预定义时间窗口引入窗口延迟，而逐点检测方法计算成本高无法实时处理，需要克服这些限制

Method: 使用轻量级因果空间邻域编码器捕获局部几何关系，然后采用基于Mamba的状态空间模型进行线性复杂度的可扩展时序建模

Result: 实现了窗口延迟和推理延迟之间的最优平衡，能够自适应事件速率调整处理速度

Conclusion: 该方法直接处理原始事件流，避免了中间表示的局限性，实现了高效实时的时空建模

Abstract: Event cameras capture asynchronous pixel-level brightness changes with
microsecond temporal resolution, offering unique advantages for high-speed
vision tasks. Existing methods often convert event streams into intermediate
representations such as frames, voxel grids, or point clouds, which inevitably
require predefined time windows and thus introduce window latency. Meanwhile,
pointwise detection methods face computational challenges that prevent
real-time efficiency due to their high computational cost. To overcome these
limitations, we propose the Variable-Rate Spatial Event Mamba, a novel
architecture that directly processes raw event streams without intermediate
representations. Our method introduces a lightweight causal spatial
neighborhood encoder to efficiently capture local geometric relations, followed
by Mamba-based state space models for scalable temporal modeling with linear
complexity. During inference, a controller adaptively adjusts the processing
speed according to the event rate, achieving an optimal balance between window
latency and inference latency.

</details>


### [47] [BWCache: Accelerating Video Diffusion Transformers through Block-Wise Caching](https://arxiv.org/abs/2509.13789)
*Hanshuai Cui,Zhiqing Tang,Zhifei Xu,Zhi Yao,Wenyi Zeng,Weijia Jia*

Main category: cs.CV

TL;DR: 提出Block-Wise Caching (BWCache)训练免费方法，通过动态缓存和重用DiT块特征，在保持视觉质量的同时实现最高2.24倍加速。


<details>
  <summary>Details</summary>
Motivation: 现有Diffusion Transformers (DiTs)的顺序去噪过程导致不可避免的延迟，限制了实际应用。现有加速方法要么因架构修改而影响视觉质量，要么无法在适当粒度上重用中间特征。

Method: BWCache方法：1）分析发现DiT块是推理延迟的主要贡献者；2）特征变化呈现U型模式，中间时间步相似度高；3）动态缓存和重用DiT块特征；4）引入相似性指示器，仅在相邻时间步块特征差异低于阈值时触发特征重用。

Result: 在多个视频扩散模型上的广泛实验表明，BWCache实现了最高2.24倍的加速，同时保持可比的视觉质量。

Conclusion: BWCache是一种有效的训练免费加速方法，通过智能特征重用显著减少Diffusion Transformers的计算冗余，为实时视频生成应用提供了可行的解决方案。

Abstract: Recent advancements in Diffusion Transformers (DiTs) have established them as
the state-of-the-art method for video generation. However, their inherently
sequential denoising process results in inevitable latency, limiting real-world
applicability. Existing acceleration methods either compromise visual quality
due to architectural modifications or fail to reuse intermediate features at
proper granularity. Our analysis reveals that DiT blocks are the primary
contributors to inference latency. Across diffusion timesteps, the feature
variations of DiT blocks exhibit a U-shaped pattern with high similarity during
intermediate timesteps, which suggests substantial computational redundancy. In
this paper, we propose Block-Wise Caching (BWCache), a training-free method to
accelerate DiT-based video generation. BWCache dynamically caches and reuses
features from DiT blocks across diffusion timesteps. Furthermore, we introduce
a similarity indicator that triggers feature reuse only when the differences
between block features at adjacent timesteps fall below a threshold, thereby
minimizing redundant computations while maintaining visual fidelity. Extensive
experiments on several video diffusion models demonstrate that BWCache achieves
up to 2.24$\times$ speedup with comparable visual quality.

</details>


### [48] [Bridging the Synthetic-Real Gap: Supervised Domain Adaptation for Robust Spacecraft 6-DoF Pose Estimation](https://arxiv.org/abs/2509.13792)
*Inder Pal Singh,Nidhal Eddine Chenni,Abd El Rahman Shabayek,Arunkumar Rathinam,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了首个针对航天器姿态估计关键点回归的监督域自适应框架，通过联合优化域不变表示和任务特定风险，显著减少域偏移下的泛化误差


<details>
  <summary>Details</summary>
Motivation: 现有的混合流水线在合成数据集上表现良好，但在真实图像上性能急剧下降，而无监督域自适应方法在有少量标记目标样本时往往表现不佳

Method: 基于学习不变表示和风险(LIRR)范式，联合使用标记的合成数据和有限的标记真实数据，优化域不变表示和任务特定风险

Result: 在SPEED+基准测试中始终优于仅源域、微调和oracle基线，仅使用5%标记目标数据就能匹配或超越使用更多标记数据的oracle性能

Conclusion: 该框架轻量级、主干无关且计算高效，为在真实空间环境中实现稳健可部署的航天器姿态估计提供了实用途径

Abstract: Spacecraft Pose Estimation (SPE) is a fundamental capability for autonomous
space operations such as rendezvous, docking, and in-orbit servicing. Hybrid
pipelines that combine object detection, keypoint regression, and
Perspective-n-Point (PnP) solvers have recently achieved strong results on
synthetic datasets, yet their performance deteriorates sharply on real or
lab-generated imagery due to the persistent synthetic-to-real domain gap.
Existing unsupervised domain adaptation approaches aim to mitigate this issue
but often underperform when a modest number of labeled target samples are
available. In this work, we propose the first Supervised Domain Adaptation
(SDA) framework tailored for SPE keypoint regression. Building on the Learning
Invariant Representation and Risk (LIRR) paradigm, our method jointly optimizes
domain-invariant representations and task-specific risk using both labeled
synthetic and limited labeled real data, thereby reducing generalization error
under domain shift. Extensive experiments on the SPEED+ benchmark demonstrate
that our approach consistently outperforms source-only, fine-tuning, and oracle
baselines. Notably, with only 5% labeled target data, our method matches or
surpasses oracle performance trained on larger fractions of labeled data. The
framework is lightweight, backbone-agnostic, and computationally efficient,
offering a practical pathway toward robust and deployable spacecraft pose
estimation in real-world space environments.

</details>


### [49] [SWA-PF: Semantic-Weighted Adaptive Particle Filter for Memory-Efficient 4-DoF UAV Localization in GNSS-Denied Environments](https://arxiv.org/abs/2509.13795)
*Jiayu Yuan,Ming Dai,Enhui Zheng,Chao Su,Nanxing Chen,Qiming Hu,Shibo Zhu,Yibin Cao*

Main category: cs.CV

TL;DR: 提出SWA-PF方法，结合语义特征和粒子滤波，在GNSS拒绝环境中实现高效无人机定位，定位误差低于10米，计算效率提升10倍


<details>
  <summary>Details</summary>
Motivation: 现有基于检索的无人机定位方法存在数据集不足、实时性能差、环境敏感性强和泛化能力有限等问题，特别是在动态或时变环境中

Method: 提出语义加权自适应粒子滤波(SWA-PF)方法，集成无人机图像和卫星图像的语义特征，包含语义加权机制和优化的粒子滤波架构

Result: 计算效率比特征提取方法提升10倍，全局定位误差低于10米，可在数秒内实现4自由度姿态估计，使用低分辨率卫星地图

Conclusion: SWA-PF方法有效解决了无人机在GNSS拒绝环境中的定位挑战，提供了高效准确的定位解决方案，并发布了大规模多高度飞行段数据集MAFS

Abstract: Vision-based Unmanned Aerial Vehicle (UAV) localization systems have been
extensively investigated for Global Navigation Satellite System (GNSS)-denied
environments. However, existing retrieval-based approaches face limitations in
dataset availability and persistent challenges including suboptimal real-time
performance, environmental sensitivity, and limited generalization capability,
particularly in dynamic or temporally varying environments. To overcome these
limitations, we present a large-scale Multi-Altitude Flight Segments dataset
(MAFS) for variable altitude scenarios and propose a novel Semantic-Weighted
Adaptive Particle Filter (SWA-PF) method. This approach integrates robust
semantic features from both UAV-captured images and satellite imagery through
two key innovations: a semantic weighting mechanism and an optimized particle
filtering architecture. Evaluated using our dataset, the proposed method
achieves 10x computational efficiency gain over feature extraction methods,
maintains global positioning errors below 10 meters, and enables rapid 4 degree
of freedom (4-DoF) pose estimation within seconds using accessible
low-resolution satellite maps. Code and dataset will be available at
https://github.com/YuanJiayuuu/SWA-PF.

</details>


### [50] [Masked Feature Modeling Enhances Adaptive Segmentation](https://arxiv.org/abs/2509.13801)
*Wenlve Zhou,Zhiheng Zhou,Tiantao Xian,Yikui Zhai,Weibin Wu,Biyun Ma*

Main category: cs.CV

TL;DR: 提出Masked Feature Modeling (MFM)作为无监督域自适应语义分割的辅助任务，通过在特征空间进行掩码和重建，与分割任务目标对齐，无需修改推理流程。


<details>
  <summary>Details</summary>
Motivation: 解决现有掩码建模方法在无监督域自适应语义分割中因架构不兼容和优化目标不一致而未被充分探索的问题。

Method: 设计MFM任务在特征空间进行掩码重建，引入轻量级Rebuilder模块进行联合训练但推理时丢弃，利用分割解码器对重建特征进行分类。

Result: 在多种架构和UDA基准测试中，MFM持续提升分割性能，证明了其简单、高效和通用性。

Conclusion: MFM为无监督域自适应语义分割提供了一个有效且计算开销为零的辅助学习策略。

Abstract: Unsupervised domain adaptation (UDA) for semantic segmentation aims to
transfer models from a labeled source domain to an unlabeled target domain.
While auxiliary self-supervised tasks-particularly contrastive learning-have
improved feature discriminability, masked modeling approaches remain
underexplored in this setting, largely due to architectural incompatibility and
misaligned optimization objectives. We propose Masked Feature Modeling (MFM), a
novel auxiliary task that performs feature masking and reconstruction directly
in the feature space. Unlike existing masked modeling methods that reconstruct
low-level inputs or perceptual features (e.g., HOG or visual tokens), MFM
aligns its learning target with the main segmentation task, ensuring
compatibility with standard architectures like DeepLab and DAFormer without
modifying the inference pipeline. To facilitate effective reconstruction, we
introduce a lightweight auxiliary module, Rebuilder, which is trained jointly
but discarded during inference, adding zero computational overhead at test
time. Crucially, MFM leverages the segmentation decoder to classify the
reconstructed features, tightly coupling the auxiliary objective with the
pixel-wise prediction task to avoid interference with the primary task.
Extensive experiments across various architectures and UDA benchmarks
demonstrate that MFM consistently enhances segmentation performance, offering a
simple, efficient, and generalizable strategy for unsupervised domain-adaptive
semantic segmentation.

</details>


### [51] [Data-Efficient Spectral Classification of Hyperspectral Data Using MiniROCKET and HDC-MiniROCKET](https://arxiv.org/abs/2509.13809)
*Nick Theisen,Kenny Schlegel,Dietrich Paulus,Peer Neubert*

Main category: cs.CV

TL;DR: 本文研究了高光谱图像像素光谱分类问题，比较了1D-Justo-LiuNet和MiniROCKET/HDC-MiniROCKET在有限训练数据下的性能表现


<details>
  <summary>Details</summary>
Motivation: 虽然当前最佳方法利用空间-光谱信息，但纯光谱分类具有模型小、训练数据需求少的优势。现有最优模型1D-Justo-LiuNet在有限数据下性能下降，需要寻找更稳健的替代方案

Method: 研究MiniROCKET和HDC-MiniROCKET模型用于光谱分类，这些模型在特征提取部分没有可训练参数，采用精心设计的特征提取方法

Result: MiniROCKET在有限数据场景下优于1D-Justo-LiuNet，在一般情况下性能相当，尽管参数更多但对训练数据不足更具鲁棒性

Conclusion: MiniROCKET系列模型是光谱分类的有效替代方案，特别适用于训练数据有限的情况，为未来空间-光谱方法的改进提供了补充

Abstract: The classification of pixel spectra of hyperspectral images, i.e. spectral
classification, is used in many fields ranging from agricultural, over medical
to remote sensing applications and is currently also expanding to areas such as
autonomous driving. Even though for full hyperspectral images the
best-performing methods exploit spatial-spectral information, performing
classification solely on spectral information has its own advantages, e.g.
smaller model size and thus less data required for training. Moreover, spectral
information is complementary to spatial information and improvements on either
part can be used to improve spatial-spectral approaches in the future.
Recently, 1D-Justo-LiuNet was proposed as a particularly efficient model with
very few parameters, which currently defines the state of the art in spectral
classification. However, we show that with limited training data the model
performance deteriorates. Therefore, we investigate MiniROCKET and
HDC-MiniROCKET for spectral classification to mitigate that problem. The model
extracts well-engineered features without trainable parameters in the feature
extraction part and is therefore less vulnerable to limited training data. We
show that even though MiniROCKET has more parameters it outperforms
1D-Justo-LiuNet in limited data scenarios and is mostly on par with it in the
general case

</details>


### [52] [Semi-MoE: Mixture-of-Experts meets Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2509.13834)
*Nguyen Lan Vi Vu,Thanh-Huy Nguyen,Thien Nguyen,Daisuke Kihara,Tianyang Wang,Xingjian Li,Min Xu*

Main category: cs.CV

TL;DR: Semi-MOE：首个用于半监督病理图像分割的多任务专家混合框架，通过三个专家网络和动态伪标签机制解决边界模糊和形态误分类问题


<details>
  <summary>Details</summary>
Motivation: 现有半监督学习方法在病理图像分割中难以处理由模糊腺体边界和形态误分类导致的噪声伪标签问题

Method: 使用三个专家网络（主分割专家、符号距离场回归专家、边界预测专家）捕获不同形态特征，结合多门控伪标签模块动态聚合专家特征，并提出自适应多目标损失平衡多个学习目标

Result: 在GlaS和CRAG基准测试中，该方法在低标签设置下优于现有最先进方法

Conclusion: 基于专家混合架构的方法在推进半监督分割方面具有巨大潜力

Abstract: Semi-supervised learning has been employed to alleviate the need for
extensive labeled data for histopathology image segmentation, but existing
methods struggle with noisy pseudo-labels due to ambiguous gland boundaries and
morphological misclassification. This paper introduces Semi-MOE, to the best of
our knowledge, the first multi-task Mixture-of-Experts framework for
semi-supervised histopathology image segmentation. Our approach leverages three
specialized expert networks: A main segmentation expert, a signed distance
field regression expert, and a boundary prediction expert, each dedicated to
capturing distinct morphological features. Subsequently, the Multi-Gating
Pseudo-labeling module dynamically aggregates expert features, enabling a
robust fuse-and-refine pseudo-labeling mechanism. Furthermore, to eliminate
manual tuning while dynamically balancing multiple learning objectives, we
propose an Adaptive Multi-Objective Loss. Extensive experiments on GlaS and
CRAG benchmarks show that our method outperforms state-of-the-art approaches in
low-label settings, highlighting the potential of MoE-based architectures in
advancing semi-supervised segmentation. Our code is available at
https://github.com/vnlvi2k3/Semi-MoE.

</details>


### [53] [Diving into Mitigating Hallucinations from a Vision Perspective for Large Vision-Language Models](https://arxiv.org/abs/2509.13836)
*Weihang Wang,Xinhao Li,Ziyue Wang,Yan Pang,Jielei Zhang,Peiyi Li,Qiang Zhang,Longwen Gao*

Main category: cs.CV

TL;DR: 提出了VHBench-10基准测试来系统评估不同视觉编码器在LVLMs中的幻觉表现，并设计了VisionWeaver动态路由网络来显著减少幻觉问题


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的物体幻觉问题严重阻碍了实际应用，不同视觉编码器的训练范式差异导致不同的幻觉特性，但现有基准测试无法捕捉这种细粒度差异

Method: 构建包含10,000样本的VHBench-10基准测试，涵盖10个细粒度幻觉类别；提出VisionWeaver上下文感知路由网络，使用全局视觉特征生成路由信号，动态聚合多个专家的视觉特征

Result: 评估证实不同编码器具有独特的幻觉特征；VisionWeaver在全面实验中有效减少幻觉并提升整体模型性能

Conclusion: 视觉编码器的选择对LVLMs幻觉问题至关重要，VisionWeaver通过动态特征聚合机制显著改善了模型幻觉表现

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly
impedes their real-world applicability. As the primary component for accurately
interpreting visual information, the choice of visual encoder is pivotal. We
hypothesize that the diverse training paradigms employed by different visual
encoders instill them with distinct inductive biases, which leads to their
diverse hallucination performances. Existing benchmarks typically focus on
coarse-grained hallucination detection and fail to capture the diverse
hallucinations elaborated in our hypothesis. To systematically analyze these
effects, we introduce VHBench-10, a comprehensive benchmark with approximately
10,000 samples for evaluating LVLMs across ten fine-grained hallucination
categories. Our evaluations confirm encoders exhibit unique hallucination
characteristics. Building on these insights and the suboptimality of simple
feature fusion, we propose VisionWeaver, a novel Context-Aware Routing Network.
It employs global visual features to generate routing signals, dynamically
aggregating visual features from multiple specialized experts. Comprehensive
experiments confirm the effectiveness of VisionWeaver in significantly reducing
hallucinations and improving overall model performance.

</details>


### [54] [Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation](https://arxiv.org/abs/2509.13846)
*Puru Vaish,Felix Meister,Tobias Heimann,Christoph Brune,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 挑战了表示学习中视图无关性假设，提出显式对齐不同视图表示的方法，在自监督学习中取得了优异性能


<details>
  <summary>Details</summary>
Motivation: 发现潜在空间中有意义的结构不会自然出现，需要显式诱导，挑战了现有方法中视图无关性即可学习有效表示的假设

Method: 提出Consistent View Alignment方法，对齐数据不同视图的表示来整合互补信息，同时避免产生假阳性

Result: 在MICCAI 2025 SSL3D挑战赛中分别获得第一和第二名，下游任务性能显著提升

Conclusion: 结构化视图对齐在学习有效表示中起着关键作用，显式对齐比隐含假设更有效

Abstract: Many recent approaches in representation learning implicitly assume that
uncorrelated views of a data point are sufficient to learn meaningful
representations for various downstream tasks. In this work, we challenge this
assumption and demonstrate that meaningful structure in the latent space does
not emerge naturally. Instead, it must be explicitly induced. We propose a
method that aligns representations from different views of the data to align
complementary information without inducing false positives. Our experiments
show that our proposed self-supervised learning method, Consistent View
Alignment, improves performance for downstream tasks, highlighting the critical
role of structured view alignment in learning effective representations. Our
method achieved first and second place in the MICCAI 2025 SSL3D challenge when
using a Primus vision transformer and ResEnc convolutional neural network,
respectively. The code and pretrained model weights are released at
https://github.com/Tenbatsu24/LatentCampus.

</details>


### [55] [SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation](https://arxiv.org/abs/2509.13848)
*Jiayi Pan,Jiaming Xu,Yongkang Zhou,Guohao Dai*

Main category: cs.CV

TL;DR: SpecDiff是一种训练免费的多级特征缓存策略，通过自推测引入未来信息来加速扩散模型推理，在保持质量的同时实现2.7-3.2倍加速


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法仅依赖历史信息，导致准确性和速度性能受限，需要突破速度-准确性的权衡瓶颈

Method: 提出自推测范式，基于不同迭代次数间相同时间步的信息相似性引入未来信息，包含基于自推测信息的特征选择算法和基于特征重要性分数的多级特征分类算法

Result: 在Stable Diffusion 3、3.5和FLUX上平均实现2.80×、2.74×和3.17×加速，质量损失可忽略

Conclusion: 通过融合推测信息和历史信息，SpecDiff突破了速度-准确性的权衡瓶颈，推动了高效扩散模型推理的帕累托前沿

Abstract: Feature caching has recently emerged as a promising method for diffusion
model acceleration. It effectively alleviates the inefficiency problem caused
by high computational requirements by caching similar features in the inference
process of the diffusion model. In this paper, we analyze existing feature
caching methods from the perspective of information utilization, and point out
that relying solely on historical information will lead to constrained accuracy
and speed performance. And we propose a novel paradigm that introduces future
information via self-speculation based on the information similarity at the
same time step across different iteration times. Based on this paradigm, we
present \textit{SpecDiff}, a training-free multi-level feature caching strategy
including a cached feature selection algorithm and a multi-level feature
classification algorithm. (1) Feature selection algorithm based on
self-speculative information. \textit{SpecDiff} determines a dynamic importance
score for each token based on self-speculative information and historical
information, and performs cached feature selection through the importance
score. (2) Multi-level feature classification algorithm based on feature
importance scores. \textit{SpecDiff} classifies tokens by leveraging the
differences in feature importance scores and introduces a multi-level feature
calculation strategy. Extensive experiments show that \textit{SpecDiff}
achieves average 2.80 \times, 2.74 \times , and 3.17\times speedup with
negligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow
on NVIDIA A800-80GB GPU. By merging speculative and historical information,
\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing
the Pareto frontier of speedup and accuracy in the efficient diffusion model
inference.

</details>


### [56] [EDITS: Enhancing Dataset Distillation with Implicit Textual Semantics](https://arxiv.org/abs/2509.13858)
*Qianxin Xia,Jiawei Du,Guoming Lu,Zhiyong Shu,Jielei Wang*

Main category: cs.CV

TL;DR: EDITS是一个新颖的数据集蒸馏框架，利用图像中的隐式文本语义信息，通过视觉语言模型和大型语言模型生成文本原型，结合扩散模型合成更高质量的压缩数据集。


<details>
  <summary>Details</summary>
Motivation: 传统数据集蒸馏方法主要捕获低层视觉特征，忽略了图像中的高层语义和结构信息，导致蒸馏效果受限。

Method: 首先使用视觉语言模型生成外部文本并与图像特征融合形成先验聚类缓冲区，然后通过局部语义感知选择代表性样本构建图像和文本原型，最后通过双原型引导策略利用扩散模型生成最终合成数据集。

Result: 大量实验证实了该方法的有效性，能够生成更高质量的压缩数据集。

Conclusion: EDITS框架通过利用文本语义信息显著提升了数据集蒸馏的性能，为高效学习提供了更好的数据基础。

Abstract: Dataset distillation aims to synthesize a compact dataset from the original
large-scale one, enabling highly efficient learning while preserving
competitive model performance. However, traditional techniques primarily
capture low-level visual features, neglecting the high-level semantic and
structural information inherent in images. In this paper, we propose EDITS, a
novel framework that exploits the implicit textual semantics within the image
data to achieve enhanced distillation. First, external texts generated by a
Vision Language Model (VLM) are fused with image features through a Global
Semantic Query module, forming the prior clustered buffer. Local Semantic
Awareness then selects representative samples from the buffer to construct
image and text prototypes, with the latter produced by guiding a Large Language
Model (LLM) with meticulously crafted prompt. Ultimately, Dual Prototype
Guidance strategy generates the final synthetic dataset through a diffusion
model. Extensive experiments confirm the effectiveness of our method.Source
code is available in: https://github.com/einsteinxia/EDITS.

</details>


### [57] [LamiGauss: Pitching Radiative Gaussian for Sparse-View X-ray Laminography Reconstruction](https://arxiv.org/abs/2509.13863)
*Chu Chen,Ander Biguri,Jean-Michel Morel,Raymond H. Chan,Carola-Bibiane Schönlieb,Jizhou Li*

Main category: cs.CV

TL;DR: LamiGauss是一种用于稀疏视图X射线层析成像重建的新算法，结合高斯溅射渲染和专用坐标变换模型，仅需3%的完整视图数据即可实现高质量重建。


<details>
  <summary>Details</summary>
Motivation: 传统CT在平板结构检测中存在几何限制问题，而稀疏视图条件下的层析成像重建质量仍然具有挑战性，需要开发更有效的重建方法。

Method: 提出LamiGauss算法，结合高斯溅射辐射光栅化和包含层析倾斜角的检测器到世界坐标变换模型，采用初始化策略过滤常见伪影，防止高斯分布分配到虚假结构。

Result: 在合成和真实数据集上的实验表明，该方法仅使用3%的完整视图即可实现优于在全数据集上优化的迭代方法的性能。

Conclusion: LamiGauss能够直接从稀疏投影中有效优化，实现有限数据下的准确高效重建，在稀疏视图层析成像中表现出优越性能。

Abstract: X-ray Computed Laminography (CL) is essential for non-destructive inspection
of plate-like structures in applications such as microchips and composite
battery materials, where traditional computed tomography (CT) struggles due to
geometric constraints. However, reconstructing high-quality volumes from
laminographic projections remains challenging, particularly under highly
sparse-view acquisition conditions. In this paper, we propose a reconstruction
algorithm, namely LamiGauss, that combines Gaussian Splatting radiative
rasterization with a dedicated detector-to-world transformation model
incorporating the laminographic tilt angle. LamiGauss leverages an
initialization strategy that explicitly filters out common laminographic
artifacts from the preliminary reconstruction, preventing redundant Gaussians
from being allocated to false structures and thereby concentrating model
capacity on representing the genuine object. Our approach effectively optimizes
directly from sparse projections, enabling accurate and efficient
reconstruction with limited data. Extensive experiments on both synthetic and
real datasets demonstrate the effectiveness and superiority of the proposed
method over existing techniques. LamiGauss uses only 3$\%$ of full views to
achieve superior performance over the iterative method optimized on a full
dataset.

</details>


### [58] [Distractor-Aware Memory-Based Visual Object Tracking](https://arxiv.org/abs/2509.13864)
*Jovana Videnovic,Matej Kristan,Alan Lukezic*

Main category: cs.CV

TL;DR: DAM4SAM：基于SAM2的干扰物感知记忆模块，通过减少跟踪漂移和提升重检测能力，在多个基准测试中实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有基于记忆的视频分割方法（如SAM2）在视觉目标跟踪中面临相似干扰物的挑战，需要专门针对干扰物问题的解决方案

Method: 提出干扰物感知的即插即用记忆模块和基于内省的管理方法，构建DiDi干扰物数据集进行分析

Result: 在13个基准测试中超越SAM2.1，在10个测试中达到SOTA；集成到实时跟踪器中提升11%性能，匹配非实时SAM2.1-L的质量

Conclusion: 干扰物感知记忆模块具有很好的架构泛化能力，能有效提升各种跟踪器在干扰环境下的性能

Abstract: Recent emergence of memory-based video segmentation methods such as SAM2 has
led to models with excellent performance in segmentation tasks, achieving
leading results on numerous benchmarks. However, these modes are not fully
adjusted for visual object tracking, where distractors (i.e., objects visually
similar to the target) pose a key challenge. In this paper we propose a
distractor-aware drop-in memory module and introspection-based management
method for SAM2, leading to DAM4SAM. Our design effectively reduces the
tracking drift toward distractors and improves redetection capability after
object occlusion. To facilitate the analysis of tracking in the presence of
distractors, we construct DiDi, a Distractor-Distilled dataset. DAM4SAM
outperforms SAM2.1 on thirteen benchmarks and sets new state-of-the-art results
on ten. Furthermore, integrating the proposed distractor-aware memory into a
real-time tracker EfficientTAM leads to 11% improvement and matches tracking
quality of the non-real-time SAM2.1-L on multiple tracking and segmentation
benchmarks, while integration with edge-based tracker EdgeTAM delivers 4%
performance boost, demonstrating a very good generalization across
architectures.

</details>


### [59] [Invisible Yet Detected: PelFANet with Attention-Guided Anatomical Fusion for Pelvic Fracture Diagnosis](https://arxiv.org/abs/2509.13873)
*Siam Tahsin Bhuiyan,Rashedur Rahman,Sefatul Wasi,Naomi Yagi,Syoji Kobashi,Ashraful Islam,Saadia Binte Alam*

Main category: cs.CV

TL;DR: 提出PelFANet双流注意力网络，融合原始X光片和分割骨图像，提高骨盆骨折分类性能，特别是在细微或不可见骨折情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 骨盆骨折在标准X光片上常常难以诊断，特别是当骨折迹象细微或不可见时，需要更精准的诊断方法。

Method: 使用双流注意力网络，通过Fused Attention Blocks迭代交换和精炼来自原始X光片和分割骨图像的特征，采用两阶段分割引导的训练流程。

Result: 在AMERI数据集上，可见骨折准确率88.68%，AUC 0.9334；不可见骨折准确率82.29%，AUC 0.8688，即使未在不可见骨折上训练也表现良好。

Conclusion: 解剖结构感知的双输入架构在骨折检测中具有重要临床潜力，特别是在放射学表现细微的情况下能够提供稳健的诊断能力。

Abstract: Pelvic fractures pose significant diagnostic challenges, particularly in
cases where fracture signs are subtle or invisible on standard radiographs. To
address this, we introduce PelFANet, a dual-stream attention network that fuses
raw pelvic X-rays with segmented bone images to improve fracture
classification. The network em-ploys Fused Attention Blocks (FABlocks) to
iteratively exchange and refine fea-tures from both inputs, capturing global
context and localized anatomical detail. Trained in a two-stage pipeline with a
segmentation-guided approach, PelFANet demonstrates superior performance over
conventional methods. On the AMERI dataset, it achieves 88.68% accuracy and
0.9334 AUC on visible fractures, while generalizing effectively to invisible
fracture cases with 82.29% accuracy and 0.8688 AUC, despite not being trained
on them. These results highlight the clini-cal potential of anatomy-aware
dual-input architectures for robust fracture detec-tion, especially in
scenarios with subtle radiographic presentations.

</details>


### [60] [EvHand-FPV: Efficient Event-Based 3D Hand Tracking from First-Person View](https://arxiv.org/abs/2509.13883)
*Zhen Xu,Guorui Lu,Chang Gao,Qinyu Chen*

Main category: cs.CV

TL;DR: EvHand-FPV是一个轻量级的事件相机第一人称视角3D手部追踪框架，通过几何ROI定位和多任务学习策略，在保持高精度的同时大幅减少了计算量和参数数量。


<details>
  <summary>Details</summary>
Motivation: 传统帧式方法在XR设备等资源受限场景中难以同时满足精度、低延迟和能效要求，而事件相机具有微秒级时间分辨率和毫瓦级功耗的优势。

Method: 构建事件相机FPV数据集，引入基于手腕的ROI定位和端到端映射策略，采用多任务学习辅助几何特征头，无需显式重建即可减少计算量。

Result: 在真实FPV测试集上，2D-AUCp从0.77提升到0.85，参数量减少89%（11.2M→1.2M），推理FLOPs减少89%（1.648G→0.185G），合成数据上保持0.84的3D-AUCp。

Conclusion: 该方法实现了准确高效的基于事件相机的第一人称手部追踪，适合XR设备的实时应用，为资源受限场景提供了可行的解决方案。

Abstract: Hand tracking holds great promise for intuitive interaction paradigms, but
frame-based methods often struggle to meet the requirements of accuracy, low
latency, and energy efficiency, especially in resource-constrained settings
such as Extended Reality (XR) devices. Event cameras provide $\mu$s-level
temporal resolution at mW-level power by asynchronously sensing brightness
changes. In this work, we present EvHand-FPV, a lightweight framework for
egocentric First-Person-View 3D hand tracking from a single event camera. We
construct an event-based FPV dataset that couples synthetic training data with
3D labels and real event data with 2D labels for evaluation to address the
scarcity of egocentric benchmarks. EvHand-FPV also introduces a wrist-based
region of interest (ROI) that localizes the hand region via geometric cues,
combined with an end-to-end mapping strategy that embeds ROI offsets into the
network to reduce computation without explicit reconstruction, and a multi-task
learning strategy with an auxiliary geometric feature head that improves
representations without test-time overhead. On our real FPV test set,
EvHand-FPV improves 2D-AUCp from 0.77 to 0.85 while reducing parameters from
11.2M to 1.2M by 89% and FLOPs per inference from 1.648G to 0.185G by 89%. It
also maintains a competitive 3D-AUCp of 0.84 on synthetic data. These results
demonstrate accurate and efficient egocentric event-based hand tracking
suitable for on-device XR applications. The dataset and code are available at
https://github.com/zen5x5/EvHand-FPV.

</details>


### [61] [White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2509.13907)
*Jiyun Im,SuBeen Lee,Miso Lee,Jae-Pil Heo*

Main category: cs.CV

TL;DR: 提出WARM模块解决少样本3D点云分割中原型生成问题，通过白化和着色变换改进注意力机制，在多个基准测试中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现有方法使用传统算法（如最远点采样）构建原型存在初始随机性问题，且原型生成过程研究不足，注意力机制虽然有效但存在可学习原型标记与支持特征之间的分布差异

Method: 提出WARM（白化聚合与恢复模块），在交叉注意力前后分别加入白化和着色变换。白化将支持特征与原型标记对齐，着色将注意力后的标记恢复原始分布，从而捕获支持特征间的语义关系生成代表性原型

Result: 在多个少样本3D点云分割基准测试中取得了最先进的性能，且优势显著

Conclusion: WARM模块通过简单的设计有效解决了注意力机制中的分布不对齐问题，能够生成更鲁棒和代表性的原型，显著提升少样本3D点云分割性能

Abstract: Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-point
labels for an unlabeled point cloud, given only a few labeled examples. To
extract discriminative representations from the limited support set, existing
methods have constructed prototypes using conventional algorithms such as
farthest point sampling. However, we point out that its initial randomness
significantly affects FS-PCS performance and that the prototype generation
process remains underexplored despite its prevalence. This motivates us to
investigate an advanced prototype generation method based on attention
mechanism. Despite its potential, we found that vanilla module suffers from the
distributional gap between learnable prototypical tokens and support features.
To overcome this, we propose White Aggregation and Restoration Module (WARM),
which resolves the misalignment by sandwiching cross-attention between
whitening and coloring transformations. Specifically, whitening aligns the
support features to prototypical tokens before attention process, and
subsequently coloring restores the original distribution to the attended
tokens. This simple yet effective design enables robust attention, thereby
generating representative prototypes by capturing the semantic relationships
among support features. Our method achieves state-of-the-art performance with a
significant margin on multiple FS-PCS benchmarks, demonstrating its
effectiveness through extensive experiments.

</details>


### [62] [Towards Rationale-Answer Alignment of LVLMs via Self-Rationale Calibration](https://arxiv.org/abs/2509.13919)
*Yuanchen Wu,Ke Yan,Shouhong Ding,Ziyin Zhou,Xiaoqiang Li*

Main category: cs.CV

TL;DR: 提出了SRC框架，通过迭代校准理由与答案的对齐，提升大视觉语言模型的推理一致性和准确性


<details>
  <summary>Details</summary>
Motivation: 现有大视觉语言模型在理由与生成答案的对齐方面存在困难，导致推理不一致和错误回答

Method: 采用轻量级"理由微调"修改响应格式，搜索候选响应，使用R-Scorer评分模型评估理由质量和事实一致性，通过置信度加权偏好微调进行对齐校准

Result: 在多个基准测试中显著提升了模型的感知、推理和泛化能力

Conclusion: 强调理由导向的对齐在探索大视觉语言模型潜力中的重要性

Abstract: Large Vision-Language Models (LVLMs) have manifested strong visual question
answering capability. However, they still struggle with aligning the rationale
and the generated answer, leading to inconsistent reasoning and incorrect
responses. To this end, this paper introduces the Self-Rationale Calibration
(SRC) framework to iteratively calibrate the alignment between rationales and
answers. SRC begins by employing a lightweight "rationale fine-tuning"
approach, which modifies the model's response format to require a rationale
before deriving an answer without explicit prompts. Next, SRC searches for a
diverse set of candidate responses from the fine-tuned LVLMs for each sample,
followed by a proposed pairwise scoring strategy using a tailored scoring
model, R-Scorer, to evaluate both rationale quality and factual consistency of
candidates. Based on a confidence-weighted preference curation process, SRC
decouples the alignment calibration into a preference fine-tuning manner,
leading to significant improvements of LVLMs in perception, reasoning, and
generalization across multiple benchmarks. Our results emphasize the
rationale-oriented alignment in exploring the potential of LVLMs.

</details>


### [63] [Towards Robust Defense against Customization via Protective Perturbation Resistant to Diffusion-based Purification](https://arxiv.org/abs/2509.13922)
*Wenkui Yang,Jie Cao,Junxian Duan,Ran He*

Main category: cs.CV

TL;DR: 提出AntiPure保护性扰动方法，通过频率引导和时间步误导来抵御净化攻击，在净化-定制流程中实现最小感知差异和最大失真效果


<details>
  <summary>Details</summary>
Motivation: 扩散模型的定制能力带来安全风险，现有保护性扰动容易被净化技术移除，需要开发抗净化的保护方法

Method: AntiPure方法采用两种引导机制：1)块状频率引导减少模型对高频成分的影响；2)错误时间步引导扰乱去噪策略，嵌入难以察觉的扰动

Result: 实验显示AntiPure在净化-定制工作流中实现最小感知差异和最大失真，优于其他保护性扰动方法

Conclusion: AntiPure作为净化技术的压力测试，能有效嵌入持久性扰动，防止图像被恶意伪造和滥用

Abstract: Diffusion models like Stable Diffusion have become prominent in visual
synthesis tasks due to their powerful customization capabilities, which also
introduce significant security risks, including deepfakes and copyright
infringement. In response, a class of methods known as protective perturbation
emerged, which mitigates image misuse by injecting imperceptible adversarial
noise. However, purification can remove protective perturbations, thereby
exposing images again to the risk of malicious forgery. In this work, we
formalize the anti-purification task, highlighting challenges that hinder
existing approaches, and propose a simple diagnostic protective perturbation
named AntiPure. AntiPure exposes vulnerabilities of purification within the
"purification-customization" workflow, owing to two guidance mechanisms: 1)
Patch-wise Frequency Guidance, which reduces the model's influence over
high-frequency components in the purified image, and 2) Erroneous Timestep
Guidance, which disrupts the model's denoising strategy across different
timesteps. With additional guidance, AntiPure embeds imperceptible
perturbations that persist under representative purification settings,
achieving effective post-customization distortion. Experiments show that, as a
stress test for purification, AntiPure achieves minimal perceptual discrepancy
and maximal distortion, outperforming other protective perturbation methods
within the purification-customization workflow.

</details>


### [64] [Noise-Level Diffusion Guidance: Well Begun is Half Done](https://arxiv.org/abs/2509.13936)
*Harvey Mannering,Zhiwu Huang,Adam Prugel-Bennett*

Main category: cs.CV

TL;DR: 提出Noise Level Guidance (NLG)方法，通过优化初始噪声来提高扩散模型生成质量和提示词一致性，无需额外数据、网络或反向传播


<details>
  <summary>Details</summary>
Motivation: 扩散模型的随机高斯初始噪声会导致输出质量变化和提示词遵循度不一致，现有优化方法依赖额外数据集、网络或反向传播，实用性有限

Method: NLG方法通过增加初始噪声与通用指导的对齐可能性来优化噪声水平，为条件和无条件扩散模型提供统一框架，兼容各种扩散级指导形式

Result: 在五个标准基准测试上验证，NLG显著提升了输出生成质量和输入条件遵循度，同时保持计算效率

Conclusion: NLG是一种实用且可扩展的扩散模型增强方法，可无缝集成现有指导方法，为噪声级优化提供了简单高效的解决方案

Abstract: Diffusion models have achieved state-of-the-art image generation. However,
the random Gaussian noise used to start the diffusion process influences the
final output, causing variations in image quality and prompt adherence.
Existing noise-level optimization approaches generally rely on extra dataset
construction, additional networks, or backpropagation-based optimization,
limiting their practicality. In this paper, we propose Noise Level Guidance
(NLG), a simple, efficient, and general noise-level optimization approach that
refines initial noise by increasing the likelihood of its alignment with
general guidance - requiring no additional training data, auxiliary networks,
or backpropagation. The proposed NLG approach provides a unified framework
generalizable to both conditional and unconditional diffusion models,
accommodating various forms of diffusion-level guidance. Extensive experiments
on five standard benchmarks demonstrate that our approach enhances output
generation quality and input condition adherence. By seamlessly integrating
with existing guidance methods while maintaining computational efficiency, our
method establishes NLG as a practical and scalable enhancement to diffusion
models. Code can be found at
https://github.com/harveymannering/NoiseLevelGuidance.

</details>


### [65] [Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation](https://arxiv.org/abs/2509.13939)
*Gia Khanh Nguyen,Yifeng Huang,Minh Hoai*

Main category: cs.CV

TL;DR: PairTally是一个专门评估细粒度视觉计数能力的基准数据集，包含681张高分辨率图像，每张图像包含两个物体类别，要求模型基于形状、大小、颜色或语义的细微差异进行区分和计数。


<details>
  <summary>Details</summary>
Motivation: 现有模型（包括类别无关计数模型和大规模视觉语言模型）在执行细粒度、意图驱动的计数任务时能力尚不明确，需要专门的评估基准。

Method: 构建包含681张高分辨率图像的PairTally数据集，每张图像包含两个物体类别，设置类别间（不同类别）和类别内（密切相关子类别）两种场景，对多种最先进模型进行基准测试。

Result: 尽管最近有所进展，但当前模型在可靠地计数用户意图方面仍存在困难，特别是在细粒度和视觉模糊的情况下。

Conclusion: PairTally为诊断和改进细粒度视觉计数系统提供了新的基础，揭示了当前模型在意图驱动计数任务中的局限性。

Abstract: Visual counting is a fundamental yet challenging task, especially when users
need to count objects of a specific type in complex scenes. While recent
models, including class-agnostic counting models and large vision-language
models (VLMs), show promise in counting tasks, their ability to perform
fine-grained, intent-driven counting remains unclear. In this paper, we
introduce PairTally, a benchmark dataset specifically designed to evaluate
fine-grained visual counting. Each of the 681 high-resolution images in
PairTally contains two object categories, requiring models to distinguish and
count based on subtle differences in shape, size, color, or semantics. The
dataset includes both inter-category (distinct categories) and intra-category
(closely related subcategories) settings, making it suitable for rigorous
evaluation of selective counting capabilities. We benchmark a variety of
state-of-the-art models, including exemplar-based methods, language-prompted
models, and large VLMs. Our results show that despite recent advances, current
models struggle to reliably count what users intend, especially in fine-grained
and visually ambiguous cases. PairTally provides a new foundation for
diagnosing and improving fine-grained visual counting systems.

</details>


### [66] [MOCHA: Multi-modal Objects-aware Cross-arcHitecture Alignment](https://arxiv.org/abs/2509.14001)
*Elena Camuffo,Francesco Barbato,Mete Ozay,Simone Milani,Umberto Michieli*

Main category: cs.CV

TL;DR: MOCHA是一种知识蒸馏方法，将大型视觉语言教师模型的多模态语义知识转移到轻量级视觉目标检测学生模型中，通过对象级对齐实现高效语义迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法主要关注密集或全局对齐，缺乏对象级别的多模态语义传递能力。需要一种能够在不需要修改教师模型或推理时文本输入的情况下，有效传递区域级多模态语义的方法。

Method: 使用翻译模块将学生特征映射到联合空间，通过双目标损失函数（局部对齐和全局关系一致性）指导学生和翻译器的训练，在对象级别进行多模态语义对齐。

Result: 在四个个性化检测基准测试中，平均得分提升10.1分，性能与更大的多模态模型相当，证明了其在现实世界部署的适用性。

Conclusion: MOCHA通过对象级知识蒸馏成功实现了多模态语义的高效迁移，为轻量级目标检测模型提供了强大的语义理解能力，具有实际应用价值。

Abstract: We introduce MOCHA (Multi-modal Objects-aware Cross-arcHitecture Alignment),
a knowledge distillation approach that transfers region-level multimodal
semantics from a large vision-language teacher (e.g., LLaVa) into a lightweight
vision-only object detector student (e.g., YOLO). A translation module maps
student features into a joint space, where the training of the student and
translator is guided by a dual-objective loss that enforces both local
alignment and global relational consistency. Unlike prior approaches focused on
dense or global alignment, MOCHA operates at the object level, enabling
efficient transfer of semantics without modifying the teacher or requiring
textual input at inference. We validate our method across four personalized
detection benchmarks under few-shot regimes. Results show consistent gains over
baselines, with a +10.1 average score improvement. Despite its compact
architecture, MOCHA reaches performance on par with larger multimodal models,
proving its suitability for real-world deployment.

</details>


### [67] [Performance Optimization of YOLO-FEDER FusionNet for Robust Drone Detection in Visually Complex Environments](https://arxiv.org/abs/2509.14012)
*Tamara R. Lenhard,Andreas Weinmann,Tobias Koch*

Main category: cs.CV

TL;DR: 提出增强版YOLO-FEDER FusionNet检测框架，通过整合通用目标检测和伪装目标检测技术，在复杂视觉环境中显著提升无人机检测性能


<details>
  <summary>Details</summary>
Motivation: 解决无人机在复杂视觉环境中检测困难的问题，包括背景干扰、小目标尺度和伪装效应，传统检测器在低目标-背景可分离性的杂乱环境中性能下降

Method: 系统改进训练数据组成、特征融合策略和骨干网络设计，使用大规模逼真合成数据配合少量真实样本，评估多尺度FEDER特征贡献，测试多种YOLO骨干配置

Result: 最佳配置(YOLOv8l骨干+DWD模块FEDER特征)相比初始基线，FNR降低39.1个百分点，mAP@0.5提升62.8个百分点

Conclusion: 集成中间FEDER特征结合骨干网络升级能带来显著性能提升，在复杂视觉环境中有效改善无人机检测效果

Abstract: Drone detection in visually complex environments remains challenging due to
background clutter, small object scale, and camouflage effects. While generic
object detectors like YOLO exhibit strong performance in low-texture scenes,
their effectiveness degrades in cluttered environments with low
object-background separability. To address these limitations, this work
presents an enhanced iteration of YOLO-FEDER FusionNet -- a detection framework
that integrates generic object detection with camouflage object detection
techniques. Building upon the original architecture, the proposed iteration
introduces systematic advancements in training data composition, feature fusion
strategies, and backbone design. Specifically, the training process leverages
large-scale, photo-realistic synthetic data, complemented by a small set of
real-world samples, to enhance robustness under visually complex conditions.
The contribution of intermediate multi-scale FEDER features is systematically
evaluated, and detection performance is comprehensively benchmarked across
multiple YOLO-based backbone configurations. Empirical results indicate that
integrating intermediate FEDER features, in combination with backbone upgrades,
contributes to notable performance improvements. In the most promising
configuration -- YOLO-FEDER FusionNet with a YOLOv8l backbone and FEDER
features derived from the DWD module -- these enhancements lead to a FNR
reduction of up to 39.1 percentage points and a mAP increase of up to 62.8
percentage points at an IoU threshold of 0.5, compared to the initial baseline.

</details>


### [68] [SAIL-VL2 Technical Report](https://arxiv.org/abs/2509.14033)
*Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng*

Main category: cs.CV

TL;DR: SAIL-VL2是一个开源的2B/8B参数视觉语言基础模型，在106个数据集上实现SOTA性能，特别是在MMMU和MathVista等复杂推理基准上表现优异


<details>
  <summary>Details</summary>
Motivation: 作为SAIL-VL的继任者，旨在构建一个全面的多模态理解和推理基础模型，为开源多模态社区提供高效可扩展的解决方案

Method: 采用大规模数据筛选管道、渐进式训练框架（从预训练视觉编码器到多模态预训练，再到思维融合SFT-RL混合范式）以及稀疏MoE架构设计

Result: 在OpenCompass排行榜上，SAIL-VL2-2B在4B参数规模以下的开源模型中排名第一，在106个数据集上展现竞争优势

Conclusion: SAIL-VL2通过三项核心创新实现了卓越的多模态性能，为开源社区提供了强大的视觉语言基础模型

Abstract: We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM)
for comprehensive multimodal understanding and reasoning. As the successor to
SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B
parameter scales across diverse image and video benchmarks, demonstrating
strong capabilities from fine-grained perception to complex reasoning. Three
core innovations drive its effectiveness. First, a large-scale data curation
pipeline with scoring and filtering strategies enhances both quality and
distribution across captioning, OCR, QA, and video data, improving training
efficiency. Second, a progressive training framework begins with a powerful
pre-trained vision encoder (SAIL-ViT), advances through multimodal
pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that
systematically strengthens model capabilities. Third, architectural advances
extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs.
With these contributions, SAIL-VL2 demonstrates competitive performance across
106 datasets and achieves state-of-the-art results on challenging reasoning
benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass
leaderboard, SAIL-VL2-2B ranks first among officially released open-source
models under the 4B parameter scale, while serving as an efficient and
extensible foundation for the open-source multimodal community.

</details>


### [69] [PROFUSEme: PROstate Cancer Biochemical Recurrence Prediction via FUSEd Multi-modal Embeddings](https://arxiv.org/abs/2509.14051)
*Suhang You,Carla Pitarch-Abaigar,Sanket Kachole,Sumedh Sonawane,Juhyung Ha,Anish Sudarshan Gada,David Crandall,Rakesh Shiradkar,Spyridon Bakas*

Main category: cs.CV

TL;DR: 提出PROFUSEme方法，通过融合临床、放射学和病理学多模态数据，早期预测前列腺癌根治术后的生化复发，相比晚期融合方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 约30%前列腺癌患者在根治性前列腺切除术后会经历生化复发，准确早期预测有助于临床决策和改善患者预后。

Method: 采用中间融合配置结合Cox比例风险回归模型，学习临床、放射学和病理学数据的跨模态交互作用。

Result: 在内部5折嵌套交叉验证框架中平均C-index为0.861，在CHIMERA 2025挑战验证排行榜的保留数据上C-index为0.7103，性能优于晚期融合配置。

Conclusion: PROFUSEme方法通过多模态数据融合有效提升了前列腺癌生化复发的早期预测准确性。

Abstract: Almost 30% of prostate cancer (PCa) patients undergoing radical prostatectomy
(RP) experience biochemical recurrence (BCR), characterized by increased
prostate specific antigen (PSA) and associated with increased mortality.
Accurate early prediction of BCR, at the time of RP, would contribute to prompt
adaptive clinical decision-making and improved patient outcomes. In this work,
we propose prostate cancer BCR prediction via fused multi-modal embeddings
(PROFUSEme), which learns cross-modal interactions of clinical, radiology, and
pathology data, following an intermediate fusion configuration in combination
with Cox Proportional Hazard regressors. Quantitative evaluation of our
proposed approach reveals superior performance, when compared with late fusion
configurations, yielding a mean C-index of 0.861 ($\sigma=0.112$) on the
internal 5-fold nested cross-validation framework, and a C-index of 0.7103 on
the hold out data of CHIMERA 2025 challenge validation leaderboard.

</details>


### [70] [Wan-Animate: Unified Character Animation and Replacement with Holistic Replication](https://arxiv.org/abs/2509.14055)
*Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo*

Main category: cs.CV

TL;DR: Wan-Animate是一个统一的角色动画和替换框架，能够根据参考视频精确复制角色的表情和动作来生成高质量角色视频，或将动画角色无缝集成到参考视频中替换原角色。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够统一处理角色动画生成和角色替换任务的框架，实现高保真度的角色动画和场景环境无缝集成。

Method: 基于Wan模型构建，采用改进的输入范式区分参考条件和生成区域，使用空间对齐的骨骼信号复制身体动作，从源图像提取隐式面部特征重现表情，并开发辅助的Relighting LoRA模块增强环境光照集成。

Result: 实验结果表明Wan-Animate达到了最先进的性能水平。

Conclusion: 该框架成功统一了多个任务到共同的符号表示中，能够生成具有高可控性和表现力的角色视频，并承诺开源模型权重和源代码。

Abstract: We introduce Wan-Animate, a unified framework for character animation and
replacement. Given a character image and a reference video, Wan-Animate can
animate the character by precisely replicating the expressions and movements of
the character in the video to generate high-fidelity character videos.
Alternatively, it can integrate the animated character into the reference video
to replace the original character, replicating the scene's lighting and color
tone to achieve seamless environmental integration. Wan-Animate is built upon
the Wan model. To adapt it for character animation tasks, we employ a modified
input paradigm to differentiate between reference conditions and regions for
generation. This design unifies multiple tasks into a common symbolic
representation. We use spatially-aligned skeleton signals to replicate body
motion and implicit facial features extracted from source images to reenact
expressions, enabling the generation of character videos with high
controllability and expressiveness. Furthermore, to enhance environmental
integration during character replacement, we develop an auxiliary Relighting
LoRA. This module preserves the character's appearance consistency while
applying the appropriate environmental lighting and color tone. Experimental
results demonstrate that Wan-Animate achieves state-of-the-art performance. We
are committed to open-sourcing the model weights and its source code.

</details>


### [71] [VSE-MOT: Multi-Object Tracking in Low-Quality Video Scenes Guided by Visual Semantic Enhancement](https://arxiv.org/abs/2509.14060)
*Jun Du,Weiwei Xing,Ming Li,Fei Richard Yu*

Main category: cs.CV

TL;DR: 提出VSE-MOT框架，通过视觉语言模型提取全局视觉语义信息，解决低质量视频中多目标跟踪性能下降问题，在真实低质量场景中性能提升8%-20%。


<details>
  <summary>Details</summary>
Motivation: 当前多目标跟踪算法在低质量视频中性能显著下降，需要解决真实世界图像退化问题以推进MOT算法在实际低质量视频场景中的应用。

Method: 设计三分支架构利用视觉语言模型提取全局视觉语义信息，引入MOT-Adapter适配多目标跟踪任务，以及VSFM模块提升特征融合效果。

Result: 在真实低质量视频场景中，跟踪性能指标比现有方法提升约8%到20%，同时在常规场景中保持稳健性能。

Conclusion: VSE-MOT框架通过视觉语义增强有效解决了低质量视频中的多目标跟踪挑战，具有实际应用价值。

Abstract: Current multi-object tracking (MOT) algorithms typically overlook issues
inherent in low-quality videos, leading to significant degradation in tracking
performance when confronted with real-world image deterioration. Therefore,
advancing the application of MOT algorithms in real-world low-quality video
scenarios represents a critical and meaningful endeavor. To address the
challenges posed by low-quality scenarios, inspired by vision-language models,
this paper proposes a Visual Semantic Enhancement-guided Multi-Object Tracking
framework (VSE-MOT). Specifically, we first design a tri-branch architecture
that leverages a vision-language model to extract global visual semantic
information from images and fuse it with query vectors. Subsequently, to
further enhance the utilization of visual semantic information, we introduce
the Multi-Object Tracking Adapter (MOT-Adapter) and the Visual Semantic Fusion
Module (VSFM). The MOT-Adapter adapts the extracted global visual semantic
information to suit multi-object tracking tasks, while the VSFM improves the
efficacy of feature fusion. Through extensive experiments, we validate the
effectiveness and superiority of the proposed method in real-world low-quality
video scenarios. Its tracking performance metrics outperform those of existing
methods by approximately 8% to 20%, while maintaining robust performance in
conventional scenarios.

</details>


### [72] [AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration](https://arxiv.org/abs/2509.14084)
*Jingyi Yuan,Jianxiong Ye,Wenkang Chen,Chenqiang Gao*

Main category: cs.CV

TL;DR: 提出了AD-DINOv3框架，首次将DINOv3适配到零样本异常检测任务，通过多模态对比学习和异常感知校准模块解决特征错位和语义偏向问题


<details>
  <summary>Details</summary>
Motivation: 传统ZSAD方法主要基于CLIP模型，而DINOv3等视觉基础模型展现出更强的可迁移表示能力，但直接适配存在域偏差和全局语义偏向两个关键挑战

Method: 构建多模态对比学习框架，使用DINOv3提取视觉特征，CLIP文本编码器提供正常/异常提示嵌入，引入轻量级适配器桥接域差距，并设计异常感知校准模块引导关注异常区域

Result: 在8个工业和医学基准测试上，AD-DINOv3持续匹配或超越最先进方法，验证了其作为通用零样本异常检测框架的优越性

Conclusion: AD-DINOv3成功解决了DINOv3在ZSAD任务中的适配挑战，通过多模态对齐和异常感知机制实现了优异的零样本异常检测性能

Abstract: Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrary
novel categories, offering a scalable and annotation-efficient solution.
Traditionally, most ZSAD works have been based on the CLIP model, which
performs anomaly detection by calculating the similarity between visual and
text embeddings. Recently, vision foundation models such as DINOv3 have
demonstrated strong transferable representation capabilities. In this work, we
are the first to adapt DINOv3 for ZSAD. However, this adaptation presents two
key challenges: (i) the domain bias between large-scale pretraining data and
anomaly detection tasks leads to feature misalignment; and (ii) the inherent
bias toward global semantics in pretrained representations often leads to
subtle anomalies being misinterpreted as part of the normal foreground objects,
rather than being distinguished as abnormal regions. To overcome these
challenges, we introduce AD-DINOv3, a novel vision-language multimodal
framework designed for ZSAD. Specifically, we formulate anomaly detection as a
multimodal contrastive learning problem, where DINOv3 is employed as the visual
backbone to extract patch tokens and a CLS token, and the CLIP text encoder
provides embeddings for both normal and abnormal prompts. To bridge the domain
gap, lightweight adapters are introduced in both modalities, enabling their
representations to be recalibrated for the anomaly detection task. Beyond this
baseline alignment, we further design an Anomaly-Aware Calibration Module
(AACM), which explicitly guides the CLS token to attend to anomalous regions
rather than generic foreground semantics, thereby enhancing discriminability.
Extensive experiments on eight industrial and medical benchmarks demonstrate
that AD-DINOv3 consistently matches or surpasses state-of-the-art methods,
verifying its superiority as a general zero-shot anomaly detection framework.

</details>


### [73] [Teacher-Guided Pseudo Supervision and Cross-Modal Alignment for Audio-Visual Video Parsing](https://arxiv.org/abs/2509.14097)
*Yaru Chen,Ruohao Guo,Liting Gao,Yang Xiang,Qingyu Luo,Zhenbo Li,Wenwu Wang*

Main category: cs.CV

TL;DR: 提出两种策略：EMA引导的伪监督框架生成可靠的片段级掩码，以及类感知跨模态一致性损失，在弱监督音频-视觉视频解析任务中实现了最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决先前工作在弱监督音频-视觉视频解析中忽视稳定片段级监督和类感知跨模态对齐的问题。

Method: 使用指数移动平均(EMA)引导的伪监督框架生成可靠片段级掩码，并结合类感知跨模态一致性(CMA)损失来对齐音频和视觉嵌入。

Result: 在LLP和UnAV-100数据集上的评估显示，该方法在多个指标上达到了最先进的性能。

Conclusion: 提出的EMA伪监督框架和CMA损失有效解决了弱监督音频-视觉视频解析中的关键挑战，实现了优异的性能表现。

Abstract: Weakly-supervised audio-visual video parsing (AVVP) seeks to detect audible,
visible, and audio-visual events without temporal annotations. Previous work
has emphasized refining global predictions through contrastive or collaborative
learning, but neglected stable segment-level supervision and class-aware
cross-modal alignment. To address this, we propose two strategies: (1) an
exponential moving average (EMA)-guided pseudo supervision framework that
generates reliable segment-level masks via adaptive thresholds or top-k
selection, offering stable temporal guidance beyond video-level labels; and (2)
a class-aware cross-modal agreement (CMA) loss that aligns audio and visual
embeddings at reliable segment-class pairs, ensuring consistency across
modalities while preserving temporal structure. Evaluations on LLP and UnAV-100
datasets shows that our method achieves state-of-the-art (SOTA) performance
across multiple metrics.

</details>


### [74] [CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts](https://arxiv.org/abs/2509.14104)
*Leonard Hackel,Tom Burgert,Begüm Demir*

Main category: cs.CV

TL;DR: 提出了一种集成Soft混合专家机制到遥感基础模型的方法，显著提升计算效率同时保持或改进表征性能


<details>
  <summary>Details</summary>
Motivation: 现有遥感基础模型存在计算复杂度高或表征能力有限的问题，限制了实际应用

Method: 将Soft混合专家机制集成到Cross-Sensor Masked Autoencoder模型中，构建CSMoE模型，并采用主题-气候描述符驱动的采样策略构建训练集

Result: CSMoE在场景分类、语义分割和图像检索任务中实现了超过两倍的计算效率提升，同时保持竞争性性能

Conclusion: 所提出的方法有效解决了遥感基础模型计算效率与表征能力之间的平衡问题，为构建高效遥感基础模型提供了有效途径

Abstract: Self-supervised learning through masked autoencoders has attracted great
attention for remote sensing (RS) foundation model (FM) development, enabling
improved representation learning across diverse sensors and downstream tasks.
However, existing RS FMs often either suffer from substantial computational
complexity during both training and inference or exhibit limited
representational capacity. These issues restrict their practical applicability
in RS. To address this limitation, we propose an adaptation for enhancing the
efficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanism
into the FM. The integration of Soft MoEs into the FM allows modality-specific
expert specialization alongside shared cross-sensor representation learning. To
demonstrate the effectiveness of our adaptation, we apply it on the
Cross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-Sensor
Mixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climatic
descriptor-driven sampling strategy for the construction of a representative
and diverse training set to train our CSMoE model. Extensive experiments on
scene classification, semantic segmentation, and content-based image retrieval
demonstrate that our adaptation yields a reduction in computational
requirements while maintaining or improving representational performance.
Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-off
between representational capacity, accuracy, and computational efficiency. On
average, CSMoE achieves more than twice the computational efficiency of
existing RS FMs, while maintaining competitive performance across all
experiments. These results show the effectiveness of the proposed adaptation
for creating computationally efficient RS FMs. The code for the model, the
training set creation, and the model weights will be available at
https://git.tu-berlin.de/rsim/csmoe.

</details>


### [75] [Generative AI for Misalignment-Resistant Virtual Staining to Accelerate Histopathology Workflows](https://arxiv.org/abs/2509.14119)
*Jiabo MA,Wenqiang Li,Jinbang Li,Ziyi Liu,Linshan Wu,Fengtao Zhou,Li Liang,Ronald Cheong Kin Chan,Terence T. W. Wong,Hao Chen*

Main category: cs.CV

TL;DR: 提出了一种具有级联配准机制的虚拟染色框架，解决了生成输出与真实染色图像之间的空间不匹配问题，显著提升了虚拟染色性能


<details>
  <summary>Details</summary>
Motivation: 传统组织染色过程耗时耗力且对环境不友好，现有虚拟染色方法依赖严格对齐的配对数据，但获取这种数据困难，因为化学染色过程会导致组织变形，且单个组织切片无法进行多次染色

Method: 采用级联配准机制来解决生成输出与真实染色图像之间的空间不匹配问题，提高了虚拟染色的准确性

Result: 在五个数据集上显著优于现有最先进模型，内部数据集平均提升3.2%，外部数据集提升10.1%，在严重不对齐数据集上PSNR提升23.8%

Conclusion: 该方法具有出色的跨数据集鲁棒性，简化了虚拟染色的数据采集过程，为虚拟染色技术的发展提供了新思路

Abstract: Accurate histopathological diagnosis often requires multiple differently
stained tissue sections, a process that is time-consuming, labor-intensive, and
environmentally taxing due to the use of multiple chemical stains. Recently,
virtual staining has emerged as a promising alternative that is faster,
tissue-conserving, and environmentally friendly. However, existing virtual
staining methods face significant challenges in clinical applications,
primarily due to their reliance on well-aligned paired data. Obtaining such
data is inherently difficult because chemical staining processes can distort
tissue structures, and a single tissue section cannot undergo multiple staining
procedures without damage or loss of information. As a result, most available
virtual staining datasets are either unpaired or roughly paired, making it
difficult for existing methods to achieve accurate pixel-level supervision. To
address this challenge, we propose a robust virtual staining framework
featuring cascaded registration mechanisms to resolve spatial mismatches
between generated outputs and their corresponding ground truth. Experimental
results demonstrate that our method significantly outperforms state-of-the-art
models across five datasets, achieving an average improvement of 3.2% on
internal datasets and 10.1% on external datasets. Moreover, in datasets with
substantial misalignment, our approach achieves a remarkable 23.8% improvement
in peak signal-to-noise ratio compared to baseline models. The exceptional
robustness of the proposed method across diverse datasets simplifies the data
acquisition process for virtual staining and offers new insights for advancing
its development.

</details>


### [76] [Deceptive Beauty: Evaluating the Impact of Beauty Filters on Deepfake and Morphing Attack Detection](https://arxiv.org/abs/2509.14120)
*Sara Concas,Simone Maurizio La Cava,Andrea Panzino,Ester Masala,Giulia Orrù,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 美颜滤镜会降低深度伪造和变形攻击检测器的性能，导致检测准确率下降


<details>
  <summary>Details</summary>
Motivation: 社交媒体美颜滤镜的普及对数字面部图像和视频的可靠性以及自动面部分析的准确性提出了挑战，特别是在区分真实与伪造数据方面，需要研究这些滤镜对检测器性能的影响

Method: 对多个最先进的检测器在基准数据集上进行全面分析，评估应用各种平滑滤镜前后的性能变化

Result: 研究发现美颜滤镜会导致检测器性能下降，揭示了面部增强技术引入的脆弱性

Conclusion: 需要开发对美颜滤镜等面部修饰具有鲁棒性的检测模型，以应对数字美化技术带来的挑战

Abstract: Digital beautification through social media filters has become increasingly
popular, raising concerns about the reliability of facial images and videos and
the effectiveness of automated face analysis. This issue is particularly
critical for digital manipulation detectors, systems aiming at distinguishing
between genuine and manipulated data, especially in cases involving deepfakes
and morphing attacks designed to deceive humans and automated facial
recognition. This study examines whether beauty filters impact the performance
of deepfake and morphing attack detectors. We perform a comprehensive analysis,
evaluating multiple state-of-the-art detectors on benchmark datasets before and
after applying various smoothing filters. Our findings reveal performance
degradation, highlighting vulnerabilities introduced by facial enhancements and
underscoring the need for robust detection models resilient to such
alterations.

</details>


### [77] [MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook](https://arxiv.org/abs/2509.14142)
*Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li*

Main category: cs.CV

TL;DR: MARS2 2025挑战赛综述，聚焦多模态推理，发布Lens和AdsQA数据集，评估40+基线模型，设立三个竞赛赛道，吸引76个团队参与。


<details>
  <summary>Details</summary>
Motivation: 通过大型基准测试整合多模态机器学习和LLMs的不同方法，推动这一快速发展领域的技术进步，并拓展MLLMs在多模态推理中的应用场景。

Method: 发布两个定制数据集（Lens和AdsQA），评估40多个基线模型（包括通用MLLMs和任务特定模型），设立三个竞赛赛道：VG-RS、VQA-SA和VR-Ads。

Result: 76个知名学术和工业机构团队注册，40+有效提交（从1200+中筛选）进入排名，数据集、代码集和排名结果已公开。

Conclusion: MARS2 2025挑战赛成功建立了多模态推理的基准测试平台，为研究者提供了跟踪该领域最新进展的资源，并将持续更新和举办相关活动。

Abstract: This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim
to bring together different approaches in multimodal machine learning and LLMs
via a large benchmark. We hope it better allows researchers to follow the
state-of-the-art in this very dynamic area. Meanwhile, a growing number of
testbeds have boosted the evolution of general-purpose large language models.
Thus, this year's MARS2 focuses on real-world and specialized scenarios to
broaden the multimodal reasoning applications of MLLMs. Our organizing team
released two tailored datasets Lens and AdsQA as test sets, which support
general reasoning in 12 daily scenarios and domain-specific reasoning in
advertisement videos, respectively. We evaluated 40+ baselines that include
both generalist MLLMs and task-specific models, and opened up three competition
tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question
Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative
Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and
industrial institutions have registered and 40+ valid submissions (out of
1200+) have been included in our ranking lists. Our datasets, code sets (40+
baselines and 15+ participants' methods), and rankings are publicly available
on the MARS2 workshop website and our GitHub organization page
https://github.com/mars2workshop/, where our updates and announcements of
upcoming events will be continuously provided.

</details>


### [78] [An Exploratory Study on Abstract Images and Visual Representations Learned from Them](https://arxiv.org/abs/2509.14149)
*Haotian Li,Jianbo Jiao*

Main category: cs.CV

TL;DR: 研究抽象图像（由基本几何形状构成）与栅格图像在视觉任务中的性能差异，通过构建分层抽象图像数据集HAID进行多任务评估


<details>
  <summary>Details</summary>
Motivation: 探索抽象图像能否有效传递视觉语义信息，以及在不同抽象层次下能捕获多少高级语义内容

Method: 构建分层抽象图像数据集HAID，包含从正常栅格图像生成的多层次抽象图像，然后在分类、分割、目标检测等任务上训练和评估传统视觉系统

Result: 抽象图像确实能传递视觉语义信息，但其表示能力通常不如传统栅格图像，存在性能差距

Conclusion: 抽象图像可以作为传递视觉语义信息的潜在有效格式，但需要进一步研究其在不同抽象层次下的表现和应用潜力

Abstract: Imagine living in a world composed solely of primitive shapes, could you
still recognise familiar objects? Recent studies have shown that abstract
images-constructed by primitive shapes-can indeed convey visual semantic
information to deep learning models. However, representations obtained from
such images often fall short compared to those derived from traditional raster
images. In this paper, we study the reasons behind this performance gap and
investigate how much high-level semantic content can be captured at different
abstraction levels. To this end, we introduce the Hierarchical Abstraction
Image Dataset (HAID), a novel data collection that comprises abstract images
generated from normal raster images at multiple levels of abstraction. We then
train and evaluate conventional vision systems on HAID across various tasks
including classification, segmentation, and object detection, providing a
comprehensive study between rasterised and abstract image representations. We
also discuss if the abstract image can be considered as a potentially effective
format for conveying visual semantic information and contributing to vision
tasks.

</details>


### [79] [BEVUDA++: Geometric-aware Unsupervised Domain Adaptation for Multi-View 3D Object Detection](https://arxiv.org/abs/2509.14151)
*Rongyu Zhang,Jiaming Liu,Xiaoqi Li,Xiaowei Chi,Dan Wang,Li Du,Yuan Du,Shanghang Zhang*

Main category: cs.CV

TL;DR: BEVUDA++：针对鸟瞰图感知中多视角3D目标检测的领域自适应方法，通过几何感知的师生框架解决跨域性能下降问题


<details>
  <summary>Details</summary>
Motivation: 鸟瞰图感知在自动驾驶中很重要，但现有研究忽略了领域偏移问题，导致跨域场景下性能显著下降

Method: 提出几何感知师生框架BEVUDA++，包含可靠深度教师(RDT)和几何一致性学生(GCS)模型，以及不确定性指导的指数移动平均(UEMA)方法

Result: 在四个跨域场景中实现最先进性能，在日夜自适应任务上获得12.9% NDS和9.5% mAP的提升

Conclusion: 该方法有效解决了BEV感知中的领域自适应挑战，显著提升了跨域3D目标检测性能

Abstract: Vision-centric Bird's Eye View (BEV) perception holds considerable promise
for autonomous driving. Recent studies have prioritized efficiency or accuracy
enhancements, yet the issue of domain shift has been overlooked, leading to
substantial performance degradation upon transfer. We identify major domain
gaps in real-world cross-domain scenarios and initiate the first effort to
address the Domain Adaptation (DA) challenge in multi-view 3D object detection
for BEV perception. Given the complexity of BEV perception approaches with
their multiple components, domain shift accumulation across multi-geometric
spaces (e.g., 2D, 3D Voxel, BEV) poses a significant challenge for BEV domain
adaptation. In this paper, we introduce an innovative geometric-aware
teacher-student framework, BEVUDA++, to diminish this issue, comprising a
Reliable Depth Teacher (RDT) and a Geometric Consistent Student (GCS) model.
Specifically, RDT effectively blends target LiDAR with dependable depth
predictions to generate depth-aware information based on uncertainty
estimation, enhancing the extraction of Voxel and BEV features that are
essential for understanding the target domain. To collaboratively reduce the
domain shift, GCS maps features from multiple spaces into a unified geometric
embedding space, thereby narrowing the gap in data distribution between the two
domains. Additionally, we introduce a novel Uncertainty-guided Exponential
Moving Average (UEMA) to further reduce error accumulation due to domain shifts
informed by previously obtained uncertainty guidance. To demonstrate the
superiority of our proposed method, we execute comprehensive experiments in
four cross-domain scenarios, securing state-of-the-art performance in BEV 3D
object detection tasks, e.g., 12.9\% NDS and 9.5\% mAP enhancement on Day-Night
adaptation.

</details>


### [80] [Where Do Tokens Go? Understanding Pruning Behaviors in STEP at High Resolutions](https://arxiv.org/abs/2509.14165)
*Michal Szczepanski,Martyna Poreba,Karim Haroun*

Main category: cs.CV

TL;DR: STEP框架通过动态patch合并和token剪枝，在保持精度的同时显著降低ViT的计算和内存成本


<details>
  <summary>Details</summary>
Motivation: ViT在语义分割中表现出色但计算和内存成本过高，需要高效的token减少方法来提升效率

Method: 提出STEP混合框架，包含轻量级CNN策略网络dCTS进行动态patch合并，并在编码器块中集成early-exit机制剪除高置信度token

Result: dCTS单独使用可减少2.5倍token数量，计算成本降低2.6倍，吞吐量提升3.4倍；完整STEP框架可达4倍计算复杂度降低和1.7倍推理加速，精度损失不超过2%

Conclusion: STEP框架有效解决了ViT的高计算成本问题，实现了效率与精度的良好平衡，40%的token可在最终层前被提前预测和终止

Abstract: Vision Transformers (ViTs) achieve state-of-the-art performance in semantic
segmentation but are hindered by high computational and memory costs. To
address this, we propose STEP (SuperToken and Early-Pruning), a hybrid
token-reduction framework that combines dynamic patch merging and token pruning
to enhance efficiency without significantly compromising accuracy. At the core
of STEP is dCTS, a lightweight CNN-based policy network that enables flexible
merging into superpatches. Encoder blocks integrate also early-exits to remove
high-confident supertokens, lowering computational load. We evaluate our method
on high-resolution semantic segmentation benchmarks, including images up to
1024 x 1024, and show that when dCTS is applied alone, the token count can be
reduced by a factor of 2.5 compared to the standard 16 x 16 pixel patching
scheme. This yields a 2.6x reduction in computational cost and a 3.4x increase
in throughput when using ViT-Large as the backbone. Applying the full STEP
framework further improves efficiency, reaching up to a 4x reduction in
computational complexity and a 1.7x gain in inference speed, with a maximum
accuracy drop of no more than 2.0%. With the proposed STEP configurations, up
to 40% of tokens can be confidently predicted and halted before reaching the
final encoder layer.

</details>


### [81] [Dense Video Understanding with Gated Residual Tokenization](https://arxiv.org/abs/2509.14199)
*Haichao Zhang,Wenhao Chai,Shwai He,Ang Li,Yun Fu*

Main category: cs.CV

TL;DR: 提出了Dense Video Understanding (DVU)框架和Gated Residual Tokenization (GRT)方法，通过运动补偿和语义融合技术实现高帧率视频理解，解决了现有视频大语言模型在密集时间信息处理上的效率问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频大语言模型主要依赖低帧率采样，丢弃了密集时间信息，无法处理需要精确时间对齐的任务（如讲座理解）。现有基准测试也仅关注粗粒度内容变化，缺乏对密集时间推理的评估。

Method: 提出了两阶段框架GRT：(1) Motion-Compensated Inter-Gated Tokenization利用像素级运动估计跳过静态区域，实现次线性增长的token数量和计算量；(2) Semantic-Scene Intra-Tokenization Merging在场景内融合静态区域的token，减少冗余同时保留动态语义。

Result: 在提出的DIVE基准测试上，GRT超越了更大的VLLM基线模型，并且随着帧率的提高性能持续提升，证明了密集时间信息的重要性。

Conclusion: GRT方法能够实现高效、可扩展的高帧率视频理解，填补了现有视频理解技术在密集时间推理方面的空白，为需要精确时间对齐的视频任务提供了有效解决方案。

Abstract: High temporal resolution is essential for capturing fine-grained details in
video understanding. However, current video large language models (VLLMs) and
benchmarks mostly rely on low-frame-rate sampling, such as uniform sampling or
keyframe selection, discarding dense temporal information. This compromise
avoids the high cost of tokenizing every frame, which otherwise leads to
redundant computation and linear token growth as video length increases. While
this trade-off works for slowly changing content, it fails for tasks like
lecture comprehension, where information appears in nearly every frame and
requires precise temporal alignment. To address this gap, we introduce Dense
Video Understanding (DVU), which enables high-FPS video comprehension by
reducing both tokenization time and token overhead. Existing benchmarks are
also limited, as their QA pairs focus on coarse content changes. We therefore
propose DIVE (Dense Information Video Evaluation), the first benchmark designed
for dense temporal reasoning. To make DVU practical, we present Gated Residual
Tokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-Gated
Tokenization uses pixel-level motion estimation to skip static regions during
tokenization, achieving sub-linear growth in token count and compute. (2)
Semantic-Scene Intra-Tokenization Merging fuses tokens across static regions
within a scene, further reducing redundancy while preserving dynamic semantics.
Experiments on DIVE show that GRT outperforms larger VLLM baselines and scales
positively with FPS. These results highlight the importance of dense temporal
information and demonstrate that GRT enables efficient, scalable high-FPS video
understanding.

</details>


### [82] [Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark](https://arxiv.org/abs/2509.14227)
*Nisarg A. Shah,Amir Ziai,Chaitanya Ekanadham,Vishal M. Patel*

Main category: cs.CV

TL;DR: 该论文提出了Cineaste基准测试，用于评估视觉语言模型在长视频电影叙事理解方面的能力，包含3119个多选题，覆盖5种细粒度推理类别，现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解基准测试主要关注短片段识别或模板化问题，缺乏对长视频叙事内容的细粒度推理能力评估。

Method: 使用GPT-4o生成多样化、上下文丰富的问题，整合视觉描述、字幕、场景标题和摘要，并采用两阶段过滤流程确保问题质量和事实一致性。

Result: 现有多模态大语言模型在Cineaste基准上表现困难，最佳开源模型准确率仅63.15%，长时序推理是主要瓶颈。

Conclusion: 该研究揭示了当前模型在细粒度上下文理解和长视频电影理解方面存在的显著挑战，需要进一步技术突破。

Abstract: While recent advancements in vision-language models have improved video
understanding, diagnosing their capacity for deep, narrative comprehension
remains a challenge. Existing benchmarks often test short-clip recognition or
use template-based questions, leaving a critical gap in evaluating fine-grained
reasoning over long-form narrative content. To address these gaps, we introduce
$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movie
understanding. Our dataset comprises 3,119 multiple-choice question-answer
pairs derived from 1,805 scenes across 200 diverse movies, spanning five novel
fine-grained contextual reasoning categories. We use GPT-4o to generate
diverse, context-rich questions by integrating visual descriptions, captions,
scene titles, and summaries, which require deep narrative understanding. To
ensure high-quality evaluation, our pipeline incorporates a two-stage filtering
process: Context-Independence filtering ensures questions require video
context, while Contextual Veracity filtering validates factual consistency
against the movie content, mitigating hallucinations. Experiments show that
existing MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis reveals
that long-range temporal reasoning is a primary bottleneck, with the top
open-source model achieving only 63.15\% accuracy. This underscores significant
challenges in fine-grained contextual understanding and the need for
advancements in long-form movie comprehension.

</details>


### [83] [GenExam: A Multidisciplinary Text-to-Image Exam](https://arxiv.org/abs/2509.14232)
*Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo*

Main category: cs.CV

TL;DR: GenExam是首个多学科文本到图像考试基准，包含1000个样本，涵盖10个学科，采用四级分类的考试风格提示。现有最先进模型在该基准上的严格得分不足15%，表明其具有巨大挑战性。


<details>
  <summary>Details</summary>
Motivation: 现有考试风格基准主要关注理解和推理任务，而生成基准侧重于世界知识和视觉概念的展示，缺乏对严格绘画考试的评价。

Method: 构建包含1000个样本的多学科文本到图像考试基准，每个问题配备真实图像和细粒度评分点，用于精确评估语义正确性和视觉合理性。

Result: 实验显示，即使是最先进的模型如GPT-Image-1和Gemini-2.5-Flash-Image，严格得分也低于15%，大多数模型得分接近0%。

Conclusion: 通过将图像生成构建为考试形式，GenExam提供了对模型整合知识、推理和生成能力的严格评估，为通向通用AGI的道路提供了见解。

Abstract: Exams are a fundamental test of expert-level intelligence and require
integrated understanding, reasoning, and generation. Existing exam-style
benchmarks mainly focus on understanding and reasoning tasks, and current
generation benchmarks emphasize the illustration of world knowledge and visual
concepts, neglecting the evaluation of rigorous drawing exams. We introduce
GenExam, the first benchmark for multidisciplinary text-to-image exams,
featuring 1,000 samples across 10 subjects with exam-style prompts organized
under a four-level taxonomy. Each problem is equipped with ground-truth images
and fine-grained scoring points to enable a precise evaluation of semantic
correctness and visual plausibility. Experiments show that even
state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve
less than 15% strict scores, and most models yield almost 0%, suggesting the
great challenge of our benchmark. By framing image generation as an exam,
GenExam offers a rigorous assessment of models' ability to integrate knowledge,
reasoning, and generation, providing insights on the path to general AGI.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [84] [Semantic 3D Reconstructions with SLAM for Central Airway Obstruction](https://arxiv.org/abs/2509.13541)
*Ayberk Acar,Fangjie Li,Hao Li,Lidia Al-Zogbi,Kanyifeechukwu Jane Oguine,Susheela Sharma Stern,Jesse F. d'Almeida,Robert J. Webster III,Ipek Oguz,Jie Ying Wu*

Main category: cs.RO

TL;DR: 这篇论文提出了一种基于单目内镜视频的实时语义3D重建流水线，用于中央气道阻塞患者的自动化手术干预。该方法结合SLAM技术和分割模型，能够在重建3D气道几何的同时标注阻塞组织区域，为机器人手术提供临床相关信息。


<details>
  <summary>Details</summary>
Motivation: 中央气道阻塞(CAO)是一种危及生命的疾病，传统治疗方法风险高。虽然机器人手术有更低风险，但需要场景理解和地图构建来支持自动化。需要一种能够在内镜手术中实时生成带有临床意义标注的3D地图的方法。

Method: 结合DROID-SLAM算法和一个训练用于识别阻塞组织的分割模型。SLAM模块负责实时重建气道3D几何，分割掩码用于在重建的点云中标注阻塞区域。通过ex vivo模型评估重建质量。

Result: 定性和定量结果显示了高度相似性：与真实CT扫描的Chamfer距离仅0.62mm。系统能够生产带有临床相关区域标注的3D地图，重建速度比以往方法更快，更准确反映手术场景。

Conclusion: 这是首次将语义分割与实时单目SLAM集成到内镜CAO场景中。该框架模块化且可以简单地沿用到其他解剖结构或手术中，为自主机器人干预提供了有前景的步骤。

Abstract: Central airway obstruction (CAO) is a life-threatening condition with
increasing incidence, caused by tumors in and outside of the airway.
Traditional treatment methods such as bronchoscopy and electrocautery can be
used to remove the tumor completely; however, these methods carry a high risk
of complications. Recent advances allow robotic interventions with lesser risk.
The combination of robot interventions with scene understanding and mapping
also opens up the possibilities for automation. We present a novel pipeline
that enables real-time, semantically informed 3D reconstructions of the central
airway using monocular endoscopic video.
  Our approach combines DROID-SLAM with a segmentation model trained to
identify obstructive tissues. The SLAM module reconstructs the 3D geometry of
the airway in real time, while the segmentation masks guide the annotation of
obstruction regions within the reconstructed point cloud. To validate our
pipeline, we evaluate the reconstruction quality using ex vivo models.
  Qualitative and quantitative results show high similarity between ground
truth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). By
integrating segmentation directly into the SLAM workflow, our system produces
annotated 3D maps that highlight clinically relevant regions in real time.
High-speed capabilities of the pipeline allows quicker reconstructions compared
to previous work, reflecting the surgical scene more accurately.
  To the best of our knowledge, this is the first work to integrate semantic
segmentation with real-time monocular SLAM for endoscopic CAO scenarios. Our
framework is modular and can generalize to other anatomies or procedures with
minimal changes, offering a promising step toward autonomous robotic
interventions.

</details>


### [85] [Object Pose Estimation through Dexterous Touch](https://arxiv.org/abs/2509.13591)
*Amir-Hossein Shahidzadeh,Jiyue Zhu,Kezhou Chen,Sha Yi,Cornelia Fermüller,Yiannis Aloimonos,Xiaolong Wang*

Main category: cs.RO

TL;DR: 使用强化学习训练机器人双手协作，通过触觉传感器主动探索物体表面，从局部3D点云数据迭代优化物体姿态估计，无需先验几何知识


<details>
  <summary>Details</summary>
Motivation: 解决在视觉数据受限或对光照、遮挡敏感的场景下，仅依靠局部触觉信息进行物体姿态估计的挑战

Method: 采用传感器运动探索策略，用强化学习训练机器人控制手部主动交互，收集触觉数据生成3D点云，通过迭代优化方法重构物体形状和姿态

Result: 该方法能够主动探索物体表面并识别关键姿态特征，无需物体几何先验知识

Conclusion: 提出的双手触觉姿态估计方法为机器人操作任务提供了有效的解决方案，特别是在视觉受限环境中

Abstract: Robust object pose estimation is essential for manipulation and interaction
tasks in robotics, particularly in scenarios where visual data is limited or
sensitive to lighting, occlusions, and appearances. Tactile sensors often offer
limited and local contact information, making it challenging to reconstruct the
pose from partial data. Our approach uses sensorimotor exploration to actively
control a robot hand to interact with the object. We train with Reinforcement
Learning (RL) to explore and collect tactile data. The collected 3D point
clouds are used to iteratively refine the object's shape and pose. In our
setup, one hand holds the object steady while the other performs active
exploration. We show that our method can actively explore an object's surface
to identify critical pose features without prior knowledge of the object's
geometry. Supplementary material and more demonstrations will be provided at
https://amirshahid.github.io/BimanualTactilePose .

</details>


### [86] [InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap](https://arxiv.org/abs/2509.13857)
*Nguyen Hoang Khoi Tran,Julie Stephany Berrio,Mao Shan,Stewart Worrall*

Main category: cs.RO

TL;DR: InterKey是一个利用道路交叉口作为标志性路标的跨模态全局定位框架，通过点云和OSM数据构建紧凑的二进制描述符，在GNSS信号受限环境下实现高精度车辆定位。


<details>
  <summary>Details</summary>
Motivation: 解决在GNSS信号退化环境（如城市峡谷和隧道）中可靠全局定位的问题，同时克服高精地图成本高和OpenStreetMap数据粗糙的局限性。

Method: 提出跨模态框架，使用道路交叉口作为地标，通过联合编码点云和OSM中的道路和建筑印记来构建二进制描述符，并引入差异缓解、方向确定和区域均衡采样策略。

Result: 在KITTI数据集上实现了最先进的精度，大幅优于现有基线方法。

Conclusion: 该框架可推广到能够产生密集结构点云的传感器，为稳健的车辆定位提供了可扩展且经济高效的解决方案。

Abstract: Reliable global localization is critical for autonomous vehicles, especially
in environments where GNSS is degraded or unavailable, such as urban canyons
and tunnels. Although high-definition (HD) maps provide accurate priors, the
cost of data collection, map construction, and maintenance limits scalability.
OpenStreetMap (OSM) offers a free and globally available alternative, but its
coarse abstraction poses challenges for matching with sensor data. We propose
InterKey, a cross-modal framework that leverages road intersections as
distinctive landmarks for global localization. Our method constructs compact
binary descriptors by jointly encoding road and building imprints from point
clouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,
orientation determination, and area-equalized sampling strategies, enabling
robust cross-modal matching. Experiments on the KITTI dataset demonstrate that
InterKey achieves state-of-the-art accuracy, outperforming recent baselines by
a large margin. The framework generalizes to sensors that can produce dense
structural point clouds, offering a scalable and cost-effective solution for
robust vehicle localization.

</details>


### [87] [MAP: End-to-End Autonomous Driving with Map-Assisted Planning](https://arxiv.org/abs/2509.13926)
*Huilin Yin,Yiming Kan,Daniel Watzenig*

Main category: cs.RO

TL;DR: 提出MAP框架，通过在线地图模块显式整合语义地图特征和车辆状态，显著提升端到端自动驾驶的轨迹规划性能


<details>
  <summary>Details</summary>
Motivation: 现有端到端自动驾驶方法未充分利用在线地图模块的潜力，地图信息对轨迹规划的增强作用未被充分挖掘

Method: MAP框架包含三个核心模块：规划增强在线地图模块、车辆状态引导规划模块、基于当前车辆状态的权重适配器，显式整合分割式地图特征和当前车辆状态

Result: 在DAIR-V2X-seq-SPD数据集上，相比UniV2X基线方法，L2位移误差降低16.6%，离道率降低56.2%，综合得分提升44.5%；在CVPR2025 MEIS Workshop竞赛中排名第一，综合得分比第二名高39.5%

Conclusion: 显式利用语义地图特征能有效提升规划性能，为端到端自动驾驶系统的结构设计提供了新的改进方向

Abstract: In recent years, end-to-end autonomous driving has attracted increasing
attention for its ability to jointly model perception, prediction, and planning
within a unified framework. However, most existing approaches underutilize the
online mapping module, leaving its potential to enhance trajectory planning
largely untapped. This paper proposes MAP (Map-Assisted Planning), a novel
map-assisted end-to-end trajectory planning framework. MAP explicitly
integrates segmentation-based map features and the current ego status through a
Plan-enhancing Online Mapping module, an Ego-status-guided Planning module, and
a Weight Adapter based on current ego status. Experiments conducted on the
DAIR-V2X-seq-SPD dataset demonstrate that the proposed method achieves a 16.6%
reduction in L2 displacement error, a 56.2% reduction in off-road rate, and a
44.5% improvement in overall score compared to the UniV2X baseline, even
without post-processing. Furthermore, it achieves top ranking in Track 2 of the
End-to-End Autonomous Driving through V2X Cooperation Challenge of MEIS
Workshop @CVPR2025, outperforming the second-best model by 39.5% in terms of
overall score. These results highlight the effectiveness of explicitly
leveraging semantic map features in planning and suggest new directions for
improving structure design in end-to-end autonomous driving systems. Our code
is available at https://gitee.com/kymkym/map.git

</details>


### [88] [MetricNet: Recovering Metric Scale in Generative Navigation Policies](https://arxiv.org/abs/2509.13965)
*Abhijeet Nayak,Débora N. P. Oliveira,Samiran Gode,Cordelia Schmid,Wolfram Burgard*

Main category: cs.RO

TL;DR: 提出MetricNet方法解决生成式导航策略中轨迹尺度不准确和短视控制的问题，通过预测路径点间的真实度量距离来改进导航性能


<details>
  <summary>Details</summary>
Motivation: 生成式导航策略存在两个结构性问题：1) 采样轨迹存在于抽象的无尺度空间，缺乏度量基础；2) 控制策略丢弃完整路径，只朝单个路径点移动，导致短视和不安全的动作

Method: 提出MetricNet作为生成式导航的附加模块，预测路径点间的度量距离，将策略输出锚定到真实世界坐标中。进一步提出MetricNav，将MetricNet集成到导航策略中，引导机器人避开障碍物同时向目标移动

Result: 在仿真中使用新的基准框架评估，执行MetricNet缩放的路径点显著提高了导航和探索性能。在真实世界实验中进一步验证了方法的有效性

Conclusion: MetricNet能够有效解决生成式导航的尺度问题和短视行为，通过度量距离预测和路径整合显著提升了导航的安全性和性能

Abstract: Generative navigation policies have made rapid progress in improving
end-to-end learned navigation. Despite their promising results, this paradigm
has two structural problems. First, the sampled trajectories exist in an
abstract, unscaled space without metric grounding. Second, the control strategy
discards the full path, instead moving directly towards a single waypoint. This
leads to short-sighted and unsafe actions, moving the robot towards obstacles
that a complete and correctly scaled path would circumvent. To address these
issues, we propose MetricNet, an effective add-on for generative navigation
that predicts the metric distance between waypoints, grounding policy outputs
in real-world coordinates. We evaluate our method in simulation with a new
benchmarking framework and show that executing MetricNet-scaled waypoints
significantly improves both navigation and exploration performance. Beyond
simulation, we further validate our approach in real-world experiments.
Finally, we propose MetricNav, which integrates MetricNet into a navigation
policy to guide the robot away from obstacles while still moving towards the
goal.

</details>


### [89] [MCGS-SLAM: A Multi-Camera SLAM Framework Using Gaussian Splatting for High-Fidelity Mapping](https://arxiv.org/abs/2509.14191)
*Zhihao Cao,Hanyu Wu,Li Wa Tang,Zizhou Luo,Zihan Zhu,Wei Zhang,Marc Pollefeys,Martin R. Oswald*

Main category: cs.RO

TL;DR: MCGS-SLAM是首个基于3D高斯点云的纯RGB多相机SLAM系统，通过多视角融合和尺度一致性实现了高精度轨迹和逼真重建


<details>
  <summary>Details</summary>
Motivation: 现有密集SLAM方法主要针对单目设置，牺牲了鲁棒性和几何覆盖范围，需要开发多相机系统来提升重建质量

Method: 使用多相机束调整(MCBA)通过密集光度/几何残差联合优化位姿和深度，采用尺度一致性模块和低秩先验实现多视图度量对齐

Result: 在合成和真实数据集上表现优于单目基线，能够重建单目系统遗漏的侧视区域，支持实时大规模RGB输入

Conclusion: 多相机高斯点云SLAM在机器人和自动驾驶的高保真地图构建中具有重要应用前景

Abstract: Recent progress in dense SLAM has primarily targeted monocular setups, often
at the expense of robustness and geometric coverage. We present MCGS-SLAM, the
first purely RGB-based multi-camera SLAM system built on 3D Gaussian Splatting
(3DGS). Unlike prior methods relying on sparse maps or inertial data, MCGS-SLAM
fuses dense RGB inputs from multiple viewpoints into a unified, continuously
optimized Gaussian map. A multi-camera bundle adjustment (MCBA) jointly refines
poses and depths via dense photometric and geometric residuals, while a scale
consistency module enforces metric alignment across views using low-rank
priors. The system supports RGB input and maintains real-time performance at
large scale. Experiments on synthetic and real-world datasets show that
MCGS-SLAM consistently yields accurate trajectories and photorealistic
reconstructions, usually outperforming monocular baselines. Notably, the wide
field of view from multi-camera input enables reconstruction of side-view
regions that monocular setups miss, critical for safe autonomous operation.
These results highlight the promise of multi-camera Gaussian Splatting SLAM for
high-fidelity mapping in robotics and autonomous driving.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [90] [Rest2Visual: Predicting Visually Evoked fMRI from Resting-State Scans](https://arxiv.org/abs/2509.13612)
*Chuyang Zhou,Ziao Ji,Daochang Liu,Dongang Wang,Chenyu Wang,Chang Xu*

Main category: q-bio.NC

TL;DR: Rest2Visual是一个条件生成模型，能够从静息态fMRI和2D视觉刺激预测视觉诱发fMRI激活，实现了将个体化自发神经活动转化为刺激对齐的功能表征。


<details>
  <summary>Details</summary>
Motivation: 解决任务态fMRI获取成本高、难以规模化的问题，同时克服静息态fMRI缺乏直接可解释性的局限，探索如何利用丰富的静息态数据预测刺激诱发的大脑激活。

Method: 采用体积编码器-解码器设计，通过自适应归一化将多尺度3D静息态fMRI特征与图像嵌入进行调制，构建大规模三元组数据集进行模型训练。

Result: 预测的激活图在相似性和表征指标上与真实值高度匹配，支持下游图像重建，并保留了被试特定的功能结构，能够生成个体化的功能替代物。

Conclusion: 研究证明了个体化自发神经活动可以转化为刺激对齐的表征，为可扩展、无需任务的功能性脑建模开辟了新途径。

Abstract: Understanding how spontaneous brain activity relates to stimulus-driven
neural responses is a fundamental challenge in cognitive neuroscience. While
task-based functional magnetic resonance imaging (fMRI) captures localized
stimulus-evoked brain activation, its acquisition is costly, time-consuming,
and difficult to scale across populations. In contrast, resting-state fMRI
(rs-fMRI) is task-free and abundant, but lacks direct interpretability. We
introduce Rest2Visual, a conditional generative model that predicts visually
evoked fMRI (ve-fMRI) from resting-state input and 2D visual stimuli. It
follows a volumetric encoder--decoder design, where multiscale 3D features from
rs-fMRI are modulated by image embeddings via adaptive normalization, enabling
spatially accurate, stimulus-specific activation synthesis. To enable model
training, we construct a large-scale triplet dataset from the Natural Scenes
Dataset (NSD), aligning each rs-fMRI volume with stimulus images and their
corresponding ve-fMRI activation maps. Quantitative evaluation shows that the
predicted activations closely match ground truth across standard similarity and
representational metrics, and support successful image reconstruction in
downstream decoding. Notably, the predicted maps preserve subject-specific
structure, demonstrating the model's capacity to generate individualized
functional surrogates. Our results provide compelling evidence that
individualized spontaneous neural activity can be transformed into
stimulus-aligned representations, opening new avenues for scalable, task-free
functional brain modeling.

</details>


<div id='q-bio.PE'></div>

# q-bio.PE [[Back]](#toc)

### [91] [Autonomous Reporting of Normal Chest X-rays by Artificial Intelligence in the United Kingdom. Can We Take the Human Out of the Loop?](https://arxiv.org/abs/2509.13428)
*Katrina Nash,James Vaz,Ahmed Maiter,Christopher Johns,Nicholas Woznitza,Aditya Kale,Abdala Espinosa Morgado,Rhidian Bramley,Mark Hall,David Lowe,Alex Novak,Sarim Ather*

Main category: q-bio.PE

TL;DR: 本文探讨AI自主报告正常胸部X光片的可行性，分析技术、法律和实践挑战，强调谨慎采用的重要性


<details>
  <summary>Details</summary>
Motivation: 解决放射科医生短缺导致的X光片报告延迟问题，通过AI识别正常X光片来减轻放射科工作负担

Method: 分析AI工具区分正常与异常胸部X光片的能力，探讨定义正常标准、泛化性、灵敏度-特异性平衡等技术问题

Result: 识别出AI自主报告正常X光片面临多重挑战，包括技术标准定义、法律合规性、责任框架缺失等问题

Conclusion: 虽然AI自主报告正常X光片具有明显优势，但必须谨慎采用，需要解决技术、法律和实践层面的各种挑战

Abstract: Chest X-rays (CXRs) are the most commonly performed imaging investigation. In
the UK, many centres experience reporting delays due to radiologist workforce
shortages. Artificial intelligence (AI) tools capable of distinguishing normal
from abnormal CXRs have emerged as a potential solution. If normal CXRs could
be safely identified and reported without human input, a substantial portion of
radiology workload could be reduced.
  This article examines the feasibility and implications of autonomous AI
reporting of normal CXRs. Key issues include defining normal, ensuring
generalisability across populations, and managing the sensitivity-specificity
trade-off. It also addresses legal and regulatory challenges, such as
compliance with IR(ME)R and GDPR, and the lack accountability frameworks for
errors. Further considerations include the impact on radiologists practice, the
need for robust post-market surveillance, and incorporation of patient
perspectives. While the benefits are clear, adoption must be cautious.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [92] [A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds](https://arxiv.org/abs/2509.13390)
*Deepti Kunte,Bram Cornelis,Claudio Colangeli,Karl Janssens,Brecht Van Baelen,Konstantinos Gryllias*

Main category: cs.SD

TL;DR: 提出基于领域知识的模型选择方法，使用健康样本的扰动生成代理异常，在无标签故障数据的情况下有效选择异常检测模型


<details>
  <summary>Details</summary>
Motivation: 汽车舱内声音异常检测在缺乏标记故障数据的情况下面临模型选择的挑战，传统验证方法如重构误差可靠性有限

Method: 通过结构化扰动健康样本的频谱图生成代理异常，构建验证集来支持模型选择

Result: 在包含五种典型故障类型的高保真电动车数据集上实验表明，该方法显著优于传统模型选择策略

Conclusion: 基于代理异常的模型选择方法为无监督异常检测提供了有效的验证框架，特别是在缺乏真实故障样本的场景下

Abstract: The detection of anomalies in automotive cabin sounds is critical for
ensuring vehicle quality and maintaining passenger comfort. In many real-world
settings, this task is more appropriately framed as an unsupervised learning
problem rather than the supervised case due to the scarcity or complete absence
of labeled faulty data. In such an unsupervised setting, the model is trained
exclusively on healthy samples and detects anomalies as deviations from normal
behavior. However, in the absence of labeled faulty samples for validation and
the limited reliability of commonly used metrics, such as validation
reconstruction error, effective model selection remains a significant
challenge. To overcome these limitations, a domain-knowledge-informed approach
for model selection is proposed, in which proxy-anomalies engineered through
structured perturbations of healthy spectrograms are used in the validation set
to support model selection. The proposed methodology is evaluated on a
high-fidelity electric vehicle dataset comprising healthy and faulty cabin
sounds across five representative fault types viz., Imbalance, Modulation,
Whine, Wind, and Pulse Width Modulation. This dataset, generated using advanced
sound synthesis techniques, and validated via expert jury assessments, has been
made publicly available to facilitate further research. Experimental
evaluations on the five fault cases demonstrate the selection of optimal models
using proxy-anomalies, significantly outperform conventional model selection
strategies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [93] [LLM-I: LLMs are Naturally Interleaved Multimodal Creators](https://arxiv.org/abs/2509.13642)
*Zirun Guo,Feng Zhang,Kai Jia,Tao Jin*

Main category: cs.LG

TL;DR: LLM-Interleaved是一个将交错图像-文本生成重新定义为工具使用问题的动态框架，通过强化学习训练LLM智能调度多种视觉工具，在多个基准测试中大幅超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决当前统一模型受限于合成图像且难以处理需要事实基础或程序化精度的任务的"单一工具"瓶颈问题。

Method: 设计基于强化学习的框架，让中心LLM/MLLM代理智能调度多种专业视觉工具（在线图像搜索、扩散生成、代码执行、图像编辑），使用结合规则逻辑和LLM/MLLM评估的混合奖励系统进行训练。

Result: 在四个基准测试中展现出最先进的性能，大幅超越现有方法，并引入了新颖的测试时缩放策略进一步提升性能。

Conclusion: LLM-I框架成功突破了现有模型的限制，通过工具使用的方法有效解决了交错图像-文本生成中的复杂任务需求。

Abstract: We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that
reframes interleaved image-text generation as a tool-use problem. LLM-I is
designed to overcome the "one-tool" bottleneck of current unified models, which
are limited to synthetic imagery and struggle with tasks requiring factual
grounding or programmatic precision. Our framework empowers a central LLM or
MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual
tools, including online image search, diffusion-based generation, code
execution, and image editing. The agent is trained to select and apply these
tools proficiently via a Reinforcement Learning (RL) framework that features a
hybrid reward system combining rule-based logic with judgments from LLM and
MLLM evaluators. Trained on a diverse new dataset using four different model
backbones, LLM-I demonstrates state-of-the-art performance, outperforming
existing methods by a large margin across four benchmarks. We also introduce a
novel test-time scaling strategy that provides further performance gains.
Project Page: https://github.com/ByteDance-BandAI/LLM-I.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [94] [The Art of Saying "Maybe": A Conformal Lens for Uncertainty Benchmarking in VLMs](https://arxiv.org/abs/2509.13379)
*Asif Azad,Mohammad Sadat Hossain,MD Sadik Hossain Shanto,M Saifur Rahman,Md Rizwan Pervez*

Main category: cs.AI

TL;DR: 对16个先进视觉语言模型在6个多模态数据集上进行全面的不确定性基准测试，发现大模型具有更好的不确定性量化能力，数学和推理任务的不确定性表现较差


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在复杂视觉理解方面取得显著进展，但不确定性量化这一关键维度未得到足够关注，现有研究设置有限

Method: 评估16个最先进的VLM模型（开源和闭源），在6个多模态数据集上使用3种不同的评分函数进行不确定性基准测试

Result: 较大模型展现出更好的不确定性量化能力；确定性更高的模型准确率更高；数学和推理任务在所有模型中表现出较差的不确定性性能

Conclusion: 本研究为多模态系统中可靠的不确定性评估奠定了基础，表明模型知道得越多，也越清楚自己不知道什么

Abstract: Vision-Language Models (VLMs) have achieved remarkable progress in complex
visual understanding across scientific and reasoning tasks. While performance
benchmarking has advanced our understanding of these capabilities, the critical
dimension of uncertainty quantification has received insufficient attention.
Therefore, unlike prior conformal prediction studies that focused on limited
settings, we conduct a comprehensive uncertainty benchmarking study, evaluating
16 state-of-the-art VLMs (open and closed-source) across 6 multimodal datasets
with 3 distinct scoring functions. Our findings demonstrate that larger models
consistently exhibit better uncertainty quantification; models that know more
also know better what they don't know. More certain models achieve higher
accuracy, while mathematical and reasoning tasks elicit poorer uncertainty
performance across all models compared to other domains. This work establishes
a foundation for reliable uncertainty evaluation in multimodal systems.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [95] [3D Reconstruction of Coronary Vessel Trees from Biplanar X-Ray Images Using a Geometric Approach](https://arxiv.org/abs/2509.13358)
*Ethan Koland,Lin Xi,Nadeev Wijesuriya,YingLiang Ma*

Main category: eess.IV

TL;DR: 提出了一种从双平面X射线图像重建3D血管树的框架，包括图像分割、运动相位匹配和3D重建三个主要组件，相比传统方法简化了工作流程并提高了准确性。


<details>
  <summary>Details</summary>
Motivation: X射线血管造影在心脏介入手术中广泛应用，但需要从不同角度拍摄的X射线视频中重建3D血管树来更好地可视化冠状动脉、评估完整性和检测狭窄。

Method: 采用自动视频分割方法进行语义分割，通过跟踪导管等静止物体实现运动相位匹配，使用启发式方法匹配关键解剖标志点，最后通过新颖的几何重建算法计算3D血管中心线。

Result: 在62个X射线血管造影视频序列上训练和验证，分割准确率达到0.703，3D重建的关键解剖标志点重投影误差为0.62mm ± 0.38mm。

Conclusion: 该框架通过简化的重建工作流程和创新的几何算法，显著提高了3D血管树重建的准确性和效率，在心脏介入手术中具有重要应用价值。

Abstract: X-ray angiography is widely used in cardiac interventions to visualize
coronary vessels, assess integrity, detect stenoses and guide treatment. We
propose a framework for reconstructing 3D vessel trees from biplanar X-ray
images which are extracted from two X-ray videos captured at different C-arm
angles. The proposed framework consists of three main components: image
segmentation, motion phase matching, and 3D reconstruction. An automatic video
segmentation method for X-ray angiography to enable semantic segmentation for
image segmentation and motion phase matching. The goal of the motion phase
matching is to identify a pair of X-ray images that correspond to a similar
respiratory and cardiac motion phase to reduce errors in 3D reconstruction.
This is achieved by tracking a stationary object such as a catheter or lead
within the X-ray video. The semantic segmentation approach assigns different
labels to different object classes enabling accurate differentiation between
blood vessels, balloons, and catheters. Once a suitable image pair is selected,
key anatomical landmarks (vessel branching points and endpoints) are matched
between the two views using a heuristic method that minimizes reconstruction
errors. This is followed by a novel geometric reconstruction algorithm to
generate the 3D vessel tree. The algorithm computes the 3D vessel centrelines
by determining the intersection of two 3D surfaces. Compared to traditional
methods based on epipolar constraints, the proposed approach simplifies there
construction workflow and improves overall accuracy. We trained and validated
our segmentation method on 62 X-ray angiography video sequences. On the test
set, our method achieved a segmentation accuracy of 0.703. The 3D
reconstruction framework was validated by measuring the reconstruction error of
key anatomical landmarks, achieving a reprojection errors of 0.62mm +/- 0.38mm.

</details>


### [96] [PREDICT-GBM: Platform for Robust Evaluation and Development of Individualized Computational Tumor Models in Glioblastoma](https://arxiv.org/abs/2509.13360)
*L. Zimmer,J. Weidner,M. Balcerak,F. Kofler,I. Ezhov,B. Menze,B. Wiestler*

Main category: eess.IV

TL;DR: PREDICT-GBM是一个用于胶质母细胞瘤生长建模和评估的综合平台，包含255名患者的临床数据集，能够系统评估肿瘤生长模型并指导个性化放疗计划。


<details>
  <summary>Details</summary>
Motivation: 传统放疗采用统一边界，无法考虑患者特异性解剖和生物学因素。尽管已有多种胶质母细胞瘤生长计算模型，但临床采用仍有限，需要搭建平台来弥合转化差距。

Method: 开发PREDICT-GBM集成管道和数据集，包含专家策划的255名患者临床数据（完整肿瘤分割和组织特征图），用于系统评估最先进的肿瘤生长模型。

Result: 分析显示，基于两种肿瘤生长模型预测的个性化放疗计划相比传统统一边界方法，实现了更好的复发覆盖效果。

Conclusion: 该工作建立了强大平台，用于推进和系统评估前沿肿瘤生长建模方法，最终目标是促进临床转化并改善患者预后。

Abstract: Glioblastoma is the most prevalent primary brain malignancy, distinguished by
its highly invasive behavior and exceptionally high rates of recurrence.
Conventional radiation therapy, which employs uniform treatment margins, fails
to account for patient-specific anatomical and biological factors that
critically influence tumor cell migration. To address this limitation, numerous
computational models of glioblastoma growth have been developed, enabling
generation of tumor cell distribution maps extending beyond radiographically
visible regions and thus informing more precise treatment strategies. However,
despite encouraging preliminary findings, the clinical adoption of these growth
models remains limited. To bridge this translational gap and accelerate both
model development and clinical validation, we introduce PREDICT-GBM, a
comprehensive integrated pipeline and dataset for modeling and evaluation. This
platform enables systematic benchmarking of state-of-the-art tumor growth
models using an expert-curated clinical dataset comprising 255 subjects with
complete tumor segmentations and tissue characterization maps. Our analysis
demonstrates that personalized radiation treatment plans derived from tumor
growth predictions achieved superior recurrence coverage compared to
conventional uniform margin approaches for two of the evaluated models. This
work establishes a robust platform for advancing and systematically evaluating
cutting-edge tumor growth modeling approaches, with the ultimate goal of
facilitating clinical translation and improving patient outcomes.

</details>


### [97] [Generative AI Pipeline for Interactive Prompt-driven 2D-to-3D Vascular Reconstruction for Fontan Geometries from Contrast-Enhanced X-Ray Fluoroscopy Imaging](https://arxiv.org/abs/2509.13372)
*Prahlad G Menon*

Main category: eess.IV

TL;DR: 开发了一个多步骤AI流水线，利用Google Gemini 2.5 Flash和腾讯Hunyuan3D-2mini模型，从单视角荧光血管造影图像生成适合计算流体动力学分析的几何优化2D投影和3D立体光刻文件，实现了快速虚拟血流可视化。


<details>
  <summary>Details</summary>
Motivation: Fontan姑息治疗后的单心室先天性心脏病进展为血流动力学衰竭，传统2D成像难以表征复杂血流模式，现有荧光血管造影评估方法提供的3D几何信息有限，无法满足计算流体动力学分析和手术规划需求。

Method: 采用基于transformer神经架构的多步骤AI流水线，包括医学图像预处理、血管分割、对比度增强、伪影去除和虚拟血流动力学可视化，最后通过立体光刻文件生成模型进行3D转换，共16个处理步骤。

Result: 成功从单视角血管造影生成几何优化的2D投影，准确保留了复杂的Fontan几何结构，虚拟血流可视化识别出中心连接处的停滞区和分支动脉的血流模式，完整处理时间不到15分钟。

Conclusion: 该方法展示了从常规血管造影数据生成适合CFD的几何结构的临床可行性，为使用现成成像数据进行高级几何和血流动力学分析的普及奠定了基础，尽管需要迭代优化来确保准确性。

Abstract: Fontan palliation for univentricular congenital heart disease progresses to
hemodynamic failure with complex flow patterns poorly characterized by
conventional 2D imaging. Current assessment relies on fluoroscopic angiography,
providing limited 3D geometric information essential for computational fluid
dynamics (CFD) analysis and surgical planning.
  A multi-step AI pipeline was developed utilizing Google's Gemini 2.5 Flash
(2.5B parameters) for systematic, iterative processing of fluoroscopic
angiograms through transformer-based neural architecture. The pipeline
encompasses medical image preprocessing, vascular segmentation, contrast
enhancement, artifact removal, and virtual hemodynamic flow visualization
within 2D projections. Final views were processed through Tencent's
Hunyuan3D-2mini (384M parameters) for stereolithography file generation.
  The pipeline successfully generated geometrically optimized 2D projections
from single-view angiograms after 16 processing steps using a custom web
interface. Initial iterations contained hallucinated vascular features
requiring iterative refinement to achieve anatomically faithful
representations. Final projections demonstrated accurate preservation of
complex Fontan geometry with enhanced contrast suitable for 3D conversion.
AI-generated virtual flow visualization identified stagnation zones in central
connections and flow patterns in branch arteries. Complete processing required
under 15 minutes with second-level API response times.
  This approach demonstrates clinical feasibility of generating CFD-suitable
geometries from routine angiographic data, enabling 3D generation and rapid
virtual flow visualization for cursory insights prior to full CFD simulation.
While requiring refinement cycles for accuracy, this establishes foundation for
democratizing advanced geometric and hemodynamic analysis using readily
available imaging data.

</details>


### [98] [Cross-Distribution Diffusion Priors-Driven Iterative Reconstruction for Sparse-View CT](https://arxiv.org/abs/2509.13576)
*Haodong Li,Shuo Han,Haiyang Mao,Yu Shi,Changsheng Fang,Jianjia Zhang,Weiwen Wu,Hengyong Yu*

Main category: eess.IV

TL;DR: 提出CDPIR框架解决稀疏视图CT重建中的分布外问题，通过跨分布扩散先验和基于模型的迭代重建方法，在OOD场景下实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CT重建虽然提高时间分辨率和降低辐射剂量，但由于视图减少和域偏移（扫描仪、协议或解剖变异）导致的伪影，在分布外场景中性能下降，限制了临床应用

Method: 使用可扩展插值变换器(SiT)建立统一随机插值框架，结合无分类器引导跨多个数据集训练，通过随机丢弃条件学习域特定和域不变先验，在采样时利用基于变换器的扩散模型控制多分布到噪声插值路径

Result: CDPIR在稀疏视图CT重建中实现最先进性能，具有优异的细节保留能力，在分布外条件下显著优于现有方法

Conclusion: 该方法展示了在挑战性成像场景中的鲁棒性和潜在临床价值，为OOD重建提供了灵活的适应性

Abstract: Sparse-View CT (SVCT) reconstruction enhances temporal resolution and reduces
radiation dose, yet its clinical use is hindered by artifacts due to view
reduction and domain shifts from scanner, protocol, or anatomical variations,
leading to performance degradation in out-of-distribution (OOD) scenarios. In
this work, we propose a Cross-Distribution Diffusion Priors-Driven Iterative
Reconstruction (CDPIR) framework to tackle the OOD problem in SVCT. CDPIR
integrates cross-distribution diffusion priors, derived from a Scalable
Interpolant Transformer (SiT), with model-based iterative reconstruction
methods. Specifically, we train a SiT backbone, an extension of the Diffusion
Transformer (DiT) architecture, to establish a unified stochastic interpolant
framework, leveraging Classifier-Free Guidance (CFG) across multiple datasets.
By randomly dropping the conditioning with a null embedding during training,
the model learns both domain-specific and domain-invariant priors, enhancing
generalizability. During sampling, the globally sensitive transformer-based
diffusion model exploits the cross-distribution prior within the unified
stochastic interpolant framework, enabling flexible and stable control over
multi-distribution-to-noise interpolation paths and decoupled sampling
strategies, thereby improving adaptation to OOD reconstruction. By alternating
between data fidelity and sampling updates, our model achieves state-of-the-art
performance with superior detail preservation in SVCT reconstructions.
Extensive experiments demonstrate that CDPIR significantly outperforms existing
approaches, particularly under OOD conditions, highlighting its robustness and
potential clinical value in challenging imaging scenarios.

</details>
