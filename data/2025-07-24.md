<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 95]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.LG](#cs.LG) [Total: 6]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.QM](#q-bio.QM) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Post-Disaster Affected Area Segmentation with a Vision Transformer (ViT)-based EVAP Model using Sentinel-2 and Formosat-5 Imagery](https://arxiv.org/abs/2507.16849)
*Yi-Shan Chu,Hsuan-Cheng Wei*

Main category: cs.CV

TL;DR: 提出基于ViT的深度学习框架，用于改进遥感影像中的灾害区域分割，支持TASA的EVAP产品。


<details>
  <summary>Details</summary>
Motivation: 解决灾害区域分割中缺乏准确地面真实数据的问题，提升分割结果的平滑性和可靠性。

Method: 使用PCA和置信指数扩展手动标注数据，训练ViT编码器-解码器模型，支持多波段输入和多阶段损失策略。

Result: 在2022年鄱阳湖干旱和2023年罗得岛野火案例中，分割结果的平滑性和可靠性得到提升。

Conclusion: 该框架为灾害制图提供了一种可扩展的方法，尤其在缺乏准确地面真实数据时表现优异。

Abstract: We propose a vision transformer (ViT)-based deep learning framework to refine
disaster-affected area segmentation from remote sensing imagery, aiming to
support and enhance the Emergent Value Added Product (EVAP) developed by the
Taiwan Space Agency (TASA). The process starts with a small set of manually
annotated regions. We then apply principal component analysis (PCA)-based
feature space analysis and construct a confidence index (CI) to expand these
labels, producing a weakly supervised training set. These expanded labels are
then used to train ViT-based encoder-decoder models with multi-band inputs from
Sentinel-2 and Formosat-5 imagery. Our architecture supports multiple decoder
variants and multi-stage loss strategies to improve performance under limited
supervision. During the evaluation, model predictions are compared with
higher-resolution EVAP output to assess spatial coherence and segmentation
consistency. Case studies on the 2022 Poyang Lake drought and the 2023 Rhodes
wildfire demonstrate that our framework improves the smoothness and reliability
of segmentation results, offering a scalable approach for disaster mapping when
accurate ground truth is unavailable.

</details>


### [2] [Toward a Real-Time Framework for Accurate Monocular 3D Human Pose Estimation with Geometric Priors](https://arxiv.org/abs/2507.16850)
*Mohamed Adjel*

Main category: cs.CV

TL;DR: 提出了一种结合实时2D关键点检测和几何感知的2D到3D提升框架，利用相机内参和解剖学先验知识，实现快速、个性化的单目3D人体姿态估计。


<details>
  <summary>Details</summary>
Motivation: 单目3D人体姿态估计在实时和无约束环境中仍是一个具有挑战性的问题，直接图像到3D的方法需要大量标注数据和复杂模型，而2D到3D提升方法更轻量且灵活。

Method: 结合实时2D关键点检测与几何感知的2D到3D提升，利用相机内参和特定对象的解剖学先验知识，通过自校准和生物力学约束的逆运动学生成大规模训练数据。

Result: 能够实现快速、个性化且准确的单目3D姿态估计，无需专用硬件。

Conclusion: 该框架通过结合数据驱动学习和基于模型的先验知识，提高了3D人体运动捕捉的准确性、可解释性和在边缘设备上的部署能力。

Abstract: Monocular 3D human pose estimation remains a challenging and ill-posed
problem, particularly in real-time settings and unconstrained environments.
While direct imageto-3D approaches require large annotated datasets and heavy
models, 2D-to-3D lifting offers a more lightweight and flexible
alternative-especially when enhanced with prior knowledge. In this work, we
propose a framework that combines real-time 2D keypoint detection with
geometry-aware 2D-to-3D lifting, explicitly leveraging known camera intrinsics
and subject-specific anatomical priors. Our approach builds on recent advances
in self-calibration and biomechanically-constrained inverse kinematics to
generate large-scale, plausible 2D-3D training pairs from MoCap and synthetic
datasets. We discuss how these ingredients can enable fast, personalized, and
accurate 3D pose estimation from monocular images without requiring specialized
hardware. This proposal aims to foster discussion on bridging data-driven
learning and model-based priors to improve accuracy, interpretability, and
deployability of 3D human motion capture on edge devices in the wild.

</details>


### [3] [Coarse-to-fine crack cue for robust crack detection](https://arxiv.org/abs/2507.16851)
*Zelong Liu,Yuliang Gu,Zhichao Sun,Huachao Zhu,Xin Xiao,Bo Du,Laurent Najman,Yongchao Xu*

Main category: cs.CV

TL;DR: CrackCue是一种基于粗到细裂纹线索生成的新方法，通过利用裂纹的细结构特性提高检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在跨域泛化能力上表现不佳，且忽视了裂纹的细结构特性。

Method: 通过最大池化和上采样操作生成粗裂纹背景，再通过重建网络获得细裂纹背景，利用差异生成细裂纹线索。

Result: 实验表明，CrackCue显著提升了基线方法的泛化能力和鲁棒性。

Conclusion: CrackCue是一种有效的即插即用方法，能够提升裂纹检测的性能。

Abstract: Crack detection is an important task in computer vision. Despite impressive
in-dataset performance, deep learning-based methods still struggle in
generalizing to unseen domains. The thin structure property of cracks is
usually overlooked by previous methods. In this work, we introduce CrackCue, a
novel method for robust crack detection based on coarse-to-fine crack cue
generation. The core concept lies on leveraging the thin structure property to
generate a robust crack cue, guiding the crack detection. Specifically, we
first employ a simple max-pooling and upsampling operation on the crack image.
This results in a coarse crack-free background, based on which a fine
crack-free background can be obtained via a reconstruction network. The
difference between the original image and fine crack-free background provides a
fine crack cue. This fine cue embeds robust crack prior information which is
unaffected by complex backgrounds, shadow, and varied lighting. As a
plug-and-play method, we incorporate the proposed CrackCue into three advanced
crack detection networks. Extensive experimental results demonstrate that the
proposed CrackCue significantly improves the generalization ability and
robustness of the baseline methods. The source code will be publicly available.

</details>


### [4] [CLAMP: Contrastive Learning with Adaptive Multi-loss and Progressive Fusion for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.16854)
*Xiaoqiang He*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLAMP的端到端对比学习框架，用于解决多模态基于方面的情感分析中的跨模态对齐噪声和细粒度表示不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态对齐噪声和细粒度表示一致性方面存在不足，尤其是全局模态对齐方法忽略了方面词与局部视觉区域的联系。

Method: CLAMP框架包含三个模块：渐进注意力融合网络、多任务对比学习和自适应多损失聚合，分别用于增强细粒度对齐、跨模态表示一致性和动态损失校准。

Result: 在标准公开基准测试中，CLAMP表现优于大多数现有最先进方法。

Conclusion: CLAMP通过渐进注意力融合和多任务对比学习，有效提升了多模态基于方面的情感分析的性能。

Abstract: Multimodal aspect-based sentiment analysis(MABSA) seeks to identify aspect
terms within paired image-text data and determine their fine grained sentiment
polarities, representing a fundamental task for improving the effectiveness of
applications such as product review systems and public opinion monitoring.
Existing methods face challenges such as cross modal alignment noise and
insufficient consistency in fine-grained representations. While global modality
alignment methods often overlook the connection between aspect terms and their
corresponding local visual regions, bridging the representation gap between
text and images remains a challenge. To address these limitations, this paper
introduces an end to end Contrastive Learning framework with Adaptive
Multi-loss and Progressive Attention Fusion(CLAMP). The framework is composed
of three novel modules: Progressive Attention Fusion network, Multi-task
Contrastive Learning, and Adaptive Multi-loss Aggregation. The Progressive
Attention Fusion network enhances fine-grained alignment between textual
features and image regions via hierarchical, multi-stage cross modal
interactions, effectively suppressing irrelevant visual noise. Secondly,
multi-task contrastive learning combines global modal contrast and local
granularity alignment to enhance cross modal representation consistency.
Adaptive Multi-loss Aggregation employs a dynamic uncertainty based weighting
mechanism to calibrate loss contributions according to each task's uncertainty,
thereby mitigating gradient interference. Evaluation on standard public
benchmarks demonstrates that CLAMP consistently outperforms the vast majority
of existing state of the art methods.

</details>


### [5] [SIA: Enhancing Safety via Intent Awareness for Vision-Language Models](https://arxiv.org/abs/2507.16856)
*Youngjin Na,Sangheon Jeong,Youngwan Lee*

Main category: cs.CV

TL;DR: SIA是一个无需训练的提示工程框架，通过三阶段推理过程主动检测和缓解多模态输入中的有害意图，显著提升模型安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以检测图像与文本组合中潜在的有害意图，SIA旨在动态适应输入中的隐含意图。

Method: SIA通过视觉抽象、意图推理和响应优化三阶段处理多模态输入。

Result: 在多个安全基准测试中，SIA显著优于现有方法，但通用推理能力略有下降。

Conclusion: SIA展示了意图感知推理在提升模型安全性方面的重要价值。

Abstract: As vision-language models (VLMs) are increasingly deployed in real-world
applications, new safety risks arise from the subtle interplay between images
and text. In particular, seemingly innocuous inputs can combine to reveal
harmful intent, leading to unsafe model responses. Despite increasing attention
to multimodal safety, previous approaches based on post hoc filtering or static
refusal prompts struggle to detect such latent risks, especially when
harmfulness emerges only from the combination of inputs. We propose SIA (Safety
via Intent Awareness), a training-free prompt engineering framework that
proactively detects and mitigates harmful intent in multimodal inputs. SIA
employs a three-stage reasoning process: (1) visual abstraction via captioning,
(2) intent inference through few-shot chain-of-thought prompting, and (3)
intent-conditioned response refinement. Rather than relying on predefined rules
or classifiers, SIA dynamically adapts to the implicit intent inferred from the
image-text pair. Through extensive experiments on safety-critical benchmarks
including SIUO, MM-SafetyBench, and HoliSafe, we demonstrate that SIA achieves
substantial safety improvements, outperforming prior methods. Although SIA
shows a minor reduction in general reasoning accuracy on MMStar, the
corresponding safety gains highlight the value of intent-aware reasoning in
aligning VLMs with human-centric values.

</details>


### [6] [Look Before You Fuse: 2D-Guided Cross-Modal Alignment for Robust 3D Detection](https://arxiv.org/abs/2507.16861)
*Xiang Li*

Main category: cs.CV

TL;DR: 论文提出了一种利用2D先验信息校正LiDAR和相机特征对齐的方法，通过PGDC、DAGF和SGDM模块提升BEV表示的3D感知能力，在nuScenes数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法中相机和LiDAR特征的对齐问题导致深度监督不准确和跨模态特征融合错误，主要源于投影误差。论文利用2D目标先验信息预对齐特征以解决这一问题。

Method: 提出PGDC模块校正局部对齐，DAGF模块处理全局对齐并增强边界过渡，SGDM模块通过门控注意力机制融合对齐后的特征。

Result: 在nuScenes验证集上mAP和NDS分别达到71.5%和73.6%。

Conclusion: 通过2D先验引导的特征对齐和融合方法显著提升了BEV表示的3D感知性能。

Abstract: Integrating LiDAR and camera inputs into a unified Bird's-Eye-View (BEV)
representation is crucial for enhancing 3D perception capabilities of
autonomous vehicles. However, current methods are often affected by
misalignment between camera and LiDAR features. This misalignment leads to
inaccurate depth supervision in camera branch and erroneous fusion during
cross-modal feature aggregation. The root cause of this misalignment lies in
projection errors, stemming from minor extrinsic calibration inaccuracies and
rolling shutter effect of LiDAR during vehicle motion. In this work, our key
insight is that these projection errors are predominantly concentrated at
object-background boundaries, which are readily identified by 2D detectors.
Based on this, our main motivation is to utilize 2D object priors to pre-align
cross-modal features before fusion. To address local misalignment, we propose
Prior Guided Depth Calibration (PGDC), which leverages 2D priors to correct
local misalignment and preserve correct cross-modal feature pairs. To resolve
global misalignment, we introduce Discontinuity Aware Geometric Fusion (DAGF)
to process calibrated results from PGDC, suppressing noise and explicitly
enhancing sharp transitions at object-background boundaries. To effectively
utilize these transition-aware depth representations, we incorporate Structural
Guidance Depth Modulator (SGDM), using a gated attention mechanism to
efficiently fuse aligned depth and image features. Our proposed method achieves
state-of-the-art performance on nuScenes validation dataset, with its mAP and
NDS reaching 71.5% and 73.6% respectively.

</details>


### [7] [Pixels, Patterns, but No Poetry: To See The World like Humans](https://arxiv.org/abs/2507.16863)
*Hongcheng Gao,Zihao Huang,Lin Xu,Jingyi Tang,Xinhao Li,Yue Liu,Haoyang Li,Taihang Hu,Minhua Lin,Xinlong Yang,Ge Wu,Balong Bi,Hongyu Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: 论文提出了Turing Eye Test (TET)，一个专注于感知而非推理的基准测试，揭示当前多模态大语言模型在感知任务上的严重缺陷。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大语言模型是否能像人类一样感知世界，而非仅关注推理能力。

Method: 引入TET基准测试，包含四项诊断任务，评估模型在合成图像上的感知能力。

Result: 当前最先进的模型在人类直觉任务上表现极差，视觉塔的微调能显著提升性能。

Conclusion: TET揭示了当前模型在视觉泛化能力上的关键缺陷，为未来研究提供了方向。

Abstract: Achieving human-like perception and reasoning in Multimodal Large Language
Models (MLLMs) remains a central challenge in artificial intelligence. While
recent research has primarily focused on enhancing reasoning capabilities in
MLLMs, a fundamental question persists: Can Multimodal Large Language Models
truly perceive the world as humans do? This paper shifts focus from reasoning
to perception. Rather than constructing benchmarks specifically for reasoning,
we introduce the Turing Eye Test (TET), a challenging perception-oriented
benchmark comprising four diagnostic tasks that evaluate MLLMs' performance on
synthetic images that humans process intuitively. Our findings reveal that
state-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks
trivial for humans. Both in-context learning and training on language
backbone-effective for previous benchmarks-fail to improve performance on our
tasks, while fine-tuning the vision tower enables rapid adaptation, suggesting
that our benchmark poses challenges for vision tower generalization rather than
for the knowledge and reasoning capabilities of the language backbone-a key gap
between current MLLMs and human perception. We release a representative subset
of TET tasks in this version, and will introduce more diverse tasks and methods
to enhance visual generalization in future work.

</details>


### [8] [HIPPO-Video: Simulating Watch Histories with Large Language Models for Personalized Video Highlighting](https://arxiv.org/abs/2507.16873)
*Jeongeun Lee,Youngjae Yu,Dongha Lee*

Main category: cs.CV

TL;DR: HIPPO-Video是一个用于个性化视频高亮的新数据集，通过LLM模拟用户行为生成多样化的观看历史，并提出了HiPHer方法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有视频数据集缺乏个性化，无法捕捉用户行为的复杂性，因此需要创建更贴近用户偏好的数据集和方法。

Method: 使用LLM模拟用户行为生成观看历史，提出HiPHer方法，利用个性化观看历史预测视频片段的显著性分数。

Result: HIPPO-Video包含2040对数据，覆盖20400个视频和170个语义类别；HiPHer方法在实验中优于现有方法。

Conclusion: HIPPO-Video和HiPHer方法为个性化视频高亮提供了有效工具，展示了在实际场景中的应用潜力。

Abstract: The exponential growth of video content has made personalized video
highlighting an essential task, as user preferences are highly variable and
complex. Existing video datasets, however, often lack personalization, relying
on isolated videos or simple text queries that fail to capture the intricacies
of user behavior. In this work, we introduce HIPPO-Video, a novel dataset for
personalized video highlighting, created using an LLM-based user simulator to
generate realistic watch histories reflecting diverse user preferences. The
dataset includes 2,040 (watch history, saliency score) pairs, covering 20,400
videos across 170 semantic categories. To validate our dataset, we propose
HiPHer, a method that leverages these personalized watch histories to predict
preference-conditioned segment-wise saliency scores. Through extensive
experiments, we demonstrate that our method outperforms existing generic and
query-based approaches, showcasing its potential for highly user-centric video
highlighting in real-world scenarios.

</details>


### [9] [ReMeREC: Relation-aware and Multi-entity Referring Expression Comprehension](https://arxiv.org/abs/2507.16877)
*Yizhi Hu,Zezhao Tian,Xingqun Qi,Chen Su,Bingkun Yang,Junhui Yin,Muyi Sun,Man Zhang,Zhenan Sun*

Main category: cs.CV

TL;DR: ReMeREC是一个新框架，用于多实体定位和关系预测，通过构建ReMeX数据集和引入TMP与EIR模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多实体场景中忽略复杂关系，且缺乏高质量数据集，限制了准确性和可靠性。

Method: 构建ReMeX数据集，提出ReMeREC框架，包含TMP（动态推断实体数量和范围）和EIR（增强关系推理）。

Result: 在四个基准数据集上，ReMeREC在多实体定位和关系预测中表现最优，大幅超越现有方法。

Conclusion: ReMeREC通过结合视觉和文本线索，解决了多实体场景中的语义模糊和关系建模问题，显著提升了性能。

Abstract: Referring Expression Comprehension (REC) aims to localize specified entities
or regions in an image based on natural language descriptions. While existing
methods handle single-entity localization, they often ignore complex
inter-entity relationships in multi-entity scenes, limiting their accuracy and
reliability. Additionally, the lack of high-quality datasets with fine-grained,
paired image-text-relation annotations hinders further progress. To address
this challenge, we first construct a relation-aware, multi-entity REC dataset
called ReMeX, which includes detailed relationship and textual annotations. We
then propose ReMeREC, a novel framework that jointly leverages visual and
textual cues to localize multiple entities while modeling their
inter-relations. To address the semantic ambiguity caused by implicit entity
boundaries in language, we introduce the Text-adaptive Multi-entity Perceptron
(TMP), which dynamically infers both the quantity and span of entities from
fine-grained textual cues, producing distinctive representations. Additionally,
our Entity Inter-relationship Reasoner (EIR) enhances relational reasoning and
global scene understanding. To further improve language comprehension for
fine-grained prompts, we also construct a small-scale auxiliary dataset,
EntityText, generated using large language models. Experiments on four
benchmark datasets show that ReMeREC achieves state-of-the-art performance in
multi-entity grounding and relation prediction, outperforming existing
approaches by a large margin.

</details>


### [10] [CausalStep: A Benchmark for Explicit Stepwise Causal Reasoning in Videos](https://arxiv.org/abs/2507.16878)
*Xuchen Li,Xuzhao Li,Shiyu Hu,Kaiqi Huang,Wentao Zhang*

Main category: cs.CV

TL;DR: CausalStep是一个专注于视频中逐步因果推理的基准测试，通过严格的逐步QA协议和诊断指标，揭示了当前模型与人类推理能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有视频基准测试主要评估浅层理解和推理，无法严格评估真正的因果和逐步推理能力，因此需要更严谨的基准来推动视频推理的发展。

Method: CausalStep将视频分割为因果关联单元，采用严格的逐步QA协议，并设计包含干扰项的问题，确保诊断价值。

Result: 实验表明，当前领先的专有和开源模型在逐步推理能力上与人类基线存在显著差距。

Conclusion: CausalStep为视频推理领域提供了一个严谨的基准，有助于推动鲁棒性和可解释性推理的进步。

Abstract: Recent advances in large language models (LLMs) have improved reasoning in
text and image domains, yet achieving robust video reasoning remains a
significant challenge. Existing video benchmarks mainly assess shallow
understanding and reasoning and allow models to exploit global context, failing
to rigorously evaluate true causal and stepwise reasoning. We present
CausalStep, a benchmark designed for explicit stepwise causal reasoning in
videos. CausalStep segments videos into causally linked units and enforces a
strict stepwise question-answer (QA) protocol, requiring sequential answers and
preventing shortcut solutions. Each question includes carefully constructed
distractors based on error type taxonomy to ensure diagnostic value. The
benchmark features 100 videos across six categories and 1,852 multiple-choice
QA pairs. We introduce seven diagnostic metrics for comprehensive evaluation,
enabling precise diagnosis of causal reasoning capabilities. Experiments with
leading proprietary and open-source models, as well as human baselines, reveal
a significant gap between current models and human-level stepwise reasoning.
CausalStep provides a rigorous benchmark to drive progress in robust and
interpretable video reasoning.

</details>


### [11] [Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less Local Than Assumed](https://arxiv.org/abs/2507.16880)
*Antoni Kowalczuk,Dominik Hintersdorf,Lukas Struppek,Kristian Kersting,Adam Dziedzic,Franziska Boenisch*

Main category: cs.CV

TL;DR: 文本到图像扩散模型（DMs）在图像生成方面取得了显著成功，但存在数据隐私和知识产权问题。研究发现现有的修剪防御方法脆弱，且记忆并非局部化。提出了一种新的对抗性微调方法以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估现有修剪防御方法的有效性，并探索更可靠的解决方案以解决数据复制问题。

Method: 通过调整输入提示的文本嵌入重新触发数据复制，并引入对抗性微调方法迭代搜索复制触发器。

Result: 修剪防御方法脆弱，记忆并非局部化；对抗性微调方法提高了模型的鲁棒性。

Conclusion: 现有防御策略不足，需开发真正移除记忆内容的方法；对抗性微调为更可靠的生成AI奠定了基础。

Abstract: Text-to-image diffusion models (DMs) have achieved remarkable success in
image generation. However, concerns about data privacy and intellectual
property remain due to their potential to inadvertently memorize and replicate
training data. Recent mitigation efforts have focused on identifying and
pruning weights responsible for triggering replication, based on the assumption
that memorization can be localized. Our research assesses the robustness of
these pruning-based approaches. We demonstrate that even after pruning, minor
adjustments to text embeddings of input prompts are sufficient to re-trigger
data replication, highlighting the fragility of these defenses. Furthermore, we
challenge the fundamental assumption of memorization locality, by showing that
replication can be triggered from diverse locations within the text embedding
space, and follows different paths in the model. Our findings indicate that
existing mitigation strategies are insufficient and underscore the need for
methods that truly remove memorized content, rather than attempting to suppress
its retrieval. As a first step in this direction, we introduce a novel
adversarial fine-tuning method that iteratively searches for replication
triggers and updates the model to increase robustness. Through our research, we
provide fresh insights into the nature of memorization in text-to-image DMs and
a foundation for building more trustworthy and compliant generative AI.

</details>


### [12] [Sparser2Sparse: Single-shot Sparser-to-Sparse Learning for Spatial Transcriptomics Imputation with Natural Image Co-learning](https://arxiv.org/abs/2507.16886)
*Yaoyu Fang,Jiahe Qian,Xinkun Wang,Lee A. Cooper,Bo Zhou*

Main category: cs.CV

TL;DR: S2S-ST是一种新型空间转录组学（ST）插补框架，仅需稀疏采样数据和自然图像进行协同训练，显著降低了高分辨率ST数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 高分辨率ST数据成本高且稀缺，限制了其广泛应用。

Method: 结合稀疏到稀疏的自监督学习、跨域协同学习和CDCIN网络，迭代优化预测。

Result: 在多种组织类型中表现优于现有方法，提高了插补准确性。

Conclusion: S2S-ST框架降低了高分辨率数据需求，有望推动生物医学研究和临床应用。

Abstract: Spatial transcriptomics (ST) has revolutionized biomedical research by
enabling high resolution gene expression profiling within tissues. However, the
high cost and scarcity of high resolution ST data remain significant
challenges. We present Single-shot Sparser-to-Sparse (S2S-ST), a novel
framework for accurate ST imputation that requires only a single and low-cost
sparsely sampled ST dataset alongside widely available natural images for
co-training. Our approach integrates three key innovations: (1) a
sparser-to-sparse self-supervised learning strategy that leverages intrinsic
spatial patterns in ST data, (2) cross-domain co-learning with natural images
to enhance feature representation, and (3) a Cascaded Data Consistent
Imputation Network (CDCIN) that iteratively refines predictions while
preserving sampled gene data fidelity. Extensive experiments on diverse tissue
types, including breast cancer, liver, and lymphoid tissue, demonstrate that
our method outperforms state-of-the-art approaches in imputation accuracy. By
enabling robust ST reconstruction from sparse inputs, our framework
significantly reduces reliance on costly high resolution data, facilitating
potential broader adoption in biomedical research and clinical applications.

</details>


### [13] [AURA: A Multi-Modal Medical Agent for Understanding, Reasoning & Annotation](https://arxiv.org/abs/2507.16940)
*Nima Fathi,Amar Kumar,Tal Arbel*

Main category: cs.CV

TL;DR: AURA是首个专为医学影像设计的视觉语言解释性代理，通过动态交互和上下文解释提升AI系统的透明度和临床适应性。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的代理系统在许多领域表现出潜力，但在医学影像中的应用仍处于起步阶段。

Method: AURA基于Qwen-32B架构，集成了分割套件、反事实图像生成模块和评估工具。

Result: AURA能够定位临床相关区域、支持图像级解释，并评估诊断相关性和视觉可解释性。

Conclusion: AURA代表了从静态预测到交互式决策支持的医学影像分析的重大进步。

Abstract: Recent advancements in Large Language Models (LLMs) have catalyzed a paradigm
shift from static prediction systems to agentic AI agents capable of reasoning,
interacting with tools, and adapting to complex tasks. While LLM-based agentic
systems have shown promise across many domains, their application to medical
imaging remains in its infancy. In this work, we introduce AURA, the first
visual linguistic explainability agent designed specifically for comprehensive
analysis, explanation, and evaluation of medical images. By enabling dynamic
interactions, contextual explanations, and hypothesis testing, AURA represents
a significant advancement toward more transparent, adaptable, and clinically
aligned AI systems. We highlight the promise of agentic AI in transforming
medical image analysis from static predictions to interactive decision support.
Leveraging Qwen-32B, an LLM-based architecture, AURA integrates a modular
toolbox comprising: (i) a segmentation suite with phase grounding, pathology
segmentation, and anatomy segmentation to localize clinically meaningful
regions; (ii) a counterfactual image-generation module that supports reasoning
through image-level explanations; and (iii) a set of evaluation tools including
pixel-wise difference-map analysis, classification, and advanced
state-of-the-art components to assess diagnostic relevance and visual
interpretability.

</details>


### [14] [Toward Long-Tailed Online Anomaly Detection through Class-Agnostic Concepts](https://arxiv.org/abs/2507.16946)
*Chiao-An Yang,Kuan-Chuan Peng,Raymond A. Yeh*

Main category: cs.CV

TL;DR: 论文提出了一种长尾在线异常检测（LTOAD）任务，并设计了一个无需类别标签的框架，在离线和在线设置中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有长尾异常检测（LTAD）方法依赖类别标签，无法直接应用于在线场景，因此需要一种类别无关的解决方案。

Method: 提出了一种类别无关的框架，并将其适配到在线学习场景中。

Result: 在离线LTAD设置中表现优于现有方法（如MVTec上提升4.63% image-AUROC），在在线设置中也有0.53%的提升。

Conclusion: LTOAD框架在无需类别标签的情况下，显著提升了长尾在线异常检测的性能，并发布了相关基准数据集。

Abstract: Anomaly detection (AD) identifies the defect regions of a given image. Recent
works have studied AD, focusing on learning AD without abnormal images, with
long-tailed distributed training data, and using a unified model for all
classes. In addition, online AD learning has also been explored. In this work,
we expand in both directions to a realistic setting by considering the novel
task of long-tailed online AD (LTOAD). We first identified that the offline
state-of-the-art LTAD methods cannot be directly applied to the online setting.
Specifically, LTAD is class-aware, requiring class labels that are not
available in the online setting. To address this challenge, we propose a
class-agnostic framework for LTAD and then adapt it to our online learning
setting. Our method outperforms the SOTA baselines in most offline LTAD
settings, including both the industrial manufacturing and the medical domain.
In particular, we observe +4.63% image-AUROC on MVTec even compared to methods
that have access to class labels and the number of classes. In the most
challenging long-tailed online setting, we achieve +0.53% image-AUROC compared
to baselines. Our LTOAD benchmark is released here:
https://doi.org/10.5281/zenodo.16283852 .

</details>


### [15] [Divisive Decisions: Improving Salience-Based Training for Generalization in Binary Classification Tasks](https://arxiv.org/abs/2507.17000)
*Jacob Piland,Chris Sweet,Adam Czajka*

Main category: cs.CV

TL;DR: 论文提出了一种新的显著性引导训练方法，通过同时利用真实类和虚假类的类激活图（CAM）来提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注真实类的CAM，忽略了虚假类的CAM。作者假设在二分类任务中，真实类和虚假类的CAM应在人类标注的重要特征上有所差异。

Method: 提出了三种新的显著性引导训练方法，结合真实类和虚假类的CAM，并开发了一种新的后处理工具用于识别重要特征。

Result: 在多个二分类任务（如合成人脸检测、生物特征攻击检测、胸部X光异常分类）中，新方法显著提升了模型的泛化能力。

Conclusion: 新方法通过利用虚假类CAM，超越了传统仅关注真实类CAM的方法，为显著性引导训练提供了新思路。

Abstract: Existing saliency-guided training approaches improve model generalization by
incorporating a loss term that compares the model's class activation map (CAM)
for a sample's true-class ({\it i.e.}, correct-label class) against a human
reference saliency map. However, prior work has ignored the false-class CAM(s),
that is the model's saliency obtained for incorrect-label class. We hypothesize
that in binary tasks the true and false CAMs should diverge on the important
classification features identified by humans (and reflected in human saliency
maps). We use this hypothesis to motivate three new saliency-guided training
methods incorporating both true- and false-class model's CAM into the training
strategy and a novel post-hoc tool for identifying important features. We
evaluate all introduced methods on several diverse binary close-set and
open-set classification tasks, including synthetic face detection, biometric
presentation attack detection, and classification of anomalies in chest X-ray
scans, and find that the proposed methods improve generalization capabilities
of deep learning models over traditional (true-class CAM only) saliency-guided
training approaches. We offer source codes and model weights\footnote{GitHub
repository link removed to preserve anonymity} to support reproducible
research.

</details>


### [16] [Bringing Balance to Hand Shape Classification: Mitigating Data Imbalance Through Generative Models](https://arxiv.org/abs/2507.17008)
*Gaston Gustavo Rios,Pedro Dal Bianco,Franco Ronchetti,Facundo Quiroga,Oscar Stanchi,Santiago Ponte Ahón,Waldo Hasperué*

Main category: cs.CV

TL;DR: 通过生成合成数据增强手形分类器的训练数据，解决了手语数据集小且不平衡的问题，提高了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有手语手形数据集规模小且不平衡，限制了模型训练效果。

Method: 使用EfficientNet分类器，结合GAN（ReACGAN和SPADE）生成合成数据，并与真实数据结合训练。

Result: 在RWTH数据集上准确率提升5%，并能泛化到其他数据集。

Conclusion: 合成数据生成有效解决了小数据集问题，且无需重新训练生成器即可实现跨数据集泛化。

Abstract: Most sign language handshape datasets are severely limited and unbalanced,
posing significant challenges to effective model training. In this paper, we
explore the effectiveness of augmenting the training data of a handshape
classifier by generating synthetic data. We use an EfficientNet classifier
trained on the RWTH German sign language handshape dataset, which is small and
heavily unbalanced, applying different strategies to combine generated and real
images. We compare two Generative Adversarial Networks (GAN) architectures for
data generation: ReACGAN, which uses label information to condition the data
generation process through an auxiliary classifier, and SPADE, which utilizes
spatially-adaptive normalization to condition the generation on pose
information. ReACGAN allows for the generation of realistic images that align
with specific handshape labels, while SPADE focuses on generating images with
accurate spatial handshape configurations. Our proposed techniques improve the
current state-of-the-art accuracy on the RWTH dataset by 5%, addressing the
limitations of small and unbalanced datasets. Additionally, our method
demonstrates the capability to generalize across different sign language
datasets by leveraging pose-based generation trained on the extensive HaGRID
dataset. We achieve comparable performance to single-source trained classifiers
without the need for retraining the generator.

</details>


### [17] [Transformer Based Building Boundary Reconstruction using Attraction Field Maps](https://arxiv.org/abs/2507.17038)
*Muhammad Kamran,Mohammad Moein Sheikholeslami,Andreas Wichmann,Gunho Sohn*

Main category: cs.CV

TL;DR: 提出了一种基于图卷积网络（GCNs）的深度学习方法，用于从卫星图像中自动提取建筑物轮廓，显著提升了精度和效率。


<details>
  <summary>Details</summary>
Motivation: 卫星图像提供了大量高分辨率数据，但建筑物轮廓重建仍依赖人工处理，亟需自动化解决方案。

Method: 结合几何规则性、多尺度特征和吸引力场图，设计了Decoupled-PolyGCN模型。

Result: 模型在AP和AR指标上分别优于现有方法6%和10%。

Conclusion: 该方法为城市规划等应用提供了高效、精确的自动化工具。

Abstract: In recent years, the number of remote satellites orbiting the Earth has grown
significantly, streaming vast amounts of high-resolution visual data to support
diverse applications across civil, public, and military domains. Among these
applications, the generation and updating of spatial maps of the built
environment have become critical due to the extensive coverage and detailed
imagery provided by satellites. However, reconstructing spatial maps from
satellite imagery is a complex computer vision task, requiring the creation of
high-level object representations, such as primitives, to accurately capture
the built environment. While the past decade has witnessed remarkable
advancements in object detection and representation using visual data,
primitives-based object representation remains a persistent challenge in
computer vision. Consequently, high-quality spatial maps often rely on
labor-intensive and manual processes. This paper introduces a novel deep
learning methodology leveraging Graph Convolutional Networks (GCNs) to address
these challenges in building footprint reconstruction. The proposed approach
enhances performance by incorporating geometric regularity into building
boundaries, integrating multi-scale and multi-resolution features, and
embedding Attraction Field Maps into the network. These innovations provide a
scalable and precise solution for automated building footprint extraction from
a single satellite image, paving the way for impactful applications in urban
planning, disaster management, and large-scale spatial analysis. Our model,
Decoupled-PolyGCN, outperforms existing methods by 6% in AP and 10% in AR,
demonstrating its ability to deliver accurate and regularized building
footprints across diverse and challenging scenarios.

</details>


### [18] [Controllable Hybrid Captioner for Improved Long-form Video Understanding](https://arxiv.org/abs/2507.17047)
*Kuleen Sasse,Efsun Sarioglu Kayi,Arun Reddy*

Main category: cs.CV

TL;DR: 提出了一种基于文本记忆的视频理解系统，结合视频描述器和LLM，通过改进视频分割和静态场景描述，提升视频问答能力。


<details>
  <summary>Details</summary>
Motivation: 长视频数据高维且密集，文本摘要能更紧凑地表示内容，便于LLM处理复杂查询。

Method: 使用LaViLa视频描述器生成短片段文本记忆，结合LLaVA VLM增强静态场景描述，优化视频分割和描述类型控制。

Result: 成功微调LaViLa生成动作和场景描述，提升描述效率，扩展可回答问题范围。

Conclusion: 可控混合描述器通过动态切换描述类型，显著提升视频理解系统的效率和能力。

Abstract: Video data, especially long-form video, is extremely dense and
high-dimensional. Text-based summaries of video content offer a way to
represent query-relevant content in a much more compact manner than raw video.
In addition, textual representations are easily ingested by state-of-the-art
large language models (LLMs), which enable reasoning over video content to
answer complex natural language queries. To solve this issue, we rely on the
progressive construction of a text-based memory by a video captioner operating
on shorter chunks of the video, where spatio-temporal modeling is
computationally feasible. We explore ways to improve the quality of the
activity log comprised solely of short video captions. Because the video
captions tend to be focused on human actions, and questions may pertain to
other information in the scene, we seek to enrich the memory with static scene
descriptions using Vision Language Models (VLMs). Our video understanding
system relies on the LaViLa video captioner in combination with a LLM to answer
questions about videos. We first explored different ways of partitioning the
video into meaningful segments such that the textual descriptions more
accurately reflect the structure of the video content. Furthermore, we
incorporated static scene descriptions into the captioning pipeline using LLaVA
VLM, resulting in a more detailed and complete caption log and expanding the
space of questions that are answerable from the textual memory. Finally, we
have successfully fine-tuned the LaViLa video captioner to produce both action
and scene captions, significantly improving the efficiency of the captioning
pipeline compared to using separate captioning models for the two tasks. Our
model, controllable hybrid captioner, can alternate between different types of
captions according to special input tokens that signals scene changes detected
in the video.

</details>


### [19] [Toward Scalable Video Narration: A Training-free Approach Using Multimodal Large Language Models](https://arxiv.org/abs/2507.17050)
*Tz-Ying Wu,Tahani Trigui,Sharath Nittur Sridhar,Anand Bodas,Subarna Tripathi*

Main category: cs.CV

TL;DR: VideoNarrator是一种无需训练的流程，用于生成带时间戳的密集视频描述，通过结合多模态大语言模型和视觉语言模型，显著减少幻觉并提高时间对齐。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视频理解中常出现时间对齐问题和幻觉，尤其是在陌生场景中。

Method: 采用灵活的流程，利用现成的MLLMs和VLMs作为描述生成器、上下文提供者或描述验证器。

Result: 实验表明，该方法显著提升了视频描述的质量和准确性，减少了幻觉并改善了时间对齐。

Conclusion: VideoNarrator不仅提升了视频理解能力，还支持下游任务如视频摘要和问答，并可能应用于广告和营销。

Abstract: In this paper, we introduce VideoNarrator, a novel training-free pipeline
designed to generate dense video captions that offer a structured snapshot of
video content. These captions offer detailed narrations with precise
timestamps, capturing the nuances present in each segment of the video. Despite
advancements in multimodal large language models (MLLMs) for video
comprehension, these models often struggle with temporally aligned narrations
and tend to hallucinate, particularly in unfamiliar scenarios. VideoNarrator
addresses these challenges by leveraging a flexible pipeline where
off-the-shelf MLLMs and visual-language models (VLMs) can function as caption
generators, context providers, or caption verifiers. Our experimental results
demonstrate that the synergistic interaction of these components significantly
enhances the quality and accuracy of video narrations, effectively reducing
hallucinations and improving temporal alignment. This structured approach not
only enhances video understanding but also facilitates downstream tasks such as
video summarization and video question answering, and can be potentially
extended for advertising and marketing applications.

</details>


### [20] [Few-Shot Learning in Video and 3D Object Detection: A Survey](https://arxiv.org/abs/2507.17079)
*Md Meftahul Ferdaus,Kendall N. Niles,Joe Tom,Mahdi Abdelguerfi,Elias Ioup*

Main category: cs.CV

TL;DR: 该论文综述了视频和3D目标检测中的小样本学习（FSL）技术，探讨了其在减少标注需求方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 减少昂贵的手动数据标注需求，尤其是在视频和3D目标检测中，标注成本更高。

Method: 通过跨帧信息传播（如tube proposals和时序匹配网络）和点云网络与损失函数的结合，解决视频和3D检测中的小样本学习问题。

Result: FSL在视频和3D检测中表现出色，能够高效利用时空结构和数据模态特性。

Conclusion: FSL在减少标注需求和实际应用中具有巨大潜力，尤其是在视频和3D领域。

Abstract: Few-shot learning (FSL) enables object detection models to recognize novel
classes given only a few annotated examples, thereby reducing expensive manual
data labeling. This survey examines recent FSL advances for video and 3D object
detection. For video, FSL is especially valuable since annotating objects
across frames is more laborious than for static images. By propagating
information across frames, techniques like tube proposals and temporal matching
networks can detect new classes from a couple examples, efficiently leveraging
spatiotemporal structure. FSL for 3D detection from LiDAR or depth data faces
challenges like sparsity and lack of texture. Solutions integrate FSL with
specialized point cloud networks and losses tailored for class imbalance.
Few-shot 3D detection enables practical autonomous driving deployment by
minimizing costly 3D annotation needs. Core issues in both domains include
balancing generalization and overfitting, integrating prototype matching, and
handling data modality properties. In summary, FSL shows promise for reducing
annotation requirements and enabling real-world video, 3D, and other
applications by efficiently leveraging information across feature, temporal,
and data modalities. By comprehensively surveying recent advancements, this
paper illuminates FSL's potential to minimize supervision needs and enable
deployment across video, 3D, and other real-world applications.

</details>


### [21] [SDGOCC: Semantic and Depth-Guided Bird's-Eye View Transformation for 3D Multimodal Occupancy Prediction](https://arxiv.org/abs/2507.17083)
*Zaipeng Duan,Chenxu Dang,Xuzhong Hu,Pei An,Junfeng Ding,Jie Zhan,Yunbiao Xu,Jie Ma*

Main category: cs.CV

TL;DR: 提出了一种名为SDG-OCC的多模态3D占用预测网络，结合语义和深度引导的视图变换与融合驱动的主动蒸馏，提升了深度估计和语义信息利用。


<details>
  <summary>Details</summary>
Motivation: 现有方法多为单模态，相机方法缺乏深度信息，LiDAR方法易受遮挡影响，且轻量级方法依赖LSS管道，存在深度估计不准确和几何语义信息利用不足的问题。

Method: SDG-OCC通过联合语义和深度引导的视图变换，结合扩散和双线性离散化构建精确深度分布；通过融合驱动的主动蒸馏从多模态数据中提取语义信息并选择性传递。

Result: 在Occ3D-nuScenes数据集上实现SOTA性能，实时处理；在SurroundOcc-nuScenes数据集上表现可比，证明其有效性和鲁棒性。

Conclusion: SDG-OCC通过多模态融合和主动蒸馏，显著提升了3D占用预测的精度和效率，适用于自动驾驶场景。

Abstract: Multimodal 3D occupancy prediction has garnered significant attention for its
potential in autonomous driving. However, most existing approaches are
single-modality: camera-based methods lack depth information, while LiDAR-based
methods struggle with occlusions. Current lightweight methods primarily rely on
the Lift-Splat-Shoot (LSS) pipeline, which suffers from inaccurate depth
estimation and fails to fully exploit the geometric and semantic information of
3D LiDAR points. Therefore, we propose a novel multimodal occupancy prediction
network called SDG-OCC, which incorporates a joint semantic and depth-guided
view transformation coupled with a fusion-to-occupancy-driven active
distillation. The enhanced view transformation constructs accurate depth
distributions by integrating pixel semantics and co-point depth through
diffusion and bilinear discretization. The fusion-to-occupancy-driven active
distillation extracts rich semantic information from multimodal data and
selectively transfers knowledge to image features based on LiDAR-identified
regions. Finally, for optimal performance, we introduce SDG-Fusion, which uses
fusion alone, and SDG-KL, which integrates both fusion and distillation for
faster inference. Our method achieves state-of-the-art (SOTA) performance with
real-time processing on the Occ3D-nuScenes dataset and shows comparable
performance on the more challenging SurroundOcc-nuScenes dataset, demonstrating
its effectiveness and robustness. The code will be released at
https://github.com/DzpLab/SDGOCC.

</details>


### [22] [FedVLM: Scalable Personalized Vision-Language Models through Federated Learning](https://arxiv.org/abs/2507.17088)
*Arkajyoti Mitra,Afia Anjum,Paul Agbaje,Mert Pesé,Habeeb Olufowobi*

Main category: cs.CV

TL;DR: FedVLM是一个联邦LoRA微调框架，用于在保护模型隐私的同时分散调整视觉语言模型（VLMs），并通过个性化LoRA（pLoRA）解决数据异构性问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效调整方法（如LoRA）在异构客户端数据上表现不佳，导致泛化能力不足。

Method: 提出FedVLM框架和pLoRA方法，动态调整LoRA参数以适应客户端数据分布。

Result: 在RLAIF-V数据集上，pLoRA比标准LoRA提高了24.5%的客户端性能。

Conclusion: FedVLM为联邦环境中的VLM微调提供了可扩展且高效的解决方案。

Abstract: Vision-language models (VLMs) demonstrate impressive zero-shot and few-shot
learning capabilities, making them essential for several downstream tasks.
However, fine-tuning these models at scale remains challenging, particularly in
federated environments where data is decentralized and non-iid across clients.
Existing parameter-efficient tuning methods like LoRA (Low-Rank Adaptation)
reduce computational overhead but struggle with heterogeneous client data,
leading to suboptimal generalization. To address these challenges, we propose
FedVLM, a federated LoRA fine-tuning framework that enables decentralized
adaptation of VLMs while preserving model privacy and reducing reliance on
centralized training. To further tackle data heterogeneity, we introduce
personalized LoRA (pLoRA), which dynamically adapts LoRA parameters to each
client's unique data distribution, significantly improving local adaptation
while maintaining global model aggregation. Experiments on the RLAIF-V dataset
show that pLoRA improves client-specific performance by 24.5% over standard
LoRA, demonstrating superior adaptation in non-iid settings. FedVLM provides a
scalable and efficient solution for fine-tuning VLMs in federated settings,
advancing personalized adaptation in distributed learning scenarios.

</details>


### [23] [IONext: Unlocking the Next Era of Inertial Odometry](https://arxiv.org/abs/2507.17089)
*Shanshan Zhang,Siyue Wang,Tianshui Wen,Qi Zhang,Ziheng Zhou,Lingxiang Zheng,Yu Yang*

Main category: cs.CV

TL;DR: 提出了一种新的CNN模块DADM和时空门控单元STGU，构建了IONext模型，显著提升了惯性里程计的性能。


<details>
  <summary>Details</summary>
Motivation: Transformer在惯性里程计中表现优异，但对局部运动变化敏感且缺乏归纳偏置，影响了定位精度和泛化能力。

Method: 设计了DADM模块以自适应捕获全局和局部运动特征，并引入STGU单元改进时间建模。

Result: 在六个公开数据集上，IONext性能优于现有方法，如RNIN数据集上ATE和RTE分别降低10%和12%。

Conclusion: IONext通过结合CNN和Transformer的优势，显著提升了惯性里程计的精度和泛化能力。

Abstract: Researchers have increasingly adopted Transformer-based models for inertial
odometry. While Transformers excel at modeling long-range dependencies, their
limited sensitivity to local, fine-grained motion variations and lack of
inherent inductive biases often hinder localization accuracy and
generalization. Recent studies have shown that incorporating large-kernel
convolutions and Transformer-inspired architectural designs into CNN can
effectively expand the receptive field, thereby improving global motion
perception. Motivated by these insights, we propose a novel CNN-based module
called the Dual-wing Adaptive Dynamic Mixer (DADM), which adaptively captures
both global motion patterns and local, fine-grained motion features from
dynamic inputs. This module dynamically generates selective weights based on
the input, enabling efficient multi-scale feature aggregation. To further
improve temporal modeling, we introduce the Spatio-Temporal Gating Unit (STGU),
which selectively extracts representative and task-relevant motion features in
the temporal domain. This unit addresses the limitations of temporal modeling
observed in existing CNN approaches. Built upon DADM and STGU, we present a new
CNN-based inertial odometry backbone, named Next Era of Inertial Odometry
(IONext). Extensive experiments on six public datasets demonstrate that IONext
consistently outperforms state-of-the-art (SOTA) Transformer- and CNN-based
methods. For instance, on the RNIN dataset, IONext reduces the average ATE by
10% and the average RTE by 12% compared to the representative model iMOT.

</details>


### [24] [Robust Five-Class and binary Diabetic Retinopathy Classification Using Transfer Learning and Data Augmentation](https://arxiv.org/abs/2507.17121)
*Faisal Ahmed,Mohammad Alfrad Nobel Bhuiyan*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的糖尿病视网膜病变分类框架，通过迁移学习和数据增强解决了类别不平衡和数据不足的问题，在APTOS 2019数据集上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变（DR）是全球视力丧失的主要原因，早期诊断可以显著降低失明风险。自动化视网膜图像分析是解决这一问题的关键。

Method: 采用迁移学习和广泛的数据增强技术，评估了包括ResNet和EfficientNet在内的多种预训练卷积神经网络架构。

Result: 在二分类任务中，模型达到了98.9%的准确率；在五分类任务中，准确率为84.6%，AUC为94.1%，优于现有方法。

Conclusion: 结合类别平衡增强和迁移学习的方法在DR诊断中表现出色，为临床环境提供了可扩展且准确的解决方案。

Abstract: Diabetic retinopathy (DR) is a leading cause of vision loss worldwide, and
early diagnosis through automated retinal image analysis can significantly
reduce the risk of blindness. This paper presents a robust deep learning
framework for both binary and five-class DR classification, leveraging transfer
learning and extensive data augmentation to address the challenges of class
imbalance and limited training data. We evaluate a range of pretrained
convolutional neural network architectures, including variants of ResNet and
EfficientNet, on the APTOS 2019 dataset.
  For binary classification, our proposed model achieves a state-of-the-art
accuracy of 98.9%, with a precision of 98.6%, recall of 99.3%, F1-score of
98.9%, and an AUC of 99.4%. In the more challenging five-class severity
classification task, our model obtains a competitive accuracy of 84.6% and an
AUC of 94.1%, outperforming several existing approaches. Our findings also
demonstrate that EfficientNet-B0 and ResNet34 offer optimal trade-offs between
accuracy and computational efficiency across both tasks.
  These results underscore the effectiveness of combining class-balanced
augmentation with transfer learning for high-performance DR diagnosis. The
proposed framework provides a scalable and accurate solution for DR screening,
with potential for deployment in real-world clinical environments.

</details>


### [25] [ScSAM: Debiasing Morphology and Distributional Variability in Subcellular Semantic Segmentation](https://arxiv.org/abs/2507.17149)
*Bo Fang,Jianan Fan,Dongnan Liu,Hang Chang,Gerald J. Shami,Filip Braet,Weidong Cai*

Main category: cs.CV

TL;DR: ScSAM通过结合SAM和MAE的先验知识，解决了亚细胞分割中的特征偏差问题，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 亚细胞组分的形态和分布多样性导致学习模型易产生特征偏差，现有方法忽视特征多样性，SAM在亚细胞场景应用受限。

Method: 提出ScSAM，融合SAM和MAE的先验知识，设计特征对齐模块和基于余弦相似度的类别提示编码器。

Result: 在多个亚细胞图像数据集上，ScSAM优于现有方法。

Conclusion: ScSAM通过特征融合和类别提示，有效缓解了数据偏差，提升了亚细胞分割的准确性。

Abstract: The significant morphological and distributional variability among
subcellular components poses a long-standing challenge for learning-based
organelle segmentation models, significantly increasing the risk of biased
feature learning. Existing methods often rely on single mapping relationships,
overlooking feature diversity and thereby inducing biased training. Although
the Segment Anything Model (SAM) provides rich feature representations, its
application to subcellular scenarios is hindered by two key challenges: (1) The
variability in subcellular morphology and distribution creates gaps in the
label space, leading the model to learn spurious or biased features. (2) SAM
focuses on global contextual understanding and often ignores fine-grained
spatial details, making it challenging to capture subtle structural alterations
and cope with skewed data distributions. To address these challenges, we
introduce ScSAM, a method that enhances feature robustness by fusing
pre-trained SAM with Masked Autoencoder (MAE)-guided cellular prior knowledge
to alleviate training bias from data imbalance. Specifically, we design a
feature alignment and fusion module to align pre-trained embeddings to the same
feature space and efficiently combine different representations. Moreover, we
present a cosine similarity matrix-based class prompt encoder to activate
class-specific features to recognize subcellular categories. Extensive
experiments on diverse subcellular image datasets demonstrate that ScSAM
outperforms state-of-the-art methods.

</details>


### [26] [UNICE: Training A Universal Image Contrast Enhancer](https://arxiv.org/abs/2507.17157)
*Ruodai Cui,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种通用的图像对比度增强方法UNICE，通过生成多曝光序列并融合，无需人工标注，且在不同任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法针对特定任务设计，泛化能力差，探索能否学习一个通用模型。

Method: 收集HDR图像生成多曝光序列，训练网络生成序列并融合为增强图像。

Result: UNICE在多个任务中表现优于现有方法，甚至超过人工标注的基准。

Conclusion: UNICE是一种无需人工标注且泛化能力强的通用图像对比度增强方法。

Abstract: Existing image contrast enhancement methods are typically designed for
specific tasks such as under-/over-exposure correction, low-light and backlit
image enhancement, etc. The learned models, however, exhibit poor
generalization performance across different tasks, even across different
datasets of a specific task. It is important to explore whether we can learn a
universal and generalized model for various contrast enhancement tasks. In this
work, we observe that the common key factor of these tasks lies in the need of
exposure and contrast adjustment, which can be well-addressed if high-dynamic
range (HDR) inputs are available. We hence collect 46,928 HDR raw images from
public sources, and render 328,496 sRGB images to build multi-exposure
sequences (MES) and the corresponding pseudo sRGB ground-truths via
multi-exposure fusion. Consequently, we train a network to generate an MES from
a single sRGB image, followed by training another network to fuse the generated
MES into an enhanced image. Our proposed method, namely UNiversal Image
Contrast Enhancer (UNICE), is free of costly human labeling. However, it
demonstrates significantly stronger generalization performance than existing
image contrast enhancement methods across and within different tasks, even
outperforming manually created ground-truths in multiple no-reference image
quality metrics. The dataset, code and model are available at
https://github.com/BeyondHeaven/UNICE.

</details>


### [27] [DOOMGAN:High-Fidelity Dynamic Identity Obfuscation Ocular Generative Morphing](https://arxiv.org/abs/2507.17158)
*Bharath Krishnamurthy,Ajita Rattani*

Main category: cs.CV

TL;DR: DOOMGAN是一种用于可见光谱眼生物特征的高级生成模型，专注于模拟变形攻击，显著提高了攻击成功率和特征生成质量。


<details>
  <summary>Details</summary>
Motivation: 可见光谱眼生物特征的变形攻击研究不足，现有方法在非受控条件下难以生成高质量合成特征。

Method: DOOMGAN结合了地标驱动编码、注意力引导生成和多方面损失动态加权，优化了变形合成。

Result: DOOMGAN在攻击成功率、虹膜结构生成和凝视一致性上分别提高了20%、20%和30%。

Conclusion: DOOMGAN填补了可见光谱眼生物特征变形攻击研究的空白，并发布了首个相关数据集。

Abstract: Ocular biometrics in the visible spectrum have emerged as a prominent
modality due to their high accuracy, resistance to spoofing, and non-invasive
nature. However, morphing attacks, synthetic biometric traits created by
blending features from multiple individuals, threaten biometric system
integrity. While extensively studied for near-infrared iris and face
biometrics, morphing in visible-spectrum ocular data remains underexplored.
Simulating such attacks demands advanced generation models that handle
uncontrolled conditions while preserving detailed ocular features like iris
boundaries and periocular textures. To address this gap, we introduce DOOMGAN,
that encompasses landmark-driven encoding of visible ocular anatomy,
attention-guided generation for realistic morph synthesis, and dynamic
weighting of multi-faceted losses for optimized convergence. DOOMGAN achieves
over 20% higher attack success rates than baseline methods under stringent
thresholds, along with 20% better elliptical iris structure generation and 30%
improved gaze consistency. We also release the first comprehensive ocular
morphing dataset to support further research in this domain.

</details>


### [28] [Multi-Scale PCB Defect Detection with YOLOv8 Network Improved via Pruning and Lightweight Network](https://arxiv.org/abs/2507.17176)
*Li Pingzhen,Xu Sheng,Chen Jing,Su Chengyue*

Main category: cs.CV

TL;DR: 改进的YOLOv8多尺度PCB缺陷检测方法，通过优化网络结构和自适应剪枝，显著提升检测速度和精度。


<details>
  <summary>Details</summary>
Motivation: 传统PCB缺陷检测模型难以兼顾高精度和低计算成本，无法满足微小缺陷的高精度实时检测需求。

Method: 采用Ghost-HGNetv2结构、C2f-Faster模块、GCDetect检测头和Inner-MPDIoU损失函数，并结合自适应剪枝优化模型。

Result: 在公开PCB缺陷数据集上，mAP0.5达99.32%，mAP0.5:0.9达75.18%，比YOLOv8n提升10.13%。

Conclusion: 该方法在精度和速度上均表现出显著优势，适用于高密度PCB的微小缺陷检测。

Abstract: With the high density of printed circuit board (PCB) design and the high
speed of production, the traditional PCB defect detection model is difficult to
take into account the accuracy and computational cost, and cannot meet the
requirements of high accuracy and real-time detection of tiny defects.
Therefore, in this paper, a multi-scale PCB defect detection method is improved
with YOLOv8 using a comprehensive strategy of tiny target sensitivity strategy,
network lightweighting and adaptive pruning, which is able to improve the
detection speed and accuracy by optimizing the backbone network, the neck
network and the detection head, the loss function and the adaptive pruning
rate. Firstly, a Ghost-HGNetv2 structure with fewer parameters is used in the
backbone network, and multilevel features are used to extract image semantic
features to discover accurate defects. Secondly, we integrate C2f-Faster with
small number of parameters in the neck section to enhance the ability of
multi-level feature fusion. Next, in the Head part, we design a new GCDetect
detection head, which allows the prediction of bounding boxes and categories to
share the weights of GroupConv, and uses a small number of grouping
convolutions to accomplish the regression and classification tasks, which
significantly reduces the number of parameters while maintaining the accuracy
of detection. We also design the Inner-MPDIoU boundary loss function to improve
the detection and localization of tiny targets. Finally, the model was pruned
by an optimized adaptive pruning rate to further reduce the complexity of the
model. Experimental results show that the model exhibits advantages in terms of
accuracy and speed. On the publicly available PCB defect dataset, mAP0.5
reaches 99.32% and mAP0.5:0.9 reaches 75.18%, which is 10.13% higher compared
to YOLOv8n.

</details>


### [29] [Hierarchical Fusion and Joint Aggregation: A Multi-Level Feature Representation Method for AIGC Image Quality Assessment](https://arxiv.org/abs/2507.17182)
*Linghe Meng,Jiarun Song*

Main category: cs.CV

TL;DR: 提出了一种多级视觉表示范式，用于AI生成内容的质量评估，包括特征提取、层次融合和联合聚合三个阶段，并开发了两种网络（MGLF-Net和MPEF-Net），在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单级视觉特征，难以捕捉AI生成内容中的复杂失真，因此需要一种多级视觉表示方法。

Method: 提出多级视觉表示范式，包括多级特征提取、层次融合和联合聚合；开发了MGLF-Net（用于感知质量评估）和MPEF-Net（用于文本-图像对应）。

Result: 在基准测试中表现出色，验证了多级视觉评估范式的有效性。

Conclusion: 多级视觉表示范式能有效提升AI生成内容的质量评估性能。

Abstract: The quality assessment of AI-generated content (AIGC) faces multi-dimensional
challenges, that span from low-level visual perception to high-level semantic
understanding. Existing methods generally rely on single-level visual features,
limiting their ability to capture complex distortions in AIGC images. To
address this limitation, a multi-level visual representation paradigm is
proposed with three stages, namely multi-level feature extraction, hierarchical
fusion, and joint aggregation. Based on this paradigm, two networks are
developed. Specifically, the Multi-Level Global-Local Fusion Network (MGLF-Net)
is designed for the perceptual quality assessment, extracting complementary
local and global features via dual CNN and Transformer visual backbones. The
Multi-Level Prompt-Embedded Fusion Network (MPEF-Net) targets Text-to-Image
correspondence by embedding prompt semantics into the visual feature fusion
process at each feature level. The fused multi-level features are then
aggregated for final evaluation. Experiments on benchmarks demonstrate
outstanding performance on both tasks, validating the effectiveness of the
proposed multi-level visual assessment paradigm.

</details>


### [30] [Asymmetric Lesion Detection with Geometric Patterns and CNN-SVM Classification](https://arxiv.org/abs/2507.17185)
*M. A. Rasel,Sameem Abdul Kareem,Zhenli Kwan,Nik Aimee Azizah Faheem,Winn Hui Han,Rebecca Kai Jan Choong,Shin Shen Yong,Unaizah Obaidellah*

Main category: cs.CV

TL;DR: 论文提出了一种结合几何分析和CNN的方法，用于分析皮肤病变形状的不对称性，帮助非专家诊断黑色素瘤，并在实验中取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变形状的不对称性是诊断黑色素瘤的重要标准，但非专家难以准确判断。论文旨在通过技术手段辅助非专家理解病变形状的不对称性。

Method: 首先基于临床评估标注对称性信息，随后提出监督学习算法分析几何模式，并利用预训练CNN提取特征训练多类SVM分类器。

Result: 几何实验中对不对称病变的检测率达到99%，CNN实验中分类性能最佳（Kappa Score 94%，Macro F1-score 95%，Weighted F1-score 97%）。

Conclusion: 该方法在分析皮肤病变形状不对称性方面表现优异，为黑色素瘤诊断提供了有效支持。

Abstract: In dermoscopic images, which allow visualization of surface skin structures
not visible to the naked eye, lesion shape offers vital insights into skin
diseases. In clinically practiced methods, asymmetric lesion shape is one of
the criteria for diagnosing melanoma. Initially, we labeled data for a
non-annotated dataset with symmetrical information based on clinical
assessments. Subsequently, we propose a supporting technique, a supervised
learning image processing algorithm, to analyze the geometrical pattern of
lesion shape, aiding non-experts in understanding the criteria of an asymmetric
lesion. We then utilize a pre-trained convolutional neural network (CNN) to
extract shape, color, and texture features from dermoscopic images for training
a multiclass support vector machine (SVM) classifier, outperforming
state-of-the-art methods from the literature. In the geometry-based experiment,
we achieved a 99.00% detection rate for dermatological asymmetric lesions. In
the CNN-based experiment, the best performance is found with 94% Kappa Score,
95% Macro F1-score, and 97% Weighted F1-score for classifying lesion shapes
(Asymmetric, Half-Symmetric, and Symmetric).

</details>


### [31] [Vec2Face+ for Face Dataset Generation](https://arxiv.org/abs/2507.17192)
*Haiyu Wu,Jaskirat Singh,Sicong Tian,Liang Zheng,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: Vec2Face+是一种生成模型，通过控制面部特征和属性生成高质量训练数据，首次在合成数据集上超越真实数据集CASIA-WebFace的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在增加类内属性变化时忽视了保持类内身份一致性的必要性，Vec2Face+旨在解决这一问题。

Method: 使用Vec2Face+生成图像，通过三种策略（向量采样、AttrOP算法、LoRA-based姿态控制）确保类间分离性和类内一致性。

Result: 生成的VFace10K数据集在7个真实测试集上达到SOTA准确率，VFace100K和VFace300K在5个测试集上超越CASIA-WebFace。

Conclusion: 合成数据集首次超越真实数据集，但存在双胞胎验证和模型偏差问题，需进一步研究。

Abstract: When synthesizing identities as face recognition training data, it is
generally believed that large inter-class separability and intra-class
attribute variation are essential for synthesizing a quality dataset. % This
belief is generally correct, and this is what we aim for. However, when
increasing intra-class variation, existing methods overlook the necessity of
maintaining intra-class identity consistency. % To address this and generate
high-quality face training data, we propose Vec2Face+, a generative model that
creates images directly from image features and allows for continuous and easy
control of face identities and attributes. Using Vec2Face+, we obtain datasets
with proper inter-class separability and intra-class variation and identity
consistency using three strategies: 1) we sample vectors sufficiently different
from others to generate well-separated identities; 2) we propose an AttrOP
algorithm for increasing general attribute variations; 3) we propose LoRA-based
pose control for generating images with profile head poses, which is more
efficient and identity-preserving than AttrOP. % Our system generates VFace10K,
a synthetic face dataset with 10K identities, which allows an FR model to
achieve state-of-the-art accuracy on seven real-world test sets. Scaling the
size to 4M and 12M images, the corresponding VFace100K and VFace300K datasets
yield higher accuracy than the real-world training dataset, CASIA-WebFace, on
five real-world test sets. This is the first time a synthetic dataset beats the
CASIA-WebFace in average accuracy. In addition, we find that only 1 out of 11
synthetic datasets outperforms random guessing (\emph{i.e., 50\%}) in twin
verification and that models trained with synthetic identities are more biased
than those trained with real identities. Both are important aspects for future
investigation.

</details>


### [32] [DesignLab: Designing Slides Through Iterative Detection and Correction](https://arxiv.org/abs/2507.17202)
*Jooyeol Yun,Heng Wang,Yotaro Shimose,Jaegul Choo,Shingo Takamatsu*

Main category: cs.CV

TL;DR: DesignLab通过分解设计过程为评审者和贡献者角色，利用迭代循环提升幻灯片设计质量，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 非专业人士在设计高质量幻灯片时面临复杂选择，现有自动化工具缺乏自我优化能力。

Method: 将设计过程分为评审者和贡献者角色，利用大语言模型模拟迭代优化。

Result: DesignLab在实验中表现优于现有设计生成方法，包括商业工具。

Conclusion: 通过迭代设计过程，DesignLab能生成更专业的幻灯片。

Abstract: Designing high-quality presentation slides can be challenging for non-experts
due to the complexity involved in navigating various design choices. Numerous
automated tools can suggest layouts and color schemes, yet often lack the
ability to refine their own output, which is a key aspect in real-world
workflows. We propose DesignLab, which separates the design process into two
roles, the design reviewer, who identifies design-related issues, and the
design contributor who corrects them. This decomposition enables an iterative
loop where the reviewer continuously detects issues and the contributor
corrects them, allowing a draft to be further polished with each iteration,
reaching qualities that were unattainable. We fine-tune large language models
for these roles and simulate intermediate drafts by introducing controlled
perturbations, enabling the design reviewer learn design errors and the
contributor learn how to fix them. Our experiments show that DesignLab
outperforms existing design-generation methods, including a commercial tool, by
embracing the iterative nature of designing which can result in polished,
professional slides.

</details>


### [33] [VBCD: A Voxel-Based Framework for Personalized Dental Crown Design](https://arxiv.org/abs/2507.17205)
*Linda Wei,Chang Liu,Wenran Zhang,Zengji Zhang,Shaoting Zhang,Hongsheng Li*

Main category: cs.CV

TL;DR: 提出了一种基于体素的自动牙冠设计框架（VBCD），通过粗生成和细粒度优化提高设计效率和质量。


<details>
  <summary>Details</summary>
Motivation: 牙科技师从口内扫描设计修复性牙冠的过程劳动密集，需要自动化解决方案。

Method: VBCD框架首先生成粗略牙冠，再通过距离感知监督的细粒度优化器提升质量；训练阶段使用曲率和边缘线惩罚损失（CMPL）增强边缘对齐，并引入基于FDI牙齿编号系统的位置提示。

Result: 在大规模口内扫描数据集上评估，该方法优于现有方法。

Conclusion: VBCD为个性化牙冠设计提供了高效且准确的解决方案。

Abstract: The design of restorative dental crowns from intraoral scans is
labor-intensive for dental technicians. To address this challenge, we propose a
novel voxel-based framework for automated dental crown design (VBCD). The VBCD
framework generates an initial coarse dental crown from voxelized intraoral
scans, followed by a fine-grained refiner incorporating distance-aware
supervision to improve accuracy and quality. During the training stage, we
employ the Curvature and Margin line Penalty Loss (CMPL) to enhance the
alignment of the generated crown with the margin line. Additionally, a
positional prompt based on the FDI tooth numbering system is introduced to
further improve the accuracy of the generated dental crowns. Evaluation on a
large-scale dataset of intraoral scans demonstrated that our approach
outperforms existing methods, providing a robust solution for personalized
dental crown design.

</details>


### [34] [A Low-Cost Machine Learning Approach for Timber Diameter Estimation](https://arxiv.org/abs/2507.17219)
*Fatemeh Hasanzadeh Fard,Sanaz Hasanzadeh Fard,Mehdi Jonoobi*

Main category: cs.CV

TL;DR: 该研究提出了一种基于YOLOv5的轻量级机器学习框架，用于在真实工业环境中通过RGB图像自动检测木材并估算其直径。


<details>
  <summary>Details</summary>
Motivation: 传统木材识别方法依赖人工，效率低且易出错，尤其在处理大量木材时。本研究旨在提供一种实用且经济高效的自动化解决方案。

Method: 使用YOLOv5目标检测算法，并在公开数据集TimberSeg 1.0上进行微调，通过边界框尺寸估算木材厚度。

Result: 模型在mAP@0.5指标上达到0.64，显示即使在有限计算资源下也能可靠检测木材。

Conclusion: 该轻量级解决方案适用于中小型木材加工厂，可集成到现有工作流程中，如库存管理和初步分类。

Abstract: The wood processing industry, particularly in facilities such as sawmills and
MDF production lines, requires accurate and efficient identification of species
and thickness of the wood. Although traditional methods rely heavily on expert
human labor, they are slow, inconsistent, and prone to error, especially when
processing large volumes. This study focuses on practical and cost-effective
machine learning frameworks that automate the estimation of timber log diameter
using standard RGB images captured under real-world working conditions. We
employ the YOLOv5 object detection algorithm, fine-tuned on a public dataset
(TimberSeg 1.0), to detect individual timber logs and estimate thickness
through bounding-box dimensions. Unlike previous methods that require expensive
sensors or controlled environments, this model is trained on images taken in
typical industrial sheds during timber delivery. Experimental results show that
the model achieves a mean Average Precision (mAP@0.5) of 0.64, demonstrating
reliable log detection even with modest computing resources. This lightweight,
scalable solution holds promise for practical integration into existing
workflows, including on-site inventory management and preliminary sorting,
particularly in small and medium-sized operations.

</details>


### [35] [PIG-Nav: Key Insights for Pretrained Image Goal Navigation Models](https://arxiv.org/abs/2507.17220)
*Jiansong Wan,Chengming Zhou,Jinkua Liu,Xiangge Huang,Xiaoyu Chen,Xiaohan Yi,Qisen Yang,Baiting Zhu,Xin-Qiang Cai,Lixing Liu,Rushuai Yang,Chuheng Zhang,Sherif Abdelfattah,Hayong Shin,Pushi Zhang,Li Zhao,Jiang Bian*

Main category: cs.CV

TL;DR: PIG-Nav提出了一种新的预训练策略，通过早期融合网络结构和辅助任务提升视觉导航模型的性能，并在数据预处理上创新，显著提高了零样本和微调性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型在视觉导航中的应用，以提升泛化能力和零样本性能。

Method: 采用早期融合网络结构和辅助任务，结合游戏视频数据预处理。

Result: 在零样本和微调设置下分别提升22.6%和37.5%，且需要更少的微调数据。

Conclusion: PIG-Nav在预训练图像目标导航模型中达到先进水平，适合实际部署。

Abstract: Recent studies have explored pretrained (foundation) models for vision-based
robotic navigation, aiming to achieve generalizable navigation and positive
transfer across diverse environments while enhancing zero-shot performance in
unseen settings. In this work, we introduce PIG-Nav (Pretrained Image-Goal
Navigation), a new approach that further investigates pretraining strategies
for vision-based navigation models and contributes in two key areas.
Model-wise, we identify two critical design choices that consistently improve
the performance of pretrained navigation models: (1) integrating an
early-fusion network structure to combine visual observations and goal images
via appropriately pretrained Vision Transformer (ViT) image encoder, and (2)
introducing suitable auxiliary tasks to enhance global navigation
representation learning, thus further improving navigation performance.
Dataset-wise, we propose a novel data preprocessing pipeline for efficiently
labeling large-scale game video datasets for navigation model training. We
demonstrate that augmenting existing open navigation datasets with diverse
gameplay videos improves model performance. Our model achieves an average
improvement of 22.6% in zero-shot settings and a 37.5% improvement in
fine-tuning settings over existing visual navigation foundation models in two
complex simulated environments and one real-world environment. These results
advance the state-of-the-art in pretrained image-goal navigation models.
Notably, our model maintains competitive performance while requiring
significantly less fine-tuning data, highlighting its potential for real-world
deployment with minimal labeled supervision.

</details>


### [36] [MaskedCLIP: Bridging the Masked and CLIP Space for Semi-Supervised Medical Vision-Language Pre-training](https://arxiv.org/abs/2507.17239)
*Lei Zhu,Jun Zhou,Rick Siow Mong Goh,Yong Liu*

Main category: cs.CV

TL;DR: 提出了一种名为MaskedCLIP的半监督视觉语言预训练框架，结合了掩码图像建模和对比语言图像预训练，以充分利用配对和非配对图像数据学习更通用的图像特征。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型仅依赖配对或非配对图像数据，限制了学习更丰富和全面的图像特征的能力。

Method: 提出MaskedCLIP框架，通过桥接变换器连接掩码特征空间和CLIP特征空间，并设计掩码知识蒸馏损失。

Result: 在视网膜图像分析任务中验证了方法的有效性和数据效率。

Conclusion: MaskedCLIP能够有效结合配对和非配对数据，学习更具泛化能力的图像特征。

Abstract: Foundation models have recently gained tremendous popularity in medical image
analysis. State-of-the-art methods leverage either paired image-text data via
vision-language pre-training or unpaired image data via self-supervised
pre-training to learn foundation models with generalizable image features to
boost downstream task performance. However, learning foundation models
exclusively on either paired or unpaired image data limits their ability to
learn richer and more comprehensive image features. In this paper, we
investigate a novel task termed semi-supervised vision-language pre-training,
aiming to fully harness the potential of both paired and unpaired image data
for foundation model learning. To this end, we propose MaskedCLIP, a
synergistic masked image modeling and contrastive language-image pre-training
framework for semi-supervised vision-language pre-training. The key challenge
in combining paired and unpaired image data for learning a foundation model
lies in the incompatible feature spaces derived from these two types of data.
To address this issue, we propose to connect the masked feature space with the
CLIP feature space with a bridge transformer. In this way, the more semantic
specific CLIP features can benefit from the more general masked features for
semantic feature extraction. We further propose a masked knowledge distillation
loss to distill semantic knowledge of original image features in CLIP feature
space back to the predicted masked image features in masked feature space. With
this mutually interactive design, our framework effectively leverages both
paired and unpaired image data to learn more generalizable image features for
downstream tasks. Extensive experiments on retinal image analysis demonstrate
the effectiveness and data efficiency of our method.

</details>


### [37] [Perceptual Classifiers: Detecting Generative Images using Perceptual Features](https://arxiv.org/abs/2507.17240)
*Krishna Srikar Durbha,Asvin Kumar Venkataramanan,Rajesh Sureddi,Alan C. Bovik*

Main category: cs.CV

TL;DR: 利用图像质量评估（IQA）模型的特征空间训练双层网络，实现生成式AI图像检测的先进性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI内容激增，现有检测方法需提升泛化能力，IQA模型能有效捕捉真实图像的统计特征。

Method: 基于IQA模型的特征空间训练双层网络，检测生成式AI图像并评估其抗图像退化能力。

Result: 该方法在检测不同生成模型的假图像上表现优异，且对图像退化具有显著鲁棒性。

Conclusion: IQA模型的特征空间为生成式AI图像检测提供了高效且鲁棒的解决方案。

Abstract: Image Quality Assessment (IQA) models are employed in many practical image
and video processing pipelines to reduce storage, minimize transmission costs,
and improve the Quality of Experience (QoE) of millions of viewers. These
models are sensitive to a diverse range of image distortions and can accurately
predict image quality as judged by human viewers. Recent advancements in
generative models have resulted in a significant influx of "GenAI" content on
the internet. Existing methods for detecting GenAI content have progressed
significantly with improved generalization performance on images from unseen
generative models. Here, we leverage the capabilities of existing IQA models,
which effectively capture the manifold of real images within a bandpass
statistical space, to distinguish between real and AI-generated images. We
investigate the generalization ability of these perceptual classifiers to the
task of GenAI image detection and evaluate their robustness against various
image degradations. Our results show that a two-layer network trained on the
feature space of IQA models demonstrates state-of-the-art performance in
detecting fake images across generative models, while maintaining significant
robustness against image degradations.

</details>


### [38] [Unsupervised Exposure Correction](https://arxiv.org/abs/2507.17252)
*Ruodai Cui,Li Niu,Guosheng Hu*

Main category: cs.CV

TL;DR: 提出了一种无监督曝光校正方法（UEC），无需人工标注数据，提高了泛化能力，并在低层视觉任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有曝光校正方法需要大量人工标注数据、泛化能力有限以及在低层视觉任务中性能下降的问题。

Method: 利用模拟ISP流程生成的免费配对数据训练模型，提出了一种保留图像细节的转换函数，并构建了一个大规模放射校正数据集。

Result: UEC方法在仅使用0.01%参数的情况下，性能优于现有监督方法，并在边缘检测等下游任务中表现出色。

Conclusion: UEC方法有效解决了曝光校正的挑战，提升了低层视觉任务的性能，且代码和数据集已开源。

Abstract: Current exposure correction methods have three challenges, labor-intensive
paired data annotation, limited generalizability, and performance degradation
in low-level computer vision tasks. In this work, we introduce an innovative
Unsupervised Exposure Correction (UEC) method that eliminates the need for
manual annotations, offers improved generalizability, and enhances performance
in low-level downstream tasks. Our model is trained using freely available
paired data from an emulated Image Signal Processing (ISP) pipeline. This
approach does not need expensive manual annotations, thereby minimizing
individual style biases from the annotation and consequently improving its
generalizability. Furthermore, we present a large-scale Radiometry Correction
Dataset, specifically designed to emphasize exposure variations, to facilitate
unsupervised learning. In addition, we develop a transformation function that
preserves image details and outperforms state-of-the-art supervised methods
[12], while utilizing only 0.01% of their parameters. Our work further
investigates the broader impact of exposure correction on downstream tasks,
including edge detection, demonstrating its effectiveness in mitigating the
adverse effects of poor exposure on low-level features. The source code and
dataset are publicly available at https://github.com/BeyondHeaven/uec_code.

</details>


### [39] [VisionTrap: Unanswerable Questions On Visual Data](https://arxiv.org/abs/2507.17262)
*Asir Saadat,Syem Aziz,Shahriar Mahmud,Abdullah Ibne Masud Mahi,Sabbir Ahmed*

Main category: cs.CV

TL;DR: 该研究探讨了视觉问答（VQA）模型在无法回答问题时的表现，特别是针对不现实图像或无法回答的问题。研究提出了一个名为VisionTrap的数据集，包含三类无法回答的问题，并测试模型是否能正确识别其局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注VQA模型在可回答问题上的表现，而忽略了其在无法回答问题时的行为。本研究旨在填补这一空白，评估模型是否会在应该放弃回答时仍尝试生成错误答案。

Method: 研究引入VisionTrap数据集，包含三类无法回答的问题：混合实体、非传统场景和虚构人物。通过这些问题测试模型的反应。

Result: 研究发现，VQA模型在面对无法回答的问题时，往往仍会尝试生成答案，而非正确识别其局限性。

Conclusion: 研究强调了在VQA基准测试中纳入无法回答问题的重要性，以评估模型是否会在应该放弃回答时仍尝试回答。

Abstract: Visual Question Answering (VQA) has been a widely studied topic, with
extensive research focusing on how VLMs respond to answerable questions based
on real-world images. However, there has been limited exploration of how these
models handle unanswerable questions, particularly in cases where they should
abstain from providing a response. This research investigates VQA performance
on unrealistically generated images or asking unanswerable questions, assessing
whether models recognize the limitations of their knowledge or attempt to
generate incorrect answers. We introduced a dataset, VisionTrap, comprising
three categories of unanswerable questions across diverse image types: (1)
hybrid entities that fuse objects and animals, (2) objects depicted in
unconventional or impossible scenarios, and (3) fictional or non-existent
figures. The questions posed are logically structured yet inherently
unanswerable, testing whether models can correctly recognize their limitations.
Our findings highlight the importance of incorporating such questions into VQA
benchmarks to evaluate whether models tend to answer, even when they should
abstain.

</details>


### [40] [PolarAnything: Diffusion-based Polarimetric Image Synthesis](https://arxiv.org/abs/2507.17268)
*Kailong Zhang,Youwei Lyu,Heng Guo,Si Li,Zhanyu Ma,Boxin Shi*

Main category: cs.CV

TL;DR: PolarAnything 是一种从单张 RGB 输入合成高质量偏振图像的方法，解决了现有偏振模拟器依赖 3D 资产的限制。


<details>
  <summary>Details</summary>
Motivation: 偏振图像在图像增强和 3D 重建中有广泛应用，但偏振相机的普及受限，需要合成逼真的偏振图像。现有模拟器依赖大量 3D 资产，难以生成大规模逼真图像。

Method: 提出 PolarAnything，利用预训练扩散模型的零样本能力，通过扩散生成框架和有效表示策略，从单张 RGB 输入合成偏振图像。

Result: 实验表明，该方法能生成高质量偏振图像，并支持偏振形状重建等下游任务。

Conclusion: PolarAnything 无需依赖 3D 资产，实现了逼真且物理准确的偏振图像合成。

Abstract: Polarization images facilitate image enhancement and 3D reconstruction tasks,
but the limited accessibility of polarization cameras hinders their broader
application. This gap drives the need for synthesizing photorealistic
polarization images.The existing polarization simulator Mitsuba relies on a
parametric polarization image formation model and requires extensive 3D assets
covering shape and PBR materials, preventing it from generating large-scale
photorealistic images. To address this problem, we propose PolarAnything,
capable of synthesizing polarization images from a single RGB input with both
photorealism and physical accuracy, eliminating the dependency on 3D asset
collections. Drawing inspiration from the zero-shot performance of pretrained
diffusion models, we introduce a diffusion-based generative framework with an
effective representation strategy that preserves the fidelity of polarization
properties. Experiments show that our model generates high-quality polarization
images and supports downstream tasks like shape from polarization.

</details>


### [41] [Fully Automated SAM for Single-source Domain Generalization in Medical Image Segmentation](https://arxiv.org/abs/2507.17281)
*Huanli Zhuo,Leilei Ma,Haifeng Zhao,Shiwei Zhou,Dengdi Sun,Yanping Fu*

Main category: cs.CV

TL;DR: FA-SAM框架通过自动生成提示和融合多尺度信息，解决了SAM在医学图像分割中依赖专家提示和提示质量差的问题。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在医学图像分割中依赖专家提示和提示质量差的问题，实现完全自动化分割。

Method: 引入AGM分支和IPEF模块，分别生成自动提示和融合多尺度信息。

Result: 在公开的前列腺和眼底血管数据集上验证了FA-SAM的有效性。

Conclusion: FA-SAM能够解决SAM在医学图像分割中的两大挑战，具有临床应用潜力。

Abstract: Although SAM-based single-source domain generalization models for medical
image segmentation can mitigate the impact of domain shift on the model in
cross-domain scenarios, these models still face two major challenges. First,
the segmentation of SAM is highly dependent on domain-specific expert-annotated
prompts, which prevents SAM from achieving fully automated medical image
segmentation and therefore limits its application in clinical settings. Second,
providing poor prompts (such as bounding boxes that are too small or too large)
to the SAM prompt encoder can mislead SAM into generating incorrect mask
results. Therefore, we propose the FA-SAM, a single-source domain
generalization framework for medical image segmentation that achieves fully
automated SAM. FA-SAM introduces two key innovations: an Auto-prompted
Generation Model (AGM) branch equipped with a Shallow Feature Uncertainty
Modeling (SUFM) module, and an Image-Prompt Embedding Fusion (IPEF) module
integrated into the SAM mask decoder. Specifically, AGM models the uncertainty
distribution of shallow features through the SUFM module to generate bounding
box prompts for the target domain, enabling fully automated segmentation with
SAM. The IPEF module integrates multiscale information from SAM image
embeddings and prompt embeddings to capture global and local details of the
target object, enabling SAM to mitigate the impact of poor prompts. Extensive
experiments on publicly available prostate and fundus vessel datasets validate
the effectiveness of FA-SAM and highlight its potential to address the above
challenges.

</details>


### [42] [PointLAMA: Latent Attention meets Mamba for Efficient Point Cloud Pretraining](https://arxiv.org/abs/2507.17296)
*Xuanyu Lin,Xiaona Zeng,Xianwei Zheng,Xutao Li*

Main category: cs.CV

TL;DR: PointLAMA是一个结合了任务感知点云序列化、混合编码器和条件扩散机制的点云预训练框架，旨在解决Mamba在3D数据中缺乏局部归纳偏置的问题。


<details>
  <summary>Details</summary>
Motivation: Mamba作为点云建模的骨干模型，虽然具有高效的全局序列建模能力，但缺乏局部归纳偏置，限制了其在3D数据中捕捉细粒度几何结构的能力。

Method: 提出PointLAMA框架，包括任务感知点云序列化、混合编码器（结合Latent Attention和Mamba块）以及基于Mamba的条件扩散机制。

Result: 实验结果表明，PointLAMA在多个基准数据集上表现出色，参数和计算量较少，验证了其高效性。

Conclusion: PointLAMA通过结合局部和全局建模能力，有效提升了点云预训练的效率和质量。

Abstract: Mamba has recently gained widespread attention as a backbone model for point
cloud modeling, leveraging a state-space architecture that enables efficient
global sequence modeling with linear complexity. However, its lack of local
inductive bias limits its capacity to capture fine-grained geometric structures
in 3D data. To address this limitation, we propose \textbf{PointLAMA}, a point
cloud pretraining framework that combines task-aware point cloud serialization,
a hybrid encoder with integrated Latent Attention and Mamba blocks, and a
conditional diffusion mechanism built upon the Mamba backbone. Specifically,
the task-aware point cloud serialization employs Hilbert/Trans-Hilbert
space-filling curves and axis-wise sorting to structurally align point tokens
for classification and segmentation tasks, respectively. Our lightweight Latent
Attention block features a Point-wise Multi-head Latent Attention (PMLA)
module, which is specifically designed to align with the Mamba architecture by
leveraging the shared latent space characteristics of PMLA and Mamba. This
enables enhanced local context modeling while preserving overall efficiency. To
further enhance representation learning, we incorporate a conditional diffusion
mechanism during pretraining, which denoises perturbed feature sequences
without relying on explicit point-wise reconstruction. Experimental results
demonstrate that PointLAMA achieves competitive performance on multiple
benchmark datasets with minimal parameter count and FLOPs, validating its
effectiveness for efficient point cloud pretraining.

</details>


### [43] [Learning-based Stage Verification System in Manual Assembly Scenarios](https://arxiv.org/abs/2507.17304)
*Xingjian Zhang,Yutong Duan,Zaishu Chen*

Main category: cs.CV

TL;DR: 提出了一种基于视觉传感器和机器学习的方法，用于工业4.0中装配过程的多目标和状态监测，准确率超过92%，且成本更低。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖多传感器或复杂硬件，成本高且难以在动态工业环境中实施。

Method: 通过整合相同时间戳的状态信息，利用多个机器学习模型实现高精度监测。

Result: 平均准确率超过92%，并提供实时错误检测和可视化指导。

Conclusion: 该方法提高了监测效率和准确性，减少了对昂贵硬件的依赖，适用于现代工业应用。

Abstract: In the context of Industry 4.0, effective monitoring of multiple targets and
states during assembly processes is crucial, particularly when constrained to
using only visual sensors. Traditional methods often rely on either multiple
sensor types or complex hardware setups to achieve high accuracy in monitoring,
which can be cost-prohibitive and difficult to implement in dynamic industrial
environments. This study presents a novel approach that leverages multiple
machine learning models to achieve precise monitoring under the limitation of
using a minimal number of visual sensors. By integrating state information from
identical timestamps, our method detects and confirms the current stage of the
assembly process with an average accuracy exceeding 92%. Furthermore, our
approach surpasses conventional methods by offering enhanced error detection
and visuali-zation capabilities, providing real-time, actionable guidance to
operators. This not only improves the accuracy and efficiency of assembly
monitoring but also re-duces dependency on expensive hardware solutions, making
it a more practical choice for modern industrial applications.

</details>


### [44] [CasP: Improving Semi-Dense Feature Matching Pipeline Leveraging Cascaded Correspondence Priors for Guidance](https://arxiv.org/abs/2507.17312)
*Peiqi Chen,Lei Yu,Yi Wan,Yingying Pei,Xinyi Liu,Yongxiang Yao,Yingying Zhang,Lixiang Ru,Liheng Zhong,Jingdong Chen,Ming Yang,Yongjun Zhang*

Main category: cs.CV

TL;DR: CasP是一种新的半密集特征匹配方法，通过级联对应先验和区域选择性交叉注意力机制，提高了匹配精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全局搜索，限制了精度和效率的进一步提升。

Method: 将匹配阶段分解为两个渐进阶段，利用区域选择性交叉注意力机制增强特征区分性，并在第二阶段限制搜索范围。

Result: CasP在1152分辨率下比ELoFTR快2.2倍，且在几何估计和跨域泛化方面表现优异。

Conclusion: CasP适用于延迟敏感和高鲁棒性应用，如SLAM和无人机系统。

Abstract: Semi-dense feature matching methods have shown strong performance in
challenging scenarios. However, the existing pipeline relies on a global search
across the entire feature map to establish coarse matches, limiting further
improvements in accuracy and efficiency. Motivated by this limitation, we
propose a novel pipeline, CasP, which leverages cascaded correspondence priors
for guidance. Specifically, the matching stage is decomposed into two
progressive phases, bridged by a region-based selective cross-attention
mechanism designed to enhance feature discriminability. In the second phase,
one-to-one matches are determined by restricting the search range to the
one-to-many prior areas identified in the first phase. Additionally, this
pipeline benefits from incorporating high-level features, which helps reduce
the computational costs of low-level feature extraction. The acceleration gains
of CasP increase with higher resolution, and our lite model achieves a speedup
of $\sim2.2\times$ at a resolution of 1152 compared to the most efficient
method, ELoFTR. Furthermore, extensive experiments demonstrate its superiority
in geometric estimation, particularly with impressive cross-domain
generalization. These advantages highlight its potential for latency-sensitive
and high-robustness applications, such as SLAM and UAV systems. Code is
available at https://github.com/pq-chen/CasP.

</details>


### [45] [CartoonAlive: Towards Expressive Live2D Modeling from Single Portraits](https://arxiv.org/abs/2507.17327)
*Chao He,Jianqiang Ren,Jianjing Xiang,Xiejie Shen*

Main category: cs.CV

TL;DR: CartoonAlive提出了一种从单张肖像图像快速生成高质量Live2D数字人的创新方法，结合3D面部建模的形状基概念，实现高效且高表现力的2D卡通风格数字人。


<details>
  <summary>Details</summary>
Motivation: 当前数字人技术主要关注3D模型和2D视频，而交互式2D卡通风格数字人研究较少。Live2D模型在效率和表现力上优于传统3D建模和2D视频方案。

Method: 利用3D面部建模的形状基概念构建适合Live2D的面部混合形状，并通过输入图像的面部关键点推断混合形状权重，快速生成Live2D模型。

Result: 可在30秒内生成与输入肖像高度相似、表现力强的Live2D模型。

Conclusion: CartoonAlive为交互式2D卡通角色创建提供了实用且可扩展的解决方案，拓展了数字内容创作和虚拟角色动画的可能性。

Abstract: With the rapid advancement of large foundation models, AIGC, cloud rendering,
and real-time motion capture technologies, digital humans are now capable of
achieving synchronized facial expressions and body movements, engaging in
intelligent dialogues driven by natural language, and enabling the fast
creation of personalized avatars. While current mainstream approaches to
digital humans primarily focus on 3D models and 2D video-based representations,
interactive 2D cartoon-style digital humans have received relatively less
attention. Compared to 3D digital humans that require complex modeling and high
rendering costs, and 2D video-based solutions that lack flexibility and
real-time interactivity, 2D cartoon-style Live2D models offer a more efficient
and expressive alternative. By simulating 3D-like motion through layered
segmentation without the need for traditional 3D modeling, Live2D enables
dynamic and real-time manipulation. In this technical report, we present
CartoonAlive, an innovative method for generating high-quality Live2D digital
humans from a single input portrait image. CartoonAlive leverages the shape
basis concept commonly used in 3D face modeling to construct facial blendshapes
suitable for Live2D. It then infers the corresponding blendshape weights based
on facial keypoints detected from the input image. This approach allows for the
rapid generation of a highly expressive and visually accurate Live2D model that
closely resembles the input portrait, within less than half a minute. Our work
provides a practical and scalable solution for creating interactive 2D cartoon
characters, opening new possibilities in digital content creation and virtual
character animation. The project homepage is
https://human3daigc.github.io/CartoonAlive_webpage/.

</details>


### [46] [PARTE: Part-Guided Texturing for 3D Human Reconstruction from a Single Image](https://arxiv.org/abs/2507.17332)
*Hyeongjin Nam,Donghwan Kim,Gyeongsik Moon,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: PARTE利用3D人体部位信息作为关键指导，通过两部分核心组件（PartSegmenter和PartTexturer）解决现有3D人体重建方法中纹理不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有3D人体重建方法未充分利用部位分割先验，导致纹理不对齐。

Method: 提出PartSegmenter预测部位标签，PartTexturer利用部位信息指导纹理重建。

Result: 实验表明PARTE在3D人体重建中达到最先进质量。

Conclusion: PARTE通过部位信息显著提升了纹理对齐效果。

Abstract: The misaligned human texture across different human parts is one of the main
limitations of existing 3D human reconstruction methods. Each human part, such
as a jacket or pants, should maintain a distinct texture without blending into
others. The structural coherence of human parts serves as a crucial cue to
infer human textures in the invisible regions of a single image. However, most
existing 3D human reconstruction methods do not explicitly exploit such part
segmentation priors, leading to misaligned textures in their reconstructions.
In this regard, we present PARTE, which utilizes 3D human part information as a
key guide to reconstruct 3D human textures. Our framework comprises two core
components. First, to infer 3D human part information from a single image, we
propose a 3D part segmentation module (PartSegmenter) that initially
reconstructs a textureless human surface and predicts human part labels based
on the textureless surface. Second, to incorporate part information into
texture reconstruction, we introduce a part-guided texturing module
(PartTexturer), which acquires prior knowledge from a pre-trained image
generation network on texture alignment of human parts. Extensive experiments
demonstrate that our framework achieves state-of-the-art quality in 3D human
reconstruction. The project page is available at
https://hygenie1228.github.io/PARTE/.

</details>


### [47] [Temporal Point-Supervised Signal Reconstruction: A Human-Annotation-Free Framework for Weak Moving Target Detection](https://arxiv.org/abs/2507.17334)
*Weihua Gao,Chunxu Ren,Wenlong Niu,Xiaodong Peng*

Main category: cs.CV

TL;DR: 提出了一种无需人工标注的弱运动目标检测框架TPS，通过像素级时序信号建模和动态多尺度注意力模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 低空监视系统中弱目标检测因信号弱、背景复杂和缺乏标注而困难，现有方法难以提取鲁棒特征。

Method: 采用时序点监督框架（TPS），开发时序信号重建网络（TSRNet）和动态多尺度注意力模块（DMSAttention），结合图轨迹挖掘策略。

Result: 在低信噪比数据集上表现优于现有方法，检测性能强且实时性高（1000 FPS）。

Conclusion: TPS框架无需人工标注，适用于实际场景中的实时弱目标检测。

Abstract: In low-altitude surveillance and early warning systems, detecting weak moving
targets remains a significant challenge due to low signal energy, small spatial
extent, and complex background clutter. Existing methods struggle with
extracting robust features and suffer from the lack of reliable annotations. To
address these limitations, we propose a novel Temporal Point-Supervised (TPS)
framework that enables high-performance detection of weak targets without any
manual annotations.Instead of conventional frame-based detection, our framework
reformulates the task as a pixel-wise temporal signal modeling problem, where
weak targets manifest as short-duration pulse-like responses. A Temporal Signal
Reconstruction Network (TSRNet) is developed under the TPS paradigm to
reconstruct these transient signals.TSRNet adopts an encoder-decoder
architecture and integrates a Dynamic Multi-Scale Attention (DMSAttention)
module to enhance its sensitivity to diverse temporal patterns. Additionally, a
graph-based trajectory mining strategy is employed to suppress false alarms and
ensure temporal consistency.Extensive experiments on a purpose-built low-SNR
dataset demonstrate that our framework outperforms state-of-the-art methods
while requiring no human annotations. It achieves strong detection performance
and operates at over 1000 FPS, underscoring its potential for real-time
deployment in practical scenarios.

</details>


### [48] [TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition](https://arxiv.org/abs/2507.17335)
*Guangzhu Xu,Zhi Ke,Pengcheng Zuo,Bangjun Lei*

Main category: cs.CV

TL;DR: 提出了一种结合轻量级视觉编码器和文本解码器的统一解决方案，用于中英文单双行车牌识别，通过合成数据集和视角校正网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决CNN和CRNN在车牌识别中的局限性，尤其是单双行车牌数据稀缺和成像条件多样的问题。

Method: 构建合成数据集，引入视角校正网络（PTN），结合预训练框架。

Result: 在CCPD测试集上平均识别准确率达99.34%，双行车牌测试集达98.70%，处理速度167帧/秒。

Conclusion: 算法在准确性和速度上表现优异，具有强实用性。

Abstract: License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.

</details>


### [49] [DeMo++: Motion Decoupling for Autonomous Driving](https://arxiv.org/abs/2507.17342)
*Bozhou Zhang,Nan Song,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: DeMo++ 是一个用于自动驾驶的运动预测和规划框架，通过解耦运动意图和时空状态，结合注意力与Mamba机制，实现了多样性和精确性的平衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在建模复杂时空轨迹时表现不足，容易导致碰撞或次优结果，需要更全面的解决方案。

Method: DeMo++ 将运动估计分为全局运动意图和精细时空状态，并引入跨场景轨迹交互机制，结合注意力与Mamba模型。

Result: 在多个基准测试（Argoverse 2、nuScenes、nuPlan、NAVSIM）中达到最先进性能。

Conclusion: DeMo++ 通过解耦和交互机制，显著提升了运动预测和规划的多样性与精确性。

Abstract: Motion forecasting and planning are tasked with estimating the trajectories
of traffic agents and the ego vehicle, respectively, to ensure the safety and
efficiency of autonomous driving systems in dynamically changing environments.
State-of-the-art methods typically adopt a one-query-one-trajectory paradigm,
where each query corresponds to a unique trajectory for predicting multi-mode
trajectories. While this paradigm can produce diverse motion intentions, it
often falls short in modeling the intricate spatiotemporal evolution of
trajectories, which can lead to collisions or suboptimal outcomes. To overcome
this limitation, we propose DeMo++, a framework that decouples motion
estimation into two distinct components: holistic motion intentions to capture
the diverse potential directions of movement, and fine spatiotemporal states to
track the agent's dynamic progress within the scene and enable a
self-refinement capability. Further, we introduce a cross-scene trajectory
interaction mechanism to explore the relationships between motions in adjacent
scenes. This allows DeMo++ to comprehensively model both the diversity of
motion intentions and the spatiotemporal evolution of each trajectory. To
effectively implement this framework, we developed a hybrid model combining
Attention and Mamba. This architecture leverages the strengths of both
mechanisms for efficient scene information aggregation and precise trajectory
state sequence modeling. Extensive experiments demonstrate that DeMo++ achieves
state-of-the-art performance across various benchmarks, including motion
forecasting (Argoverse 2 and nuScenes), motion planning (nuPlan), and
end-to-end planning (NAVSIM).

</details>


### [50] [Principled Multimodal Representation Learning](https://arxiv.org/abs/2507.17343)
*Xiaohao Liu,Xiaobo Xia,See-Kiong Ng,Tat-Seng Chua*

Main category: cs.CV

TL;DR: PMRL提出了一种无需锚点的多模态表示学习框架，通过优化主导奇异值实现稳定对齐。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖预定义锚点，限制了多模态对齐的灵活性，且优化过程中存在不稳定性。

Method: PMRL基于Gram矩阵秩为1的理论，优化主导奇异值，并设计软最大损失函数和对比正则化。

Result: 实验表明PMRL在多种任务上优于基线方法。

Conclusion: PMRL实现了稳定且无需锚点的多模态对齐，具有广泛适用性。

Abstract: Multimodal representation learning seeks to create a unified representation
space by integrating diverse data modalities to improve multimodal
understanding. Traditional methods often depend on pairwise contrastive
learning, which relies on a predefined anchor modality, restricting alignment
across all modalities. Recent advances have investigated the simultaneous
alignment of multiple modalities, yet several challenges remain, such as
limitations imposed by fixed anchor points and instability arising from
optimizing the product of singular values. To address the challenges, in this
paper, we propose Principled Multimodal Representation Learning (PMRL), a novel
framework that achieves simultaneous alignment of multiple modalities without
anchor dependency in a more stable manner. Specifically, grounded in the
theoretical insight that full alignment corresponds to a rank-1 Gram matrix,
PMRL optimizes the dominant singular value of the representation matrix to
align modalities along a shared leading direction. We propose a softmax-based
loss function that treats singular values as logits to prioritize the largest
singular value. Besides, instance-wise contrastive regularization on the
leading eigenvectors maintains inter-instance separability and prevents
representation collapse. Extensive experiments across diverse tasks demonstrate
PMRL's superiority compared to baseline methods. The source code will be
publicly available.

</details>


### [51] [Swin-TUNA : A Novel PEFT Approach for Accurate Food Image Segmentation](https://arxiv.org/abs/2507.17347)
*Haotian Chen,Zhiyong Xiao*

Main category: cs.CV

TL;DR: Swin-TUNA是一种参数高效微调方法，通过多尺度可训练适配器改进Swin Transformer，仅更新4%参数即可实现高性能食物图像分割。


<details>
  <summary>Details</summary>
Motivation: 现有大规模Transformer模型（如FoodSAM）因参数过多和计算资源需求高，难以满足实际部署需求。

Method: 在Swin Transformer中集成多尺度可训练适配器，设计分层特征适应机制，结合动态平衡策略。

Result: 在FoodSeg103和UECFoodPix Complete数据集上分别达到50.56%和74.94%的mIoU，参数减少98.7%。

Conclusion: Swin-TUNA在低数据场景下收敛更快、泛化能力更强，为轻量级食物图像处理提供高效解决方案。

Abstract: In the field of food image processing, efficient semantic segmentation
techniques are crucial for industrial applications. However, existing
large-scale Transformer-based models (such as FoodSAM) face challenges in
meeting practical deploymentrequirements due to their massive parameter counts
and high computational resource demands. This paper introduces TUNable Adapter
module (Swin-TUNA), a Parameter Efficient Fine-Tuning (PEFT) method that
integrates multiscale trainable adapters into the Swin Transformer
architecture, achieving high-performance food image segmentation by updating
only 4% of the parameters. The core innovation of Swin-TUNA lies in its
hierarchical feature adaptation mechanism: it designs separable convolutions in
depth and dimensional mappings of varying scales to address the differences in
features between shallow and deep networks, combined with a dynamic balancing
strategy for tasks-agnostic and task-specific features. Experiments demonstrate
that this method achieves mIoU of 50.56% and 74.94% on the FoodSeg103 and
UECFoodPix Complete datasets, respectively, surpassing the fully parameterized
FoodSAM model while reducing the parameter count by 98.7% (to only 8.13M).
Furthermore, Swin-TUNA exhibits faster convergence and stronger generalization
capabilities in low-data scenarios, providing an efficient solution for
assembling lightweight food image.

</details>


### [52] [Exploring Active Learning for Label-Efficient Training of Semantic Neural Radiance Field](https://arxiv.org/abs/2507.17351)
*Yuzhe Zhu,Lile Cai,Kangkang Lu,Fayao Liu,Xulei Yang*

Main category: cs.CV

TL;DR: Active learning reduces annotation cost for semantically-aware NeRF by over 2X compared to random sampling.


<details>
  <summary>Details</summary>
Motivation: Pixel-level class labels for semantically-aware NeRF training are expensive to collect.

Method: Explores active learning with novel strategies, including 3D geometric constraints.

Result: Achieves more than 2X reduction in annotation cost.

Conclusion: Active learning is effective for reducing annotation burden in semantically-aware NeRF training.

Abstract: Neural Radiance Field (NeRF) models are implicit neural scene representation
methods that offer unprecedented capabilities in novel view synthesis.
Semantically-aware NeRFs not only capture the shape and radiance of a scene,
but also encode semantic information of the scene. The training of
semantically-aware NeRFs typically requires pixel-level class labels, which can
be prohibitively expensive to collect. In this work, we explore active learning
as a potential solution to alleviate the annotation burden. We investigate
various design choices for active learning of semantically-aware NeRF,
including selection granularity and selection strategies. We further propose a
novel active learning strategy that takes into account 3D geometric constraints
in sample selection. Our experiments demonstrate that active learning can
effectively reduce the annotation cost of training semantically-aware NeRF,
achieving more than 2X reduction in annotation cost compared to random
sampling.

</details>


### [53] [Exploring Active Learning for Semiconductor Defect Segmentation](https://arxiv.org/abs/2507.17359)
*Lile Cai,Ramanpreet Singh Pahwa,Xun Xu,Jie Wang,Richard Chang,Lining Zhang,Chuan-Sheng Foo*

Main category: cs.CV

TL;DR: 论文探讨了在半导体X射线显微镜扫描中应用主动学习以减少标注负担的方法，提出了对比预训练和稀有类别感知的样本选择策略。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在半导体XRM扫描中需要大量标注数据的问题，尤其是面对大域偏移和严重类别不平衡的挑战。

Method: 采用对比预训练初始化权重，并提出稀有类别感知的样本选择函数。

Result: 在半导体数据集上实现了最先进的性能。

Conclusion: 提出的方法有效缓解了标注负担，并在XRM扫描的语义分割任务中表现出色。

Abstract: The development of X-Ray microscopy (XRM) technology has enabled
non-destructive inspection of semiconductor structures for defect
identification. Deep learning is widely used as the state-of-the-art approach
to perform visual analysis tasks. However, deep learning based models require
large amount of annotated data to train. This can be time-consuming and
expensive to obtain especially for dense prediction tasks like semantic
segmentation. In this work, we explore active learning (AL) as a potential
solution to alleviate the annotation burden. We identify two unique challenges
when applying AL on semiconductor XRM scans: large domain shift and severe
class-imbalance. To address these challenges, we propose to perform contrastive
pretraining on the unlabelled data to obtain the initialization weights for
each AL cycle, and a rareness-aware acquisition function that favors the
selection of samples containing rare classes. We evaluate our method on a
semiconductor dataset that is compiled from XRM scans of high bandwidth memory
structures composed of logic and memory dies, and demonstrate that our method
achieves state-of-the-art performance.

</details>


### [54] [Exploring Spatial Diversity for Region-based Active Learning](https://arxiv.org/abs/2507.17367)
*Lile Cai,Xun Xu,Lining Zhang,Chuan-Sheng Foo*

Main category: cs.CV

TL;DR: 提出了一种基于区域主动学习的框架，通过选择信息丰富的图像区域而非整张图像进行标注，结合空间多样性和传统选择标准，显著降低了标注成本并保持了高性能。


<details>
  <summary>Details</summary>
Motivation: 减少语义分割任务中密集像素级标注的高成本，同时保持模型性能。

Method: 提出区域主动学习框架，结合空间多样性和传统选择标准（如不确定性）进行优化。

Result: 在Cityscapes和PASCAL VOC数据集上，仅需标注5-9%的像素即可达到全监督方法95%的性能。

Conclusion: 空间多样性的引入显著提升了区域主动学习方法的性能，优于现有方法。

Abstract: State-of-the-art methods for semantic segmentation are based on deep neural
networks trained on large-scale labeled datasets. Acquiring such datasets would
incur large annotation costs, especially for dense pixel-level prediction tasks
like semantic segmentation. We consider region-based active learning as a
strategy to reduce annotation costs while maintaining high performance. In this
setting, batches of informative image regions instead of entire images are
selected for labeling. Importantly, we propose that enforcing local spatial
diversity is beneficial for active learning in this case, and to incorporate
spatial diversity along with the traditional active selection criterion, e.g.,
data sample uncertainty, in a unified optimization framework for region-based
active learning. We apply this framework to the Cityscapes and PASCAL VOC
datasets and demonstrate that the inclusion of spatial diversity effectively
improves the performance of uncertainty-based and feature diversity-based
active learning methods. Our framework achieves $95\%$ performance of fully
supervised methods with only $5-9\%$ of the labeled pixels, outperforming all
state-of-the-art region-based active learning methods for semantic
segmentation.

</details>


### [55] [SFUOD: Source-Free Unknown Object Detection](https://arxiv.org/abs/2507.17373)
*Keon-Hee Park,Seun-An Choe,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: 论文提出了一种新的无源未知物体检测（SFUOD）场景，并提出了CollaPAUL框架，通过协作调优和基于主轴的未知标记方法，实现了在无源数据情况下检测已知和未知物体的目标。


<details>
  <summary>Details</summary>
Motivation: 解决传统无源物体检测仅能检测预定义物体的限制，扩展至未知物体的检测。

Method: 提出CollaPAUL框架，结合协作调优（跨域注意力机制）和基于主轴的未知标记方法。

Result: 在SFUOD基准测试中达到最先进性能，实验验证了其有效性。

Conclusion: CollaPAUL框架成功解决了无源未知物体检测问题，具有实际应用价值。

Abstract: Source-free object detection adapts a detector pre-trained on a source domain
to an unlabeled target domain without requiring access to labeled source data.
While this setting is practical as it eliminates the need for the source
dataset during domain adaptation, it operates under the restrictive assumption
that only pre-defined objects from the source domain exist in the target
domain. This closed-set setting prevents the detector from detecting undefined
objects. To ease this assumption, we propose Source-Free Unknown Object
Detection (SFUOD), a novel scenario which enables the detector to not only
recognize known objects but also detect undefined objects as unknown objects.
To this end, we propose CollaPAUL (Collaborative tuning and Principal
Axis-based Unknown Labeling), a novel framework for SFUOD. Collaborative tuning
enhances knowledge adaptation by integrating target-dependent knowledge from
the auxiliary encoder with source-dependent knowledge from the pre-trained
detector through a cross-domain attention mechanism. Additionally, principal
axes-based unknown labeling assigns pseudo-labels to unknown objects by
estimating objectness via principal axes projection and confidence scores from
model predictions. The proposed CollaPAUL achieves state-of-the-art
performances on SFUOD benchmarks, and extensive experiments validate its
effectiveness.

</details>


### [56] [A Conditional Probability Framework for Compositional Zero-shot Learning](https://arxiv.org/abs/2507.17377)
*Peng Wu,Qiuxia Lai,Hao Fang,Guo-Sen Xie,Yilong Yin,Xiankai Lu,Wenguan Wang*

Main category: cs.CV

TL;DR: 本文提出了一种条件概率框架（CPF）来显式建模属性与对象之间的依赖关系，通过联合优化对象似然和条件属性似然，有效捕捉组合依赖关系。


<details>
  <summary>Details</summary>
Motivation: 传统方法将属性和对象视为独立实体，忽略了组合中的语义约束和上下文依赖。本文旨在解决这一挑战。

Method: 采用条件概率框架（CPF），分解组合概率为对象似然和条件属性似然，并结合文本描述和跨注意力机制增强特征学习。

Result: 在多个CZSL基准测试中表现优越，验证了方法的有效性。

Conclusion: 通过显式建模属性-对象依赖关系，本文方法显著提升了组合零样本学习的性能。

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize unseen combinations
of known objects and attributes by leveraging knowledge from previously seen
compositions. Traditional approaches primarily focus on disentangling
attributes and objects, treating them as independent entities during learning.
However, this assumption overlooks the semantic constraints and contextual
dependencies inside a composition. For example, certain attributes naturally
pair with specific objects (e.g., "striped" applies to "zebra" or "shirts" but
not "sky" or "water"), while the same attribute can manifest differently
depending on context (e.g., "young" in "young tree" vs. "young dog"). Thus,
capturing attribute-object interdependence remains a fundamental yet
long-ignored challenge in CZSL. In this paper, we adopt a Conditional
Probability Framework (CPF) to explicitly model attribute-object dependencies.
We decompose the probability of a composition into two components: the
likelihood of an object and the conditional likelihood of its attribute. To
enhance object feature learning, we incorporate textual descriptors to
highlight semantically relevant image regions. These enhanced object features
then guide attribute learning through a cross-attention mechanism, ensuring
better contextual alignment. By jointly optimizing object likelihood and
conditional attribute likelihood, our method effectively captures compositional
dependencies and generalizes well to unseen compositions. Extensive experiments
on multiple CZSL benchmarks demonstrate the superiority of our approach. Code
is available at here.

</details>


### [57] [EndoGen: Conditional Autoregressive Endoscopic Video Generation](https://arxiv.org/abs/2507.17388)
*Xinyu Liu,Hengyu Liu,Cheng Wang,Tianming Liu,Yixuan Yuan*

Main category: cs.CV

TL;DR: 提出首个条件性内窥镜视频生成框架EndoGen，结合时空网格帧模式（SGP）和语义感知标记掩码（SAT），生成高质量视频并提升息肉分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于静态图像或无条件的生成，缺乏动态上下文和临床参考价值。

Method: 采用自回归模型，结合SGP策略和SAT机制，优化多帧生成和语义区域关注。

Result: 实验证明EndoGen能生成高质量条件性内窥镜视频，并提升下游任务性能。

Conclusion: EndoGen为内窥镜视频生成提供了有效解决方案，具有临床应用潜力。

Abstract: Endoscopic video generation is crucial for advancing medical imaging and
enhancing diagnostic capabilities. However, prior efforts in this field have
either focused on static images, lacking the dynamic context required for
practical applications, or have relied on unconditional generation that fails
to provide meaningful references for clinicians. Therefore, in this paper, we
propose the first conditional endoscopic video generation framework, namely
EndoGen. Specifically, we build an autoregressive model with a tailored
Spatiotemporal Grid-Frame Patterning (SGP) strategy. It reformulates the
learning of generating multiple frames as a grid-based image generation
pattern, which effectively capitalizes the inherent global dependency modeling
capabilities of autoregressive architectures. Furthermore, we propose a
Semantic-Aware Token Masking (SAT) mechanism, which enhances the model's
ability to produce rich and diverse content by selectively focusing on
semantically meaningful regions during the generation process. Through
extensive experiments, we demonstrate the effectiveness of our framework in
generating high-quality, conditionally guided endoscopic content, and improves
the performance of downstream task of polyp segmentation. Code released at
https://www.github.com/CUHK-AIM-Group/EndoGen.

</details>


### [58] [HiProbe-VAD: Video Anomaly Detection via Hidden States Probing in Tuning-Free Multimodal LLMs](https://arxiv.org/abs/2507.17394)
*Zhaolin Cai,Fan Li,Ziwei Zheng,Yanjun Qin*

Main category: cs.CV

TL;DR: HiProbe-VAD利用预训练的多模态大语言模型（MLLMs）进行视频异常检测，无需微调，通过动态层显著性探测（DLSP）提取信息丰富的隐藏状态，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频异常检测方法计算量大且依赖大量标注数据，限制了实际应用。HiProbe-VAD旨在解决这些问题。

Method: 提出动态层显著性探测（DLSP）机制，从MLLMs的中间层提取信息丰富的隐藏状态，结合轻量级异常评分器和时序定位模块进行检测。

Result: 在UCF-Crime和XD-Violence数据集上表现优于现有无训练方法和大多数传统方法，并展示了跨模型泛化能力。

Conclusion: HiProbe-VAD为视频异常检测提供了更实用和可扩展的解决方案，释放了预训练MLLMs的潜力。

Abstract: Video Anomaly Detection (VAD) aims to identify and locate deviations from
normal patterns in video sequences. Traditional methods often struggle with
substantial computational demands and a reliance on extensive labeled datasets,
thereby restricting their practical applicability. To address these
constraints, we propose HiProbe-VAD, a novel framework that leverages
pre-trained Multimodal Large Language Models (MLLMs) for VAD without requiring
fine-tuning. In this paper, we discover that the intermediate hidden states of
MLLMs contain information-rich representations, exhibiting higher sensitivity
and linear separability for anomalies compared to the output layer. To
capitalize on this, we propose a Dynamic Layer Saliency Probing (DLSP)
mechanism that intelligently identifies and extracts the most informative
hidden states from the optimal intermediate layer during the MLLMs reasoning.
Then a lightweight anomaly scorer and temporal localization module efficiently
detects anomalies using these extracted hidden states and finally generate
explanations. Experiments on the UCF-Crime and XD-Violence datasets demonstrate
that HiProbe-VAD outperforms existing training-free and most traditional
approaches. Furthermore, our framework exhibits remarkable cross-model
generalization capabilities in different MLLMs without any tuning, unlocking
the potential of pre-trained MLLMs for video anomaly detection and paving the
way for more practical and scalable solutions.

</details>


### [59] [HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning](https://arxiv.org/abs/2507.17402)
*Li Jun,Wang Jinpeng,Tan Chaolei,Lian Niu,Chen Long,Zhang Min,Wang Yaowei,Xia Shu-Tao,Chen Bin*

Main category: cs.CV

TL;DR: HLFormer提出了一种双曲空间建模框架，通过混合空间编码和动态特征融合，解决了PRVR中欧几里得空间的几何失真问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在欧几里得空间中存在几何失真，无法充分捕捉视频的层次结构，导致次优的时间建模。

Method: HLFormer结合了Lorentz Attention Block和Euclidean Attention Block，通过Mean-Guided Adaptive Interaction Module动态融合特征，并引入Partial Order Preservation Loss强化层次约束。

Result: 实验表明HLFormer优于现有方法。

Conclusion: 双曲空间建模能有效提升PRVR任务中的层次语义捕捉和跨模态匹配能力。

Abstract: Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.

</details>


### [60] [Physics-based Human Pose Estimation from a Single Moving RGB Camera](https://arxiv.org/abs/2507.17406)
*Ayce Idil Aytekin,Chuqiao Li,Diogo Luvizon,Rishabh Dabral,Martin Oswald,Marc Habermann,Christian Theobalt*

Main category: cs.CV

TL;DR: MoviCam数据集和PhysDynPose方法解决了动态相机和非平面场景下的人体姿态跟踪问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在非平面场景或动态相机下表现不佳，且缺乏真实世界数据的支持。

Method: 结合SLAM和物理优化，利用MoviCam数据集进行场景感知的姿态优化。

Result: PhysDynPose在动态相机和非平面场景下表现优于现有方法。

Conclusion: 新数据集和方法为复杂场景下的人体姿态跟踪提供了更可靠的解决方案。

Abstract: Most monocular and physics-based human pose tracking methods, while achieving
state-of-the-art results, suffer from artifacts when the scene does not have a
strictly flat ground plane or when the camera is moving. Moreover, these
methods are often evaluated on in-the-wild real world videos without
ground-truth data or on synthetic datasets, which fail to model the real world
light transport, camera motion, and pose-induced appearance and geometry
changes. To tackle these two problems, we introduce MoviCam, the first
non-synthetic dataset containing ground-truth camera trajectories of a
dynamically moving monocular RGB camera, scene geometry, and 3D human motion
with human-scene contact labels. Additionally, we propose PhysDynPose, a
physics-based method that incorporates scene geometry and physical constraints
for more accurate human motion tracking in case of camera motion and non-flat
scenes. More precisely, we use a state-of-the-art kinematics estimator to
obtain the human pose and a robust SLAM method to capture the dynamic camera
trajectory, enabling the recovery of the human pose in the world frame. We then
refine the kinematic pose estimate using our scene-aware physics optimizer.
From our new benchmark, we found that even state-of-the-art methods struggle
with this inherently challenging setting, i.e. a moving camera and non-planar
environments, while our method robustly estimates both human and camera poses
in world coordinates.

</details>


### [61] [Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging](https://arxiv.org/abs/2507.17412)
*Farnaz Khun Jush,Steffen Vogler,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文提出了一种名为C-MIR的新型体积医学图像检索方法，通过消除对预分割数据的依赖并引入上下文感知的重新排序机制，显著提升了肿瘤标记和分期的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像数量的增加给放射科医生检索相关病例带来了挑战，现有的CBIR系统缺乏标准化评估和全面研究。

Method: 提出了C-MIR方法，基于ColBERT的上下文交互机制，适用于3D医学图像，并进行了多肿瘤部位、特征提取器和数据库配置的全面评估。

Result: C-MIR在肿瘤标记（尤其是结肠和肺部肿瘤）中表现出显著改进（p<0.05），并展示了在肿瘤分期中的潜力。

Conclusion: C-MIR为医学图像检索提供了高效且实用的解决方案，有望改善医疗诊断流程。

Abstract: The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.

</details>


### [62] [CAPRI-CT: Causal Analysis and Predictive Reasoning for Image Quality Optimization in Computed Tomography](https://arxiv.org/abs/2507.17420)
*Sneha George Gnanakalavathy,Hairil Abdul Razak,Robert Meertens,Jonathan E. Fieldsend,Xujiong Ye,Mohammed M. Abdelsamea*

Main category: cs.CV

TL;DR: CAPRI-CT是一个因果感知的深度学习框架，用于优化CT成像质量，通过建模因果关系预测信噪比，支持反事实推理。


<details>
  <summary>Details</summary>
Motivation: 在CT成像中，如何在减少辐射暴露的同时保持高质量图像是一个关键挑战。

Method: CAPRI-CT整合图像数据和采集元数据，使用变分自编码器提取特征并生成因果表示，预测信噪比并支持反事实推理。

Result: CAPRI-CT通过集成学习方法取得了强预测性能，提供了可操作的见解。

Conclusion: CAPRI-CT帮助设计更高效的CT协议，减少重复扫描，代码和数据集已公开。

Abstract: In computed tomography (CT), achieving high image quality while minimizing
radiation exposure remains a key clinical challenge. This paper presents
CAPRI-CT, a novel causal-aware deep learning framework for Causal Analysis and
Predictive Reasoning for Image Quality Optimization in CT imaging. CAPRI-CT
integrates image data with acquisition metadata (such as tube voltage, tube
current, and contrast agent types) to model the underlying causal relationships
that influence image quality. An ensemble of Variational Autoencoders (VAEs) is
employed to extract meaningful features and generate causal representations
from observational data, including CT images and associated imaging parameters.
These input features are fused to predict the Signal-to-Noise Ratio (SNR) and
support counterfactual inference, enabling what-if simulations, such as changes
in contrast agents (types and concentrations) or scan parameters. CAPRI-CT is
trained and validated using an ensemble learning approach, achieving strong
predictive performance. By facilitating both prediction and interpretability,
CAPRI-CT provides actionable insights that could help radiologists and
technicians design more efficient CT protocols without repeated physical scans.
The source code and dataset are publicly available at
https://github.com/SnehaGeorge22/capri-ct.

</details>


### [63] [Dynamic-DINO: Fine-Grained Mixture of Experts Tuning for Real-time Open-Vocabulary Object Detection](https://arxiv.org/abs/2507.17436)
*Yehao Lu,Minghe Weng,Zekang Xiao,Rui Jiang,Wei Su,Guangcong Zheng,Ping Lu,Xi Li*

Main category: cs.CV

TL;DR: Dynamic-DINO通过MoE-Tuning策略将Grounding DINO 1.5 Edge从密集模型扩展为动态推理框架，提升了实时开放词汇目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 探索MoE架构在小型模型中的潜力，特别是在实时开放词汇目标检测领域。

Method: 提出动态推理框架Dynamic-DINO，包括专家网络分解、预训练权重分配和路由器初始化策略。

Result: Dynamic-DINO在仅使用1.56M开源数据预训练的情况下，性能优于使用私有数据集的Grounding DINO 1.5 Edge。

Conclusion: MoE架构在小型模型中具有潜力，Dynamic-DINO为实时开放词汇目标检测提供了高效解决方案。

Abstract: The Mixture of Experts (MoE) architecture has excelled in Large
Vision-Language Models (LVLMs), yet its potential in real-time open-vocabulary
object detectors, which also leverage large-scale vision-language datasets but
smaller models, remains unexplored. This work investigates this domain,
revealing intriguing insights. In the shallow layers, experts tend to cooperate
with diverse peers to expand the search space. While in the deeper layers,
fixed collaborative structures emerge, where each expert maintains 2-3 fixed
partners and distinct expert combinations are specialized in processing
specific patterns. Concretely, we propose Dynamic-DINO, which extends Grounding
DINO 1.5 Edge from a dense model to a dynamic inference framework via an
efficient MoE-Tuning strategy. Additionally, we design a granularity
decomposition mechanism to decompose the Feed-Forward Network (FFN) of base
model into multiple smaller expert networks, expanding the subnet search space.
To prevent performance degradation at the start of fine-tuning, we further
propose a pre-trained weight allocation strategy for the experts, coupled with
a specific router initialization. During inference, only the input-relevant
experts are activated to form a compact subnet. Experiments show that,
pretrained with merely 1.56M open-source data, Dynamic-DINO outperforms
Grounding DINO 1.5 Edge, pretrained on the private Grounding20M dataset.

</details>


### [64] [VLM-Guided Visual Place Recognition for Planet-Scale Geo-Localization](https://arxiv.org/abs/2507.17455)
*Sania Waheed,Na Min An,Michael Milford,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出了一种结合视觉语言模型（VLM）和检索式视觉地点识别（VPR）的混合地理定位框架，显著提升了定位精度。


<details>
  <summary>Details</summary>
Motivation: 传统检索方法难以扩展且易受感知混淆影响，分类方法缺乏泛化性且需大量训练数据。VLM虽准确但易产生幻觉且缺乏可解释性。

Method: 先利用VLM生成地理先验以约束检索空间，再通过检索和重排序机制选择最合理匹配。

Result: 在多个基准测试中表现优于现有方法，街道和城市级别精度分别提升4.51%和13.52%。

Conclusion: VLM生成的地理先验与VPR结合，能构建可扩展、鲁棒且高精度的地理定位系统。

Abstract: Geo-localization from a single image at planet scale (essentially an advanced
or extreme version of the kidnapped robot problem) is a fundamental and
challenging task in applications such as navigation, autonomous driving and
disaster response due to the vast diversity of locations, environmental
conditions, and scene variations. Traditional retrieval-based methods for
geo-localization struggle with scalability and perceptual aliasing, while
classification-based approaches lack generalization and require extensive
training data. Recent advances in vision-language models (VLMs) offer a
promising alternative by leveraging contextual understanding and reasoning.
However, while VLMs achieve high accuracy, they are often prone to
hallucinations and lack interpretability, making them unreliable as standalone
solutions. In this work, we propose a novel hybrid geo-localization framework
that combines the strengths of VLMs with retrieval-based visual place
recognition (VPR) methods. Our approach first leverages a VLM to generate a
prior, effectively guiding and constraining the retrieval search space. We then
employ a retrieval step, followed by a re-ranking mechanism that selects the
most geographically plausible matches based on feature similarity and proximity
to the initially estimated coordinates. We evaluate our approach on multiple
geo-localization benchmarks and show that it consistently outperforms prior
state-of-the-art methods, particularly at street (up to 4.51%) and city level
(up to 13.52%). Our results demonstrate that VLM-generated geographic priors in
combination with VPR lead to scalable, robust, and accurate geo-localization
systems.

</details>


### [65] [Dynamic Scoring with Enhanced Semantics for Training-Free Human-Object Interaction Detection](https://arxiv.org/abs/2507.17456)
*Francesco Tonini,Lorenzo Vaquero,Alessandro Conti,Cigdem Beyan,Elisa Ricci*

Main category: cs.CV

TL;DR: DYSCO是一个无需训练的HOI检测框架，利用多模态注册表增强语义，在罕见交互中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有HOI方法依赖大量人工标注数据，成本高且难以扩展。VLMs的潜力未被充分挖掘。

Method: 提出DYSCO框架，结合文本和视觉交互表示，使用多模态注册表和创新的交互签名，并采用多头注意力机制。

Result: DYSCO超越无需训练的最先进模型，在罕见交互中表现突出。

Conclusion: DYSCO展示了VLMs在HOI检测中的潜力，无需训练即可实现高性能。

Abstract: Human-Object Interaction (HOI) detection aims to identify humans and objects
within images and interpret their interactions. Existing HOI methods rely
heavily on large datasets with manual annotations to learn interactions from
visual cues. These annotations are labor-intensive to create, prone to
inconsistency, and limit scalability to new domains and rare interactions. We
argue that recent advances in Vision-Language Models (VLMs) offer untapped
potential, particularly in enhancing interaction representation. While prior
work has injected such potential and even proposed training-free methods, there
remain key gaps. Consequently, we propose a novel training-free HOI detection
framework for Dynamic Scoring with enhanced semantics (DYSCO) that effectively
utilizes textual and visual interaction representations within a multimodal
registry, enabling robust and nuanced interaction understanding. This registry
incorporates a small set of visual cues and uses innovative interaction
signatures to improve the semantic alignment of verbs, facilitating effective
generalization to rare interactions. Additionally, we propose a unique
multi-head attention mechanism that adaptively weights the contributions of the
visual and textual features. Experimental results demonstrate that our DYSCO
surpasses training-free state-of-the-art models and is competitive with
training-based approaches, particularly excelling in rare interactions. Code is
available at https://github.com/francescotonini/dysco.

</details>


### [66] [ERMV: Editing 4D Robotic Multi-view images to enhance embodied agents](https://arxiv.org/abs/2507.17462)
*Chang Nie,Guangming Wang,Zhe Lie,Hesheng Wang*

Main category: cs.CV

TL;DR: ERMV是一种新颖的数据增强框架，通过单帧编辑和机器人状态条件高效编辑多视角4D数据，解决了机器人模仿学习中数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 机器人模仿学习依赖4D多视角序列图像，但数据收集成本高且高质量数据稀缺，限制了VLA模型的泛化和应用。

Method: ERMV通过Epipolar Motion-Aware Attention机制、Sparse Spatio-Temporal模块和反馈干预机制，解决了时空一致性、计算成本和语义完整性等挑战。

Result: 实验表明，ERMV增强的数据显著提升了VLA模型在模拟和真实环境中的鲁棒性和泛化能力。

Conclusion: ERMV为机器人模仿学习提供了一种高效的数据增强解决方案，推动了VLA模型的实际应用。

Abstract: Robot imitation learning relies on 4D multi-view sequential images. However,
the high cost of data collection and the scarcity of high-quality data severely
constrain the generalization and application of embodied intelligence policies
like Vision-Language-Action (VLA) models. Data augmentation is a powerful
strategy to overcome data scarcity, but methods for editing 4D multi-view
sequential images for manipulation tasks are currently lacking. Thus, we
propose ERMV (Editing Robotic Multi-View 4D data), a novel data augmentation
framework that efficiently edits an entire multi-view sequence based on
single-frame editing and robot state conditions. This task presents three core
challenges: (1) maintaining geometric and appearance consistency across dynamic
views and long time horizons; (2) expanding the working window with low
computational costs; and (3) ensuring the semantic integrity of critical
objects like the robot arm. ERMV addresses these challenges through a series of
innovations. First, to ensure spatio-temporal consistency in motion blur, we
introduce a novel Epipolar Motion-Aware Attention (EMA-Attn) mechanism that
learns pixel shift caused by movement before applying geometric constraints.
Second, to maximize the editing working window, ERMV pioneers a Sparse
Spatio-Temporal (STT) module, which decouples the temporal and spatial views
and remodels a single-frame multi-view problem through sparse sampling of the
views to reduce computational demands. Third, to alleviate error accumulation,
we incorporate a feedback intervention Mechanism, which uses a Multimodal Large
Language Model (MLLM) to check editing inconsistencies and request targeted
expert guidance only when necessary. Extensive experiments demonstrate that
ERMV-augmented data significantly boosts the robustness and generalization of
VLA models in both simulated and real-world environments.

</details>


### [67] [Probing Vision-Language Understanding through the Visual Entailment Task: promises and pitfalls](https://arxiv.org/abs/2507.17467)
*Elena Pitta,Tom Kouwenhoven,Tessa Verhoef*

Main category: cs.CV

TL;DR: 该研究探讨了视觉蕴含（VE）任务作为多模态语言模型视觉-语言理解可靠探针的程度，使用LLaMA 3.2 11B Vision模型进行测试。实验表明，三样本推理优于零样本基线，但更多样本会引入噪声。标签顺序和视觉信息缺失对模型预测有显著影响。微调后模型表现优异，但视觉基础性存疑。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估VE任务是否能有效揭示多模态语言模型在视觉-语言理解中的潜力和局限性。

Method: 通过零样本、少样本和微调实验，探究提示设计、样本数量与顺序、视觉信息等因素对VE性能的影响，并采用基于解释的评估方法。

Result: 三样本推理表现最佳，微调后模型在e-SNLI-VE数据集上达到83.3%准确率，解释评估显示其语义解释接近人类水平（BERTScore F1 89.2%）。但视觉基础性存疑。

Conclusion: VE任务作为视觉-语言理解的诊断工具具有实用性和局限性，需进一步优化多模态评估方法。

Abstract: This study investigates the extent to which the Visual Entailment (VE) task
serves as a reliable probe of vision-language understanding in multimodal
language models, using the LLaMA 3.2 11B Vision model as a test case. Beyond
reporting performance metrics, we aim to interpret what these results reveal
about the underlying possibilities and limitations of the VE task. We conduct a
series of experiments across zero-shot, few-shot, and fine-tuning settings,
exploring how factors such as prompt design, the number and order of in-context
examples and access to visual information might affect VE performance. To
further probe the reasoning processes of the model, we used explanation-based
evaluations. Results indicate that three-shot inference outperforms the
zero-shot baselines. However, additional examples introduce more noise than
they provide benefits. Additionally, the order of the labels in the prompt is a
critical factor that influences the predictions. In the absence of visual
information, the model has a strong tendency to hallucinate and imagine
content, raising questions about the model's over-reliance on linguistic
priors. Fine-tuning yields strong results, achieving an accuracy of 83.3% on
the e-SNLI-VE dataset and outperforming the state-of-the-art OFA-X model.
Additionally, the explanation evaluation demonstrates that the fine-tuned model
provides semantically meaningful explanations similar to those of humans, with
a BERTScore F1-score of 89.2%. We do, however, find comparable BERTScore
results in experiments with limited vision, questioning the visual grounding of
this task. Overall, our results highlight both the utility and limitations of
VE as a diagnostic task for vision-language understanding and point to
directions for refining multimodal evaluation methods.

</details>


### [68] [SRMambaV2: Biomimetic Attention for Sparse Point Cloud Upsampling in Autonomous Driving](https://arxiv.org/abs/2507.17479)
*Chuang Chen,Xiaolin Qin,Jing Hu,Wenyi Ge*

Main category: cs.CV

TL;DR: SRMambaV2是一种新型稀疏点云上采样方法，通过仿生2D选择性扫描自注意力机制和双分支网络架构提升远距离稀疏区域的上采样精度。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云在自动驾驶场景中稀疏且结构复杂，现有方法将3D场景转为2D图像超分辨率任务，但稀疏和模糊的特征表示难以准确重建复杂空间拓扑。

Method: 提出2D选择性扫描自注意力机制（2DSSA）和双分支网络架构，并引入渐进自适应损失函数（PAL）优化细节重建。

Result: 实验表明SRMambaV2在定性和定量评估中均表现优异。

Conclusion: SRMambaV2在汽车稀疏点云上采样任务中具有高效性和实用价值。

Abstract: Upsampling LiDAR point clouds in autonomous driving scenarios remains a
significant challenge due to the inherent sparsity and complex 3D structures of
the data. Recent studies have attempted to address this problem by converting
the complex 3D spatial scenes into 2D image super-resolution tasks. However,
due to the sparse and blurry feature representation of range images, accurately
reconstructing detailed and complex spatial topologies remains a major
difficulty. To tackle this, we propose a novel sparse point cloud upsampling
method named SRMambaV2, which enhances the upsampling accuracy in long-range
sparse regions while preserving the overall geometric reconstruction quality.
Specifically, inspired by human driver visual perception, we design a
biomimetic 2D selective scanning self-attention (2DSSA) mechanism to model the
feature distribution in distant sparse areas. Meanwhile, we introduce a
dual-branch network architecture to enhance the representation of sparse
features. In addition, we introduce a progressive adaptive loss (PAL) function
to further refine the reconstruction of fine-grained details during the
upsampling process. Experimental results demonstrate that SRMambaV2 achieves
superior performance in both qualitative and quantitative evaluations,
highlighting its effectiveness and practical value in automotive sparse point
cloud upsampling tasks.

</details>


### [69] [Unsupervised anomaly detection using Bayesian flow networks: application to brain FDG PET in the context of Alzheimer's disease](https://arxiv.org/abs/2507.17486)
*Hugues Roy,Reuben Dorent,Ninon Burgos*

Main category: cs.CV

TL;DR: AnoBFN是一种基于贝叶斯流网络的无监督异常检测方法，用于神经影像中的异常检测，尤其在阿尔茨海默病相关任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 无监督异常检测在神经影像中至关重要，但现有方法如VAEs、GANs和扩散模型存在局限性。贝叶斯流网络（BFNs）尚未应用于医学影像或异常检测，因此探索其潜力。

Method: 提出AnoBFN，扩展BFNs以处理高噪声条件下的条件图像生成，并通过递归反馈保留受试者特异性。

Result: 在FDG PET图像中，AnoBFN在阿尔茨海默病异常检测任务中优于其他先进方法（如beta-VAE、f-AnoGAN、AnoDDPM），降低误报率。

Conclusion: AnoBFN展示了贝叶斯流网络在医学影像异常检测中的有效性，为未来研究提供了新方向。

Abstract: Unsupervised anomaly detection (UAD) plays a crucial role in neuroimaging for
identifying deviations from healthy subject data and thus facilitating the
diagnosis of neurological disorders. In this work, we focus on Bayesian flow
networks (BFNs), a novel class of generative models, which have not yet been
applied to medical imaging or anomaly detection. BFNs combine the strength of
diffusion frameworks and Bayesian inference. We introduce AnoBFN, an extension
of BFNs for UAD, designed to: i) perform conditional image generation under
high levels of spatially correlated noise, and ii) preserve subject specificity
by incorporating a recursive feedback from the input image throughout the
generative process. We evaluate AnoBFN on the challenging task of Alzheimer's
disease-related anomaly detection in FDG PET images. Our approach outperforms
other state-of-the-art methods based on VAEs (beta-VAE), GANs (f-AnoGAN), and
diffusion models (AnoDDPM), demonstrating its effectiveness at detecting
anomalies while reducing false positive rates.

</details>


### [70] [DFDNet: Dynamic Frequency-Guided De-Flare Network](https://arxiv.org/abs/2507.17489)
*Minglong Xue,Aoxiang Ning,Shivakumara Palaiahnakote,Mingliang Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种动态频率引导的去光晕网络（DFDNet），通过频域解耦内容与光晕信息，有效去除大范围光晕伪影。


<details>
  <summary>Details</summary>
Motivation: 夜间摄影中的强光源常导致图像中出现光晕，降低视觉质量并影响下游任务性能，现有方法难以处理大范围光晕和光源附近的结构损坏。

Method: DFDNet包含全局动态频域引导模块（GDFG）和局部细节引导模块（LDGM），分别通过频域特征优化和对比学习策略分离光晕与内容信息。

Result: 实验表明，DFDNet在去除光晕和恢复图像细节方面优于现有方法。

Conclusion: DFDNet通过频域引导和局部细节优化，有效解决了大范围光晕去除和图像修复问题。

Abstract: Strong light sources in nighttime photography frequently produce flares in
images, significantly degrading visual quality and impacting the performance of
downstream tasks. While some progress has been made, existing methods continue
to struggle with removing large-scale flare artifacts and repairing structural
damage in regions near the light source. We observe that these challenging
flare artifacts exhibit more significant discrepancies from the reference
images in the frequency domain compared to the spatial domain. Therefore, this
paper presents a novel dynamic frequency-guided deflare network (DFDNet) that
decouples content information from flare artifacts in the frequency domain,
effectively removing large-scale flare artifacts. Specifically, DFDNet consists
mainly of a global dynamic frequency-domain guidance (GDFG) module and a local
detail guidance module (LDGM). The GDFG module guides the network to perceive
the frequency characteristics of flare artifacts by dynamically optimizing
global frequency domain features, effectively separating flare information from
content information. Additionally, we design an LDGM via a contrastive learning
strategy that aligns the local features of the light source with the reference
image, reduces local detail damage from flare removal, and improves
fine-grained image restoration. The experimental results demonstrate that the
proposed method outperforms existing state-of-the-art methods in terms of
performance. The code is available at
\href{https://github.com/AXNing/DFDNet}{https://github.com/AXNing/DFDNet}.

</details>


### [71] [Illicit object detection in X-ray imaging using deep learning techniques: A comparative evaluation](https://arxiv.org/abs/2507.17508)
*Jorgen Cani,Christos Diou,Spyridon Evangelatos,Vasileios Argyriou,Panagiotis Radoglou-Grammatikis,Panagiotis Sarigiannidis,Iraklis Varlamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 对基于深度学习的X射线违禁品检测方法进行了系统性比较评估，涵盖多个数据集和检测方案，并公开了代码和模型权重。


<details>
  <summary>Details</summary>
Motivation: X射线自动检测在公共安全中至关重要，但存在遮挡、物品多样性、设备差异和数据不足等问题，且现有研究评估不完整。

Method: 开发了一个综合评估框架，包括六个公共数据集、十种最先进的检测方案及多种性能指标。

Result: 分析了检测方案的整体表现、对象级检测性能、数据集特异性及时间效率与计算复杂度。

Conclusion: 研究提供了关键观察和见解，并公开了代码和模型权重以支持可重复性。

Abstract: Automated X-ray inspection is crucial for efficient and unobtrusive security
screening in various public settings. However, challenges such as object
occlusion, variations in the physical properties of items, diversity in X-ray
scanning devices, and limited training data hinder accurate and reliable
detection of illicit items. Despite the large body of research in the field,
reported experimental evaluations are often incomplete, with frequently
conflicting outcomes. To shed light on the research landscape and facilitate
further research, a systematic, detailed, and thorough comparative evaluation
of recent Deep Learning (DL)-based methods for X-ray object detection is
conducted. For this, a comprehensive evaluation framework is developed,
composed of: a) Six recent, large-scale, and widely used public datasets for
X-ray illicit item detection (OPIXray, CLCXray, SIXray, EDS, HiXray, and
PIDray), b) Ten different state-of-the-art object detection schemes covering
all main categories in the literature, including generic Convolutional Neural
Network (CNN), custom CNN, generic transformer, and hybrid CNN-transformer
architectures, and c) Various detection (mAP50 and mAP50:95) and
time/computational-complexity (inference time (ms), parameter size (M), and
computational load (GFLOPS)) metrics. A thorough analysis of the results leads
to critical observations and insights, emphasizing key aspects such as: a)
Overall behavior of the object detection schemes, b) Object-level detection
performance, c) Dataset-specific observations, and d) Time efficiency and
computational complexity analysis. To support reproducibility of the reported
experimental results, the evaluation code and model weights are made publicly
available at https://github.com/jgenc/xray-comparative-evaluation.

</details>


### [72] [Accelerating Parallel Diffusion Model Serving with Residual Compression](https://arxiv.org/abs/2507.17511)
*Jiajun Luo,Yicheng Xiao,Jianru Xu,Yangxiu You,Rongwei Lu,Chen Tang,Jingyan Jiang,Zhi Wang*

Main category: cs.CV

TL;DR: CompactFusion是一种压缩框架，通过减少并行扩散模型推理中的通信开销，显著提升效率和质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在实时部署时需要多加速器并行，但并行推理中的大激活交换导致通信开销大，限制了效率和可扩展性。

Method: 提出Residual Compression方法，仅传输压缩的残差（激活差异），并结合轻量级误差反馈防止误差累积。

Result: 在4xL20上实现3.0倍加速，同时显著提高生成质量；在慢速网络上支持序列并行，实现6.7倍加速。

Conclusion: CompactFusion为并行扩散推理提供了新范式，广泛适用于各种扩散模型和并行设置，且易于集成。

Abstract: Diffusion models produce realistic images and videos but require substantial
computational resources, necessitating multi-accelerator parallelism for
real-time deployment. However, parallel inference introduces significant
communication overhead from exchanging large activations between devices,
limiting efficiency and scalability. We present CompactFusion, a compression
framework that significantly reduces communication while preserving generation
quality. Our key observation is that diffusion activations exhibit strong
temporal redundancy-adjacent steps produce highly similar activations,
saturating bandwidth with near-duplicate data carrying little new information.
To address this inefficiency, we seek a more compact representation that
encodes only the essential information. CompactFusion achieves this via
Residual Compression that transmits only compressed residuals (step-wise
activation differences). Based on empirical analysis and theoretical
justification, we show that it effectively removes redundant data, enabling
substantial data reduction while maintaining high fidelity. We also integrate
lightweight error feedback to prevent error accumulation. CompactFusion
establishes a new paradigm for parallel diffusion inference, delivering lower
latency and significantly higher generation quality than prior methods. On
4xL20, it achieves 3.0x speedup while greatly improving fidelity. It also
uniquely supports communication-heavy strategies like sequence parallelism on
slow networks, achieving 6.7x speedup over prior overlap-based method.
CompactFusion applies broadly across diffusion models and parallel settings,
and integrates easily without requiring pipeline rework. Portable
implementation demonstrated on xDiT is publicly available at
https://github.com/Cobalt-27/CompactFusion

</details>


### [73] [URPO: A Unified Reward & Policy Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.17515)
*Songshuo Lu,Hua Wang,Zhi Chen,Yaohua Tang*

Main category: cs.CV

TL;DR: URPO框架将指令遵循和奖励建模统一在一个模型和训练阶段中，显著提升了性能并简化了流程。


<details>
  <summary>Details</summary>
Motivation: 传统大规模对齐流程中，策略模型和奖励模型分离导致资源消耗大且性能受限。

Method: 提出URPO框架，通过Group-Relative Policy Optimization (GRPO)循环统一优化指令遵循和奖励建模。

Result: 在Qwen2.5-7B模型上，URPO显著提升了指令遵循和推理能力，并生成了更优的内部评估器。

Conclusion: URPO通过简化流程和促进生成与评估的协同进化，为语言模型对齐提供了更高效和有效的路径。

Abstract: Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.

</details>


### [74] [STQE: Spatial-Temporal Quality Enhancement for G-PCC Compressed Dynamic Point Clouds](https://arxiv.org/abs/2507.17522)
*Tian Guo,Hui Yuan,Xiaolong Mao,Shiqi Jiang,Raouf Hamzaoui,Sam Kwong*

Main category: cs.CV

TL;DR: STQE网络通过利用时空相关性提升G-PCC压缩动态点云的视觉质量，包括运动补偿、时间注意力、空间特征聚合和联合损失函数，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少关注压缩动态点云的质量增强，尤其是时空相关性的有效利用未被充分探索。

Method: 提出STQE网络，包含基于重着色的运动补偿模块、通道感知时间注意力模块、高斯引导邻域特征聚合模块和基于皮尔逊相关系数的联合损失函数。

Result: 在G-PCC测试模型中，STQE在delta PSNR上提升了0.855 dB、0.682 dB和0.828 dB，BD-rate分别降低了25.2%、31.6%和32.5%。

Conclusion: STQE通过多模块协同工作，显著提升了压缩动态点云的视觉质量，填补了现有研究的空白。

Abstract: Very few studies have addressed quality enhancement for compressed dynamic
point clouds. In particular, the effective exploitation of spatial-temporal
correlations between point cloud frames remains largely unexplored. Addressing
this gap, we propose a spatial-temporal attribute quality enhancement (STQE)
network that exploits both spatial and temporal correlations to improve the
visual quality of G-PCC compressed dynamic point clouds. Our contributions
include a recoloring-based motion compensation module that remaps reference
attribute information to the current frame geometry to achieve precise
inter-frame geometric alignment, a channel-aware temporal attention module that
dynamically highlights relevant regions across bidirectional reference frames,
a Gaussian-guided neighborhood feature aggregation module that efficiently
captures spatial dependencies between geometry and color attributes, and a
joint loss function based on the Pearson correlation coefficient, designed to
alleviate over-smoothing effects typical of point-wise mean squared error
optimization. When applied to the latest G-PCC test model, STQE achieved
improvements of 0.855 dB, 0.682 dB, and 0.828 dB in delta PSNR, with
Bj{\o}ntegaard Delta rate (BD-rate) reductions of -25.2%, -31.6%, and -32.5%
for the Luma, Cb, and Cr components, respectively.

</details>


### [75] [Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding](https://arxiv.org/abs/2507.17533)
*Liwen Liu,Weidong Yang,Lipeng Ma,Ben Fei*

Main category: cs.CV

TL;DR: MMPT是一个多模态多任务预训练框架，通过三个预训练任务增强点云理解，无需3D标注，并在下游任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖单一预训练任务，限制了模型性能，特别是在复杂多样领域。

Method: 设计了三个预训练任务：Token级重建（TLR）、Point级重建（PLR）和多模态对比学习（MCL）。

Result: 在判别和生成任务中优于现有方法。

Conclusion: MMPT通过多任务学习提升了点云理解的性能，具有可扩展性和通用性。

Abstract: Recent advances in multi-modal pre-training methods have shown promising
effectiveness in learning 3D representations by aligning multi-modal features
between 3D shapes and their corresponding 2D counterparts. However, existing
multi-modal pre-training frameworks primarily rely on a single pre-training
task to gather multi-modal data in 3D applications. This limitation prevents
the models from obtaining the abundant information provided by other relevant
tasks, which can hinder their performance in downstream tasks, particularly in
complex and diverse domains. In order to tackle this issue, we propose MMPT, a
Multi-modal Multi-task Pre-training framework designed to enhance point cloud
understanding. Specifically, three pre-training tasks are devised: (i)
Token-level reconstruction (TLR) aims to recover masked point tokens, endowing
the model with representative learning abilities. (ii) Point-level
reconstruction (PLR) is integrated to predict the masked point positions
directly, and the reconstructed point cloud can be considered as a transformed
point cloud used in the subsequent task. (iii) Multi-modal contrastive learning
(MCL) combines feature correspondences within and across modalities, thus
assembling a rich learning signal from both 3D point cloud and 2D image
modalities in a self-supervised manner. Moreover, this framework operates
without requiring any 3D annotations, making it scalable for use with large
datasets. The trained encoder can be effectively transferred to various
downstream tasks. To demonstrate its effectiveness, we evaluated its
performance compared to state-of-the-art methods in various discriminant and
generative applications under widely-used benchmarks.

</details>


### [76] [An h-space Based Adversarial Attack for Protection Against Few-shot Personalization](https://arxiv.org/abs/2507.17554)
*Xide Xu,Sandesh Kamath,Muhammad Atif Butt,Bogdan Raducanu*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型潜在空间（h-space）的新型抗定制方法HAAD及其高效变体HAAD-KV，通过对抗攻击有效干扰图像生成过程，保护隐私内容。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在少量样本下生成定制图像的能力引发了隐私问题，尤其是未经授权的私人内容修改。本文旨在开发基于对抗攻击的保护机制。

Method: 提出HAAD方法，利用h-space中的对抗攻击生成扰动；进一步提出HAAD-KV，仅基于h-space的KV参数构建扰动，计算成本更低。

Result: HAAD和HAAD-KV在干扰图像生成方面优于现有对抗攻击方法，且HAAD-KV计算效率更高。

Conclusion: 基于h-space的对抗攻击方法（HAAD和HAAD-KV）能有效保护隐私内容，且计算效率高，优于现有技术。

Abstract: The versatility of diffusion models in generating customized images from few
samples raises significant privacy concerns, particularly regarding
unauthorized modifications of private content. This concerning issue has
renewed the efforts in developing protection mechanisms based on adversarial
attacks, which generate effective perturbations to poison diffusion models. Our
work is motivated by the observation that these models exhibit a high degree of
abstraction within their semantic latent space (`h-space'), which encodes
critical high-level features for generating coherent and meaningful content. In
this paper, we propose a novel anti-customization approach, called HAAD
(h-space based Adversarial Attack for Diffusion models), that leverages
adversarial attacks to craft perturbations based on the h-space that can
efficiently degrade the image generation process. Building upon HAAD, we
further introduce a more efficient variant, HAAD-KV, that constructs
perturbations solely based on the KV parameters of the h-space. This strategy
offers a stronger protection, that is computationally less expensive. Despite
their simplicity, our methods outperform state-of-the-art adversarial attacks,
highlighting their effectiveness.

</details>


### [77] [Boosting Ray Search Procedure of Hard-label Attacks with Transfer-based Priors](https://arxiv.org/abs/2507.17577)
*Chen Ma,Xinjie Xu,Shuyu Cheng,Qi Xuan*

Main category: cs.CV

TL;DR: 本文提出了一种基于先验引导的方法，用于提高硬标签黑盒对抗攻击中的射线搜索效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 硬标签攻击仅能获取预测的top-1标签，现有方法通过梯度估计减少查询次数，但梯度估计质量有待提升。

Method: 利用从替代模型转移的先验，通过近似真实梯度在由这些先验和随机方向张成的子空间上的投影，高效估计梯度。

Result: 在ImageNet和CIFAR-10数据集上的实验表明，该方法在查询效率上显著优于11种现有方法。

Conclusion: 通过理论分析和实验验证，先验引导的梯度估计方法显著提升了硬标签攻击的效率。

Abstract: One of the most practical and challenging types of black-box adversarial
attacks is the hard-label attack, where only the top-1 predicted label is
available. One effective approach is to search for the optimal ray direction
from the benign image that minimizes the $\ell_p$-norm distance to the
adversarial region. The unique advantage of this approach is that it transforms
the hard-label attack into a continuous optimization problem. The objective
function value is the ray's radius, which can be obtained via binary search at
a high query cost. Existing methods use a "sign trick" in gradient estimation
to reduce the number of queries. In this paper, we theoretically analyze the
quality of this gradient estimation and propose a novel prior-guided approach
to improve ray search efficiency both theoretically and empirically.
Specifically, we utilize the transfer-based priors from surrogate models, and
our gradient estimators appropriately integrate them by approximating the
projection of the true gradient onto the subspace spanned by these priors and
random directions, in a query-efficient manner. We theoretically derive the
expected cosine similarities between the obtained gradient estimators and the
true gradient, and demonstrate the improvement achieved by incorporating
priors. Extensive experiments on the ImageNet and CIFAR-10 datasets show that
our approach significantly outperforms 11 state-of-the-art methods in terms of
query efficiency.

</details>


### [78] [From Scan to Action: Leveraging Realistic Scans for Embodied Scene Understanding](https://arxiv.org/abs/2507.17585)
*Anna-Maria Halacheva,Jan-Nico Zaech,Sombit Dey,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: 提出了一种利用真实世界3D场景扫描数据的方法，通过统一标注格式（USD）解决数据量大、标注格式多样和工具兼容性问题，并在LLM场景编辑和机器人仿真中验证了效果。


<details>
  <summary>Details</summary>
Motivation: 真实世界3D场景扫描数据具有高真实性和泛化能力，但数据量大、标注格式多样和工具兼容性问题限制了其应用。

Method: 提出基于USD的统一标注集成方法，并针对不同应用设计特定USD变体。

Result: 在LLM场景编辑中达到80%成功率，机器人仿真中政策学习成功率为87%。

Conclusion: 该方法有效解决了真实世界扫描数据的利用问题，并在实际应用中验证了其效果。

Abstract: Real-world 3D scene-level scans offer realism and can enable better
real-world generalizability for downstream applications. However, challenges
such as data volume, diverse annotation formats, and tool compatibility limit
their use. This paper demonstrates a methodology to effectively leverage these
scans and their annotations. We propose a unified annotation integration using
USD, with application-specific USD flavors. We identify challenges in utilizing
holistic real-world scan datasets and present mitigation strategies. The
efficacy of our approach is demonstrated through two downstream applications:
LLM-based scene editing, enabling effective LLM understanding and adaptation of
the data (80% success), and robotic simulation, achieving an 87% success rate
in policy learning.

</details>


### [79] [Dual-branch Prompting for Multimodal Machine Translation](https://arxiv.org/abs/2507.17588)
*Jie Wang,Zhendong Yang,Liansong Zong,Xiaobo Zhang,Dexian Wang,Ji Zhang*

Main category: cs.CV

TL;DR: D2P-MMT是一种基于扩散模型的双分支提示框架，用于鲁棒的视觉引导翻译，仅需源文本和重建图像，避免了无关视觉噪声的影响。


<details>
  <summary>Details</summary>
Motivation: 现有MMT方法在推理时依赖成对图像-文本输入，且对无关视觉噪声敏感，限制了其鲁棒性和实际应用。

Method: 提出D2P-MMT框架，利用预训练扩散模型生成重建图像，通过双分支提示策略学习真实和重建图像，并引入分布对齐损失减少模态差异。

Result: 在Multi30K数据集上，D2P-MMT优于现有最先进方法。

Conclusion: D2P-MMT通过扩散模型和双分支策略有效提升了翻译性能和鲁棒性。

Abstract: Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.

</details>


### [80] [RemixFusion: Residual-based Mixed Representation for Large-scale Online RGB-D Reconstruction](https://arxiv.org/abs/2507.17594)
*Yuqing Lan,Chenyang Zhu,Shuaifeng Zhi,Jiazhao Zhang,Zhoufeng Wang,Renjiao Yi,Yijie Wang,Kai Xu*

Main category: cs.CV

TL;DR: RemixFusion提出了一种基于残差的混合表示方法，用于高质量、大规模的在线RGB-D重建，结合显式和隐式表示，提升了细节重建和相机姿态估计的精度。


<details>
  <summary>Details</summary>
Motivation: 传统神经隐式表示在在线密集重建中存在细节不足和学习耗时的问题，限制了其在大规模场景中的应用。

Method: 提出了一种混合表示方法，结合显式TSDF网格和隐式神经模块生成残差细节，并通过自适应梯度放大和局部移动体积优化姿态估计。

Result: 实验表明，该方法在大型场景的映射和跟踪精度上超越了现有技术。

Conclusion: RemixFusion通过混合表示和优化策略，实现了高效、高质量的在线重建，为大规模场景应用提供了新思路。

Abstract: The introduction of the neural implicit representation has notably propelled
the advancement of online dense reconstruction techniques. Compared to
traditional explicit representations, such as TSDF, it improves the mapping
completeness and memory efficiency. However, the lack of reconstruction details
and the time-consuming learning of neural representations hinder the widespread
application of neural-based methods to large-scale online reconstruction. We
introduce RemixFusion, a novel residual-based mixed representation for scene
reconstruction and camera pose estimation dedicated to high-quality and
large-scale online RGB-D reconstruction. In particular, we propose a
residual-based map representation comprised of an explicit coarse TSDF grid and
an implicit neural module that produces residuals representing fine-grained
details to be added to the coarse grid. Such mixed representation allows for
detail-rich reconstruction with bounded time and memory budget, contrasting
with the overly-smoothed results by the purely implicit representations, thus
paving the way for high-quality camera tracking. Furthermore, we extend the
residual-based representation to handle multi-frame joint pose optimization via
bundle adjustment (BA). In contrast to the existing methods, which optimize
poses directly, we opt to optimize pose changes. Combined with a novel
technique for adaptive gradient amplification, our method attains better
optimization convergence and global optimality. Furthermore, we adopt a local
moving volume to factorize the mixed scene representation with a
divide-and-conquer design to facilitate efficient online learning in our
residual-based framework. Extensive experiments demonstrate that our method
surpasses all state-of-the-art ones, including those based either on explicit
or implicit representations, in terms of the accuracy of both mapping and
tracking on large-scale scenes.

</details>


### [81] [PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.17596)
*Maciej K. Wozniak,Lianhang Liu,Yixi Cai,Patric Jensfelt*

Main category: cs.CV

TL;DR: PRIX是一种仅使用摄像头数据的高效端到端自动驾驶架构，无需LiDAR或显式BEV表示，通过视觉特征提取器和生成规划头直接预测轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端自动驾驶模型依赖LiDAR、计算密集的BEV表示及大模型尺寸的问题，提升在仅配备摄像头的量产车中的实用性。

Method: 提出PRIX架构，结合视觉特征提取器和生成规划头，引入Context-aware Recalibration Transformer（CaRT）增强多级视觉特征。

Result: 在NavSim和nuScenes基准测试中达到SOTA性能，与多模态扩散规划器相当，但推理速度和模型尺寸更高效。

Conclusion: PRIX是一种高效、实用的自动驾驶解决方案，适用于现实部署，代码已开源。

Abstract: While end-to-end autonomous driving models show promising results, their
practical deployment is often hindered by large model sizes, a reliance on
expensive LiDAR sensors and computationally intensive BEV feature
representations. This limits their scalability, especially for mass-market
vehicles equipped only with cameras. To address these challenges, we propose
PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving
architecture operates using only camera data, without explicit BEV
representation and forgoing the need for LiDAR. PRIX leverages a visual feature
extractor coupled with a generative planning head to predict safe trajectories
from raw pixel inputs directly. A core component of our architecture is the
Context-aware Recalibration Transformer (CaRT), a novel module designed to
effectively enhance multi-level visual features for more robust planning. We
demonstrate through comprehensive experiments that PRIX achieves
state-of-the-art performance on the NavSim and nuScenes benchmarks, matching
the capabilities of larger, multimodal diffusion planners while being
significantly more efficient in terms of inference speed and model size, making
it a practical solution for real-world deployment. Our work is open-source and
the code will be at https://maxiuw.github.io/prix.

</details>


### [82] [InvRGB+L: Inverse Rendering of Complex Scenes with Unified Color and LiDAR Reflectance Modeling](https://arxiv.org/abs/2507.17613)
*Xiaoxue Chen,Bhargav Chandaka,Chih-Hao Lin,Ya-Qin Zhang,David Forsyth,Hao Zhao,Shenlong Wang*

Main category: cs.CV

TL;DR: InvRGB+L是一种新型逆渲染模型，通过结合RGB和LiDAR数据重建动态场景，解决了传统方法因可见光干扰导致的材质估计问题。


<details>
  <summary>Details</summary>
Motivation: 传统逆图形方法主要依赖RGB数据，LiDAR仅用于几何信息，导致材质估计不理想。LiDAR的强度值在另一光谱范围内捕获，为可变光照下的材质估计提供了补充线索。

Method: 提出两种创新：(1)基于物理的LiDAR着色模型；(2)RGB-LiDAR材质一致性损失。

Result: 模型能够生成新颖视角的RGB和LiDAR渲染，支持重光照、夜间模拟和动态对象插入，在城市场景逆渲染和LiDAR模拟中超越现有方法。

Conclusion: InvRGB+L通过利用LiDAR强度线索，显著提升了逆渲染的鲁棒性和效果。

Abstract: We present InvRGB+L, a novel inverse rendering model that reconstructs large,
relightable, and dynamic scenes from a single RGB+LiDAR sequence. Conventional
inverse graphics methods rely primarily on RGB observations and use LiDAR
mainly for geometric information, often resulting in suboptimal material
estimates due to visible light interference. We find that LiDAR's intensity
values-captured with active illumination in a different spectral range-offer
complementary cues for robust material estimation under variable lighting.
Inspired by this, InvRGB+L leverages LiDAR intensity cues to overcome
challenges inherent in RGB-centric inverse graphics through two key
innovations: (1) a novel physics-based LiDAR shading model and (2) RGB-LiDAR
material consistency losses. The model produces novel-view RGB and LiDAR
renderings of urban and indoor scenes and supports relighting, night
simulations, and dynamic object insertions, achieving results that surpass
current state-of-the-art methods in both scene-level urban inverse rendering
and LiDAR simulation.

</details>


### [83] [Vision Transformer attention alignment with human visual perception in aesthetic object evaluation](https://arxiv.org/abs/2507.17616)
*Miguel Carrasco,César González-Martín,José Aranda,Luis Oliveros*

Main category: cs.CV

TL;DR: 研究探讨了人类视觉注意力与ViT注意力机制在评估手工艺品时的相关性，发现某些ViT注意力头能近似人类视觉行为。


<details>
  <summary>Details</summary>
Motivation: 探索ViT注意力机制与人类视觉注意力模式在美学评估中的一致性。

Method: 通过眼动实验和ViT模型（DINO）分析手工艺品，比较人类和ViT的注意力分布。

Result: ViT的注意力头#12与人类视觉模式最一致，而#7和#9差异最大。ViT表现出更全局的注意力模式。

Conclusion: ViT注意力机制在产品设计和美学评估中有潜在应用，但与人脑注意力策略存在根本差异。

Abstract: Visual attention mechanisms play a crucial role in human perception and
aesthetic evaluation. Recent advances in Vision Transformers (ViTs) have
demonstrated remarkable capabilities in computer vision tasks, yet their
alignment with human visual attention patterns remains underexplored,
particularly in aesthetic contexts. This study investigates the correlation
between human visual attention and ViT attention mechanisms when evaluating
handcrafted objects. We conducted an eye-tracking experiment with 30
participants (9 female, 21 male, mean age 24.6 years) who viewed 20 artisanal
objects comprising basketry bags and ginger jars. Using a Pupil Labs
eye-tracker, we recorded gaze patterns and generated heat maps representing
human visual attention. Simultaneously, we analyzed the same objects using a
pre-trained ViT model with DINO (Self-DIstillation with NO Labels), extracting
attention maps from each of the 12 attention heads. We compared human and ViT
attention distributions using Kullback-Leibler divergence across varying
Gaussian parameters (sigma=0.1 to 3.0). Statistical analysis revealed optimal
correlation at sigma=2.4 +-0.03, with attention head #12 showing the strongest
alignment with human visual patterns. Significant differences were found
between attention heads, with heads #7 and #9 demonstrating the greatest
divergence from human attention (p< 0.05, Tukey HSD test). Results indicate
that while ViTs exhibit more global attention patterns compared to human focal
attention, certain attention heads can approximate human visual behavior,
particularly for specific object features like buckles in basketry items. These
findings suggest potential applications of ViT attention mechanisms in product
design and aesthetic evaluation, while highlighting fundamental differences in
attention strategies between human perception and current AI models.

</details>


### [84] [Reusing Attention for One-stage Lane Topology Understanding](https://arxiv.org/abs/2507.17617)
*Yang Li,Zongzheng Zhang,Xuchong Qiu,Xinrun Li,Ziming Liu,Leichen Wang,Ruikai Li,Zhenxin Zhu,Huan-ang Gao,Xiaojian Lin,Zhiyong Cui,Hang Zhao,Hao Zhao*

Main category: cs.CV

TL;DR: 提出一种单阶段架构，同时预测交通元素、车道中心线和拓扑关系，提升自动驾驶中车道拓扑理解的准确性和推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有两阶段方法因错误传播和计算开销大而效率低下，需改进。

Method: 利用Transformer解码器中的中间注意力资源，避免额外计算开销的图网络，并首次实现从SD地图模型到非SD地图模型的知识蒸馏。

Result: 在OpenLane-V2数据集上，方法在车道检测、交通元素识别和拓扑推理方面优于基线方法。

Conclusion: 单阶段架构显著提升了自动驾驶中车道拓扑理解的性能，且无需SD地图即可实现优越表现。

Abstract: Understanding lane toplogy relationships accurately is critical for safe
autonomous driving. However, existing two-stage methods suffer from
inefficiencies due to error propagations and increased computational overheads.
To address these challenges, we propose a one-stage architecture that
simultaneously predicts traffic elements, lane centerlines and topology
relationship, improving both the accuracy and inference speed of lane topology
understanding for autonomous driving. Our key innovation lies in reusing
intermediate attention resources within distinct transformer decoders. This
approach effectively leverages the inherent relational knowledge within the
element detection module to enable the modeling of topology relationships among
traffic elements and lanes without requiring additional computationally
expensive graph networks. Furthermore, we are the first to demonstrate that
knowledge can be distilled from models that utilize standard definition (SD)
maps to those operates without using SD maps, enabling superior performance
even in the absence of SD maps. Extensive experiments on the OpenLane-V2
dataset show that our approach outperforms baseline methods in both accuracy
and efficiency, achieving superior results in lane detection, traffic element
identification, and topology reasoning. Our code is available at
https://github.com/Yang-Li-2000/one-stage.git.

</details>


### [85] [The Early Bird Identifies the Worm: You Can't Beat a Head Start in Long-Term Body Re-ID (ECHO-BID)](https://arxiv.org/abs/2507.17640)
*Thomas M. Metz,Matthew Q. Hill,Alice J. O'Toole*

Main category: cs.CV

TL;DR: ECHO-BID是一种基于EVA-02 Large骨干网络的长时重识别模型，在衣物变化和遮挡场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决无约束环境中因距离、视角、成像条件和衣物变化导致的人员识别挑战。

Method: 使用EVA-02 Large骨干网络，结合掩码图像建模预训练和迁移学习，对比了不同架构、规模和预训练规模的模型。

Result: 在衣物变化和遮挡场景下，ECHO-BID显著优于其他方法，取得了最先进的性能。

Conclusion: 选择合适的预训练骨干架构和迁移学习协议可以显著提升长时重识别性能。

Abstract: Person identification in unconstrained viewing environments presents
significant challenges due to variations in distance, viewpoint, imaging
conditions, and clothing. We introduce $\textbf{E}$va $\textbf{C}$lothes-Change
from $\textbf{H}$idden $\textbf{O}$bjects - $\textbf{B}$ody
$\textbf{ID}$entification (ECHO-BID), a class of long-term re-id models built
on object-pretrained EVA-02 Large backbones. We compare ECHO-BID to 9 other
models that vary systematically in backbone architecture, model size, scale of
object classification pretraining, and transfer learning protocol. Models were
evaluated on benchmark datasets across constrained, unconstrained, and occluded
settings. ECHO-BID, with transfer learning on the most challenging
clothes-change data, achieved state-of-the-art results on long-term re-id --
substantially outperforming other methods. ECHO-BID also surpassed other
methods by a wide margin in occluded viewing scenarios. A combination of
increased model size and Masked Image Modeling during pretraining underlie
ECHO-BID's strong performance on long-term re-id. Notably, a smaller, but more
challenging transfer learning dataset, generalized better across datasets than
a larger, less challenging one. However, the larger dataset with an additional
fine-tuning step proved best on the most difficult data. Selecting the correct
pretrained backbone architecture and transfer learning protocols can drive
substantial gains in long-term re-id performance.

</details>


### [86] [CNS-Bench: Benchmarking Image Classifier Robustness Under Continuous Nuisance Shifts](https://arxiv.org/abs/2507.17651)
*Olaf Dünkel,Artur Jesslen,Jiahao Xie,Christian Theobalt,Christian Rupprecht,Adam Kortylewski*

Main category: cs.CV

TL;DR: CNS-Bench是一个连续干扰偏移基准，用于量化图像分类器在连续和真实生成干扰偏移下的OOD鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 评估计算机视觉模型在真实世界中的性能时，简单的合成干扰往往无法捕捉真实世界的干扰变化。

Method: 通过LoRA适配器在扩散模型中生成连续干扰偏移，并提出过滤机制以解决失败案例。

Result: 对40多个分类器进行了大规模研究，发现模型排名会因干扰偏移类型和规模而变化。

Conclusion: 连续干扰偏移评估能更细致地理解模型鲁棒性，并识别模型失败点。

Abstract: An important challenge when using computer vision models in the real world is
to evaluate their performance in potential out-of-distribution (OOD) scenarios.
While simple synthetic corruptions are commonly applied to test OOD robustness,
they often fail to capture nuisance shifts that occur in the real world.
Recently, diffusion models have been applied to generate realistic images for
benchmarking, but they are restricted to binary nuisance shifts. In this work,
we introduce CNS-Bench, a Continuous Nuisance Shift Benchmark to quantify OOD
robustness of image classifiers for continuous and realistic generative
nuisance shifts. CNS-Bench allows generating a wide range of individual
nuisance shifts in continuous severities by applying LoRA adapters to diffusion
models. To address failure cases, we propose a filtering mechanism that
outperforms previous methods, thereby enabling reliable benchmarking with
generative models. With the proposed benchmark, we perform a large-scale study
to evaluate the robustness of more than 40 classifiers under various nuisance
shifts. Through carefully designed comparisons and analyses, we find that model
rankings can change for varying shifts and shift scales, which cannot be
captured when applying common binary shifts. Additionally, we show that
evaluating the model performance on a continuous scale allows the
identification of model failure points, providing a more nuanced understanding
of model robustness. Project page including code and data:
https://genintel.github.io/CNS.

</details>


### [87] [Attention (as Discrete-Time Markov) Chains](https://arxiv.org/abs/2507.17657)
*Yotam Erel,Olaf Dünkel,Rishabh Dabral,Vladislav Golyanik,Christian Theobalt,Amit H. Bermano*

Main category: cs.CV

TL;DR: 将注意力矩阵解释为离散时间马尔可夫链，提出间接注意力传播，通过轻量级工具实现零样本分割，并定义TokenRank衡量全局token重要性。


<details>
  <summary>Details</summary>
Motivation: 统一解释注意力矩阵的常见操作（如选择、求和、平均），并扩展为间接注意力传播，超越仅建模直接效应的研究。

Method: 将注意力矩阵视为马尔可夫链，通过矩阵乘法和特征分析计算元稳定状态及其普遍性。

Result: 在零样本分割任务中达到SOTA，TokenRank在无条件图像生成中带来改进。

Conclusion: 为现代视觉Transformer中token的注意力机制提供了新视角。

Abstract: We introduce a new interpretation of the attention matrix as a discrete-time
Markov chain. Our interpretation sheds light on common operations involving
attention scores such as selection, summation, and averaging in a unified
framework. It further extends them by considering indirect attention,
propagated through the Markov chain, as opposed to previous studies that only
model immediate effects. Our main observation is that tokens corresponding to
semantically similar regions form a set of metastable states, where the
attention clusters, while noisy attention scores tend to disperse. Metastable
states and their prevalence can be easily computed through simple matrix
multiplication and eigenanalysis, respectively. Using these lightweight tools,
we demonstrate state-of-the-art zero-shot segmentation. Lastly, we define
TokenRank -- the steady state vector of the Markov chain, which measures global
token importance. We demonstrate that using it brings improvements in
unconditional image generation. We believe our framework offers a fresh view of
how tokens are being attended in modern visual transformers.

</details>


### [88] [See the Forest and the Trees: A Synergistic Reasoning Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2507.17659)
*Junjie Wang,Yunhan Tang,Yijie Wang,Zhihao Yuan,Huan Wang,Yangfan He,Bin Li*

Main category: cs.CV

TL;DR: Synergos-VQA提出了一种新颖的协同推理框架，通过融合多维度证据提升KBVQA性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在KBVQA中依赖单一证据，限制了多角度理解能力。

Method: 同时生成并融合三种互补证据流：整体证据、结构证据和因果证据。

Result: 在OK-VQA和A-OKVQA等基准上达到新SOTA，并具备强插件兼容性。

Conclusion: 方法论设计优于单纯模型规模，协同推理显著提升MLLMs性能。

Abstract: Multimodal Large Language Models (MLLMs) have pushed the frontiers of
Knowledge-Based Visual Question Answering (KBVQA), yet their reasoning is
fundamentally bottlenecked by a reliance on uni-dimensional evidence. This
"seeing only the trees, but not the forest" approach prevents robust,
multi-faceted understanding. Inspired by the principle of seeing both the
forest and trees, we propose Synergos-VQA, a novel synergistic reasoning
framework. At its core, Synergos-VQA concurrently generates and fuses three
complementary evidence streams at inference time: (1) Holistic Evidence to
perceive the entire scene (the "forest"), (2) Structural Evidence from a
prototype-driven module to identify key objects (the "trees"), and (3) Causal
Evidence from a counterfactual probe to ensure the reasoning is robustly
grounded. By synergistically fusing this multi-faceted evidence, our framework
achieves a more comprehensive and reliable reasoning process. Extensive
experiments show that Synergos-VQA decisively establishes a new
state-of-the-art on three challenging benchmarks, including OK-VQA and A-OKVQA.
Furthermore, our approach demonstrates strong plug-and-play capabilities,
significantly boosting various open-source MLLMs and proving that superior
methodological design can outperform sheer model scale.

</details>


### [89] [Monocular Semantic Scene Completion via Masked Recurrent Networks](https://arxiv.org/abs/2507.17661)
*Xuzhi Wang,Xinran Wu,Song Wang,Lingdong Kong,Ziping Zhao*

Main category: cs.CV

TL;DR: 提出了一种新颖的两阶段框架MonoMRN，通过分解单目语义场景补全任务为粗补全和掩码循环网络，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有单阶段方法在复杂场景中表现不佳，且受深度估计不准确影响。

Method: 采用两阶段框架，包括粗补全和掩码循环网络（MS-GRU），并提出距离注意力投影减少误差。

Result: 在NYUv2和SemanticKITTI数据集上达到最优性能，且增强了模型鲁棒性。

Conclusion: MonoMRN框架有效支持室内外场景，并通过掩码循环网络提升了模型对干扰的鲁棒性。

Abstract: Monocular Semantic Scene Completion (MSSC) aims to predict the voxel-wise
occupancy and semantic category from a single-view RGB image. Existing methods
adopt a single-stage framework that aims to simultaneously achieve visible
region segmentation and occluded region hallucination, while also being
affected by inaccurate depth estimation. Such methods often achieve suboptimal
performance, especially in complex scenes. We propose a novel two-stage
framework that decomposes MSSC into coarse MSSC followed by the Masked
Recurrent Network. Specifically, we propose the Masked Sparse Gated Recurrent
Unit (MS-GRU) which concentrates on the occupied regions by the proposed mask
updating mechanism, and a sparse GRU design is proposed to reduce the
computation cost. Additionally, we propose the distance attention projection to
reduce projection errors by assigning different attention scores according to
the distance to the observed surface. Experimental results demonstrate that our
proposed unified framework, MonoMRN, effectively supports both indoor and
outdoor scenes and achieves state-of-the-art performance on the NYUv2 and
SemanticKITTI datasets. Furthermore, we conduct robustness analysis under
various disturbances, highlighting the role of the Masked Recurrent Network in
enhancing the model's resilience to such challenges. The source code is
publicly available.

</details>


### [90] [Talk2Event: Grounded Understanding of Dynamic Scenes from Event Cameras](https://arxiv.org/abs/2507.17664)
*Lingdong Kong,Dongyue Lu,Ao Liang,Rong Li,Yuhao Dong,Tianshuai Hu,Lai Xing Ng,Wei Tsang Ooi,Benoit R. Cottereau*

Main category: cs.CV

TL;DR: Talk2Event是首个大规模语言驱动的事件相机对象定位基准，结合EventRefer方法在多模态场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决事件相机异步数据流与人类语言连接的挑战，推动动态环境中的多模态感知。

Method: 提出EventRefer框架，通过Mixture of Event-Attribute Experts动态融合多属性表示。

Result: 在事件、帧及融合场景中均优于现有方法。

Conclusion: 为机器人及自动驾驶中的多模态语言驱动感知奠定基础。

Abstract: Event cameras offer microsecond-level latency and robustness to motion blur,
making them ideal for understanding dynamic environments. Yet, connecting these
asynchronous streams to human language remains an open challenge. We introduce
Talk2Event, the first large-scale benchmark for language-driven object
grounding in event-based perception. Built from real-world driving data, we
provide over 30,000 validated referring expressions, each enriched with four
grounding attributes -- appearance, status, relation to viewer, and relation to
other objects -- bridging spatial, temporal, and relational reasoning. To fully
exploit these cues, we propose EventRefer, an attribute-aware grounding
framework that dynamically fuses multi-attribute representations through a
Mixture of Event-Attribute Experts (MoEE). Our method adapts to different
modalities and scene dynamics, achieving consistent gains over state-of-the-art
baselines in event-only, frame-only, and event-frame fusion settings. We hope
our dataset and approach will establish a foundation for advancing multimodal,
temporally-aware, and language-driven perception in real-world robotics and
autonomy.

</details>


### [91] [Perspective-Invariant 3D Object Detection](https://arxiv.org/abs/2507.17665)
*Ao Liang,Lingdong Kong,Dongyue Lu,Youquan Liu,Jian Fang,Huaici Zhao,Wei Tsang Ooi*

Main category: cs.CV

TL;DR: Pi3DET是一个新的基准数据集，包含来自车辆、四足机器人和无人机的LiDAR数据和3D边界框标注，旨在填补非车辆平台3D目标检测的研究空白。


<details>
  <summary>Details</summary>
Motivation: 现有数据集和方法主要关注车载平台，其他自主平台的研究不足，需要跨平台3D检测的通用解决方案。

Method: 提出跨平台适应框架，通过几何和特征层面的鲁棒对齐实现视角不变的3D检测。

Result: 实验验证了该方法在跨平台任务中的有效性，性能显著优于现有适应方法。

Conclusion: Pi3DET为跨平台3D感知系统的发展提供了基础，数据集和工具已公开。

Abstract: With the rise of robotics, LiDAR-based 3D object detection has garnered
significant attention in both academia and industry. However, existing datasets
and methods predominantly focus on vehicle-mounted platforms, leaving other
autonomous platforms underexplored. To bridge this gap, we introduce Pi3DET,
the first benchmark featuring LiDAR data and 3D bounding box annotations
collected from multiple platforms: vehicle, quadruped, and drone, thereby
facilitating research in 3D object detection for non-vehicle platforms as well
as cross-platform 3D detection. Based on Pi3DET, we propose a novel
cross-platform adaptation framework that transfers knowledge from the
well-studied vehicle platform to other platforms. This framework achieves
perspective-invariant 3D detection through robust alignment at both geometric
and feature levels. Additionally, we establish a benchmark to evaluate the
resilience and robustness of current 3D detectors in cross-platform scenarios,
providing valuable insights for developing adaptive 3D perception systems.
Extensive experiments validate the effectiveness of our approach on challenging
cross-platform tasks, demonstrating substantial gains over existing adaptation
methods. We hope this work paves the way for generalizable and unified 3D
perception systems across diverse and complex environments. Our Pi3DET dataset,
cross-platform benchmark suite, and annotation toolkit have been made publicly
available.

</details>


### [92] [BetterCheck: Towards Safeguarding VLMs for Automotive Perception Systems](https://arxiv.org/abs/2507.17722)
*Malsha Ashani Mahawatta Dona,Beatriz Cabrero-Daniel,Yinan Yu,Christian Berger*

Main category: cs.CV

TL;DR: LLMs和VLMs在理解复杂交通场景方面表现优异，但存在幻觉问题，可能导致自动驾驶系统错误决策。本文评估了3种先进VLMs的性能，并提出检测幻觉的策略BetterCheck。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs和VLMs在自动驾驶感知系统中的潜力，尤其是它们在复杂交通场景中的表现和幻觉问题。

Method: 系统评估3种先进VLMs在Waymo Open Dataset上的性能，并提出幻觉检测策略BetterCheck。

Result: VLMs在图像理解方面表现优异，但仍存在幻觉问题，需要检测策略。

Conclusion: VLMs虽强大，但需结合幻觉检测策略以确保自动驾驶系统的安全性。

Abstract: Large language models (LLMs) are growingly extended to process multimodal
data such as text and video simultaneously. Their remarkable performance in
understanding what is shown in images is surpassing specialized neural networks
(NNs) such as Yolo that is supporting only a well-formed but very limited
vocabulary, ie., objects that they are able to detect. When being
non-restricted, LLMs and in particular state-of-the-art vision language models
(VLMs) show impressive performance to describe even complex traffic situations.
This is making them potentially suitable components for automotive perception
systems to support the understanding of complex traffic situations or edge case
situation. However, LLMs and VLMs are prone to hallucination, which mean to
either potentially not seeing traffic agents such as vulnerable road users who
are present in a situation, or to seeing traffic agents who are not there in
reality. While the latter is unwanted making an ADAS or autonomous driving
systems (ADS) to unnecessarily slow down, the former could lead to disastrous
decisions from an ADS. In our work, we are systematically assessing the
performance of 3 state-of-the-art VLMs on a diverse subset of traffic
situations sampled from the Waymo Open Dataset to support safety guardrails for
capturing such hallucinations in VLM-supported perception systems. We observe
that both, proprietary and open VLMs exhibit remarkable image understanding
capabilities even paying thorough attention to fine details sometimes difficult
to spot for us humans. However, they are also still prone to making up elements
in their descriptions to date requiring hallucination detection strategies such
as BetterCheck that we propose in our work.

</details>


### [93] [A Comprehensive Evaluation Framework for the Study of the Effects of Facial Filters on Face Recognition Accuracy](https://arxiv.org/abs/2507.17729)
*Kagan Ozturk,Louisa Conwill,Jacob Gutierrez,Kevin Bowyer,Walter J. Scheirer*

Main category: cs.CV

TL;DR: 论文提出了一个框架，用于大规模研究面部滤镜对自动人脸识别的影响，并通过案例研究揭示了跨文化差异。


<details>
  <summary>Details</summary>
Motivation: 现有研究仅关注少量特定风格的手选滤镜，无法全面反映社交媒体上广泛使用的滤镜对自动人脸识别的影响。

Method: 提出了一个包含控制数据集、代表性滤镜选择过程和实验评估的框架，并以Instagram、Snapchat、Meitu和Pitu为例进行案例研究。

Result: 揭示了滤镜在跨文化应用中的差异，并展示了如何检测和修复滤镜对人脸嵌入空间的影响以提高识别性能。

Conclusion: 该框架为大规模研究滤镜对自动人脸识别的影响提供了有效工具，并展示了实际应用中的改进潜力。

Abstract: Facial filters are now commonplace for social media users around the world.
Previous work has demonstrated that facial filters can negatively impact
automated face recognition performance. However, these studies focus on small
numbers of hand-picked filters in particular styles. In order to more
effectively incorporate the wide ranges of filters present on various social
media applications, we introduce a framework that allows for larger-scale study
of the impact of facial filters on automated recognition. This framework
includes a controlled dataset of face images, a principled filter selection
process that selects a representative range of filters for experimentation, and
a set of experiments to evaluate the filters' impact on recognition. We
demonstrate our framework with a case study of filters from the American
applications Instagram and Snapchat and the Chinese applications Meitu and Pitu
to uncover cross-cultural differences. Finally, we show how the filtering
effect in a face embedding space can easily be detected and restored to improve
face recognition performance.

</details>


### [94] [Yume: An Interactive World Generation Model](https://arxiv.org/abs/2507.17744)
*Xiaofeng Mao,Shaoheng Lin,Zhen Li,Chuanhao Li,Wenshuo Peng,Tong He,Jiangmiao Pang,Mingmin Chi,Yu Qiao,Kaipeng Zhang*

Main category: cs.CV

TL;DR: Yume是一个通过图像、文本或视频创建交互式、逼真动态世界的项目，支持通过外设或神经信号探索和控制。本文介绍了其预览版本，使用键盘输入从图像生成动态世界，并提出了一个包含四个主要组件的框架。


<details>
  <summary>Details</summary>
Motivation: 目标是实现高保真和交互式的视频世界生成，使用户能够通过简单的方式（如键盘）探索和控制虚拟世界。

Method: 方法包括相机运动量化、视频生成架构（MVDT）、高级采样器（AAM和TTS-SDE）以及模型加速（对抗蒸馏和缓存机制）。

Result: 在高质量数据集上训练后，Yume在多样场景和应用中取得了显著成果。

Conclusion: Yume将持续更新以实现其最终目标，所有数据和代码均已开源。

Abstract: Yume aims to use images, text, or videos to create an interactive, realistic,
and dynamic world, which allows exploration and control using peripheral
devices or neural signals. In this report, we present a preview version of
\method, which creates a dynamic world from an input image and allows
exploration of the world using keyboard actions. To achieve this high-fidelity
and interactive video world generation, we introduce a well-designed framework,
which consists of four main components, including camera motion quantization,
video generation architecture, advanced sampler, and model acceleration. First,
we quantize camera motions for stable training and user-friendly interaction
using keyboard inputs. Then, we introduce the Masked Video Diffusion
Transformer~(MVDT) with a memory module for infinite video generation in an
autoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)
and Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)
are introduced to the sampler for better visual quality and more precise
control. Moreover, we investigate model acceleration by synergistic
optimization of adversarial distillation and caching mechanisms. We use the
high-quality world exploration dataset \sekai to train \method, and it achieves
remarkable results in diverse scenes and applications. All data, codebase, and
model weights are available on https://github.com/stdstu12/YUME. Yume will
update monthly to achieve its original goal. Project page:
https://stdstu12.github.io/YUME-Project/.

</details>


### [95] [Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention](https://arxiv.org/abs/2507.17745)
*Yiwen Chen,Zhihao Li,Yikai Wang,Hu Zhang,Qin Li,Chi Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: Ultra3D提出了一种高效的3D生成框架，通过VecSet表示和局部注意力机制加速稀疏体素建模，同时保持高质量。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏体素表示框架因注意力机制的二次复杂度导致计算效率低下。

Method: 采用VecSet表示生成粗略布局，引入局部注意力机制（Part Attention）优化体素特征。

Result: Ultra3D在1024分辨率下实现高效3D生成，速度提升6.7倍，视觉保真度和用户偏好均达到SOTA。

Conclusion: Ultra3D在效率和生成质量上取得显著改进，适用于高分辨率3D内容生成。

Abstract: Recent advances in sparse voxel representations have significantly improved
the quality of 3D content generation, enabling high-resolution modeling with
fine-grained geometry. However, existing frameworks suffer from severe
computational inefficiencies due to the quadratic complexity of attention
mechanisms in their two-stage diffusion pipelines. In this work, we propose
Ultra3D, an efficient 3D generation framework that significantly accelerates
sparse voxel modeling without compromising quality. Our method leverages the
compact VecSet representation to efficiently generate a coarse object layout in
the first stage, reducing token count and accelerating voxel coordinate
prediction. To refine per-voxel latent features in the second stage, we
introduce Part Attention, a geometry-aware localized attention mechanism that
restricts attention computation within semantically consistent part regions.
This design preserves structural continuity while avoiding unnecessary global
attention, achieving up to 6.7x speed-up in latent generation. To support this
mechanism, we construct a scalable part annotation pipeline that converts raw
meshes into part-labeled sparse voxels. Extensive experiments demonstrate that
Ultra3D supports high-resolution 3D generation at 1024 resolution and achieves
state-of-the-art performance in both visual fidelity and user preference.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [96] [Weak Links in LinkedIn: Enhancing Fake Profile Detection in the Age of LLMs](https://arxiv.org/abs/2507.16860)
*Apoorva Gulati,Rajesh Kumar,Vinti Agarwal,Aditya Sharma*

Main category: cs.SI

TL;DR: 研究发现现有虚假个人资料检测器对LLM生成的个人资料效果不佳，提出GPT辅助对抗训练方法，显著降低误判率。


<details>
  <summary>Details</summary>
Motivation: 评估现有虚假个人资料检测器对LLM生成个人资料的鲁棒性，发现其效果不佳，需改进。

Method: 采用GPT辅助对抗训练，结合数值和文本嵌入进行训练。

Result: 新方法将误判率从42-52%降至1-7%，且不影响误拒率（0.5-2%）。

Conclusion: 结合数值和文本嵌入的检测器鲁棒性最佳，需推广自动化检测方法。

Abstract: Large Language Models (LLMs) have made it easier to create realistic fake
profiles on platforms like LinkedIn. This poses a significant risk for
text-based fake profile detectors. In this study, we evaluate the robustness of
existing detectors against LLM-generated profiles. While highly effective in
detecting manually created fake profiles (False Accept Rate: 6-7%), the
existing detectors fail to identify GPT-generated profiles (False Accept Rate:
42-52%). We propose GPT-assisted adversarial training as a countermeasure,
restoring the False Accept Rate to between 1-7% without impacting the False
Reject Rates (0.5-2%). Ablation studies revealed that detectors trained on
combined numerical and textual embeddings exhibit the highest robustness,
followed by those using numerical-only embeddings, and lastly those using
textual-only embeddings. Complementary analysis on the ability of prompt-based
GPT-4Turbo and human evaluators affirms the need for robust automated detectors
such as the one proposed in this study.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [97] [SADA: Stability-guided Adaptive Diffusion Acceleration](https://arxiv.org/abs/2507.17135)
*Ting Jiang,Yixiao Wang,Hancheng Ye,Zishan Shao,Jingwei Sun,Jingyang Zhang,Zekai Chen,Jianyi Zhang,Yiran Chen,Hai Li*

Main category: cs.LG

TL;DR: SADA是一种新的稳定性引导的自适应扩散加速方法，通过统一基于稳定性的稀疏决策，显著加速ODE生成模型的采样过程。


<details>
  <summary>Details</summary>
Motivation: 现有加速方法在降低计算成本的同时，牺牲了生成质量，主要因为未能考虑不同提示对应的去噪轨迹差异和ODE数值解的基础。

Method: SADA通过稳定性准则统一稀疏决策，自适应分配稀疏性，并利用ODE求解器的精确梯度信息进行近似。

Result: 在多个模型和求解器上，SADA实现了≥1.8倍的加速，且保真度损失极小（LPIPS≤0.10，FID≤4.5），优于现有方法。

Conclusion: SADA不仅高效加速扩散模型，还能无缝适配其他管道和模态，如ControlNet和MusicLDM。

Abstract: Diffusion models have achieved remarkable success in generative tasks but
suffer from high computational costs due to their iterative sampling process
and quadratic attention costs. Existing training-free acceleration strategies
that reduce per-step computation cost, while effectively reducing sampling
time, demonstrate low faithfulness compared to the original baseline. We
hypothesize that this fidelity gap arises because (a) different prompts
correspond to varying denoising trajectory, and (b) such methods do not
consider the underlying ODE formulation and its numerical solution. In this
paper, we propose Stability-guided Adaptive Diffusion Acceleration (SADA), a
novel paradigm that unifies step-wise and token-wise sparsity decisions via a
single stability criterion to accelerate sampling of ODE-based generative
models (Diffusion and Flow-matching). For (a), SADA adaptively allocates
sparsity based on the sampling trajectory. For (b), SADA introduces principled
approximation schemes that leverage the precise gradient information from the
numerical ODE solver. Comprehensive evaluations on SD-2, SDXL, and Flux using
both EDM and DPM++ solvers reveal consistent $\ge 1.8\times$ speedups with
minimal fidelity degradation (LPIPS $\leq 0.10$ and FID $\leq 4.5$) compared to
unmodified baselines, significantly outperforming prior methods. Moreover, SADA
adapts seamlessly to other pipelines and modalities: It accelerates ControlNet
without any modifications and speeds up MusicLDM by $1.8\times$ with $\sim
0.01$ spectrogram LPIPS.

</details>


### [98] [Dataset Distillation as Data Compression: A Rate-Utility Perspective](https://arxiv.org/abs/2507.17221)
*Youneng Bao,Yiping Liu,Zhuo Chen,Yongsheng Liang,Mu Li,Kede Ma*

Main category: cs.LG

TL;DR: 提出了一种联合优化压缩率和效用的数据集蒸馏方法，通过量化潜在编码和轻量网络解码，显著提升了压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习对大规模数据集和模型的需求导致计算和存储成本过高，现有方法未能同时优化压缩率和数据效用。

Method: 参数化合成样本为可优化的潜在编码，通过轻量网络解码，利用香农熵量化压缩率，并结合现有蒸馏损失函数优化效用。

Result: 在CIFAR-10、CIFAR-100和ImageNet-128上，压缩率提升170倍，同时保持准确率。

Conclusion: 该方法在多种压缩率和效用权衡下表现优异，为数据集蒸馏提供了更高效的解决方案。

Abstract: Driven by the ``scale-is-everything'' paradigm, modern machine learning
increasingly demands ever-larger datasets and models, yielding prohibitive
computational and storage requirements. Dataset distillation mitigates this by
compressing an original dataset into a small set of synthetic samples, while
preserving its full utility. Yet, existing methods either maximize performance
under fixed storage budgets or pursue suitable synthetic data representations
for redundancy removal, without jointly optimizing both objectives. In this
work, we propose a joint rate-utility optimization method for dataset
distillation. We parameterize synthetic samples as optimizable latent codes
decoded by extremely lightweight networks. We estimate the Shannon entropy of
quantized latents as the rate measure and plug any existing distillation loss
as the utility measure, trading them off via a Lagrange multiplier. To enable
fair, cross-method comparisons, we introduce bits per class (bpc), a precise
storage metric that accounts for sample, label, and decoder parameter costs. On
CIFAR-10, CIFAR-100, and ImageNet-128, our method achieves up to $170\times$
greater compression than standard distillation at comparable accuracy. Across
diverse bpc budgets, distillation losses, and backbone architectures, our
approach consistently establishes better rate-utility trade-offs.

</details>


### [99] [DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD](https://arxiv.org/abs/2507.17501)
*Xianbiao Qi,Marco Chen,Wenjie Xiao,Jiaquan Ye,Yelin He,Chun-Guang Li,Zhouchen Lin*

Main category: cs.LG

TL;DR: 提出了一种深度归一化Transformer（DNT），通过精心设计的归一化技术，使其能够使用普通的动量SGDW（mSGDW）训练，性能与AdamW训练的Transformer相当。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer训练中梯度分布重尾的问题，使其适应mSGDW优化器。

Method: 在Transformer中策略性地集成归一化技术，调节每层的Jacobian矩阵，平衡权重、激活及其相互作用的影响，使梯度分布更集中。

Result: DNT在ViT和GPT架构上表现优于传统Transformer，且能有效使用mSGDW训练。

Conclusion: DNT通过归一化技术成功解决了Transformer对AdamW的依赖，为训练提供了更高效的选择。

Abstract: Transformers have become the de facto backbone of modern deep learning, yet
their training typically demands an advanced optimizer with adaptive learning
rate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that
it is mainly due to a heavy-tailed distribution of the gradients. In this
paper, we introduce a Deeply Normalized Transformer (DNT), which is
meticulously engineered to overcome this limitation enabling seamless training
with vanilla mSGDW while yielding comparable performance to the Transformers
trained via AdamW. To be specific, in DNT, we strategically integrate
normalization techniques at proper positions in the Transformers to effectively
modulate the Jacobian matrices of each layer, balance the influence of weights,
activations, and their interactions, and thus enable the distributions of
gradients concentrated. We provide both theoretical justifications of the
normalization technique used in our DNT and extensive empirical evaluation on
two popular Transformer architectures to validate that: a) DNT outperforms its
counterparts (\ie, ViT and GPT), and b) DNT can be effectively trained with
vanilla mSGDW.

</details>


### [100] [Joint Asymmetric Loss for Learning with Noisy Labels](https://arxiv.org/abs/2507.17692)
*Jialiang Wang,Xianming Liu,Xiong Zhou,Gangfeng Hu,Deming Zhai,Junjun Jiang,Xiangyang Ji*

Main category: cs.LG

TL;DR: 论文提出了一种新的非对称损失函数AMSE，并将其与APL框架结合，形成JAL框架，用于解决带噪声标签的学习问题。


<details>
  <summary>Details</summary>
Motivation: 现有对称损失函数存在欠拟合问题，而非对称损失函数虽具有优越性，但无法与高级优化框架（如APL）兼容。

Method: 扩展非对称损失函数到被动损失场景，提出AMSE，并将其与APL结合形成JAL框架。

Result: 实验证明JAL框架在减少标签噪声方面有效。

Conclusion: AMSE和JAL框架为非对称损失函数的应用提供了新思路，并在实验中表现出色。

Abstract: Learning with noisy labels is a crucial task for training accurate deep
neural networks. To mitigate label noise, prior studies have proposed various
robust loss functions, particularly symmetric losses. Nevertheless, symmetric
losses usually suffer from the underfitting issue due to the overly strict
constraint. To address this problem, the Active Passive Loss (APL) jointly
optimizes an active and a passive loss to mutually enhance the overall fitting
ability. Within APL, symmetric losses have been successfully extended, yielding
advanced robust loss functions. Despite these advancements, emerging
theoretical analyses indicate that asymmetric losses, a new class of robust
loss functions, possess superior properties compared to symmetric losses.
However, existing asymmetric losses are not compatible with advanced
optimization frameworks such as APL, limiting their potential and
applicability. Motivated by this theoretical gap and the prospect of asymmetric
losses, we extend the asymmetric loss to the more complex passive loss scenario
and propose the Asymetric Mean Square Error (AMSE), a novel asymmetric loss. We
rigorously establish the necessary and sufficient condition under which AMSE
satisfies the asymmetric condition. By substituting the traditional symmetric
passive loss in APL with our proposed AMSE, we introduce a novel robust loss
framework termed Joint Asymmetric Loss (JAL). Extensive experiments demonstrate
the effectiveness of our method in mitigating label noise. Code available at:
https://github.com/cswjl/joint-asymmetric-loss

</details>


### [101] [On the Interaction of Compressibility and Adversarial Robustness](https://arxiv.org/abs/2507.17725)
*Melih Barsbey,Antônio H. Ribeiro,Umut Şimşekli,Tolga Birdal*

Main category: cs.LG

TL;DR: 论文探讨了神经网络压缩性与对抗鲁棒性之间的相互作用，揭示了压缩性如何导致表示空间中的敏感方向，从而影响模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现代神经网络需要在多个方面表现良好，包括准确性、泛化性、效率和鲁棒性。然而，压缩性与鲁棒性之间的相互作用尚未被充分理解。

Method: 作者提出了一个理论框架，分析了神经元稀疏性和谱压缩性等压缩形式如何影响对抗鲁棒性，并通过实验验证了理论预测。

Result: 研究发现压缩性会引入表示空间中的敏感方向，导致对抗攻击更容易成功，且这种脆弱性在不同压缩方式和任务中普遍存在。

Conclusion: 压缩性与鲁棒性之间存在根本性冲突，研究为设计既高效又安全的模型提供了新的思路。

Abstract: Modern neural networks are expected to simultaneously satisfy a host of
desirable properties: accurate fitting to training data, generalization to
unseen inputs, parameter and computational efficiency, and robustness to
adversarial perturbations. While compressibility and robustness have each been
studied extensively, a unified understanding of their interaction still remains
elusive. In this work, we develop a principled framework to analyze how
different forms of compressibility - such as neuron-level sparsity and spectral
compressibility - affect adversarial robustness. We show that these forms of
compression can induce a small number of highly sensitive directions in the
representation space, which adversaries can exploit to construct effective
perturbations. Our analysis yields a simple yet instructive robustness bound,
revealing how neuron and spectral compressibility impact $L_\infty$ and $L_2$
robustness via their effects on the learned representations. Crucially, the
vulnerabilities we identify arise irrespective of how compression is achieved -
whether via regularization, architectural bias, or implicit learning dynamics.
Through empirical evaluations across synthetic and realistic tasks, we confirm
our theoretical predictions, and further demonstrate that these vulnerabilities
persist under adversarial training and transfer learning, and contribute to the
emergence of universal adversarial perturbations. Our findings show a
fundamental tension between structured compressibility and robustness, and
suggest new pathways for designing models that are both efficient and secure.

</details>


### [102] [Large Learning Rates Simultaneously Achieve Robustness to Spurious Correlations and Compressibility](https://arxiv.org/abs/2507.17748)
*Melih Barsbey,Lucas Prieto,Stefanos Zafeiriou,Tolga Birdal*

Main category: cs.LG

TL;DR: 高学习率可以同时提升模型的鲁棒性和资源效率，并改善特征利用、类别分离和激活稀疏性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习模型需要同时具备鲁棒性和资源效率，但目前实现这两者仍具挑战性。

Method: 通过高学习率训练模型，分析其对虚假相关性和网络压缩性的影响。

Result: 高学习率在多种数据集、模型和优化器中均表现出色，且优于其他超参数和正则化方法。

Conclusion: 高学习率不仅能提升标准分类任务的性能，还能有效解决训练数据中的隐藏/罕见虚假相关性。

Abstract: Robustness and resource-efficiency are two highly desirable properties for
modern machine learning models. However, achieving them jointly remains a
challenge. In this paper, we position high learning rates as a facilitator for
simultaneously achieving robustness to spurious correlations and network
compressibility. We demonstrate that large learning rates also produce
desirable representation properties such as invariant feature utilization,
class separation, and activation sparsity. Importantly, our findings indicate
that large learning rates compare favorably to other hyperparameters and
regularization methods, in consistently satisfying these properties in tandem.
In addition to demonstrating the positive effect of large learning rates across
diverse spurious correlation datasets, models, and optimizers, we also present
strong evidence that the previously documented success of large learning rates
in standard classification tasks is likely due to its effect on addressing
hidden/rare spurious correlations in the training dataset.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [103] [Assessing Medical Training Skills via Eye and Head Movements](https://arxiv.org/abs/2507.16819)
*Kayhan Latifzadeh,Luis A. Leiva,Klen Čopič Pucihar,Matjaž Kljun,Iztok Devetak,Lili Steblovnik*

Main category: cs.HC

TL;DR: 通过眼部和头部运动分析临床技能发展，研究发现眼动追踪可有效区分训练和未训练的从业者。


<details>
  <summary>Details</summary>
Motivation: 探索眼部和头部运动在临床技能评估中的应用，以支持技能发展的量化分析。

Method: 24名从业者参与模拟婴儿分娩训练，计算瞳孔反应率、注视时长和角速度等指标。

Result: 头部特征（F1=0.85，AUC=0.86）和瞳孔特征（F1=0.77，AUC=0.85）能有效区分技能水平。

Conclusion: 眼动追踪眼镜可作为传统评估方法的补充工具，支持临床技能的隐式评估和训练。

Abstract: We examined eye and head movements to gain insights into skill development in
clinical settings. A total of 24 practitioners participated in simulated baby
delivery training sessions. We calculated key metrics, including pupillary
response rate, fixation duration, or angular velocity. Our findings indicate
that eye and head tracking can effectively differentiate between trained and
untrained practitioners, particularly during labor tasks. For example,
head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas
pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results
lay the groundwork for computational models that support implicit skill
assessment and training in clinical settings by using commodity eye-tracking
glasses as a complementary device to more traditional evaluation methods such
as subjective scores.

</details>


### [104] [Explainable AI for Collaborative Assessment of 2D/3D Registration Quality](https://arxiv.org/abs/2507.17597)
*Sue Min Cho,Alexander Do,Russell H. Taylor,Mathias Unberath*

Main category: cs.HC

TL;DR: 论文提出了一种用于2D/3D配准质量验证的AI框架，结合可解释性特征，以提升人类操作员的决策能力。


<details>
  <summary>Details</summary>
Motivation: 手术数字化转型中，算法输出验证对患者安全至关重要，但现有可视化方法不足以可靠检测配准误差。

Method: 开发了首个专门用于2D/3D配准质量验证的AI框架，并引入可解释性特征。

Result: 研究发现可解释性特征虽提升用户信任，但整体性能未超过独立AI。

Conclusion: 未来工作可通过优化算法设计和人机协作，进一步提升配准质量验证的鲁棒性。

Abstract: As surgery embraces digital transformation--integrating sophisticated
imaging, advanced algorithms, and robotics to support and automate complex
sub-tasks--human judgment of system correctness remains a vital safeguard for
patient safety. This shift introduces new "operator-type" roles tasked with
verifying complex algorithmic outputs, particularly at critical junctures of
the procedure, such as the intermediary check before drilling or implant
placement. A prime example is 2D/3D registration, a key enabler of image-based
surgical navigation that aligns intraoperative 2D images with preoperative 3D
data. Although registration algorithms have advanced significantly, they
occasionally yield inaccurate results. Because even small misalignments can
lead to revision surgery or irreversible surgical errors, there is a critical
need for robust quality assurance. Current visualization-based strategies alone
have been found insufficient to enable humans to reliably detect 2D/3D
registration misalignments. In response, we propose the first artificial
intelligence (AI) framework trained specifically for 2D/3D registration quality
verification, augmented by explainability features that clarify the model's
decision-making. Our explainable AI (XAI) approach aims to enhance informed
decision-making for human operators by providing a second opinion together with
a rationale behind it. Through algorithm-centric and human-centered
evaluations, we systematically compare four conditions: AI-only, human-only,
human-AI, and human-XAI. Our findings reveal that while explainability features
modestly improve user trust and willingness to override AI errors, they do not
exceed the standalone AI in aggregate performance. Nevertheless, future work
extending both the algorithmic design and the human-XAI collaboration elements
holds promise for more robust quality assurance of 2D/3D registration.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [105] [Audio-Vision Contrastive Learning for Phonological Class Recognition](https://arxiv.org/abs/2507.17682)
*Daiqi Liu,Tomás Arias-Vergara,Jana Hutter,Andreas Maier,Paula Andrea Pérez-Toro*

Main category: cs.SD

TL;DR: 提出了一种多模态深度学习框架，结合实时磁共振成像和语音信号，用于分类发音特征，对比学习方法取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 准确的发音-音系特征分类对理解人类语音生产和开发鲁棒的语音技术至关重要，尤其在临床环境中，有助于疾病诊断和个性化康复。

Method: 采用多模态深度学习框架，结合rtMRI和语音信号，分类15个音系类别，评估了四种配置：单模态rtMRI、单模态音频、多模态中间融合和基于对比学习的音频-视觉融合。

Result: 在USC-TIMIT数据集上，基于对比学习的方法平均F1得分为0.81，比单模态基线绝对提高了0.23。

Conclusion: 对比表示学习在多模态发音分析中有效，代码和数据集将公开以支持未来研究。

Abstract: Accurate classification of articulatory-phonological features plays a vital
role in understanding human speech production and developing robust speech
technologies, particularly in clinical contexts where targeted phonemic
analysis and therapy can improve disease diagnosis accuracy and personalized
rehabilitation. In this work, we propose a multimodal deep learning framework
that combines real-time magnetic resonance imaging (rtMRI) and speech signals
to classify three key articulatory dimensions: manner of articulation, place of
articulation, and voicing. We perform classification on 15 phonological classes
derived from the aforementioned articulatory dimensions and evaluate the system
with four audio/vision configurations: unimodal rtMRI, unimodal audio signals,
multimodal middle fusion, and contrastive learning-based audio-vision fusion.
Experimental results on the USC-TIMIT dataset show that our contrastive
learning-based approach achieves state-of-the-art performance, with an average
F1-score of 0.81, representing an absolute increase of 0.23 over the unimodal
baseline. The results confirm the effectiveness of contrastive representation
learning for multimodal articulatory analysis. Our code and processed dataset
will be made publicly available at
https://github.com/DaE-plz/AC_Contrastive_Phonology to support future research.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [106] [A Hybrid CNN-VSSM model for Multi-View, Multi-Task Mammography Analysis: Robust Diagnosis with Attention-Based Fusion](https://arxiv.org/abs/2507.16955)
*Yalda Zafari,Roaa Elalfy,Mohamed Mabrok,Somaya Al-Maadeed,Tamer Khattab,Essam A. Rashed*

Main category: eess.IV

TL;DR: 提出了一种多视图、多任务的混合深度学习框架，用于乳腺X光片的早期和准确解读，显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法因单视图输入或单任务输出限制了临床实用性，需解决乳腺X光片解读的复杂性和诊断模糊性。

Method: 结合CNN和VSSM的混合架构，通过门控注意力融合模块动态加权多视图信息，处理缺失数据。

Result: 在多项诊断任务中，混合模型表现优于基线模型，如二元分类AUC达0.9967，F1分数0.9830。

Conclusion: 混合框架展示了多任务学习在提升乳腺X光片诊断性能方面的潜力，但也揭示了其局限性。

Abstract: Early and accurate interpretation of screening mammograms is essential for
effective breast cancer detection, yet it remains a complex challenge due to
subtle imaging findings and diagnostic ambiguity. Many existing AI approaches
fall short by focusing on single view inputs or single-task outputs, limiting
their clinical utility. To address these limitations, we propose a novel
multi-view, multitask hybrid deep learning framework that processes all four
standard mammography views and jointly predicts diagnostic labels and BI-RADS
scores for each breast. Our architecture integrates a hybrid CNN VSSM backbone,
combining convolutional encoders for rich local feature extraction with Visual
State Space Models (VSSMs) to capture global contextual dependencies. To
improve robustness and interpretability, we incorporate a gated attention-based
fusion module that dynamically weights information across views, effectively
handling cases with missing data. We conduct extensive experiments across
diagnostic tasks of varying complexity, benchmarking our proposed hybrid models
against baseline CNN architectures and VSSM models in both single task and
multi task learning settings. Across all tasks, the hybrid models consistently
outperform the baselines. In the binary BI-RADS 1 vs. 5 classification task,
the shared hybrid model achieves an AUC of 0.9967 and an F1 score of 0.9830.
For the more challenging ternary classification, it attains an F1 score of
0.7790, while in the five-class BI-RADS task, the best F1 score reaches 0.4904.
These results highlight the effectiveness of the proposed hybrid framework and
underscore both the potential and limitations of multitask learning for
improving diagnostic performance and enabling clinically meaningful mammography
analysis.

</details>


### [107] [Harmonization in Magnetic Resonance Imaging: A Survey of Acquisition, Image-level, and Feature-level Methods](https://arxiv.org/abs/2507.16962)
*Qinqin Yang,Firoozeh Shomal-Zadeh,Ali Gholipour*

Main category: eess.IV

TL;DR: 医学影像标准化综述，重点介绍MRI中的方法、挑战与未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决不同扫描仪、协议或站点导致的影像异质性（批次效应），以提高数据可比性和模型泛化能力。

Method: 分类为前瞻性采集与重建策略、回顾性图像级与特征级方法、以及基于旅行受试者的技术，特别关注深度学习方法。

Result: 总结了代表性方法，并提供了公开数据集和当前技术进展。

Conclusion: 指出了剩余挑战，并提出了未来研究方向，强调深度学习的潜力。

Abstract: Modern medical imaging technologies have greatly advanced neuroscience
research and clinical diagnostics. However, imaging data collected across
different scanners, acquisition protocols, or imaging sites often exhibit
substantial heterogeneity, known as "batch effects" or "site effects". These
non-biological sources of variability can obscure true biological signals,
reduce reproducibility and statistical power, and severely impair the
generalizability of learning-based models across datasets. Image harmonization
aims to eliminate or mitigate such site-related biases while preserving
meaningful biological information, thereby improving data comparability and
consistency. This review provides a comprehensive overview of key concepts,
methodological advances, publicly available datasets, current challenges, and
future directions in the field of medical image harmonization, with a focus on
magnetic resonance imaging (MRI). We systematically cover the full imaging
pipeline, and categorize harmonization approaches into prospective acquisition
and reconstruction strategies, retrospective image-level and feature-level
methods, and traveling-subject-based techniques. Rather than providing an
exhaustive survey, we focus on representative methods, with particular emphasis
on deep learning-based approaches. Finally, we summarize the major challenges
that remain and outline promising avenues for future research.

</details>


### [108] [MyGO: Make your Goals Obvious, Avoiding Semantic Confusion in Prostate Cancer Lesion Region Segmentation](https://arxiv.org/abs/2507.17269)
*Zhengcheng Lin,Zuobin Ying,Zhenyu Li,Zhenyu Liu,Jian Lu,Weiping Ding*

Main category: eess.IV

TL;DR: 提出了一种新的像素锚模块，通过稀疏特征锚点捕捉全局上下文信息，结合自注意力Top_k选择和焦点损失函数，显著提高了前列腺癌病变分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于前列腺癌病变与非病变区域语义高度相似，现有医学图像分割方法难以准确区分，导致语义混淆问题。

Method: 设计了像素锚模块和自注意力Top_k选择策略，结合焦点损失函数，增强模型非线性表示能力并解决类别不平衡问题。

Result: 在PI-CAI数据集上达到69.73% IoU和74.32% Dice分数，显著提升了病变检测性能。

Conclusion: 该方法通过全局上下文捕捉和语义精炼，有效解决了前列腺癌图像分割中的语义混淆问题。

Abstract: Early diagnosis and accurate identification of lesion location and
progression in prostate cancer (PCa) are critical for assisting clinicians in
formulating effective treatment strategies. However, due to the high semantic
homogeneity between lesion and non-lesion areas, existing medical image
segmentation methods often struggle to accurately comprehend lesion semantics,
resulting in the problem of semantic confusion. To address this challenge, we
propose a novel Pixel Anchor Module, which guides the model to discover a
sparse set of feature anchors that serve to capture and interpret global
contextual information. This mechanism enhances the model's nonlinear
representation capacity and improves segmentation accuracy within lesion
regions. Moreover, we design a self-attention-based Top_k selection strategy to
further refine the identification of these feature anchors, and incorporate a
focal loss function to mitigate class imbalance, thereby facilitating more
precise semantic interpretation across diverse regions. Our method achieves
state-of-the-art performance on the PI-CAI dataset, demonstrating 69.73% IoU
and 74.32% Dice scores, and significantly improving prostate cancer lesion
detection.

</details>


### [109] [A Versatile Pathology Co-pilot via Reasoning Enhanced Multimodal Large Language Model](https://arxiv.org/abs/2507.17303)
*Zhe Xu,Ziyi Liu,Junlin Hou,Jiabo Ma,Cheng Jin,Yihui Wang,Zhixuan Chen,Zhengyu Zhang,Zhengrui Guo,Fengtao Zhou,Yingxue Xu,Xi Wang,Ronald Cheong Kin Chan,Li Liang,Hao Chen*

Main category: eess.IV

TL;DR: SmartPath-R1是一种多功能多模态大语言模型（MLLM），能够同时处理ROI和WSI级别的任务，并通过混合专家机制实现多尺度多任务分析，显著提升了病理诊断的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前病理学中的MLLM方法推理能力受限，主要依赖昂贵的链式思维标注，且仅适用于ROI级别的视觉问答（VQA），无法满足临床诊断的多样化需求。

Method: SmartPath-R1结合了尺度依赖的监督微调和任务感知的强化微调，利用MLLM的内在知识避免链式思维监督，并通过混合专家机制实现多任务动态处理。

Result: 在72个任务上的广泛实验验证了SmartPath-R1的有效性和优越性，使用了230万ROI样本和18.8万WSI样本进行训练和评估。

Conclusion: SmartPath-R1是开发多功能、推理增强的AI系统用于精准病理学的重要进展。

Abstract: Multimodal large language models (MLLMs) have emerged as powerful tools for
computational pathology, offering unprecedented opportunities to integrate
pathological images with language context for comprehensive diagnostic
analysis. These models hold particular promise for automating complex tasks
that traditionally require expert interpretation of pathologists. However,
current MLLM approaches in pathology demonstrate significantly constrained
reasoning capabilities, primarily due to their reliance on expensive
chain-of-thought annotations. Additionally, existing methods remain limited to
simplex application of visual question answering (VQA) at region-of-interest
(ROI) level, failing to address the full spectrum of diagnostic needs such as
ROI classification, detection, segmentation, whole-slide-image (WSI)
classification and VQA in clinical practice. In this study, we present
SmartPath-R1, a versatile MLLM capable of simultaneously addressing both
ROI-level and WSI-level tasks while demonstrating robust pathological reasoning
capability. Our framework combines scale-dependent supervised fine-tuning and
task-aware reinforcement fine-tuning, which circumvents the requirement for
chain-of-thought supervision by leveraging the intrinsic knowledge within MLLM.
Furthermore, SmartPath-R1 integrates multiscale and multitask analysis through
a mixture-of-experts mechanism, enabling dynamic processing for diverse tasks.
We curate a large-scale dataset comprising 2.3M ROI samples and 188K WSI
samples for training and evaluation. Extensive experiments across 72 tasks
validate the effectiveness and superiority of the proposed approach. This work
represents a significant step toward developing versatile, reasoning-enhanced
AI systems for precision pathology.

</details>


### [110] [Mammo-Mamba: A Hybrid State-Space and Transformer Architecture with Sequential Mixture of Experts for Multi-View Mammography](https://arxiv.org/abs/2507.17662)
*Farnoush Bayatmakou,Reza Taleei,Nicole Simone,Arash Mohammadi*

Main category: eess.IV

TL;DR: Mammo-Mamba是一种结合选择性状态空间模型和Transformer的新型框架，用于高效的多视角乳腺X光片分类。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性癌症相关死亡的主要原因之一，现有基于Transformer的模型计算复杂度高，需要更高效的替代方案。

Method: 提出Mammo-Mamba框架，整合选择性状态空间模型（SSMs）、Transformer注意力机制和专家驱动的特征细化，通过SeqMoE机制增强表示学习。

Result: 在CBIS-DDSM数据集上，Mammo-Mamba在所有关键指标上均表现出优越的分类性能，同时保持计算效率。

Conclusion: Mammo-Mamba通过动态专家门控和内容自适应特征细化，有效解决了传统Transformer模型的局限性，为乳腺癌早期检测提供了高效工具。

Abstract: Breast cancer (BC) remains one of the leading causes of cancer-related
mortality among women, despite recent advances in Computer-Aided Diagnosis
(CAD) systems. Accurate and efficient interpretation of multi-view mammograms
is essential for early detection, driving a surge of interest in Artificial
Intelligence (AI)-powered CAD models. While state-of-the-art multi-view
mammogram classification models are largely based on Transformer architectures,
their computational complexity scales quadratically with the number of image
patches, highlighting the need for more efficient alternatives. To address this
challenge, we propose Mammo-Mamba, a novel framework that integrates Selective
State-Space Models (SSMs), transformer-based attention, and expert-driven
feature refinement into a unified architecture. Mammo-Mamba extends the
MambaVision backbone by introducing the Sequential Mixture of Experts (SeqMoE)
mechanism through its customized SecMamba block. The SecMamba is a modified
MambaVision block that enhances representation learning in high-resolution
mammographic images by enabling content-adaptive feature refinement. These
blocks are integrated into the deeper stages of MambaVision, allowing the model
to progressively adjust feature emphasis through dynamic expert gating,
effectively mitigating the limitations of traditional Transformer models.
Evaluated on the CBIS-DDSM benchmark dataset, Mammo-Mamba achieves superior
classification performance across all key metrics while maintaining
computational efficiency.

</details>


### [111] [MCM: Mamba-based Cardiac Motion Tracking using Sequential Images in MRI](https://arxiv.org/abs/2507.17678)
*Jiahui Yin,Xinxing Cheng,Jinming Duan,Yan Pang,Declan O'Regan,Hadrien Reynaud,Qingjie Meng*

Main category: eess.IV

TL;DR: 提出了一种基于Mamba的心脏运动跟踪网络（MCM），通过结合目标图像序列和双向扫描机制，实现了平滑且时间一致的运动跟踪。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常从单对图像中学习运动，忽略了心脏运动的连续性，导致估计结果不一致且不平滑。

Method: 开发了双向Mamba块和运动解码器，利用序列图像学习心肌的连续动态，同时保持计算效率。

Result: 在两个公开数据集上的实验表明，MCM在定量和定性上均优于传统和最先进的学习方法。

Conclusion: MCM通过结合序列信息和Mamba的结构化状态空间，显著提升了心脏运动跟踪的性能。

Abstract: Myocardial motion tracking is important for assessing cardiac function and
diagnosing cardiovascular diseases, for which cine cardiac magnetic resonance
(CMR) has been established as the gold standard imaging modality. Many existing
methods learn motion from single image pairs consisting of a reference frame
and a randomly selected target frame from the cardiac cycle. However, these
methods overlook the continuous nature of cardiac motion and often yield
inconsistent and non-smooth motion estimations. In this work, we propose a
novel Mamba-based cardiac motion tracking network (MCM) that explicitly
incorporates target image sequence from the cardiac cycle to achieve smooth and
temporally consistent motion tracking. By developing a bi-directional Mamba
block equipped with a bi-directional scanning mechanism, our method facilitates
the estimation of plausible deformation fields. With our proposed motion
decoder that integrates motion information from frames adjacent to the target
frame, our method further enhances temporal coherence. Moreover, by taking
advantage of Mamba's structured state-space formulation, the proposed method
learns the continuous dynamics of the myocardium from sequential images without
increasing computational complexity. We evaluate the proposed method on two
public datasets. The experimental results demonstrate that the proposed method
quantitatively and qualitatively outperforms both conventional and
state-of-the-art learning-based cardiac motion tracking methods. The code is
available at https://github.com/yjh-0104/MCM.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [112] [VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings](https://arxiv.org/abs/2507.17080)
*Ramin Giahi,Kehui Yao,Sriram Kollipara,Kai Zhao,Vahid Mirjalili,Jianpeng Xu,Topojoy Biswas,Evren Korpeoglu,Kannan Achan*

Main category: cs.IR

TL;DR: VL-CLIP框架通过视觉定位和LLM增强文本表示，解决了CLIP在电商推荐中的细粒度对齐、文本模糊性和领域不匹配问题，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（如CLIP）在电商推荐中存在细粒度对齐不足、文本描述模糊和领域不匹配的问题，影响了推荐性能。

Method: 提出VL-CLIP框架，结合视觉定位（细化图像表示）和LLM代理（增强文本嵌入），优化多模态匹配。

Result: 在美国大型电商平台上，点击率（CTR）提升18.6%，加购率（ATC）提升15.5%，GMV增长4.0%，优于CLIP等模型。

Conclusion: 结合视觉定位和LLM增强文本表示能显著提升多模态推荐效果，为电商推荐系统提供了新思路。

Abstract: Multimodal learning plays a critical role in e-commerce recommendation
platforms today, enabling accurate recommendations and product understanding.
However, existing vision-language models, such as CLIP, face key challenges in
e-commerce recommendation systems: 1) Weak object-level alignment, where global
image embeddings fail to capture fine-grained product attributes, leading to
suboptimal retrieval performance; 2) Ambiguous textual representations, where
product descriptions often lack contextual clarity, affecting cross-modal
matching; and 3) Domain mismatch, as generic vision-language models may not
generalize well to e-commerce-specific data. To address these limitations, we
propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating
Visual Grounding for fine-grained visual understanding and an LLM-based agent
for generating enriched text embeddings. Visual Grounding refines image
representations by localizing key products, while the LLM agent enhances
textual features by disambiguating product descriptions. Our approach
significantly improves retrieval accuracy, multimodal retrieval effectiveness,
and recommendation quality across tens of millions of items on one of the
largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by
15.5%, and GMV by 4.0%. Additional experimental results show that our framework
outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in
both precision and semantic alignment, demonstrating the potential of combining
object-aware visual grounding and LLM-enhanced text representation for robust
multimodal recommendations.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [113] [InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation](https://arxiv.org/abs/2507.17520)
*Shuai Yang,Hao Li,Yilun Chen,Bin Wang,Yang Tian,Tai Wang,Hanqing Wang,Feng Zhao,Yiyi Liao,Jiangmiao Pang*

Main category: cs.RO

TL;DR: InstructVLA是一种新型的视觉-语言-动作模型，通过多模态训练和专家混合适应，在推理和动作生成上表现优异，显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在推理和动作生成之间存在权衡，且容易遗忘预训练能力。InstructVLA旨在解决这一问题，实现高效的多模态推理和动作生成。

Method: 提出Vision-Language-Action Instruction Tuning（VLA-IT）训练范式，结合多模态训练和专家混合适应，优化文本推理和动作生成。

Result: 在SimplerEnv任务上提升30.5%，在80任务基准测试中优于OpenVLA和GPT-4o辅助的专家模型。

Conclusion: InstructVLA成功结合了直观的人机交互和高效策略学习，展示了其在多模态任务中的潜力。

Abstract: To operate effectively in the real world, robots must integrate multimodal
reasoning with precise action generation. However, existing
vision-language-action (VLA) models often sacrifice one for the other, narrow
their abilities to task-specific manipulation data, and suffer catastrophic
forgetting of pre-trained vision-language capabilities. To bridge this gap, we
introduce InstructVLA, an end-to-end VLA model that preserves the flexible
reasoning of large vision-language models (VLMs) while delivering leading
manipulation performance. InstructVLA introduces a novel training paradigm,
Vision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal
training with mixture-of-experts adaptation to jointly optimize textual
reasoning and action generation on both standard VLM corpora and a curated
650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves
30.5% improvement over SpatialVLA. To evaluate generalization, we introduce
SimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and
high-level instruction understanding, where it outperforms a fine-tuned OpenVLA
by 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA
surpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling
by leveraging textual reasoning to boost manipulation performance in both
simulated and real-world settings. These results demonstrate InstructVLA's
potential for bridging intuitive and steerable human-robot interaction with
efficient policy learning.

</details>


### [114] [CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation](https://arxiv.org/abs/2507.17727)
*Robel Mamo,Taeyeong Choi*

Main category: cs.RO

TL;DR: 提出了一种新的数据增强方法Crop-Aligned Cutout (CA-Cut)，通过模拟遮挡提高视觉导航模型的鲁棒性，显著减少预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法在复杂环境下表现不佳，需要更有效的方法来模拟遮挡和增强模型鲁棒性。

Method: 提出CA-Cut方法，通过在作物行周围随机遮挡区域，鼓励模型捕捉高级上下文特征。

Result: 实验表明，CA-Cut显著提高了语义关键点预测的准确性，预测误差减少高达36.9%。

Conclusion: CA-Cut是一种有效的增强方法，尤其在复杂环境下表现优异，通过优化遮挡分布进一步提升性能。

Abstract: State-of-the-art visual under-canopy navigation methods are designed with
deep learning-based perception models to distinguish traversable space from
crop rows. While these models have demonstrated successful performance, they
require large amounts of training data to ensure reliability in real-world
field deployment. However, data collection is costly, demanding significant
human resources for in-field sampling and annotation. To address this
challenge, various data augmentation techniques are commonly employed during
model training, such as color jittering, Gaussian blur, and horizontal flip, to
diversify training data and enhance model robustness. In this paper, we
hypothesize that utilizing only these augmentation techniques may lead to
suboptimal performance, particularly in complex under-canopy environments with
frequent occlusions, debris, and non-uniform spacing of crops. Instead, we
propose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)
which masks random regions out in input images that are spatially distributed
around crop rows on the sides to encourage trained models to capture high-level
contextual features even when fine-grained information is obstructed. Our
extensive experiments with a public cornfield dataset demonstrate that
masking-based augmentations are effective for simulating occlusions and
significantly improving robustness in semantic keypoint predictions for visual
navigation. In particular, we show that biasing the mask distribution toward
crop rows in CA-Cut is critical for enhancing both prediction accuracy and
generalizability across diverse environments achieving up to a 36.9% reduction
in prediction error. In addition, we conduct ablation studies to determine the
number of masks, the size of each mask, and the spatial distribution of masks
to maximize overall performance.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [115] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: FundusExpert是一个眼科专用的多模态大语言模型，通过集成定位-诊断推理能力和高质量数据集FundusGen，显著提升了眼科诊断的准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在眼科领域面临的注释粒度碎片化和临床推理逻辑不一致的问题。

Method: 开发Fundus-Engine系统自动化定位和语义扩展，构建FundusGen数据集，并设计临床对齐的认知链指导模型推理。

Result: FundusExpert在眼科问答任务中超越MedRegA 40B模型26.6%，在零样本报告生成任务中临床一致性达77.0%，显著优于GPT-4o。

Conclusion: 通过区域级定位与诊断推理链的结合，为特定领域多模态大语言模型的发展提供了可扩展的解决方案。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [116] [Controllable Video Generation: A Survey](https://arxiv.org/abs/2507.16869)
*Yue Ma,Kunyu Feng,Zhongyuan Hu,Xinyu Wang,Yucheng Wang,Mingzhe Zheng,Xuanhua He,Chenyang Zhu,Hongyu Liu,Yingqing He,Zeyu Wang,Zhifeng Li,Xiu Li,Wei Liu,Dan Xu,Linfeng Zhang,Qifeng Chen*

Main category: cs.GR

TL;DR: 综述探讨了可控视频生成的理论基础和最新进展，重点分析了如何通过非文本条件增强视频生成的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到视频生成模型难以满足复杂、多模态和细粒度的用户需求，因此需要引入额外控制条件。

Method: 通过整合相机运动、深度图和人体姿态等非文本条件，扩展预训练视频生成模型。

Result: 提出了分类方法，包括单条件生成、多条件生成和通用可控生成。

Conclusion: 综述为可控视频生成领域提供了系统回顾，并总结了现有方法的分类和进展。

Abstract: With the rapid development of AI-generated content (AIGC), video generation
has emerged as one of its most dynamic and impactful subfields. In particular,
the advancement of video generation foundation models has led to growing demand
for controllable video generation methods that can more accurately reflect user
intent. Most existing foundation models are designed for text-to-video
generation, where text prompts alone are often insufficient to express complex,
multi-modal, and fine-grained user requirements. This limitation makes it
challenging for users to generate videos with precise control using current
models. To address this issue, recent research has explored the integration of
additional non-textual conditions, such as camera motion, depth maps, and human
pose, to extend pretrained video generation models and enable more controllable
video synthesis. These approaches aim to enhance the flexibility and practical
applicability of AIGC-driven video generation systems. In this survey, we
provide a systematic review of controllable video generation, covering both
theoretical foundations and recent advances in the field. We begin by
introducing the key concepts and commonly used open-source video generation
models. We then focus on control mechanisms in video diffusion models,
analyzing how different types of conditions can be incorporated into the
denoising process to guide generation. Finally, we categorize existing methods
based on the types of control signals they leverage, including single-condition
generation, multi-condition generation, and universal controllable generation.
For a complete list of the literature on controllable video generation
reviewed, please visit our curated repository at
https://github.com/mayuelala/Awesome-Controllable-Video-Generation.

</details>


### [117] [StreamME: Simplify 3D Gaussian Avatar within Live Stream](https://arxiv.org/abs/2507.17029)
*Luchuan Song,Yang Zhou,Zhan Xu,Yi Zhou,Deepali Aneja,Chenliang Xu*

Main category: cs.GR

TL;DR: StreamME是一种快速3D头像重建方法，通过实时视频流同步记录和重建头像，无需预缓存数据，适用于下游应用。


<details>
  <summary>Details</summary>
Motivation: 解决传统3D头像重建方法依赖预缓存数据、速度慢的问题，同时保护隐私并减少通信带宽。

Method: 基于3D高斯泼溅（3DGS），消除对MLP的依赖，引入基于主点的简化策略优化点云分布。

Result: 显著提高了面部表情的适应速度，同时保持渲染质量。

Conclusion: StreamME在VR系统、在线会议及动画等下游应用中具有高效性和实用性。

Abstract: We propose StreamME, a method focuses on fast 3D avatar reconstruction. The
StreamME synchronously records and reconstructs a head avatar from live video
streams without any pre-cached data, enabling seamless integration of the
reconstructed appearance into downstream applications. This exceptionally fast
training strategy, which we refer to as on-the-fly training, is central to our
approach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating
the reliance on MLPs in deformable 3DGS and relying solely on geometry, which
significantly improves the adaptation speed to facial expression. To further
ensure high efficiency in on-the-fly training, we introduced a simplification
strategy based on primary points, which distributes the point clouds more
sparsely across the facial surface, optimizing points number while maintaining
rendering quality. Leveraging the on-the-fly training capabilities, our method
protects the facial privacy and reduces communication bandwidth in VR system or
online conference. Additionally, it can be directly applied to downstream
application such as animation, toonify, and relighting. Please refer to our
project page for more details: https://songluchuan.github.io/StreamME/.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [118] [A tissue and cell-level annotated H&E and PD-L1 histopathology image dataset in non-small cell lung cancer](https://arxiv.org/abs/2507.16855)
*Joey Spronck,Leander van Eekelen,Dominique van Midden,Joep Bogaerts,Leslie Tessier,Valerie Dechering,Muradije Demirel-Andishmand,Gabriel Silva de Souza,Roland Nemeth,Enrico Munari,Giuseppe Bogina,Ilaria Girolami,Albino Eccher,Balazs Acs,Ceren Boyaci,Natalie Klubickova,Monika Looijen-Salamon,Shoko Vos,Francesco Ciompi*

Main category: q-bio.QM

TL;DR: IGNITE数据工具包填补了非小细胞肺癌（NSCLC）数字病理数据集的空白，提供了多染色、多中心、多扫描仪的注释数据集，支持组织分割、细胞检测和PD-L1阳性肿瘤细胞检测。


<details>
  <summary>Details</summary>
Motivation: 现有NSCLC数字病理数据集在范围、转移部位注释和分子信息（如PD-L1免疫组化）方面存在不足，限制了免疫治疗生物标志物的开发。

Method: IGNITE数据工具包包含887个完全注释的感兴趣区域，来自155名患者，支持三种任务：H&E染色切片的多类组织分割、细胞核检测和PD-L1 IHC切片中的阳性肿瘤细胞检测。

Result: IGNITE是首个公开的NSCLC数据集，包含转移部位H&E注释和PD-L1 IHC手动注释。

Conclusion: IGNITE数据集为NSCLC免疫微环境研究提供了重要资源，有助于推动免疫治疗生物标志物的开发。

Abstract: The tumor immune microenvironment (TIME) in non-small cell lung cancer
(NSCLC) histopathology contains morphological and molecular characteristics
predictive of immunotherapy response. Computational quantification of TIME
characteristics, such as cell detection and tissue segmentation, can support
biomarker development. However, currently available digital pathology datasets
of NSCLC for the development of cell detection or tissue segmentation
algorithms are limited in scope, lack annotations of clinically prevalent
metastatic sites, and forgo molecular information such as PD-L1
immunohistochemistry (IHC). To fill this gap, we introduce the IGNITE data
toolkit, a multi-stain, multi-centric, and multi-scanner dataset of annotated
NSCLC whole-slide images. We publicly release 887 fully annotated regions of
interest from 155 unique patients across three complementary tasks: (i)
multi-class semantic segmentation of tissue compartments in H&E-stained slides,
with 16 classes spanning primary and metastatic NSCLC, (ii) nuclei detection,
and (iii) PD-L1 positive tumor cell detection in PD-L1 IHC slides. To the best
of our knowledge, this is the first public NSCLC dataset with manual
annotations of H&E in metastatic sites and PD-L1 IHC.

</details>
