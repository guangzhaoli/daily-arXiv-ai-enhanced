<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 52]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.CL](#cs.CL) [Total: 3]
- [eess.IV](#eess.IV) [Total: 4]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Facial Emotion Recognition does not detect feeling unsafe in automated driving](https://arxiv.org/abs/2509.04490)
*Abel van Elburg,Konstantinos Gkentsidis,Mathieu Sarrazin,Sarah Barendswaard,Varun Kotian,Riender Happee*

Main category: cs.CV

TL;DR: 这篇论文通过驾驶模拟器实验研究自动驾驶车的风险感知，发现车辆运动和皮电导可用于对象风险评估，而面部表情识别效果不佳。


<details>
  <summary>Details</summary>
Motivation: 研究自动驾驶车的风险感知对公众接受度的重要性，需要寻找可靠的对象风险评估方法。

Method: 使用32名参与者进行驾驶模拟器实验，收集主观舒适度、车辆运动、面部表情、皮电导、心率和眼动数据，并建立神经网络模型预测风险感知。

Result: 动态驾驶风格导致更强的不舒适感，行人突然出现加剧了风险感知。面部表情识别在大多数参与者中无法检测到明显反应，而车辆运动和皮电导的神经网络模型与主观风险感知呈现良好相关性。

Conclusion: 车辆运动和皮电导可作为自动驾驶车风险感知的对象评估指标，面部表情识别在此领域效果有限，为了减少主观偏差需要进一步研究。

Abstract: Trust and perceived safety play a crucial role in the public acceptance of
automated vehicles. To understand perceived risk, an experiment was conducted
using a driving simulator under two automated driving styles and optionally
introducing a crossing pedestrian. Data was collected from 32 participants,
consisting of continuous subjective comfort ratings, motion, webcam footage for
facial expression, skin conductance, heart rate, and eye tracking. The
continuous subjective perceived risk ratings showed significant discomfort
associated with perceived risk during cornering and braking followed by relief
or even positive comfort on continuing the ride. The dynamic driving style
induced a stronger discomfort as compared to the calm driving style. The
crossing pedestrian did not affect discomfort with the calm driving style but
doubled the comfort decrement with the dynamic driving style. This illustrates
the importance of consequences of critical interactions in risk perception.
Facial expression was successfully analyzed for 24 participants but most
(15/24) did not show any detectable facial reaction to the critical event.
Among the 9 participants who did, 8 showed a Happy expression, and only 4
showed a Surprise expression. Fear was never dominant. This indicates that
facial expression recognition is not a reliable method for assessing perceived
risk in automated vehicles. To predict perceived risk a neural network model
was implemented using vehicle motion and skin conductance. The model correlated
well with reported perceived risk, demonstrating its potential for objective
perceived risk assessment in automated vehicles, reducing subjective bias and
highlighting areas for future research.

</details>


### [2] [PromptEnhancer: A Simple Approach to Enhance Text-to-Image Models via Chain-of-Thought Prompt Rewriting](https://arxiv.org/abs/2509.04545)
*Linqing Wang,Ximing Xing,Yiji Cheng,Zhiyuan Zhao,Jiale Tao,Qixun Wang,Ruihuang Li,Xin Li,Mingrui Wu,Xinchi Deng,Chunyu Wang,Qinglin Lu*

Main category: cs.CV

TL;DR: PromptEnhancer是一个通用的提示词重写框架，通过强化学习训练Chain-of-Thought重写器来提升文本到图像模型的提示词质量，无需修改模型权重即可显著改善图像-文本对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在处理复杂用户提示时经常出现属性绑定、否定和组合关系方面的困难，导致用户意图与生成结果之间存在显著不匹配。

Method: 训练一个Chain-of-Thought重写器通过强化学习，使用专门的AlignEvaluator奖励模型提供细粒度反馈，该评估器基于24个关键点的系统分类法进行训练。

Result: 在HunyuanImage 2.1模型上的广泛实验表明，PromptEnhancer在广泛的语义和组合挑战中显著改善了图像-文本对齐效果。

Conclusion: 该框架成功解决了文本到图像模型提示词理解不准确的问题，并引入了新的高质量人类偏好基准来推动未来研究。

Abstract: Recent advancements in text-to-image (T2I) diffusion models have demonstrated
remarkable capabilities in generating high-fidelity images. However, these
models often struggle to faithfully render complex user prompts, particularly
in aspects like attribute binding, negation, and compositional relationships.
This leads to a significant mismatch between user intent and the generated
output. To address this challenge, we introduce PromptEnhancer, a novel and
universal prompt rewriting framework that enhances any pretrained T2I model
without requiring modifications to its weights. Unlike prior methods that rely
on model-specific fine-tuning or implicit reward signals like image-reward
scores, our framework decouples the rewriter from the generator. We achieve
this by training a Chain-of-Thought (CoT) rewriter through reinforcement
learning, guided by a dedicated reward model we term the AlignEvaluator. The
AlignEvaluator is trained to provide explicit and fine-grained feedback based
on a systematic taxonomy of 24 key points, which are derived from a
comprehensive analysis of common T2I failure modes. By optimizing the CoT
rewriter to maximize the reward from our AlignEvaluator, our framework learns
to generate prompts that are more precisely interpreted by T2I models.
Extensive experiments on the HunyuanImage 2.1 model demonstrate that
PromptEnhancer significantly improves image-text alignment across a wide range
of semantic and compositional challenges. Furthermore, we introduce a new,
high-quality human preference benchmark to facilitate future research in this
direction.

</details>


### [3] [Skywork UniPic 2.0: Building Kontext Model with Online RL for Unified Multimodal Model](https://arxiv.org/abs/2509.04548)
*Hongyang Wei,Baixin Xu,Hongbo Liu,Cyrus Wu,Jie Liu,Yi Peng,Peiyu Wang,Zexiang Liu,Jingwen He,Yidan Xietian,Chuanxin Tang,Zidong Wang,Yichen Wei,Liang Hu,Boyi Jiang,William Li,Ying He,Yang Liu,Xuchen Song,Eric Li,Yahui Zhou*

Main category: cs.CV

TL;DR: UniPic2-SD3.5M-Kontext是一个2B参数的DiT模型，基于SD3.5-Medium架构改进，通过渐进式双任务强化策略(PDTR)实现了最先进的图像生成和编辑能力，并在统一多模态框架中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前开源多模态模型过度关注参数规模扩展而忽视训练策略优化，导致效率和性能受限，需要更高效的训练方法来提升模型能力。

Method: 采用SD3.5-Medium架构改进、大规模高质量数据预训练、渐进式双任务强化策略(PDTR)，并与Qwen2.5-VL-7B通过连接器联合训练构建统一多模态模型。

Result: 模型在图像生成和编辑方面超越参数更大的模型(BAGEL 7B和Flux-Kontext 12B)，UniPic2-Metaquery在多种任务上达到顶级性能。

Conclusion: 提出的Skywork UniPic 2.0训练范式有效且具有良好泛化能力，证明了优化训练策略比单纯增加参数规模更重要。

Abstract: Recent advances in multimodal models have demonstrated impressive
capabilities in unified image generation and editing. However, many prominent
open-source models prioritize scaling model parameters over optimizing training
strategies, limiting their efficiency and performance. In this work, we present
UniPic2-SD3.5M-Kontext, a 2B-parameter DiT model based on SD3.5-Medium, which
achieves state-of-the-art image generation and editing while extending
seamlessly into a unified multimodal framework. Our approach begins with
architectural modifications to SD3.5-Medium and large-scale pre-training on
high-quality data, enabling joint text-to-image generation and editing
capabilities. To enhance instruction following and editing consistency, we
propose a novel Progressive Dual-Task Reinforcement strategy (PDTR), which
effectively strengthens both tasks in a staged manner. We empirically validate
that the reinforcement phases for different tasks are mutually beneficial and
do not induce negative interference. After pre-training and reinforcement
strategies, UniPic2-SD3.5M-Kontext demonstrates stronger image generation and
editing capabilities than models with significantly larger generation
parameters-including BAGEL (7B) and Flux-Kontext (12B). Furthermore, following
the MetaQuery, we connect the UniPic2-SD3.5M-Kontext and Qwen2.5-VL-7B via a
connector and perform joint training to launch a unified multimodal model
UniPic2-Metaquery. UniPic2-Metaquery integrates understanding, generation, and
editing, achieving top-tier performance across diverse tasks with a simple and
scalable training paradigm. This consistently validates the effectiveness and
generalizability of our proposed training paradigm, which we formalize as
Skywork UniPic 2.0.

</details>


### [4] [Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing via Bidirectional Warping](https://arxiv.org/abs/2509.04582)
*Jingyi Lu,Kai Han*

Main category: cs.CV

TL;DR: Inpaint4Drag是一个基于像素空间的双向扭曲和图像修复的拖拽式图像编辑框架，实现了实时扭曲预览和高效修复，显著提升了交互体验。


<details>
  <summary>Details</summary>
Motivation: 现有基于生成模型潜在空间的方法存在精度有限、反馈延迟和模型特定限制的问题，需要更直接高效的编辑方案。

Method: 将拖拽编辑分解为像素空间双向扭曲和图像修复两个步骤，借鉴物理世界中弹性物体变形的思想，将图像区域视为可变形材料。

Result: 在512x512分辨率下实现0.01秒的实时扭曲预览和0.3秒的高效修复，相比需要数分钟编辑的现有方法显著提升效率，且能作为通用适配器兼容任何修复模型。

Conclusion: 该方法在保持实时性能的同时实现了卓越的视觉质量和精确控制，为拖拽式图像编辑提供了新的有效解决方案。

Abstract: Drag-based image editing has emerged as a powerful paradigm for intuitive
image manipulation. However, existing approaches predominantly rely on
manipulating the latent space of generative models, leading to limited
precision, delayed feedback, and model-specific constraints. Accordingly, we
present Inpaint4Drag, a novel framework that decomposes drag-based editing into
pixel-space bidirectional warping and image inpainting. Inspired by elastic
object deformation in the physical world, we treat image regions as deformable
materials that maintain natural shape under user manipulation. Our method
achieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at
512x512 resolution, significantly improving the interaction experience compared
to existing methods that require minutes per edit. By transforming drag inputs
directly into standard inpainting formats, our approach serves as a universal
adapter for any inpainting model without architecture modification,
automatically inheriting all future improvements in inpainting technology.
Extensive experiments demonstrate that our method achieves superior visual
quality and precise control while maintaining real-time performance. Project
page: https://visual-ai.github.io/inpaint4drag/

</details>


### [5] [DisPatch: Disarming Adversarial Patches in Object Detection with Diffusion Models](https://arxiv.org/abs/2509.04597)
*Jin Ma,Mohammed Aldeen,Christopher Salas,Feng Luo,Mashrur Chowdhury,Mert Pesé,Long Cheng*

Main category: cs.CV

TL;DR: DISPATCH是一个基于扩散模型的物体检测防御框架，采用"再生与修正"策略来对抗对抗性补丁攻击，无需先验知识且对多种攻击有效。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的物体检测器容易受到对抗性补丁攻击，这些攻击可以在真实物体上隐藏实际物品或创建不存在的物品，造成严重后果。需要一种有效、通用且能抵抗自适应攻击的防御方法。

Method: 利用扩散模型的分布内生成能力重新生成整个图像，使其与良性数据对齐，然后通过修正过程识别并替换对抗性区域为重新生成的良性对应部分。

Result: 在多个检测器和攻击场景下的实验表明，DISPATCH在隐藏攻击上达到89.3%的mAP.5最佳得分，在非目标创建攻击上将攻击成功率降低至24.8%，并保持对自适应攻击的强大鲁棒性。

Conclusion: DISPATCH是第一个基于扩散模型的物体检测防御框架，具有攻击无关性、无需先验知识的特点，为物体检测系统提供了实用可靠的防御方案。

Abstract: Object detection is fundamental to various real-world applications, such as
security monitoring and surveillance video analysis. Despite their
advancements, state-of-theart object detectors are still vulnerable to
adversarial patch attacks, which can be easily applied to real-world objects to
either conceal actual items or create non-existent ones, leading to severe
consequences. Given the current diversity of adversarial patch attacks and
potential unknown threats, an ideal defense method should be effective,
generalizable, and robust against adaptive attacks. In this work, we introduce
DISPATCH, the first diffusion-based defense framework for object detection.
Unlike previous works that aim to "detect and remove" adversarial patches,
DISPATCH adopts a "regenerate and rectify" strategy, leveraging generative
models to disarm attack effects while preserving the integrity of the input
image. Specifically, we utilize the in-distribution generative power of
diffusion models to regenerate the entire image, aligning it with benign data.
A rectification process is then employed to identify and replace adversarial
regions with their regenerated benign counterparts. DISPATCH is attack-agnostic
and requires no prior knowledge of the existing patches. Extensive experiments
across multiple detectors and attacks demonstrate that DISPATCH consistently
outperforms state-of-the-art defenses on both hiding attacks and creating
attacks, achieving the best overall mAP.5 score of 89.3% on hiding attacks, and
lowering the attack success rate to 24.8% on untargeted creating attacks.
Moreover, it maintains strong robustness against adaptive attacks, making it a
practical and reliable defense for object detection systems.

</details>


### [6] [WATCH: World-aware Allied Trajectory and pose reconstruction for Camera and Human](https://arxiv.org/abs/2509.04600)
*Qijun Ying,Zhongyuan Hu,Rui Zhang,Ronghui Li,Yu Lu,Zijiao Zeng*

Main category: cs.CV

TL;DR: WATCH是一个统一的全球人体运动重建框架，通过创新的航向角分解技术和相机轨迹整合机制，解决了相机-人体运动关系建模中的关键挑战，在野外单目视频上实现了最先进的轨迹重建性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于人体运动的单目视频全局运动重建方法面临两个关键限制：相机方向信息利用不足和相机平移线索整合效果不佳，这阻碍了从相机坐标到世界坐标的准确映射。

Method: 提出了WATCH框架，包含：1）分析性航向角分解技术，相比现有几何方法具有更好的效率和可扩展性；2）受世界模型启发的相机轨迹整合机制，提供比简单硬解码方法更有效的相机平移信息利用途径。

Result: 在野外基准测试中，WATCH在端到端轨迹重建方面实现了最先进的性能，证明了联合建模相机-人体运动关系的有效性。

Conclusion: 该工作为长期存在的相机平移整合挑战提供了新的解决思路，展示了在全局人体运动重建中联合建模相机-人体运动关系的有效性，代码将公开提供。

Abstract: Global human motion reconstruction from in-the-wild monocular videos is
increasingly demanded across VR, graphics, and robotics applications, yet
requires accurate mapping of human poses from camera to world coordinates-a
task challenged by depth ambiguity, motion ambiguity, and the entanglement
between camera and human movements. While human-motion-centric approaches excel
in preserving motion details and physical plausibility, they suffer from two
critical limitations: insufficient exploitation of camera orientation
information and ineffective integration of camera translation cues. We present
WATCH (World-aware Allied Trajectory and pose reconstruction for Camera and
Human), a unified framework addressing both challenges. Our approach introduces
an analytical heading angle decomposition technique that offers superior
efficiency and extensibility compared to existing geometric methods.
Additionally, we design a camera trajectory integration mechanism inspired by
world models, providing an effective pathway for leveraging camera translation
information beyond naive hard-decoding approaches. Through experiments on
in-the-wild benchmarks, WATCH achieves state-of-the-art performance in
end-to-end trajectory reconstruction. Our work demonstrates the effectiveness
of jointly modeling camera-human motion relationships and offers new insights
for addressing the long-standing challenge of camera translation integration in
global human motion reconstruction. The code will be available publicly.

</details>


### [7] [Sali4Vid: Saliency-Aware Video Reweighting and Adaptive Caption Retrieval for Dense Video Captioning](https://arxiv.org/abs/2509.04602)
*MinJu Jeon,Si-Woo Kim,Ye-Chan Kim,HyunGee Kim,Dong-Jin Kim*

Main category: cs.CV

TL;DR: Sali4Vid：一种简单有效的显著性感知框架，通过视频重加权和自适应字幕检索改进密集视频描述任务


<details>
  <summary>Details</summary>
Motivation: 现有端到端模型存在两个局限性：(1) 时间戳监督仅应用于文本，对所有视频帧平等对待；(2) 从固定大小的视频块中检索字幕，忽略了场景转换

Method: 提出显著性感知视频重加权（将时间戳注释转换为基于sigmoid的帧重要性权重）和基于语义的自适应字幕检索（通过帧相似性分割视频以捕捉场景转换）

Result: 在YouCook2和ViTT数据集上实现了最先进的结果

Conclusion: 联合改进视频加权和检索对密集视频描述任务有益，Sali4Vid框架简单而有效

Abstract: Dense video captioning aims to temporally localize events in video and
generate captions for each event. While recent works propose end-to-end models,
they suffer from two limitations: (1) applying timestamp supervision only to
text while treating all video frames equally, and (2) retrieving captions from
fixed-size video chunks, overlooking scene transitions. To address these, we
propose Sali4Vid, a simple yet effective saliency-aware framework. We introduce
Saliency-aware Video Reweighting, which converts timestamp annotations into
sigmoid-based frame importance weights, and Semantic-based Adaptive Caption
Retrieval, which segments videos by frame similarity to capture scene
transitions and improve caption retrieval. Sali4Vid achieves state-of-the-art
results on YouCook2 and ViTT, demonstrating the benefit of jointly improving
video weighting and retrieval for dense video captioning

</details>


### [8] [UAV-Based Intelligent Traffic Surveillance System: Real-Time Vehicle Detection, Classification, Tracking, and Behavioral Analysis](https://arxiv.org/abs/2509.04624)
*Ali Khanpour,Tianyi Wang,Afra Vahidi-Shams,Wim Ectors,Farzam Nakhaie,Amirhossein Taheri,Christian Claudel*

Main category: cs.CV

TL;DR: 提出基于无人机的交通监控系统，实现91.8%检测精度和90.5% F1分数，支持车辆分类、交通违规检测和多维度城市交通分析


<details>
  <summary>Details</summary>
Motivation: 传统交通监控系统存在覆盖范围有限、适应性差和扩展性不足的问题，需要更先进的解决方案来应对城市交通拥堵和违规行为

Method: 采用多尺度多角度模板匹配、卡尔曼滤波和单应性校准技术处理200米高空采集的航拍视频数据，结合地理围栏、运动过滤和轨迹偏差分析

Result: 在城区案例研究中达到91.8%检测精度、90.5% F1分数，跟踪指标MOTA 92.1%、MOTP 93.7%，成功分类5种车辆类型并自动检测多种交通违规

Conclusion: 系统展示了出色的可扩展性、准确性和实用价值，可作为下一代智慧城市中独立于基础设施的交通监控解决方案

Abstract: Traffic congestion and violations pose significant challenges for urban
mobility and road safety. Traditional traffic monitoring systems, such as fixed
cameras and sensor-based methods, are often constrained by limited coverage,
low adaptability, and poor scalability. To address these challenges, this paper
introduces an advanced unmanned aerial vehicle (UAV)-based traffic surveillance
system capable of accurate vehicle detection, classification, tracking, and
behavioral analysis in real-world, unconstrained urban environments. The system
leverages multi-scale and multi-angle template matching, Kalman filtering, and
homography-based calibration to process aerial video data collected from
altitudes of approximately 200 meters. A case study in urban area demonstrates
robust performance, achieving a detection precision of 91.8%, an F1-score of
90.5%, and tracking metrics (MOTA/MOTP) of 92.1% and 93.7%, respectively.
Beyond precise detection, the system classifies five vehicle types and
automatically detects critical traffic violations, including unsafe lane
changes, illegal double parking, and crosswalk obstructions, through the fusion
of geofencing, motion filtering, and trajectory deviation analysis. The
integrated analytics module supports origin-destination tracking, vehicle count
visualization, inter-class correlation analysis, and heatmap-based congestion
modeling. Additionally, the system enables entry-exit trajectory profiling,
vehicle density estimation across road segments, and movement direction
logging, supporting comprehensive multi-scale urban mobility analytics.
Experimental results confirms the system's scalability, accuracy, and practical
relevance, highlighting its potential as an enforcement-aware,
infrastructure-independent traffic monitoring solution for next-generation
smart cities.

</details>


### [9] [VCMamba: Bridging Convolutions with Multi-Directional Mamba for Efficient Visual Representation](https://arxiv.org/abs/2509.04669)
*Mustafa Munir,Alex Zhang,Radu Marculescu*

Main category: cs.CV

TL;DR: VCMamba是一种新颖的视觉骨干网络，通过结合CNN的局部特征提取能力和多向Mamba SSM的全局建模能力，在ImageNet-1K和ADE20K任务上取得了优越性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决ViT和SSM在捕获细粒度局部特征方面的不足，同时弥补CNN缺乏全局推理能力的问题，需要开发一种能够结合两者优势的混合架构。

Method: 采用卷积主干和分层结构，早期阶段使用卷积块提取局部特征，后期阶段使用多向Mamba块建模长距离依赖和全局上下文，保持线性计算复杂度。

Result: VCMamba-B在ImageNet-1K上达到82.6% top-1准确率，参数量比PlainMamba-L3少37%；在ADE20K上获得47.1 mIoU，参数量比EfficientFormer-L7少62%。

Conclusion: VCMamba成功整合了CNN和Mamba SSM的优势，在减少参数量的同时实现了优越的性能，为视觉任务提供了高效的混合架构解决方案。

Abstract: Recent advances in Vision Transformers (ViTs) and State Space Models (SSMs)
have challenged the dominance of Convolutional Neural Networks (CNNs) in
computer vision. ViTs excel at capturing global context, and SSMs like Mamba
offer linear complexity for long sequences, yet they do not capture
fine-grained local features as effectively as CNNs. Conversely, CNNs possess
strong inductive biases for local features but lack the global reasoning
capabilities of transformers and Mamba. To bridge this gap, we introduce
\textit{VCMamba}, a novel vision backbone that integrates the strengths of CNNs
and multi-directional Mamba SSMs. VCMamba employs a convolutional stem and a
hierarchical structure with convolutional blocks in its early stages to extract
rich local features. These convolutional blocks are then processed by later
stages incorporating multi-directional Mamba blocks designed to efficiently
model long-range dependencies and global context. This hybrid design allows for
superior feature representation while maintaining linear complexity with
respect to image resolution. We demonstrate VCMamba's effectiveness through
extensive experiments on ImageNet-1K classification and ADE20K semantic
segmentation. Our VCMamba-B achieves 82.6% top-1 accuracy on ImageNet-1K,
surpassing PlainMamba-L3 by 0.3% with 37% fewer parameters, and outperforming
Vision GNN-B by 0.3% with 64% fewer parameters. Furthermore, VCMamba-B obtains
47.1 mIoU on ADE20K, exceeding EfficientFormer-L7 by 2.0 mIoU while utilizing
62% fewer parameters. Code is available at
https://github.com/Wertyuui345/VCMamba.

</details>


### [10] [Guideline-Consistent Segmentation via Multi-Agent Refinement](https://arxiv.org/abs/2509.04687)
*Vanshika Vats,Ashwani Rathee,James Davis*

Main category: cs.CV

TL;DR: 提出多智能体训练免费框架，通过Worker-Supervisor迭代架构协调通用视觉语言模型，实现复杂文本标注指南下的语义分割


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中语义分割需要严格遵守复杂文本标注指南的问题，传统方法需要昂贵的任务特定重训练且难以适应指南变化

Method: 多智能体框架：Worker执行分割，Supervisor根据检索的指南进行批判，轻量级强化学习停止策略决定循环终止时机

Result: 在Waymo和ReasonSeg数据集上显著优于最先进基线方法，展示出强大的泛化能力和指令遵循能力

Conclusion: 该训练免费框架能够有效处理段落长度的复杂分割规则，在保证指南一致性的同时平衡资源使用

Abstract: Semantic segmentation in real-world applications often requires not only
accurate masks but also strict adherence to textual labeling guidelines. These
guidelines are typically complex and long, and both human and automated
labeling often fail to follow them faithfully. Traditional approaches depend on
expensive task-specific retraining that must be repeated as the guidelines
evolve. Although recent open-vocabulary segmentation methods excel with simple
prompts, they often fail when confronted with sets of paragraph-length
guidelines that specify intricate segmentation rules. To address this, we
introduce a multi-agent, training-free framework that coordinates
general-purpose vision-language models within an iterative Worker-Supervisor
refinement architecture. The Worker performs the segmentation, the Supervisor
critiques it against the retrieved guidelines, and a lightweight reinforcement
learning stop policy decides when to terminate the loop, ensuring
guideline-consistent masks while balancing resource use. Evaluated on the Waymo
and ReasonSeg datasets, our method notably outperforms state-of-the-art
baselines, demonstrating strong generalization and instruction adherence.

</details>


### [11] [Domain Adaptation for Different Sensor Configurations in 3D Object Detection](https://arxiv.org/abs/2509.04711)
*Satoshi Tanaka,Kok Seang Tan,Isamu Yamashita*

Main category: cs.CV

TL;DR: 提出了两种技术来解决不同传感器配置之间的域适应问题：下游微调和部分层微调，在多传感器配置的3D目标检测中取得了比简单联合训练更好的性能


<details>
  <summary>Details</summary>
Motivation: 不同车辆平台部署不同的传感器配置会导致点云分布变化，当在一个配置上训练的模型应用到另一个配置时会出现性能下降，而现有研究主要关注环境域差距和单一LiDAR内的密度变化，不同传感器配置的域差距尚未充分探索

Method: 提出两种技术：1）下游微调（在多数据集训练后进行数据集特定的微调）；2）部分层微调（只更新部分层以提高跨配置泛化能力）。使用同一地理区域收集的多个传感器配置的配对数据集进行实验

Result: 联合训练结合下游微调和部分层微调在每种配置上都持续优于简单的联合训练方法

Conclusion: 为3D目标检测模型适应多样化车辆平台提供了一个实用且可扩展的解决方案

Abstract: Recent advances in autonomous driving have underscored the importance of
accurate 3D object detection, with LiDAR playing a central role due to its
robustness under diverse visibility conditions. However, different vehicle
platforms often deploy distinct sensor configurations, causing performance
degradation when models trained on one configuration are applied to another
because of shifts in the point cloud distribution. Prior work on multi-dataset
training and domain adaptation for 3D object detection has largely addressed
environmental domain gaps and density variation within a single LiDAR; in
contrast, the domain gap for different sensor configurations remains largely
unexplored. In this work, we address domain adaptation across different sensor
configurations in 3D object detection. We propose two techniques: Downstream
Fine-tuning (dataset-specific fine-tuning after multi-dataset training) and
Partial Layer Fine-tuning (updating only a subset of layers to improve
cross-configuration generalization). Using paired datasets collected in the
same geographic region with multiple sensor configurations, we show that joint
training with Downstream Fine-tuning and Partial Layer Fine-tuning consistently
outperforms naive joint training for each configuration. Our findings provide a
practical and scalable solution for adapting 3D object detection models to the
diverse vehicle platforms.

</details>


### [12] [CD-Mamba: Cloud detection with long-range spatial dependency modeling](https://arxiv.org/abs/2509.04729)
*Tianxiang Xue,Jiayi Zhao,Jingsheng Li,Changlu Chen,Kun Zhan*

Main category: cs.CV

TL;DR: CD-Mamba是一个结合卷积神经网络和Mamba状态空间模型的混合网络，用于遥感图像云检测，能够同时处理局部纹理细节和长程依赖关系


<details>
  <summary>Details</summary>
Motivation: 遥感图像常被云层遮挡，影响数据完整性。云检测需要同时处理短程空间冗余和长程大气相似性，现有方法难以兼顾局部和全局特征

Method: 提出CD-Mamba混合模型，集成卷积神经网络（处理局部空间依赖）和Mamba状态空间建模（处理长程依赖），构建统一的云检测网络

Result: 大量实验验证了CD-Mamba的有效性，证明其在检测精度上优于现有方法，能够跨不同空间尺度提升检测性能

Conclusion: CD-Mamba成功整合了卷积和Mamba的优势，能够同时处理像素级交互和大范围块级依赖关系，为遥感图像云检测提供了有效的解决方案

Abstract: Remote sensing images are frequently obscured by cloud cover, posing
significant challenges to data integrity and reliability. Effective cloud
detection requires addressing both short-range spatial redundancies and
long-range atmospheric similarities among cloud patches. Convolutional neural
networks are effective at capturing local spatial dependencies, while Mamba has
strong capabilities in modeling long-range dependencies. To fully leverage both
local spatial relations and long-range dependencies, we propose CD-Mamba, a
hybrid model that integrates convolution and Mamba's state-space modeling into
a unified cloud detection network. CD-Mamba is designed to comprehensively
capture pixelwise textural details and long term patchwise dependencies for
cloud detection. This design enables CD-Mamba to manage both pixel-wise
interactions and extensive patch-wise dependencies simultaneously, improving
detection accuracy across diverse spatial scales. Extensive experiments
validate the effectiveness of CD-Mamba and demonstrate its superior performance
over existing methods.

</details>


### [13] [Exploiting Unlabeled Structures through Task Consistency Training for Versatile Medical Image Segmentation](https://arxiv.org/abs/2509.04732)
*Shengqian Zhu,Jiafei Wu,Xiaogang Xu,Chengrong Yu,Ying Song,Zhang Yi,Guangjun Li,Junjie Hu*

Main category: cs.CV

TL;DR: 提出Task Consistency Training (TCT)框架解决医学图像分割中的类别不平衡问题，无需额外模型，通过一致性约束和过滤策略有效利用未标注数据


<details>
  <summary>Details</summary>
Motivation: 解决多类别医学图像分割中部分标注数据集导致的类别不平衡问题，避免现有方法需要额外模型和标签噪声导致的性能下降

Method: TCT框架包含主分割头和多个辅助任务头，通过一致性约束利用未标注解剖结构，采用过滤策略排除低一致性数据，并使用UAUWL损失缓解特定任务主导问题

Result: 在八个不同临床站点的腹部数据集上进行广泛实验，证明了方法的有效性

Conclusion: TCT框架能够有效处理部分标注数据集中的类别不平衡问题，无需额外模型，提高了医学图像分割的性能

Abstract: Versatile medical image segmentation (VMIS) targets the segmentation of
multiple classes, while obtaining full annotations for all classes is often
impractical due to the time and labor required. Leveraging partially labeled
datasets (PLDs) presents a promising alternative; however, current VMIS
approaches face significant class imbalance due to the unequal category
distribution in PLDs. Existing methods attempt to address this by generating
pseudo-full labels. Nevertheless, these typically require additional models and
often result in potential performance degradation from label noise. In this
work, we introduce a Task Consistency Training (TCT) framework to address class
imbalance without requiring extra models. TCT includes a backbone network with
a main segmentation head (MSH) for multi-channel predictions and multiple
auxiliary task heads (ATHs) for task-specific predictions. By enforcing a
consistency constraint between the MSH and ATH predictions, TCT effectively
utilizes unlabeled anatomical structures. To avoid error propagation from
low-consistency, potentially noisy data, we propose a filtering strategy to
exclude such data. Additionally, we introduce a unified auxiliary
uncertainty-weighted loss (UAUWL) to mitigate segmentation quality declines
caused by the dominance of specific tasks. Extensive experiments on eight
abdominal datasets from diverse clinical sites demonstrate our approach's
effectiveness.

</details>


### [14] [Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization](https://arxiv.org/abs/2509.04735)
*Dharsan Ravindran,Kevin Wang,Zhuoyuan Cao,Saleh Abdelrahman,Jeffery Wu*

Main category: cs.CV

TL;DR: 论文提出两种不确定性感知方法来增强SAM模型在恶劣天气条件下的分割性能：多步微调SAM2引入不确定性损失，以及将医学影像的UAT适配器迁移到自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型（如SAM和SAM2）在视觉模糊的恶劣天气条件下表现不佳，缺乏不确定性量化能力，而自动驾驶需要在这种安全关键场景中保持可靠性。

Method: 1. 为SAM2设计多步微调程序，将不确定性指标直接融入损失函数；2. 将医学影像中的不确定性感知适配器（UAT）适配到自动驾驶场景。

Result: 在CamVid、BDD100K和GTA驾驶数据集上的实验表明，UAT-SAM在极端天气下优于标准SAM，而带不确定性损失的SAM2在多样化驾驶场景中性能均有提升。

Conclusion: 明确的不确定性建模对于自动驾驶在挑战性环境中的安全关键应用具有重要价值，证明了从医学影像迁移不确定性感知技术的有效性。

Abstract: Recent advances in vision foundation models, such as the Segment Anything
Model (SAM) and its successor SAM2, have achieved state-of-the-art performance
on general image segmentation benchmarks. However, these models struggle in
adverse weather conditions where visual ambiguity is high, largely due to their
lack of uncertainty quantification. Inspired by progress in medical imaging,
where uncertainty-aware training has improved reliability in ambiguous cases,
we investigate two approaches to enhance segmentation robustness for autonomous
driving. First, we introduce a multi-step finetuning procedure for SAM2 that
incorporates uncertainty metrics directly into the loss function, improving
overall scene recognition. Second, we adapt the Uncertainty-Aware Adapter
(UAT), originally designed for medical image segmentation, to driving contexts.
We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.
Experiments show that UAT-SAM outperforms standard SAM in extreme weather,
while SAM2 with uncertainty-aware loss achieves improved performance across
diverse driving scenes. These findings underscore the value of explicit
uncertainty modeling for safety-critical autonomous driving in challenging
environments.

</details>


### [15] [WatchHAR: Real-time On-device Human Activity Recognition System for Smartwatches](https://arxiv.org/abs/2509.04736)
*Taeyoung Yeon,Vasco Xu,Henry Hoffmann,Karan Ahuja*

Main category: cs.CV

TL;DR: WatchHAR是一个完全在智能手表上运行的音频和惯性传感器融合的细粒度人类活动识别系统，通过端到端优化实现了5倍处理速度提升，在25+活动类别上保持90%以上准确率。


<details>
  <summary>Details</summary>
Motivation: 解决智能手表在无约束环境中完全本地化运行活动识别系统的挑战，应对隐私保护和延迟问题。

Method: 提出新颖的统一架构，将传感器数据预处理和推理整合为端到端可训练模块，优化流水线每个组件。

Result: 处理时间显著缩短：活动事件检测9.3毫秒，多模态活动分类11.8毫秒，性能优于现有最优模型。

Conclusion: 该研究推动了设备端活动识别技术的发展，实现了智能手表作为独立、隐私保护、最小侵入性连续活动追踪设备的潜力。

Abstract: Despite advances in practical and multimodal fine-grained Human Activity
Recognition (HAR), a system that runs entirely on smartwatches in unconstrained
environments remains elusive. We present WatchHAR, an audio and inertial-based
HAR system that operates fully on smartwatches, addressing privacy and latency
issues associated with external data processing. By optimizing each component
of the pipeline, WatchHAR achieves compounding performance gains. We introduce
a novel architecture that unifies sensor data preprocessing and inference into
an end-to-end trainable module, achieving 5x faster processing while
maintaining over 90% accuracy across more than 25 activity classes. WatchHAR
outperforms state-of-the-art models for event detection and activity
classification while running directly on the smartwatch, achieving 9.3 ms
processing time for activity event detection and 11.8 ms for multimodal
activity classification. This research advances on-device activity recognition,
realizing smartwatches' potential as standalone, privacy-aware, and
minimally-invasive continuous activity tracking devices.

</details>


### [16] [MCANet: A Multi-Scale Class-Specific Attention Network for Multi-Label Post-Hurricane Damage Assessment using UAV Imagery](https://arxiv.org/abs/2509.04757)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: MCANet是一个多标签分类框架，通过多尺度特征学习和类别特定注意力机制，在飓风后无人机图像损伤评估中达到91.75%的mAP，优于多个基准模型。


<details>
  <summary>Details</summary>
Motivation: 现有CNN方法难以捕捉多尺度空间特征，且难以区分视觉相似或共现的损伤类型，需要更准确的灾后损伤评估方法。

Method: 使用Res2Net分层主干网络丰富多尺度空间上下文，结合多头类别特定残差注意力模块，每个注意力分支关注不同空间粒度，平衡局部细节和全局上下文。

Result: 在RescueNet数据集（4,494张图像）上达到91.75% mAP，8个注意力头时提升至92.35%，对Road Blocked等困难类别的AP提升超过6%。

Conclusion: MCANet能够准确定位损伤相关区域，支持可解释性，可用于灾后风险制图、应急路由和数字孪生灾害响应，未来可集成知识图谱和多模态大语言模型提升适应性。

Abstract: Rapid and accurate post-hurricane damage assessment is vital for disaster
response and recovery. Yet existing CNN-based methods struggle to capture
multi-scale spatial features and to distinguish visually similar or
co-occurring damage types. To address these issues, we propose MCANet, a
multi-label classification framework that learns multi-scale representations
and adaptively attends to spatially relevant regions for each damage category.
MCANet employs a Res2Net-based hierarchical backbone to enrich spatial context
across scales and a multi-head class-specific residual attention module to
enhance discrimination. Each attention branch focuses on different spatial
granularities, balancing local detail with global context. We evaluate MCANet
on the RescueNet dataset of 4,494 UAV images collected after Hurricane Michael.
MCANet achieves a mean average precision (mAP) of 91.75%, outperforming ResNet,
Res2Net, VGG, MobileNet, EfficientNet, and ViT. With eight attention heads,
performance further improves to 92.35%, boosting average precision for
challenging classes such as Road Blocked by over 6%. Class activation mapping
confirms MCANet's ability to localize damage-relevant regions, supporting
interpretability. Outputs from MCANet can inform post-disaster risk mapping,
emergency routing, and digital twin-based disaster response. Future work could
integrate disaster-specific knowledge graphs and multimodal large language
models to improve adaptability to unseen disasters and enrich semantic
understanding for real-world decision-making.

</details>


### [17] [Dynamic Group Detection using VLM-augmented Temporal Groupness Graph](https://arxiv.org/abs/2509.04758)
*Kaname Yokoyama,Chihiro Nakatani,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型的动态人群组检测方法，通过局部和全局特征提取以及时间一致性优化，在视频中检测动态变化的群体


<details>
  <summary>Details</summary>
Motivation: 传统方法假设群体在视频中不变，无法处理动态变化的群体。需要同时考虑局部外观特征和全局场景上下文来检测复杂群体

Method: 使用增强的视觉语言模型(VLM)提取每帧的局部和全局特征，通过基于图模型的全局优化方法处理时间一致性，利用groupness增强的CLIP特征估计概率

Result: 在公开数据集上超越了最先进的群体检测方法

Conclusion: 该方法能够有效检测视频中动态变化的群体，结合局部和全局特征以及时间一致性优化取得了优异性能

Abstract: This paper proposes dynamic human group detection in videos. For detecting
complex groups, not only the local appearance features of in-group members but
also the global context of the scene are important. Such local and global
appearance features in each frame are extracted using a Vision-Language Model
(VLM) augmented for group detection in our method. For further improvement, the
group structure should be consistent over time. While previous methods are
stabilized on the assumption that groups are not changed in a video, our method
detects dynamically changing groups by global optimization using a graph with
all frames' groupness probabilities estimated by our groupness-augmented CLIP
features. Our experimental results demonstrate that our method outperforms
state-of-the-art group detection methods on public datasets. Code:
https://github.com/irajisamurai/VLM-GroupDetection.git

</details>


### [18] [FloodVision: Urban Flood Depth Estimation Using Foundation Vision-Language Models and Domain Knowledge Graph](https://arxiv.org/abs/2509.04772)
*Zhangding Liu,Neda Mohammadi,John E. Taylor*

Main category: cs.CV

TL;DR: FloodVision是一个零样本洪水深度估计框架，结合GPT-4o视觉语言模型和领域知识图谱，在110张图像上实现8.17厘米平均绝对误差，比基线提升20.5%


<details>
  <summary>Details</summary>
Motivation: 现有计算机视觉方法依赖固定物体检测器和特定任务训练，存在精度限制和泛化能力差的问题，需要开发能够准确估计洪水深度并适应多样化场景的方法

Method: 结合GPT-4o的语义推理能力和结构化领域知识图谱，动态识别可见参考物体，检索验证高度，估计淹没比例，并应用统计异常值过滤来计算最终深度值

Result: 在MyCoast New York的110张众包图像上评估，平均绝对误差为8.17厘米，比GPT-4o基线（10.28厘米）降低20.5%，超越先前基于CNN的方法

Conclusion: 该系统在不同场景中泛化良好，近乎实时运行，适合未来集成到数字孪生平台和公民报告应用中，用于智慧城市洪水韧性建设

Abstract: Timely and accurate floodwater depth estimation is critical for road
accessibility and emergency response. While recent computer vision methods have
enabled flood detection, they suffer from both accuracy limitations and poor
generalization due to dependence on fixed object detectors and task-specific
training. To enable accurate depth estimation that can generalize across
diverse flood scenarios, this paper presents FloodVision, a zero-shot framework
that combines the semantic reasoning abilities of the foundation
vision-language model GPT-4o with a structured domain knowledge graph. The
knowledge graph encodes canonical real-world dimensions for common urban
objects including vehicles, people, and infrastructure elements to ground the
model's reasoning in physical reality. FloodVision dynamically identifies
visible reference objects in RGB images, retrieves verified heights from the
knowledge graph to mitigate hallucination, estimates submergence ratios, and
applies statistical outlier filtering to compute final depth values. Evaluated
on 110 crowdsourced images from MyCoast New York, FloodVision achieves a mean
absolute error of 8.17 cm, reducing the GPT-4o baseline 10.28 cm by 20.5% and
surpassing prior CNN-based methods. The system generalizes well across varying
scenes and operates in near real-time, making it suitable for future
integration into digital twin platforms and citizen-reporting apps for smart
city flood resilience.

</details>


### [19] [Hybrid-Tower: Fine-grained Pseudo-query Interaction and Generation for Text-to-Video Retrieval](https://arxiv.org/abs/2509.04773)
*Bangxiang Lan,Ruobing Xie,Ruixiang Zhao,Xingwu Sun,Zhanhui Kang,Gang Yang,Xirong Li*

Main category: cs.CV

TL;DR: 提出PIG方法，通过伪查询生成器为视频生成伪查询，实现细粒度交互，在保持双塔框架效率的同时达到接近单塔框架的效果


<details>
  <summary>Details</summary>
Motivation: 解决文本-视频检索任务中双塔框架效果差和单塔框架效率低的问题，希望同时获得高效果和高效率

Method: 提出混合塔框架PIG，包含伪查询生成器为每个视频生成伪查询，使视频特征和文本特征能够进行细粒度交互

Result: 在五个常用基准测试上R@1提升1.6%~3.9%，效率与双塔模型相当，性能接近state-of-the-art

Conclusion: 混合塔框架能够有效结合双塔和单塔框架的优势，在文本-视频检索任务中同时实现高效果和高效率

Abstract: The Text-to-Video Retrieval (T2VR) task aims to retrieve unlabeled videos by
textual queries with the same semantic meanings. Recent CLIP-based approaches
have explored two frameworks: Two-Tower versus Single-Tower framework, yet the
former suffers from low effectiveness, while the latter suffers from low
efficiency. In this study, we explore a new Hybrid-Tower framework that can
hybridize the advantages of the Two-Tower and Single-Tower framework, achieving
high effectiveness and efficiency simultaneously. We propose a novel hybrid
method, Fine-grained Pseudo-query Interaction and Generation for T2VR, ie, PIG,
which includes a new pseudo-query generator designed to generate a pseudo-query
for each video. This enables the video feature and the textual features of
pseudo-query to interact in a fine-grained manner, similar to the Single-Tower
approaches to hold high effectiveness, even before the real textual query is
received. Simultaneously, our method introduces no additional storage or
computational overhead compared to the Two-Tower framework during the inference
stage, thus maintaining high efficiency. Extensive experiments on five commonly
used text-video retrieval benchmarks demonstrate that our method achieves a
significant improvement over the baseline, with an increase of $1.6\% \sim
3.9\%$ in R@1. Furthermore, our method matches the efficiency of Two-Tower
models while achieving near state-of-the-art performance, highlighting the
advantages of the Hybrid-Tower framework.

</details>


### [20] [Comparative Evaluation of Traditional and Deep Learning Feature Matching Algorithms using Chandrayaan-2 Lunar Data](https://arxiv.org/abs/2509.04775)
*R. Makharia,J. G. Singla,Amitabh,N. Dube,H. Sharma*

Main category: cs.CV

TL;DR: 评估五种特征匹配算法在月球多模态图像配准中的性能，包括传统方法(SIFT、ASIFT、AKAZE、RIFT2)和深度学习(SuperGlue)，并提出预处理流程。SuperGlue在精度和速度上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 月球探测需要精确的图像配准来实现表面测绘、资源定位和任务规划，但不同传感器(光学、高光谱、雷达)的图像存在分辨率、光照和畸变差异，配准具有挑战性。

Method: 提出包含地理参考、分辨率对齐、强度归一化的预处理流程，采用自适应直方图均衡、主成分分析和阴影校正等增强技术，比较五种特征匹配算法在赤道和极区图像对上的性能。

Result: SuperGlue始终获得最低的均方根误差和最快运行时间，传统方法在赤道附近表现良好但在极地光照条件下性能下降。

Conclusion: 预处理和学习型方法对于在不同条件下实现鲁棒的月球图像配准至关重要，深度学习算法SuperGlue在跨模态配准中表现最优。

Abstract: Accurate image registration is critical for lunar exploration, enabling
surface mapping, resource localization, and mission planning. Aligning data
from diverse lunar sensors -- optical (e.g., Orbital High Resolution Camera,
Narrow and Wide Angle Cameras), hyperspectral (Imaging Infrared Spectrometer),
and radar (e.g., Dual-Frequency Synthetic Aperture Radar, Selene/Kaguya
mission) -- is challenging due to differences in resolution, illumination, and
sensor distortion. We evaluate five feature matching algorithms: SIFT, ASIFT,
AKAZE, RIFT2, and SuperGlue (a deep learning-based matcher), using
cross-modality image pairs from equatorial and polar regions. A preprocessing
pipeline is proposed, including georeferencing, resolution alignment, intensity
normalization, and enhancements like adaptive histogram equalization, principal
component analysis, and shadow correction. SuperGlue consistently yields the
lowest root mean square error and fastest runtimes. Classical methods such as
SIFT and AKAZE perform well near the equator but degrade under polar lighting.
The results highlight the importance of preprocessing and learning-based
approaches for robust lunar image registration across diverse conditions.

</details>


### [21] [Toward Accessible Dermatology: Skin Lesion Classification Using Deep Learning Models on Mobile-Acquired Images](https://arxiv.org/abs/2509.04800)
*Asif Newaz,Masum Mushfiq Ishti,A Z M Ashraful Azam,Asif Ur Rahman Adib*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的移动设备皮肤病变分类方法，在包含50多种皮肤疾病的大规模数据集上表现优异，特别适合资源有限环境下的AI辅助皮肤病筛查。


<details>
  <summary>Details</summary>
Motivation: 传统皮肤病诊断方法成本高、复杂且在资源匮乏地区难以获得，现有深度学习研究大多局限于皮肤镜数据集和有限疾病类别，需要更贴近真实世界条件的移动设备采集数据和更广泛的疾病分类方法。

Method: 收集了超过50种皮肤疾病的移动设备拍摄数据集，评估了多种卷积神经网络和Transformer架构，特别采用Swin Transformer模型，并整合Grad-CAM技术增强模型可解释性。

Result: Transformer模型（特别是Swin Transformer）表现出最佳性能，能够有效捕捉全局上下文特征，Grad-CAM技术成功突出了临床相关区域，为模型预测提供了透明度。

Conclusion: 基于Transformer的方法在移动设备采集的皮肤病变分类中具有巨大潜力，为资源有限环境下的可访问AI辅助皮肤病筛查和早期诊断开辟了新途径。

Abstract: Skin diseases are among the most prevalent health concerns worldwide, yet
conventional diagnostic methods are often costly, complex, and unavailable in
low-resource settings. Automated classification using deep learning has emerged
as a promising alternative, but existing studies are mostly limited to
dermoscopic datasets and a narrow range of disease classes. In this work, we
curate a large dataset of over 50 skin disease categories captured with mobile
devices, making it more representative of real-world conditions. We evaluate
multiple convolutional neural networks and Transformer-based architectures,
demonstrating that Transformer models, particularly the Swin Transformer,
achieve superior performance by effectively capturing global contextual
features. To enhance interpretability, we incorporate Gradient-weighted Class
Activation Mapping (Grad-CAM), which highlights clinically relevant regions and
provides transparency in model predictions. Our results underscore the
potential of Transformer-based approaches for mobile-acquired skin lesion
classification, paving the way toward accessible AI-assisted dermatological
screening and early diagnosis in resource-limited environments.

</details>


### [22] [Extracting Uncertainty Estimates from Mixtures of Experts for Semantic Segmentation](https://arxiv.org/abs/2509.04816)
*Svetlana Pavlitska,Beyza Keskin,Alwin Faßbender,Christian Hubschneider,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出使用混合专家模型(MoE)从计算机视觉模型中提取准确且良好校准的预测不确定性估计，相比传统集成方法更高效可靠


<details>
  <summary>Details</summary>
Motivation: 在安全关键应用如交通场景感知中，需要准确可靠的不确定性估计来增强计算机视觉模型的可靠性。传统集成方法计算成本高，MoE提供了更高效的替代方案

Method: 研究三种从MoE中提取预测不确定性的方法：预测熵、互信息和专家方差。使用A2D2数据集训练双专家MoE，并评估在分布外数据下的性能

Result: MoE比集成方法产生更可靠的不确定性估计，在条件正确性指标上表现更好。简单门控机制比复杂类别门控产生更好的路由不确定性校准。增加专家数量可进一步提升不确定性校准

Conclusion: MoE无需架构修改即可提供良好校准的不确定性估计，是比传统集成方法更高效的替代方案，在安全关键计算机视觉应用中具有重要价值

Abstract: Estimating accurate and well-calibrated predictive uncertainty is important
for enhancing the reliability of computer vision models, especially in
safety-critical applications like traffic scene perception. While ensemble
methods are commonly used to quantify uncertainty by combining multiple models,
a mixture of experts (MoE) offers an efficient alternative by leveraging a
gating network to dynamically weight expert predictions based on the input.
Building on the promising use of MoEs for semantic segmentation in our previous
works, we show that well-calibrated predictive uncertainty estimates can be
extracted from MoEs without architectural modifications. We investigate three
methods to extract predictive uncertainty estimates: predictive entropy, mutual
information, and expert variance. We evaluate these methods for an MoE with two
experts trained on a semantical split of the A2D2 dataset. Our results show
that MoEs yield more reliable uncertainty estimates than ensembles in terms of
conditional correctness metrics under out-of-distribution (OOD) data.
Additionally, we evaluate routing uncertainty computed via gate entropy and
find that simple gating mechanisms lead to better calibration of routing
uncertainty estimates than more complex classwise gates. Finally, our
experiments on the Cityscapes dataset suggest that increasing the number of
experts can further enhance uncertainty calibration. Our code is available at
https://github.com/KASTEL-MobilityLab/mixtures-of-experts/.

</details>


### [23] [Exploring Non-Local Spatial-Angular Correlations with a Hybrid Mamba-Transformer Framework for Light Field Super-Resolution](https://arxiv.org/abs/2509.04824)
*Haosong Liu,Xiancheng Zhu,Huanqiang Zeng,Jianqing Zhu,Jiuwen Cao,Junhui Hou*

Main category: cs.CV

TL;DR: 提出基于Mamba的LFMT框架，通过Sub-SS策略和双阶段建模，在光场图像超分辨率任务中实现更高效的特征提取和性能提升


<details>
  <summary>Details</summary>
Motivation: 当前Mamba方法在多方向扫描策略应用于复杂光场数据时存在特征提取效率低下和冗余的问题，需要更高效的特征提取方法来优化计算成本和性能

Method: 提出Subspace Simple Scanning策略和Subspace Simple Mamba Block；采用双阶段建模：第一阶段使用SA-RSMB进行浅层空间-角度特征提取，第二阶段使用EPMB和EPTB双分支并行结构进行深层极线特征精炼

Result: LFMT在真实和合成光场数据集上显著优于当前最先进方法，在保持低计算复杂度的同时实现了性能的实质性提升

Conclusion: LFMT成功整合了Mamba和Transformer的优势，实现了跨空间、角度和极线平面域的全面信息探索，为光场图像超分辨率提供了有效的解决方案

Abstract: Recently, Mamba-based methods, with its advantage in long-range information
modeling and linear complexity, have shown great potential in optimizing both
computational cost and performance of light field image super-resolution
(LFSR). However, current multi-directional scanning strategies lead to
inefficient and redundant feature extraction when applied to complex LF data.
To overcome this challenge, we propose a Subspace Simple Scanning (Sub-SS)
strategy, based on which we design the Subspace Simple Mamba Block (SSMB) to
achieve more efficient and precise feature extraction. Furthermore, we propose
a dual-stage modeling strategy to address the limitation of state space in
preserving spatial-angular and disparity information, thereby enabling a more
comprehensive exploration of non-local spatial-angular correlations.
Specifically, in stage I, we introduce the Spatial-Angular Residual Subspace
Mamba Block (SA-RSMB) for shallow spatial-angular feature extraction; in stage
II, we use a dual-branch parallel structure combining the Epipolar Plane Mamba
Block (EPMB) and Epipolar Plane Transformer Block (EPTB) for deep epipolar
feature refinement. Building upon meticulously designed modules and strategies,
we introduce a hybrid Mamba-Transformer framework, termed LFMT. LFMT integrates
the strengths of Mamba and Transformer models for LFSR, enabling comprehensive
information exploration across spatial, angular, and epipolar-plane domains.
Experimental results demonstrate that LFMT significantly outperforms current
state-of-the-art methods in LFSR, achieving substantial improvements in
performance while maintaining low computational complexity on both real-word
and synthetic LF datasets.

</details>


### [24] [PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination](https://arxiv.org/abs/2509.04833)
*Ming Dai,Wenxuan Cheng,Jiedong Zhuang,Jiang-jiang Liu,Hongshen Zhao,Zhenhua Feng,Wankou Yang*

Main category: cs.CV

TL;DR: PropVG是一种端到端的基于候选框的视觉定位框架，首次将前景目标候选框生成与指代目标理解无缝集成，无需额外检测器，并通过对比学习和多粒度判别提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有端到端视觉定位方法仅依赖指代目标监督而忽略潜在前景目标的问题，以及缺乏多粒度判别能力，在复杂场景中目标识别不够鲁棒的局限性。

Method: 提出PropVG框架，包含Contrastive-based Refer Scoring (CRS)模块（在句子和单词级别进行对比学习）和Multi-granularity Target Discrimination (MTD)模块（融合对象和语义级别信息）。

Result: 在gRefCOCO、Ref-ZOM、R-RefCOCO和RefCOCO等多个基准测试上进行了广泛实验，证明了方法的有效性。

Conclusion: PropVG通过集成候选框生成和对比学习机制，显著提升了视觉定位的性能，特别是在复杂场景中的目标识别能力。

Abstract: Recent advances in visual grounding have largely shifted away from
traditional proposal-based two-stage frameworks due to their inefficiency and
high computational complexity, favoring end-to-end direct reference paradigms.
However, these methods rely exclusively on the referred target for supervision,
overlooking the potential benefits of prominent prospective targets. Moreover,
existing approaches often fail to incorporate multi-granularity discrimination,
which is crucial for robust object identification in complex scenarios. To
address these limitations, we propose PropVG, an end-to-end proposal-based
framework that, to the best of our knowledge, is the first to seamlessly
integrate foreground object proposal generation with referential object
comprehension without requiring additional detectors. Furthermore, we introduce
a Contrastive-based Refer Scoring (CRS) module, which employs contrastive
learning at both sentence and word levels to enhance the capability in
understanding and distinguishing referred objects. Additionally, we design a
Multi-granularity Target Discrimination (MTD) module that fuses object- and
semantic-level information to improve the recognition of absent targets.
Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO
(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes and
models are available at https://github.com/Dmmm1997/PropVG.

</details>


### [25] [TemporalFlowViz: Parameter-Aware Visual Analytics for Interpreting Scramjet Combustion Evolution](https://arxiv.org/abs/2509.04834)
*Yifei Jia,Shiyu Cheng,Yu Dong,Guan Li,Dong Tian,Ruixiao Peng,Xuyi Lu,Yu Wang,Wei Yao,Guihua Shan*

Main category: cs.CV

TL;DR: TemporalFlowViz是一个用于分析超燃冲压发动机燃烧模拟时序流场的可视化分析系统，通过ViT提取特征、降维聚类、轨迹追踪和视觉语言模型生成自然语言描述，支持专家驱动的燃烧模式发现和知识发现。


<details>
  <summary>Details</summary>
Motivation: 超燃冲压发动机燃烧模拟产生的时序流场数据规模大、维度高，给可视化解释、特征区分和跨案例比较带来巨大挑战，需要专门的分析工具来支持专家理解复杂的燃烧动力学。

Method: 使用预训练Vision Transformers提取高维嵌入特征，应用降维和基于密度的聚类发现潜在燃烧模式，构建嵌入空间中的时序轨迹追踪模拟演化，通过专家标注和视觉语言模型生成自然语言描述，支持参数过滤和相似性检索。

Result: 通过两个专家案例研究和专家反馈证明，TemporalFlowViz能够增强假设生成、支持可解释模式发现，并在大规模超燃冲压发动机燃烧分析中促进知识发现。

Conclusion: TemporalFlowViz提供了一个有效的可视化分析工作流，成功解决了高维时序流场数据的分析挑战，为超燃冲压发动机燃烧动力学研究提供了强有力的工具支持。

Abstract: Understanding the complex combustion dynamics within scramjet engines is
critical for advancing high-speed propulsion technologies. However, the large
scale and high dimensionality of simulation-generated temporal flow field data
present significant challenges for visual interpretation, feature
differentiation, and cross-case comparison. In this paper, we present
TemporalFlowViz, a parameter-aware visual analytics workflow and system
designed to support expert-driven clustering, visualization, and interpretation
of temporal flow fields from scramjet combustion simulations. Our approach
leverages hundreds of simulated combustion cases with varying initial
conditions, each producing time-sequenced flow field images. We use pretrained
Vision Transformers to extract high-dimensional embeddings from these frames,
apply dimensionality reduction and density-based clustering to uncover latent
combustion modes, and construct temporal trajectories in the embedding space to
track the evolution of each simulation over time. To bridge the gap between
latent representations and expert reasoning, domain specialists annotate
representative cluster centroids with descriptive labels. These annotations are
used as contextual prompts for a vision-language model, which generates
natural-language summaries for individual frames and full simulation cases. The
system also supports parameter-based filtering, similarity-based case
retrieval, and coordinated multi-view exploration to facilitate in-depth
analysis. We demonstrate the effectiveness of TemporalFlowViz through two
expert-informed case studies and expert feedback, showing TemporalFlowViz
enhances hypothesis generation, supports interpretable pattern discovery, and
enhances knowledge discovery in large-scale scramjet combustion analysis.

</details>


### [26] [Pose-Free 3D Quantitative Phase Imaging of Flowing Cellular Populations](https://arxiv.org/abs/2509.04848)
*Enze Ye,Wei Lin,Shaochi Ren,Yakun Liu,Xiaoping Li,Hao Wang,He Sun,Feng Pan*

Main category: cs.CV

TL;DR: OmniFHT是一个基于傅里叶衍射定理和隐式神经表示的3D折射率重建框架，无需已知细胞姿态即可实现高通量流式细胞术断层成像，支持任意几何形状和多轴旋转的细胞。


<details>
  <summary>Details</summary>
Motivation: 现有3D定量相位成像方法假设细胞进行均匀单轴旋转且需要知道每帧姿态，这限制了只能分析近球形细胞，无法准确成像不规则形状和复杂旋转的细胞，限制了流式分析的统计能力。

Method: 采用傅里叶衍射定理和隐式神经表示(INRs)，在弱散射假设下联合优化细胞的未知旋转轨迹和体积结构，支持稀疏采样投影和受限角度范围的重建。

Result: OmniFHT能够仅用10个视图或120度角度范围就产生高保真结果，首次实现了对整个流动细胞群体的原位高通量断层成像。

Conclusion: 该框架为流式细胞术平台提供了可扩展且无偏的无标记形态计量分析解决方案，突破了现有方法的限制。

Abstract: High-throughput 3D quantitative phase imaging (QPI) in flow cytometry enables
label-free, volumetric characterization of individual cells by reconstructing
their refractive index (RI) distributions from multiple viewing angles during
flow through microfluidic channels. However, current imaging methods assume
that cells undergo uniform, single-axis rotation, which require their poses to
be known at each frame. This assumption restricts applicability to
near-spherical cells and prevents accurate imaging of irregularly shaped cells
with complex rotations. As a result, only a subset of the cellular population
can be analyzed, limiting the ability of flow-based assays to perform robust
statistical analysis. We introduce OmniFHT, a pose-free 3D RI reconstruction
framework that leverages the Fourier diffraction theorem and implicit neural
representations (INRs) for high-throughput flow cytometry tomographic imaging.
By jointly optimizing each cell's unknown rotational trajectory and volumetric
structure under weak scattering assumptions, OmniFHT supports arbitrary cell
geometries and multi-axis rotations. Its continuous representation also allows
accurate reconstruction from sparsely sampled projections and restricted
angular coverage, producing high-fidelity results with as few as 10 views or
only 120 degrees of angular range. OmniFHT enables, for the first time, in
situ, high-throughput tomographic imaging of entire flowing cell populations,
providing a scalable and unbiased solution for label-free morphometric analysis
in flow cytometry platforms.

</details>


### [27] [CoRe-GS: Coarse-to-Refined Gaussian Splatting with Semantic Object Focus](https://arxiv.org/abs/2509.04859)
*Hannah Schieber,Dominik Frischmann,Simon Boche,Victor Schaack,Angela Schoellig,Stefan Leutenegger,Daniel Roth*

Main category: cs.CV

TL;DR: CoRe-GS是一种基于语义高斯溅射的移动重建方法，通过粗分割和颜色过滤实现快速高质量的对象重建，训练时间比完整语义GS减少约四分之一


<details>
  <summary>Details</summary>
Motivation: 移动机器人应用（如远程指导和灾难响应）需要快速准确的3D重建，但传统方法需要完整场景优化，效率低下。通过专注于特定兴趣点对象可以显著提高效率

Method: 首先使用语义高斯溅射生成粗分割场景，然后通过新颖的基于颜色的有效过滤方法对语义对象进行细化，实现有效的对象隔离

Result: 在两个数据集（SCRREAM真实室外和NeRDS 360合成室内）上评估，显示运行时间减少且新视角合成质量更高，训练过程比完整语义GS训练周期快约四分之一

Conclusion: CoRe-GS方法在保持高质量重建的同时显著减少了训练时间，为移动机器人应用提供了高效的3D对象重建解决方案

Abstract: Mobile reconstruction for autonomous aerial robotics holds strong potential
for critical applications such as tele-guidance and disaster response. These
tasks demand both accurate 3D reconstruction and fast scene processing. Instead
of reconstructing the entire scene in detail, it is often more efficient to
focus on specific objects, i.e., points of interest (PoIs). Mobile robots
equipped with advanced sensing can usually detect these early during data
acquisition or preliminary analysis, reducing the need for full-scene
optimization. Gaussian Splatting (GS) has recently shown promise in delivering
high-quality novel view synthesis and 3D representation by an incremental
learning process. Extending GS with scene editing, semantics adds useful
per-splat features to isolate objects effectively.
  Semantic 3D Gaussian editing can already be achieved before the full training
cycle is completed, reducing the overall training time. Moreover, the
semantically relevant area, the PoI, is usually already known during capturing.
To balance high-quality reconstruction with reduced training time, we propose
CoRe-GS. We first generate a coarse segmentation-ready scene with semantic GS
and then refine it for the semantic object using our novel color-based
effective filtering for effective object isolation. This is speeding up the
training process to be about a quarter less than a full training cycle for
semantic GS. We evaluate our approach on two datasets, SCRREAM (real-world,
outdoor) and NeRDS 360 (synthetic, indoor), showing reduced runtime and higher
novel-view-synthesis quality.

</details>


### [28] [Cryo-RL: automating prostate cancer cryoablation planning with reinforcement learning](https://arxiv.org/abs/2509.04886)
*Trixia Simangan,Ahmed Nadeem Abbasi,Yipeng Hu,Shaheer U. Saeed*

Main category: cs.CV

TL;DR: Cryo-RL是一个基于强化学习的冷冻消融规划框架，通过模拟临床环境自动优化冷冻探针放置策略，在583例前列腺癌病例中达到专家水平性能，且规划时间大幅减少。


<details>
  <summary>Details</summary>
Motivation: 当前前列腺癌冷冻消融治疗的术前规划依赖人工经验，存在耗时长、质量不一和可扩展性有限的问题，需要自动化解决方案来提高治疗质量和效率。

Method: 将冷冻消融规划建模为马尔可夫决策过程，在模拟临床约束和术中变异性的环境中，通过强化学习代理顺序选择冷冻探针位置和冰球直径，基于肿瘤覆盖率的奖励函数学习最优策略。

Result: 在583例回顾性前列腺癌病例评估中，Cryo-RL相比基于几何优化的最佳自动化基线方法Dice系数提高了8个百分点以上，达到人类专家性能水平，且规划时间显著减少。

Conclusion: 强化学习能够提供临床可行、可重复且高效的冷冻消融规划方案，具有重要的临床应用潜力。

Abstract: Cryoablation is a minimally invasive localised treatment for prostate cancer
that destroys malignant tissue during de-freezing, while sparing surrounding
healthy structures. Its success depends on accurate preoperative planning of
cryoprobe placements to fully cover the tumour and avoid critical anatomy. This
planning is currently manual, expertise-dependent, and time-consuming, leading
to variability in treatment quality and limited scalability. In this work, we
introduce Cryo-RL, a reinforcement learning framework that models cryoablation
planning as a Markov decision process and learns an optimal policy for
cryoprobe placement. Within a simulated environment that models clinical
constraints and stochastic intraoperative variability, an agent sequentially
selects cryoprobe positions and ice sphere diameters. Guided by a reward
function based on tumour coverage, this agent learns a cryoablation strategy
that leads to optimal cryoprobe placements without the need for any
manually-designed plans. Evaluated on 583 retrospective prostate cancer cases,
Cryo-RL achieved over 8 percentage-point Dice improvements compared with the
best automated baselines, based on geometric optimisation, and matched human
expert performance while requiring substantially less planning time. These
results highlight the potential of reinforcement learning to deliver clinically
viable, reproducible, and efficient cryoablation plans.

</details>


### [29] [SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models](https://arxiv.org/abs/2509.04889)
*Dominik Pegler,David Steyrl,Mengfan Zhang,Alexander Karner,Jozsef Arato,Frank Scharnowski,Filip Melinscak*

Main category: cs.CV

TL;DR: 本研究通过迁移学习将预训练计算机视觉模型用于预测蜘蛛图像引发的恐惧程度，平均绝对误差在10.1-11.0之间，证明了计算机视觉在情绪感知治疗技术中的潜力。


<details>
  <summary>Details</summary>
Motivation: 计算机视觉的进步为临床应用开辟了新途径，特别是在计算机化暴露疗法中，可以根据患者反应动态调整视觉刺激。本研究旨在探索预训练计算机视觉模型是否能准确预测蜘蛛相关图像的恐惧水平。

Method: 使用迁移学习适配三种不同的预训练计算机视觉模型，预测人类对313张标准化图像集的恐惧评分（0-100分制）。采用交叉验证评估模型性能，并进行学习曲线分析和可解释性评估。

Result: 模型平均绝对误差（MAE）在10.1到11.0之间。学习曲线分析显示减少数据集大小会显著损害性能，但进一步增加数据集大小没有带来实质性改善。可解释性评估表明模型预测基于蜘蛛相关特征。类别错误分析识别出与较高错误相关的视觉条件（如远距离视图和人造/绘画蜘蛛）。

Conclusion: 研究结果证明了可解释计算机视觉模型在预测恐惧评分方面的潜力，强调了模型可解释性和足够数据集大小对于开发有效情绪感知治疗技术的重要性。

Abstract: Advances in computer vision have opened new avenues for clinical
applications, particularly in computerized exposure therapy where visual
stimuli can be dynamically adjusted based on patient responses. As a critical
step toward such adaptive systems, we investigated whether pretrained computer
vision models can accurately predict fear levels from spider-related images. We
adapted three diverse models using transfer learning to predict human fear
ratings (on a 0-100 scale) from a standardized dataset of 313 images. The
models were evaluated using cross-validation, achieving an average mean
absolute error (MAE) between 10.1 and 11.0. Our learning curve analysis
revealed that reducing the dataset size significantly harmed performance,
though further increases yielded no substantial gains. Explainability
assessments showed the models' predictions were based on spider-related
features. A category-wise error analysis further identified visual conditions
associated with higher errors (e.g., distant views and artificial/painted
spiders). These findings demonstrate the potential of explainable computer
vision models in predicting fear ratings, highlighting the importance of both
model explainability and a sufficient dataset size for developing effective
emotion-aware therapeutic technologies.

</details>


### [30] [SynGen-Vision: Synthetic Data Generation for training industrial vision models](https://arxiv.org/abs/2509.04894)
*Alpana Dubey,Suma Mani Kuriakose,Nitish Bhardwaj*

Main category: cs.CV

TL;DR: 使用视觉语言模型和3D仿真引擎生成锈蚀合成数据，用于训练工业磨损检测CV模型，在真实测试中达到0.87 mAP50分数


<details>
  <summary>Details</summary>
Motivation: 工业磨损检测对预测性维护很重要，但真实数据收集成本高且耗时，缺乏不同磨损场景的数据集

Method: 结合视觉语言模型和3D仿真渲染引擎，生成不同锈蚀条件的合成数据来训练CV模型

Result: 使用合成数据训练的锈蚀检测模型在真实图像测试中表现优异，mAP50分数达到0.87，优于其他方法

Conclusion: 该方法可定制且易于扩展到其他工业磨损检测场景，为缺乏真实数据的CV问题提供了有效的合成数据解决方案

Abstract: We propose an approach to generate synthetic data to train computer vision
(CV) models for industrial wear and tear detection. Wear and tear detection is
an important CV problem for predictive maintenance tasks in any industry.
However, data curation for training such models is expensive and time-consuming
due to the unavailability of datasets for different wear and tear scenarios.
Our approach employs a vision language model along with a 3D simulation and
rendering engine to generate synthetic data for varying rust conditions. We
evaluate our approach by training a CV model for rust detection using the
generated dataset and tested the trained model on real images of rusted
industrial objects. The model trained with the synthetic data generated by our
approach, outperforms the other approaches with a mAP50 score of 0.87. The
approach is customizable and can be easily extended to other industrial wear
and tear detection scenarios

</details>


### [31] [Evaluating Multiple Instance Learning Strategies for Automated Sebocyte Droplet Counting](https://arxiv.org/abs/2509.04895)
*Maryam Adelipour,Gustavo Carneiro,Jeongkwon Kim*

Main category: cs.CV

TL;DR: 提出基于注意力机制的多实例学习框架用于皮脂腺细胞脂滴计数，发现简单的聚合方法比复杂的注意力模型更稳定可靠


<details>
  <summary>Details</summary>
Motivation: 皮脂腺细胞脂滴计数是研究其分化的关键指标，手动计数费时且主观，需要自动化解决方案

Method: 使用尼罗红染色图像，标注14个脂滴数量类别，通过数据增强扩展到5万个细胞。比较了基于聚合的多层感知机基线模型和基于注意力的多实例学习模型

Result: 基线MLP模型表现更稳定（平均MAE=5.6），注意力MIL模型一致性较差（平均MAE=10.7）但在某些fold中表现更好

Conclusion: 简单的袋级聚合为玻片级脂滴计数提供了稳健基线，注意力MIL需要任务对齐的池化和正则化才能充分发挥潜力

Abstract: Sebocytes are lipid-secreting cells whose differentiation is marked by the
accumulation of intracellular lipid droplets, making their quantification a key
readout in sebocyte biology. Manual counting is labor-intensive and subjective,
motivating automated solutions. Here, we introduce a simple attention-based
multiple instance learning (MIL) framework for sebocyte image analysis. Nile
Red-stained sebocyte images were annotated into 14 classes according to droplet
counts, expanded via data augmentation to about 50,000 cells. Two models were
benchmarked: a baseline multi-layer perceptron (MLP) trained on aggregated
patch-level counts, and an attention-based MIL model leveraging ResNet-50
features with instance weighting. Experiments using five-fold cross-validation
showed that the baseline MLP achieved more stable performance (mean MAE = 5.6)
compared with the attention-based MIL, which was less consistent (mean MAE =
10.7) but occasionally superior in specific folds. These findings indicate that
simple bag-level aggregation provides a robust baseline for slide-level droplet
counting, while attention-based MIL requires task-aligned pooling and
regularization to fully realize its potential in sebocyte image analysis.

</details>


### [32] [UniView: Enhancing Novel View Synthesis From A Single Image By Unifying Reference Features](https://arxiv.org/abs/2509.04932)
*Haowang Cui,Rui Chen,Tao Luo,Rui Li,Jiaze Wang*

Main category: cs.CV

TL;DR: UniView模型通过检索相似物体的参考图像来解决单图像新视角合成中的模糊性问题，利用MLLM选择参考图像并采用多级隔离层和三元注意力机制来提升合成质量。


<details>
  <summary>Details</summary>
Motivation: 单图像新视角合成任务存在严重模糊性，现有方法基于模糊先验和插值往往导致严重失真，需要引入外部参考信息来提供强先验。

Method: 构建检索增强系统，使用多模态大语言模型选择参考图像；设计带多级隔离层的即插即用适配器模块动态生成参考特征；采用解耦的三元注意力机制对齐和整合多分支特征。

Result: 在多个挑战性数据集上的大量实验表明，UniView显著提升了新视角合成性能，优于现有最先进方法。

Conclusion: 通过引入相似物体的参考图像作为强先验信息，UniView有效解决了单图像视角合成中的模糊性问题，实现了更好的合成效果。

Abstract: The task of synthesizing novel views from a single image is highly ill-posed
due to multiple explanations for unobserved areas. Most current methods tend to
generate unseen regions from ambiguity priors and interpolation near input
views, which often lead to severe distortions. To address this limitation, we
propose a novel model dubbed as UniView, which can leverage reference images
from a similar object to provide strong prior information during view
synthesis. More specifically, we construct a retrieval and augmentation system
and employ a multimodal large language model (MLLM) to assist in selecting
reference images that meet our requirements. Additionally, a plug-and-play
adapter module with multi-level isolation layers is introduced to dynamically
generate reference features for the target views. Moreover, in order to
preserve the details of an original input image, we design a decoupled triple
attention mechanism, which can effectively align and integrate multi-branch
features into the synthesis process. Extensive experiments have demonstrated
that our UniView significantly improves novel view synthesis performance and
outperforms state-of-the-art methods on the challenging datasets.

</details>


### [33] [Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper](https://arxiv.org/abs/2509.04957)
*Gehui Chen,Guan'an Wang,Xiaowen Huang,Jitao Sang*

Main category: cs.CV

TL;DR: MFM-Mapper是一种高效的多基础模型映射器，通过融合双视觉编码器特征和使用GPT-2进行特征对齐，显著提升了视频到音频生成的性能，同时大幅降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: 现有视频到音频生成方法需要大量资源训练，而利用基础模型的知识迁移能力可以降低训练成本。受先前轻量级映射器工作的启发，希望开发更高效的解决方案。

Method: 提出MFM-Mapper方法：1）融合双视觉编码器的特征以获得更丰富的语义和时间信息；2）用GPT-2替换线性映射器，将跨模态特征映射视为自回归翻译任务来改善特征对齐

Result: 方法表现出卓越的训练效率，仅需先前映射器方法16%的训练规模就能达到竞争性性能，在语义和时间一致性方面表现更好

Conclusion: MFM-Mapper通过多基础模型融合和GPT-2特征对齐，实现了高效且高性能的视频到音频生成，为资源受限的应用提供了有前景的解决方案

Abstract: Recent Video-to-Audio (V2A) generation relies on extracting semantic and
temporal features from video to condition generative models. Training these
models from scratch is resource intensive. Consequently, leveraging foundation
models (FMs) has gained traction due to their cross-modal knowledge transfer
and generalization capabilities. One prior work has explored fine-tuning a
lightweight mapper network to connect a pre-trained visual encoder with a
text-to-audio generation model for V2A. Inspired by this, we introduce the
Multiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapper
approach, MFM-Mapper benefits from richer semantic and temporal information by
fusing features from dual visual encoders. Furthermore, by replacing a linear
mapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallels
between cross-modal features mapping and autoregressive translation tasks. Our
MFM-Mapper exhibits remarkable training efficiency. It achieves better
performance in semantic and temporal consistency with fewer training consuming,
requiring only 16\% of the training scale compared to previous mapper-based
work, yet achieves competitive performance with models trained on a much larger
scale.

</details>


### [34] [Dual-Domain Perspective on Degradation-Aware Fusion: A VLM-Guided Robust Infrared and Visible Image Fusion Framework](https://arxiv.org/abs/2509.05000)
*Tianpei Zhang,Jufeng Zhao,Yiming Zhu,Guangmang Cui*

Main category: cs.CV

TL;DR: 提出GD^2Fusion框架，通过视觉语言模型感知退化并在双域（频域/空间域）联合优化，解决双源退化场景下的红外-可见光图像融合问题


<details>
  <summary>Details</summary>
Motivation: 现有IVIF方法假设高质量输入，无法处理双源退化场景，需要手动选择多个预增强步骤，导致误差累积和性能下降

Method: 设计GFMSE模块进行频域退化感知和抑制，提取融合相关子带特征；GSMAF模块在空间域进行跨模态退化过滤和自适应多源特征聚合

Result: 在双源退化场景下相比现有算法和策略实现了更优越的融合性能

Conclusion: GD^2Fusion通过VLMs和双域联合优化的协同集成，有效解决了双源退化图像融合的挑战

Abstract: Most existing infrared-visible image fusion (IVIF) methods assume
high-quality inputs, and therefore struggle to handle dual-source degraded
scenarios, typically requiring manual selection and sequential application of
multiple pre-enhancement steps. This decoupled pre-enhancement-to-fusion
pipeline inevitably leads to error accumulation and performance degradation. To
overcome these limitations, we propose Guided Dual-Domain Fusion (GD^2Fusion),
a novel framework that synergistically integrates vision-language models (VLMs)
for degradation perception with dual-domain (frequency/spatial) joint
optimization. Concretely, the designed Guided Frequency Modality-Specific
Extraction (GFMSE) module performs frequency-domain degradation perception and
suppression and discriminatively extracts fusion-relevant sub-band features.
Meanwhile, the Guided Spatial Modality-Aggregated Fusion (GSMAF) module carries
out cross-modal degradation filtering and adaptive multi-source feature
aggregation in the spatial domain to enhance modality complementarity and
structural consistency. Extensive qualitative and quantitative experiments
demonstrate that GD^2Fusion achieves superior fusion performance compared with
existing algorithms and strategies in dual-source degraded scenarios. The code
will be publicly released after acceptance of this paper.

</details>


### [35] [Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study](https://arxiv.org/abs/2509.05004)
*Mohammad Abbadi,Yassine Himeur,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 这篇论文研究了机器学习和深度学习技术在乳腺超声图像癌症分类中的应用，ResNet-18模型取得了99.7%的最高准确率和完美的恶性病变敏感性。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性癌症相关死亡的主要原因，超声成像因其安全性和成本效益在早期检测中发挥关键作用，特别是在致密乳腺组织患者中。

Method: 使用BUSI、BUS-BRA和BrEaST-Lesions USG数据集，评估了经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet），并采用Grad-CAM可视化提高模型透明度。

Result: ResNet-18达到最高准确率99.7%和完美的恶性病变敏感性。经典ML模型虽然性能不如CNN，但通过深度特征提取也能获得有竞争力的表现。

Conclusion: 研究结果支持将基于AI的诊断工具整合到临床工作流程中，证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。

Abstract: Breast cancer remains a leading cause of cancer-related mortality among women
worldwide. Ultrasound imaging, widely used due to its safety and
cost-effectiveness, plays a key role in early detection, especially in patients
with dense breast tissue. This paper presents a comprehensive study on the
application of machine learning and deep learning techniques for breast cancer
classification using ultrasound images. Using datasets such as BUSI, BUS-BRA,
and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,
KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,
GoogLeNet). Experimental results show that ResNet-18 achieves the highest
accuracy (99.7%) and perfect sensitivity for malignant lesions. Classical ML
models, though outperformed by CNNs, achieve competitive performance when
enhanced with deep feature extraction. Grad-CAM visualizations further improve
model transparency by highlighting diagnostically relevant image regions. These
findings support the integration of AI-based diagnostic tools into clinical
workflows and demonstrate the feasibility of deploying high-performing,
interpretable systems for ultrasound-based breast cancer detection.

</details>


### [36] [A biologically inspired separable learning vision model for real-time traffic object perception in Dark](https://arxiv.org/abs/2509.05012)
*Hulin Li,Qiliang Ren,Jun Li,Hanbing Wei,Zheng Liu,Linfang Fan*

Main category: cs.CV

TL;DR: 提出了首个大规模低光照交通场景数据集Dark-traffic和生物启发的可分学习视觉模型SLVM，在低光照环境下实现快速准确的目标感知，显著超越现有方法


<details>
  <summary>Details</summary>
Motivation: 解决低光照交通场景中因严重光照退化导致现有感知模型性能下降的问题，填补缺乏专门基准数据集的空白

Method: 提出物理真实的光照退化方法构建Dark-traffic数据集；设计SLVM模型包含光适应瞳孔机制、特征级可分学习策略、任务特定解耦分支和空间错位感知融合模块

Result: 在Dark-traffic数据集上检测性能超越RT-DETR 11.2个百分点，实例分割超越YOLOv12 6.1个百分点，光流端点误差降低12.37%；在LIS基准上平均超越Swin Transformer+EnlightenGAN等11个百分点

Conclusion: SLVM模型在低光照交通场景感知中达到最先进性能，计算开销更低，Dark-traffic数据集和完整代码已开源

Abstract: Fast and accurate object perception in low-light traffic scenes has attracted
increasing attention. However, due to severe illumination degradation and the
lack of reliable visual cues, existing perception models and methods struggle
to quickly adapt to and accurately predict in low-light environments. Moreover,
there is the absence of available large-scale benchmark specifically focused on
low-light traffic scenes. To bridge this gap, we introduce a physically
grounded illumination degradation method tailored to real-world low-light
settings and construct Dark-traffic, the largest densely annotated dataset to
date for low-light traffic scenes, supporting object detection, instance
segmentation, and optical flow estimation. We further propose the Separable
Learning Vision Model (SLVM), a biologically inspired framework designed to
enhance perception under adverse lighting. SLVM integrates four key components:
a light-adaptive pupillary mechanism for illumination-sensitive feature
extraction, a feature-level separable learning strategy for efficient
representation, task-specific decoupled branches for multi-task separable
learning, and a spatial misalignment-aware fusion module for precise
multi-feature alignment. Extensive experiments demonstrate that SLVM achieves
state-of-the-art performance with reduced computational overhead. Notably, it
outperforms RT-DETR by 11.2 percentage points in detection, YOLOv12 by 6.1
percentage points in instance segmentation, and reduces endpoint error (EPE) of
baseline by 12.37% on Dark-traffic. On the LIS benchmark, the end-to-end
trained SLVM surpasses Swin Transformer+EnlightenGAN and
ConvNeXt-T+EnlightenGAN by an average of 11 percentage points across key
metrics, and exceeds Mask RCNN (with light enhancement) by 3.1 percentage
points. The Dark-traffic dataset and complete code is released at
https://github.com/alanli1997/slvm.

</details>


### [37] [Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition](https://arxiv.org/abs/2509.05019)
*Mohsine El Khayati,Ayyad Maafiri,Yassine Himeur,Hamzah Ali Alkhazaleh,Shadi Atalla,Wathiq Mansoor*

Main category: cs.CV

TL;DR: 该研究探索了将迁移学习与移动卷积神经网络结合用于阿拉伯手写字符识别，MobileNet表现最佳，在IFHCDB数据集上达到99%准确率，全微调策略效果最好。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯手写字符识别中计算资源需求大和数据集稀缺的挑战，寻求资源高效的识别方案。

Method: 使用三种迁移学习策略（全微调、部分微调、从头训练）和四种轻量级移动网络（MobileNet、SqueezeNet、MnasNet、ShuffleNet），在三个基准数据集上进行实验评估。

Result: MobileNet表现最优，IFHCDB数据集上MnasNet达到99%准确率，AHCD数据集97%准确率，HIJJA数据集92%准确率。全微调策略效果最佳。

Conclusion: 迁移学习与移动网络的结合为资源高效的阿拉伯手写字符识别提供了有效解决方案，未来可进一步优化架构和增强模型鲁棒性。

Abstract: The study explores the integration of transfer learning (TL) with
mobile-enabled convolutional neural networks (MbNets) to enhance Arabic
Handwritten Character Recognition (AHCR). Addressing challenges like extensive
computational requirements and dataset scarcity, this research evaluates three
TL strategies--full fine-tuning, partial fine-tuning, and training from
scratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, and
ShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,
HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistently
achieving superior accuracy, robustness, and efficiency, with ShuffleNet
excelling in generalization, particularly under full fine-tuning. The IFHCDB
dataset yielded the highest results, with 99% accuracy using MnasNet under full
fine-tuning, highlighting its suitability for robust character recognition. The
AHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJA
posed significant challenges due to its variability, achieving a peak accuracy
of 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overall
performance, balancing accuracy and convergence speed, while partial
fine-tuning underperformed across metrics. These findings underscore the
potential of combining TL and MbNets for resource-efficient AHCR, paving the
way for further optimizations and broader applications. Future work will
explore architectural modifications, in-depth dataset feature analysis, data
augmentation, and advanced sensitivity analysis to enhance model robustness and
generalizability.

</details>


### [38] [LUIVITON: Learned Universal Interoperable VIrtual Try-ON](https://arxiv.org/abs/2509.05030)
*Cong Cao,Xianhang Cheng,Jingyuan Liu,Yujian Zheng,Zhenhui Lin,Meriem Chkir,Hao Li*

Main category: cs.CV

TL;DR: LUIVITON是一个端到端的全自动虚拟试衣系统，能够将复杂的多层服装披挂在多样化和任意姿态的人形角色上，无需人工干预即可生成高质量的3D服装拟合效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂服装与任意多样化身体形状对齐的挑战，特别是处理非流形网格和复杂几何形状，同时支持快速服装尺寸定制和材质属性调整。

Method: 使用SMPL作为代理表示，将服装到身体的披挂问题分解为两个对应任务：1）服装到SMPL的几何学习对应预测，2）基于扩散模型的身体到SMPL对应，利用多视角一致的外观特征和预训练的2D基础模型。

Result: 系统能够有效处理复杂几何形状，泛化到广泛的人形角色（包括人类、机器人、卡通角色、生物和外星人），并保持计算效率以实现实际应用。

Conclusion: LUIVITON提供了一个完全自动化的拟合解决方案，即使在2D服装缝制图案不可用的情况下，也能生成高质量的3D服装拟合效果，并支持服装尺寸和材质的快速定制。

Abstract: We present LUIVITON, an end-to-end system for fully automated virtual try-on,
capable of draping complex, multi-layer clothing onto diverse and arbitrarily
posed humanoid characters. To address the challenge of aligning complex
garments with arbitrary and highly diverse body shapes, we use SMPL as a proxy
representation and separate the clothing-to-body draping problem into two
correspondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,
where each has its unique challenges. While we address the clothing-to-SMPL
fitting problem using a geometric learning-based approach for
partial-to-complete shape correspondence prediction, we introduce a diffusion
model-based approach for body-to-SMPL correspondence using multi-view
consistent appearance features and a pre-trained 2D foundation model. Our
method can handle complex geometries, non-manifold meshes, and generalizes
effectively to a wide range of humanoid characters -- including humans, robots,
cartoon subjects, creatures, and aliens, while maintaining computational
efficiency for practical adoption. In addition to offering a fully automatic
fitting solution, LUIVITON supports fast customization of clothing size,
allowing users to adjust clothing sizes and material properties after they have
been draped. We show that our system can produce high-quality 3D clothing
fittings without any human labor, even when 2D clothing sewing patterns are not
available.

</details>


### [39] [Towards Efficient Pixel Labeling for Industrial Anomaly Detection and Localization](https://arxiv.org/abs/2509.05034)
*Jingqi Wu,Hanxi Li,Lin Yuanbo Wu,Hao Chen,Deyin Liu,Peng Wang*

Main category: cs.CV

TL;DR: ADClick是一个用于工业异常检测的交互式图像分割算法，通过少量用户点击和文本描述生成像素级异常标注，显著提升异常检测模型性能。ADClick-Seg进一步通过跨模态框架将视觉特征与文本提示对齐，在MVTec AD数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 工业产品检测通常仅使用正常样本训练异常检测模型，虽然可以收集缺陷样本，但需要像素级标注限制了可扩展性。需要一种能够高效生成精确标注的方法来提升异常检测性能。

Method: 提出ADClick交互式图像分割算法，通过少量用户点击和文本描述生成像素级异常标注。进一步开发ADClick-Seg跨模态框架，使用基于原型的方法对齐视觉特征和文本提示，结合像素级先验和语言引导线索。

Result: ADClick在MVTec AD上达到AP=96.1%的优异性能。ADClick-Seg在"多类别"异常检测任务上取得AP=80.0%、PRO=97.5%、Pixel-AUROC=99.1%的state-of-the-art结果。

Conclusion: ADClick和ADClick-Seg通过交互式标注和跨模态对齐，有效解决了工业异常检测中标注效率低的问题，显著提升了检测性能，为工业质量检测提供了高效的解决方案。

Abstract: Industrial product inspection is often performed using Anomaly Detection (AD)
frameworks trained solely on non-defective samples. Although defective samples
can be collected during production, leveraging them usually requires
pixel-level annotations, limiting scalability. To address this, we propose
ADClick, an Interactive Image Segmentation (IIS) algorithm for industrial
anomaly detection. ADClick generates pixel-wise anomaly annotations from only a
few user clicks and a brief textual description, enabling precise and efficient
labeling that significantly improves AD model performance (e.g., AP = 96.1\% on
MVTec AD). We further introduce ADClick-Seg, a cross-modal framework that
aligns visual features and textual prompts via a prototype-based approach for
anomaly detection and localization. By combining pixel-level priors with
language-guided cues, ADClick-Seg achieves state-of-the-art results on the
challenging ``Multi-class'' AD task (AP = 80.0\%, PRO = 97.5\%, Pixel-AUROC =
99.1\% on MVTec AD).

</details>


### [40] [Systematic Review and Meta-analysis of AI-driven MRI Motion Artifact Detection and Correction](https://arxiv.org/abs/2509.05071)
*Mojtaba Safari,Zach Eidex,Richard L. J. Qiu,Matthew Goette,Tonghe Wang,Xiaofeng Yang*

Main category: cs.CV

TL;DR: AI驱动方法特别是深度学习生成模型在MRI运动伪影检测与校正中显示出潜力，但仍面临泛化性有限、依赖配对数据和视觉失真等挑战。


<details>
  <summary>Details</summary>
Motivation: 系统评估人工智能方法在MRI运动伪影处理中的效果，分析当前发展状况、面临的挑战和未来研究方向。

Method: 采用系统性综述和荟萃分析方法，重点关注深度学习特别是生成模型在MRI运动伪影检测与校正中的应用，提取数据集、网络架构和性能指标的定量数据。

Result: 深度学习特别是生成模型在减少运动伪影和改善图像质量方面显示出前景，但存在泛化性有限、依赖配对训练数据和视觉失真风险等关键挑战。

Conclusion: AI驱动方法特别是深度学习生成模型在改善MRI图像质量方面具有显著潜力，但需要解决公共数据集缺乏、标准化报告协议和更先进自适应技术等关键挑战，以提升诊断准确性、降低医疗成本并改善患者护理结果。

Abstract: Background: To systematically review and perform a meta-analysis of
artificial intelligence (AI)-driven methods for detecting and correcting
magnetic resonance imaging (MRI) motion artifacts, assessing current
developments, effectiveness, challenges, and future research directions.
Methods: A comprehensive systematic review and meta-analysis were conducted,
focusing on deep learning (DL) approaches, particularly generative models, for
the detection and correction of MRI motion artifacts. Quantitative data were
extracted regarding utilized datasets, DL architectures, and performance
metrics. Results: DL, particularly generative models, show promise for reducing
motion artifacts and improving image quality; however, limited
generalizability, reliance on paired training data, and risk of visual
distortions remain key challenges that motivate standardized datasets and
reporting. Conclusions: AI-driven methods, particularly DL generative models,
show significant potential for improving MRI image quality by effectively
addressing motion artifacts. However, critical challenges must be addressed,
including the need for comprehensive public datasets, standardized reporting
protocols for artifact levels, and more advanced, adaptable DL techniques to
reduce reliance on extensive paired datasets. Addressing these aspects could
substantially enhance MRI diagnostic accuracy, reduce healthcare costs, and
improve patient care outcomes.

</details>


### [41] [GeoSplat: A Deep Dive into Geometry-Constrained Gaussian Splatting](https://arxiv.org/abs/2509.05075)
*Yangming Li,Chaoyu Liu,Lihao Liu,Simon Masnou,Carola-Bibian Schönlieb*

Main category: cs.CV

TL;DR: GeoSplat是一个几何约束优化框架，利用一阶和二阶几何量改进高斯溅射的整个训练流程，包括初始化、梯度更新和致密化，显著提升了性能


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用低阶几何先验（如法向量），且通过噪声敏感方法估计不可靠。需要更鲁棒的几何约束来改进高斯溅射优化

Method: 提出基于主曲率初始化高斯尺度，引入基于局部流形等几何结构的高效噪声鲁棒估计方法，提供动态几何先验

Result: 在多个数据集的新视角合成实验中，GeoSplat显著提升了高斯溅射性能，超越了之前的基线方法

Conclusion: 该框架成功将高阶几何先验整合到高斯溅射优化中，通过几何约束改进了整个训练流程，取得了更好的效果

Abstract: A few recent works explored incorporating geometric priors to regularize the
optimization of Gaussian splatting, further improving its performance. However,
those early studies mainly focused on the use of low-order geometric priors
(e.g., normal vector), and they are also unreliably estimated by
noise-sensitive methods, like local principal component analysis. To address
their limitations, we first present GeoSplat, a general geometry-constrained
optimization framework that exploits both first-order and second-order
geometric quantities to improve the entire training pipeline of Gaussian
splatting, including Gaussian initialization, gradient update, and
densification. As an example, we initialize the scales of 3D Gaussian
primitives in terms of principal curvatures, leading to a better coverage of
the object surface than random initialization. Secondly, based on certain
geometric structures (e.g., local manifold), we introduce efficient and
noise-robust estimation methods that provide dynamic geometric priors for our
framework. We conduct extensive experiments on multiple datasets for novel view
synthesis, showing that our framework: GeoSplat, significantly improves the
performance of Gaussian splatting and outperforms previous baselines.

</details>


### [42] [Scale-interaction transformer: a hybrid cnn-transformer model for facial beauty prediction](https://arxiv.org/abs/2509.05078)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出Scale-Interaction Transformer (SIT)混合架构，结合CNN特征提取和Transformer关系建模能力，在面部美颜预测任务上达到新的SOTA性能


<details>
  <summary>Details</summary>
Motivation: 传统CNN在固定尺度处理特征，难以捕捉不同粒度特征间的相互依赖关系，而面部美颜预测需要理解多尺度视觉线索的复杂交互

Method: 使用多尺度模块并行卷积捕获不同感受野的面部特征，然后将多尺度表示构建为序列，通过Transformer编码器的自注意力机制显式建模特征间交互关系

Result: 在SCUT-FBP5500基准数据集上获得0.9187的皮尔逊相关系数，超越了先前所有方法

Conclusion: 显式建模多尺度视觉线索的交互对于高性能面部美颜预测至关重要，混合CNN-Transformer架构在需要整体上下文理解的复杂图像回归任务中具有巨大潜力

Abstract: Automated Facial Beauty Prediction (FBP) is a challenging computer vision
task due to the complex interplay of local and global facial features that
influence human perception. While Convolutional Neural Networks (CNNs) excel at
feature extraction, they often process information at a fixed scale,
potentially overlooking the critical inter-dependencies between features at
different levels of granularity. To address this limitation, we introduce the
Scale-Interaction Transformer (SIT), a novel hybrid deep learning architecture
that synergizes the feature extraction power of CNNs with the relational
modeling capabilities of Transformers. The SIT first employs a multi-scale
module with parallel convolutions to capture facial characteristics at varying
receptive fields. These multi-scale representations are then framed as a
sequence and processed by a Transformer encoder, which explicitly models their
interactions and contextual relationships via a self-attention mechanism. We
conduct extensive experiments on the widely-used SCUT-FBP5500 benchmark
dataset, where the proposed SIT model establishes a new state-of-the-art. It
achieves a Pearson Correlation of 0.9187, outperforming previous methods. Our
findings demonstrate that explicitly modeling the interplay between multi-scale
visual cues is crucial for high-performance FBP. The success of the SIT
architecture highlights the potential of hybrid CNN-Transformer models for
complex image regression tasks that demand a holistic, context-aware
understanding.

</details>


### [43] [Robust Experts: the Effect of Adversarial Training on CNNs with Sparse Mixture-of-Experts Layers](https://arxiv.org/abs/2509.05086)
*Svetlana Pavlitska,Haixi Fan,Konstantin Ditschuneit,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 使用稀疏专家混合层提升CNN对抗攻击鲁棒性，在ResNet架构中插入单个MoE层结合对抗训练可显著提高鲁棒性，但存在专家使用不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前CNN对抗攻击鲁棒性提升方法通常需要大量资源，探索通过稀疏MoE层增加模型容量而不增加推理成本的方法。

Method: 在ResNet架构的深层阶段插入单个MoE层替换残差块或卷积层，结合对抗训练，并使用switch loss进行专家平衡。

Result: 在CIFAR-100上，结合对抗训练后对PGD和AutoPGD攻击的鲁棒性有持续提升，但发现switch loss导致专家使用不平衡，部分单个专家的鲁棒性甚至超过整个MoE模型。

Conclusion: 稀疏MoE层可有效提升模型鲁棒性，但需要更好的专家平衡机制；鲁棒性子路径通过专家特化自然涌现。

Abstract: Robustifying convolutional neural networks (CNNs) against adversarial attacks
remains challenging and often requires resource-intensive countermeasures. We
explore the use of sparse mixture-of-experts (MoE) layers to improve robustness
by replacing selected residual blocks or convolutional layers, thereby
increasing model capacity without additional inference cost. On ResNet
architectures trained on CIFAR-100, we find that inserting a single MoE layer
in the deeper stages leads to consistent improvements in robustness under PGD
and AutoPGD attacks when combined with adversarial training. Furthermore, we
discover that when switch loss is used for balancing, it causes routing to
collapse onto a small set of overused experts, thereby concentrating
adversarial training on these paths and inadvertently making them more robust.
As a result, some individual experts outperform the gated MoE model in
robustness, suggesting that robust subpaths emerge through specialization. Our
code is available at https://github.com/KASTEL-MobilityLab/robust-sparse-moes.

</details>


### [44] [Semi-supervised Deep Transfer for Regression without Domain Alignment](https://arxiv.org/abs/2509.05092)
*Mainak Biswas,Ambedkar Dukkipati,Devarajan Sridharan*

Main category: cs.CV

TL;DR: 提出了CRAFT方法，用于源数据不可用、标记目标数据稀缺的回归任务中的源自由半监督领域自适应，在神经科学和医学应用中表现优异


<details>
  <summary>Details</summary>
Motivation: 解决现实应用中源模型无法很好泛化到领域偏移目标数据的问题，特别是在源数据无法共享（隐私或计算成本）且标记目标数据稀缺的连续值输出场景

Method: 基于Contradistinguisher框架，开发CRAFT方法，通过矛盾区分器正则化方法进行源自由半监督迁移，无需中间表示对齐

Result: 在两个神经科学数据集上，CRAFT比微调模型在RMSE上提升达9%，比四种最先进的源自由领域自适应模型性能高出3%以上

Conclusion: CRAFT是生物学和医学中普遍存在的回归任务的源自由半监督深度迁移的高效方法

Abstract: Deep learning models deployed in real-world applications (e.g., medicine)
face challenges because source models do not generalize well to domain-shifted
target data. Many successful domain adaptation (DA) approaches require full
access to source data. Yet, such requirements are unrealistic in scenarios
where source data cannot be shared either because of privacy concerns or
because it is too large and incurs prohibitive storage or computational costs.
Moreover, resource constraints may limit the availability of labeled targets.
We illustrate this challenge in a neuroscience setting where source data are
unavailable, labeled target data are meager, and predictions involve
continuous-valued outputs. We build upon Contradistinguisher (CUDA), an
efficient framework that learns a shared model across the labeled source and
unlabeled target samples, without intermediate representation alignment. Yet,
CUDA was designed for unsupervised DA, with full access to source data, and for
classification tasks. We develop CRAFT -- a Contradistinguisher-based
Regularization Approach for Flexible Training -- for source-free (SF),
semi-supervised transfer of pretrained models in regression tasks. We showcase
the efficacy of CRAFT in two neuroscience settings: gaze prediction with
electroencephalography (EEG) data and ``brain age'' prediction with structural
MRI data. For both datasets, CRAFT yielded up to 9% improvement in
root-mean-squared error (RMSE) over fine-tuned models when labeled training
examples were scarce. Moreover, CRAFT leveraged unlabeled target data and
outperformed four competing state-of-the-art source-free domain adaptation
models by more than 3%. Lastly, we demonstrate the efficacy of CRAFT on two
other real-world regression benchmarks. We propose CRAFT as an efficient
approach for source-free, semi-supervised deep transfer for regression that is
ubiquitous in biology and medicine.

</details>


### [45] [A Scalable Attention-Based Approach for Image-to-3D Texture Mapping](https://arxiv.org/abs/2509.05131)
*Arianna Rampini,Kanika Madan,Bruno Roy,AmirHossein Zamani,Derek Cheung*

Main category: cs.CV

TL;DR: 提出基于Transformer的框架，直接从单张图像和网格预测3D纹理场，无需UV映射和可微分渲染，实现快速纹理生成（0.2秒/形状）


<details>
  <summary>Details</summary>
Motivation: 现有纹理生成方法速度慢、依赖UV映射、且难以保持参考图像保真度，需要更高效高质量的解决方案

Method: 集成triplane表示与基于深度的反向投影损失，通过Transformer架构直接预测3D纹理场

Result: 在单图像纹理重建任务中，在保真度和感知质量方面均优于最先进基线方法

Conclusion: 该方法为可扩展、高质量、可控的3D内容创建提供了实用解决方案

Abstract: High-quality textures are critical for realistic 3D content creation, yet
existing generative methods are slow, rely on UV maps, and often fail to remain
faithful to a reference image. To address these challenges, we propose a
transformer-based framework that predicts a 3D texture field directly from a
single image and a mesh, eliminating the need for UV mapping and differentiable
rendering, and enabling faster texture generation. Our method integrates a
triplane representation with depth-based backprojection losses, enabling
efficient training and faster inference. Once trained, it generates
high-fidelity textures in a single forward pass, requiring only 0.2s per shape.
Extensive qualitative, quantitative, and user preference evaluations
demonstrate that our method outperforms state-of-the-art baselines on
single-image texture reconstruction in terms of both fidelity to the input
image and perceptual quality, highlighting its practicality for scalable,
high-quality, and controllable 3D content creation.

</details>


### [46] [SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing](https://arxiv.org/abs/2509.05144)
*Chaolei Wang,Yang Luo,Jing Du,Siyu Chen,Yiping Chen,Ting Han*

Main category: cs.CV

TL;DR: 提出SGS-3D方法，通过先分割后生长的策略，结合语义和几何信息提升3D实例分割精度，解决2D到3D提升方法中的语义模糊和深度约束不足问题


<details>
  <summary>Details</summary>
Motivation: 现有的基于2D到3D提升的3D实例分割方法由于提升过程中的累积误差、模糊语义引导和深度约束不足，难以产生精确的实例级分割结果

Method: 提出"split-then-grow"框架：首先使用几何基元净化和分割模糊的提升掩码，然后在场景中将它们生长为完整实例。包括基于几何基元共现的掩码过滤策略和利用空间连续性及高层特征的几何细化

Result: 在ScanNet200、ScanNet++和KITTI-360数据集上的实验表明，SGS-3D显著提高了分割精度和鲁棒性，能够处理预训练模型的不准确掩码，在室内外环境中都表现出良好的泛化能力

Conclusion: SGS-3D作为一种无需训练的细化方法，通过联合融合语义和几何信息，实现了高保真度的3D对象实例分割，有效解决了现有方法在语义一致性和几何精度方面的挑战

Abstract: Accurate 3D instance segmentation is crucial for high-quality scene
understanding in the 3D vision domain. However, 3D instance segmentation based
on 2D-to-3D lifting approaches struggle to produce precise instance-level
segmentation, due to accumulated errors introduced during the lifting process
from ambiguous semantic guidance and insufficient depth constraints. To tackle
these challenges, we propose splitting and growing reliable semantic mask for
high-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"
framework that first purifies and splits ambiguous lifted masks using geometric
primitives, and then grows them into complete instances within the scene.
Unlike existing approaches that directly rely on raw lifted masks and sacrifice
segmentation accuracy, SGS-3D serves as a training-free refinement method that
jointly fuses semantic and geometric information, enabling effective
cooperation between the two levels of representation. Specifically, for
semantic guidance, we introduce a mask filtering strategy that leverages the
co-occurrence of 3D geometry primitives to identify and remove ambiguous masks,
thereby ensuring more reliable semantic consistency with the 3D object
instances. For the geometric refinement, we construct fine-grained object
instances by exploiting both spatial continuity and high-level features,
particularly in the case of semantic ambiguity between distinct objects.
Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate that
SGS-3D substantially improves segmentation accuracy and robustness against
inaccurate masks from pre-trained models, yielding high-fidelity object
instances while maintaining strong generalization across diverse indoor and
outdoor environments. Code is available in the supplementary materials.

</details>


### [47] [SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition](https://arxiv.org/abs/2509.05188)
*Ariel Basso Madjoukeng,Jérôme Fink,Pierre Poitier,Edith Belise Kenmogne,Benoit Frenay*

Main category: cs.CV

TL;DR: 本文针对手语识别中对比学习存在的问题，提出了包含自由负样本对的自监督学习框架和新数据增强技术，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 手语识别中标注数据稀缺，对比学习方法存在两个问题：1）对所有视频部分同等对待，忽略了关键信息区域的重要性；2）不同手语间的相似动作导致负样本对高度相似，增加了识别难度。

Method: 提出了一个自监督学习框架，包含两个关键组件：1）采用自由负样本对的新自监督方法；2）新的数据增强技术。这两个组件协同工作以学习更有区分性的特征表示。

Result: 与多种对比学习和自监督方法相比，该方法在线性评估、半监督学习以及跨手语迁移任务中都显示出显著的准确率提升。

Conclusion: 所提出的自监督学习框架有效解决了手语识别中对比学习的局限性，通过学习更有区分性的特征表示，在各个评估场景下都取得了优异的性能表现。

Abstract: Sign language recognition (SLR) is a machine learning task aiming to identify
signs in videos. Due to the scarcity of annotated data, unsupervised methods
like contrastive learning have become promising in this field. They learn
meaningful representations by pulling positive pairs (two augmented versions of
the same instance) closer and pushing negative pairs (different from the
positive pairs) apart. In SLR, in a sign video, only certain parts provide
information that is truly useful for its recognition. Applying contrastive
methods to SLR raises two issues: (i) contrastive learning methods treat all
parts of a video in the same way, without taking into account the relevance of
certain parts over others; (ii) shared movements between different signs make
negative pairs highly similar, complicating sign discrimination. These issues
lead to learning non-discriminative features for sign recognition and poor
results in downstream tasks. In response, this paper proposes a self-supervised
learning framework designed to learn meaningful representations for SLR. This
framework consists of two key components designed to work together: (i) a new
self-supervised approach with free-negative pairs; (ii) a new data augmentation
technique. This approach shows a considerable gain in accuracy compared to
several contrastive and self-supervised methods, across linear evaluation,
semi-supervised learning, and transferability between sign languages.

</details>


### [48] [Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet](https://arxiv.org/abs/2509.05198)
*Mohammad Saeid,Amir Salarpour,Pedram MohajerAnsari*

Main category: cs.CV

TL;DR: 提出了ModelNet-R数据集和Point-SkipNet网络，解决了ModelNet40的数据质量问题，实现了更高精度的3D点云分类且计算开销更低


<details>
  <summary>Details</summary>
Motivation: 现有ModelNet40数据集存在标签不一致、2D数据、尺寸不匹配和类别区分不足等问题，限制了3D点云分类模型的性能

Method: 开发了精心优化的ModelNet-R数据集，并提出轻量级图神经网络Point-SkipNet，采用高效采样、邻域分组和跳跃连接技术

Result: 在ModelNet-R上训练的模型性能显著提升，Point-SkipNet在保持最先进精度的同时参数量大幅减少

Conclusion: 数据集质量对优化3D点云分类模型效率至关重要，该研究为相关应用提供了更可靠的基准和高效解决方案

Abstract: The classification of 3D point clouds is crucial for applications such as
autonomous driving, robotics, and augmented reality. However, the commonly used
ModelNet40 dataset suffers from limitations such as inconsistent labeling, 2D
data, size mismatches, and inadequate class differentiation, which hinder model
performance. This paper introduces ModelNet-R, a meticulously refined version
of ModelNet40 designed to address these issues and serve as a more reliable
benchmark. Additionally, this paper proposes Point-SkipNet, a lightweight
graph-based neural network that leverages efficient sampling, neighborhood
grouping, and skip connections to achieve high classification accuracy with
reduced computational overhead. Extensive experiments demonstrate that models
trained in ModelNet-R exhibit significant performance improvements. Notably,
Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with a
substantially lower parameter count compared to contemporary models. This
research highlights the crucial role of dataset quality in optimizing model
efficiency for 3D point cloud classification. For more details, see the code
at: https://github.com/m-saeid/ModeNetR_PointSkipNet.

</details>


### [49] [Symbolic Graphics Programming with Large Language Models](https://arxiv.org/abs/2509.05208)
*Yamei Chen,Haoquan Zhang,Yangyi Huang,Zeju Qiu,Kaipeng Zhang,Yandong Wen,Weiyang Liu*

Main category: cs.CV

TL;DR: 本文研究了LLM生成符号图形程序(SGPs)的能力，提出了SGP-GenBench基准测试，并开发了基于强化学习的改进方法，显著提升了SVG生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在生成符号图形程序方面的能力，特别是SVG程序，以了解LLM如何通过程序合成来理解视觉世界。

Method: 引入SGP-GenBench基准测试，提出基于强化学习的方法，使用格式有效性验证和跨模态奖励（SigLIP和DINO编码器）来提升SVG生成质量。

Result: 前沿专有模型显著优于开源模型，提出的强化学习方法使Qwen-2.5-7B模型达到与前沿系统相当的性能，RL训练改善了对象分解和场景连贯性。

Conclusion: 符号图形编程为跨模态基础提供了精确且可解释的研究视角，RL方法有效提升了LLM的SVG生成能力。

Abstract: Large language models (LLMs) excel at program synthesis, yet their ability to
produce symbolic graphics programs (SGPs) that render into precise visual
content remains underexplored. We study symbolic graphics programming, where
the goal is to generate an SGP from a natural-language description. This task
also serves as a lens into how LLMs understand the visual world by prompting
them to generate images rendered from SGPs. Among various SGPs, our paper
sticks to scalable vector graphics (SVGs). We begin by examining the extent to
which LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a
comprehensive benchmark covering object fidelity, scene fidelity, and
compositionality (attribute binding, spatial relations, numeracy). On
SGP-GenBench, we discover that frontier proprietary models substantially
outperform open-source models, and performance correlates well with general
coding capabilities. Motivated by this gap, we aim to improve LLMs' ability to
generate SGPs. We propose a reinforcement learning (RL) with verifiable rewards
approach, where a format-validity gate ensures renderable SVG, and a
cross-modal reward aligns text and the rendered image via strong vision
encoders (e.g., SigLIP for text-image and DINO for image-image). Applied to
Qwen-2.5-7B, our method substantially improves SVG generation quality and
semantics, achieving performance on par with frontier systems. We further
analyze training dynamics, showing that RL induces (i) finer decomposition of
objects into controllable primitives and (ii) contextual details that improve
scene coherence. Our results demonstrate that symbolic graphics programming
offers a precise and interpretable lens on cross-modal grounding.

</details>


### [50] [COGITAO: A Visual Reasoning Framework To Study Compositionality & Generalization](https://arxiv.org/abs/2509.05249)
*Yassine Taoudi-Benchekroun,Klim Troyan,Pascal Sager,Stefan Gerber,Lukas Tuggener,Benjamin Grewe*

Main category: cs.CV

TL;DR: COGITAO是一个模块化可扩展的数据生成框架和基准测试，用于系统研究视觉领域的组合性和泛化能力，通过基于规则的任务和可组合变换来评估模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习模型在组合学习概念并将其应用于新场景方面的局限性，这是人类智能的关键能力但在现有模型中表现不佳。

Method: 引入COGITAO框架，构建基于规则的任务，在网格环境中对对象应用28种可互操作的变换，支持可调整深度的组合，并允许对网格参数和对象属性进行广泛控制。

Result: 能够创建数百万个独特的任务规则，比现有数据集规模大几个数量级，生成范围广泛的难度级别任务，并提供每个规则的无限样本生成能力。

Conclusion: 使用最先进的视觉模型进行的基线实验显示，尽管在域内性能很强，但这些模型无法泛化到熟悉元素的新组合，COGITAO开源以支持该领域的持续研究。

Abstract: The ability to compose learned concepts and apply them in novel settings is
key to human intelligence, but remains a persistent limitation in
state-of-the-art machine learning models. To address this issue, we introduce
COGITAO, a modular and extensible data generation framework and benchmark
designed to systematically study compositionality and generalization in visual
domains. Drawing inspiration from ARC-AGI's problem-setting, COGITAO constructs
rule-based tasks which apply a set of transformations to objects in grid-like
environments. It supports composition, at adjustable depth, over a set of 28
interoperable transformations, along with extensive control over grid
parametrization and object properties. This flexibility enables the creation of
millions of unique task rules -- surpassing concurrent datasets by several
orders of magnitude -- across a wide range of difficulties, while allowing
virtually unlimited sample generation per rule. We provide baseline experiments
using state-of-the-art vision models, highlighting their consistent failures to
generalize to novel combinations of familiar elements, despite strong in-domain
performance. COGITAO is fully open-sourced, including all code and datasets, to
support continued research in this field.

</details>


### [51] [WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool](https://arxiv.org/abs/2509.05296)
*Zizun Li,Jianjun Zhou,Yifan Wang,Haoyu Guo,Wenzheng Chang,Yang Zhou,Haoyi Zhu,Junyi Chen,Chunhua Shen,Tong He*

Main category: cs.CV

TL;DR: WinT3R是一个前馈重建模型，能够在在线预测精确相机姿态和高质量点云地图，解决了重建质量与实时性能之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建质量和实时性能之间存在权衡，无法同时满足高质量几何预测和实时计算的需求。

Method: 引入滑动窗口机制确保窗口内帧间充分信息交换，使用紧凑相机表示并维护全局相机令牌池，在不牺牲效率的情况下提高相机姿态估计的可靠性。

Result: 在多样化数据集上的广泛实验验证，WinT3R在在线重建质量、相机姿态估计和重建速度方面达到了最先进的性能。

Conclusion: 通过滑动窗口机制和紧凑相机表示设计，WinT3R成功解决了重建质量与实时性能的权衡问题，实现了高质量的在线重建。

Abstract: We present WinT3R, a feed-forward reconstruction model capable of online
prediction of precise camera poses and high-quality point maps. Previous
methods suffer from a trade-off between reconstruction quality and real-time
performance. To address this, we first introduce a sliding window mechanism
that ensures sufficient information exchange among frames within the window,
thereby improving the quality of geometric predictions without large
computation. In addition, we leverage a compact representation of cameras and
maintain a global camera token pool, which enhances the reliability of camera
pose estimation without sacrificing efficiency. These designs enable WinT3R to
achieve state-of-the-art performance in terms of online reconstruction quality,
camera pose estimation, and reconstruction speed, as validated by extensive
experiments on diverse datasets. Code and model are publicly available at
https://github.com/LiZizun/WinT3R.

</details>


### [52] [FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases](https://arxiv.org/abs/2509.05297)
*Matteo Poggi,Fabio Tosi*

Main category: cs.CV

TL;DR: FlowSeek是一个轻量级光流估计框架，在单张消费级GPU上训练，硬件需求降低8倍，但在Sintel Final和KITTI等数据集上比SOTA方法SEA-RAFT提升10-15%


<details>
  <summary>Details</summary>
Motivation: 当前光流估计方法需要大量硬件资源进行训练，限制了其实际应用。FlowSeek旨在开发一个既紧凑又准确的光流架构，大幅降低训练硬件需求

Method: 结合光流网络设计空间的最新进展、先进的单图像深度基础模型和经典的低维运动参数化方法，实现紧凑而精确的架构设计

Result: 在单张消费级GPU上训练成功，硬件预算比最新方法降低约8倍，在Sintel Final和KITTI数据集上实现卓越的跨数据集泛化能力，相对SEA-RAFT分别提升10%和15%，在Spring和LayeredFlow数据集上也表现优异

Conclusion: FlowSeek证明了通过精心设计的架构整合，可以在极低硬件资源下实现高性能光流估计，为光流技术的广泛应用提供了可行的解决方案

Abstract: We present FlowSeek, a novel framework for optical flow requiring minimal
hardware resources for training. FlowSeek marries the latest advances on the
design space of optical flow networks with cutting-edge single-image depth
foundation models and classical low-dimensional motion parametrization,
implementing a compact, yet accurate architecture. FlowSeek is trained on a
single consumer-grade GPU, a hardware budget about 8x lower compared to most
recent methods, and still achieves superior cross-dataset generalization on
Sintel Final and KITTI, with a relative improvement of 10 and 15% over the
previous state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlow
datasets.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [53] [Towards an Accurate and Effective Robot Vision (The Problem of Topological Localization for Mobile Robots)](https://arxiv.org/abs/2509.04948)
*Emanuela Boros*

Main category: cs.RO

TL;DR: 本文系统评估了多种视觉描述符在办公室环境拓扑定位中的性能，包括颜色直方图、SIFT系列方法和词袋模型，通过定量比较和标准评估验证了不同配置的优势。


<details>
  <summary>Details</summary>
Motivation: 解决移动机器人在感知模糊、传感器噪声和光照变化条件下的视觉定位问题，仅使用单目彩色相机图像而不依赖图像序列的时间连续性。

Method: 评估了Color Histograms、SIFT、ASIFT、RGB-SIFT和Bag-of-Visual-Words等先进视觉描述符，系统比较了不同特征、距离度量和分类器的组合配置。

Result: 结果显示合适的表观描述符、相似性度量和分类器配置具有显著优势，在ImageCLEF评估活动中成功识别新图像序列的最可能位置。

Conclusion: 未来工作将探索分层模型、排序方法和特征组合，以构建更鲁棒的定位系统，减少训练和运行时间，避免维度灾难，实现跨不同光照条件和更长路径的实时集成定位。

Abstract: Topological localization is a fundamental problem in mobile robotics, since
robots must be able to determine their position in order to accomplish tasks.
Visual localization and place recognition are challenging due to perceptual
ambiguity, sensor noise, and illumination variations. This work addresses
topological localization in an office environment using only images acquired
with a perspective color camera mounted on a robot platform, without relying on
temporal continuity of image sequences. We evaluate state-of-the-art visual
descriptors, including Color Histograms, SIFT, ASIFT, RGB-SIFT, and
Bag-of-Visual-Words approaches inspired by text retrieval. Our contributions
include a systematic, quantitative comparison of these features, distance
measures, and classifiers. Performance was analyzed using standard evaluation
metrics and visualizations, extending previous experiments. Results demonstrate
the advantages of proper configurations of appearance descriptors, similarity
measures, and classifiers. The quality of these configurations was further
validated in the Robot Vision task of the ImageCLEF evaluation campaign, where
the system identified the most likely location of novel image sequences. Future
work will explore hierarchical models, ranking methods, and feature
combinations to build more robust localization systems, reducing training and
runtime while avoiding the curse of dimensionality. Ultimately, this aims
toward integrated, real-time localization across varied illumination and longer
routes.

</details>


### [54] [Pointing-Guided Target Estimation via Transformer-Based Attention](https://arxiv.org/abs/2509.05031)
*Luca Müller,Hassan Ali,Philipp Allgeuer,Lukáš Gajdošech,Stefan Wermter*

Main category: cs.RO

TL;DR: 提出MM-ITF模型，通过多模态注意力机制将2D指向手势映射到物体位置，准确预测人类指向意图，实现直观的人机协作


<details>
  <summary>Details</summary>
Motivation: 指向手势是人类非语言交流的基本形式，在HRI中机器人需要能够预测人类意图并做出适当响应，这对人机协作至关重要

Method: 提出多模态交互变换器(MM-ITF)架构，利用跨模态注意力机制处理单目RGB数据，将2D指向手势映射到物体位置并分配似然分数

Result: 方法能够准确预测目标物体，并在受控桌面场景中与NICOL机器人成功实现直观的人机协作

Conclusion: MM-ITF模型通过引入补丁混淆矩阵评估性能，证明了使用单目RGB数据实现精确指向意图预测的可行性，为人机交互提供了有效的技术方案

Abstract: Deictic gestures, like pointing, are a fundamental form of non-verbal
communication, enabling humans to direct attention to specific objects or
locations. This capability is essential in Human-Robot Interaction (HRI), where
robots should be able to predict human intent and anticipate appropriate
responses. In this work, we propose the Multi-Modality Inter-TransFormer
(MM-ITF), a modular architecture to predict objects in a controlled tabletop
scenario with the NICOL robot, where humans indicate targets through natural
pointing gestures. Leveraging inter-modality attention, MM-ITF maps 2D pointing
gestures to object locations, assigns a likelihood score to each, and
identifies the most likely target. Our results demonstrate that the method can
accurately predict the intended object using monocular RGB data, thus enabling
intuitive and accessible human-robot collaboration. To evaluate the
performance, we introduce a patch confusion matrix, providing insights into the
model's predictions across candidate object locations. Code available at:
https://github.com/lucamuellercode/MMITF.

</details>


### [55] [Robust Model Predictive Control Design for Autonomous Vehicles with Perception-based Observers](https://arxiv.org/abs/2509.05201)
*Nariman Niknejad,Gokul S. Sankar,Bahare Kiumarsi,Hamidreza Modares*

Main category: cs.RO

TL;DR: 提出一种针对深度学习感知模块非高斯噪声的鲁棒模型预测控制框架，使用约束zonotopes进行基于集合的状态估计，将问题重构为线性规划，在重尾噪声条件下显著优于传统高斯噪声设计。


<details>
  <summary>Details</summary>
Motivation: 传统MPC假设感知误差为零均值高斯噪声，但深度学习感知模块实际存在有偏、重尾的非高斯噪声，需要更准确的感知不确定性量化来保证反馈控制的安全性。

Method: 采用约束zonotopes进行基于集合的状态估计，捕捉有偏重尾不确定性；将鲁棒MPC重构为线性规划问题，使用Minkowski-Lyapunov成本函数和松弛变量；通过Minkowski-Lyapunov不等式和收缩zonotopic不变集保证闭环稳定性。

Result: 在全向移动机器人上进行仿真和硬件实验，结果表明感知感知MPC在重尾噪声条件下提供稳定准确的控制性能，在状态估计误差边界和整体控制性能方面显著优于传统高斯噪声设计。

Conclusion: 所提出的框架能够有效处理深度学习感知模块的非高斯噪声，为安全反馈控制提供了更准确的感知不确定性量化方法，在重尾噪声条件下具有优越的控制性能。

Abstract: This paper presents a robust model predictive control (MPC) framework that
explicitly addresses the non-Gaussian noise inherent in deep learning-based
perception modules used for state estimation. Recognizing that accurate
uncertainty quantification of the perception module is essential for safe
feedback control, our approach departs from the conventional assumption of
zero-mean noise quantification of the perception error. Instead, it employs
set-based state estimation with constrained zonotopes to capture biased,
heavy-tailed uncertainties while maintaining bounded estimation errors. To
improve computational efficiency, the robust MPC is reformulated as a linear
program (LP), using a Minkowski-Lyapunov-based cost function with an added
slack variable to prevent degenerate solutions. Closed-loop stability is
ensured through Minkowski-Lyapunov inequalities and contractive zonotopic
invariant sets. The largest stabilizing terminal set and its corresponding
feedback gain are then derived via an ellipsoidal approximation of the
zonotopes. The proposed framework is validated through both simulations and
hardware experiments on an omnidirectional mobile robot along with a camera and
a convolutional neural network-based perception module implemented within a
ROS2 framework. The results demonstrate that the perception-aware MPC provides
stable and accurate control performance under heavy-tailed noise conditions,
significantly outperforming traditional Gaussian-noise-based designs in terms
of both state estimation error bounding and overall control performance.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [56] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一个针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度实现负载均衡，相比现有方法减少45%延迟


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型并行推理方案在异构多GPU环境中资源利用率低，硬件差异和后台任务导致负载不均衡

Method: 混合调度器在时空维度协调细粒度并行：时间维度使用计算感知步长分配器和最小公倍数量化技术减少慢速GPU的去噪步骤；空间维度采用弹性块并行机制按GPU计算能力分配不同大小的图像块

Result: 在负载不均衡和异构多GPU集群上的实验验证了有效性，相比最先进的块并行框架显著减少端到端推理延迟达45%，并提高异构GPU资源利用率

Conclusion: STADI框架通过时空自适应调度有效解决了异构环境中的负载不均衡问题，显著提升了扩散模型推理效率

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [57] [Ecologically Valid Benchmarking and Adaptive Attention: Scalable Marine Bioacoustic Monitoring](https://arxiv.org/abs/2509.04682)
*Nicholas R. Rasmussen,Rodrigue Rizk,Longwei Wang,KC Santosh*

Main category: cs.SD

TL;DR: 提出了GetNetUPAM评估框架和ARPA-N神经网络架构，用于提升水下被动声学监测的模型稳定性和泛化能力，在生态多样性评估中取得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 解决水下被动声学监测中固有噪声、复杂信号依赖性和环境变化导致的模型不稳定和泛化能力差的问题

Method: 开发了分层嵌套交叉验证框架GetNetUPAM进行生态真实性评估，并提出自适应分辨率池化和注意力网络ARPA-N处理不规则频谱图维度

Result: ARPA-N在GetNetUPAM评估下相比DenseNet基线平均精度提升14.4%，所有指标的变异性降低log2尺度数量级

Conclusion: 该框架和架构能够实现跨站点-年份的一致检测，推动可扩展、准确的生物声学监测发展

Abstract: Underwater Passive Acoustic Monitoring (UPAM) provides rich spatiotemporal
data for long-term ecological analysis, but intrinsic noise and complex signal
dependencies hinder model stability and generalization. Multilayered windowing
has improved target sound localization, yet variability from shifting ambient
noise, diverse propagation effects, and mixed biological and anthropogenic
sources demands robust architectures and rigorous evaluation. We introduce
GetNetUPAM, a hierarchical nested cross-validation framework designed to
quantify model stability under ecologically realistic variability. Data are
partitioned into distinct site-year segments, preserving recording
heterogeneity and ensuring each validation fold reflects a unique environmental
subset, reducing overfitting to localized noise and sensor artifacts. Site-year
blocking enforces evaluation against genuine environmental diversity, while
standard cross-validation on random subsets measures generalization across
UPAM's full signal distribution, a dimension absent from current benchmarks.
Using GetNetUPAM as the evaluation backbone, we propose the Adaptive Resolution
Pooling and Attention Network (ARPA-N), a neural architecture for irregular
spectrogram dimensions. Adaptive pooling with spatial attention extends the
receptive field, capturing global context without excessive parameters. Under
GetNetUPAM, ARPA-N achieves a 14.4% gain in average precision over DenseNet
baselines and a log2-scale order-of-magnitude drop in variability across all
metrics, enabling consistent detection across site-year folds and advancing
scalable, accurate bioacoustic monitoring.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [58] [SparkUI-Parser: Enhancing GUI Perception with Robust Grounding and Parsing](https://arxiv.org/abs/2509.04908)
*Hongyi Jing,Jiafu Chen,Chen Rao,Ziqiang Dang,Jiajie Teng,Tianyi Chu,Juncheng Mo,Shuo Fang,Huaizhong Lin,Rui Lv,Chenguang Ma,Lei Zhao*

Main category: cs.AI

TL;DR: SparkUI-Parser是一个端到端GUI解析框架，通过连续坐标建模和拒绝机制，显著提升了定位精度和解析能力，在多个基准测试中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在GUI感知中存在两个主要问题：1）基于文本自回归机制的离散坐标建模导致定位精度低、推理速度慢；2）只能定位预定义元素集合，无法解析整个界面，限制了广泛应用。

Method: 提出基于预训练MLLM的连续坐标建模方法，添加token路由器和坐标解码器；引入基于改进匈牙利匹配算法的拒绝机制来识别和拒绝不存在元素；构建ScreenParse基准测试系统评估GUI模型。

Result: 在ScreenSpot、ScreenSpot-v2、CAGUI-Grounding和ScreenParse等多个基准测试中，该方法一致优于最先进方法，显著提升了准确性和推理速度。

Conclusion: SparkUI-Parser通过连续坐标建模和拒绝机制，有效解决了现有GUI解析方法的局限性，实现了更高的定位精度和细粒度界面解析能力，为下游任务提供了更好的支持。

Abstract: The existing Multimodal Large Language Models (MLLMs) for GUI perception have
made great progress. However, the following challenges still exist in prior
methods: 1) They model discrete coordinates based on text autoregressive
mechanism, which results in lower grounding accuracy and slower inference
speed. 2) They can only locate predefined sets of elements and are not capable
of parsing the entire interface, which hampers the broad application and
support for downstream tasks. To address the above issues, we propose
SparkUI-Parser, a novel end-to-end framework where higher localization
precision and fine-grained parsing capability of the entire interface are
simultaneously achieved. Specifically, instead of using probability-based
discrete modeling, we perform continuous modeling of coordinates based on a
pre-trained Multimodal Large Language Model (MLLM) with an additional token
router and coordinate decoder. This effectively mitigates the limitations
inherent in the discrete output characteristics and the token-by-token
generation process of MLLMs, consequently boosting both the accuracy and the
inference speed. To further enhance robustness, a rejection mechanism based on
a modified Hungarian matching algorithm is introduced, which empowers the model
to identify and reject non-existent elements, thereby reducing false positives.
Moreover, we present ScreenParse, a rigorously constructed benchmark to
systematically assess structural perception capabilities of GUI models across
diverse scenarios. Extensive experiments demonstrate that our approach
consistently outperforms SOTA methods on ScreenSpot, ScreenSpot-v2,
CAGUI-Grounding and ScreenParse benchmarks. The resources are available at
https://github.com/antgroup/SparkUI-Parser.

</details>


### [59] [LatticeWorld: A Multimodal Large Language Model-Empowered Framework for Interactive Complex World Generation](https://arxiv.org/abs/2509.05263)
*Yinglin Duan,Zhengxia Zou,Tongwei Gu,Wei Jia,Zhan Zhao,Luyi Xu,Xinzhu Liu,Hao Jiang,Kang Chen,Shuang Qiu*

Main category: cs.AI

TL;DR: LatticeWorld是一个基于轻量级LLM和虚幻引擎5的3D世界生成框架，通过多模态输入创建大规模动态交互环境，在保持高质量的同时将工业生产效率提升90倍。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统手动建模3D场景效率低下的问题，同时利用机器学习算法生成更真实、物理准确的虚拟世界，以缩小仿真与现实的差距。

Method: 结合轻量级LLaMA-2-7B语言模型和工业级渲染引擎（如Unreal Engine 5），接受文本描述和视觉指令作为多模态输入，生成包含动态代理的大规模3D交互世界。

Result: 在场景布局生成和视觉保真度方面达到优异精度，相比传统手动生产方法，工业生产效率提升90倍以上，同时保持高创意质量。

Conclusion: LatticeWorld提供了一个简单有效的3D世界生成框架，成功实现了高效、高质量的动态环境创建，具有重要的工业应用价值。

Abstract: Recent research has been increasingly focusing on developing 3D world models
that simulate complex real-world scenarios. World models have found broad
applications across various domains, including embodied AI, autonomous driving,
entertainment, etc. A more realistic simulation with accurate physics will
effectively narrow the sim-to-real gap and allow us to gather rich information
about the real world conveniently. While traditional manual modeling has
enabled the creation of virtual 3D scenes, modern approaches have leveraged
advanced machine learning algorithms for 3D world generation, with most recent
advances focusing on generative methods that can create virtual worlds based on
user instructions. This work explores such a research direction by proposing
LatticeWorld, a simple yet effective 3D world generation framework that
streamlines the industrial production pipeline of 3D environments. LatticeWorld
leverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering
engine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed
framework accepts textual descriptions and visual instructions as multimodal
inputs and creates large-scale 3D interactive worlds with dynamic agents,
featuring competitive multi-agent interaction, high-fidelity physics
simulation, and real-time rendering. We conduct comprehensive experiments to
evaluate LatticeWorld, showing that it achieves superior accuracy in scene
layout generation and visual fidelity. Moreover, LatticeWorld achieves over a
$90\times$ increase in industrial production efficiency while maintaining high
creative quality compared with traditional manual production methods. Our demo
video is available at https://youtu.be/8VWZXpERR18

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [60] [Sample-efficient Integration of New Modalities into Large Language Models](https://arxiv.org/abs/2509.04606)
*Osman Batur İnce,André F. T. Martins,Oisin Mac Aodha,Edoardo M. Ponti*

Main category: cs.CL

TL;DR: 提出SEMI方法，通过超网络适配器实现少样本高效多模态整合，将新模态集成到LLM中


<details>
  <summary>Details</summary>
Motivation: 多模态基础模型需要处理多种模态，但从头训练所有模态不可行，且现有方法需要大量配对数据，对于低资源模态难以实现

Method: 设计超网络适配共享投影器，利用高资源模态训练，在推理时使用少量样本生成适配器，并通过等距变换增加训练模态多样性

Result: SEMI在新模态整合中显著提升样本效率，32-shot SEMI达到的准确率需要64倍数据才能通过从头训练实现

Conclusion: SEMI有望扩展基础模型的模态覆盖范围，为低资源模态集成提供高效解决方案

Abstract: Multimodal foundation models can process several modalities. However, since
the space of possible modalities is large and evolving over time, training a
model from scratch to encompass all modalities is unfeasible. Moreover,
integrating a modality into a pre-existing foundation model currently requires
a significant amount of paired data, which is often not available for
low-resource modalities. In this paper, we introduce a method for
sample-efficient modality integration (SEMI) into Large Language Models (LLMs).
To this end, we devise a hypernetwork that can adapt a shared projector --
placed between modality-specific encoders and an LLM -- to any modality. The
hypernetwork, trained on high-resource modalities (i.e., text, speech, audio,
video), is conditioned on a few samples from any arbitrary modality at
inference time to generate a suitable adapter. To increase the diversity of
training modalities, we artificially multiply the number of encoders through
isometric transformations. We find that SEMI achieves a significant boost in
sample efficiency during few-shot integration of new modalities (i.e.,
satellite images, astronomical images, inertial measurements, and molecules)
with encoders of arbitrary embedding dimensionality. For instance, to reach the
same accuracy as 32-shot SEMI, training the projector from scratch needs
64$\times$ more data. As a result, SEMI holds promise to extend the modality
coverage of foundation models.

</details>


### [61] [Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization](https://arxiv.org/abs/2509.04745)
*Lee Kezar,Zed Sehyr,Jesse Thomason*

Main category: cs.CL

TL;DR: 本研究探讨了两种语音学归纳偏置（参数解耦和语音学半监督）如何提升向量量化自编码器在手语识别和重建中的泛化能力，特别是在未见手语词汇上的表现。


<details>
  <summary>Details</summary>
Motivation: 手语数据集在词汇代表性方面往往不足，需要能够泛化到未见手语符号的模型。虽然向量量化是学习离散表征的有前景方法，但需要验证其学习单元是否捕获了阻碍泛化性能的伪相关性。

Method: 使用向量量化自编码器，引入两种语音学归纳偏置：参数解耦（架构偏置）和语音学半监督（正则化技术），以改进已知手语的孤立识别和未见手语的重建质量。

Result: 所提出模型学习到的表征在未见手语的一次性重建方面更有效，在手语识别方面比受控基线更具判别性。

Conclusion: 这项工作量化分析了明确的、语言学驱动的偏置如何能够改善手语学习表征的泛化能力，为手语处理提供了重要的方法论贡献。

Abstract: Sign language datasets are often not representative in terms of vocabulary,
underscoring the need for models that generalize to unseen signs. Vector
quantization is a promising approach for learning discrete, token-like
representations, but it has not been evaluated whether the learned units
capture spurious correlations that hinder out-of-vocabulary performance. This
work investigates two phonological inductive biases: Parameter Disentanglement,
an architectural bias, and Phonological Semi-Supervision, a regularization
technique, to improve isolated sign recognition of known signs and
reconstruction quality of unseen signs with a vector-quantized autoencoder. The
primary finding is that the learned representations from the proposed model are
more effective for one-shot reconstruction of unseen signs and more
discriminative for sign identification compared to a controlled baseline. This
work provides a quantitative analysis of how explicit, linguistically-motivated
biases can improve the generalization of learned representations of sign
language.

</details>


### [62] [PRIM: Towards Practical In-Image Multilingual Machine Translation](https://arxiv.org/abs/2509.05146)
*Yanzhi Tian,Zeming Liu,Zhengyang Liu,Chong Feng,Xin Li,Heyan Huang,Yuhang Guo*

Main category: cs.CL

TL;DR: 提出了PRIM数据集和VisTrans模型，用于解决实际场景中的图像内多语言机器翻译问题，填补了合成数据与现实应用之间的差距


<details>
  <summary>Details</summary>
Motivation: 当前端到端图像内机器翻译研究主要基于合成数据，背景简单、字体单一、文本位置固定，无法反映真实世界场景，导致研究与实践之间存在显著差距

Method: 构建PRIM真实世界数据集，并提出VisTrans端到端模型，分别处理图像中的视觉文本和背景信息，支持多语言翻译并提升视觉质量

Result: 实验结果表明VisTrans相比其他模型在翻译质量和视觉效果方面表现更好

Conclusion: PRIM数据集和VisTrans模型为实际场景中的图像内多语言机器翻译研究提供了有效解决方案，推动了该领域的发展

Abstract: In-Image Machine Translation (IIMT) aims to translate images containing texts
from one language to another. Current research of end-to-end IIMT mainly
conducts on synthetic data, with simple background, single font, fixed text
position, and bilingual translation, which can not fully reflect real world,
causing a significant gap between the research and practical conditions. To
facilitate research of IIMT in real-world scenarios, we explore Practical
In-Image Multilingual Machine Translation (IIMMT). In order to convince the
lack of publicly available data, we annotate the PRIM dataset, which contains
real-world captured one-line text images with complex background, various
fonts, diverse text positions, and supports multilingual translation
directions. We propose an end-to-end model VisTrans to handle the challenge of
practical conditions in PRIM, which processes visual text and background
information in the image separately, ensuring the capability of multilingual
translation while improving the visual quality. Experimental results indicate
the VisTrans achieves a better translation quality and visual effect compared
to other models. The code and dataset are available at:
https://github.com/BITHLP/PRIM.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [63] [Inferring the Graph Structure of Images for Graph Neural Networks](https://arxiv.org/abs/2509.04677)
*Mayur S Gowda,John Shi,Augusto Santos,José M. F. Moura*

Main category: eess.IV

TL;DR: 通过寻找比传统网格图和超像素方法更好的图表示来改进MNIST和Fashion-MNIST数据集的GNN分类准确率


<details>
  <summary>Details</summary>
Motivation: 传统方法将图像表示为网格图，每个节点代表一个像素，边连接相邻像素，但这种方法可能不是最优的图表示

Method: 基于像素值之间的相关性构建行相关性图、列相关性图和乘积图作为替代图表示

Result: 实验表明使用这些不同的图表示作为GNN输入相比传统网格图和超像素方法提高了分类准确率

Conclusion: 选择合适的图表示对GNN性能至关重要，基于像素相关性的图表示优于传统网格图方法

Abstract: Image datasets such as MNIST are a key benchmark for testing Graph Neural
Network (GNN) architectures. The images are traditionally represented as a grid
graph with each node representing a pixel and edges connecting neighboring
pixels (vertically and horizontally). The graph signal is the values
(intensities) of each pixel in the image. The graphs are commonly used as input
to graph neural networks (e.g., Graph Convolutional Neural Networks (Graph
CNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify the
images. In this work, we improve the accuracy of downstream graph neural
network tasks by finding alternative graphs to the grid graph and superpixel
methods to represent the dataset images, following the approach in [5, 6]. We
find row correlation, column correlation, and product graphs for each image in
MNIST and Fashion-MNIST using correlations between the pixel values building on
the method in [5, 6]. Experiments show that using these different graph
representations and features as input into downstream GNN models improves the
accuracy over using the traditional grid graph and superpixel methods in the
literature.

</details>


### [64] [AURAD: Anatomy-Pathology Unified Radiology Synthesis with Progressive Representations](https://arxiv.org/abs/2509.04819)
*Shuhan Ding,Jingjing Fu,Yu Gu,Naiteek Sangani,Mu Wei,Paul Vozila,Nan Liu,Jiang Bian,Hoifung Poon*

Main category: eess.IV

TL;DR: AURAD是一个可控的放射学合成框架，能联合生成高质量胸部X光片和伪语义掩码，通过渐进式流程和预训练医学模型确保临床合理性


<details>
  <summary>Details</summary>
Motivation: 解决医学图像合成中细粒度控制和临床相关性不足的问题，特别是在胸部X光片中疾病模式形态多样且与解剖结构紧密交织的挑战

Method: 采用渐进式流程：先根据解剖结构条件生成伪掩码，再用掩码指导图像合成；利用预训练医学模型过滤输出确保临床合理性

Result: 78%的合成图像被认证放射医师分类为真实图像，超过40%的分割覆盖被评定为临床有用，在多个任务和数据集上表现出有效性和泛化性

Conclusion: 该方法不仅实现了视觉真实性，还通过合成掩码为检测和分割等下游任务提供标签，弥合了生成建模与真实临床应用之间的差距

Abstract: Medical image synthesis has become an essential strategy for augmenting
datasets and improving model generalization in data-scarce clinical settings.
However, fine-grained and controllable synthesis remains difficult due to
limited high-quality annotations and domain shifts across datasets. Existing
methods, often designed for natural images or well-defined tumors, struggle to
generalize to chest radiographs, where disease patterns are morphologically
diverse and tightly intertwined with anatomical structures. To address these
challenges, we propose AURAD, a controllable radiology synthesis framework that
jointly generates high-fidelity chest X-rays and pseudo semantic masks. Unlike
prior approaches that rely on randomly sampled masks-limiting diversity,
controllability, and clinical relevance-our method learns to generate masks
that capture multi-pathology coexistence and anatomical-pathological
consistency. It follows a progressive pipeline: pseudo masks are first
generated from clinical prompts conditioned on anatomical structures, and then
used to guide image synthesis. We also leverage pretrained expert medical
models to filter outputs and ensure clinical plausibility. Beyond visual
realism, the synthesized masks also serve as labels for downstream tasks such
as detection and segmentation, bridging the gap between generative modeling and
real-world clinical applications. Extensive experiments and blinded radiologist
evaluations demonstrate the effectiveness and generalizability of our method
across tasks and datasets. In particular, 78% of our synthesized images are
classified as authentic by board-certified radiologists, and over 40% of
predicted segmentation overlays are rated as clinically useful. All code,
pre-trained models, and the synthesized dataset will be released upon
publication.

</details>


### [65] [Multi-modal Uncertainty Robust Tree Cover Segmentation For High-Resolution Remote Sensing Images](https://arxiv.org/abs/2509.04870)
*Yuanyuan Gui,Wei Li,Yinjian Wang,Xiang-Gen Xia,Mauro Marty,Christian Ginzler,Zuyuan Wang*

Main category: eess.IV

TL;DR: 提出MURTreeFormer多模态分割框架，通过概率潜在表示建模辅助模态的不确定性，利用VAE重采样机制重建不确定斑块，结合梯度幅度注意力模块和轻量化细化头，有效降低时间错位带来的不确定性影响。


<details>
  <summary>Details</summary>
Motivation: 多模态遥感影像在获取时存在时间错位，导致植被扰动和成像质量变化等问题，引入跨模态不确定性，严重影响分割精度。需要解决时间错位带来的不确定性挑战。

Method: 提出MURTreeFormer框架：1）将一种模态设为主模态，其他为辅助模态；2）通过概率潜在表示显式建模辅助模态的斑块级不确定性；3）使用VAE重采样机制从主模态分布重建不确定斑块；4）在解码器中集成梯度幅度注意力模块和轻量化细化头。

Result: 在上海和苏黎世的多模态数据集上进行广泛实验，证明MURTreeFormer显著提高了分割性能，有效降低了时间诱导的随机不确定性的影响。

Conclusion: MURTreeFormer通过建模和利用不确定性，能够有效处理多模态遥感影像中的时间错位问题，为稳健的树木覆盖制图提供了有效解决方案。

Abstract: Recent advances in semantic segmentation of multi-modal remote sensing images
have significantly improved the accuracy of tree cover mapping, supporting
applications in urban planning, forest monitoring, and ecological assessment.
Integrating data from multiple modalities-such as optical imagery, light
detection and ranging (LiDAR), and synthetic aperture radar (SAR)-has shown
superior performance over single-modality methods. However, these data are
often acquired days or even months apart, during which various changes may
occur, such as vegetation disturbances (e.g., logging, and wildfires) and
variations in imaging quality. Such temporal misalignments introduce
cross-modal uncertainty, especially in high-resolution imagery, which can
severely degrade segmentation accuracy. To address this challenge, we propose
MURTreeFormer, a novel multi-modal segmentation framework that mitigates and
leverages aleatoric uncertainty for robust tree cover mapping. MURTreeFormer
treats one modality as primary and others as auxiliary, explicitly modeling
patch-level uncertainty in the auxiliary modalities via a probabilistic latent
representation. Uncertain patches are identified and reconstructed from the
primary modality's distribution through a VAE-based resampling mechanism,
producing enhanced auxiliary features for fusion. In the decoder, a gradient
magnitude attention (GMA) module and a lightweight refinement head (RH) are
further integrated to guide attention toward tree-like structures and to
preserve fine-grained spatial details. Extensive experiments on multi-modal
datasets from Shanghai and Zurich demonstrate that MURTreeFormer significantly
improves segmentation performance and effectively reduces the impact of
temporally induced aleatoric uncertainty.

</details>


### [66] [VLSM-Ensemble: Ensembling CLIP-based Vision-Language Models for Enhanced Medical Image Segmentation](https://arxiv.org/abs/2509.05154)
*Julia Dietlmeier,Oluwabukola Grace Adegboro,Vayangi Ganepola,Claudia Mazo,Noel E. O'Connor*

Main category: eess.IV

TL;DR: 通过将视觉语言分割模型(VLSMs)与低复杂度CNN集成，在医学图像分割任务中显著提升性能，特别是在BKAI息肉数据集上Dice分数提高了6.3%。


<details>
  <summary>Details</summary>
Motivation: 基于CLIP和BiomedCLIP的视觉语言模型在图像分割任务中表现不如CRIS等复杂架构，需要探索新的方法来缩小这一差距。

Method: 采用集成学习方法，将视觉语言分割模型与低复杂度卷积神经网络进行集成，而不是传统的文本提示工程方法。

Result: 在BKAI息肉数据集上Dice分数提升了6.3%，其他数据集也有1%-6%的提升，但在不同数据集上集成效果存在差异。

Conclusion: 集成方法在不同数据集上的表现不一致，有些超越CRIS模型，有些则不如，这为未来研究提供了新的方向。

Abstract: Vision-language models and their adaptations to image segmentation tasks
present enormous potential for producing highly accurate and interpretable
results. However, implementations based on CLIP and BiomedCLIP are still
lagging behind more sophisticated architectures such as CRIS. In this work,
instead of focusing on text prompt engineering as is the norm, we attempt to
narrow this gap by showing how to ensemble vision-language segmentation models
(VLSMs) with a low-complexity CNN. By doing so, we achieve a significant Dice
score improvement of 6.3% on the BKAI polyp dataset using the ensembled
BiomedCLIPSeg, while other datasets exhibit gains ranging from 1% to 6%.
Furthermore, we provide initial results on additional four radiology and
non-radiology datasets. We conclude that ensembling works differently across
these datasets (from outperforming to underperforming the CRIS model),
indicating a topic for future investigation by the community. The code is
available at https://github.com/juliadietlmeier/VLSM-Ensemble.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [67] [Histogram Driven Amplitude Embedding for Qubit Efficient Quantum Image Compression](https://arxiv.org/abs/2509.04849)
*Sahil Tomar,Sandeep Kumar*

Main category: quant-ph

TL;DR: 提出一种使用近期量子设备压缩彩色图像的紧凑硬件高效方法，通过分块处理、强度直方图构建和量子态振幅嵌入实现图像压缩


<details>
  <summary>Details</summary>
Motivation: 开发适用于当前NISQ时代量子系统的实用图像压缩方法，解决传统像素级编码量子比特效率低的问题

Method: 将图像分割为固定大小的bixel块，计算每个块的总强度，构建全局直方图，将归一化的桶计数平方根编码为n量子比特量子态的振幅

Result: 使用仅5-7个量子比特即可实现高质量图像重建，在量子比特效率方面显著优于传统像素级编码方法

Conclusion: 该方法为当前量子硬件提供了一种实用的图像压缩解决方案，通过调整直方图桶数可以在保真度和资源使用之间进行权衡

Abstract: This work introduces a compact and hardware efficient method for compressing
color images using near term quantum devices. The approach segments the image
into fixed size blocks called bixels, and computes the total intensity within
each block. A global histogram with B bins is then constructed from these block
intensities, and the normalized square roots of the bin counts are encoded as
amplitudes into an n qubit quantum state. Amplitude embedding is performed
using PennyLane and executed on real IBM Quantum hardware. The resulting state
is measured to reconstruct the histogram, enabling approximate recovery of
block intensities and full image reassembly. The method maintains a constant
qubit requirement based solely on the number of histogram bins, independent of
the resolution of the image. By adjusting B, users can control the trade off
between fidelity and resource usage. Empirical results demonstrate high quality
reconstructions using as few as 5 to 7 qubits, significantly outperforming
conventional pixel level encodings in terms of qubit efficiency and validating
the practical application of the method for current NISQ era quantum systems.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [68] [Improved 3D Scene Stylization via Text-Guided Generative Image Editing with Region-Based Control](https://arxiv.org/abs/2509.05285)
*Haruo Fujiwara,Yusuke Mukuta,Tatsuya Harada*

Main category: cs.GR

TL;DR: 本文提出了一种改进的文本驱动3D场景风格化方法，通过增强多视角一致性、引入参考注意力机制和区域控制风格迁移，解决了现有方法在质量、视角一致性和语义对应方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的文本驱动3D场景编辑和风格化方法虽然在2D生成模型方面取得了进展，但在同时确保高质量风格化和视角一致性方面仍存在挑战，特别是在对不同区域或对象进行语义对应的风格一致性应用方面。

Method: 1. 重新训练初始3D表示使用风格化的多视角2D图像；2. 扩展风格对齐的深度条件视角生成框架，用单参考注意力共享机制替代完全共享注意力机制；3. 使用多深度图网格作为单图像参考增强视角一致性；4. 提出多区域重要性加权切片Wasserstein距离损失，支持基于分割掩码的区域风格化。

Result: 实验评估表明，该管道有效提升了文本驱动3D风格化的结果，在定性和定量评估中都表现出色，能够实现更好的风格保真度和不同区域风格混合。

Conclusion: 该方法通过改进的注意力机制、深度参考系统和区域控制技术，成功解决了3D风格化中的视角一致性和语义对应问题，为高质量的文本驱动3D场景编辑提供了有效解决方案。

Abstract: Recent advances in text-driven 3D scene editing and stylization, which
leverage the powerful capabilities of 2D generative models, have demonstrated
promising outcomes. However, challenges remain in ensuring high-quality
stylization and view consistency simultaneously. Moreover, applying style
consistently to different regions or objects in the scene with semantic
correspondence is a challenging task. To address these limitations, we
introduce techniques that enhance the quality of 3D stylization while
maintaining view consistency and providing optional region-controlled style
transfer. Our method achieves stylization by re-training an initial 3D
representation using stylized multi-view 2D images of the source views.
Therefore, ensuring both style consistency and view consistency of stylized
multi-view images is crucial. We achieve this by extending the style-aligned
depth-conditioned view generation framework, replacing the fully shared
attention mechanism with a single reference-based attention-sharing mechanism,
which effectively aligns style across different viewpoints. Additionally,
inspired by recent 3D inpainting methods, we utilize a grid of multiple depth
maps as a single-image reference to further strengthen view consistency among
stylized images. Finally, we propose Multi-Region Importance-Weighted Sliced
Wasserstein Distance Loss, allowing styles to be applied to distinct image
regions using segmentation masks from off-the-shelf models. We demonstrate that
this optional feature enhances the faithfulness of style transfer and enables
the mixing of different styles across distinct regions of the scene.
Experimental evaluations, both qualitative and quantitative, demonstrate that
our pipeline effectively improves the results of text-driven 3D stylization.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [69] [Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning](https://arxiv.org/abs/2509.04734)
*Jasmine Shone,Shaden Alshammari,Mark Hamilton,Zhening Li,William Freeman*

Main category: cs.LG

TL;DR: Beyond I-Con框架通过探索替代统计散度和相似性核，系统性地发现新的损失函数，在多个表示学习任务中超越基于KL散度的标准方法


<details>
  <summary>Details</summary>
Motivation: I-Con框架发现23+表示学习方法隐含地最小化数据和学习分布间的KL散度，但KL散度的不对称性、无界性等特性可能导致优化挑战和目标不对齐

Method: 提出Beyond I-Con框架，系统探索替代统计散度（如总变差距离、有界f-散度）和相似性核（如距离核替代角度核），并应用于无监督聚类、监督对比学习和降维任务

Result: (1)在DINO-ViT嵌入的无监督聚类中，使用TV距离的PMI算法达到SOTA；(2)在监督对比学习中，TV+距离核优于标准KL+角度核；(3)在降维中，有界f-散度比SNE的KL散度获得更好的定性结果和下游任务性能

Conclusion: 统计散度和相似性核的选择对表示学习优化至关重要，Beyond I-Con框架为系统探索替代方案提供了有效途径

Abstract: The Information Contrastive (I-Con) framework revealed that over 23
representation learning methods implicitly minimize KL divergence between data
and learned distributions that encode similarities between data points.
However, a KL-based loss may be misaligned with the true objective, and
properties of KL divergence such as asymmetry and unboundedness may create
optimization challenges. We present Beyond I-Con, a framework that enables
systematic discovery of novel loss functions by exploring alternative
statistical divergences and similarity kernels. Key findings: (1) on
unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art
results by modifying the PMI algorithm to use total variation (TV) distance;
(2) on supervised contrastive learning, we outperform the standard approach by
using TV and a distance-based similarity kernel instead of KL and an angular
kernel; (3) on dimensionality reduction, we achieve superior qualitative
results and better performance on downstream tasks than SNE by replacing KL
with a bounded f-divergence. Our results highlight the importance of
considering divergence and similarity kernel choices in representation learning
optimization.

</details>
