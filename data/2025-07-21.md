<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 82]
- [eess.IV](#eess.IV) [Total: 12]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.GR](#cs.GR) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Open-Vocabulary Object Detection in UAV Imagery: A Review and Future Perspectives](https://arxiv.org/abs/2507.13359)
*Yang Zhou,Junjie Li,CongYang Ou,Dawei Yan,Haokui Zhang,Xizhe Xue*

Main category: cs.CV

TL;DR: 本文综述了无人机（UAV）航拍场景中的开放词汇目标检测（OVOD），探讨了其核心原理、现有方法分类、数据集、挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统无人机目标检测方法局限于预定义类别，而OVOD通过跨模态文本-图像对齐（如CLIP）实现了对未见物体的检测，提升了无人机在航拍场景中的智能性和自主性。

Method: 通过系统分类现有OVOD方法，并结合无人机视觉特性，构建了一个全面的综述框架，包括数据集分析和挑战讨论。

Result: 总结了OVOD在无人机航拍中的关键问题和开放挑战，并提出了未来研究方向。

Conclusion: 本文为研究人员提供了清晰的路线图和参考，旨在推动这一快速发展的领域的创新。

Abstract: Due to its extensive applications, aerial image object detection has long
been a hot topic in computer vision. In recent years, advancements in Unmanned
Aerial Vehicles (UAV) technology have further propelled this field to new
heights, giving rise to a broader range of application requirements. However,
traditional UAV aerial object detection methods primarily focus on detecting
predefined categories, which significantly limits their applicability. The
advent of cross-modal text-image alignment (e.g., CLIP) has overcome this
limitation, enabling open-vocabulary object detection (OVOD), which can
identify previously unseen objects through natural language descriptions. This
breakthrough significantly enhances the intelligence and autonomy of UAVs in
aerial scene understanding. This paper presents a comprehensive survey of OVOD
in the context of UAV aerial scenes. We begin by aligning the core principles
of OVOD with the unique characteristics of UAV vision, setting the stage for a
specialized discussion. Building on this foundation, we construct a systematic
taxonomy that categorizes existing OVOD methods for aerial imagery and provides
a comprehensive overview of the relevant datasets. This structured review
enables us to critically dissect the key challenges and open problems at the
intersection of these fields. Finally, based on this analysis, we outline
promising future research directions and application prospects. This survey
aims to provide a clear road map and a valuable reference for both newcomers
and seasoned researchers, fostering innovation in this rapidly evolving domain.
We keep tracing related works at
https://github.com/zhouyang2002/OVOD-in-UVA-imagery

</details>


### [2] [Low-Light Enhancement via Encoder-Decoder Network with Illumination Guidance](https://arxiv.org/abs/2507.13360)
*Le-Anh Tran,Chung Nguyen Tran,Ngoc-Luu Nguyen,Nhan Cach Dang,Jordi Carrabina,David Castells-Rufas,Minh Son Nguyen*

Main category: cs.CV

TL;DR: EDNIG是一种基于U-Net架构的新型深度学习框架，用于低光图像增强，通过亮度引导和多尺度特征提取提升性能。


<details>
  <summary>Details</summary>
Motivation: 低光环境下图像质量下降，传统方法难以有效增强，需要更智能的解决方案。

Method: 结合亮度图（BCP）、SPP模块和Swish激活函数，在GAN框架下优化。

Result: 在定量指标和视觉质量上优于现有方法，模型复杂度较低。

Conclusion: EDNIG适合实际应用，代码已开源。

Abstract: This paper introduces a novel deep learning framework for low-light image
enhancement, named the Encoder-Decoder Network with Illumination Guidance
(EDNIG). Building upon the U-Net architecture, EDNIG integrates an illumination
map, derived from Bright Channel Prior (BCP), as a guidance input. This
illumination guidance helps the network focus on underexposed regions,
effectively steering the enhancement process. To further improve the model's
representational power, a Spatial Pyramid Pooling (SPP) module is incorporated
to extract multi-scale contextual features, enabling better handling of diverse
lighting conditions. Additionally, the Swish activation function is employed to
ensure smoother gradient propagation during training. EDNIG is optimized within
a Generative Adversarial Network (GAN) framework using a composite loss
function that combines adversarial loss, pixel-wise mean squared error (MSE),
and perceptual loss. Experimental results show that EDNIG achieves competitive
performance compared to state-of-the-art methods in quantitative metrics and
visual quality, while maintaining lower model complexity, demonstrating its
suitability for real-world applications. The source code for this work is
available at https://github.com/tranleanh/ednig.

</details>


### [3] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: VLMs在复杂视觉任务中表现出色，但在非局部视觉推理任务中表现不佳，甚至不如人类。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在非局部视觉推理任务中的表现，揭示其核心视觉推理能力的不足。

Method: 设计三种非局部视觉任务（比较感知、扫视搜索和平滑视觉搜索），测试主流VLMs的性能。

Result: 主流VLMs在这些任务中表现不佳，准确率接近随机猜测，远低于人类水平。

Conclusion: 当前VLMs在视觉推理能力上仍有显著缺陷，需进一步改进。

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [4] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLM）的空间推理能力，通过链式思维（CoT）提示和强化学习（GRPO）优化，发现结构化多阶段提示（SceneGraph CoT）显著提升性能，而GRPO在泛化性和鲁棒性上优于监督微调（SFT）。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过提示策略和强化学习提升VLM的空间推理能力，并解决SFT在泛化性上的不足。

Method: 采用SceneGraph CoT提示策略和GRPO强化学习方法，在SAT数据集上微调模型，并在CVBench上评估性能。

Result: SceneGraph CoT显著提升空间推理准确性；GRPO在Pass@1评估和OOD条件下表现优于SFT，泛化性更强。

Conclusion: 结构化提示和强化学习能有效提升VLM的空间推理能力和泛化性，GRPO在鲁棒性上表现突出。

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [5] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: 利用2D视觉语言模型实现无标注的开放词汇3D物体检测。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测数据集类别有限且标注成本高，而2D视觉语言模型具有丰富的语义理解和开放词汇能力。

Method: 通过2D视觉语言模型生成文本条件提案，结合SAM分割和相机几何投影到3D空间，使用DBSCAN聚类和旋转卡尺推断3D边界框。

Result: 在LiDAR和RGB-D输入下均取得竞争性定位性能，且无需训练和标注。

Conclusion: 展示了2D基础模型在可扩展3D感知中的潜力。

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [6] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出了一种新型多模态多任务网络及训练算法，支持12种模态数据输入，通过共享架构和跨模态注意力实现统一嵌入空间，并在25个数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态和多任务场景下的数据融合与任务协同问题，提升模型在多种模态数据上的表现。

Method: 采用模态专用分词器、共享Transformer架构和跨注意力机制，提出迭代模态切换预训练策略和成对模态训练算法。

Result: 在12种模态的25个数据集上实现了最优性能，验证了架构和训练策略的有效性。

Conclusion: 提出的方法在多模态多任务场景中表现优异，为复杂数据融合和任务协同提供了有效解决方案。

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [7] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: 提出了一种结合光学动作捕捉和Transformer模型的端到端深度学习框架，用于增强医疗康复效果。


<details>
  <summary>Details</summary>
Motivation: 解决因遮挡和环境因素导致的数据噪声和缺失问题，同时实时检测异常动作以确保患者安全。

Method: 利用时间序列建模对动作捕捉数据进行去噪和补全，提高鲁棒性。

Result: 在卒中和骨科康复数据集上的评估显示，该框架在数据重建和异常检测方面表现优异。

Conclusion: 该框架为远程康复提供了一种可扩展、经济高效的解决方案，减少了对现场监督的需求。

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [8] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: 论文提出了一种结合Vision Transformers和Graph Neural Networks的创新框架，用于乳腺癌检测，准确率达84.2%。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是女性死亡的主要原因之一，早期检测对提高生存率至关重要。

Method: 结合ViT的全局图像特征捕捉能力和GNN的结构关系建模能力，使用CBIS-DDSM数据集。

Result: 准确率达到84.2%，优于传统方法，并提供可解释的注意力热图。

Conclusion: 该框架在乳腺癌检测中表现出色，并能为临床决策提供支持。

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [9] [Butter: Frequency Consistency and Hierarchical Fusion for Autonomous Driving Object Detection](https://arxiv.org/abs/2507.13373)
*Xiaojian Lin,Wenxin Zhang,Yuchu Jiang,Wangyu Wu,Yiran Guo,Kangxu Wang,Zongzheng Zhang,Guijin Wang,Lei Jin,Hao Zhao*

Main category: cs.CV

TL;DR: Butter是一个新颖的目标检测框架，通过增强分层特征表示来提高检测鲁棒性，适用于自动驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有架构（如YOLO和DETR）在多尺度特征一致性和计算效率之间难以平衡，Butter旨在解决这一问题。

Method: Butter引入了频率自适应特征一致性增强（FAFCE）和渐进分层特征融合网络（PHFFNet）模块。

Result: 在BDD100K、KITTI和Cityscapes数据集上，Butter表现出卓越的特征表示能力，显著提高了检测精度并降低了模型复杂度。

Conclusion: Butter通过分层特征优化和集成，在准确性、可部署性和计算效率之间取得了平衡，适用于实时自动驾驶场景。

Abstract: Hierarchical feature representations play a pivotal role in computer vision,
particularly in object detection for autonomous driving. Multi-level semantic
understanding is crucial for accurately identifying pedestrians, vehicles, and
traffic signs in dynamic environments. However, existing architectures, such as
YOLO and DETR, struggle to maintain feature consistency across different scales
while balancing detection precision and computational efficiency. To address
these challenges, we propose Butter, a novel object detection framework
designed to enhance hierarchical feature representations for improving
detection robustness. Specifically, Butter introduces two key innovations:
Frequency-Adaptive Feature Consistency Enhancement (FAFCE) Component, which
refines multi-scale feature consistency by leveraging adaptive frequency
filtering to enhance structural and boundary precision, and Progressive
Hierarchical Feature Fusion Network (PHFFNet) Module, which progressively
integrates multi-level features to mitigate semantic gaps and strengthen
hierarchical feature learning. Through extensive experiments on BDD100K, KITTI,
and Cityscapes, Butter demonstrates superior feature representation
capabilities, leading to notable improvements in detection accuracy while
reducing model complexity. By focusing on hierarchical feature refinement and
integration, Butter provides an advanced approach to object detection that
achieves a balance between accuracy, deployability, and computational
efficiency in real-time autonomous driving scenarios. Our model and
implementation are publicly available at https://github.com/Aveiro-Lin/Butter,
facilitating further research and validation within the autonomous driving
community.

</details>


### [10] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute是一种基于LLM的智能路由系统，通过动态选择最优模态提升多模态视频检索效率。


<details>
  <summary>Details</summary>
Motivation: 现有密集文本标注方法计算成本高且遗漏关键视觉信息，需要更高效的解决方案。

Method: 利用GPT-4.1分析查询意图，动态路由到ASR、OCR和视觉索引，减少模态搜索数量。

Result: 计算开销降低41%，Recall@5达60.9%，平均每查询使用1.78个模态。

Conclusion: 智能路由为多模态检索系统提供了可扩展的实用解决方案，兼顾成本与效果。

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [11] [A Comprehensive Survey for Real-World Industrial Defect Detection: Challenges, Approaches, and Prospects](https://arxiv.org/abs/2507.13378)
*Yuqi Cheng,Yunkang Cao,Haiming Yao,Wei Luo,Cheng Jiang,Hui Zhang,Weiming Shen*

Main category: cs.CV

TL;DR: 该论文综述了工业缺陷检测领域的最新进展，重点分析了从封闭集到开放集检测框架的转变，并探讨了2D和3D模态下的挑战与趋势。


<details>
  <summary>Details</summary>
Motivation: 传统工业缺陷检测方法难以满足现代制造业对精度、自动化和可扩展性的需求，而计算机视觉和深度学习的进步为这一领域带来了新的可能性。

Method: 论文通过深入分析封闭集和开放集缺陷检测策略，梳理了近年来2D和3D模态下的技术演变，并强调了开放集技术的崛起。

Result: 研究总结了实际检测环境中的关键挑战，并揭示了新兴趋势，为该领域提供了全面的视角。

Conclusion: 论文为工业缺陷检测领域提供了一个当前且全面的综述，突出了开放集技术的潜力及其未来发展方向。

Abstract: Industrial defect detection is vital for upholding product quality across
contemporary manufacturing systems. As the expectations for precision,
automation, and scalability intensify, conventional inspection approaches are
increasingly found wanting in addressing real-world demands. Notable progress
in computer vision and deep learning has substantially bolstered defect
detection capabilities across both 2D and 3D modalities. A significant
development has been the pivot from closed-set to open-set defect detection
frameworks, which diminishes the necessity for extensive defect annotations and
facilitates the recognition of novel anomalies. Despite such strides, a
cohesive and contemporary understanding of industrial defect detection remains
elusive. Consequently, this survey delivers an in-depth analysis of both
closed-set and open-set defect detection strategies within 2D and 3D
modalities, charting their evolution in recent years and underscoring the
rising prominence of open-set techniques. We distill critical challenges
inherent in practical detection environments and illuminate emerging trends,
thereby providing a current and comprehensive vista of this swiftly progressing
field.

</details>


### [12] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: 研究探讨了在卫星图像机器学习任务中融合其他地理数据层对模型性能的影响，发现多模态输入能显著提升性能，尤其在数据有限和地理样本外场景中。


<details>
  <summary>Details</summary>
Motivation: 现有卫星图像机器学习模型主要依赖光学输入，研究旨在探索其他地理数据层在监督学习中的价值。

Method: 通过将额外地理数据层附加到基准数据集上，生成增强版本，用于分类、回归和分割任务。

Result: 多模态输入显著提升模型性能，尤其在数据有限和样本外场景中，且硬编码融合策略优于学习策略。

Conclusion: 多模态输入对卫星图像机器学习的数据效率和样本外性能具有重要价值，硬编码融合策略值得未来研究关注。

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [13] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 提出了一种基于生成输出分布距离的最小化概念擦除方法，通过端到端优化和神经元掩码技术，在不影响模型性能的情况下有效擦除概念。


<details>
  <summary>Details</summary>
Motivation: 生成模型依赖大规模无标签数据引发安全和版权问题，现有擦除方法过度修改模型影响其效用。

Method: 基于生成输出的分布距离设计目标函数，利用反向传播进行端到端优化，并引入神经元掩码增强鲁棒性。

Result: 在流匹配模型上验证，方法能稳健擦除概念且不降低模型性能。

Conclusion: 为更安全、负责任的生成模型提供了可行路径。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [14] [From Binary to Semantic: Utilizing Large-Scale Binary Occupancy Data for 3D Semantic Occupancy Prediction](https://arxiv.org/abs/2507.13387)
*Chihiro Noguchi,Takaki Yamamoto*

Main category: cs.CV

TL;DR: 论文提出了一种利用大规模二进制占用数据（无语义标签）来增强3D语义占用预测的框架，通过分解预测过程为二进制和语义模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 3D语义占用预测需要标注的LiDAR点云数据，成本高昂；而二进制占用数据成本低但未被充分利用。论文探索了利用二进制数据的潜力。

Method: 提出了一种基于二进制占用的框架，将预测过程分解为二进制和语义占用模块，分别利用二进制数据进行预训练和学习型自动标注。

Result: 实验表明，该框架在预训练和自动标注任务中均优于现有方法，有效提升了3D语义占用预测的性能。

Conclusion: 通过利用低成本二进制占用数据，论文提出的框架显著提升了3D语义占用预测的效果，为视觉中心自动驾驶系统提供了更经济的解决方案。

Abstract: Accurate perception of the surrounding environment is essential for safe
autonomous driving. 3D occupancy prediction, which estimates detailed 3D
structures of roads, buildings, and other objects, is particularly important
for vision-centric autonomous driving systems that do not rely on LiDAR
sensors. However, in 3D semantic occupancy prediction -- where each voxel is
assigned a semantic label -- annotated LiDAR point clouds are required, making
data acquisition costly. In contrast, large-scale binary occupancy data, which
only indicate occupied or free space without semantic labels, can be collected
at a lower cost. Despite their availability, the potential of leveraging such
data remains unexplored. In this study, we investigate the utilization of
large-scale binary occupancy data from two perspectives: (1) pre-training and
(2) learning-based auto-labeling. We propose a novel binary occupancy-based
framework that decomposes the prediction process into binary and semantic
occupancy modules, enabling effective use of binary occupancy data. Our
experimental results demonstrate that the proposed framework outperforms
existing methods in both pre-training and auto-labeling tasks, highlighting its
effectiveness in enhancing 3D semantic occupancy prediction. The code is
available at https://github.com/ToyotaInfoTech/b2s-occupancy

</details>


### [15] [InSyn: Modeling Complex Interactions for Pedestrian Trajectory Prediction](https://arxiv.org/abs/2507.13397)
*Kaiyuan Zhai,Juan Chen,Chao Wang,Zeyi Xu*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的模型InSyn，通过显式捕捉多样化的交互模式（如同步行走或冲突行为）来提升行人轨迹预测的准确性，并引入SSOS训练策略以减少初始步预测误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖相对位置建模行人交互，忽略了特定交互模式（如配对行走或冲突行为），导致在拥挤场景中预测准确性受限。

Method: 提出InSyn模型，基于Transformer显式捕捉多样化交互模式，并引入SSOS训练策略以减少初始步预测误差。

Result: 在ETH和UCY数据集上显著优于基线模型，尤其是在高密度场景中；SSOS策略将初始步预测误差降低约6.58%。

Conclusion: InSyn模型通过显式建模多样化交互模式和SSOS策略，显著提升了行人轨迹预测的准确性，尤其在复杂场景中表现突出。

Abstract: Accurate pedestrian trajectory prediction is crucial for intelligent
applications, yet it remains highly challenging due to the complexity of
interactions among pedestrians. Previous methods have primarily relied on
relative positions to model pedestrian interactions; however, they tend to
overlook specific interaction patterns such as paired walking or conflicting
behaviors, limiting the prediction accuracy in crowded scenarios. To address
this issue, we propose InSyn (Interaction-Synchronization Network), a novel
Transformer-based model that explicitly captures diverse interaction patterns
(e.g., walking in sync or conflicting) while effectively modeling
direction-sensitive social behaviors. Additionally, we introduce a training
strategy termed Seq-Start of Seq (SSOS), designed to alleviate the common issue
of initial-step divergence in numerical time-series prediction. Experiments on
the ETH and UCY datasets demonstrate that our model outperforms recent
baselines significantly, especially in high-density scenarios. Furthermore, the
SSOS strategy proves effective in improving sequential prediction performance,
reducing the initial-step prediction error by approximately 6.58%.

</details>


### [16] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: MADI框架通过Masking-Augmented gaussian Diffusion和Pause Tokens机制，显著提升了扩散模型的可编辑性和可控性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像生成方面取得了显著成功，但在结构化视觉编辑和组合控制方面仍面临挑战。

Method: 提出MADI框架，包括MAgD训练策略（结合去噪和掩码重建）和基于Pause Tokens的推理时容量扩展机制。

Result: MADI显著提升了扩散模型的可编辑性、组合性和可控性。

Conclusion: MADI为扩散模型在通用上下文生成架构中的集成铺平了道路。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [17] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 提出一个多模态的驾驶员嗜睡检测数据集，包含面部、行为和生物信号。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏连续性和多模态信号，本研究旨在填补这一空白。

Method: 使用3D面部视频、IR摄像头、生物信号和模拟器数据，记录19名受试者在清醒和嗜睡状态下的数据。

Result: 数据集包含1,400分钟的连续记录，捕捉了驾驶员状态的渐变变化。

Conclusion: 该数据集为驾驶员嗜睡研究提供了更全面的多模态信号支持。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [18] [AortaDiff: Volume-Guided Conditional Diffusion Models for Multi-Branch Aortic Surface Generation](https://arxiv.org/abs/2507.13404)
*Delin An,Pan Du,Jian-Xun Wang,Chaoli Wang*

Main category: cs.CV

TL;DR: AortaDiff是一种基于扩散的框架，直接从CT/MRI体积生成平滑的主动脉表面，解决了现有方法依赖大量标注数据和手动干预的问题，适用于CFD分析。


<details>
  <summary>Details</summary>
Motivation: 准确的3D主动脉构建对临床诊断、术前规划和CFD模拟至关重要，但现有方法依赖大量标注数据和手动干预，且生成的网格几何一致性较差。

Method: AortaDiff采用体积引导的条件扩散模型生成主动脉中心线，并自动提取血管轮廓，最终拟合为平滑的3D表面。

Result: 实验表明，AortaDiff在有限训练数据下仍能有效构建正常和病理主动脉网格，几何保真度高，适用于CFD分析。

Conclusion: AortaDiff是一种端到端解决方案，减少了对标注数据的依赖，为心血管研究提供了高质量的主动脉网格生成工具。

Abstract: Accurate 3D aortic construction is crucial for clinical diagnosis,
preoperative planning, and computational fluid dynamics (CFD) simulations, as
it enables the estimation of critical hemodynamic parameters such as blood flow
velocity, pressure distribution, and wall shear stress. Existing construction
methods often rely on large annotated training datasets and extensive manual
intervention. While the resulting meshes can serve for visualization purposes,
they struggle to produce geometrically consistent, well-constructed surfaces
suitable for downstream CFD analysis. To address these challenges, we introduce
AortaDiff, a diffusion-based framework that generates smooth aortic surfaces
directly from CT/MRI volumes. AortaDiff first employs a volume-guided
conditional diffusion model (CDM) to iteratively generate aortic centerlines
conditioned on volumetric medical images. Each centerline point is then
automatically used as a prompt to extract the corresponding vessel contour,
ensuring accurate boundary delineation. Finally, the extracted contours are
fitted into a smooth 3D surface, yielding a continuous, CFD-compatible mesh
representation. AortaDiff offers distinct advantages over existing methods,
including an end-to-end workflow, minimal dependency on large labeled datasets,
and the ability to generate CFD-compatible aorta meshes with high geometric
fidelity. Experimental results demonstrate that AortaDiff performs effectively
even with limited training data, successfully constructing both normal and
pathologically altered aorta meshes, including cases with aneurysms or
coarctation. This capability enables the generation of high-quality
visualizations and positions AortaDiff as a practical solution for
cardiovascular research.

</details>


### [19] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 提出了COREVQA基准测试，用于评估视觉语言模型在拥挤场景下的视觉蕴含推理能力，结果显示现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少评估模型在视觉蕴含任务（如基于图像接受或反驳假设）上的能力，尤其是在拥挤场景中。

Method: 使用CrowdHuman数据集中的图像，生成了5608对图像与真假陈述对，构建了COREVQA基准测试。

Result: 即使表现最好的视觉语言模型准确率也低于80%，其他模型表现更差（39.98%-69.95%）。

Conclusion: 视觉语言模型在拥挤场景中的视觉蕴含推理能力存在显著不足，揭示了其局限性。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [20] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: IConMark是一种新型的语义水印方法，通过嵌入可解释的概念到AI生成的图像中，提高水印的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和合成媒体的快速发展，区分AI生成图像与真实图像变得至关重要，传统水印技术易受对抗攻击，需要更鲁棒且可解释的方法。

Method: IConMark通过嵌入有意义的语义属性，而非传统噪声或扰动，使水印对人类可解释且抗对抗攻击。还结合了StegaStamp和TrustMark形成混合方法。

Result: IConMark及其变体（+TM和+SS）在多个数据集上的水印检测AUROC得分分别比最佳基线高出10.8%、14.5%和15.9%。

Conclusion: IConMark不仅提高了水印的鲁棒性和可解释性，还能与现有技术结合进一步增强性能，为数字真实性提供了有效保障。

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [21] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: AI驱动的多模型深度学习系统通过集成技术显著提高了肩部骨折的检测准确率，适用于临床筛查。


<details>
  <summary>Details</summary>
Motivation: 解决肩部骨折在急诊和高流量临床环境中漏诊率高的问题，利用AI技术提供早期检测支持。

Method: 开发了基于10,000张标注肩部X光片的多模型深度学习系统，采用Faster R-CNN、EfficientDet和RF-DETR架构，并应用了多种集成技术。

Result: NMW集成技术实现了95.5%的准确率和0.9610的F1分数，优于单个模型，表现出高召回率和定位精度。

Conclusion: 集成AI系统能可靠检测肩部骨折，适合实时诊断流程，但当前模型仅限于二元骨折检测，用于快速筛查而非详细分类。

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [22] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 通过结合古老的CORONA卫星影像升级深度学习模型，显著提升了考古遗址自动识别的精度，并发现了传统方法未识别的新遗址。


<details>
  <summary>Details</summary>
Motivation: 利用CORONA卫星影像弥补现代环境变化导致的考古遗址消失问题，验证AI技术在考古领域的潜力。

Method: 基于Bing卷积网络模型，使用CORONA影像对伊拉克阿布格莱布地区进行重新训练。

Result: 检测精度显著提升（IoU>85%，准确率90%），并发现4个新考古遗址。

Conclusion: AI结合历史影像可有效发现已消失的考古遗址，为考古研究提供新突破。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [23] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: CaSTFormer是一种因果时空Transformer模型，用于精确预测驾驶意图，通过创新的机制提升预测的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以准确建模驾驶行为的复杂时空依赖性和不可预测性，需要更鲁棒的预测模型。

Method: 提出CaSTFormer，包含RSF机制、CPE模块和FSN网络，分别用于特征对齐、消除虚假相关性和合成特征。

Result: 在Brain4Cars数据集上达到最优性能，有效捕捉复杂因果依赖。

Conclusion: CaSTFormer显著提升了驾驶意图预测的准确性和可解释性。

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [24] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: PhyWorldBench是一个评估视频生成模型物理模拟能力的基准，涵盖从基础物理现象到复杂场景，并引入“反物理”类别。通过人类评估和MLLM方法，测试了12个模型，揭示了其在物理一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在物理现象模拟方面存在不足，需要系统评估其物理一致性。

Method: 设计PhyWorldBench基准，包含多级物理现象和“反物理”类别，结合人类评估和MLLM方法测试12个模型。

Result: 模型在物理一致性上表现不佳，尤其在复杂场景和“反物理”提示中。

Conclusion: 需改进模型物理模拟能力，并优化提示设计以提升物理真实性。

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [25] [Uncertainty Quantification Framework for Aerial and UAV Photogrammetry through Error Propagation](https://arxiv.org/abs/2507.13486)
*Debao Huang,Rongjun Qin*

Main category: cs.CV

TL;DR: 提出了一种用于摄影测量点云不确定性量化的自校准框架，填补了多视图立体（MVS）阶段不确定性估计的空白。


<details>
  <summary>Details</summary>
Motivation: 摄影测量点云的精度高度依赖场景，且MVS阶段的不确定性估计尚未标准化。本文旨在解决这一问题。

Method: 通过结合SfM和MVS阶段，提出自校准方法，利用可靠的多视图点回归视差不确定性。

Result: 在多种公开数据集上验证，方法优于现有方法，实现了高边界率且未高估不确定性。

Conclusion: 提出的框架为摄影测量点云提供了鲁棒且可验证的不确定性量化。

Abstract: Uncertainty quantification of the photogrammetry process is essential for
providing per-point accuracy credentials of the point clouds. Unlike airborne
LiDAR, which typically delivers consistent accuracy across various scenes, the
accuracy of photogrammetric point clouds is highly scene-dependent, since it
relies on algorithm-generated measurements (i.e., stereo or multi-view stereo).
Generally, errors of the photogrammetric point clouds propagate through a
two-step process: Structure-from-Motion (SfM) with Bundle adjustment (BA),
followed by Multi-view Stereo (MVS). While uncertainty estimation in the SfM
stage has been well studied using the first-order statistics of the
reprojection error function, that in the MVS stage remains largely unsolved and
non-standardized, primarily due to its non-differentiable and multi-modal
nature (i.e., from pixel values to geometry). In this paper, we present an
uncertainty quantification framework closing this gap by associating an error
covariance matrix per point accounting for this two-step photogrammetry
process. Specifically, to estimate the uncertainty in the MVS stage, we propose
a novel, self-calibrating method by taking reliable n-view points (n>=6)
per-view to regress the disparity uncertainty using highly relevant cues (such
as matching cost values) from the MVS stage. Compared to existing approaches,
our method uses self-contained, reliable 3D points extracted directly from the
MVS process, with the benefit of being self-supervised and naturally adhering
to error propagation path of the photogrammetry process, thereby providing a
robust and certifiable uncertainty quantification across diverse scenes. We
evaluate the framework using a variety of publicly available airborne and UAV
imagery datasets. Results demonstrate that our method outperforms existing
approaches by achieving high bounding rates without overestimating uncertainty.

</details>


### [26] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: 提出了一种基于3D卷积自编码器的无监督方法，用于从Sentinel-2卫星图像序列中检测甜菜田的压力状态。


<details>
  <summary>Details</summary>
Motivation: 卫星图像时间序列（SITS）数据因其丰富的频谱和时间特性，适用于农业任务，但需要一种无需标注的无监督方法来检测甜菜田的压力状态。

Method: 使用3D卷积自编码器提取Sentinel-2图像序列的特征，并结合特定采集时间的时间编码，以捕捉甜菜的生长动态。

Result: 模型提取的特征用于下游聚类任务，成功区分了压力田和健康田，且系统可直接应用于不同年份的数据。

Conclusion: 该方法为甜菜田压力检测提供了一种实用且无需监督的工具。

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [27] [SparseC-AFM: a deep learning method for fast and accurate characterization of MoS$_2$ with C-AFM](https://arxiv.org/abs/2507.13527)
*Levi Harris,Md Jayed Hossain,Mufan Qiu,Ruichen Zhang,Pingchuan Ma,Tianlong Chen,Jiaqi Gu,Seth Ariel Tongay,Umberto Celano*

Main category: cs.CV

TL;DR: SparseC-AFM是一种基于深度学习的模型，能够从稀疏的C-AFM扫描中快速准确地重建2D材料的导电性图，显著减少采集时间。


<details>
  <summary>Details</summary>
Motivation: 传统C-AFM技术虽然精度高，但数据采集速度慢，难以满足大规模生产的需求。

Method: 提出SparseC-AFM模型，利用稀疏扫描数据重建高分辨率导电性图。

Result: 相比传统方法，采集时间减少11倍，且模型预测结果与高分辨率数据相似。

Conclusion: SparseC-AFM为AI辅助2D材料表征从实验室研究到工业应用的转化迈出重要一步。

Abstract: The increasing use of two-dimensional (2D) materials in nanoelectronics
demands robust metrology techniques for electrical characterization, especially
for large-scale production. While atomic force microscopy (AFM) techniques like
conductive AFM (C-AFM) offer high accuracy, they suffer from slow data
acquisition speeds due to the raster scanning process. To address this, we
introduce SparseC-AFM, a deep learning model that rapidly and accurately
reconstructs conductivity maps of 2D materials like MoS$_2$ from sparse C-AFM
scans. Our approach is robust across various scanning modes, substrates, and
experimental conditions. We report a comparison between (a) classic flow
implementation, where a high pixel density C-AFM image (e.g., 15 minutes to
collect) is manually parsed to extract relevant material parameters, and (b)
our SparseC-AFM method, which achieves the same operation using data that
requires substantially less acquisition time (e.g., under 5 minutes).
SparseC-AFM enables efficient extraction of critical material parameters in
MoS$_2$, including film coverage, defect density, and identification of
crystalline island boundaries, edges, and cracks. We achieve over 11x reduction
in acquisition time compared to manual extraction from a full-resolution C-AFM
image. Moreover, we demonstrate that our model-predicted samples exhibit
remarkably similar electrical properties to full-resolution data gathered using
classic-flow scanning. This work represents a significant step toward
translating AI-assisted 2D material characterization from laboratory research
to industrial fabrication. Code and model weights are available at
github.com/UNITES-Lab/sparse-cafm.

</details>


### [28] [Total Generalized Variation of the Normal Vector Field and Applications to Mesh Denoising](https://arxiv.org/abs/2507.13530)
*Lukas Baumgärtner,Ronny Bergmann,Roland Herzog,Stephan Schmidt,Manuel Weiß*

Main category: cs.CV

TL;DR: 提出了一种新的二阶总广义变分（TGV）公式，用于处理三维空间中三角形网格上的法向量，并将其与现有方法在网格去噪实验中进行了比较。


<details>
  <summary>Details</summary>
Motivation: 现有的离散TGV模型主要用于标量数据，而法向量作为流形值函数（单位球面上的值）需要新的扩展方法。

Method: 构建了一个定制的切向Raviart-Thomas型有限元空间，以扩展TGV公式到流形设置。

Result: 新正则化器在网格去噪实验中表现出色。

Conclusion: 该方法为处理流形值函数提供了有效的正则化工具，并在网格去噪中展示了优越性能。

Abstract: We propose a novel formulation for the second-order total generalized
variation (TGV) of the normal vector on an oriented, triangular mesh embedded
in $\mathbb{R}^3$. The normal vector is considered as a manifold-valued
function, taking values on the unit sphere. Our formulation extends previous
discrete TGV models for piecewise constant scalar data that utilize a
Raviart-Thomas function space. To exctend this formulation to the manifold
setting, a tailor-made tangential Raviart-Thomas type finite element space is
constructed in this work. The new regularizer is compared to existing methods
in mesh denoising experiments.

</details>


### [29] [$\nabla$NABLA: Neighborhood Adaptive Block-Level Attention](https://arxiv.org/abs/2507.13546)
*Dmitrii Mikhailov,Aleksey Letunovskiy,Maria Kovaleva,Vladimir Arkhipkin,Vladimir Korviakov,Vladimir Polovnikov,Viacheslav Vasilev,Evelina Sidorova,Denis Dimitrov*

Main category: cs.CV

TL;DR: NABLA提出了一种新型的邻域自适应块级注意力机制，用于降低视频扩散变换器中注意力计算的复杂度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决基于变换器的视频生成任务中，全注意力机制的二次复杂度问题，尤其是高分辨率和长视频序列的场景。

Method: 通过块级注意力和自适应稀疏阈值，动态适应视频扩散变换器中的稀疏模式，减少计算开销。

Result: 实验表明，NABLA在训练和推理速度上比基线快2.7倍，且几乎不影响生成质量。

Conclusion: NABLA是一种高效且无需定制低层操作设计的注意力机制，可无缝集成到现有框架中。

Abstract: Recent progress in transformer-based architectures has demonstrated
remarkable success in video generation tasks. However, the quadratic complexity
of full attention mechanisms remains a critical bottleneck, particularly for
high-resolution and long-duration video sequences. In this paper, we propose
NABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that
dynamically adapts to sparsity patterns in video diffusion transformers (DiTs).
By leveraging block-wise attention with adaptive sparsity-driven threshold,
NABLA reduces computational overhead while preserving generative quality. Our
method does not require custom low-level operator design and can be seamlessly
integrated with PyTorch's Flex Attention operator. Experiments demonstrate that
NABLA achieves up to 2.7x faster training and inference compared to baseline
almost without compromising quantitative metrics (CLIP score, VBench score,
human evaluation score) and visual quality drop. The code and model weights are
available here: https://github.com/gen-ai-team/Wan2.1-NABLA

</details>


### [30] [LoRA-Loop: Closing the Synthetic Replay Cycle for Continual VLM Learning](https://arxiv.org/abs/2507.13568)
*Kaihong Wang,Donghyun Kim,Margrit Betke*

Main category: cs.CV

TL;DR: 提出了一种基于LoRA增强的合成重放框架，通过任务特定的低秩适配器提升生成样本的保真度，从而优化持续学习性能。


<details>
  <summary>Details</summary>
Motivation: 现有合成重放方法生成的样本可能因未捕捉领域特定细节而误导微调，导致知识遗忘。

Method: 采用两阶段置信度样本选择：先筛选真实任务数据微调LoRA，再生成并筛选合成样本用于蒸馏。

Result: 在MTIL基准测试中表现优于现有方法，平衡了可塑性、稳定性和零样本能力。

Conclusion: 通过LoRA适配生成器，显著提升了视觉语言模型在持续学习中的鲁棒性。

Abstract: Continual learning for vision-language models has achieved remarkable
performance through synthetic replay, where samples are generated using Stable
Diffusion to regularize during finetuning and retain knowledge. However,
real-world downstream applications often exhibit domain-specific nuances and
fine-grained semantics not captured by generators, causing synthetic-replay
methods to produce misaligned samples that misguide finetuning and undermine
retention of prior knowledge. In this work, we propose a LoRA-enhanced
synthetic-replay framework that injects task-specific low-rank adapters into a
frozen Stable Diffusion model, efficiently capturing each new task's unique
visual and semantic patterns. Specifically, we introduce a two-stage,
confidence-based sample selection: we first rank real task data by
post-finetuning VLM confidence to focus LoRA finetuning on the most
representative examples, then generate synthetic samples and again select them
by confidence for distillation. Our approach integrates seamlessly with
existing replay pipelines-simply swap in the adapted generator to boost replay
fidelity. Extensive experiments on the Multi-domain Task Incremental Learning
(MTIL) benchmark show that our method outperforms previous synthetic-replay
techniques, achieving an optimal balance among plasticity, stability, and
zero-shot capability. These results demonstrate the effectiveness of generator
adaptation via LoRA for robust continual learning in VLMs.

</details>


### [31] [NoiseSDF2NoiseSDF: Learning Clean Neural Fields from Noisy Supervision](https://arxiv.org/abs/2507.13595)
*Tengkai Wang,Weihao Li,Ruikai Cui,Shi Qiu,Nick Barnes*

Main category: cs.CV

TL;DR: NoiseSDF2NoiseSDF是一种新方法，通过从噪声点云中学习干净的神经SDF，显著提高了噪声输入下的表面重建质量。


<details>
  <summary>Details</summary>
Motivation: 低质量扫描设备捕获的点云通常包含大量噪声，导致表面重建不准确。

Method: 通过最小化噪声SDF表示之间的MSE损失，直接从噪声点云中学习干净的神经SDF。

Result: 在ShapeNet、ABC、Famous和Real数据集上的实验表明，该方法显著提高了表面重建质量。

Conclusion: NoiseSDF2NoiseSDF有效解决了噪声点云下的表面重建问题。

Abstract: Reconstructing accurate implicit surface representations from point clouds
remains a challenging task, particularly when data is captured using
low-quality scanning devices. These point clouds often contain substantial
noise, leading to inaccurate surface reconstructions. Inspired by the
Noise2Noise paradigm for 2D images, we introduce NoiseSDF2NoiseSDF, a novel
method designed to extend this concept to 3D neural fields. Our approach
enables learning clean neural SDFs directly from noisy point clouds through
noisy supervision by minimizing the MSE loss between noisy SDF representations,
allowing the network to implicitly denoise and refine surface estimations. We
evaluate the effectiveness of NoiseSDF2NoiseSDF on benchmarks, including the
ShapeNet, ABC, Famous, and Real datasets. Experimental results demonstrate that
our framework significantly improves surface reconstruction quality from noisy
inputs.

</details>


### [32] [Learning Deblurring Texture Prior from Unpaired Data with Diffusion Model](https://arxiv.org/abs/2507.13599)
*Chengxu Liu,Lu Qi,Jinshan Pan,Xueming Qian,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的图像去模糊框架，通过从非配对数据中学习空间变化的纹理先验，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于获取大量真实的模糊-清晰图像对困难且昂贵，从非配对数据中学习盲图像去模糊更具实际意义。现有方法依赖对抗学习，忽略了真实世界模糊模式的复杂性。

Method: 提出了一种基于扩散模型的框架（\ours），包括纹理先验编码器（TPE）和纹理转移变换层（TTformer），利用FM-MSA自适应滤波去除空间变化的模糊。

Result: 在广泛使用的基准测试中，\ours表现优于现有最先进方法。

Conclusion: \ours为无监督去模糊提供了一种有前景的解决方案，能够有效恢复模糊图像的纹理细节。

Abstract: Since acquiring large amounts of realistic blurry-sharp image pairs is
difficult and expensive, learning blind image deblurring from unpaired data is
a more practical and promising solution. Unfortunately, dominant approaches
rely heavily on adversarial learning to bridge the gap from blurry domains to
sharp domains, ignoring the complex and unpredictable nature of real-world blur
patterns. In this paper, we propose a novel diffusion model (DM)-based
framework, dubbed \ours, for image deblurring by learning spatially varying
texture prior from unpaired data. In particular, \ours performs DM to generate
the prior knowledge that aids in recovering the textures of blurry images. To
implement this, we propose a Texture Prior Encoder (TPE) that introduces a
memory mechanism to represent the image textures and provides supervision for
DM training. To fully exploit the generated texture priors, we present the
Texture Transfer Transformer layer (TTformer), in which a novel
Filter-Modulated Multi-head Self-Attention (FM-MSA) efficiently removes
spatially varying blurring through adaptive filtering. Furthermore, we
implement a wavelet-based adversarial loss to preserve high-frequency texture
details. Extensive evaluations show that \ours provides a promising
unsupervised deblurring solution and outperforms SOTA methods in widely-used
benchmarks.

</details>


### [33] [Efficient Burst Super-Resolution with One-step Diffusion](https://arxiv.org/abs/2507.13607)
*Kento Kawai,Takeru Oba,Kyotaro Tokoro,Kazutoshi Akita,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的随机采样方法，用于从低分辨率（LR）图像序列中生成高保真超分辨率（SR）图像，显著减少了运行时间。


<details>
  <summary>Details</summary>
Motivation: 现有确定性方法生成的SR图像模糊且感知质量差，希望通过扩散模型提升清晰度和保真度。

Method: 采用随机采样器结合高阶ODE和一步扩散知识蒸馏，提升扩散模型效率。

Result: 实验表明，方法将运行时间降至基线的1.6%，同时保持图像失真和感知质量。

Conclusion: 该方法在高效生成高质量SR图像方面具有潜力。

Abstract: While burst Low-Resolution (LR) images are useful for improving their Super
Resolution (SR) image compared to a single LR image, prior burst SR methods are
trained in a deterministic manner, which produces a blurry SR image. Since such
blurry images are perceptually degraded, we aim to reconstruct sharp and
high-fidelity SR images by a diffusion model. Our method improves the
efficiency of the diffusion model with a stochastic sampler with a high-order
ODE as well as one-step diffusion using knowledge distillation. Our
experimental results demonstrate that our method can reduce the runtime to 1.6
% of its baseline while maintaining the SR quality measured based on image
distortion and perceptual quality.

</details>


### [34] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: CoTasks框架通过分解复杂视频问题为四个基础任务，显著提升了视频大语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型缺乏细粒度对象级理解的链式推理能力，需要结构化标注支持逐步推理。

Method: 提出CoTasks框架，将复杂视频问题分解为帧定位、实体跟踪、时空关系提取等四个基础任务，嵌入中间推理步骤。

Result: 在NeXT-QA基准测试中，LLaVA-video-7B和Qwen2.5-VL-3B模型推理性能显著提升，部分子类别提升达48.1分。

Conclusion: CoTasks作为一种结构化监督框架，有效提升了视频推理的组合性能力。

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [35] [Moving Object Detection from Moving Camera Using Focus of Expansion Likelihood and Segmentation](https://arxiv.org/abs/2507.13628)
*Masahiro Ogawa,Qi An,Atsushi Yamashita*

Main category: cs.CV

TL;DR: FoELS方法通过结合光流和纹理信息，有效分离移动和静态物体，适用于复杂场景和相机运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖光流，难以在复杂结构化场景中检测移动物体。

Method: FoELS通过计算光流的扩展焦点（FoE）并融合纹理信息，生成运动概率。

Result: 在DAVIS 2016数据集和真实交通视频中表现优异，达到先进水平。

Conclusion: FoELS在复杂场景和相机运动中表现出色，解决了现有方法的局限性。

Abstract: Separating moving and static objects from a moving camera viewpoint is
essential for 3D reconstruction, autonomous navigation, and scene understanding
in robotics. Existing approaches often rely primarily on optical flow, which
struggles to detect moving objects in complex, structured scenes involving
camera motion. To address this limitation, we propose Focus of Expansion
Likelihood and Segmentation (FoELS), a method based on the core idea of
integrating both optical flow and texture information. FoELS computes the focus
of expansion (FoE) from optical flow and derives an initial motion likelihood
from the outliers of the FoE computation. This likelihood is then fused with a
segmentation-based prior to estimate the final moving probability. The method
effectively handles challenges including complex structured scenes, rotational
camera motion, and parallel motion. Comprehensive evaluations on the DAVIS 2016
dataset and real-world traffic videos demonstrate its effectiveness and
state-of-the-art performance.

</details>


### [36] [EPSilon: Efficient Point Sampling for Lightening of Hybrid-based 3D Avatar Generation](https://arxiv.org/abs/2507.13648)
*Seungjun Moon,Sangjoon Yu,Gyeong-Moon Park*

Main category: cs.CV

TL;DR: EPSilon提出了一种高效的混合3D头像生成方法，通过空射线省略（ERO）和空区间省略（EIO）策略，显著减少了计算成本，同时保持了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于NeRF和SMPL的混合模型在生成高质量头像时，由于变形计算成本高，导致推理速度慢。EPSilon旨在通过优化采样策略解决这一问题。

Method: EPSilon采用ERO和EIO两种方法，分别省略空射线和空区间，减少无效采样点，从而降低计算成本。

Result: EPSilon仅需3.9%的采样点，推理速度提升约20倍，训练收敛速度提升4倍，同时保持生成质量。

Conclusion: EPSilon通过高效采样策略，显著提升了混合3D头像生成的效率，为实时应用提供了可能。

Abstract: The rapid advancement of neural radiance fields (NeRF) has paved the way to
generate animatable human avatars from a monocular video. However, the sole
usage of NeRF suffers from a lack of details, which results in the emergence of
hybrid representation that utilizes SMPL-based mesh together with NeRF
representation. While hybrid-based models show photo-realistic human avatar
generation qualities, they suffer from extremely slow inference due to their
deformation scheme: to be aligned with the mesh, hybrid-based models use the
deformation based on SMPL skinning weights, which needs high computational
costs on each sampled point. We observe that since most of the sampled points
are located in empty space, they do not affect the generation quality but
result in inference latency with deformation. In light of this observation, we
propose EPSilon, a hybrid-based 3D avatar generation scheme with novel
efficient point sampling strategies that boost both training and inference. In
EPSilon, we propose two methods to omit empty points at rendering; empty ray
omission (ERO) and empty interval omission (EIO). In ERO, we wipe out rays that
progress through the empty space. Then, EIO narrows down the sampling interval
on the ray, which wipes out the region not occupied by either clothes or mesh.
The delicate sampling scheme of EPSilon enables not only great computational
cost reduction during deformation but also the designation of the important
regions to be sampled, which enables a single-stage NeRF structure without
hierarchical sampling. Compared to existing methods, EPSilon maintains the
generation quality while using only 3.9% of sampled points and achieves around
20 times faster inference, together with 4 times faster training convergence.
We provide video results on https://github.com/seungjun-moon/epsilon.

</details>


### [37] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 本文提出了一个大规模RGB-Event行人重识别数据集EvReID，并提出了TriPro-ReID框架，通过对比学习和行人属性增强特征学习。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机行人重识别方法在小规模或模拟数据集上训练，难以评估真实性能和泛化能力，因此需要大规模数据集和更有效的算法。

Method: 构建EvReID数据集，并提出TriPro-ReID框架，结合RGB和事件流数据，利用行人属性进行对比学习。

Result: 在EvReID和MARS数据集上的实验验证了TriPro-ReID的有效性。

Conclusion: EvReID数据集和TriPro-ReID框架为未来研究提供了数据和基准支持。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [38] [Global Modeling Matters: A Fast, Lightweight and Effective Baseline for Efficient Image Restoration](https://arxiv.org/abs/2507.13663)
*Xingyu Jiang,Ning Gao,Hongkun Dou,Xiuhui Zhang,Xiaoqing Zhong,Yue Deng,Hongjue Li*

Main category: cs.CV

TL;DR: PW-FNet是一种基于金字塔小波-傅里叶迭代管道的高效图像修复网络，通过多尺度分解和傅里叶变换替代自注意力机制，显著提升修复质量和效率。


<details>
  <summary>Details</summary>
Motivation: 恶劣天气条件会降低图像质量，影响下游任务性能。现有基于Transformer的方法复杂度高，难以实时处理。

Method: 提出PW-FNet，结合金字塔小波多输入多输出结构和傅里叶变换，实现多尺度分解并降低计算复杂度。

Result: 在多种修复任务中，PW-FNet在质量和效率上均优于现有方法，参数和计算成本显著降低。

Conclusion: PW-FNet展示了小波-傅里叶处理在图像修复中的潜力，为高效修复提供了新基准。

Abstract: Natural image quality is often degraded by adverse weather conditions,
significantly impairing the performance of downstream tasks. Image restoration
has emerged as a core solution to this challenge and has been widely discussed
in the literature. Although recent transformer-based approaches have made
remarkable progress in image restoration, their increasing system complexity
poses significant challenges for real-time processing, particularly in
real-world deployment scenarios. To this end, most existing methods attempt to
simplify the self-attention mechanism, such as by channel self-attention or
state space model. However, these methods primarily focus on network
architecture while neglecting the inherent characteristics of image restoration
itself. In this context, we explore a pyramid Wavelet-Fourier iterative
pipeline to demonstrate the potential of Wavelet-Fourier processing for image
restoration. Inspired by the above findings, we propose a novel and efficient
restoration baseline, named Pyramid Wavelet-Fourier Network (PW-FNet).
Specifically, PW-FNet features two key design principles: 1) at the inter-block
level, integrates a pyramid wavelet-based multi-input multi-output structure to
achieve multi-scale and multi-frequency bands decomposition; and 2) at the
intra-block level, incorporates Fourier transforms as an efficient alternative
to self-attention mechanisms, effectively reducing computational complexity
while preserving global modeling capability. Extensive experiments on tasks
such as image deraining, raindrop removal, image super-resolution, motion
deblurring, image dehazing, image desnowing and underwater/low-light
enhancement demonstrate that PW-FNet not only surpasses state-of-the-art
methods in restoration quality but also achieves superior efficiency, with
significantly reduced parameter size, computational cost and inference time.

</details>


### [39] [MaskHOI: Robust 3D Hand-Object Interaction Estimation via Masked Pre-training](https://arxiv.org/abs/2507.13673)
*Yuechen Xie,Haobo Jiang,Jian Yang,Yigong Zhang,Jin Xie*

Main category: cs.CV

TL;DR: 提出了一种基于掩码自编码器（MAE）的预训练框架MaskHOI，用于提升单目RGB输入下的手-物体交互（HOI）姿态估计。


<details>
  <summary>Details</summary>
Motivation: 由于RGB图像的几何模糊性和交互过程中的严重遮挡，精确估计手和物体的关节姿态非常困难。

Method: 采用区域特定的掩码比率分配和骨架驱动的手部掩码引导，结合掩码符号距离场（SDF）驱动的多模态学习机制。

Result: 实验表明，该方法显著优于现有最先进方法。

Conclusion: MaskHOI通过几何感知和遮挡鲁棒的特征学习，有效提升了HOI姿态估计的精度。

Abstract: In 3D hand-object interaction (HOI) tasks, estimating precise joint poses of
hands and objects from monocular RGB input remains highly challenging due to
the inherent geometric ambiguity of RGB images and the severe mutual occlusions
that occur during interaction.To address these challenges, we propose MaskHOI,
a novel Masked Autoencoder (MAE)-driven pretraining framework for enhanced HOI
pose estimation. Our core idea is to leverage the masking-then-reconstruction
strategy of MAE to encourage the feature encoder to infer missing spatial and
structural information, thereby facilitating geometric-aware and
occlusion-robust representation learning. Specifically, based on our
observation that human hands exhibit far greater geometric complexity than
rigid objects, conventional uniform masking fails to effectively guide the
reconstruction of fine-grained hand structures. To overcome this limitation, we
introduce a Region-specific Mask Ratio Allocation, primarily comprising the
region-specific masking assignment and the skeleton-driven hand masking
guidance. The former adaptively assigns lower masking ratios to hand regions
than to rigid objects, balancing their feature learning difficulty, while the
latter prioritizes masking critical hand parts (e.g., fingertips or entire
fingers) to realistically simulate occlusion patterns in real-world
interactions. Furthermore, to enhance the geometric awareness of the pretrained
encoder, we introduce a novel Masked Signed Distance Field (SDF)-driven
multimodal learning mechanism. Through the self-masking 3D SDF prediction, the
learned encoder is able to perceive the global geometric structure of hands and
objects beyond the 2D image plane, overcoming the inherent limitations of
monocular input and alleviating self-occlusion issues. Extensive experiments
demonstrate that our method significantly outperforms existing state-of-the-art
approaches.

</details>


### [40] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse是一个统一的框架，用于解决异构传感器配置下的协同感知问题，通过分层融合机制和自适应空间分辨率调整，显著提升了感知性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的V2X协同感知系统常因异构传感器配置导致特征融合和感知可靠性问题，需要一种通用解决方案。

Method: 采用分层融合机制（通道和空间注意力）和自适应空间分辨率调整模块，结合动态融合类型调整策略。

Result: 在TUMTraf-V2X数据集上，HeCoFuse在多种传感器配置下表现优异，3D mAP最高达43.38%，优于基线方法。

Conclusion: HeCoFuse在异构传感器配置下表现出色，成为当前TUM-Traf V2X数据集上的最先进方法。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [41] [Gaussian kernel-based motion measurement](https://arxiv.org/abs/2507.13693)
*Hongyi Liu,Haifeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于高斯核的运动测量方法，用于高精度结构健康监测，无需手动参数调整。


<details>
  <summary>Details</summary>
Motivation: 结构健康监测需求增长，但现有视觉方法在亚像素级运动测量中精度不足或需大量手动调参。

Method: 开发了基于高斯核的运动测量方法，引入运动一致性和超分辨率约束以提高精度和鲁棒性。

Result: 数值和实验验证表明，该方法无需定制参数即可实现高精度。

Conclusion: 新方法在结构健康监测中具有高精度和鲁棒性，适用于实际应用。

Abstract: The growing demand for structural health monitoring has driven increasing
interest in high-precision motion measurement, as structural information
derived from extracted motions can effectively reflect the current condition of
the structure. Among various motion measurement techniques, vision-based
methods stand out due to their low cost, easy installation, and large-scale
measurement. However, when it comes to sub-pixel-level motion measurement,
current vision-based methods either lack sufficient accuracy or require
extensive manual parameter tuning (e.g., pyramid layers, target pixels, and
filter parameters) to reach good precision. To address this issue, we developed
a novel Gaussian kernel-based motion measurement method, which can extract the
motion between different frames via tracking the location of Gaussian kernels.
The motion consistency, which fits practical structural conditions, and a
super-resolution constraint, are introduced to increase accuracy and robustness
of our method. Numerical and experimental validations show that it can
consistently reach high accuracy without customized parameter setup for
different test samples.

</details>


### [42] [GOSPA and T-GOSPA quasi-metrics for evaluation of multi-object tracking algorithms](https://arxiv.org/abs/2507.13706)
*Ángel F. García-Fernández,Jinhao Gu,Lennart Svensson,Yuxuan Xia,Jan Krejčí,Oliver Kost,Ondřej Straka*

Main category: cs.CV

TL;DR: 本文提出了两种用于多目标跟踪（MOT）算法性能评估的准度量，分别基于GOSPA和T-GOSPA的扩展，具有灵活的成本设置和非对称定位误差惩罚能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GOSPA和T-GOSPA度量在多目标跟踪评估中缺乏灵活性，无法对不同错误类型（如漏检和误检）设置不同成本，且定位误差惩罚必须对称。本文旨在解决这些问题。

Method: 提出两种准度量：一种基于GOSPA扩展，用于衡量目标集合的差异；另一种基于T-GOSPA扩展，用于衡量轨迹集合的差异。两者支持非对称成本设置。

Result: 通过仿真实验，使用T-GOSPA准度量评估了多种贝叶斯MOT算法的性能，验证了其灵活性和实用性。

Conclusion: 所提出的准度量在多目标跟踪评估中更具灵活性，能够适应不同应用场景的需求，特别是在需要非对称惩罚和差异化成本设置的场景中表现突出。

Abstract: This paper introduces two quasi-metrics for performance assessment of
multi-object tracking (MOT) algorithms. In particular, one quasi-metric is an
extension of the generalised optimal subpattern assignment (GOSPA) metric and
measures the discrepancy between sets of objects. The other quasi-metric is an
extension of the trajectory GOSPA (T-GOSPA) metric and measures the discrepancy
between sets of trajectories. Similar to the GOSPA-based metrics, these
quasi-metrics include costs for localisation error for properly detected
objects, the number of false objects and the number of missed objects. The
T-GOSPA quasi-metric also includes a track switching cost. Differently from the
GOSPA and T-GOSPA metrics, the proposed quasi-metrics have the flexibility of
penalising missed and false objects with different costs, and the localisation
costs are not required to be symmetric. These properties can be useful in MOT
evaluation in certain applications. The performance of several Bayesian MOT
algorithms is assessed with the T-GOSPA quasi-metric via simulations.

</details>


### [43] [PoemTale Diffusion: Minimising Information Loss in Poem to Image Generation with Multi-Stage Prompt Refinement](https://arxiv.org/abs/2507.13708)
*Sofia Jamil,Bollampalli Areen Reddy,Raghvendra Kumar,Sriparna Saha,Koustava Goswami,K. J. Joseph*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法PoemTale Diffusion，通过多阶段提示优化和自注意力机制改进诗歌文本到图像的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型在处理诗歌等复杂、抽象的创意表达时效果不佳，信息丢失严重。

Method: 采用多阶段提示优化循环和一致性自注意力技术，生成多张一致图像以传达诗歌含义。

Result: 通过专家评估和定量实验验证了方法的有效性，并发布了P4I数据集。

Conclusion: PoemTale Diffusion为诗歌到图像生成提供了新视角，显著提升了生成图像的信息捕获能力。

Abstract: Recent advancements in text-to-image diffusion models have achieved
remarkable success in generating realistic and diverse visual content. A
critical factor in this process is the model's ability to accurately interpret
textual prompts. However, these models often struggle with creative
expressions, particularly those involving complex, abstract, or highly
descriptive language. In this work, we introduce a novel training-free approach
tailored to improve image generation for a unique form of creative language:
poetic verse, which frequently features layered, abstract, and dual meanings.
Our proposed PoemTale Diffusion approach aims to minimise the information that
is lost during poetic text-to-image conversion by integrating a multi stage
prompt refinement loop into Language Models to enhance the interpretability of
poetic texts. To support this, we adapt existing state-of-the-art diffusion
models by modifying their self-attention mechanisms with a consistent
self-attention technique to generate multiple consistent images, which are then
collectively used to convey the poem's meaning. Moreover, to encourage research
in the field of poetry, we introduce the P4I (PoemForImage) dataset, consisting
of 1111 poems sourced from multiple online and offline resources. We engaged a
panel of poetry experts for qualitative assessments. The results from both
human and quantitative evaluations validate the efficacy of our method and
contribute a novel perspective to poem-to-image generation with enhanced
information capture in the generated images.

</details>


### [44] [Augmented Reality in Cultural Heritage: A Dual-Model Pipeline for 3D Artwork Reconstruction](https://arxiv.org/abs/2507.13719)
*Daniele Pannone,Alessia Castronovo,Maurizio Mancini,Gian Luca Foresti,Claudio Piciarelli,Rossana Gabrieli,Muhammad Yasir Bilal,Danilo Avola*

Main category: cs.CV

TL;DR: 提出了一种针对博物馆环境的增强现实流程，通过单张图像识别艺术品并生成精确的3D模型。


<details>
  <summary>Details</summary>
Motivation: 博物馆希望通过交互式数字内容提升游客参与度，但艺术品的不规则轮廓和多样纹理增加了3D重建的难度。

Method: 结合GLPN和Depth-Anything两种预训练深度估计模型，生成优化的深度图，并转换为高质量点云和网格。

Result: 实验结果显示重建精度和视觉真实感显著提升。

Conclusion: 该系统为博物馆提供了一种强大的工具，能够通过沉浸式AR体验增强游客互动。

Abstract: This paper presents an innovative augmented reality pipeline tailored for
museum environments, aimed at recognizing artworks and generating accurate 3D
models from single images. By integrating two complementary pre-trained depth
estimation models, i.e., GLPN for capturing global scene structure and
Depth-Anything for detailed local reconstruction, the proposed approach
produces optimized depth maps that effectively represent complex artistic
features. These maps are then converted into high-quality point clouds and
meshes, enabling the creation of immersive AR experiences. The methodology
leverages state-of-the-art neural network architectures and advanced computer
vision techniques to overcome challenges posed by irregular contours and
variable textures in artworks. Experimental results demonstrate significant
improvements in reconstruction accuracy and visual realism, making the system a
highly robust tool for museums seeking to enhance visitor engagement through
interactive digital content.

</details>


### [45] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: 分析StyleGAN生成器的内部机制，揭示其权重可修剪性及潜在伦理风险。


<details>
  <summary>Details</summary>
Motivation: 探索StyleGAN生成器的工作原理，以理解其生成高度逼真合成图像的能力。

Method: 使用PyTorch训练StyleGAN模型，通过修剪权重和检查潜在向量来分析其行为。

Result: 发现大量权重可修剪而不显著影响输出，且潜在向量可精确控制面部特征。

Conclusion: StyleGAN的技术能力带来学术价值，但也存在被滥用于伪造身份的重大伦理风险。

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [46] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL利用冻结的文本到图像扩散模型解决少样本类增量学习问题，通过多尺度特征提取和潜在重放提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决少样本类增量学习中数据有限和灾难性遗忘的挑战。

Method: 使用冻结的扩散模型作为主干，提取多尺度特征并结合特征蒸馏。

Result: 在CUB-200、miniImageNet和CIFAR-100上超越现有方法。

Conclusion: Diffusion-FSCIL有效平衡新旧类别的学习，性能优越。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [47] [Encapsulated Composition of Text-to-Image and Text-to-Video Models for High-Quality Video Synthesis](https://arxiv.org/abs/2507.13753)
*Tongtong Su,Chengyu Wang,Bingyan Liu,Jun Huang,Dongming Lu*

Main category: cs.CV

TL;DR: EVS是一种无需训练的封装视频合成器，结合T2I和T2V模型，提升生成视频的视觉质量和运动平滑性。


<details>
  <summary>Details</summary>
Motivation: 现有T2V模型在生成高质量视频时存在画面闪烁和伪影问题，EVS旨在解决这些问题。

Method: 利用预训练的T2I模型优化低质量视频帧，同时结合T2V模型确保运动一致性。

Result: 实验证明EVS在视觉质量和运动平滑性上优于现有方法，推理速度提升1.6x-4.5x。

Conclusion: EVS通过结合T2I和T2V模型的优势，显著提升了视频生成的质量和效率。

Abstract: In recent years, large text-to-video (T2V) synthesis models have garnered
considerable attention for their abilities to generate videos from textual
descriptions. However, achieving both high imaging quality and effective motion
representation remains a significant challenge for these T2V models. Existing
approaches often adapt pre-trained text-to-image (T2I) models to refine video
frames, leading to issues such as flickering and artifacts due to
inconsistencies across frames. In this paper, we introduce EVS, a training-free
Encapsulated Video Synthesizer that composes T2I and T2V models to enhance both
visual fidelity and motion smoothness of generated videos. Our approach
utilizes a well-trained diffusion-based T2I model to refine low-quality video
frames by treating them as out-of-distribution samples, effectively optimizing
them with noising and denoising steps. Meanwhile, we employ T2V backbones to
ensure consistent motion dynamics. By encapsulating the T2V temporal-only prior
into the T2I generation process, EVS successfully leverages the strengths of
both types of models, resulting in videos of improved imaging and motion
quality. Experimental results validate the effectiveness of our approach
compared to previous approaches. Our composition process also leads to a
significant improvement of 1.6x-4.5x speedup in inference time. Source codes:
https://github.com/Tonniia/EVS.

</details>


### [48] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的光谱扩散先验（SDP）和光谱先验注入模块（SPIM），显著提升了高光谱图像（HSI）重建的性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法难以准确捕捉HSI的高频细节，需要一种更有效的方法来提升重建质量。

Method: 通过扩散模型隐式学习HSI的光谱扩散先验（SDP），并结合动态指导的光谱先验注入模块（SPIM）来恢复细节。

Result: 在MST和BISRNet两种HSI方法上，性能提升约0.5 dB。

Conclusion: SDP和SPIM的组合显著提升了HSI重建的细节恢复能力，优于现有方法。

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [49] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: 论文提出了一种基于排列熵（PE）的新型图像分类方法，结合HOG和LBP特征，提供了一种轻量级且可解释的替代方案。


<details>
  <summary>Details</summary>
Motivation: 在图像分类中，当需要优先考虑可解释性和计算效率时，传统机器学习方法仍具有重要价值。本文旨在探索排列熵在图像数据中的应用潜力。

Method: 将PE扩展到二维图像，并结合HOG和LBP特征，构建了一个780维的手工特征集，用于训练SVM分类器。

Result: 在多个基准数据集上表现优异，证明了该方法在不依赖深度学习的情况下仍具有竞争力。

Conclusion: PE与HOG和LBP的结合为图像分类提供了一种轻量级、可解释且高效的解决方案，展示了熵基描述符的潜力。

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [50] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: ClearVQA benchmark introduced to address ambiguity in VQA through interactive clarification, overcoming lack of benchmarks and VLM training biases.


<details>
  <summary>Details</summary>
Motivation: Existing methods neglect interactive clarification in VQA, focusing only on rephrasing questions.

Method: Introduces ClearVQA benchmark targeting three common ambiguity categories in VQA.

Result: Provides a framework to assess VLMs' ability to resolve ambiguities interactively.

Conclusion: ClearVQA addresses gaps in interactive clarification research for VLMs.

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [51] [SuperCM: Improving Semi-Supervised Learning and Domain Adaptation through differentiable clustering](https://arxiv.org/abs/2507.13779)
*Durgesh Singh,Ahcène Boubekki,Robert Jenssen,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 论文提出了一种显式引入可微分聚类模块的方法，用于半监督学习和无监督域适应，通过利用监督数据计算聚类中心，提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 通过显式利用聚类假设，改进半监督学习和无监督域适应的性能，尤其是在低监督情况下。

Method: 引入可微分聚类模块，利用监督数据计算聚类中心，采用端到端训练策略。

Result: 实验表明该方法在低监督情况下表现优异，可作为独立模型或现有方法的正则化器。

Conclusion: 显式聚类模块是一种简单有效的策略，能够提升半监督学习和无监督域适应的性能。

Abstract: Semi-Supervised Learning (SSL) and Unsupervised Domain Adaptation (UDA)
enhance the model performance by exploiting information from labeled and
unlabeled data. The clustering assumption has proven advantageous for learning
with limited supervision and states that data points belonging to the same
cluster in a high-dimensional space should be assigned to the same category.
Recent works have utilized different training mechanisms to implicitly enforce
this assumption for the SSL and UDA. In this work, we take a different approach
by explicitly involving a differentiable clustering module which is extended to
leverage the supervised data to compute its centroids. We demonstrate the
effectiveness of our straightforward end-to-end training strategy for SSL and
UDA over extensive experiments and highlight its benefits, especially in low
supervision regimes, both as a standalone model and as a regularizer for
existing approaches.

</details>


### [52] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: LoFNO是一种新型3D架构，通过结合几何先验和神经算子框架，提升血流数据的时空分辨率，直接预测壁剪切应力（WSS），优于传统方法和深度学习替代方案。


<details>
  <summary>Details</summary>
Motivation: 磁共振血流成像的低时空分辨率和信噪比限制了其诊断效果，需要一种方法提升分辨率和预测能力。

Method: 提出LoFNO架构，整合拉普拉斯特征向量作为几何先验，结合EDSR层进行鲁棒上采样，实现去噪和时空上采样。

Result: LoFNO在速度和WSS预测上优于插值和其他深度学习方法，提升了脑血管诊断的精确性。

Conclusion: LoFNO通过几何先验和神经算子的结合，显著提升了血流数据的时空分辨率和诊断能力。

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [53] [DynFaceRestore: Balancing Fidelity and Quality in Diffusion-Guided Blind Face Restoration with Dynamic Blur-Level Mapping and Guidance](https://arxiv.org/abs/2507.13797)
*Huu-Phu Do,Yu-Wei Chen,Yi-Cheng Liao,Chi-Wei Hsiao,Han-Yang Wang,Wei-Chen Chiu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: DynFaceRestore提出了一种动态盲脸恢复方法，通过动态选择扩散起始时间步和局部调整引导强度，平衡保真度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有盲脸恢复方法因固定扩散采样时间步和全局引导尺度，导致保真度与质量不平衡。

Method: 利用高斯模糊图像及其核动态选择扩散起始时间步，并引入动态引导缩放调整器局部调整引导强度。

Result: 在定量和定性评估中均达到最优性能，展示了鲁棒性和有效性。

Conclusion: DynFaceRestore通过动态策略有效解决了盲脸恢复中的保真度与质量平衡问题。

Abstract: Blind Face Restoration aims to recover high-fidelity, detail-rich facial
images from unknown degraded inputs, presenting significant challenges in
preserving both identity and detail. Pre-trained diffusion models have been
increasingly used as image priors to generate fine details. Still, existing
methods often use fixed diffusion sampling timesteps and a global guidance
scale, assuming uniform degradation. This limitation and potentially imperfect
degradation kernel estimation frequently lead to under- or over-diffusion,
resulting in an imbalance between fidelity and quality. We propose
DynFaceRestore, a novel blind face restoration approach that learns to map any
blindly degraded input to Gaussian blurry images. By leveraging these blurry
images and their respective Gaussian kernels, we dynamically select the
starting timesteps for each blurry image and apply closed-form guidance during
the diffusion sampling process to maintain fidelity. Additionally, we introduce
a dynamic guidance scaling adjuster that modulates the guidance strength across
local regions, enhancing detail generation in complex areas while preserving
structural fidelity in contours. This strategy effectively balances the
trade-off between fidelity and quality. DynFaceRestore achieves
state-of-the-art performance in both quantitative and qualitative evaluations,
demonstrating robustness and effectiveness in blind face restoration.

</details>


### [54] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: 提出了一种新的时间3D语义场景补全框架CF-SSC，通过预测伪未来帧扩展感知范围，提升遮挡推理和场景补全精度。


<details>
  <summary>Details</summary>
Motivation: 现有单目SSC方法在真实交通场景中因遮挡或视野限制表现不佳，需解决这一问题。

Method: 结合姿态和深度建立3D对应关系，在3D空间中融合过去、当前和预测的未来帧，显式建模时空关系。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准测试中达到最先进性能。

Conclusion: CF-SSC通过时空建模显著提升了遮挡推理和3D场景补全的准确性。

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [55] [GRAM-MAMBA: Holistic Feature Alignment for Wireless Perception with Adaptive Low-Rank Compensation](https://arxiv.org/abs/2507.13803)
*Weiqi Yang,Xu Zhou,Jingfu Guan,Hao Du,Tianyu Bai*

Main category: cs.CV

TL;DR: GRAM-MAMBA框架通过线性复杂度的Mamba模型和优化的GRAM矩阵策略，解决了多模态融合中的效率、对齐和鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 现有系统在资源受限环境中部署困难，模态对齐不充分，且对缺失数据鲁棒性差，阻碍了高效多模态感知的实现。

Method: 结合Mamba模型处理时间序列数据，使用GRAM矩阵优化模态对齐，并引入低秩自适应层补偿缺失模态。

Result: 在SPAWC2021和USC-HAD数据集上表现优异，显著提升了性能并减少了参数训练量。

Conclusion: GRAM-MAMBA在资源受限环境中实现了高效且鲁棒的多模态感知。

Abstract: Multi-modal fusion is crucial for Internet of Things (IoT) perception, widely
deployed in smart homes, intelligent transport, industrial automation, and
healthcare. However, existing systems often face challenges: high model
complexity hinders deployment in resource-constrained environments,
unidirectional modal alignment neglects inter-modal relationships, and
robustness suffers when sensor data is missing. These issues impede efficient
and robust multimodal perception in real-world IoT settings. To overcome these
limitations, we propose GRAM-MAMBA. This framework utilizes the
linear-complexity Mamba model for efficient sensor time-series processing,
combined with an optimized GRAM matrix strategy for pairwise alignment among
modalities, addressing the shortcomings of traditional single-modality
alignment. Inspired by Low-Rank Adaptation (LoRA), we introduce an adaptive
low-rank layer compensation strategy to handle missing modalities
post-training. This strategy freezes the pre-trained model core and irrelevant
adaptive layers, fine-tuning only those related to available modalities and the
fusion process. Extensive experiments validate GRAM-MAMBA's effectiveness. On
the SPAWC2021 indoor positioning dataset, the pre-trained model shows lower
error than baselines; adapting to missing modalities yields a 24.5% performance
boost by training less than 0.2% of parameters. On the USC-HAD human activity
recognition dataset, it achieves 93.55% F1 and 93.81% Overall Accuracy (OA),
outperforming prior work; the update strategy increases F1 by 23% while
training less than 0.3% of parameters. These results highlight GRAM-MAMBA's
potential for achieving efficient and robust multimodal perception in
resource-constrained environments.

</details>


### [56] [SkySense V2: A Unified Foundation Model for Multi-modal Remote Sensing](https://arxiv.org/abs/2507.13812)
*Yingying Zhang,Lixiang Ru,Kang Wu,Lei Yu,Lei Liang,Yansheng Li,Jingdong Chen*

Main category: cs.CV

TL;DR: SkySense V2是一个统一的多模态遥感基础模型，通过单一Transformer主干处理多模态数据，采用自适应补丁合并和可学习模态提示令牌，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要为每种数据模态训练单独的主干网络，导致冗余和参数利用效率低下，且预训练方法未充分考虑遥感图像的特性。

Method: 使用单一Transformer主干，结合自适应补丁合并模块和可学习模态提示令牌，并引入专家混合（MoE）模块。

Result: 在7个任务的16个数据集上评估，平均性能比SkySense提升1.8分。

Conclusion: SkySense V2通过统一架构和针对性预训练策略，显著提升了多模态遥感任务的性能。

Abstract: The multi-modal remote sensing foundation model (MM-RSFM) has significantly
advanced various Earth observation tasks, such as urban planning, environmental
monitoring, and natural disaster management. However, most existing approaches
generally require the training of separate backbone networks for each data
modality, leading to redundancy and inefficient parameter utilization.
Moreover, prevalent pre-training methods typically apply self-supervised
learning (SSL) techniques from natural images without adequately accommodating
the characteristics of remote sensing (RS) images, such as the complicated
semantic distribution within a single RS image. In this work, we present
SkySense V2, a unified MM-RSFM that employs a single transformer backbone to
handle multiple modalities. This backbone is pre-trained with a novel SSL
strategy tailored to the distinct traits of RS data. In particular, SkySense V2
incorporates an innovative adaptive patch merging module and learnable modality
prompt tokens to address challenges related to varying resolutions and limited
feature diversity across modalities. In additional, we incorporate the mixture
of experts (MoE) module to further enhance the performance of the foundation
model. SkySense V2 demonstrates impressive generalization abilities through an
extensive evaluation involving 16 datasets over 7 tasks, outperforming SkySense
by an average of 1.8 points.

</details>


### [57] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的开放视频问答框架，通过多模型协作提升推理深度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频-大型多模态模型（Video-LMMs）在复杂场景中表现不佳，存在上下文理解有限、时序建模弱、泛化能力差等问题。

Method: 引入提示-响应集成机制，协调多个异构视频-语言模型（VLMs），通过结构化思维链进行推理，并由外部大型语言模型（LLM）作为评估和集成器。

Result: 实验表明，该方法在所有评估指标上显著优于现有基线，展现出卓越的泛化能力和鲁棒性。

Conclusion: 该方法为无需重新训练模型的多模态推理提供了一种轻量级、可扩展的策略，为未来Video-LMM的发展奠定了基础。

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [58] [A Quantum-assisted Attention U-Net for Building Segmentation over Tunis using Sentinel-1 Data](https://arxiv.org/abs/2507.13852)
*Luigi Russo,Francesco Mauro,Babak Memar,Alessandro Sebastianelli,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 论文研究了使用Quanvolution预处理增强Attention U-Net模型在建筑分割中的能力，结果显示该方法在保持精度的同时减少了参数。


<details>
  <summary>Details</summary>
Motivation: 城市建筑分割在规划和灾害响应中很重要，但高分辨率卫星图像带来挑战。

Method: 结合Quanvolution和Attention U-Net处理Sentinel-1 SAR图像，提取更丰富的特征。

Result: 方法在测试精度上与标准模型相当，但参数更少。

Conclusion: 量子辅助深度学习框架在大规模城市建筑分割中具有潜力。

Abstract: Building segmentation in urban areas is essential in fields such as urban
planning, disaster response, and population mapping. Yet accurately segmenting
buildings in dense urban regions presents challenges due to the large size and
high resolution of satellite images. This study investigates the use of a
Quanvolutional pre-processing to enhance the capability of the Attention U-Net
model in the building segmentation. Specifically, this paper focuses on the
urban landscape of Tunis, utilizing Sentinel-1 Synthetic Aperture Radar (SAR)
imagery. In this work, Quanvolution was used to extract more informative
feature maps that capture essential structural details in radar imagery,
proving beneficial for accurate building segmentation. Preliminary results
indicate that proposed methodology achieves comparable test accuracy to the
standard Attention U-Net model while significantly reducing network parameters.
This result aligns with findings from previous works, confirming that
Quanvolution not only maintains model accuracy but also increases computational
efficiency. These promising outcomes highlight the potential of
quantum-assisted Deep Learning frameworks for large-scale building segmentation
in urban environments.

</details>


### [59] [Depth3DLane: Fusing Monocular 3D Lane Detection with Self-Supervised Monocular Depth Estimation](https://arxiv.org/abs/2507.13857)
*Max van den Hoven,Kishaan Jeeveswaran,Pieter Piscaer,Thijs Wensveen,Elahe Arani,Bahram Zonooz*

Main category: cs.CV

TL;DR: Depth3DLane是一种新颖的双路径框架，通过自监督单目深度估计提供显式结构信息，无需昂贵传感器或额外深度数据。


<details>
  <summary>Details</summary>
Motivation: 解决单目3D车道检测因缺乏显式空间信息而具有挑战性的问题，同时避免依赖昂贵传感器或难以大规模收集的深度数据。

Method: 采用双路径框架，结合自监督深度网络获取点云表示，并通过3D车道锚点从两个路径采样特征以推断准确3D车道几何。

Result: 在OpenLane基准数据集上表现优异，且无需相机标定即可应用。

Conclusion: Depth3DLane在无需昂贵传感器或深度数据的情况下，实现了高性能的3D车道检测，并扩展了适用场景。

Abstract: Monocular 3D lane detection is essential for autonomous driving, but
challenging due to the inherent lack of explicit spatial information.
Multi-modal approaches rely on expensive depth sensors, while methods
incorporating fully-supervised depth networks rely on ground-truth depth data
that is impractical to collect at scale. Additionally, existing methods assume
that camera parameters are available, limiting their applicability in scenarios
like crowdsourced high-definition (HD) lane mapping. To address these
limitations, we propose Depth3DLane, a novel dual-pathway framework that
integrates self-supervised monocular depth estimation to provide explicit
structural information, without the need for expensive sensors or additional
ground-truth depth data. Leveraging a self-supervised depth network to obtain a
point cloud representation of the scene, our bird's-eye view pathway extracts
explicit spatial information, while our front view pathway simultaneously
extracts rich semantic information. Depth3DLane then uses 3D lane anchors to
sample features from both pathways and infer accurate 3D lane geometry.
Furthermore, we extend the framework to predict camera parameters on a
per-frame basis and introduce a theoretically motivated fitting procedure to
enhance stability on a per-segment basis. Extensive experiments demonstrate
that Depth3DLane achieves competitive performance on the OpenLane benchmark
dataset. Furthermore, experimental results show that using learned parameters
instead of ground-truth parameters allows Depth3DLane to be applied in
scenarios where camera calibration is infeasible, unlike previous methods.

</details>


### [60] [PositionIC: Unified Position and Identity Consistency for Image Customization](https://arxiv.org/abs/2507.13861)
*Junjie Hu,Tianyang Han,Kai Ma,Jialin Gao,Hao Dou,Song Yang,Xianhua He,Jianhui Zhang,Junfeng Luo,Xiaoming Wei,Wenqiang Zhang*

Main category: cs.CV

TL;DR: PositionIC是一个统一框架，通过位置和身份一致性实现多主题图像定制，解决了现有方法在细粒度空间控制上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有图像定制方法在实体级空间控制上表现不佳，主要缺乏绑定身份与精确位置的可扩展数据集。

Method: 构建了一个双向生成范式的合成管道，设计了轻量级位置调制层，解耦空间嵌入以独立准确放置主题。

Result: 实验表明，PositionIC在保持图像定制高一致性的同时实现了精确的空间控制。

Conclusion: PositionIC为开放世界多实体场景下的可控高保真图像定制铺平了道路，并将公开发布以促进进一步研究。

Abstract: Recent subject-driven image customization has achieved significant
advancements in fidelity, yet fine-grained entity-level spatial control remains
elusive, hindering the broader real-world application. This limitation is
mainly attributed to scalable datasets that bind identity with precise
positional cues are absent. To this end, we introduce PositionIC, a unified
framework that enforces position and identity consistency for multi-subject
customization. We construct a scalable synthesis pipeline that employs a
bidirectional generation paradigm to eliminate subject drift and maintain
semantic coherence. On top of these data, we design a lightweight positional
modulation layer that decouples spatial embeddings among subjects, enabling
independent, accurate placement while preserving visual fidelity. Extensive
experiments demonstrate that our approach can achieve precise spatial control
while maintaining high consistency in image customization task. PositionIC
paves the way for controllable, high-fidelity image customization in
open-world, multi-entity scenarios and will be released to foster further
research.

</details>


### [61] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 研究分析了视觉语言模型（VLMs）如何处理内部知识与外部信息之间的冲突，通过引入多模态反事实查询数据集，定位了控制冲突的关键注意力头，并展示了其优于梯度归因的精确性。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理复杂任务时，内部参数化知识与外部信息之间可能产生冲突，导致幻觉和不可靠响应，但其机制尚不明确。

Method: 引入多模态反事实查询数据集，通过logit检查定位控制冲突的注意力头，并修改这些头以引导模型偏向内部知识或视觉输入。

Result: 研究发现特定注意力头能精确定位驱动视觉覆盖的图像区域，且其表现优于梯度归因方法。

Conclusion: 研究揭示了VLMs处理知识冲突的机制，为模型优化提供了新方向。

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [62] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 提出了一种通过融合实时视觉数据和海图信息来增强海洋视觉的新方法，利用基于transformer的端到端神经网络实现导航辅助物的准确匹配。


<details>
  <summary>Details</summary>
Motivation: 海洋环境中实时视觉数据与海图信息的融合可以提高导航辅助物的定位和关联准确性，从而增强海洋视觉。

Method: 使用基于transformer的端到端神经网络预测浮标查询的边界框和置信度分数，直接匹配图像域检测与世界空间海图标记。

Result: 在真实海洋场景数据集上的实验表明，该方法显著提高了动态和挑战性环境中的目标定位和关联准确性。

Conclusion: 该方法在海洋视觉增强中表现出优越性能，为动态环境中的导航辅助物匹配提供了有效解决方案。

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [63] [PCR-GS: COLMAP-Free 3D Gaussian Splatting via Pose Co-Regularizations](https://arxiv.org/abs/2507.13891)
*Yu Wei,Jiahui Zhang,Xiaoqin Zhang,Ling Shao,Shijian Lu*

Main category: cs.CV

TL;DR: PCR-GS是一种无需COLMAP的3D高斯泼溅技术，通过相机姿态共正则化提升复杂相机轨迹下的3D场景建模和相机姿态估计。


<details>
  <summary>Details</summary>
Motivation: 现有3D-GS技术在复杂相机轨迹（如剧烈旋转和平移）下表现不佳，导致相机姿态估计和联合优化陷入局部最优。

Method: PCR-GS通过特征重投影正则化和基于小波的高频细节正则化，优化相机姿态。

Result: 实验表明，PCR-GS在复杂相机轨迹下实现了优越的无姿态3D-GS场景建模。

Conclusion: PCR-GS通过双重正则化机制，显著提升了复杂场景下的3D建模和相机姿态估计性能。

Abstract: COLMAP-free 3D Gaussian Splatting (3D-GS) has recently attracted increasing
attention due to its remarkable performance in reconstructing high-quality 3D
scenes from unposed images or videos. However, it often struggles to handle
scenes with complex camera trajectories as featured by drastic rotation and
translation across adjacent camera views, leading to degraded estimation of
camera poses and further local minima in joint optimization of camera poses and
3D-GS. We propose PCR-GS, an innovative COLMAP-free 3DGS technique that
achieves superior 3D scene modeling and camera pose estimation via camera pose
co-regularization. PCR-GS achieves regularization from two perspectives. The
first is feature reprojection regularization which extracts view-robust DINO
features from adjacent camera views and aligns their semantic information for
camera pose regularization. The second is wavelet-based frequency
regularization which exploits discrepancy in high-frequency details to further
optimize the rotation matrix in camera poses. Extensive experiments over
multiple real-world scenes show that the proposed PCR-GS achieves superior
pose-free 3D-GS scene modeling under dramatic changes of camera trajectories.

</details>


### [64] [Enhancing LiDAR Point Features with Foundation Model Priors for 3D Object Detection](https://arxiv.org/abs/2507.13899)
*Yujian Mo,Yan Wu,Junqiao Zhao,Jijun Wang,Yinghao Hu,Jun Yan*

Main category: cs.CV

TL;DR: 论文提出了一种利用DepthAnything生成的深度先验增强LiDAR点云特征的方法，通过双路径RoI特征提取和双向门控融合模块提升3D目标检测精度。


<details>
  <summary>Details</summary>
Motivation: LiDAR点云特征表达能力有限，尤其是反射率属性的区分能力较弱，而视觉基础模型（如DepthAnything）提供的深度先验未被充分利用。

Method: 融合DepthAnything的深度先验与LiDAR原始属性，提出点级特征提取模块和双路径RoI特征提取框架（体素分支和点分支），并通过双向门控融合模块整合全局与局部特征。

Result: 在KITTI基准测试中，检测精度显著提升。

Conclusion: 视觉基础模型的深度先验可以有效增强LiDAR点云特征，提升3D目标检测性能。

Abstract: Recent advances in foundation models have opened up new possibilities for
enhancing 3D perception. In particular, DepthAnything offers dense and reliable
geometric priors from monocular RGB images, which can complement sparse LiDAR
data in autonomous driving scenarios. However, such priors remain underutilized
in LiDAR-based 3D object detection. In this paper, we address the limited
expressiveness of raw LiDAR point features, especially the weak discriminative
capability of the reflectance attribute, by introducing depth priors predicted
by DepthAnything. These priors are fused with the original LiDAR attributes to
enrich each point's representation. To leverage the enhanced point features, we
propose a point-wise feature extraction module. Then, a Dual-Path RoI feature
extraction framework is employed, comprising a voxel-based branch for global
semantic context and a point-based branch for fine-grained structural details.
To effectively integrate the complementary RoI features, we introduce a
bidirectional gated RoI feature fusion module that balances global and local
cues. Extensive experiments on the KITTI benchmark show that our method
consistently improves detection accuracy, demonstrating the value of
incorporating visual foundation model priors into LiDAR-based 3D object
detection.

</details>


### [65] [TimeNeRF: Building Generalizable Neural Radiance Fields across Time from Few-Shot Input Views](https://arxiv.org/abs/2507.13929)
*Hsiang-Hui Hung,Huu-Phu Do,Yung-Hui Li,Ching-Chun Huang*

Main category: cs.CV

TL;DR: TimeNeRF是一种可泛化的神经渲染方法，支持从任意视角和时间渲染新视图，适用于少样本输入。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，多视图采集成本高且优化效率低，而元宇宙等需要动态3D场景建模，尤其是昼夜过渡。现有NeRF技术虽擅长视图合成，但对时间建模的研究不足。

Method: 结合多视图立体、神经辐射场和解耦策略，构建隐式内容辐射场，支持少样本泛化和任意时间点的辐射场建模。

Result: TimeNeRF在少样本设置下无需逐场景优化即可渲染新视图，并能平滑过渡不同时间，捕捉自然场景变化。

Conclusion: TimeNeRF在动态场景建模中表现出色，为元宇宙等应用提供了高效解决方案。

Abstract: We present TimeNeRF, a generalizable neural rendering approach for rendering
novel views at arbitrary viewpoints and at arbitrary times, even with few input
views. For real-world applications, it is expensive to collect multiple views
and inefficient to re-optimize for unseen scenes. Moreover, as the digital
realm, particularly the metaverse, strives for increasingly immersive
experiences, the ability to model 3D environments that naturally transition
between day and night becomes paramount. While current techniques based on
Neural Radiance Fields (NeRF) have shown remarkable proficiency in synthesizing
novel views, the exploration of NeRF's potential for temporal 3D scene modeling
remains limited, with no dedicated datasets available for this purpose. To this
end, our approach harnesses the strengths of multi-view stereo, neural radiance
fields, and disentanglement strategies across diverse datasets. This equips our
model with the capability for generalizability in a few-shot setting, allows us
to construct an implicit content radiance field for scene representation, and
further enables the building of neural radiance fields at any arbitrary time.
Finally, we synthesize novel views of that time via volume rendering.
Experiments show that TimeNeRF can render novel views in a few-shot setting
without per-scene optimization. Most notably, it excels in creating realistic
novel views that transition smoothly across different times, adeptly capturing
intricate natural scene changes from dawn to dusk.

</details>


### [66] [DiViD: Disentangled Video Diffusion for Static-Dynamic Factorization](https://arxiv.org/abs/2507.13934)
*Marzieh Gheisari,Auguste Genovesio*

Main category: cs.CV

TL;DR: DiViD是一种端到端视频扩散框架，用于显式分离静态外观和动态运动，通过全局静态令牌和帧特定动态令牌实现，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于VAE和GAN的方法存在信息泄漏和模糊重建问题，需要一种更有效的视频静态-动态分离方法。

Method: DiViD使用序列编码器提取全局静态令牌和动态令牌，结合共享噪声计划、时间变化的KL瓶颈和交叉注意力机制。

Result: DiViD在真实数据集上表现优异，具有最高的交换联合准确率，同时减少交叉泄漏。

Conclusion: DiViD通过显式分离静态和动态内容，显著提升了视频分解的性能。

Abstract: Unsupervised disentanglement of static appearance and dynamic motion in video
remains a fundamental challenge, often hindered by information leakage and
blurry reconstructions in existing VAE- and GAN-based approaches. We introduce
DiViD, the first end-to-end video diffusion framework for explicit
static-dynamic factorization. DiViD's sequence encoder extracts a global static
token from the first frame and per-frame dynamic tokens, explicitly removing
static content from the motion code. Its conditional DDPM decoder incorporates
three key inductive biases: a shared-noise schedule for temporal consistency, a
time-varying KL-based bottleneck that tightens at early timesteps (compressing
static information) and relaxes later (enriching dynamics), and cross-attention
that routes the global static token to all frames while keeping dynamic tokens
frame-specific. An orthogonality regularizer further prevents residual
static-dynamic leakage. We evaluate DiViD on real-world benchmarks using
swap-based accuracy and cross-leakage metrics. DiViD outperforms
state-of-the-art sequential disentanglement methods: it achieves the highest
swap-based joint accuracy, preserves static fidelity while improving dynamic
transfer, and reduces average cross-leakage.

</details>


### [67] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: 研究发现视觉模型的感知能力与其短期预测性能强相关，提出了一种基于冻结视觉骨干的通用预测框架。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模型的感知能力与预测性能的关系，以提升视频理解的时空基础。

Method: 使用冻结视觉骨干训练潜在扩散模型预测未来特征，并通过轻量级任务特定解码器解码。

Result: 在九个模型和四个任务中验证了方法的有效性，展示了表示学习与生成建模结合的价值。

Conclusion: 研究强调了表示学习与生成建模结合对视频理解的重要性。

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [68] [Evaluation of Human Visual Privacy Protection: A Three-Dimensional Framework and Benchmark Dataset](https://arxiv.org/abs/2507.13981)
*Sara Abdulaziz,Giacomo D'Amicantonio,Egor Bondarev*

Main category: cs.CV

TL;DR: 本文提出了一个评估视觉隐私保护方法的综合框架，并引入了HR-VISPR数据集，用于训练可解释的隐私度量。


<details>
  <summary>Details</summary>
Motivation: AI驱动的监控技术引发了对敏感个人数据处理的担忧，需要客观评估隐私保护方法。

Method: 提出了一个三维评估框架（隐私、实用性和实用性），并利用HR-VISPR数据集评估了11种隐私保护方法。

Result: 框架能够区分隐私级别，并揭示隐私、实用性和实用性之间的权衡。

Conclusion: 该研究和数据集为隐私保护提供了结构化评估工具，适用于多样化场景。

Abstract: Recent advances in AI-powered surveillance have intensified concerns over the
collection and processing of sensitive personal data. In response, research has
increasingly focused on privacy-by-design solutions, raising the need for
objective techniques to evaluate privacy protection. This paper presents a
comprehensive framework for evaluating visual privacy-protection methods across
three dimensions: privacy, utility, and practicality. In addition, it
introduces HR-VISPR, a publicly available human-centric dataset with biometric,
soft-biometric, and non-biometric labels to train an interpretable privacy
metric. We evaluate 11 privacy protection methods, ranging from conventional
techniques to advanced deep-learning methods, through the proposed framework.
The framework differentiates privacy levels in alignment with human visual
perception, while highlighting trade-offs between privacy, utility, and
practicality. This study, along with the HR-VISPR dataset, serves as an
insightful tool and offers a structured evaluation framework applicable across
diverse contexts.

</details>


### [69] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: CSD-VAR是一种基于VAR框架的内容-风格分解方法，通过尺度感知优化、SVD校正和增强K-V记忆实现更好的分离效果。


<details>
  <summary>Details</summary>
Motivation: 探索VAR作为生成框架用于内容-风格分解，利用其逐尺度生成过程改进解耦能力。

Method: 提出CSD-VAR，包含尺度感知交替优化、SVD校正和增强K-V记忆三项创新。

Result: 在CSD-100数据集上，CSD-VAR优于现有方法，内容保留和风格化保真度更高。

Conclusion: CSD-VAR为内容-风格分解任务提供了高效解决方案，展示了VAR框架的潜力。

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [70] [DreamScene: 3D Gaussian-based End-to-end Text-to-3D Scene Generation](https://arxiv.org/abs/2507.13985)
*Haoran Li,Yuli Tian,Kun Lan,Yong Liao,Lin Wang,Pan Hui,Peng Yuan Zhou*

Main category: cs.CV

TL;DR: DreamScene是一个端到端框架，通过文本或对话生成高质量且可编辑的3D场景，解决了现有方法在自动化、3D一致性和细粒度控制方面的不足。


<details>
  <summary>Details</summary>
Motivation: 从自然语言生成3D场景在游戏、电影和设计等领域具有巨大潜力，但现有方法在自动化、3D一致性和细粒度控制方面表现不佳。

Method: DreamScene通过场景规划模块（GPT-4代理推断对象语义和空间约束）、基于图的布局算法、几何生成（FPS）和渐进相机采样策略，实现高质量场景生成。

Result: 实验表明，DreamScene在质量、一致性和灵活性上优于现有方法，支持细粒度编辑和动态运动。

Conclusion: DreamScene为开放域3D内容创作提供了实用解决方案，代码和演示已开源。

Abstract: Generating 3D scenes from natural language holds great promise for
applications in gaming, film, and design. However, existing methods struggle
with automation, 3D consistency, and fine-grained control. We present
DreamScene, an end-to-end framework for high-quality and editable 3D scene
generation from text or dialogue. DreamScene begins with a scene planning
module, where a GPT-4 agent infers object semantics and spatial constraints to
construct a hybrid graph. A graph-based placement algorithm then produces a
structured, collision-free layout. Based on this layout, Formation Pattern
Sampling (FPS) generates object geometry using multi-timestep sampling and
reconstructive optimization, enabling fast and realistic synthesis. To ensure
global consistent, DreamScene employs a progressive camera sampling strategy
tailored to both indoor and outdoor settings. Finally, the system supports
fine-grained scene editing, including object movement, appearance changes, and
4D dynamic motion. Experiments demonstrate that DreamScene surpasses prior
methods in quality, consistency, and flexibility, offering a practical solution
for open-domain 3D content creation. Code and demos are available at
https://dreamscene-project.github.io.

</details>


### [71] [Automatic Classification and Segmentation of Tunnel Cracks Based on Deep Learning and Visual Explanations](https://arxiv.org/abs/2507.14010)
*Yong Feng,Xiaolei Zhang,Shijin Feng,Yong Zhao,Yihan Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种基于深度学习的两步法，用于隧道衬砌裂缝的分类和分割，提高了检测的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 隧道衬砌裂缝是隧道安全状态的重要指标，需要一种更准确和高效的方法来分类和分割这些裂缝。

Method: 研究采用两步法：第一步使用DenseNet-169进行隧道图像分类；第二步基于DeepLabV3+进行裂缝分割，并通过评分加权视觉解释技术评估模型内部逻辑。

Result: 隧道裂缝分类模型的准确率为92.23%，FPS为39.80；分割模型的IoU为57.01%，F1得分为67.44%，均优于其他先进模型。

Conclusion: 结合视觉解释的两阶段深度学习方法为隧道健康状况的快速准确评估提供了基础。

Abstract: Tunnel lining crack is a crucial indicator of tunnels' safety status. Aiming
to classify and segment tunnel cracks with enhanced accuracy and efficiency,
this study proposes a two-step deep learning-based method. An automatic tunnel
image classification model is developed using the DenseNet-169 in the first
step. The proposed crack segmentation model in the second step is based on the
DeepLabV3+, whose internal logic is evaluated via a score-weighted visual
explanation technique. Proposed method combines tunnel image classification and
segmentation together, so that the selected images containing cracks from the
first step are segmented in the second step to improve the detection accuracy
and efficiency. The superior performances of the two-step method are validated
by experiments. The results show that the accuracy and frames per second (FPS)
of the tunnel crack classification model are 92.23% and 39.80, respectively,
which are higher than other convolutional neural networks (CNN) based and
Transformer based models. Also, the intersection over union (IoU) and F1 score
of the tunnel crack segmentation model are 57.01% and 67.44%, respectively,
outperforming other state-of-the-art models. Moreover, the provided visual
explanations in this study are conducive to understanding the "black box" of
deep learning-based models. The developed two-stage deep learning-based method
integrating visual explanations provides a basis for fast and accurate
quantitative assessment of tunnel health status.

</details>


### [72] [Analysis of Plant Nutrient Deficiencies Using Multi-Spectral Imaging and Optimized Segmentation Model](https://arxiv.org/abs/2507.14013)
*Ji-Yan Wu,Zheng Yong Poh,Anoop C. Patil,Bongsoo Park,Giovanni Volpe,Daisuke Urano*

Main category: cs.CV

TL;DR: 提出了一种基于多光谱成像和改进YOLOv5模型的深度学习框架，用于植物叶片异常分割，显著提升了检测效果。


<details>
  <summary>Details</summary>
Motivation: 精准农业需要准确检测植物叶片营养缺乏，以实现早期干预施肥、疾病和压力管理。

Method: 使用多光谱成像和增强的YOLOv5模型，结合基于Transformer的注意力头，处理九通道多光谱输入，利用自注意力机制捕捉细微症状。

Result: 模型在Dice分数和IoU上比基线YOLOv5提高了约12%，尤其在检测叶绿素缺失和色素积累等挑战性症状上表现优异。

Conclusion: 结合多光谱成像和光谱-空间特征学习，有望推动植物表型分析和精准农业的发展。

Abstract: Accurate detection of nutrient deficiency in plant leaves is essential for
precision agriculture, enabling early intervention in fertilization, disease,
and stress management. This study presents a deep learning framework for leaf
anomaly segmentation using multispectral imaging and an enhanced YOLOv5 model
with a transformer-based attention head. The model is tailored for processing
nine-channel multispectral input and uses self-attention mechanisms to better
capture subtle, spatially-distributed symptoms. The plants in the experiments
were grown under controlled nutrient stress conditions for evaluation. We carry
out extensive experiments to benchmark the proposed model against the baseline
YOLOv5. Extensive experiments show that the proposed model significantly
outperforms the baseline YOLOv5, with an average Dice score and IoU
(Intersection over Union) improvement of about 12%. In particular, this model
is effective in detecting challenging symptoms like chlorosis and pigment
accumulation. These results highlight the promise of combining multi-spectral
imaging with spectral-spatial feature learning for advancing plant phenotyping
and precision agriculture.

</details>


### [73] [Moodifier: MLLM-Enhanced Emotion-Driven Image Editing](https://arxiv.org/abs/2507.14024)
*Jiarong Ye,Sharon X. Huang*

Main category: cs.CV

TL;DR: 提出了一种结合情感与视觉内容的方法，用于情感驱动的图像编辑，包括数据集、模型和编辑工具。


<details>
  <summary>Details</summary>
Motivation: 情感驱动的图像编辑在创意产业中潜力巨大，但由于情感的抽象性和多样性，精确操作仍具挑战性。

Method: 1. 构建MoodArchive数据集（8M+图像，带情感标注）；2. 开发MoodifyCLIP模型，将情感映射为视觉属性；3. 提出Moodifier编辑模型，结合MLLMs实现精确情感转换。

Result: Moodifier在情感准确性和内容保留上优于现有方法，适用于多种领域（如时尚、家居）。

Conclusion: 通过将抽象情感与具体视觉变化关联，为实际应用中的情感内容创作提供了新可能。

Abstract: Bridging emotions and visual content for emotion-driven image editing holds
great potential in creative industries, yet precise manipulation remains
challenging due to the abstract nature of emotions and their varied
manifestations across different contexts. We tackle this challenge with an
integrated approach consisting of three complementary components. First, we
introduce MoodArchive, an 8M+ image dataset with detailed hierarchical
emotional annotations generated by LLaVA and partially validated by human
evaluators. Second, we develop MoodifyCLIP, a vision-language model fine-tuned
on MoodArchive to translate abstract emotions into specific visual attributes.
Third, we propose Moodifier, a training-free editing model leveraging
MoodifyCLIP and multimodal large language models (MLLMs) to enable precise
emotional transformations while preserving content integrity. Our system works
across diverse domains such as character expressions, fashion design, jewelry,
and home d\'ecor, enabling creators to quickly visualize emotional variations
while preserving identity and structure. Extensive experimental evaluations
show that Moodifier outperforms existing methods in both emotional accuracy and
content preservation, providing contextually appropriate edits. By linking
abstract emotions to concrete visual changes, our solution unlocks new
possibilities for emotional content creation in real-world applications. We
will release the MoodArchive dataset, MoodifyCLIP model, and make the Moodifier
code and demo publicly available upon acceptance.

</details>


### [74] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: 提出了一种超轻量级量子辅助推理框架（QuantEIT），用于EIT图像重建，显著降低了模型复杂度，并在无监督、无训练数据的情况下实现了高精度重建。


<details>
  <summary>Details</summary>
Motivation: EIT作为一种低成本、非侵入性的床边成像技术，其逆问题具有固有的不适定性，传统深度学习方法依赖复杂网络架构，效率低且难以扩展。

Method: QuantEIT利用量子辅助网络（QA-Net），结合并行2量子比特电路生成潜在表示，并通过单层线性层重建电导率，无需训练数据。

Result: 在模拟和真实2D/3D EIT肺部成像数据上，QuantEIT优于传统方法，仅使用0.2%的参数即可实现相当或更高的重建精度，且对噪声具有更强鲁棒性。

Conclusion: QuantEIT首次将量子电路集成到EIT图像重建中，为高效、可扩展的EIT成像提供了新思路。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [75] [Training-free Token Reduction for Vision Mamba](https://arxiv.org/abs/2507.14042)
*Qiankun Ma,Ziyao Zhang,Chi Su,Jie Chen,Zhen Song,Hairong Zheng,Wen Gao*

Main category: cs.CV

TL;DR: Vision Mamba的MTR框架通过无训练的方式减少计算量，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 探索Vision Mamba的效率，发现直接应用ViTs的token reduction技术会导致性能下降。

Method: 提出MTR框架，基于Mamba结构感知的重要性评分，无需训练即可减少token。

Result: 在Vim-B骨干上减少约40% FLOPs，ImageNet性能仅下降1.6%。

Conclusion: MTR是一种高效且无需训练的token reduction方法，适用于多种Mamba模型。

Abstract: Vision Mamba has emerged as a strong competitor to Vision Transformers (ViTs)
due to its ability to efficiently capture long-range dependencies with linear
computational complexity. While token reduction, an effective compression
technique in ViTs, has rarely been explored in Vision Mamba. Exploring Vision
Mamba's efficiency is essential for enabling broader applications. However, we
find that directly applying existing token reduction techniques for ViTs to
Vision Mamba leads to significant performance degradation. This is primarily
because Mamba is a sequence model without attention mechanisms, whereas most
token reduction techniques for ViTs rely on attention mechanisms for importance
measurement and overlook the order of compressed tokens. In this paper, we
investigate a Mamba structure-aware importance score to evaluate token
importance in a simple and effective manner. Building on this score, we further
propose MTR, a training-free \textbf{M}amba \textbf{T}oken \textbf{R}eduction
framework. Without the need for training or additional tuning parameters, our
method can be seamlessly integrated as a plug-and-play component across various
Mamba models. Extensive experiments demonstrate that our approach significantly
reduces computational workload while minimizing performance impact across
various tasks and multiple backbones. Notably, MTR reduces FLOPs by
approximately 40\% on the Vim-B backbone, with only a 1.6\% drop in ImageNet
performance without retraining.

</details>


### [76] [Foundation Models as Class-Incremental Learners for Dermatological Image Classification](https://arxiv.org/abs/2507.14050)
*Mohamed Elkhayat,Mohamed Mahmoud,Jamil Fayyad,Nourhan Bayasi*

Main category: cs.CV

TL;DR: 该论文提出了一种基于冻结基础模型的简单有效方法，用于皮肤病分类的类增量学习，无需遗忘且性能优越。


<details>
  <summary>Details</summary>
Motivation: 探索基础模型在皮肤病分类中的增量学习潜力，填补该领域的研究空白。

Method: 冻结预训练基础模型的主干，仅增量训练轻量级MLP；同时探索零训练场景下的原型分类方法。

Result: 该方法在类增量学习中表现优异，超越现有方法，且原型分类也能取得竞争性结果。

Conclusion: 冻结基础模型在皮肤病持续学习中具有强大潜力，值得在医疗应用中推广。

Abstract: Class-Incremental Learning (CIL) aims to learn new classes over time without
forgetting previously acquired knowledge. The emergence of foundation models
(FM) pretrained on large datasets presents new opportunities for CIL by
offering rich, transferable representations. However, their potential for
enabling incremental learning in dermatology remains largely unexplored. In
this paper, we systematically evaluate frozen FMs pretrained on large-scale
skin lesion datasets for CIL in dermatological disease classification. We
propose a simple yet effective approach where the backbone remains frozen, and
a lightweight MLP is trained incrementally for each task. This setup achieves
state-of-the-art performance without forgetting, outperforming regularization,
replay, and architecture based methods. To further explore the capabilities of
frozen FMs, we examine zero training scenarios using nearest mean classifiers
with prototypes derived from their embeddings. Through extensive ablation
studies, we demonstrate that this prototype based variant can also achieve
competitive results. Our findings highlight the strength of frozen FMs for
continual learning in dermatology and support their broader adoption in real
world medical applications. Our code and datasets are available here.

</details>


### [77] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: VLA-Mark是一种视觉对齐的水印框架，通过跨模态协调嵌入可检测水印，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法破坏了视觉-文本对齐，导致语义关键概念易受攻击。

Method: 整合多尺度视觉-文本对齐指标，通过局部补丁亲和性、全局语义一致性和上下文注意力模式指导水印注入。

Result: 实验显示，PPL降低7.4%，BLEU提高26.6%，检测AUC接近完美（98.8%），攻击恢复力达96.1%。

Conclusion: VLA-Mark为质量保持的多模态水印设定了新标准。

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


### [78] [Unmasking Performance Gaps: A Comparative Study of Human Anonymization and Its Effects on Video Anomaly Detection](https://arxiv.org/abs/2507.14083)
*Sara Abdulaziz,Egor Bondarev*

Main category: cs.CV

TL;DR: 论文分析了四种人体匿名化技术对异常检测性能的影响，发现某些匿名化技术下模型性能反而提升，强调了隐私保护与检测效用之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 深度学习在监控视频异常检测中取得进展，但收集敏感数据引发隐私问题，研究旨在评估匿名化技术对检测性能的影响。

Method: 在UCF-Crime数据集上应用四种匿名化技术（模糊、掩码、加密、虚拟人替换），评估四种异常检测方法（MGFN、UR-DMU、BN-WVAD、PEL4VAD）的性能。

Result: 实验表明，匿名化数据下异常检测仍可行，且性能与算法设计相关；某些匿名化模式（如加密和掩码）下模型AUC性能甚至优于原始数据。

Conclusion: 研究揭示了算法对匿名化的敏感性，并强调了隐私保护与检测效用的权衡，为平衡隐私与异常检测需求提供了基准和见解。

Abstract: Advancements in deep learning have improved anomaly detection in surveillance
videos, yet they raise urgent privacy concerns due to the collection of
sensitive human data. In this paper, we present a comprehensive analysis of
anomaly detection performance under four human anonymization techniques,
including blurring, masking, encryption, and avatar replacement, applied to the
UCF-Crime dataset. We evaluate four anomaly detection methods, MGFN, UR-DMU,
BN-WVAD, and PEL4VAD, on the anonymized UCF-Crime to reveal how each method
responds to different obfuscation techniques. Experimental results demonstrate
that anomaly detection remains viable under anonymized data and is dependent on
the algorithmic design and the learning strategy. For instance, under certain
anonymization patterns, such as encryption and masking, some models
inadvertently achieve higher AUC performance compared to raw data, due to the
strong responsiveness of their algorithmic components to these noise patterns.
These results highlight the algorithm-specific sensitivities to anonymization
and emphasize the trade-off between preserving privacy and maintaining
detection utility. Furthermore, we compare these conventional anonymization
techniques with the emerging privacy-by-design solutions, highlighting an often
overlooked trade-off between robust privacy protection and utility flexibility.
Through comprehensive experiments and analyses, this study provides a
compelling benchmark and insights into balancing human privacy with the demands
of anomaly detection.

</details>


### [79] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: 该论文评估了一种基于深度学习的自动化软件（Carebot AI Bones）在测量脊柱侧弯Cobb角方面的性能，结果显示其与放射科医生的测量结果具有高度一致性。


<details>
  <summary>Details</summary>
Motivation: 脊柱侧弯影响2-4%的青少年，传统手动测量Cobb角耗时且存在观察者间差异，因此需要一种自动化解决方案以提高效率和准确性。

Method: 研究采用回顾性多中心评估，使用103张站立位全脊柱X光片，由两位放射科医生独立测量作为参考，通过Bland-Altman分析、MAE、RMSE、Pearson相关系数和Cohen kappa评估AI与医生的一致性。

Result: AI与两位放射科医生的MAE分别为3.89度和3.90度，Pearson相关系数分别为0.906和0.880，Cohen kappa分别为0.51和0.64，表明AI能够复现专家水平的测量结果。

Conclusion: 该软件在多中心环境中表现出与专家一致的Cobb角测量能力，可用于优化脊柱侧弯的临床报告和分诊流程。

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [80] [C-DOG: Training-Free Multi-View Multi-Object Association in Dense Scenes Without Visual Feature via Connected δ-Overlap Graphs](https://arxiv.org/abs/2507.14095)
*Yung-Hong Sun,Ting-Hung Lin,Jiangang Chen,Hongrui Jiang,Yu Hen Hu*

Main category: cs.CV

TL;DR: C-DOG是一个无需训练的多视角多目标关联框架，结合图建模和极线几何，在无视觉特征的情况下实现鲁棒关联。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖外观特征或几何约束，在目标视觉相似或观测噪声时效果不佳。

Method: 通过连接delta-overlap图建模和极线几何，结合IQR过滤和3D反投影误差，实现鲁棒关联。

Result: 在合成基准测试中优于几何基线，在高目标密度、无视觉特征和有限相机重叠下仍稳健。

Conclusion: C-DOG适用于现实场景中可扩展的3D重建。

Abstract: Multi-view multi-object association is a fundamental step in 3D
reconstruction pipelines, enabling consistent grouping of object instances
across multiple camera views. Existing methods often rely on appearance
features or geometric constraints such as epipolar consistency. However, these
approaches can fail when objects are visually indistinguishable or observations
are corrupted by noise. We propose C-DOG, a training-free framework that serves
as an intermediate module bridging object detection (or pose estimation) and 3D
reconstruction, without relying on visual features. It combines connected
delta-overlap graph modeling with epipolar geometry to robustly associate
detections across views. Each 2D observation is represented as a graph node,
with edges weighted by epipolar consistency. A delta-neighbor-overlap
clustering step identifies strongly consistent groups while tolerating noise
and partial connectivity. To further improve robustness, we incorporate
Interquartile Range (IQR)-based filtering and a 3D back-projection error
criterion to eliminate inconsistent observations. Extensive experiments on
synthetic benchmarks demonstrate that C-DOG outperforms geometry-based
baselines and remains robust under challenging conditions, including high
object density, without visual features, and limited camera overlap, making it
well-suited for scalable 3D reconstruction in real-world scenarios.

</details>


### [81] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: 论文提出了一种自动化、模块化的流程，用于生成高质量的训练数据三元组（原始图像、指令、编辑后图像），以支持基于自然语言指令的图像编辑助手。


<details>
  <summary>Details</summary>
Motivation: 现有的监督训练需要大量高质量的三元组数据，但手动生成这些数据成本高昂且难以保证质量。缺乏可靠的自动化编辑质量评估方法也限制了数据规模。

Method: 利用公开生成模型构建自动化流程，通过任务调优的Gemini验证器直接评估指令遵循和美学质量，无需分割或基础模型。通过反转和组合自举技术扩展数据集。

Result: 发布了NHR-Edit数据集（358k高质量三元组）和Bagel-NHR-Edit模型，在跨数据集评估中表现优于所有公开替代方案。

Conclusion: 该方法实现了大规模高质量训练数据的自动化生成，推动了资源密集型领域的研究民主化。

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [82] [Franca: Nested Matryoshka Clustering for Scalable Visual Representation Learning](https://arxiv.org/abs/2507.14137)
*Shashanka Venkataramanan,Valentinos Pariza,Mohammadreza Salehi,Lukas Knobel,Spyros Gidaris,Elias Ramzi,Andrei Bursuc,Yuki M. Asano*

Main category: cs.CV

TL;DR: Franca是一个完全开源的视觉基础模型，性能优于或匹配现有专有模型，通过透明训练流程和多头聚类投影器提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有专有模型不透明和聚类方法局限性问题，推动开源高性能视觉模型发展。

Method: 采用透明训练流程（基于Web-SSL和公开数据），引入多头聚类投影器和位置解耦策略。

Result: 在多个下游任务中表现优异，证明了特征空间优化的有效性。

Conclusion: Franca为透明、高性能视觉模型设定了新标准，推动了可复现基础模型的发展。

Abstract: We present Franca (pronounced Fran-ka): free one; the first fully open-source
(data, code, weights) vision foundation model that matches and in many cases
surpasses the performance of state-of-the-art proprietary models, e.g., DINOv2,
CLIP, SigLIPv2, etc. Our approach is grounded in a transparent training
pipeline inspired by Web-SSL and uses publicly available data: ImageNet-21K and
a subset of ReLAION-2B. Beyond model release, we tackle critical limitations in
SSL clustering methods. While modern models rely on assigning image features to
large codebooks via clustering algorithms like Sinkhorn-Knopp, they fail to
account for the inherent ambiguity in clustering semantics. To address this, we
introduce a parameter-efficient, multi-head clustering projector based on
nested Matryoshka representations. This design progressively refines features
into increasingly fine-grained clusters without increasing the model size,
enabling both performance and memory efficiency. Additionally, we propose a
novel positional disentanglement strategy that explicitly removes positional
biases from dense representations, thereby improving the encoding of semantic
content. This leads to consistent gains on several downstream benchmarks,
demonstrating the utility of cleaner feature spaces. Our contributions
establish a new standard for transparent, high-performance vision models and
open a path toward more reproducible and generalizable foundation models for
the broader AI community. The code and model checkpoints are available at
https://github.com/valeoai/Franca.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [83] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: 本文研究了Vision Mamba模型中图像扫描顺序对MRI分割的影响，提出了Multi-Scan 2D模块，并通过大规模实验验证了扫描顺序的显著性影响。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba模型在医学图像处理中依赖1D序列化2D图像，但扫描顺序的选择未被充分研究，尤其是在具有强解剖先验的MRI中。

Method: 提出Multi-Scan 2D模块，支持多种扫描路径的探索，并在三个公共数据集上对21种扫描策略进行了大规模基准测试。

Result: 扫描顺序对分割性能有显著影响（Friedman检验：χ²=43.9, p=0.0016），性能差异可达27 Dice点，连续扫描路径优于非连续路径。

Conclusion: 扫描顺序是Vision Mamba模型中的重要超参数，研究提供了优化路径的实证建议。

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [84] [Enhanced DeepLab Based Nerve Segmentation with Optimized Tuning](https://arxiv.org/abs/2507.13394)
*Akhil John Thomas,Christiaan Boerkamp*

Main category: eess.IV

TL;DR: 本文提出了一种基于DeepLabV3的优化神经分割方法，通过自动阈值微调和参数优化，显著提升了超声神经图像的分割精度。


<details>
  <summary>Details</summary>
Motivation: 神经分割在医学影像中对精确识别神经结构至关重要，但现有方法在分割精度上仍有提升空间。

Method: 采用优化的DeepLabV3分割流程，结合自动阈值微调和参数优化，改进预处理步骤。

Result: 在超声神经图像上取得了Dice Score 0.78、IoU 0.70和Pixel Accuracy 0.95的优异结果。

Conclusion: 该方法显著优于基线模型，强调了定制化参数选择在自动化神经检测中的重要性。

Abstract: Nerve segmentation is crucial in medical imaging for precise identification
of nerve structures. This study presents an optimized DeepLabV3-based
segmentation pipeline that incorporates automated threshold fine-tuning to
improve segmentation accuracy. By refining preprocessing steps and implementing
parameter optimization, we achieved a Dice Score of 0.78, an IoU of 0.70, and a
Pixel Accuracy of 0.95 on ultrasound nerve imaging. The results demonstrate
significant improvements over baseline models and highlight the importance of
tailored parameter selection in automated nerve detection.

</details>


### [85] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: 论文提出了一种通过合成多样化数据训练深度学习模型的方法，以解决神经影像分析中数据多样性不足导致的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 解决MRI等神经影像分析中因训练数据范围狭窄导致的模型鲁棒性和泛化性不足的问题。

Method: 采用域随机化策略，通过生成具有随机强度和解剖内容的合成图像来训练深度神经网络。

Result: 该方法在多种影像模态中表现出色，无需重新训练或微调即可处理未见过的图像类型。

Conclusion: 合成驱动训练范式能显著提升模型泛化能力，但需权衡计算资源需求，有望推动深度学习在神经影像分析中的广泛应用。

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [86] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: BreastSegNet是一种多标签分割算法，用于乳腺MRI，覆盖九种解剖结构，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺MRI分割方法范围有限，仅关注少数结构，限制了定量分析的实用性。

Method: 手动标注1123张MRI切片，并比较了九种分割模型，包括U-Net、SwinUNet等。

Result: nnU-Net ResEncM表现最佳，平均Dice分数为0.694，心脏和肝脏分割接近0.90。

Conclusion: BreastSegNet扩展了乳腺MRI分割的范围，为定量分析提供了更全面的工具。

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [87] [Converting T1-weighted MRI from 3T to 7T quality using deep learning](https://arxiv.org/abs/2507.13782)
*Malo Gicquel,Ruoyi Zhao,Anika Wuestefeld,Nicola Spotorno,Olof Strandberg,Kalle Åström,Yu Xiao,Laura EM Wisse,Danielle van Westen,Rik Ossenkoppele,Niklas Mattsson-Carlgren,David Berron,Oskar Hansson,Gabrielle Flood,Jacob Vogel*

Main category: eess.IV

TL;DR: 利用深度学习模型从3T MRI合成7T MRI，提升图像质量和分割效果，同时不影响下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 7T MRI提供更高分辨率和组织对比度，但可及性较低，因此需要从更普及的3T MRI合成7T MRI。

Method: 训练了两种模型：专用U-Net和结合GAN的U-Net，用于从3T MRI合成7T MRI。

Result: 合成图像在细节上与真实7T MRI相当，主观视觉质量更优，且分割效果更好。

Conclusion: 合成7T MRI图像质量接近真实7T，可能改善临床应用，但需进一步探讨其局限性和未来方向。

Abstract: Ultra-high resolution 7 tesla (7T) magnetic resonance imaging (MRI) provides
detailed anatomical views, offering better signal-to-noise ratio, resolution
and tissue contrast than 3T MRI, though at the cost of accessibility. We
present an advanced deep learning model for synthesizing 7T brain MRI from 3T
brain MRI. Paired 7T and 3T T1-weighted images were acquired from 172
participants (124 cognitively unimpaired, 48 impaired) from the Swedish
BioFINDER-2 study. To synthesize 7T MRI from 3T images, we trained two models:
a specialized U-Net, and a U-Net integrated with a generative adversarial
network (GAN U-Net). Our models outperformed two additional state-of-the-art
3T-to-7T models in image-based evaluation metrics. Four blinded MRI
professionals judged our synthetic 7T images as comparable in detail to real 7T
images, and superior in subjective visual quality to 7T images, apparently due
to the reduction of artifacts. Importantly, automated segmentations of the
amygdalae of synthetic GAN U-Net 7T images were more similar to manually
segmented amygdalae (n=20), than automated segmentations from the 3T images
that were used to synthesize the 7T images. Finally, synthetic 7T images showed
similar performance to real 3T images in downstream prediction of cognitive
status using MRI derivatives (n=3,168). In all, we show that synthetic
T1-weighted brain images approaching 7T quality can be generated from 3T
images, which may improve image quality and segmentation, without compromising
performance in downstream tasks. Future directions, possible clinical use
cases, and limitations are discussed.

</details>


### [88] [Divide and Conquer: A Large-Scale Dataset and Model for Left-Right Breast MRI Segmentation](https://arxiv.org/abs/2507.13830)
*Maximilian Rokuss,Benjamin Hamm,Yannick Kirchhoff,Klaus Maier-Hein*

Main category: eess.IV

TL;DR: 首个公开的乳腺MRI数据集，包含超过13,000例标注的左、右乳腺分割标签，并提供了深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 解决乳腺MRI分析中的关键空白，为女性健康领域提供资源。

Method: 提供标注数据集和训练好的深度学习模型。

Result: 数据集和模型已公开，可用于开发高级工具。

Conclusion: 填补了乳腺MRI分析的空白，为研究和应用提供了重要资源。

Abstract: We introduce the first publicly available breast MRI dataset with explicit
left and right breast segmentation labels, encompassing more than 13,000
annotated cases. Alongside this dataset, we provide a robust deep-learning
model trained for left-right breast segmentation. This work addresses a
critical gap in breast MRI analysis and offers a valuable resource for the
development of advanced tools in women's health. The dataset and trained model
are publicly available at: www.github.com/MIC-DKFZ/BreastDivider

</details>


### [89] [Software architecture and manual for novel versatile CT image analysis toolbox -- AnatomyArchive](https://arxiv.org/abs/2507.13901)
*Lei Xu,Torkel B Brismar*

Main category: eess.IV

TL;DR: AnatomyArchive是一个基于TotalSegmentator的CT图像分析工具，提供自动目标体积选择、解剖结构管理、体素特征提取等功能，支持2D和3D分析，并包含GPU加速的渲染工具。


<details>
  <summary>Details</summary>
Motivation: 开发一个高效、自动化的CT图像分析工具，以支持精确的身体成分分析和医学图像数据库管理。

Method: 基于知识图谱和TotalSegmentator模型，实现自动体积选择、解剖结构管理、特征提取和渲染功能。

Result: 成功开发了AnatomyArchive工具，支持多种自动化功能，并提供了开源代码供研究和教育使用。

Conclusion: AnatomyArchive是一个功能强大的CT图像分析工具，适用于医学研究和机器学习模型开发。

Abstract: We have developed a novel CT image analysis package named AnatomyArchive,
built on top of the recent full body segmentation model TotalSegmentator. It
provides automatic target volume selection and deselection capabilities
according to user-configured anatomies for volumetric upper- and lower-bounds.
It has a knowledge graph-based and time efficient tool for anatomy segmentation
mask management and medical image database maintenance. AnatomyArchive enables
automatic body volume cropping, as well as automatic arm-detection and
exclusion, for more precise body composition analysis in both 2D and 3D
formats. It provides robust voxel-based radiomic feature extraction, feature
visualization, and an integrated toolchain for statistical tests and analysis.
A python-based GPU-accelerated nearly photo-realistic segmentation-integrated
composite cinematic rendering is also included. We present here its software
architecture design, illustrate its workflow and working principle of
algorithms as well provide a few examples on how the software can be used to
assist development of modern machine learning models. Open-source codes will be
released at https://github.com/lxu-medai/AnatomyArchive for only research and
educational purposes.

</details>


### [90] [Blind Super Resolution with Reference Images and Implicit Degradation Representation](https://arxiv.org/abs/2507.13915)
*Huu-Phu Do,Po-Chih Hu,Hao-Chien Hsueh,Che-Kai Liu,Vu-Hoang Tran,Ching-Chun Huang*

Main category: eess.IV

TL;DR: 提出了一种利用高分辨率参考图像生成尺度感知退化核的新策略，用于盲超分辨率任务。


<details>
  <summary>Details</summary>
Motivation: 现有盲超分辨率方法通常直接估计退化核，但忽略了不同超分辨率尺度下退化核的差异，导致性能受限。

Method: 通过引入内容无关的高分辨率参考图像，自适应地学习退化过程，并生成额外的LR-HR对以提升超分辨率性能。

Result: 该方法在训练好的盲超分辨率模型和零样本盲超分辨率方法中均优于现有方法。

Conclusion: 结合模糊核和尺度因子，并利用参考图像，显著提升了盲超分辨率任务的效果。

Abstract: Previous studies in blind super-resolution (BSR) have primarily concentrated
on estimating degradation kernels directly from low-resolution (LR) inputs to
enhance super-resolution. However, these degradation kernels, which model the
transition from a high-resolution (HR) image to its LR version, should account
for not only the degradation process but also the downscaling factor. Applying
the same degradation kernel across varying super-resolution scales may be
impractical. Our research acknowledges degradation kernels and scaling factors
as pivotal elements for the BSR task and introduces a novel strategy that
utilizes HR images as references to establish scale-aware degradation kernels.
By employing content-irrelevant HR reference images alongside the target LR
image, our model adaptively discerns the degradation process. It is then
applied to generate additional LR-HR pairs through down-sampling the HR
reference images, which are keys to improving the SR performance. Our
reference-based training procedure is applicable to proficiently trained blind
SR models and zero-shot blind SR methods, consistently outperforming previous
methods in both scenarios. This dual consideration of blur kernels and scaling
factors, coupled with the use of a reference image, contributes to the
effectiveness of our approach in blind super-resolution tasks.

</details>


### [91] [Leveraging Pathology Foundation Models for Panoptic Segmentation of Melanoma in H&E Images](https://arxiv.org/abs/2507.13974)
*Jiaqi Lv,Yijie Zhu,Carmen Guadalupe Colin Tenorio,Brinder Singh Chohan,Mark Eastwood,Shan E Ahmed Raza*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的新型网络，用于黑色素瘤H&E图像中五种组织类别的分割，结合病理基础模型Virchow2和Efficient-UNet，取得了PUMA Grand Challenge的第一名。


<details>
  <summary>Details</summary>
Motivation: 手动分割黑色素瘤H&E图像中的组织区域耗时且易受观察者差异影响，因此需要可靠的自动化分割方法。

Method: 利用病理基础模型Virchow2作为特征提取器，将特征与原始RGB图像融合，再通过Efficient-UNet编码器-解码器网络生成分割图。

Result: 模型在PUMA Grand Challenge的组织分割任务中表现优异，展示了鲁棒性和泛化能力。

Conclusion: 结合病理基础模型的分割网络可加速计算病理学工作流程，具有潜在的高效性。

Abstract: Melanoma is an aggressive form of skin cancer with rapid progression and high
metastatic potential. Accurate characterisation of tissue morphology in
melanoma is crucial for prognosis and treatment planning. However, manual
segmentation of tissue regions from haematoxylin and eosin (H&E) stained
whole-slide images (WSIs) is labour-intensive and prone to inter-observer
variability, this motivates the need for reliable automated tissue segmentation
methods. In this study, we propose a novel deep learning network for the
segmentation of five tissue classes in melanoma H&E images. Our approach
leverages Virchow2, a pathology foundation model trained on 3.1 million
histopathology images as a feature extractor. These features are fused with the
original RGB images and subsequently processed by an encoder-decoder
segmentation network (Efficient-UNet) to produce accurate segmentation maps.
The proposed model achieved first place in the tissue segmentation task of the
PUMA Grand Challenge, demonstrating robust performance and generalizability.
Our results show the potential and efficacy of incorporating pathology
foundation models into segmentation networks to accelerate computational
pathology workflows.

</details>


### [92] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: OrthoInsight是一个多模态深度学习框架，用于肋骨骨折诊断和报告生成，结合了YOLOv9、医学知识图谱和LLaVA语言模型，性能优于GPT-4和Claude-3。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据增长需要自动化诊断工具，手动解读耗时且易出错。

Method: 结合YOLOv9检测骨折、医学知识图谱获取临床背景、LLaVA生成诊断报告。

Result: 在28,675张CT图像上表现优异，平均得分4.28，优于GPT-4和Claude-3。

Conclusion: 多模态学习在医学影像分析中潜力巨大，可为放射科医生提供有效支持。

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [93] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: D2IP是一种用于3D时间序列成像的无监督学习框架，通过UPWS、TPP和3D-FastResUNet加速收敛并提高计算效率，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决Deep Image Prior在3D或时间序列成像中计算成本高的问题。

Method: 提出D2IP框架，包含UPWS、TPP和3D-FastResUNet三种策略。

Result: 在模拟和临床数据上，D2IP图像质量提升（MSSIM增加24.8%，ERR降低8.1%），计算时间减少7.1倍。

Conclusion: D2IP在动态肺部成像中具有临床应用潜力。

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [94] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: UGPL是一种不确定性引导的渐进学习框架，通过全局到局部分析提升CT图像分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理病理特征的细微和空间多样性，UGPL旨在解决这一问题。

Method: UGPL利用证据深度学习量化预测不确定性，通过非极大值抑制机制提取信息丰富的局部区域，并结合自适应融合机制。

Result: 在三个CT数据集上，UGPL在肾脏异常、肺癌和COVID-19检测中的准确率分别提高了3.29%、2.46%和8.08%。

Conclusion: UGPL通过不确定性引导和渐进学习显著提升了CT图像分类性能。

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [95] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: ADPC框架通过视觉-语言因果干预，结合MRI/fMRI和LLM生成的文本数据，有效区分CN/MCI/AD，性能达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 早期诊断阿尔茨海默病（AD）因多模态数据的选择偏差和变量间复杂关系而困难，需解决混杂因素问题。

Method: 提出ADPC框架，利用LLM总结临床数据，结合MRI/fMRI图像进行因果干预，消除混杂因素。

Result: 实验显示ADPC在区分CN/MCI/AD上表现优异，多数评估指标达到SOTA。

Conclusion: 结合因果推理与多模态学习在神经疾病诊断中具有潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [96] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS利用生成式AI提升工业任务中的人体运动模拟质量，通过文本到运动和运动验证技术，显著减少了运动误差。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法运动保真度低，G-AI-HMS旨在通过生成式AI技术提升模拟质量。

Method: 结合文本到文本和文本到运动模型，利用大语言模型和MotionGPT词汇对齐生成运动感知语言，并通过计算机视觉验证运动。

Result: 在八项任务中，AI增强的运动在多数场景下误差更低，显著减少了关节误差和时间错位。

Conclusion: G-AI-HMS通过生成式AI技术显著提升了运动模拟的保真度和准确性。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [97] [StructInbet: Integrating Explicit Structural Guidance into Inbetween Frame Generation](https://arxiv.org/abs/2507.13377)
*Zhenglin Pan,Haoran Xie*

Main category: cs.GR

TL;DR: StructInbet提出了一种基于显式结构引导的中间帧生成系统，通过结构指导和时序注意力机制减少模糊性并保持一致性。


<details>
  <summary>Details</summary>
Motivation: 解决中间帧生成中像素轨迹的模糊性问题，并确保角色外观的一致性。

Method: 引入显式结构指导和时序注意力机制，结合前后关键帧的视觉信息。

Result: 减少了模糊性，提高了角色外观的一致性。

Conclusion: StructInbet通过结构指导和时序注意力机制有效改善了中间帧生成的质量。

Abstract: In this paper, we propose StructInbet, an inbetweening system designed to
generate controllable transitions over explicit structural guidance.
StructInbet introduces two key contributions. First, we propose explicit
structural guidance to the inbetweening problem to reduce the ambiguity
inherent in pixel trajectories. Second, we adopt a temporal attention mechanism
that incorporates visual identity from both the preceding and succeeding
keyframes, ensuring consistency in character appearance.

</details>


### [98] [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
*Kaiyuan Tang,Kuangshi Ai,Jun Han,Chaoli Wang*

Main category: cs.GR

TL;DR: TexGS-VolVis是一种基于纹理高斯泼溅的体可视化框架，通过结合预训练大模型实现任意风格迁移和实时渲染，提升了体数据的可视化质量和编辑灵活性。


<details>
  <summary>Details</summary>
Motivation: 现有体可视化方法依赖复杂预定义规则且风格迁移受限，TexGS-VolVis旨在解决这些问题。

Method: 采用2D高斯基元扩展纹理和光照属性，结合图像/文本驱动的非真实感编辑和2D-lift-3D分割技术。

Result: 在效率和视觉质量上优于现有方法，支持细粒度编辑。

Conclusion: TexGS-VolVis为体可视化提供了高质量、灵活的风格化与编辑能力。

Abstract: Advancements in volume visualization (VolVis) focus on extracting insights
from 3D volumetric data by generating visually compelling renderings that
reveal complex internal structures. Existing VolVis approaches have explored
non-photorealistic rendering techniques to enhance the clarity, expressiveness,
and informativeness of visual communication. While effective, these methods
often rely on complex predefined rules and are limited to transferring a single
style, restricting their flexibility. To overcome these limitations, we
advocate the representation of VolVis scenes using differentiable Gaussian
primitives combined with pretrained large models to enable arbitrary style
transfer and real-time rendering. However, conventional 3D Gaussian primitives
tightly couple geometry and appearance, leading to suboptimal stylization
results. To address this, we introduce TexGS-VolVis, a textured Gaussian
splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,
extending each Gaussian with additional texture and shading attributes,
resulting in higher-quality, geometry-consistent stylization and enhanced
lighting control during inference. Despite these improvements, achieving
flexible and controllable scene editing remains challenging. To further enhance
stylization, we develop image- and text-driven non-photorealistic scene editing
tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing
with fine-grained control. We evaluate TexGS-VolVis both qualitatively and
quantitatively across various volume rendering scenes, demonstrating its
superiority over existing methods in terms of efficiency, visual quality, and
editing flexibility.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [99] [Convergent transformations of visual representation in brains and models](https://arxiv.org/abs/2507.13941)
*Pablo Marcos-Manchón,Lluís Fuentemilla*

Main category: q-bio.NC

TL;DR: 论文探讨了视觉感知是由外部世界结构还是大脑内部架构塑造的，发现人类和深度神经网络在视觉编码上存在共同的解决方案。


<details>
  <summary>Details</summary>
Motivation: 研究视觉感知的驱动因素，验证人类和人工视觉系统在视觉编码上的相似性。

Method: 结合被试间相似性和模型层次对齐的统一框架，分析三个独立的fMRI数据集。

Result: 发现了一个跨个体保守的皮层网络，分为两个通路，且视觉DNNs能捕捉这种功能组织。

Conclusion: 人类和人工视觉系统在视觉编码上存在共同的解决方案，由外部世界结构驱动。

Abstract: A fundamental question in cognitive neuroscience is what shapes visual
perception: the external world's structure or the brain's internal
architecture. Although some perceptual variability can be traced to individual
differences, brain responses to naturalistic stimuli evoke similar activity
patterns across individuals, suggesting a convergent representational
principle. Here, we test if this stimulus-driven convergence follows a common
trajectory across people and deep neural networks (DNNs) during its
transformation from sensory to high-level internal representations. We
introduce a unified framework that traces representational flow by combining
inter-subject similarity with alignment to model hierarchies. Applying this
framework to three independent fMRI datasets of visual scene perception, we
reveal a cortex-wide network, conserved across individuals, organized into two
pathways: a medial-ventral stream for scene structure and a lateral-dorsal
stream tuned for social and biological content. This functional organization is
captured by the hierarchies of vision DNNs but not language models, reinforcing
the specificity of the visual-to-semantic transformation. These findings show a
convergent computational solution for visual encoding in both human and
artificial vision, driven by the structure of the external world.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [100] [A Novel APVD Steganography Technique Incorporating Pseudorandom Pixel Selection for Robust Image Security](https://arxiv.org/abs/2507.13367)
*Mehrab Hosain,Rajiv Kapoor*

Main category: cs.CR

TL;DR: 提出了一种结合APVD和伪随机像素选择的新型隐写方法，解决了未使用块问题，提升了安全性、嵌入容量和图像质量。


<details>
  <summary>Details</summary>
Motivation: APVD隐写方法存在未使用块问题，导致安全性降低、嵌入容量减少和视觉质量下降。

Method: 将APVD与伪随机像素选择结合，优化嵌入过程。

Result: 新方法在安全性、数据隐藏容量和图像质量方面优于现有技术，显著提升了PSNR、UIQ和SSIM等指标。

Conclusion: 新方法适用于多种图像类型，确保安全传输且不损害图像美观。

Abstract: Steganography is the process of embedding secret information discreetly
within a carrier, ensuring secure exchange of confidential data. The Adaptive
Pixel Value Differencing (APVD) steganography method, while effective,
encounters certain challenges like the "unused blocks" issue. This problem can
cause a decrease in security, compromise the embedding capacity, and lead to
lower visual quality. This research presents a novel steganographic strategy
that integrates APVD with pseudorandom pixel selection to effectively mitigate
these issues. The results indicate that the new method outperforms existing
techniques in aspects of security, data hiding capacity, and the preservation
of image quality. Empirical results reveal that the combination of APVD with
pseudorandom pixel selection significantly enhances key image quality metrics
such as Peak Signal-to-Noise Ratio (PSNR), Universal Image Quality Index (UIQ),
and Structural Similarity Index (SSIM), surpassing other contemporary methods
in performance. The newly proposed method is versatile, able to handle a
variety of cover and secret images in both color and grayscale, thereby
ensuring secure data transmission without compromising the aesthetic quality of
the image.

</details>


### [101] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT是一种梯度感知免疫技术，用于防御扩散模型对抗恶意微调，同时保留其生成安全内容的能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制（如安全检查器）容易被绕过，概念擦除方法在对抗性微调下失效。

Method: GIFT将免疫问题建模为双层优化问题：上层目标通过表示噪声和最大化降低模型对有害概念的表示能力，下层目标保留对安全数据的性能。

Result: 实验表明，GIFT显著削弱模型重新学习有害概念的能力，同时保持对安全内容的生成质量。

Conclusion: GIFT为创建抵抗对抗性微调攻击的固有安全生成模型提供了有前景的方向。

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [102] [Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database](https://arxiv.org/abs/2507.13802)
*Nehir Kizililsoley,Floor van Meer,Osman Mutlu,Wouter F Hoenderdaal,Rosan G. Hobé,Wenjuan Mu,Arjen Gerssen,H. J. van der Fels-Klerx,Ákos Jóźwiak,Ioannis Manikas,Ali Hürriyetoǧlu,Bas H. M. van der Velden*

Main category: cs.CY

TL;DR: CHEFS数据库整合了EFSA的食品安全监测数据，解决了数据分散问题，并展示了其在趋势分析和政策支持中的潜力。


<details>
  <summary>Details</summary>
Motivation: EFSA的食品安全监测数据分散且难以访问，阻碍了分析和利用。

Method: 创建CHEFS数据库，整合EFSA的农药残留、兽药残留和化学污染物数据。

Result: CHEFS数据库成功集中了数据，并用于分析2000-2024年的食品安全趋势。

Conclusion: CHEFS数据库是食品安全政策、研究和监管的重要工具。

Abstract: In the European Union, official food safety monitoring data collected by
member states are submitted to the European Food Safety Authority (EFSA) and
published on Zenodo. This data includes 392 million analytical results derived
from over 15.2 million samples covering more than 4,000 different types of food
products, offering great opportunities for artificial intelligence to analyze
trends, predict hazards, and support early warning systems. However, the
current format with data distributed across approximately 1000 files totaling
several hundred gigabytes hinders accessibility and analysis. To address this,
we introduce the CompreHensive European Food Safety (CHEFS) database, which
consolidates EFSA monitoring data on pesticide residues, veterinary medicinal
product residues, and chemical contaminants into a unified and structured
dataset. We describe the creation and structure of the CHEFS database and
demonstrate its potential by analyzing trends in European food safety
monitoring data from 2000 to 2024. Our analyses explore changes in monitoring
activities, the most frequently tested products, which products were most often
non-compliant and which contaminants were most often found, and differences
across countries. These findings highlight the CHEFS database as both a
centralized data source and a strategic tool for guiding food safety policy,
research, and regulation.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [103] [Neural Architecture Search with Mixed Bio-inspired Learning Rules](https://arxiv.org/abs/2507.13485)
*Imane Hamzaoui,Riyadh Baghdadi*

Main category: cs.NE

TL;DR: 通过神经架构搜索（NAS）自动发现并混合使用不同层的生物启发学习规则，提高了生物启发神经网络的准确性和可扩展性，甚至在某些情况下超越了基于反向传播的模型。


<details>
  <summary>Details</summary>
Motivation: 生物启发神经网络在对抗鲁棒性、能耗和生理对齐方面具有优势，但在准确性和可扩展性上落后于基于反向传播的模型。研究旨在通过混合不同学习规则来弥补这一差距。

Method: 扩展NAS搜索空间以包含生物启发学习规则，自动发现每层的最佳架构和学习规则。

Result: 混合学习规则的网络在多个数据集上创下生物启发模型的记录，甚至在某些情况下超越基于反向传播的模型。

Conclusion: 层间学习规则的多样性有助于提高准确性和可扩展性，未来研究可进一步探索混合多种生物启发学习规则。

Abstract: Bio-inspired neural networks are attractive for their adversarial robustness,
energy frugality, and closer alignment with cortical physiology, yet they often
lag behind back-propagation (BP) based models in accuracy and ability to scale.
We show that allowing the use of different bio-inspired learning rules in
different layers, discovered automatically by a tailored
neural-architecture-search (NAS) procedure, bridges this gap. Starting from
standard NAS baselines, we enlarge the search space to include bio-inspired
learning rules and use NAS to find the best architecture and learning rule to
use in each layer. We show that neural networks that use different bio-inspired
learning rules for different layers have better accuracy than those that use a
single rule across all the layers. The resulting NN that uses a mix of
bio-inspired learning rules sets new records for bio-inspired models: 95.16% on
CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on
ImageNet. In some regimes, they even surpass comparable BP-based networks while
retaining their robustness advantages. Our results suggest that layer-wise
diversity in learning rules allows better scalability and accuracy, and
motivates further research on mixing multiple bio-inspired learning rules in
the same network.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR是一个两阶段框架，通过大规模视频预训练和掩码逆向动力学模型解决双手机器人操作中的数据稀缺和异构性问题。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作中的数据稀缺和异构性是主要挑战，需要一种能够高效利用有限数据并泛化到新任务和背景的方法。

Method: VIDAR结合了扩散式视频预训练和掩码逆向动力学模型，通过750K多视角视频预训练，并利用掩码提取动作相关信息。

Result: 仅需20分钟的人类演示数据（1%的典型需求），VIDAR在未见过的任务和背景中表现优异，超越现有方法。

Conclusion: 视频基础模型与掩码动作预测结合，为多样化现实场景中的机器人操作提供了可扩展和泛化的解决方案。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [105] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 论文提出了一种多元对齐方法，通过新数据集DIVE和实证研究，改进文本到图像模型的多样性和安全性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型未能涵盖多样化的人类体验，导致系统偏差。研究旨在实现多元对齐，使AI能理解和适应不同甚至冲突的人类价值观。

Method: 引入DIVE数据集，通过多样化人口统计学背景的评估者提供反馈；实证验证人口统计学作为多样化观点的代理；讨论数据收集策略和模型可操控性。

Result: DIVE数据集捕捉了细微的安全感知差异；人口统计学是多样化观点的关键代理；提出了构建对齐模型的具体策略。

Conclusion: 研究为构建更公平和对齐的文本到图像系统提供了基础工具。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [106] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出了一种跨模态自监督预训练方法，用于从大规模未标记的IMU-视频数据中学习表示，提高了在分布外IMU数据集上的HAR任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于IMU的HAR方法依赖特定应用标签，泛化性不足，无法适应不同环境或人群的数据。

Method: 采用跨模态自监督预训练方法，利用IMU-视频数据学习通用表示。

Result: 在零样本和少样本评估中，该方法优于当前最先进的IMU-视频预训练和仅IMU预训练方法。

Conclusion: 跨模态预训练是学习动态数据模态（如IMU信号）通用表示的有效工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [107] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: 提出了一种基于小波系数衰减行为的边缘检测方法，适用于非均匀采样的多元信号，通过快速样本变换实现局部规律性检测。


<details>
  <summary>Details</summary>
Motivation: 传统小波方法在低维结构化数据中表现良好，但在高维和非均匀采样数据中效果有限，因此需要一种更通用的方法。

Method: 利用快速样本变换（一种针对分散数据的小波变换）分析样本系数的衰减行为，量化信号的局部规律性。

Result: 建立了样本系数衰减与多元信号点规律性之间的联系，并在数值实验中验证了方法在一维、二维和三维信号中的有效性。

Conclusion: 样本变换在高维和非均匀采样数据中表现出色，为信号处理提供了新的工具。

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [108] [Leveraging the Spatial Hierarchy: Coarse-to-fine Trajectory Generation via Cascaded Hybrid Diffusion](https://arxiv.org/abs/2507.13366)
*Baoshen Guo,Zhiqing Hong,Junyi Li,Shenhao Wang,Jinhua Zhao*

Main category: cs.SI

TL;DR: Cardiff是一种基于级联混合扩散的轨迹合成框架，用于细粒度和隐私保护的移动性生成。


<details>
  <summary>Details</summary>
Motivation: 由于隐私问题和数据收集成本高，细粒度的人类移动轨迹难以大规模公开。现有方法常忽略轨迹的结构复杂性，无法处理高维分布或生成真实的细粒度轨迹。

Method: Cardiff将生成过程分解为离散路段级和连续GPS级两个层次，分别使用扩散变换器潜在去噪网络和条件去噪网络进行合成。

Result: 在三个大型真实轨迹数据集上的实验表明，Cardiff在多种指标上优于现有基线方法。

Conclusion: Cardiff通过级联去噪逐步生成高保真轨迹，并灵活平衡隐私保护与实用性。

Abstract: Urban mobility data has significant connections with economic growth and
plays an essential role in various smart-city applications. However, due to
privacy concerns and substantial data collection costs, fine-grained human
mobility trajectories are difficult to become publicly available on a large
scale. A promising solution to address this issue is trajectory synthesizing.
However, existing works often ignore the inherent structural complexity of
trajectories, unable to handle complicated high-dimensional distributions and
generate realistic fine-grained trajectories. In this paper, we propose
Cardiff, a coarse-to-fine Cascaded hybrid diffusion-based trajectory
synthesizing framework for fine-grained and privacy-preserving mobility
generation. By leveraging the hierarchical nature of urban mobility, Cardiff
decomposes the generation process into two distinct levels, i.e., discrete road
segment-level and continuous fine-grained GPS-level: (i) In the segment-level,
to reduce computational costs and redundancy in raw trajectories, we first
encode the discrete road segments into low-dimensional latent embeddings and
design a diffusion transformer-based latent denoising network for segment-level
trajectory synthesis. (ii) Taking the first stage of generation as conditions,
we then design a fine-grained GPS-level conditional denoising network with a
noise augmentation mechanism to achieve robust and high-fidelity generation.
Additionally, the Cardiff framework not only progressively generates
high-fidelity trajectories through cascaded denoising but also flexibly enables
a tunable balance between privacy preservation and utility. Experimental
results on three large real-world trajectory datasets demonstrate that our
method outperforms state-of-the-art baselines in various metrics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [109] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出了一种半监督框架，利用世界模型的潜在空间中的控制屏障证书（CBCs）来合成安全的视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 从视觉数据合成安全控制器通常需要大量标记安全关键数据，这在现实场景中不切实际。

Method: 联合学习神经屏障函数和安全控制器，利用现代视觉变换器的预测能力进行潜在动力学建模。

Result: 通过有限标记数据实现了安全的视觉运动策略合成。

Conclusion: 该方法为可扩展且数据高效的安全控制提供了新途径。

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>
