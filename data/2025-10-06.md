<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.CL](#cs.CL) [Total: 1]
- [stat.AP](#stat.AP) [Total: 1]
- [cs.LG](#cs.LG) [Total: 2]
- [cs.CR](#cs.CR) [Total: 2]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 3]
- [cs.CY](#cs.CY) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Exploring OCR-augmented Generation for Bilingual VQA](https://arxiv.org/abs/2510.02543)
*JoonHo Lee,Sunho Park*

Main category: cs.CV

TL;DR: 本文研究了OCR增强的视觉语言模型生成，专注于韩英双语任务，提出了KLOCR OCR基线和KOCRBench韩语VQA基准，实验表明OCR提取的文本显著提升多语言VQA性能。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在OCR增强下的多语言生成能力，特别是针对韩语和英语的双语任务，填补现有VQA基准在韩语方面的不足。

Method: 训练了KLOCR双语OCR基线模型（基于1亿个实例），构建了KOCRBench韩语VQA基准，分析了不同提示方法，并在开源和商业模型上进行了广泛实验。

Result: OCR提取的文本显著提升了开源和商业模型的性能，特别是在双语VQA任务上表现突出。

Conclusion: 该工作为OCR增强的生成提供了新见解，特别是在双语VQA领域，并发布了模型、代码和数据供研究使用。

Abstract: We investigate OCR-augmented generation with Vision Language Models (VLMs),
exploring tasks in Korean and English toward multilingualism. To support
research in this domain, we train and release KLOCR, a strong bilingual OCR
baseline trained on 100M instances to augment VLMs with OCR ability. To
complement existing VQA benchmarks, we curate KOCRBench for Korean VQA, and
analyze different prompting methods. Extensive experiments show that
OCR-extracted text significantly boosts performance across open source and
commercial models. Our work offers new insights into OCR-augmented generation
for bilingual VQA. Model, code, and data are available at
https://github.com/JHLee0513/KLOCR.

</details>


### [2] [Oracle-RLAIF: An Improved Fine-Tuning Framework for Multi-modal Video Models through Reinforcement Learning from Ranking Feedback](https://arxiv.org/abs/2510.02561)
*Derek Shi,Ruben Glatt,Christine Klymko,Shubham Mohole,Hongjun Choi,Shashank Kushwaha,Sam Sakla,Felipe Leno da Silva*

Main category: cs.CV

TL;DR: 本文提出Oracle-RLAIF框架，使用通用Oracle排序器替代传统奖励模型，结合新型rank-based损失函数$GRPO_{rank}$，在视频理解任务中优于现有微调方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频语言模型（VLMs）的微调成本高昂，特别是随着模型参数规模扩大，收集人类反馈的成本急剧增加。现有的RLAIF框架依赖专门训练的奖励模型，这种流程既昂贵又受限。

Method: 提出Oracle-RLAIF框架：1）用通用Oracle排序器替代传统奖励模型，对候选模型响应进行排序而非评分；2）引入基于Group Relative Policy Optimization的$GRPO_{rank}$损失函数，直接优化序数反馈。

Result: 实验证明Oracle-RLAIF在各种视频理解基准测试中持续优于领先的VLMs使用现有微调方法的表现。

Conclusion: Oracle-RLAIF为创建灵活且数据高效的大型多模态视频模型对齐框架开辟了新路径，通过基于排序而非评分的强化学习实现更好的对齐效果。

Abstract: Recent advances in large video-language models (VLMs) rely on extensive
fine-tuning techniques that strengthen alignment between textual and visual
comprehension. Leading pipelines typically pair supervised fine-tuning (SFT)
with reinforcement learning from preference data to enhance video
comprehension. However, as VLMs scale in parameter size, so does the cost of
gathering enough human feedback. To make fine-tuning more cost-effective,
recent frameworks explore reinforcement learning with AI feedback (RLAIF),
which replace human preference with AI as a judge. Current RLAIF frameworks
rely on a specialized reward model trained with video narratives to create
calibrated scalar rewards -- an expensive and restrictive pipeline. We propose
Oracle-RLAIF, a novel framework that replaces the trained reward model with a
more general Oracle ranker which acts as a drop-in model ranking candidate
model responses rather than scoring them. Alongside Oracle-RLAIF, we introduce
$GRPO_{rank}$, a novel rank-based loss function based on Group Relative Policy
Optimization (GRPO) that directly optimizes ordinal feedback with rank-aware
advantages. Empirically, we demonstrate that Oracle-RLAIF consistently
outperforms leading VLMs using existing fine-tuning methods when evaluated
across various video comprehension benchmarks. Oracle-RLAIF paves the path to
creating flexible and data-efficient frameworks for aligning large multi-modal
video models with reinforcement learning from rank rather than score.

</details>


### [3] [PhysHMR: Learning Humanoid Control Policies from Vision for Physically Plausible Human Motion Reconstruction](https://arxiv.org/abs/2510.02566)
*Qiao Feng,Yiming Huang,Yufu Wang,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysHMR提出了一个统一的视觉到动作策略框架，直接从单目视频重建物理合理的人体运动，避免了传统两阶段方法导致的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于运动学姿态估计，缺乏物理约束导致不真实的结果。传统两阶段设计（先运动估计后物理后处理）会引入误差积累，限制了重建质量。

Method: 使用像素射线策略将2D关键点提升为3D空间射线并转换到全局空间，结合预训练编码器的局部视觉特征，通过蒸馏方案将运动捕捉专家的知识迁移到视觉条件策略中，最后使用物理激励的强化学习奖励进行优化。

Result: PhysHMR在多样化场景下产生高保真、物理合理的运动，在视觉精度和物理真实性方面优于现有方法。

Conclusion: 该框架成功实现了物理基础和视觉对齐的运动重建，解决了传统方法的局限性，为单目视频运动重建提供了更有效的解决方案。

Abstract: Reconstructing physically plausible human motion from monocular videos
remains a challenging problem in computer vision and graphics. Existing methods
primarily focus on kinematics-based pose estimation, often leading to
unrealistic results due to the lack of physical constraints. To address such
artifacts, prior methods have typically relied on physics-based post-processing
following the initial kinematics-based motion estimation. However, this
two-stage design introduces error accumulation, ultimately limiting the overall
reconstruction quality. In this paper, we present PhysHMR, a unified framework
that directly learns a visual-to-action policy for humanoid control in a
physics-based simulator, enabling motion reconstruction that is both physically
grounded and visually aligned with the input video. A key component of our
approach is the pixel-as-ray strategy, which lifts 2D keypoints into 3D spatial
rays and transforms them into global space. These rays are incorporated as
policy inputs, providing robust global pose guidance without depending on noisy
3D root predictions. This soft global grounding, combined with local visual
features from a pretrained encoder, allows the policy to reason over both
detailed pose and global positioning. To overcome the sample inefficiency of
reinforcement learning, we further introduce a distillation scheme that
transfers motion knowledge from a mocap-trained expert to the
vision-conditioned policy, which is then refined using physically motivated
reinforcement learning rewards. Extensive experiments demonstrate that PhysHMR
produces high-fidelity, physically plausible motion across diverse scenarios,
outperforming prior approaches in both visual accuracy and physical realism.

</details>


### [4] [Unlocking the power of partnership: How humans and machines can work together to improve face recognition](https://arxiv.org/abs/2510.02570)
*P. Jonathon Phillips,Geraldine Jeckeln,Carina A. Hahn,Amy N. Yates,Peter C. Fontana,Alice J. O'Toole*

Main category: cs.CV

TL;DR: 本文研究了人机协作在面部识别中的效果，发现当人类和机器基线准确率差异较小时，协作效果最好。提出了近端准确率规则(PAR)来预测协作效益，并建立了关键融合区域。智能人机融合比单独使用机器或简单组合所有判断更准确。


<details>
  <summary>Details</summary>
Motivation: 研究人机协作在面部识别决策中的效果，探索在什么情况下人机协作能够提高识别准确率，以及如何优化这种协作系统。

Method: 使用专家和非专家面部识别者的数据，分析人-人和人-机协作的效果。通过PAR规则预测协作效益，建立关键融合区域，并实施智能人机融合策略。使用图论分析全人类系统的性能。

Result: 发现协作效益随协作双方基线准确率差异减小而增加。智能人机融合比单独机器更准确，也比简单组合所有判断更有效。全人类系统的性能接近智能人机协作的平均水平，但后者能更好地减少低性能人类对系统准确率的影响。

Conclusion: 研究证明了人类和机器在确保准确面部识别中都有重要作用，为AI在面部识别中的智能使用提供了基于证据的路线图。

Abstract: Human review of consequential decisions by face recognition algorithms
creates a "collaborative" human-machine system. Individual differences between
people and machines, however, affect whether collaboration improves or degrades
accuracy in any given case. We establish the circumstances under which
combining human and machine face identification decisions improves accuracy.
Using data from expert and non-expert face identifiers, we examined the
benefits of human-human and human-machine collaborations. The benefits of
collaboration increased as the difference in baseline accuracy between
collaborators decreased-following the Proximal Accuracy Rule (PAR). This rule
predicted collaborative (fusion) benefit across a wide range of baseline
abilities, from people with no training to those with extensive training. Using
the PAR, we established a critical fusion zone, where humans are less accurate
than the machine, but fusing the two improves system accuracy. This zone was
surprisingly large. We implemented "intelligent human-machine fusion" by
selecting people with the potential to increase the accuracy of a
high-performing machine. Intelligent fusion was more accurate than the machine
operating alone and more accurate than combining all human and machine
judgments. The highest system-wide accuracy achievable with human-only
partnerships was found by graph theory. This fully human system approximated
the average performance achieved by intelligent human-machine collaboration.
However, intelligent human-machine collaboration more effectively minimized the
impact of low-performing humans on system-wide accuracy. The results
demonstrate a meaningful role for both humans and machines in assuring accurate
face identification. This study offers an evidence-based road map for the
intelligent use of AI in face identification.

</details>


### [5] [How Confident are Video Models? Empowering Video Models to Express their Uncertainty](https://arxiv.org/abs/2510.02571)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 本文首次提出了针对生成式视频模型的不确定性量化方法，通过S-QUBED框架分解预测不确定性为偶然性和认知性成分，并建立了校准评估指标和基准数据集。


<details>
  <summary>Details</summary>
Motivation: 生成式视频模型虽然能生成逼真视频，但存在幻觉问题，可能产生事实错误的视频内容。目前缺乏对视频模型的不确定性量化方法，存在安全隐患。

Method: 提出了S-QUBED框架：1）基于稳健秩相关估计的校准评估指标；2）利用潜在建模将预测不确定性分解为偶然性和认知性成分的黑盒UQ方法；3）用于视频模型校准基准测试的UQ数据集。

Result: 在基准视频数据集上的实验表明，S-QUBED能够计算校准的总不确定性估计，与任务准确度呈负相关，并能有效分解偶然性和认知性不确定性成分。

Conclusion: 该研究填补了视频模型不确定性量化的空白，提出的S-QUBED框架为视频生成模型的安全应用提供了重要的不确定性评估工具。

Abstract: Generative video models demonstrate impressive text-to-video capabilities,
spurring widespread adoption in many real-world applications. However, like
large language models (LLMs), video generation models tend to hallucinate,
producing plausible videos even when they are factually wrong. Although
uncertainty quantification (UQ) of LLMs has been extensively studied in prior
work, no UQ method for video models exists, raising critical safety concerns.
To our knowledge, this paper represents the first work towards quantifying the
uncertainty of video models. We present a framework for uncertainty
quantification of generative video models, consisting of: (i) a metric for
evaluating the calibration of video models based on robust rank correlation
estimation with no stringent modeling assumptions; (ii) a black-box UQ method
for video models (termed S-QUBED), which leverages latent modeling to
rigorously decompose predictive uncertainty into its aleatoric and epistemic
components; and (iii) a UQ dataset to facilitate benchmarking calibration in
video models. By conditioning the generation task in the latent space, we
disentangle uncertainty arising due to vague task specifications from that
arising from lack of knowledge. Through extensive experiments on benchmark
video datasets, we demonstrate that S-QUBED computes calibrated total
uncertainty estimates that are negatively correlated with the task accuracy and
effectively computes the aleatoric and epistemic constituents.

</details>


### [6] [PEO: Training-Free Aesthetic Quality Enhancement in Pre-Trained Text-to-Image Diffusion Models with Prompt Embedding Optimization](https://arxiv.org/abs/2510.02599)
*Hovhannes Margaryan,Bo Wan,Tinne Tuytelaars*

Main category: cs.CV

TL;DR: 本文提出了一种名为Prompt Embedding Optimization (PEO)的新方法，通过优化预训练文本到图像扩散模型中的文本嵌入来提升生成图像的美学质量，无需额外训练且与模型架构无关。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在给定简单提示词时，生成的图像美学质量往往不足。需要一种无需重新训练模型就能提升生成图像美学质量的方法。

Method: PEO方法利用预训练的文本到图像扩散模型作为主干，通过三重目标函数优化给定简单提示词的文本嵌入：1）提升生成图像的美学保真度；2）确保与优化后文本嵌入的一致性；3）通过提示词保持项最小化与初始提示词的差异。

Result: 定量和定性评估表明，PEO方法在性能上达到或超过了最先进的文本到图像和提示词适应方法。

Conclusion: PEO是一种训练无关、主干无关的有效方法，能够显著提升预训练扩散模型在简单提示词下的图像生成美学质量。

Abstract: This paper introduces a novel approach to aesthetic quality improvement in
pre-trained text-to-image diffusion models when given a simple prompt. Our
method, dubbed Prompt Embedding Optimization (PEO), leverages a pre-trained
text-to-image diffusion model as a backbone and optimizes the text embedding of
a given simple and uncurated prompt to enhance the visual quality of the
generated image. We achieve this by a tripartite objective function that
improves the aesthetic fidelity of the generated image, ensures adherence to
the optimized text embedding, and minimal divergence from the initial prompt.
The latter is accomplished through a prompt preservation term. Additionally,
PEO is training-free and backbone-independent. Quantitative and qualitative
evaluations confirm the effectiveness of the proposed method, exceeding or
equating the performance of state-of-the-art text-to-image and prompt
adaptation methods.

</details>


### [7] [Ego-Exo 3D Hand Tracking in the Wild with a Mobile Multi-Camera Rig](https://arxiv.org/abs/2510.02601)
*Patrick Rim,Kun He,Kevin Harris,Braden Copple,Shangchen Han,Sizhe An,Ivan Shugurov,Tomas Hodan,He Wen,Xu Xie*

Main category: cs.CV

TL;DR: 本文提出了一种新型无标记多摄像头系统，用于在真实野外环境中捕捉精确的3D手部姿势和物体交互，解决了现有数据集环境多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有手部跟踪数据集主要在受控实验室环境中采集，限制了环境多样性和模型泛化能力。需要开发能够在真实野外条件下捕捉精确3D手部姿势的方法。

Method: 设计轻量级背戴式捕捉装置，配备8个外中心摄像头和Meta Quest 3头显提供的2个内中心视角。开发了自我-外中心跟踪流程来生成准确的3D手部姿势真值。

Result: 成功收集了包含同步多视角图像和精确3D手部姿势的标注数据集，显著降低了环境真实性与3D标注精度之间的权衡。

Conclusion: 该系统能够在几乎不受限制的移动条件下，在真实野外环境中实现高质量的3D手部跟踪，为手部交互研究提供了更真实的数据基础。

Abstract: Accurate 3D tracking of hands and their interactions with the world in
unconstrained settings remains a significant challenge for egocentric computer
vision. With few exceptions, existing datasets are predominantly captured in
controlled lab setups, limiting environmental diversity and model
generalization. To address this, we introduce a novel marker-less multi-camera
system designed to capture precise 3D hands and objects, which allows for
nearly unconstrained mobility in genuinely in-the-wild conditions. We combine a
lightweight, back-mounted capture rig with eight exocentric cameras, and a
user-worn Meta Quest 3 headset, which contributes two egocentric views. We
design an ego-exo tracking pipeline to generate accurate 3D hand pose ground
truth from this system, and rigorously evaluate its quality. By collecting an
annotated dataset featuring synchronized multi-view images and precise 3D hand
poses, we demonstrate the capability of our approach to significantly reduce
the trade-off between environmental realism and 3D annotation accuracy.

</details>


### [8] [Input-Aware Sparse Attention for Real-Time Co-Speech Video Generation](https://arxiv.org/abs/2510.02617)
*Beijia Lu,Ziyi Chen,Jing Xiao,Jun-Yan Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种新的视频蒸馏方法，通过输入人体姿态条件来改进注意力机制和损失函数，实现了从多步扩散模型到少步学生模型的蒸馏，达到实时性能并提升视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的语音驱动视频生成方法由于需要大量去噪步骤和昂贵的注意力机制，导致速度慢，无法实现实时部署。

Method: 1. 使用输入人体姿态关键点之间的精确对应关系来引导注意力到相关区域（如人脸、手部和上半身）；2. 提出输入感知稀疏注意力减少冗余计算并增强身体部位的时间对应关系；3. 引入输入感知蒸馏损失来改善唇部同步和手部运动真实性。

Result: 该方法实现了实时性能，在视觉质量上优于最近的音频驱动和输入驱动方法，并通过大量实验验证了算法设计的有效性。

Conclusion: 通过集成输入感知稀疏注意力和蒸馏损失，本文的方法在保持高质量的同时实现了实时语音驱动视频生成，为视频创作和虚拟代理等应用提供了实用解决方案。

Abstract: Diffusion models can synthesize realistic co-speech video from audio for
various applications, such as video creation and virtual agents. However,
existing diffusion-based methods are slow due to numerous denoising steps and
costly attention mechanisms, preventing real-time deployment. In this work, we
distill a many-step diffusion video model into a few-step student model.
Unfortunately, directly applying recent diffusion distillation methods degrades
video quality and falls short of real-time performance. To address these
issues, our new video distillation method leverages input human pose
conditioning for both attention and loss functions. We first propose using
accurate correspondence between input human pose keypoints to guide attention
to relevant regions, such as the speaker's face, hands, and upper body. This
input-aware sparse attention reduces redundant computations and strengthens
temporal correspondences of body parts, improving inference efficiency and
motion coherence. To further enhance visual quality, we introduce an
input-aware distillation loss that improves lip synchronization and hand motion
realism. By integrating our input-aware sparse attention and distillation loss,
our method achieves real-time performance with improved visual quality compared
to recent audio-driven and input-driven methods. We also conduct extensive
experiments showing the effectiveness of our algorithmic design choices.

</details>


### [9] [Deep Generative Continual Learning using Functional LoRA: FunLoRA](https://arxiv.org/abs/2510.02631)
*Victor Enescu,Hichem Sahbi*

Main category: cs.CV

TL;DR: 提出了一种基于LoRA的新型条件机制FunLoRA，用于缓解深度生成模型在持续学习中的灾难性遗忘问题，通过仅使用秩1矩阵和函数重参数化来提高表达能力，实现参数高效微调。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在持续学习中面临灾难性遗忘问题，传统方法依赖合成数据进行再训练会导致训练时间不可控和长期性能下降，需要更高效的适应机制。

Method: 设计FunLoRA方法，使用秩1矩阵和函数重参数化构建动态条件机制，使生成模型只需在当前任务数据上训练即可避免遗忘，基于流匹配模型进行训练。

Result: 实验表明FunLoRA在分类准确率上超越基于扩散模型的现有最佳方法，同时显著降低内存成本和采样时间。

Conclusion: FunLoRA提供了一种参数高效的持续学习解决方案，有效解决了生成模型增量训练中的核心挑战。

Abstract: Continual adaptation of deep generative models holds tremendous potential and
critical importance, given their rapid and expanding usage in text and vision
based applications. Incremental training, however, remains highly challenging
due to catastrophic forgetting phenomenon, which makes it difficult for neural
networks to effectively incorporate new knowledge. A common strategy consists
in retraining the generative model on its own synthetic data in order to
mitigate forgetting. Yet, such an approach faces two major limitations: (i) the
continually increasing training time eventually becomes intractable, and (ii)
reliance on synthetic data inevitably leads to long-term performance
degradation, since synthetic samples lack the richness of real training data.
In this paper, we attenuate these issues by designing a novel and more
expressive conditioning mechanism for generative models based on low rank
adaptation (LoRA), that exclusively employs rank 1 matrices, whose
reparametrized matrix rank is functionally increased using carefully selected
functions -- and dubbed functional LoRA: FunLoRA. Using this dynamic
conditioning, the generative model is guaranteed to avoid catastrophic
forgetting and needs only to be trained on data from the current task.
Extensive experiments using flow-matching based models trained from scratch,
showcase that our proposed parameter-efficient fine-tuning (PEFT) method
surpasses prior state-of-the-art results based on diffusion models, reaching
higher classification accuracy scores, while only requiring a fraction of the
memory cost and sampling time.

</details>


### [10] [Sequence-Preserving Dual-FoV Defense for Traffic Sign and Light Recognition in Autonomous Vehicles](https://arxiv.org/abs/2510.02642)
*Abhishek Joshi,Jahnavi Krishna Koda,Abhishek Phadke*

Main category: cs.CV

TL;DR: 提出了一种双视野、序列保持的鲁棒性框架，用于美国交通灯和标志识别，通过统一防御堆栈显著提升模型对抗数字和自然退化的能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法缺乏对时间连续性、多静态视野感知以及数字和自然退化鲁棒性的考虑，而感知错误直接影响自动驾驶的导航和安全。

Method: 基于多源数据集构建双视野序列，采用统一三层防御堆栈（特征压缩、防御蒸馏、基于熵的异常检测）和序列时间投票增强。

Result: 统一防御堆栈达到79.8mAP，攻击成功率降至18.2%，优于YOLOv8、YOLOv9和BEVFormer，高风险误分类降至32%。

Conclusion: 该框架有效提升了交通灯和标志识别的鲁棒性，证明了物理可转移性，为自动驾驶安全提供了重要保障。

Abstract: Traffic light and sign recognition are key for Autonomous Vehicles (AVs)
because perception mistakes directly influence navigation and safety. In
addition to digital adversarial attacks, models are vulnerable to existing
perturbations (glare, rain, dirt, or graffiti), which could lead to dangerous
misclassifications. The current work lacks consideration of temporal
continuity, multistatic field-of-view (FoV) sensing, and robustness to both
digital and natural degradation. This study proposes a dual FoV,
sequence-preserving robustness framework for traffic lights and signs in the
USA based on a multi-source dataset built on aiMotive, Udacity, Waymo, and
self-recorded videos from the region of Texas. Mid and long-term sequences of
RGB images are temporally aligned for four operational design domains (ODDs):
highway, night, rainy, and urban. Over a series of experiments on a real-life
application of anomaly detection, this study outlines a unified three-layer
defense stack framework that incorporates feature squeezing, defensive
distillation, and entropy-based anomaly detection, as well as sequence-wise
temporal voting for further enhancement. The evaluation measures included
accuracy, attack success rate (ASR), risk-weighted misclassification severity,
and confidence stability. Physical transferability was confirmed using probes
for recapture. The results showed that the Unified Defense Stack achieved
79.8mAP and reduced the ASR to 18.2%, which is superior to YOLOv8, YOLOv9, and
BEVFormer, while reducing the high-risk misclassification to 32%.

</details>


### [11] [Smart-GRPO: Smartly Sampling Noise for Efficient RL of Flow-Matching Models](https://arxiv.org/abs/2510.02654)
*Benjamin Yu,Jackie Liu,Justin Cui*

Main category: cs.CV

TL;DR: Smart-GRPO是一种优化流匹配模型中噪声扰动的方法，通过迭代搜索策略改进强化学习效果，解决了确定性流匹配模型不适合强化学习的问题。


<details>
  <summary>Details</summary>
Motivation: 流匹配模型的确定性使其不适合强化学习，而现有方法通过随机噪声扰动引入随机性效率低下且不稳定。

Method: Smart-GRPO采用迭代搜索策略：解码候选扰动、用奖励函数评估、向高奖励区域优化噪声分布。

Result: 实验表明Smart-GRPO在奖励优化和视觉质量方面优于基线方法。

Conclusion: Smart-GRPO为流匹配框架中的强化学习提供了实用路径，弥合了高效训练与人类对齐生成之间的差距。

Abstract: Recent advancements in flow-matching have enabled high-quality text-to-image
generation. However, the deterministic nature of flow-matching models makes
them poorly suited for reinforcement learning, a key tool for improving image
quality and human alignment. Prior work has introduced stochasticity by
perturbing latents with random noise, but such perturbations are inefficient
and unstable. We propose Smart-GRPO, the first method to optimize noise
perturbations for reinforcement learning in flow-matching models. Smart-GRPO
employs an iterative search strategy that decodes candidate perturbations,
evaluates them with a reward function, and refines the noise distribution
toward higher-reward regions. Experiments demonstrate that Smart-GRPO improves
both reward optimization and visual quality compared to baseline methods. Our
results suggest a practical path toward reinforcement learning in flow-matching
frameworks, bridging the gap between efficient training and human-aligned
generation.

</details>


### [12] [FSFSplatter: Build Surface and Novel Views with Sparse-Views within 3min](https://arxiv.org/abs/2510.02691)
*Yibin Zhao,Yihan Pan,Jun Nan,Jianjun Yi*

Main category: cs.CV

TL;DR: FSFSplatter是一种从稀疏自由图像进行快速表面重建的新方法，通过端到端密集高斯初始化、相机参数估计和几何增强场景优化，解决了传统方法需要密集校准视图的限制。


<details>
  <summary>Details</summary>
Motivation: 现有高斯重建方法大多需要密集、校准的视图，从稀疏自由图像重建时由于视图重叠有限和过拟合问题，往往导致表面重建质量差。

Method: 使用大型Transformer编码多视图图像，通过自分割高斯头生成密集且几何一致的高斯场景初始化；采用基于贡献的剪枝消除局部浮动点，利用深度和多视图特征监督以及可微分相机参数来缓解过拟合问题。

Result: FSFSplatter在广泛使用的DTU和Replica数据集上优于当前最先进的方法。

Conclusion: 该方法能够有效处理稀疏自由图像的重建问题，在保持高质量的同时实现了快速表面重建。

Abstract: Gaussian Splatting has become a leading reconstruction technique, known for
its high-quality novel view synthesis and detailed reconstruction. However,
most existing methods require dense, calibrated views. Reconstructing from free
sparse images often leads to poor surface due to limited overlap and
overfitting. We introduce FSFSplatter, a new approach for fast surface
reconstruction from free sparse images. Our method integrates end-to-end dense
Gaussian initialization, camera parameter estimation, and geometry-enhanced
scene optimization. Specifically, FSFSplatter employs a large Transformer to
encode multi-view images and generates a dense and geometrically consistent
Gaussian scene initialization via a self-splitting Gaussian head. It eliminates
local floaters through contribution-based pruning and mitigates overfitting to
limited views by leveraging depth and multi-view feature supervision with
differentiable camera parameters during rapid optimization. FSFSplatter
outperforms current state-of-the-art methods on widely used DTU and Replica.

</details>


### [13] [MoGIC: Boosting Motion Generation via Intention Understanding and Visual Context](https://arxiv.org/abs/2510.02722)
*Junyu Shi,Yong Sun,Zhiyuan Zhang,Lijiang Liu,Zhengjie Zhang,Yuxin He,Qiang Nie*

Main category: cs.CV

TL;DR: MoGIC是一个统一的运动生成框架，通过整合意图建模和视觉先验来提升多模态运动合成的精度和个性化能力


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动运动生成方法存在局限性：无法捕捉动作执行的因果逻辑和人类意图，缺乏视觉基础导致无法指定细粒度时空细节

Method: 提出MoGIC框架，联合优化多模态条件运动生成和意图预测，引入混合注意力机制实现条件标记与运动子序列的有效局部对齐，构建440小时基准数据集Mo440H

Result: 微调后MoGIC在HumanML3D上FID降低38.6%，在Mo440H上降低34.6%，在运动描述任务上超越基于LLM的方法，支持意图预测和视觉条件生成

Conclusion: MoGIC推进了可控运动合成和意图理解，实现了多模态生成能力

Abstract: Existing text-driven motion generation methods often treat synthesis as a
bidirectional mapping between language and motion, but remain limited in
capturing the causal logic of action execution and the human intentions that
drive behavior. The absence of visual grounding further restricts precision and
personalization, as language alone cannot specify fine-grained spatiotemporal
details. We propose MoGIC, a unified framework that integrates intention
modeling and visual priors into multimodal motion synthesis. By jointly
optimizing multimodal-conditioned motion generation and intention prediction,
MoGIC uncovers latent human goals, leverages visual priors to enhance
generation, and exhibits versatile multimodal generative capability. We further
introduce a mixture-of-attention mechanism with adaptive scope to enable
effective local alignment between conditional tokens and motion subsequences.
To support this paradigm, we curate Mo440H, a 440-hour benchmark from 21
high-quality motion datasets. Experiments show that after finetuning, MoGIC
reduces FID by 38.6\% on HumanML3D and 34.6\% on Mo440H, surpasses LLM-based
methods in motion captioning with a lightweight text head, and further enables
intention prediction and vision-conditioned generation, advancing controllable
motion synthesis and intention understanding. The code is available at
https://github.com/JunyuShi02/MoGIC

</details>


### [14] [From Tokens to Nodes: Semantic-Guided Motion Control for Dynamic 3D Gaussian Splatting](https://arxiv.org/abs/2510.02732)
*Jianing Chen,Zehao Li,Yujun Cai,Hao Jiang,Shuqin Gao,Honglong Zhao,Tianlu Mao,Yucheng Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于运动自适应的3D重建框架，通过语义和运动先验优化控制点分配，解决了传统方法中静态冗余和动态不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏控制方法仅基于几何分配控制点，导致静态区域冗余和动态区域控制不足，无法有效处理动态3D重建中的运动复杂性。

Method: 利用视觉基础模型的语义和运动先验建立patch-token-node对应关系，通过运动自适应压缩机制在动态区域集中控制点，同时抑制静态背景冗余；采用样条轨迹参数化替代MLP变形场以实现更平滑的运动表示。

Result: 大量实验表明，该方法在重建质量和效率上显著优于现有最先进方法。

Conclusion: 提出的运动自适应框架通过智能控制点分配和优化轨迹表示，有效解决了动态3D重建中的关键挑战，为单目视频的3D重建提供了更高效的解决方案。

Abstract: Dynamic 3D reconstruction from monocular videos remains difficult due to the
ambiguity inferring 3D motion from limited views and computational demands of
modeling temporally varying scenes. While recent sparse control methods
alleviate computation by reducing millions of Gaussians to thousands of control
points, they suffer from a critical limitation: they allocate points purely by
geometry, leading to static redundancy and dynamic insufficiency. We propose a
motion-adaptive framework that aligns control density with motion complexity.
Leveraging semantic and motion priors from vision foundation models, we
establish patch-token-node correspondences and apply motion-adaptive
compression to concentrate control points in dynamic regions while suppressing
redundancy in static backgrounds. Our approach achieves flexible
representational density adaptation through iterative voxelization and motion
tendency scoring, directly addressing the fundamental mismatch between control
point allocation and motion complexity. To capture temporal evolution, we
introduce spline-based trajectory parameterization initialized by 2D tracklets,
replacing MLP-based deformation fields to achieve smoother motion
representation and more stable optimization. Extensive experiments demonstrate
significant improvements in reconstruction quality and efficiency over existing
state-of-the-art methods.

</details>


### [15] [Net2Net: When Un-trained Meets Pre-trained Networks for Robust Real-World Denoising](https://arxiv.org/abs/2510.02733)
*Weimin Yuan,Cai Meng*

Main category: cs.CV

TL;DR: Net2Net是一种结合未训练网络和预训练网络的创新方法，用于解决真实世界噪声去除的挑战，通过无监督DIP和监督预训练模型DRUNet的结合，利用RED正则化实现更好的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 传统去噪方法依赖手工先验，在受控环境中表现良好但难以处理真实噪声的复杂性；基于深度学习的方法需要大量标注数据且跨噪声类型泛化能力有限。

Method: Net2Net结合未训练网络和预训练网络，通过RED正则化将无监督DIP与监督预训练模型DRUNet集成，未训练网络适应输入图像的独特噪声特征，预训练网络利用大规模数据集学习表示。

Result: 在基准数据集上的广泛实验表明，该方法在真实世界噪声去除方面具有优越性，特别是在训练数据有限的情况下表现更好。

Conclusion: Net2Net的混合框架增强了跨不同噪声模式的泛化能力，提高了去噪性能，为真实世界噪声去除提供了有效解决方案。

Abstract: Traditional denoising methods for noise removal have largely relied on
handcrafted priors, often perform well in controlled environments but struggle
to address the complexity and variability of real noise. In contrast, deep
learning-based approaches have gained prominence for learning noise
characteristics from large datasets, but these methods frequently require
extensive labeled data and may not generalize effectively across diverse noise
types and imaging conditions. In this paper, we present an innovative method,
termed as Net2Net, that combines the strengths of untrained and pre-trained
networks to tackle the challenges of real-world noise removal. The innovation
of Net2Net lies in its combination of unsupervised DIP and supervised
pre-trained model DRUNet by regularization by denoising (RED). The untrained
network adapts to the unique noise characteristics of each input image without
requiring labeled data, while the pre-trained network leverages learned
representations from large-scale datasets to deliver robust denoising
performance. This hybrid framework enhances generalization across varying noise
patterns and improves performance, particularly in scenarios with limited
training data. Extensive experiments on benchmark datasets demonstrate the
superiority of our method for real-world noise removal.

</details>


### [16] [Retrv-R1: A Reasoning-Driven MLLM Framework for Universal and Efficient Multimodal Retrieval](https://arxiv.org/abs/2510.02745)
*Lanyun Zhu,Deyi Ji,Tianrun Chen,Haiyang Wu,Shiqi Wang*

Main category: cs.CV

TL;DR: Retrv-R1是首个专门为多模态通用检索设计的R1风格MLLM，通过逐步推理提高检索准确性，解决了DeepSeek-R1在检索任务中计算成本高和训练不稳定的问题。


<details>
  <summary>Details</summary>
Motivation: 直接应用DeepSeek-R1方法到检索任务不可行，主要因为多候选推理过程导致的高计算成本，以及RL训练检索任务时的不稳定性和次优结果。

Method: 引入信息压缩模块和细节检查机制减少token数量，提出新的训练范式包括检索定制的合成CoT数据集激活阶段和带有课程奖励的RL训练。

Result: Retrv-R1在多个基准测试和任务中实现了SOTA性能、高效率和强泛化能力。

Conclusion: 通过新颖的设计，Retrv-R1成功解决了多模态检索中的计算效率和训练稳定性问题，展示了RL增强LLM推理能力的巨大潜力。

Abstract: The success of DeepSeek-R1 demonstrates the immense potential of using
reinforcement learning (RL) to enhance LLMs' reasoning capabilities. This paper
introduces Retrv-R1, the first R1-style MLLM specifically designed for
multimodal universal retrieval, achieving higher performance by employing
step-by-step reasoning to produce more accurate retrieval results. We find that
directly applying the methods of DeepSeek-R1 to retrieval tasks is not
feasible, mainly due to (1) the high computational cost caused by the large
token consumption required for multiple candidates with reasoning processes,
and (2) the instability and suboptimal results when directly applying RL to
train for retrieval tasks. To address these issues, Retrv-R1 introduces an
information compression module with a details inspection mechanism, which
enhances computational efficiency by reducing the number of tokens while
ensuring that critical information for challenging candidates is preserved.
Furthermore, a new training paradigm is proposed, including an activation stage
using a retrieval-tailored synthetic CoT dataset for more effective
optimization, followed by RL with a novel curriculum reward to improve both
performance and efficiency. Incorporating these novel designs, Retrv-R1
achieves SOTA performance, high efficiency, and strong generalization ability,
as demonstrated by experiments across multiple benchmarks and tasks.

</details>


### [17] [Bayesian Test-time Adaptation for Object Recognition and Detection with Vision-language Models](https://arxiv.org/abs/2510.02750)
*Lihua Zhou,Mao Ye,Shuaifeng Li,Nianxin Li,Jinlin Wu,Xiatian Zhu,Lei Deng,Hongbin Liu,Jiebo Luo,Zhen Lei*

Main category: cs.CV

TL;DR: BCA+是一个无需训练、基于贝叶斯推理的测试时自适应框架，用于提升视觉语言模型在目标识别和检测任务中的分布偏移鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有测试时自适应方法要么计算成本高（依赖反向传播），要么只关注似然度适应而忽略先验分布的重要性。

Method: 引入动态缓存机制，自适应存储和更新类别嵌入、空间尺度（检测任务）以及基于历史预测的自适应类别先验，将适应问题建模为贝叶斯推理问题。

Result: 在识别和检测基准测试中达到了最先进的性能。

Conclusion: BCA+通过双适应机制和不确定性引导的融合，能够有效校正模型的语义理解和上下文置信度，且具有高效性。

Abstract: Vision-language models (VLMs) such as CLIP and Grounding DINO have achieved
remarkable success in object recognition and detection. However, their
performance often degrades under real-world distribution shifts. Test-time
adaptation (TTA) aims to mitigate this issue by adapting models during
inference. Existing methods either rely on computationally expensive
backpropagation, which hinders real-time deployment, or focus solely on
likelihood adaptation, which overlooks the critical role of the prior. Our
prior work, Bayesian Class Adaptation (BCA), addressed these shortcomings for
object recognition by introducing a training-free framework that incorporates
adaptive priors. Building upon this foundation, we now present Bayesian Class
Adaptation plus (BCA+), a unified, training-free framework for TTA for both
object recognition and detection. BCA+ introduces a dynamic cache that
adaptively stores and updates class embeddings, spatial scales (for detection),
and, crucially, adaptive class priors derived from historical predictions. We
formulate adaptation as a Bayesian inference problem, where final predictions
are generated by fusing the initial VLM output with a cache-based prediction.
This cache-based prediction combines a dynamically updated likelihood
(measuring feature and scale similarity) and a prior (reflecting the evolving
class distribution). This dual-adaptation mechanism, coupled with
uncertainty-guided fusion, enables BCA+ to correct both the model's semantic
understanding and its contextual confidence. As a training-free method
requiring no backpropagation, BCA+ is highly efficient. Extensive experiments
demonstrate that BCA+ achieves state-of-the-art performance on both recognition
and detection benchmarks.

</details>


### [18] [Hierarchical Generalized Category Discovery for Brain Tumor Classification in Digital Pathology](https://arxiv.org/abs/2510.02760)
*Matthias Perkonigg,Patrick Rockenschaub,Georg Göbel,Adelheid Wöhrer*

Main category: cs.CV

TL;DR: 提出了HGCD-BT方法，通过层次聚类和对比学习相结合，在脑肿瘤分类中实现了对已知和未知类别的识别，在OpenSRH数据集上比现有GCD方法准确率提升28%。


<details>
  <summary>Details</summary>
Motivation: 现有脑肿瘤分类方法局限于预定义的固定类别，无法识别训练时未见的肿瘤类型。无监督学习缺乏利用标注数据先验知识的能力，半监督方法假设所有潜在类别都在标注数据中。

Method: HGCD-BT将层次聚类与对比学习相结合，在对比学习基础上引入半监督层次聚类损失，能够捕获脑肿瘤分类学的层次结构。

Result: 在OpenSRH数据集上，HGCD-BT在斑块级分类中比最先进GCD方法准确率提升28%，特别是在识别未见肿瘤类别方面表现优异。在Digital Brain Tumor Atlas的玻片级分类中也展示了良好泛化能力。

Conclusion: HGCD-BT方法能够有效识别已知和未知脑肿瘤类别，在不同成像模态中都具有良好性能，为神经肿瘤手术中的术中决策提供了重要支持。

Abstract: Accurate brain tumor classification is critical for intra-operative decision
making in neuro-oncological surgery. However, existing approaches are
restricted to a fixed set of predefined classes and are therefore unable to
capture patterns of tumor types not available during training. Unsupervised
learning can extract general-purpose features, but it lacks the ability to
incorporate prior knowledge from labelled data, and semi-supervised methods
often assume that all potential classes are represented in the labelled data.
Generalized Category Discovery (GCD) aims to bridge this gap by categorizing
both known and unknown classes within unlabelled data. To reflect the
hierarchical structure of brain tumor taxonomies, in this work, we introduce
Hierarchical Generalized Category Discovery for Brain Tumor Classification
(HGCD-BT), a novel approach that integrates hierarchical clustering with
contrastive learning. Our method extends contrastive learning based GCD by
incorporating a novel semi-supervised hierarchical clustering loss. We evaluate
HGCD-BT on OpenSRH, a dataset of stimulated Raman histology brain tumor images,
achieving a +28% improvement in accuracy over state-of-the-art GCD methods for
patch-level classification, particularly in identifying previously unseen tumor
categories. Furthermore, we demonstrate the generalizability of HGCD-BT on
slide-level classification of hematoxylin and eosin stained whole-slide images
from the Digital Brain Tumor Atlas, confirming its utility across imaging
modalities.

</details>


### [19] [AdaRD-key: Adaptive Relevance-Diversity Keyframe Sampling for Long-form Video understanding](https://arxiv.org/abs/2510.02778)
*Xian Zhang,Zexi Wu,Zinuo Li,Hongming Xu,Luqi Gong,Farid Boussaid,Naoufel Werghi,Mohammed Bennamoun*

Main category: cs.CV

TL;DR: AdaRD-Key是一种无需训练的关键帧采样模块，通过最大化相关性-多样性目标，为长视频理解提供信息丰富且非冗余的帧选择，在长视频基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在关键问题：均匀采样容易忽略关键时刻，传统关键帧选择方法采用刚性时间间隔会错过精细线索，而强调视觉多样性的方法又忽略了查询相关性。

Method: 提出训练免费的AdaRD-Key模块，最大化统一的相关性-多样性最大体积目标，结合查询条件相关性评分和对数行列式多样性组件，并包含轻量级相关性感知门控机制来处理弱对齐查询。

Result: 在LongVideoBench和Video-MME上的广泛实验表明，该方法在长视频理解任务上达到了最先进的性能，特别是在长视频上表现优异。

Conclusion: AdaRD-Key是一个计算高效、无需训练、即插即用的解决方案，能够有效解决长视频理解中的关键帧选择问题。

Abstract: Understanding long-form videos remains a significant challenge for
vision--language models (VLMs) due to their extensive temporal length and high
information density. Most current multimodal large language models (MLLMs) rely
on uniform sampling, which often overlooks critical moments, leading to
incorrect responses to queries. In parallel, many keyframe selection approaches
impose rigid temporal spacing: once a frame is chosen, an exclusion window
suppresses adjacent timestamps to reduce redundancy. While effective at
limiting overlap, this strategy frequently misses short, fine-grained cues near
important events. Other methods instead emphasize visual diversity but neglect
query relevance. We propose AdaRD-Key, a training-free keyframe sampling module
for query-driven long-form video understanding. AdaRD-Key maximizes a unified
Relevance--Diversity Max-Volume (RD-MV) objective, combining a
query-conditioned relevance score with a log-determinant diversity component to
yield informative yet non-redundant frames. To handle broad queries with weak
alignment to the video, AdaRD-Key employs a lightweight relevance-aware gating
mechanism; when the relevance distribution indicates weak alignment, the method
seamlessly shifts into a diversity-only mode, enhancing coverage without
additional supervision. Our pipeline is training-free, computationally
efficient (running in real time on a single GPU), and compatible with existing
VLMs in a plug-and-play manner. Extensive experiments on LongVideoBench and
Video-MME demonstrate state-of-the-art performance, particularly on long-form
videos. Code available at https://github.com/Xian867/AdaRD-Key.

</details>


### [20] [Reasoning Riddles: How Explainability Reveals Cognitive Limits in Vision-Language Models](https://arxiv.org/abs/2510.02780)
*Prahitha Movva*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型在解决复杂横向思维任务（如字谜拼图）时的认知过程，通过可解释性分析揭示了模型的推理模式和失败原因。


<details>
  <summary>Details</summary>
Motivation: 虽然现有研究表明视觉语言模型在字谜拼图任务上表现不佳，但其底层的推理过程和失败模式尚未得到深入探索。论文旨在填补这一空白，理解模型如何应对复杂的横向思维挑战。

Method: 研究构建了一个包含221个字谜拼图的系统标注数据集，涵盖六个认知类别，并开发了一个评估框架，将推理质量与答案正确性分开评估。同时研究了三种提示策略以激发不同类型的解释过程。

Result: 研究发现推理质量在不同谜题类别间差异显著，模型在视觉组合方面表现出系统性优势，但在缺失解释和文化象征方面存在基本限制。提示策略对认知方法和问题解决效果有显著影响。

Conclusion: 可解释性应被视为模型性能的内在组成部分，而非事后考虑。研究为理解视觉语言模型的认知过程提供了重要见解，揭示了模型在复杂横向思维任务中的系统优势和局限性。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet their
cognitive processes remain opaque on complex lateral thinking challenges like
rebus puzzles. While recent work has demonstrated these models struggle
significantly with rebus puzzle solving, the underlying reasoning processes and
failure patterns remain largely unexplored. We address this gap through a
comprehensive explainability analysis that moves beyond performance metrics to
understand how VLMs approach these complex lateral thinking challenges. Our
study contributes a systematically annotated dataset of 221 rebus puzzles
across six cognitive categories, paired with an evaluation framework that
separates reasoning quality from answer correctness. We investigate three
prompting strategies designed to elicit different types of explanatory
processes and reveal critical insights into VLM cognitive processes. Our
findings demonstrate that reasoning quality varies dramatically across puzzle
categories, with models showing systematic strengths in visual composition
while exhibiting fundamental limitations in absence interpretation and cultural
symbolism. We also discover that prompting strategy substantially influences
both cognitive approach and problem-solving effectiveness, establishing
explainability as an integral component of model performance rather than a
post-hoc consideration.

</details>


### [21] [OTR: Synthesizing Overlay Text Dataset for Text Removal](https://arxiv.org/abs/2510.02787)
*Jan Zdenek,Wataru Shimoda,Kota Yamaguchi*

Main category: cs.CV

TL;DR: 本文提出了一种合成文本移除基准数据集的方法，旨在解决现有数据集的局限性，如地面实况伪影、背景过于简单以及评估指标不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本移除研究主要关注自然图像中的场景文本，但当前数据集在域外泛化或准确评估方面存在局限。广泛使用的基准如SCUT-EnsText存在人工编辑导致的地面实况伪影、背景过于简单以及评估指标无法捕捉生成结果质量的问题。

Method: 引入一种合成文本移除基准的方法，该方法使用对象感知放置和视觉语言模型生成的内容，在复杂背景上渲染文本，确保干净的地面实况和具有挑战性的文本移除场景。

Result: 创建了一个适用于场景文本以外领域的文本移除基准数据集，该数据集可在https://huggingface.co/datasets/cyberagent/OTR获取。

Conclusion: 提出的数据集解决了现有基准的局限性，为文本移除任务提供了更准确和具有挑战性的评估环境。

Abstract: Text removal is a crucial task in computer vision with applications such as
privacy preservation, image editing, and media reuse. While existing research
has primarily focused on scene text removal in natural images, limitations in
current datasets hinder out-of-domain generalization or accurate evaluation. In
particular, widely used benchmarks such as SCUT-EnsText suffer from ground
truth artifacts due to manual editing, overly simplistic text backgrounds, and
evaluation metrics that do not capture the quality of generated results. To
address these issues, we introduce an approach to synthesizing a text removal
benchmark applicable to domains other than scene texts. Our dataset features
text rendered on complex backgrounds using object-aware placement and
vision-language model-generated content, ensuring clean ground truth and
challenging text removal scenarios. The dataset is available at
https://huggingface.co/datasets/cyberagent/OTR .

</details>


### [22] [Align Your Query: Representation Alignment for Multimodality Medical Object Detection](https://arxiv.org/abs/2510.02789)
*Ara Seo,Bryan Sangwoo Kim,Hyungjin Chung,Jong Chul Ye*

Main category: cs.CV

TL;DR: 本文提出了一种用于多模态医学目标检测的表示对齐方法，通过模态令牌和查询对齐技术解决不同医学成像模态间的统计异质性和表示空间不一致问题。


<details>
  <summary>Details</summary>
Motivation: 医学目标检测在单一检测器训练混合医学模态（如CXR、CT、MRI）时表现不佳，主要原因是不同模态间的统计异质性和不相交的表示空间。

Method: 提出MoCA（多模态上下文注意力）机制，通过文本衍生的模态令牌将模态上下文注入到DETR风格的目标查询中；同时引入QueryREPA预训练阶段，使用任务特定的对比目标对齐查询表示与模态令牌。

Result: 该方法在多种医学模态上一致提高了AP（平均精度），且开销极小，无需架构修改。

Conclusion: 该方法为构建稳健的多模态医学目标检测系统提供了一条实用路径，能够有效提升跨模态检测性能。

Abstract: Medical object detection suffers when a single detector is trained on mixed
medical modalities (e.g., CXR, CT, MRI) due to heterogeneous statistics and
disjoint representation spaces. To address this challenge, we turn to
representation alignment, an approach that has proven effective for bringing
features from different sources into a shared space. Specifically, we target
the representations of DETR-style object queries and propose a simple,
detector-agnostic framework to align them with modality context. First, we
define modality tokens: compact, text-derived embeddings encoding imaging
modality that are lightweight and require no extra annotations. We integrate
the modality tokens into the detection process via Multimodality Context
Attention (MoCA), mixing object-query representations via self-attention to
propagate modality context within the query set. This preserves DETR-style
architectures and adds negligible latency while injecting modality cues into
object queries. We further introduce QueryREPA, a short pretraining stage that
aligns query representations to their modality tokens using a task-specific
contrastive objective with modality-balanced batches. Together, MoCA and
QueryREPA produce modality-aware, class-faithful queries that transfer
effectively to downstream training. Across diverse modalities trained
altogether, the proposed approach consistently improves AP with minimal
overhead and no architectural modifications, offering a practical path toward
robust multimodality medical object detection. Project page:
https://araseo.github.io/alignyourquery/.

</details>


### [23] [MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding](https://arxiv.org/abs/2510.02790)
*Jingyuan Deng,Yujiu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种名为MaskCD的方法，通过掩码大型视觉语言模型中的"图像头"来构建对比样本，有效缓解幻觉问题，同时保持模型的通用能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在视觉语言理解方面表现出色，但存在幻觉问题（生成与输入视觉和文本内容矛盾的内容）。现有方法如对比解码和注意力操纵存在对比样本构建困难或稳定性不足的问题。

Method: 提出图像头掩码对比解码（MaskCD）方法，利用LVLMs中的"图像头"，通过掩码这些头来构建对比样本进行对比解码。

Result: 在LLaVA-1.5-7b和Qwen-VL-7b模型上使用CHAIR、POPE、AMBER和MME等基准测试，结果表明MaskCD能有效缓解幻觉现象并保持LVLMs的通用能力。

Conclusion: MaskCD方法为解决LVLMs的幻觉问题提供了一种有效且稳定的解决方案，在多个基准测试中表现出色。

Abstract: Large vision-language models (LVLMs) have shown remarkable performance in
visual-language understanding for downstream multimodal tasks. While their
capabilities are improving, problems emerge simultaneously. Among those
problems, the hallucinations have attracted much attention, which stands for
the phenomenon where LVLMs generate contradictory content to their input visual
and text contents. Many approaches have been proposed to deal with this issue,
such as contrastive decoding and attention manipulation. However, contrastive
decoding methods struggle in constructing appropriate contrastive samples, and
attention manipulation methods are highly sensitive, lacking stability. In this
work, we propose image head Masked Contrastive Decoding (MaskCD). Our approach
utilizes the "image heads" in LVLMs, masking them to construct contrastive
samples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and
Qwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The
results demonstrate that MaskCD effectively alleviates the phenomenon of
hallucinations and retains the general capabilities of LVLMs. Corresponding
resources could be found at: https://github.com/Deng-Jingyuan/MaskCD .

</details>


### [24] [VERNIER: an open-source software pushing marker pose estimation down to the micrometer and nanometer scales](https://arxiv.org/abs/2510.02791)
*Patrick Sandoz,Antoine N. André,Guillaume J. Laurent*

Main category: cs.CV

TL;DR: VERNIER是一个开源相位处理软件，用于基于伪周期模式实现快速可靠的姿态测量，特别适用于纳米级精度的显微应用。


<details>
  <summary>Details</summary>
Motivation: 在微小尺度下进行姿态估计仍然具有挑战性，现有解决方案难以在相对大范围内实现纳米级和微弧度级的分辨率。

Method: 采用基于相位的局部阈值算法，通过相位处理伪周期模式来实现姿态测量，该方法对噪声、散焦和遮挡具有鲁棒性。

Result: 软件在合成和实验图像中验证了有效性，能够实现厘米级范围和纳米级分辨率的姿态测量。

Conclusion: 提供了选择适当模式设计和显微镜放大镜头的指导原则，以满足不同应用需求的性能要求。

Abstract: Pose estimation is still a challenge at the small scales. Few solutions exist
to capture the 6 degrees of freedom of an object with nanometric and
microradians resolutions over relatively large ranges. Over the years, we have
proposed several fiducial marker and pattern designs to achieve reliable
performance for various microscopy applications. Centimeter ranges are possible
using pattern encoding methods, while nanometer resolutions can be achieved
using phase processing of the periodic frames. This paper presents VERNIER, an
open source phase processing software designed to provide fast and reliable
pose measurement based on pseudo-periodic patterns. Thanks to a phase-based
local thresholding algorithm, the software has proven to be particularly robust
to noise, defocus and occlusion. The successive steps of the phase processing
are presented, as well as the different types of patterns that address
different application needs. The implementation procedure is illustrated with
synthetic and experimental images. Finally, guidelines are given for selecting
the appropriate pattern design and microscope magnification lenses as a
function of the desired performance.

</details>


### [25] [Med-K2N: Flexible K-to-N Modality Translation for Medical Image Synthesis](https://arxiv.org/abs/2510.02815)
*Feng Yuan,Yifan Gao,Yuehua Ye,Haoyue Li,Xin Gao*

Main category: cs.CV

TL;DR: 该论文提出Med-K2N方法，用于解决K到N的医学图像生成问题，通过自适应权重学习和因果模态一致性模块，实现了多模态医学图像的高质量合成。


<details>
  <summary>Details</summary>
Motivation: 临床诊断需要灵活的多模态重建，但面临三个关键挑战：不同模态对目标任务的异质贡献建模、融合质量控制和多输出生成中的模态身份一致性维护。

Method: 提出三个协作模块：PreWeightNet用于全局贡献评估，ThresholdNet用于自适应过滤，EffiWeightNet用于有效权重计算；同时提出CMIM模块通过视觉语言建模建立因果约束。

Result: 在多个基准测试上，Med-K2N显著优于现有最先进方法。

Conclusion: 该方法通过质量驱动的选择机制和渐进增强策略，有效解决了多模态医学图像合成的关键挑战。

Abstract: Cross-modal medical image synthesis research focuses on reconstructing
missing imaging modalities from available ones to support clinical diagnosis.
Driven by clinical necessities for flexible modality reconstruction, we explore
K to N medical generation, where three critical challenges emerge: How can we
model the heterogeneous contributions of different modalities to various target
tasks? How can we ensure fusion quality control to prevent degradation from
noisy information? How can we maintain modality identity consistency in
multi-output generation? Driven by these clinical necessities, and drawing
inspiration from SAM2's sequential frame paradigm and clinicians' progressive
workflow of incrementally adding and selectively integrating multi-modal
information, we treat multi-modal medical data as sequential frames with
quality-driven selection mechanisms. Our key idea is to "learn" adaptive
weights for each modality-task pair and "memorize" beneficial fusion patterns
through progressive enhancement. To achieve this, we design three collaborative
modules: PreWeightNet for global contribution assessment, ThresholdNet for
adaptive filtering, and EffiWeightNet for effective weight computation.
Meanwhile, to maintain modality identity consistency, we propose the Causal
Modality Identity Module (CMIM) that establishes causal constraints between
generated images and target modality descriptions using vision-language
modeling. Extensive experimental results demonstrate that our proposed Med-K2N
outperforms state-of-the-art methods by significant margins on multiple
benchmarks. Source code is available.

</details>


### [26] [ELMF4EggQ: Ensemble Learning with Multimodal Feature Fusion for Non-Destructive Egg Quality Assessment](https://arxiv.org/abs/2510.02876)
*Md Zahim Hassan,Md. Osama,Muhammad Ashad Kabir,Md. Saiful Islam,Zannatul Naim*

Main category: cs.CV

TL;DR: ELMF4EggQ是一个集成学习框架，使用多模态特征融合（图像、形状、重量）来非侵入式评估鸡蛋等级和新鲜度，准确率分别达到86.57%和70.83%。


<details>
  <summary>Details</summary>
Motivation: 开发一种准确、非破坏性的鸡蛋质量评估方法，以确保食品安全、维持产品标准和提高商业禽类生产运营效率。

Method: 使用预训练CNN模型提取外部鸡蛋图像特征，结合形状和重量等结构特征，通过PCA降维、SMOTE增强和多种机器学习算法进行分类，最后采用集成投票机制结合最佳分类器的预测结果。

Result: 多模态方法显著优于仅使用图像或仅使用表格数据的基线方法，在等级分类和新鲜度预测方面分别达到86.57%和70.83%的准确率。

Conclusion: 该研究首次将机器学习方法应用于仅使用外部非侵入特征进行内部鸡蛋质量评估，并发布了首个相应的标记数据集，促进了该领域的透明度、可重复性和进一步研究。

Abstract: Accurate, non-destructive assessment of egg quality is critical for ensuring
food safety, maintaining product standards, and operational efficiency in
commercial poultry production. This paper introduces ELMF4EggQ, an ensemble
learning framework that employs multimodal feature fusion to classify egg grade
and freshness using only external attributes - image, shape, and weight. A
novel, publicly available dataset of 186 brown-shelled eggs was constructed,
with egg grade and freshness levels determined through laboratory-based expert
assessments involving internal quality measurements, such as yolk index and
Haugh unit. To the best of our knowledge, this is the first study to apply
machine learning methods for internal egg quality assessment using only
external, non-invasive features, and the first to release a corresponding
labeled dataset. The proposed framework integrates deep features extracted from
external egg images with structural characteristics such as egg shape and
weight, enabling a comprehensive representation of each egg. Image feature
extraction is performed using top-performing pre-trained CNN models (ResNet152,
DenseNet169, and ResNet152V2), followed by PCA-based dimensionality reduction,
SMOTE augmentation, and classification using multiple machine learning
algorithms. An ensemble voting mechanism combines predictions from the
best-performing classifiers to enhance overall accuracy. Experimental results
demonstrate that the multimodal approach significantly outperforms image-only
and tabular (shape and weight) only baselines, with the multimodal ensemble
approach achieving 86.57% accuracy in grade classification and 70.83% in
freshness prediction. All code and data are publicly available at
https://github.com/Kenshin-Keeps/Egg_Quality_Prediction_ELMF4EggQ, promoting
transparency, reproducibility, and further research in this domain.

</details>


### [27] [One Patch to Caption Them All: A Unified Zero-Shot Captioning Framework](https://arxiv.org/abs/2510.02898)
*Lorenzo Bianchi,Giacomo Pacini,Fabio Carrara,Nicola Messina,Giuseppe Amato,Fabrizio Falchi*

Main category: cs.CV

TL;DR: 提出了一个名为Patch-ioner的零样本图像描述框架，从图像中心转向patch中心范式，无需区域级监督即可对任意区域进行描述


<details>
  <summary>Details</summary>
Motivation: 现有的零样本描述模型仅限于全局表示和整图描述，无法处理任意区域。需要一种能够描述从单个patch到非连续区域的方法

Method: 将单个patch作为原子描述单元，通过聚合这些单元来描述任意区域。使用DINO等产生有意义密集视觉特征的骨干网络

Result: 在多个基于区域的描述任务中达到最先进性能，在零样本密集描述、区域集描述和新引入的追踪描述任务中表现优于其他基线模型

Conclusion: patch级别的语义表示对于可扩展的描述生成非常有效，证明了patch中心范式在零样本描述中的优势

Abstract: Zero-shot captioners are recently proposed models that utilize common-space
vision-language representations to caption images without relying on paired
image-text data. To caption an image, they proceed by textually decoding a
text-aligned image feature, but they limit their scope to global
representations and whole-image captions. We present \frameworkName{}, a
unified framework for zero-shot captioning that shifts from an image-centric to
a patch-centric paradigm, enabling the captioning of arbitrary regions without
the need of region-level supervision. Instead of relying on global image
representations, we treat individual patches as atomic captioning units and
aggregate them to describe arbitrary regions, from single patches to
non-contiguous areas and entire images. We analyze the key ingredients that
enable current latent captioners to work in our novel proposed framework.
Experiments demonstrate that backbones producing meaningful, dense visual
features, such as DINO, are key to achieving state-of-the-art performance in
multiple region-based captioning tasks. Compared to other baselines and
state-of-the-art competitors, our models achieve better performance on
zero-shot dense, region-set, and a newly introduced trace captioning task,
highlighting the effectiveness of patch-wise semantic representations for
scalable caption generation. Project page at https://paciosoft.com/Patch-ioner/ .

</details>


### [28] [Training-Free Out-Of-Distribution Segmentation With Foundation Models](https://arxiv.org/abs/2510.02909)
*Laith Nayal,Hadi Salloum,Ahmad Taha,Yaroslav Kholodov,Alexander Gasnikov*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法，利用InternImage骨干网络特征，通过K-Means聚类和置信度阈值来检测语义分割中的未知物体，在多个基准测试中优于有监督和无监督基线。


<details>
  <summary>Details</summary>
Motivation: 大型视觉基础模型在封闭集语义任务中表现出色，但其在语义分割中检测分布外区域的能力尚未充分探索。安全关键应用如自动驾驶需要有效检测未知物体。

Method: 使用在分割数据集上微调的基础模型特征，结合K-Means聚类和原始解码器逻辑的置信度阈值，无需异常监督即可识别分布外区域。

Result: 在RoadAnomaly基准上达到50.02平均精度，在ADE-OoD基准上达到48.77平均精度，超越了多个有监督和无监督基线方法。

Conclusion: 该方法展示了基础模型在分布外分割中的潜力，为需要最少假设或额外数据的通用方法提供了有前景的方向。

Abstract: Detecting unknown objects in semantic segmentation is crucial for
safety-critical applications such as autonomous driving. Large vision
foundation models, including DINOv2, InternImage, and CLIP, have advanced
visual representation learning by providing rich features that generalize well
across diverse tasks. While their strength in closed-set semantic tasks is
established, their capability to detect out-of-distribution (OoD) regions in
semantic segmentation remains underexplored. In this work, we investigate
whether foundation models fine-tuned on segmentation datasets can inherently
distinguish in-distribution (ID) from OoD regions without any outlier
supervision. We propose a simple, training-free approach that utilizes features
from the InternImage backbone and applies K-Means clustering alongside
confidence thresholding on raw decoder logits to identify OoD clusters. Our
method achieves 50.02 Average Precision on the RoadAnomaly benchmark and 48.77
on the benchmark of ADE-OoD with InternImage-L, surpassing several supervised
and unsupervised baselines. These results suggest a promising direction for
generic OoD segmentation methods that require minimal assumptions or additional
data.

</details>


### [29] [Don't Just Chase "Highlighted Tokens" in MLLMs: Revisiting Visual Holistic Context Retention](https://arxiv.org/abs/2510.02912)
*Xin Zou,Di Lu,Yizhou Wang,Yibo Yan,Yuanhuiyi Lyu,Xu Zheng,Linfeng Zhang,Xuming Hu*

Main category: cs.CV

TL;DR: HoloV是一个简单有效的视觉token剪枝框架，通过从整体视角重新思考token保留策略，在不同空间区域自适应分配剪枝预算，在保持性能的同时显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型依赖大量视觉token导致计算开销大，现有基于注意力的剪枝方法在高剪枝率下性能下降明显，因为倾向于保留语义相似的token。

Method: 提出HoloV框架，从整体视角而非注意力优先的角度进行token剪枝，通过在不同空间区域自适应分配剪枝预算，确保保留的token能捕捉全局视觉上下文而非孤立特征。

Result: HoloV在各种任务、MLLM架构和剪枝率下均优于SOTA方法，LLaVA1.5配备HoloV在剪枝88.9%视觉token后仍能保持95.8%的原始性能。

Conclusion: HoloV通过整体视角的剪枝策略实现了优越的效率-准确性权衡，证明从整体而非注意力优先的角度进行token剪枝是更有效的方法。

Abstract: Despite their powerful capabilities, Multimodal Large Language Models (MLLMs)
suffer from considerable computational overhead due to their reliance on
massive visual tokens. Recent studies have explored token pruning to alleviate
this problem, which typically uses text-vision cross-attention or
[\texttt{CLS}] attention to assess and discard redundant visual tokens. In this
work, we identify a critical limitation of such attention-first pruning
approaches, i.e., they tend to preserve semantically similar tokens, resulting
in pronounced performance drops under high pruning ratios. To this end, we
propose {HoloV}, a simple yet effective, plug-and-play visual token pruning
framework for efficient inference. Distinct from previous attention-first
schemes, HoloV rethinks token retention from a holistic perspective. By
adaptively distributing the pruning budget across different spatial crops,
HoloV ensures that the retained tokens capture the global visual context rather
than isolated salient features. This strategy minimizes representational
collapse and maintains task-relevant information even under aggressive pruning.
Experimental results demonstrate that our HoloV achieves superior performance
across various tasks, MLLM architectures, and pruning ratios compared to SOTA
methods. For instance, LLaVA1.5 equipped with HoloV preserves 95.8\% of the
original performance after pruning 88.9\% of visual tokens, achieving superior
efficiency-accuracy trade-offs.

</details>


### [30] [Zero-Shot Robustness of Vision Language Models Via Confidence-Aware Weighting](https://arxiv.org/abs/2510.02913)
*Nikoo Naghavian,Mostafa Tavassolipour*

Main category: cs.CV

TL;DR: 提出Confidence-Aware Weighting (CAW)方法增强视觉语言模型的零样本鲁棒性，包含置信度感知损失和特征对齐正则化两个组件


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型如CLIP具有优秀的零样本泛化能力，但对对抗攻击高度脆弱，需要提升其鲁棒性

Method: CAW方法包含：(1) 置信度感知损失，通过缩放干净和对抗预测之间的KL散度来优先处理不确定的对抗样本；(2) 特征对齐正则化，通过最小化冻结和微调图像编码器在对抗输入上的特征距离来保持语义一致性

Result: 在TinyImageNet和14个额外数据集上的实验表明，CAW在强攻击如AutoAttack下优于PMG-AFT和TGA-ZSR等最新方法，且内存使用更少

Conclusion: CAW方法能够在不牺牲泛化能力的情况下同时提高干净准确率和鲁棒准确率

Abstract: Vision-language models like CLIP demonstrate impressive zero-shot
generalization but remain highly vulnerable to adversarial attacks. In this
work, we propose Confidence-Aware Weighting (CAW) to enhance zero-shot
robustness in vision-language models. CAW consists of two components: (1) a
Confidence-Aware loss that prioritizes uncertain adversarial examples by
scaling the KL divergence between clean and adversarial predictions, and (2) a
feature alignment regularization that preserves semantic consistency by
minimizing the distance between frozen and fine-tuned image encoder features on
adversarial inputs. These components work jointly to improve both clean and
robust accuracy without sacrificing generalization. Extensive experiments on
TinyImageNet and 14 additional datasets show that CAW outperforms recent
methods such as PMG-AFT and TGA-ZSR under strong attacks like AutoAttack, while
using less memory.

</details>


### [31] [Multimodal Carotid Risk Stratification with Large Vision-Language Models: Benchmarking, Fine-Tuning, and Clinical Insights](https://arxiv.org/abs/2510.02922)
*Daphne Tsolissou,Theofanis Ganitidis,Konstantinos Mitsis,Stergios CHristodoulidis,Maria Vakalopoulou,Konstantina Nikita*

Main category: cs.CV

TL;DR: 该研究评估了大型视觉语言模型在颈动脉粥样硬化疾病风险评估中的应用，发现现有模型在识别成像模式和风险分类方面存在局限，通过LoRA适应超声域后性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 颈动脉粥样硬化疾病的风险评估需要整合多样化的临床和影像信息，但现有方法缺乏透明性和可解释性。本研究旨在探索大型视觉语言模型在多模态颈动脉斑块评估中的潜力。

Method: 提出了一个通过访谈式问题序列模拟真实诊断场景的框架，比较了多种开源LVLM模型。使用低秩适应（LoRA）将LLaVa-NeXT-Vicuna模型适应到超声领域，并整合多模态表格数据。

Result: 零样本实验显示现有LVLM在识别成像模式和风险分类方面表现不佳。经过LoRA适应后，卒中风险分层性能显著改善，多模态数据整合进一步提高了特异性和平衡准确率，性能与之前在同一数据集上训练的CNN基线相当。

Conclusion: 研究强调了LVLM在超声心血管风险预测中的潜力和局限性，表明多模态整合、模型校准和领域适应对临床转化至关重要。

Abstract: Reliable risk assessment for carotid atheromatous disease remains a major
clinical challenge, as it requires integrating diverse clinical and imaging
information in a manner that is transparent and interpretable to clinicians.
This study investigates the potential of state-of-the-art and recent large
vision-language models (LVLMs) for multimodal carotid plaque assessment by
integrating ultrasound imaging (USI) with structured clinical, demographic,
laboratory, and protein biomarker data. A framework that simulates realistic
diagnostic scenarios through interview-style question sequences is proposed,
comparing a range of open-source LVLMs, including both general-purpose and
medically tuned models. Zero-shot experiments reveal that even if they are very
powerful, not all LVLMs can accurately identify imaging modality and anatomy,
while all of them perform poorly in accurate risk classification. To address
this limitation, LLaVa-NeXT-Vicuna is adapted to the ultrasound domain using
low-rank adaptation (LoRA), resulting in substantial improvements in stroke
risk stratification. The integration of multimodal tabular data in the form of
text further enhances specificity and balanced accuracy, yielding competitive
performance compared to prior convolutional neural network (CNN) baselines
trained on the same dataset. Our findings highlight both the promise and
limitations of LVLMs in ultrasound-based cardiovascular risk prediction,
underscoring the importance of multimodal integration, model calibration, and
domain adaptation for clinical translation.

</details>


### [32] [Flip Distribution Alignment VAE for Multi-Phase MRI Synthesis](https://arxiv.org/abs/2510.02970)
*Xiaoyan Kui,Qianmu Xiao,Qqinsong Li,Zexin Ji,JIelin Zhang,Beiji Zou*

Main category: cs.CV

TL;DR: 提出了FDA-VAE，一种轻量级特征解耦的变分自编码器模型，用于多期相增强MRI合成，通过对称的潜在分布分离共享和独立特征，显著减少参数和推理时间。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用深度自编码器生成器参数效率低，缺乏可解释的训练策略，无法有效分离多期相增强MRI中的共享和独立特征。

Method: FDA-VAE将输入和目标图像编码为关于标准正态分布对称的两个潜在分布，使用Y形双向训练策略增强特征分离的可解释性。

Result: 相比现有深度自编码器端到端合成方法，FDA-VAE显著减少模型参数和推理时间，同时有效提高合成质量。

Conclusion: FDA-VAE是一种高效且可解释的多期相增强MRI合成方法，在减少计算成本的同时提升了合成性能。

Abstract: Separating shared and independent features is crucial for multi-phase
contrast-enhanced (CE) MRI synthesis. However, existing methods use deep
autoencoder generators with low parameter efficiency and lack interpretable
training strategies. In this paper, we propose Flip Distribution Alignment
Variational Autoencoder (FDA-VAE), a lightweight feature-decoupled VAE model
for multi-phase CE MRI synthesis. Our method encodes input and target images
into two latent distributions that are symmetric concerning a standard normal
distribution, effectively separating shared and independent features. The
Y-shaped bidirectional training strategy further enhances the interpretability
of feature separation. Experimental results show that compared to existing deep
autoencoder-based end-to-end synthesis methods, FDA-VAE significantly reduces
model parameters and inference time while effectively improving synthesis
quality. The source code is publicly available at
https://github.com/QianMuXiao/FDA-VAE.

</details>


### [33] [TIT-Score: Evaluating Long-Prompt Based Text-to-Image Alignment via Text-to-Image-to-Text Consistency](https://arxiv.org/abs/2510.02987)
*Juntong Wang,Huiyu Duan,Jiarui Wang,Ziheng Jia,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 本文提出了LPG-Bench基准测试和TIT评估方法，用于解决大型多模态模型在长提示文本到图像生成中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像模型在短提示下表现良好，但在理解和遵循长而详细的提示时存在困难，生成结果不一致。现有评估指标与人类偏好的一致性较差。

Method: 1) 构建LPG-Bench基准：包含200个平均长度超过250词的长提示；2) 提出TIT评估框架：通过文本-图像-文本一致性量化对齐度，包括TIT-Score和TIT-Score-LLM两种实现。

Result: 实验表明TIT框架与人类判断的一致性优于现有方法，TIT-Score-LLM在成对准确率上比最强基线提升7.31%。

Conclusion: LPG-Bench和TIT方法为T2I模型的基准测试和发展提供了新的视角，所有资源将公开。

Abstract: With the rapid advancement of large multimodal models (LMMs), recent
text-to-image (T2I) models can generate high-quality images and demonstrate
great alignment to short prompts. However, they still struggle to effectively
understand and follow long and detailed prompts, displaying inconsistent
generation. To address this challenge, we introduce LPG-Bench, a comprehensive
benchmark for evaluating long-prompt-based text-to-image generation. LPG-Bench
features 200 meticulously crafted prompts with an average length of over 250
words, approaching the input capacity of several leading commercial models.
Using these prompts, we generate 2,600 images from 13 state-of-the-art models
and further perform comprehensive human-ranked annotations. Based on LPG-Bench,
we observe that state-of-the-art T2I alignment evaluation metrics exhibit poor
consistency with human preferences on long-prompt-based image generation. To
address the gap, we introduce a novel zero-shot metric based on
text-to-image-to-text consistency, termed TIT, for evaluating
long-prompt-generated images. The core concept of TIT is to quantify T2I
alignment by directly comparing the consistency between the raw prompt and the
LMM-produced description on the generated image, which includes an efficient
score-based instantiation TIT-Score and a large-language-model (LLM) based
instantiation TIT-Score-LLM. Extensive experiments demonstrate that our
framework achieves superior alignment with human judgment compared to
CLIP-score, LMM-score, etc., with TIT-Score-LLM attaining a 7.31% absolute
improvement in pairwise accuracy over the strongest baseline. LPG-Bench and TIT
methods together offer a deeper perspective to benchmark and foster the
development of T2I models. All resources will be made publicly available.

</details>


### [34] [Towards Scalable and Consistent 3D Editing](https://arxiv.org/abs/2510.02994)
*Ruihao Xia,Yang Tang,Pan Zhou*

Main category: cs.CV

TL;DR: 本文提出了3DEditFormer框架，通过构建最大的3D编辑基准数据集3DEditVerse和提出无需3D掩码的3D结构保持条件变换器，解决了3D编辑中的跨视图一致性、结构保真度和精细控制等挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3D编辑方法存在速度慢、几何失真、依赖手动3D掩码等问题，难以实现实用且可扩展的3D编辑。

Method: 提出3DEditFormer框架：1）构建3DEditVerse数据集（116,309个训练对和1,500个测试对）；2）设计3D结构保持条件变换器，通过双引导注意力和时间自适应门控实现可编辑区域与保留结构的解耦。

Result: 实验表明，该框架在定量和定性评估上均优于现有最先进方法，为实用且可扩展的3D编辑建立了新标准。

Conclusion: 3DEditFormer通过数据和模型两方面的创新，实现了无需辅助3D掩码的精确一致3D编辑，解决了3D编辑领域的关键挑战。

Abstract: 3D editing - the task of locally modifying the geometry or appearance of a 3D
asset - has wide applications in immersive content creation, digital
entertainment, and AR/VR. However, unlike 2D editing, it remains challenging
due to the need for cross-view consistency, structural fidelity, and
fine-grained controllability. Existing approaches are often slow, prone to
geometric distortions, or dependent on manual and accurate 3D masks that are
error-prone and impractical. To address these challenges, we advance both the
data and model fronts. On the data side, we introduce 3DEditVerse, the largest
paired 3D editing benchmark to date, comprising 116,309 high-quality training
pairs and 1,500 curated test pairs. Built through complementary pipelines of
pose-driven geometric edits and foundation model-guided appearance edits,
3DEditVerse ensures edit locality, multi-view consistency, and semantic
alignment. On the model side, we propose 3DEditFormer, a
3D-structure-preserving conditional transformer. By enhancing image-to-3D
generation with dual-guidance attention and time-adaptive gating, 3DEditFormer
disentangles editable regions from preserved structure, enabling precise and
consistent edits without requiring auxiliary 3D masks. Extensive experiments
demonstrate that our framework outperforms state-of-the-art baselines both
quantitatively and qualitatively, establishing a new standard for practical and
scalable 3D editing. Dataset and code will be released. Project:
https://www.lv-lab.org/3DEditFormer/

</details>


### [35] [Not every day is a sunny day: Synthetic cloud injection for deep land cover segmentation robustness evaluation across data sources](https://arxiv.org/abs/2510.03006)
*Sara Mobsite,Renaud Hostache,Laure Berti Equille,Emmanuel Roux,Joris Guerin*

Main category: cs.CV

TL;DR: 提出了一种用于改善热带地区云层覆盖下土地覆盖语义分割的方法，结合云层注入算法和归一化差异指数注入技术，利用Sentinel-1雷达数据填补光学图像的空缺。


<details>
  <summary>Details</summary>
Motivation: 现有Sentinel-2数据集多为无云图像，在热带云层常见地区实用性有限；深度学习编码器下采样过程中会丢失空间和光谱细节。

Method: 开发云层注入算法模拟真实云层覆盖；提出轻量级方法在解码层注入归一化差异指数(NDIs)以保留关键空间特征；结合Sentinel-1雷达数据与光学数据进行融合。

Result: 在DFC2020数据集上，NDIs注入使U-Net性能提升1.99%，DeepLabV3提升2.78%；在云层覆盖条件下，结合Sentinel-1数据相比单独使用光学数据有显著性能提升。

Conclusion: 雷达-光学融合在挑战性大气条件下具有显著效果，提出的方法能有效改善热带云层地区的土地覆盖分割性能。

Abstract: Supervised deep learning for land cover semantic segmentation (LCS) relies on
labeled satellite data. However, most existing Sentinel-2 datasets are
cloud-free, which limits their usefulness in tropical regions where clouds are
common. To properly evaluate the extent of this problem, we developed a cloud
injection algorithm that simulates realistic cloud cover, allowing us to test
how Sentinel-1 radar data can fill in the gaps caused by cloud-obstructed
optical imagery. We also tackle the issue of losing spatial and/or spectral
details during encoder downsampling in deep networks. To mitigate this loss, we
propose a lightweight method that injects Normalized Difference Indices (NDIs)
into the final decoding layers, enabling the model to retain key spatial
features with minimal additional computation. Injecting NDIs enhanced land
cover segmentation performance on the DFC2020 dataset, yielding improvements of
1.99% for U-Net and 2.78% for DeepLabV3 on cloud-free imagery. Under
cloud-covered conditions, incorporating Sentinel-1 data led to significant
performance gains across all models compared to using optical data alone,
highlighting the effectiveness of radar-optical fusion in challenging
atmospheric scenarios.

</details>


### [36] [PocketSR: The Super-Resolution Expert in Your Pocket Mobiles](https://arxiv.org/abs/2510.03012)
*Haoze Sun,Linfeng Jiang,Fan Li,Renjing Pei,Zhixin Wang,Yong Guo,Jiaqi Xu,Haoyu Chen,Jin Han,Fenglong Song,Yujiu Yang,Wenbo Li*

Main category: cs.CV

TL;DR: PocketSR是一种超轻量级的单步图像超分辨率模型，专为边缘设备设计，能够在保持高保真度的同时大幅降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大型生成模型的RealSR方法虽然效果出色，但计算成本高、延迟大，不适合边缘设备部署。

Method: 设计了LiteED作为高效VAE替代方案，减少97.5%参数；提出在线退火剪枝技术将生成先验从重模块转移到轻量模块；使用多层特征蒸馏损失来缓解剪枝过程中的知识损失。

Result: PocketSR仅146M参数，处理4K图像仅需0.8秒，性能与最先进的单步和多步RealSR模型相当。

Conclusion: PocketSR为边缘设备提供了一种高效实用的RealSR解决方案，并为未来研究提供了有价值的见解。

Abstract: Real-world image super-resolution (RealSR) aims to enhance the visual quality
of in-the-wild images, such as those captured by mobile phones. While existing
methods leveraging large generative models demonstrate impressive results, the
high computational cost and latency make them impractical for edge deployment.
In this paper, we introduce PocketSR, an ultra-lightweight, single-step model
that brings generative modeling capabilities to RealSR while maintaining high
fidelity. To achieve this, we design LiteED, a highly efficient alternative to
the original computationally intensive VAE in SD, reducing parameters by 97.5%
while preserving high-quality encoding and decoding. Additionally, we propose
online annealing pruning for the U-Net, which progressively shifts generative
priors from heavy modules to lightweight counterparts, ensuring effective
knowledge transfer and further optimizing efficiency. To mitigate the loss of
prior knowledge during pruning, we incorporate a multi-layer feature
distillation loss. Through an in-depth analysis of each design component, we
provide valuable insights for future research. PocketSR, with a model size of
146M parameters, processes 4K images in just 0.8 seconds, achieving a
remarkable speedup over previous methods. Notably, it delivers performance on
par with state-of-the-art single-step and even multi-step RealSR models, making
it a highly practical solution for edge-device applications.

</details>


### [37] [When and Where do Events Switch in Multi-Event Video Generation?](https://arxiv.org/abs/2510.03049)
*Ruotong Liao,Guowen Huang,Qing Cheng,Thomas Seidl,Daniel Cremers,Volker Tresp*

Main category: cs.CV

TL;DR: 论文提出MEve评估套件，系统研究多事件文本到视频生成中的事件转换控制问题，发现早期干预去噪步骤和分块模型层对多事件视频生成至关重要。


<details>
  <summary>Details</summary>
Motivation: 现有方法在扩展到多事件生成时忽略了事件转换的内在因素，本文旨在回答多事件提示何时何地控制事件转换这一核心问题。

Method: 引入MEve自构建提示套件，对OpenSora和CogVideoX两个代表性模型家族进行系统研究，通过大量实验分析去噪步骤和模型层的影响。

Result: 实验表明早期干预去噪步骤和分块模型层是多事件视频生成的关键因素，揭示了未来模型进行多事件调节的可能性。

Conclusion: 多事件提示在生成过程中对事件转换的控制时机和位置是影响生成质量的核心因素，为未来多事件条件化模型提供了重要启示。

Abstract: Text-to-video (T2V) generation has surged in response to challenging
questions, especially when a long video must depict multiple sequential events
with temporal coherence and controllable content. Existing methods that extend
to multi-event generation omit an inspection of the intrinsic factor in event
shifting. The paper aims to answer the central question: When and where
multi-event prompts control event transition during T2V generation. This work
introduces MEve, a self-curated prompt suite for evaluating multi-event
text-to-video (T2V) generation, and conducts a systematic study of two
representative model families, i.e., OpenSora and CogVideoX. Extensive
experiments demonstrate the importance of early intervention in denoising steps
and block-wise model layers, revealing the essential factor for multi-event
video generation and highlighting the possibilities for multi-event
conditioning in future models.

</details>


### [38] [InsideOut: An EfficientNetV2-S Based Deep Learning Framework for Robust Multi-Class Facial Emotion Recognition](https://arxiv.org/abs/2510.03066)
*Ahsan Farabi,Israt Khandaker,Ibrahim Khalil Shanto,Md Abdul Ahad Minhaz,Tanisha Zaman*

Main category: cs.CV

TL;DR: InsideOut是一个基于EfficientNetV2-S的可复现面部表情识别框架，通过迁移学习、数据增强和类别不平衡优化，在FER2013数据集上达到62.8%准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在多个领域有重要应用，但存在遮挡、光照变化、姿态变化、类内差异细微以及数据集不平衡等问题，特别是少数类别表情识别困难。

Method: 使用EfficientNetV2-S架构，结合迁移学习、强数据增强和分层划分策略，采用类别加权损失函数处理数据不平衡问题，微调轻量级分类头。

Result: 在FER2013数据集上达到62.8%的准确率和0.590的宏平均F1分数，与传统CNN基线相比具有竞争力。

Conclusion: 研究表明，高效架构结合针对性的不平衡处理策略，能够提供实用、透明且可复现的面部表情识别解决方案。

Abstract: Facial Emotion Recognition (FER) is a key task in affective computing,
enabling applications in human-computer interaction, e-learning, healthcare,
and safety systems. Despite advances in deep learning, FER remains challenging
due to occlusions, illumination and pose variations, subtle intra-class
differences, and dataset imbalance that hinders recognition of minority
emotions. We present InsideOut, a reproducible FER framework built on
EfficientNetV2-S with transfer learning, strong data augmentation, and
imbalance-aware optimization. The approach standardizes FER2013 images, applies
stratified splitting and augmentation, and fine-tunes a lightweight
classification head with class-weighted loss to address skewed distributions.
InsideOut achieves 62.8% accuracy with a macro averaged F1 of 0.590 on FER2013,
showing competitive results compared to conventional CNN baselines. The novelty
lies in demonstrating that efficient architectures, combined with tailored
imbalance handling, can provide practical, transparent, and reproducible FER
solutions.

</details>


### [39] [What Drives Compositional Generalization in Visual Generative Models?](https://arxiv.org/abs/2510.03075)
*Karim Farid,Rajat Sahay,Yumna Ali Alnaggar,Simon Schrodi,Volker Fischer,Cordelia Schmid,Thomas Brox*

Main category: cs.CV

TL;DR: 该论文系统研究了视觉生成模型中影响组合泛化的设计选择，发现训练目标的离散/连续分布特性以及条件信息的提供程度是关键因素，并提出通过结合连续JEPA目标来改进MaskGIT等离散模型的组合性能。


<details>
  <summary>Details</summary>
Motivation: 组合泛化是视觉生成模型的关键能力，但目前对其促进或抑制机制的理解尚不充分。本文旨在系统研究不同设计选择如何影响图像和视频生成中的组合泛化能力。

Method: 通过受控实验分析设计选择的影响，重点关注训练目标的离散/连续分布特性以及条件信息在训练期间提供概念信息的方式。基于这些发现，提出在MaskGIT离散损失基础上加入连续JEPA目标来改进组合性能。

Result: 识别出两个关键影响因素：训练目标的离散/连续分布特性，以及条件信息在训练期间提供概念信息的程度。实验表明，通过放松MaskGIT离散损失并加入连续JEPA目标可以改善离散模型的组合性能。

Conclusion: 组合泛化受训练目标的分布特性和条件信息提供方式的影响，结合连续目标可以改进离散模型的组合能力，这为设计更好的视觉生成模型提供了指导。

Abstract: Compositional generalization, the ability to generate novel combinations of
known concepts, is a key ingredient for visual generative models. Yet, not all
mechanisms that enable or inhibit it are fully understood. In this work, we
conduct a systematic study of how various design choices influence
compositional generalization in image and video generation in a positive or
negative way. Through controlled experiments, we identify two key factors: (i)
whether the training objective operates on a discrete or continuous
distribution, and (ii) to what extent conditioning provides information about
the constituent concepts during training. Building on these insights, we show
that relaxing the MaskGIT discrete loss with an auxiliary continuous JEPA-based
objective can improve compositional performance in discrete models like
MaskGIT.

</details>


### [40] [Latent Diffusion Unlearning: Protecting Against Unauthorized Personalization Through Trajectory Shifted Perturbations](https://arxiv.org/abs/2510.03089)
*Naresh Kumar Devulapally,Shruti Agarwal,Tejas Gokhale,Vishnu Suresh Lokhande*

Main category: cs.CV

TL;DR: 提出了一种基于潜在扩散模型的图像扰动方法，通过在潜在空间中交替去噪和反转来生成不可学习的训练样本，保护数据隐私和防止未经授权的模型个性化。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型在个性化方面表现出色，但引发了数据隐私、知识产权保护和未经授权使用的担忧。现有方法在像素空间中操作，导致图像存在噪声和伪影，视觉保真度有限。

Method: 提出了一种基于模型的扰动策略，在扩散模型的潜在空间中操作。该方法交替进行去噪和反转，同时修改去噪轨迹的起点，确保扰动图像保持高视觉保真度，同时对下游生成模型的反转和个性化具有抵抗力。

Result: 在四个基准数据集上的验证表明，该方法在不可感知性方面显著改善（PSNR、SSIM和FID等感知指标提升约8%-10%），在五个对抗设置下平均鲁棒性提升约10%。

Conclusion: 该方法将不可学习性集成到潜在扩散模型框架中，提供了一种实用且不可感知的防御机制，有效保护敏感数据免受未经授权的模型适应。

Abstract: Text-to-image diffusion models have demonstrated remarkable effectiveness in
rapid and high-fidelity personalization, even when provided with only a few
user images. However, the effectiveness of personalization techniques has lead
to concerns regarding data privacy, intellectual property protection, and
unauthorized usage. To mitigate such unauthorized usage and model replication,
the idea of generating ``unlearnable'' training samples utilizing image
poisoning techniques has emerged. Existing methods for this have limited
imperceptibility as they operate in the pixel space which results in images
with noise and artifacts. In this work, we propose a novel model-based
perturbation strategy that operates within the latent space of diffusion
models. Our method alternates between denoising and inversion while modifying
the starting point of the denoising trajectory: of diffusion models. This
trajectory-shifted sampling ensures that the perturbed images maintain high
visual fidelity to the original inputs while being resistant to inversion and
personalization by downstream generative models. This approach integrates
unlearnability into the framework of Latent Diffusion Models (LDMs), enabling a
practical and imperceptible defense against unauthorized model adaptation. We
validate our approach on four benchmark datasets to demonstrate robustness
against state-of-the-art inversion attacks. Results demonstrate that our method
achieves significant improvements in imperceptibility ($\sim 8 \% -10\%$ on
perceptual metrics including PSNR, SSIM, and FID) and robustness ( $\sim 10\%$
on average across five adversarial settings), highlighting its effectiveness in
safeguarding sensitive data.

</details>


### [41] [Geometry Meets Vision: Revisiting Pretrained Semantics in Distilled Fields](https://arxiv.org/abs/2510.03104)
*Zhiting Mei,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.CV

TL;DR: 该论文探讨了在辐射场语义蒸馏中几何基础特征与纯视觉特征的效果对比，发现几何基础特征虽然包含更多几何细节，但在姿态估计等任务中反而表现更差，而纯视觉特征在更广泛的下游任务中更具优势。


<details>
  <summary>Details</summary>
Motivation: 研究几何基础语义特征在蒸馏辐射场中的潜在优势，特别是在空间任务如姿态估计中是否比纯视觉特征更有效。

Method: 提出SPINE框架，包含基于蒸馏语义的粗反演和基于光度优化的精细反演两个核心组件，用于无初始猜测的辐射场反演。

Result: 几何基础特征包含更精细的结构细节，但在语义目标定位任务中无显著差异，在姿态估计任务中准确率反而下降。

Conclusion: 纯视觉特征在更广泛的下游任务中更具优势，未来需要研究更有效的几何基础策略来增强预训练语义特征的通用性和性能。

Abstract: Semantic distillation in radiance fields has spurred significant advances in
open-vocabulary robot policies, e.g., in manipulation and navigation, founded
on pretrained semantics from large vision models. While prior work has
demonstrated the effectiveness of visual-only semantic features (e.g., DINO and
CLIP) in Gaussian Splatting and neural radiance fields, the potential benefit
of geometry-grounding in distilled fields remains an open question. In
principle, visual-geometry features seem very promising for spatial tasks such
as pose estimation, prompting the question: Do geometry-grounded semantic
features offer an edge in distilled fields? Specifically, we ask three critical
questions: First, does spatial-grounding produce higher-fidelity geometry-aware
semantic features? We find that image features from geometry-grounded backbones
contain finer structural details compared to their counterparts. Secondly, does
geometry-grounding improve semantic object localization? We observe no
significant difference in this task. Thirdly, does geometry-grounding enable
higher-accuracy radiance field inversion? Given the limitations of prior work
and their lack of semantics integration, we propose a novel framework SPINE for
inverting radiance fields without an initial guess, consisting of two core
components: coarse inversion using distilled semantics, and fine inversion
using photometric-based optimization. Surprisingly, we find that the pose
estimation accuracy decreases with geometry-grounded features. Our results
suggest that visual-only features offer greater versatility for a broader range
of downstream tasks, although geometry-grounded features contain more geometric
detail. Notably, our findings underscore the necessity of future research on
effective strategies for geometry-grounding that augment the versatility and
performance of pretrained semantic features.

</details>


### [42] [GeoComplete: Geometry-Aware Diffusion for Reference-Driven Image Completion](https://arxiv.org/abs/2510.03110)
*Beibei Lin,Tingting Chen,Robby T. Tan*

Main category: cs.CV

TL;DR: GeoComplete是一个新颖的参考驱动图像补全框架，通过引入显式3D结构指导来增强几何一致性，相比现有方法在PSNR指标上提升了17.1。


<details>
  <summary>Details</summary>
Motivation: 现有的生成方法仅依赖扩散先验，缺乏几何线索（如相机位姿或深度），导致补全内容错位或不合理。需要解决目标视图与参考图像差异较大时的几何一致性问题。

Method: 采用双分支扩散架构：一个分支从掩码目标合成缺失区域，另一个分支从投影点云提取几何特征。通过联合自注意力机制确保一致性，并引入目标感知掩码策略来指导模型关注相关参考线索。

Result: 实验表明GeoComplete在PSNR指标上比最先进方法提升了17.1，显著提高了几何准确性同时保持高视觉质量。

Conclusion: 通过整合几何感知的双分支扩散架构和目标感知掩码策略，GeoComplete为几何条件图像补全提供了统一且鲁棒的解决方案。

Abstract: Reference-driven image completion, which restores missing regions in a target
view using additional images, is particularly challenging when the target view
differs significantly from the references. Existing generative methods rely
solely on diffusion priors and, without geometric cues such as camera pose or
depth, often produce misaligned or implausible content. We propose GeoComplete,
a novel framework that incorporates explicit 3D structural guidance to enforce
geometric consistency in the completed regions, setting it apart from prior
image-only approaches. GeoComplete introduces two key ideas: conditioning the
diffusion process on projected point clouds to infuse geometric information,
and applying target-aware masking to guide the model toward relevant reference
cues. The framework features a dual-branch diffusion architecture. One branch
synthesizes the missing regions from the masked target, while the other
extracts geometric features from the projected point cloud. Joint
self-attention across branches ensures coherent and accurate completion. To
address regions visible in references but absent in the target, we project the
target view into each reference to detect occluded areas, which are then masked
during training. This target-aware masking directs the model to focus on useful
cues, enhancing performance in difficult scenarios. By integrating a
geometry-aware dual-branch diffusion architecture with a target-aware masking
strategy, GeoComplete offers a unified and robust solution for
geometry-conditioned image completion. Experiments show that GeoComplete
achieves a 17.1 PSNR improvement over state-of-the-art methods, significantly
boosting geometric accuracy while maintaining high visual quality.

</details>


### [43] [Taming Text-to-Sounding Video Generation via Advanced Modality Condition and Interaction](https://arxiv.org/abs/2510.03117)
*Kaisi Guan,Xihua Wang,Zhengfeng Lai,Xin Cheng,Peng Zhang,XiaoJiang Liu,Ruihua Song,Meng Cao*

Main category: cs.CV

TL;DR: 本文提出了一个解决文本到声音视频生成（T2SV）任务的新方法，通过分层视觉基础字幕生成（HVGC）框架和BridgeDiT双塔扩散变换器，解决了模态干扰和跨模态特征交互问题，在多个基准数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到声音视频生成方法面临两个关键挑战：单一共享文本标题导致模态干扰，以及跨模态特征交互机制不明确。这些限制影响了生成的视频和音频的同步性和质量。

Method: 提出了HVGC框架生成解耦的视频和音频字幕对，消除条件阶段的干扰。基于此开发了BridgeDiT双塔扩散变换器，采用双交叉注意力机制实现对称双向信息交换，确保语义和时间同步。

Result: 在三个基准数据集上的广泛实验和人类评估表明，该方法在大多数指标上达到了最先进的性能。全面的消融研究验证了所提贡献的有效性。

Conclusion: 该方法为T2SV任务提供了有效的解决方案，通过解耦字幕和创新的跨模态交互机制，显著提升了生成质量，为未来研究提供了重要见解。所有代码和检查点将公开。

Abstract: This study focuses on a challenging yet promising task,
Text-to-Sounding-Video (T2SV) generation, which aims to generate a video with
synchronized audio from text conditions, meanwhile ensuring both modalities are
aligned with text. Despite progress in joint audio-video training, two critical
challenges still remain unaddressed: (1) a single, shared text caption where
the text for video is equal to the text for audio often creates modal
interference, confusing the pretrained backbones, and (2) the optimal mechanism
for cross-modal feature interaction remains unclear. To address these
challenges, we first propose the Hierarchical Visual-Grounded Captioning (HVGC)
framework that generates pairs of disentangled captions, a video caption, and
an audio caption, eliminating interference at the conditioning stage. Based on
HVGC, we further introduce BridgeDiT, a novel dual-tower diffusion transformer,
which employs a Dual CrossAttention (DCA) mechanism that acts as a robust
``bridge" to enable a symmetric, bidirectional exchange of information,
achieving both semantic and temporal synchronization. Extensive experiments on
three benchmark datasets, supported by human evaluations, demonstrate that our
method achieves state-of-the-art results on most metrics. Comprehensive
ablation studies further validate the effectiveness of our contributions,
offering key insights for the future T2SV task. All the codes and checkpoints
will be publicly released.

</details>


### [44] [HAVIR: HierArchical Vision to Image Reconstruction using CLIP-Guided Versatile Diffusion](https://arxiv.org/abs/2510.03122)
*Shiyi Zhang,Dong Liang,Hairong Zheng,Yihang Zhou*

Main category: cs.CV

TL;DR: HAVIR模型通过分离视觉皮层的两个层次区域提取不同特征，结合结构生成器和语义提取器，利用Versatile Diffusion模型合成图像，在复杂场景下提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在恢复高度复杂视觉刺激时面临挑战，主要因为自然场景中低层特征异质性高，高层特征因上下文重叠而语义纠缠。

Method: HAVIR模型将视觉皮层分为两个层次区域：结构生成器从空间处理体素提取结构信息并转换为潜在扩散先验，语义提取器将语义处理体素转换为CLIP嵌入，通过Versatile Diffusion模型集成合成最终图像。

Result: 实验结果表明HAVIR在复杂场景下提升了重建的结构和语义质量，优于现有模型。

Conclusion: 基于视觉皮层层次表示理论提出的HAVIR模型有效解决了复杂视觉刺激重建的挑战，为神经科学和计算机视觉的跨学科整合提供了新方法。

Abstract: The reconstruction of visual information from brain activity fosters
interdisciplinary integration between neuroscience and computer vision.
However, existing methods still face challenges in accurately recovering highly
complex visual stimuli. This difficulty stems from the characteristics of
natural scenes: low-level features exhibit heterogeneity, while high-level
features show semantic entanglement due to contextual overlaps. Inspired by the
hierarchical representation theory of the visual cortex, we propose the HAVIR
model, which separates the visual cortex into two hierarchical regions and
extracts distinct features from each. Specifically, the Structural Generator
extracts structural information from spatial processing voxels and converts it
into latent diffusion priors, while the Semantic Extractor converts semantic
processing voxels into CLIP embeddings. These components are integrated via the
Versatile Diffusion model to synthesize the final image. Experimental results
demonstrate that HAVIR enhances both the structural and semantic quality of
reconstructions, even in complex scenes, and outperforms existing models.

</details>


### [45] [Mask2IV: Interaction-Centric Video Generation via Mask Trajectories](https://arxiv.org/abs/2510.03135)
*Gen Li,Bo Zhao,Jianfei Yang,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: Mask2IV是一个用于生成交互中心视频的两阶段框架，无需密集掩码输入，通过预测运动轨迹实现高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以建模复杂动态交互，而获取精确掩码注释在现实应用中具有挑战性。

Method: 采用解耦的两阶段流程：先预测执行者和对象的运动轨迹，再基于轨迹生成视频。支持通过动作描述或空间位置线索控制运动轨迹。

Result: 在两个涵盖人-物交互和机器人操作场景的基准测试中，该方法在视觉真实性和可控性方面优于现有基线。

Conclusion: Mask2IV框架有效解决了交互中心视频生成的挑战，提供了灵活的控制方式，在真实性和可控性方面表现优异。

Abstract: Generating interaction-centric videos, such as those depicting humans or
robots interacting with objects, is crucial for embodied intelligence, as they
provide rich and diverse visual priors for robot learning, manipulation policy
training, and affordance reasoning. However, existing methods often struggle to
model such complex and dynamic interactions. While recent studies show that
masks can serve as effective control signals and enhance generation quality,
obtaining dense and precise mask annotations remains a major challenge for
real-world use. To overcome this limitation, we introduce Mask2IV, a novel
framework specifically designed for interaction-centric video generation. It
adopts a decoupled two-stage pipeline that first predicts plausible motion
trajectories for both actor and object, then generates a video conditioned on
these trajectories. This design eliminates the need for dense mask inputs from
users while preserving the flexibility to manipulate the interaction process.
Furthermore, Mask2IV supports versatile and intuitive control, allowing users
to specify the target object of interaction and guide the motion trajectory
through action descriptions or spatial position cues. To support systematic
training and evaluation, we curate two benchmarks covering diverse action and
object categories across both human-object interaction and robotic manipulation
scenarios. Extensive experiments demonstrate that our method achieves superior
visual realism and controllability compared to existing baselines.

</details>


### [46] [ReeMark: Reeb Graphs for Simulating Patterns of Life in Spatiotemporal Trajectories](https://arxiv.org/abs/2510.03152)
*Anantajit Subrahmanya,Chandrakanth Gudavalli,Connor Levenson,Umang Garg,B. S. Manjunath*

Main category: cs.CV

TL;DR: 提出Markovian Reeb Graphs框架，用于模拟保持基线数据中生活模式（PoLs）的时空轨迹，在人口和个体层面结合移动结构，生成既一致又变化的现实轨迹


<details>
  <summary>Details</summary>
Motivation: 准确建模人类移动性对城市规划、流行病学和交通管理至关重要，需要能够生成保持生活模式的现实轨迹

Method: 结合个体和人口层面的移动结构，在概率拓扑模型中构建Markovian Reeb Graphs框架，学习并模拟时空轨迹

Result: 在Urban Anomalies数据集（亚特兰大和柏林子集）上评估，使用Jensen-Shannon Divergence指标，在人口和个体层面都表现出强保真度，同时保持数据和计算效率

Conclusion: Markovian Reeb Graphs是一个可扩展的轨迹模拟框架，适用于各种城市环境

Abstract: Accurately modeling human mobility is critical for urban planning,
epidemiology, and traffic management. In this work, we introduce Markovian Reeb
Graphs, a novel framework for simulating spatiotemporal trajectories that
preserve Patterns of Life (PoLs) learned from baseline data. By combining
individual- and population-level mobility structures within a probabilistic
topological model, our approach generates realistic future trajectories that
capture both consistency and variability in daily life. Evaluations on the
Urban Anomalies dataset (Atlanta and Berlin subsets) using the Jensen-Shannon
Divergence (JSD) across population- and agent-level metrics demonstrate that
the proposed method achieves strong fidelity while remaining data- and
compute-efficient. These results position Markovian Reeb Graphs as a scalable
framework for trajectory simulation with broad applicability across diverse
urban environments.

</details>


### [47] [SpineBench: A Clinically Salient, Level-Aware Benchmark Powered by the SpineMed-450k Corpus](https://arxiv.org/abs/2510.03160)
*Ming Zhao,Wenhui Dong,Yang Zhang,Xiang Zheng,Zhonghao Zhang,Zian Zhou,Yunzhi Guan,Liukun Xu,Wei Peng,Zhaoyang Gong,Zhicheng Zhang,Dachuan Li,Xiaosheng Ma,Yuli Ma,Jianing Ni,Changjiang Jiang,Lixia Tian,Qixin Chen,Kaishun Xia,Pingping Liu,Tongshun Zhang,Zhiqiang Liu,Zhongan Bi,Chenyang Si,Tiansheng Sun,Caifeng Shan*

Main category: cs.CV

TL;DR: SpineMed是一个专门针对脊柱疾病诊断的AI生态系统，包含首个大规模多模态脊柱数据集SpineMed-450k和临床评估框架SpineBench，旨在解决AI辅助脊柱诊断中缺乏层级感知和多模态数据的问题。


<details>
  <summary>Details</summary>
Motivation: 脊柱疾病影响全球6.19亿人，是导致残疾的主要原因，但AI辅助诊断受到缺乏层级感知、多模态数据集的限制。临床决策需要跨X光、CT和MRI在特定椎体级别进行复杂推理，但缺乏可追溯的临床指令数据和标准化脊柱特定基准制约了进展。

Method: 开发了SpineMed生态系统，包含SpineMed-450k数据集（从教科书、指南、开放数据集和约1000个医院病例中提取的45万条指令实例）和SpineBench评估框架。采用临床医生参与的两阶段LLM生成方法（草稿和修订）确保高质量、可追溯的数据。

Result: 对多个先进大型视觉语言模型在SpineBench上的评估显示其在细粒度、层级特定推理方面存在系统性弱点。而在SpineMed-450k上微调的模型在所有任务中都表现出持续且显著的改进。临床医生评估证实了模型输出的诊断清晰度和实用性。

Conclusion: SpineMed为AI辅助脊柱诊断提供了重要的数据集和评估基准，证明了专门针对脊柱疾病设计的训练数据可以显著提升模型在临床相关任务上的表现，具有重要的临床应用价值。

Abstract: Spine disorders affect 619 million people globally and are a leading cause of
disability, yet AI-assisted diagnosis remains limited by the lack of
level-aware, multimodal datasets. Clinical decision-making for spine disorders
requires sophisticated reasoning across X-ray, CT, and MRI at specific
vertebral levels. However, progress has been constrained by the absence of
traceable, clinically-grounded instruction data and standardized,
spine-specific benchmarks. To address this, we introduce SpineMed, an ecosystem
co-designed with practicing spine surgeons. It features SpineMed-450k, the
first large-scale dataset explicitly designed for vertebral-level reasoning
across imaging modalities with over 450,000 instruction instances, and
SpineBench, a clinically-grounded evaluation framework. SpineMed-450k is
curated from diverse sources, including textbooks, guidelines, open datasets,
and ~1,000 de-identified hospital cases, using a clinician-in-the-loop pipeline
with a two-stage LLM generation method (draft and revision) to ensure
high-quality, traceable data for question-answering, multi-turn consultations,
and report generation. SpineBench evaluates models on clinically salient axes,
including level identification, pathology assessment, and surgical planning.
Our comprehensive evaluation of several recently advanced large vision-language
models (LVLMs) on SpineBench reveals systematic weaknesses in fine-grained,
level-specific reasoning. In contrast, our model fine-tuned on SpineMed-450k
demonstrates consistent and significant improvements across all tasks.
Clinician assessments confirm the diagnostic clarity and practical utility of
our model's outputs.

</details>


### [48] [UniShield: An Adaptive Multi-Agent Framework for Unified Forgery Image Detection and Localization](https://arxiv.org/abs/2510.03161)
*Qing Huang,Zhipei Xu,Xuanyu Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: UniShield是一个基于多智能体的统一图像伪造检测与定位系统，能够跨多个领域检测和定位图像伪造，包括图像处理、文档处理、DeepFake和AI生成图像。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成技术的快速发展，合成图像变得越来越逼真，带来了严重的社会风险，如虚假信息和欺诈。现有的领域特定检测方法虽然性能不错，但存在专业化程度过窄、跨领域泛化能力差以及缺乏集成自适应框架等局限性。

Method: UniShield创新性地整合了感知智能体和检测智能体。感知智能体智能分析图像特征以动态选择适合的检测模型，而检测智能体将各种专家检测器整合到统一框架中，并生成可解释的报告。

Result: 大量实验表明，UniShield取得了最先进的结果，超越了现有的统一方法和领域特定检测器，显示出其优越的实用性、自适应性和可扩展性。

Conclusion: UniShield为解决图像伪造检测中的跨领域泛化和集成问题提供了有效的解决方案，具有重要的实际应用价值。

Abstract: With the rapid advancements in image generation, synthetic images have become
increasingly realistic, posing significant societal risks, such as
misinformation and fraud. Forgery Image Detection and Localization (FIDL) thus
emerges as essential for maintaining information integrity and societal
security. Despite impressive performances by existing domain-specific detection
methods, their practical applicability remains limited, primarily due to their
narrow specialization, poor cross-domain generalization, and the absence of an
integrated adaptive framework. To address these issues, we propose UniShield,
the novel multi-agent-based unified system capable of detecting and localizing
image forgeries across diverse domains, including image manipulation, document
manipulation, DeepFake, and AI-generated images. UniShield innovatively
integrates a perception agent with a detection agent. The perception agent
intelligently analyzes image features to dynamically select suitable detection
models, while the detection agent consolidates various expert detectors into a
unified framework and generates interpretable reports. Extensive experiments
show that UniShield achieves state-of-the-art results, surpassing both existing
unified approaches and domain-specific detectors, highlighting its superior
practicality, adaptiveness, and scalability.

</details>


### [49] [ROGR: Relightable 3D Objects using Generative Relighting](https://arxiv.org/abs/2510.03163)
*Jiapeng Tang,Matthew Lavine,Dor Verbin,Stephan J. Garbin,Matthias Nießner,Ricardo Martin Brualla,Pratul P. Srinivasan,Philipp Henzler*

Main category: cs.CV

TL;DR: ROGR是一种新颖的重光照3D重建方法，通过生成式重光照模型从多视角图像重建可重光照的3D模型，使用双分支架构的NeRF来分别编码光照效果和镜面反射。


<details>
  <summary>Details</summary>
Motivation: 现有的3D重建方法难以实现高效的重光照效果，通常需要针对每种光照条件进行优化或复杂的光传输模拟，限制了实际应用。

Method: 提出光照条件化NeRF，采用双分支架构分别处理一般光照效果和镜面反射，通过在不同光照环境下采样训练数据，实现任意环境光照下的前向重光照。

Result: 在TensoIR和Stanford-ORB数据集上评估，在多数指标上优于现有最佳方法，并在真实物体捕获上展示了良好效果。

Conclusion: ROGR方法实现了无需逐光照优化或光传输模拟的高效重光照，为3D重建的重光照应用提供了实用解决方案。

Abstract: We introduce ROGR, a novel approach that reconstructs a relightable 3D model
of an object captured from multiple views, driven by a generative relighting
model that simulates the effects of placing the object under novel environment
illuminations. Our method samples the appearance of the object under multiple
lighting environments, creating a dataset that is used to train a
lighting-conditioned Neural Radiance Field (NeRF) that outputs the object's
appearance under any input environmental lighting. The lighting-conditioned
NeRF uses a novel dual-branch architecture to encode the general lighting
effects and specularities separately. The optimized lighting-conditioned NeRF
enables efficient feed-forward relighting under arbitrary environment maps
without requiring per-illumination optimization or light transport simulation.
We evaluate our approach on the established TensoIR and Stanford-ORB datasets,
where it improves upon the state-of-the-art on most metrics, and showcase our
approach on real-world object captures.

</details>


### [50] [Dynamic Prompt Generation for Interactive 3D Medical Image Segmentation Training](https://arxiv.org/abs/2510.03189)
*Tidiane Camaret Ndir,Alexander Pfefferle,Robin Tibor Schirrmeister*

Main category: cs.CV

TL;DR: 提出了一种结合动态体积提示生成和内容感知自适应裁剪的训练策略，用于优化3D生物医学图像交互式分割模型，在有限计算资源下实现高效训练。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型要么缺乏体积感知能力，要么交互能力有限，无法满足3D生物医学图像交互式分割的需求。

Method: 使用动态体积提示生成模拟真实用户交互模式，结合内容感知自适应裁剪优化图像编码器使用，基于nnInteractive分割模型权重进行初始化。

Result: 在竞赛评估中取得平均Dice分数0.6385，归一化表面距离0.6614，AUC指标分别为2.4799（Dice）和2.5671（NSD）。

Conclusion: 该方法在单GPU上有效解决了序列细化反馈学习的计算挑战，为3D生物医学图像交互式分割提供了可行的解决方案。

Abstract: Interactive 3D biomedical image segmentation requires efficient models that
can iteratively refine predictions based on user prompts. Current foundation
models either lack volumetric awareness or suffer from limited interactive
capabilities. We propose a training strategy that combines dynamic volumetric
prompt generation with content-aware adaptive cropping to optimize the use of
the image encoder. Our method simulates realistic user interaction patterns
during training while addressing the computational challenges of learning from
sequential refinement feedback on a single GPU. For efficient training, we
initialize our network using the publicly available weights from the
nnInteractive segmentation model. Evaluation on the \textbf{Foundation Models
for Interactive 3D Biomedical Image Segmentation} competition demonstrates
strong performance with an average final Dice score of 0.6385, normalized
surface distance of 0.6614, and area-under-the-curve metrics of 2.4799 (Dice)
and 2.5671 (NSD).

</details>


### [51] [Product-Quantised Image Representation for High-Quality Image Synthesis](https://arxiv.org/abs/2510.03191)
*Denis Zavadski,Nikita Philip Tatsch,Carsten Rother*

Main category: cs.CV

TL;DR: PQGAN是一种将产品量化(PQ)集成到VQGAN框架中的量化图像自编码器，在重建性能上显著优于现有方法，并可以无缝集成到预训练扩散模型中实现更高效生成。


<details>
  <summary>Details</summary>
Motivation: 产品量化(PQ)在可扩展向量编码中是一种经典方法，但在高保真图像生成的潜在表示中应用有限。本研究旨在将PQ集成到VQGAN框架中，探索其在图像合成中的潜力。

Method: 提出PQGAN量化图像自编码器，将产品量化集成到VQGAN的向量量化框架中。通过分析码本大小、嵌入维度和子空间分解之间的相互作用，优化超参数选择。

Result: PSNR达到37dB（先前工作为27dB），FID、LPIPS和CMMD分数降低高达96%。研究发现VQ和PQ在缩放嵌入维度时表现相反，为PQ提供了性能趋势指导。

Conclusion: PQGAN能够无缝集成到预训练扩散模型中，实现显著更快、更计算高效的生成，或在不增加成本的情况下将输出分辨率加倍，表明PQ是图像合成中离散潜在表示的强大扩展。

Abstract: Product quantisation (PQ) is a classical method for scalable vector encoding,
yet it has seen limited usage for latent representations in high-fidelity image
generation. In this work, we introduce PQGAN, a quantised image autoencoder
that integrates PQ into the well-known vector quantisation (VQ) framework of
VQGAN. PQGAN achieves a noticeable improvement over state-of-the-art methods in
terms of reconstruction performance, including both quantisation methods and
their continuous counterparts. We achieve a PSNR score of 37dB, where prior
work achieves 27dB, and are able to reduce the FID, LPIPS, and CMMD score by up
to 96%. Our key to success is a thorough analysis of the interaction between
codebook size, embedding dimensionality, and subspace factorisation, with
vector and scalar quantisation as special cases. We obtain novel findings, such
that the performance of VQ and PQ behaves in opposite ways when scaling the
embedding dimension. Furthermore, our analysis shows performance trends for PQ
that help guide optimal hyperparameter selection. Finally, we demonstrate that
PQGAN can be seamlessly integrated into pre-trained diffusion models. This
enables either a significantly faster and more compute-efficient generation, or
a doubling of the output resolution at no additional cost, positioning PQ as a
strong extension for discrete latent representation in image synthesis.

</details>


### [52] [Memory Forcing: Spatio-Temporal Memory for Consistent Scene Generation on Minecraft](https://arxiv.org/abs/2510.03198)
*Junchao Huang,Xinting Hu,Boyao Han,Shaoshuai Shi,Zhuotao Tian,Tianyu He,Li Jiang*

Main category: cs.CV

TL;DR: 提出Memory Forcing学习框架，通过混合训练和链式前向训练结合几何索引空间记忆，解决自回归视频扩散模型在有限计算预算下长期空间一致性与新场景生成的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 自回归视频扩散模型在Minecraft游戏等应用中需要同时生成自然新内容并保持探索区域的空间一致性，但有限上下文窗口下存在时间记忆缺乏长期空间一致性与空间记忆可能降低新场景生成质量的矛盾。

Method: Memory Forcing框架包含：1）混合训练区分探索和重访阶段；2）链式前向训练通过模型滚动创建更大姿态变化；3）点对帧检索和增量3D重建维护显式3D缓存。

Result: 实验表明该方法在多样化环境中实现了优越的长期空间一致性和生成质量，同时保持扩展序列的计算效率。

Conclusion: Memory Forcing通过精心设计的训练协议和几何索引空间记忆，有效平衡了自回归视频扩散模型的空间一致性与生成质量需求。

Abstract: Autoregressive video diffusion models have proved effective for world
modeling and interactive scene generation, with Minecraft gameplay as a
representative application. To faithfully simulate play, a model must generate
natural content while exploring new scenes and preserve spatial consistency
when revisiting explored areas. Under limited computation budgets, it must
compress and exploit historical cues within a finite context window, which
exposes a trade-off: Temporal-only memory lacks long-term spatial consistency,
whereas adding spatial memory strengthens consistency but may degrade new scene
generation quality when the model over-relies on insufficient spatial context.
We present Memory Forcing, a learning framework that pairs training protocols
with a geometry-indexed spatial memory. Hybrid Training exposes distinct
gameplay regimes, guiding the model to rely on temporal memory during
exploration and incorporate spatial memory for revisits. Chained Forward
Training extends autoregressive training with model rollouts, where chained
predictions create larger pose variations and encourage reliance on spatial
memory for maintaining consistency. Point-to-Frame Retrieval efficiently
retrieves history by mapping currently visible points to their source frames,
while Incremental 3D Reconstruction maintains and updates an explicit 3D cache.
Extensive experiments demonstrate that Memory Forcing achieves superior
long-term spatial consistency and generative quality across diverse
environments, while maintaining computational efficiency for extended
sequences.

</details>


### [53] [MonSTeR: a Unified Model for Motion, Scene, Text Retrieval](https://arxiv.org/abs/2510.03200)
*Luca Collorone,Matteo Gioia,Massimiliano Pappa,Paolo Leoni,Giovanni Ficarra,Or Litany,Indro Spinelli,Fabio Galasso*

Main category: cs.CV

TL;DR: MonSTeR是首个运动-场景-文本检索模型，通过构建统一潜在空间来评估骨骼运动、意图和场景之间的对齐关系


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏评估骨骼运动、意图和周围场景对齐关系的工具，尽管这种机制在人类复杂环境运动中很直观

Method: 利用单模态和跨模态表示构建统一潜在空间，捕捉模态间复杂依赖关系，实现灵活而鲁棒的跨任务检索

Result: MonSTeR优于仅依赖单模态表示的三模态模型，并通过用户研究验证了检索分数与人类偏好的一致性

Conclusion: MonSTeR在零样本场景内物体放置和运动描述任务中展示了其潜在空间的多功能性

Abstract: Intention drives human movement in complex environments, but such movement
can only happen if the surrounding context supports it. Despite the intuitive
nature of this mechanism, existing research has not yet provided tools to
evaluate the alignment between skeletal movement (motion), intention (text),
and the surrounding context (scene). In this work, we introduce MonSTeR, the
first MOtioN-Scene-TExt Retrieval model. Inspired by the modeling of
higher-order relations, MonSTeR constructs a unified latent space by leveraging
unimodal and cross-modal representations. This allows MonSTeR to capture the
intricate dependencies between modalities, enabling flexible but robust
retrieval across various tasks. Our results show that MonSTeR outperforms
trimodal models that rely solely on unimodal representations. Furthermore, we
validate the alignment of our retrieval scores with human preferences through a
dedicated user study. We demonstrate the versatility of MonSTeR's latent space
on zero-shot in-Scene Object Placement and Motion Captioning. Code and
pre-trained models are available at github.com/colloroneluca/MonSTeR.

</details>


### [54] [Test-Time Defense Against Adversarial Attacks via Stochastic Resonance of Latent Ensembles](https://arxiv.org/abs/2510.03224)
*Dong Lao,Yuxiang Zhang,Haniyeh Ehsani Oskouie,Yangchao Wu,Alex Wong,Stefano Soatto*

Main category: cs.CV

TL;DR: 提出一种基于随机共振的测试时防御机制，通过引入小幅度平移扰动来对抗对抗性攻击，无需训练、架构无关且攻击类型无关。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法依赖特征过滤或平滑处理，容易导致信息损失，需要一种既能增强鲁棒性又能最小化信息损失的通用防御机制。

Method: 在输入图像中引入小的平移扰动，对齐变换后的特征嵌入，然后聚合这些特征并映射回原始参考图像，整个过程可用闭式公式表达。

Result: 在图像分类任务上恢复68.1%的准确率损失，立体匹配恢复71.9%，光流估计恢复29.2%，首次为密集预测任务建立通用测试时防御。

Conclusion: 该方法展示了出色的泛化能力和实用性，为多种计算机视觉任务提供了有效的对抗攻击防御解决方案。

Abstract: We propose a test-time defense mechanism against adversarial attacks:
imperceptible image perturbations that significantly alter the predictions of a
model. Unlike existing methods that rely on feature filtering or smoothing,
which can lead to information loss, we propose to "combat noise with noise" by
leveraging stochastic resonance to enhance robustness while minimizing
information loss. Our approach introduces small translational perturbations to
the input image, aligns the transformed feature embeddings, and aggregates them
before mapping back to the original reference image. This can be expressed in a
closed-form formula, which can be deployed on diverse existing network
architectures without introducing additional network modules or fine-tuning for
specific attack types. The resulting method is entirely training-free,
architecture-agnostic, and attack-agnostic. Empirical results show
state-of-the-art robustness on image classification and, for the first time,
establish a generic test-time defense for dense prediction tasks, including
stereo matching and optical flow, highlighting the method's versatility and
practicality. Specifically, relative to clean (unperturbed) performance, our
method recovers up to 68.1% of the accuracy loss on image classification, 71.9%
on stereo matching, and 29.2% on optical flow under various types of
adversarial attacks.

</details>


### [55] [MIXER: Mixed Hyperspherical Random Embedding Neural Network for Texture Recognition](https://arxiv.org/abs/2510.03228)
*Ricardo T. Fares,Lucas C. Ribas*

Main category: cs.CV

TL;DR: 提出Mixer，一种新颖的随机神经网络，用于纹理表示学习，通过超球面随机嵌入和双分支学习模块捕获通道内和通道间关系，并在多个纹理基准测试中取得良好结果。


<details>
  <summary>Details</summary>
Motivation: 现有随机神经网络方法主要关注改进跨信息预测，未对整体网络架构进行显著改进，因此需要一种新的架构来更好地进行纹理表示学习。

Method: Mixer方法利用超球面随机嵌入结合双分支学习模块来捕获通道内和通道间关系，并通过新制定的优化问题构建丰富的纹理表示。

Result: 实验结果表明该方法在多个具有不同特征和挑战的纯纹理基准测试中取得了有趣的结果。

Conclusion: Mixer方法通过创新的架构设计在纹理识别任务中表现出色，源代码将在发表后提供。

Abstract: Randomized neural networks for representation learning have consistently
achieved prominent results in texture recognition tasks, effectively combining
the advantages of both traditional techniques and learning-based approaches.
However, existing approaches have so far focused mainly on improving
cross-information prediction, without introducing significant advancements to
the overall randomized network architecture. In this paper, we propose Mixer, a
novel randomized neural network for texture representation learning. At its
core, the method leverages hyperspherical random embeddings coupled with a
dual-branch learning module to capture both intra- and inter-channel
relationships, further enhanced by a newly formulated optimization problem for
building rich texture representations. Experimental results have shown the
interesting results of the proposed approach across several pure texture
benchmarks, each with distinct characteristics and challenges. The source code
will be available upon publication.

</details>


### [56] [Improving GUI Grounding with Explicit Position-to-Coordinate Mapping](https://arxiv.org/abs/2510.03230)
*Suyuchen Wang,Tianyu Zhang,Ahmed Masry,Christopher Pal,Spandana Gella,Bang Liu,Perouz Taslakian*

Main category: cs.CV

TL;DR: 本文提出RULER tokens和I-MRoPE两种创新方法，解决GUI grounding任务中patch-to-pixel映射在未见高分辨率显示上的泛化问题，显著提升了跨分辨率平台的GUI自动化可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前VLM在GUI grounding任务中面临核心瓶颈：可靠的patch-to-pixel映射在训练未见的高分辨率显示上会失效。现有方法直接从视觉特征生成文本坐标，迫使模型隐式学习复杂的位置到像素映射，导致精度下降和新分辨率上的失败。

Method: 1. RULER tokens作为显式坐标标记，让模型像地图网格线一样参考位置并调整坐标，而不是从头生成；2. 交错MRoPE（I-MRoPE）通过确保宽度和高度维度平等表示来改进空间编码，解决标准位置方案的不对称性问题。

Result: 在ScreenSpot、ScreenSpot-V2和ScreenSpot-Pro数据集上的实验显示，接地精度持续提升，在高分辨率界面上的改进最大。

Conclusion: 通过提供显式空间指导而非依赖隐式学习，该方法能够在不同分辨率和平台上实现更可靠的GUI自动化。

Abstract: GUI grounding, the task of mapping natural-language instructions to pixel
coordinates, is crucial for autonomous agents, yet remains difficult for
current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which
breaks when extrapolating to high-resolution displays unseen during training.
Current approaches generate coordinates as text tokens directly from visual
features, forcing the model to infer complex position-to-pixel mappings
implicitly; as a result, accuracy degrades and failures proliferate on new
resolutions. We address this with two complementary innovations. First, RULER
tokens serve as explicit coordinate markers, letting the model reference
positions similar to gridlines on a map and adjust rather than generate
coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial
encoding by ensuring that width and height dimensions are represented equally,
addressing the asymmetry of standard positional schemes. Experiments on
ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in
grounding accuracy, with the largest improvements on high-resolution
interfaces. By providing explicit spatial guidance rather than relying on
implicit learning, our approach enables more reliable GUI automation across
diverse resolutions and platforms.

</details>


### [57] [LEAML: Label-Efficient Adaptation to Out-of-Distribution Visual Tasks for Multimodal Large Language Models](https://arxiv.org/abs/2510.03232)
*Ci-Siang Lin,Min-Hung Chen,Yu-Yang Sheng,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: LEAML是一个标签高效的适应框架，用于解决多模态大语言模型在专业领域（如医学影像）中数据稀缺的问题，通过生成伪问答对和选择性神经元更新来提升性能。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在通用视觉基准上表现良好，但在专业领域（如医学影像）中，由于标注数据有限且昂贵，面对分布外任务时表现不佳。

Method: LEAML利用少量标注的VQA样本和大量未标注图像，通过QA生成器生成领域相关的伪问答对，并使用标题蒸馏进行正则化。关键创新是选择性更新与问答最相关的神经元，使QA生成器在蒸馏过程中高效获取领域特定知识。

Result: 在胃肠道内窥镜和体育VQA上的实验表明，LEAML在最小监督下始终优于标准微调方法。

Conclusion: LEAML框架通过标签高效适应策略，有效提升了多模态大语言模型在专业领域的性能，证明了该方法的有效性。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance on
general visual benchmarks but struggle with out-of-distribution (OOD) tasks in
specialized domains such as medical imaging, where labeled data is limited and
expensive. We introduce LEAML, a label-efficient adaptation framework that
leverages both scarce labeled VQA samples and abundant unlabeled images. Our
approach generates domain-relevant pseudo question-answer pairs for unlabeled
data using a QA generator regularized by caption distillation. Importantly, we
selectively update only those neurons most relevant to question-answering,
enabling the QA Generator to efficiently acquire domain-specific knowledge
during distillation. Experiments on gastrointestinal endoscopy and sports VQA
demonstrate that LEAML consistently outperforms standard fine-tuning under
minimal supervision, highlighting the effectiveness of our proposed LEAML
framework.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [58] [Glaucoma Detection and Structured OCT Report Generation via a Fine-tuned Multimodal Large Language Model](https://arxiv.org/abs/2510.02403)
*Jalil Jalili,Yashraj Gavhane,Evan Walker,Anna Heinke,Christopher Bowd,Akram Belghith,Massimo A. Fazio,Christopher A. Girkin,C. Gustavo De Moraes,Jeffrey M. Liebmann,Sally L. Baxter,Robert N. Weinreb,Linda M. Zangwill,Mark Christopher*

Main category: q-bio.QM

TL;DR: 开发了一个可解释的多模态大语言模型，用于筛选视神经头OCT环形扫描图像质量并生成包含青光眼诊断和视网膜神经纤维层变薄评估的结构化临床报告。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一个能够自动分析OCT图像、评估图像质量、检测青光眼并提供详细临床描述的人工智能系统，以支持临床决策。

Method: 使用Llama 3.2 Vision-Instruct模型进行微调，训练数据包括配对的OCT图像和自动生成的结构化临床报告，评估了质量评估、青光眼检测和RNFL变薄分类三个任务。

Result: 模型在质量筛查中准确率达0.90，特异性0.98；青光眼检测准确率0.86；RNFL变薄预测准确率0.83-0.94；文本生成评分显示与参考报告高度一致。

Conclusion: 微调后的多模态大语言模型能够基于OCT成像生成准确的临床描述，在识别图像质量问题和检测青光眼方面表现出高准确性，并为RNFL变薄提供分区描述以支持临床评估。

Abstract: Objective: To develop an explainable multimodal large language model (MM-LLM)
that (1) screens optic nerve head (ONH) OCT circle scans for quality and (2)
generates structured clinical reports that include glaucoma diagnosis and
sector-wise retinal nerve fiber layer (RNFL) thinning assessments. Design:
Retrospective cohort study of 1,310 subjects contributing 43,849 Spectralis ONH
OCT circle scans (1,331 glaucomatous and 867 healthy eyes) from the DIGS and
ADAGES cohorts. Methods: A MM-LLM (Llama 3.2 Vision-Instruct model) was
fine-tuned to generate clinical descriptions of OCT imaging data. Training data
included paired OCT images and automatically generated, structured clinical
reports that described global and sectoral RNFL thinning. Poor-quality scans
were labeled as unusable and paired with a fixed refusal statement. The model
was evaluated on a held-out test set for three tasks: quality assessment,
glaucoma detection, and RNFL thinning classification across seven anatomical
sectors. Evaluation metrics included accuracy, sensitivity, specificity,
precision, and F1-score. Model description quality was also evaluated using
standard text evaluation metrics. Results: The model achieved 0.90 accuracy and
0.98 specificity for quality triage. For glaucoma detection, accuracy was 0.86
(sensitivity 0.91, specificity 0.73, F1-score 0.91). RNFL thinning prediction
accuracy ranged from 0.83 to 0.94, with highest performance in global and
temporal sectors. Text generation scores showed strong alignment with reference
reports (BLEU: 0.82; ROUGE-1: 0.94; ROUGE-2: 0.87; ROUGE-L: 0.92; BERTScore-F1:
0.99). Conclusions: The fine-tuned MM-LLM generated accurate clinical
descriptions based on OCT imaging. The model achieved high accuracy in
identifying image quality issues and detecting glaucoma. The model also
provided sectoral descriptions of RNFL thinning to help support clinical OCT
evaluation.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [59] [Learning a distance measure from the information-estimation geometry of data](https://arxiv.org/abs/2510.02514)
*Guy Ohayon,Pierre-Etienne H. Fiquet,Florentin Guth,Jona Ballé,Eero P. Simoncelli*

Main category: eess.IV

TL;DR: 提出了一种新的距离度量方法——信息估计度量（IEM），它基于信息论和估计理论的基本关系，通过比较信号在多个噪声水平下的去噪误差向量来定义距离。IEM在ImageNet上的实验表明，其在预测人类感知判断方面优于现有监督图像质量度量方法。


<details>
  <summary>Details</summary>
Motivation: 现有的距离度量方法可能无法很好地适应复杂信号分布的几何结构。作者希望开发一种能够同时考虑局部和全局分布几何的距离度量，从而更好地反映人类感知判断。

Method: IEM通过比较信号在多个噪声水平下的最优去噪器误差向量来定义距离。该方法基于信息论和估计理论的基本关系，将信号的对数概率与最优去噪器的误差联系起来。在实践中，可以使用学习到的去噪器（类似于生成扩散模型）来计算IEM，并通过求解一维积分来实现。

Result: 在ImageNet数据库上学习的IEM在预测人类感知判断方面具有竞争力，甚至优于最先进的监督图像质量度量方法。实验证明IEM能够有效适应复杂分布的几何结构。

Conclusion: IEM是一种有效的距离度量方法，它通过结合信息论和估计理论，能够自适应地捕捉信号分布的几何特性。该方法在图像质量评估等感知任务中表现出优越性能，为信号处理领域提供了新的工具。

Abstract: We introduce the Information-Estimation Metric (IEM), a novel form of
distance function derived from an underlying continuous probability density
over a domain of signals. The IEM is rooted in a fundamental relationship
between information theory and estimation theory, which links the
log-probability of a signal with the errors of an optimal denoiser, applied to
noisy observations of the signal. In particular, the IEM between a pair of
signals is obtained by comparing their denoising error vectors over a range of
noise amplitudes. Geometrically, this amounts to comparing the score vector
fields of the blurred density around the signals over a range of blur levels.
We prove that the IEM is a valid global metric and derive a closed-form
expression for its local second-order approximation, which yields a Riemannian
metric. For Gaussian-distributed signals, the IEM coincides with the
Mahalanobis distance. But for more complex distributions, it adapts, both
locally and globally, to the geometry of the distribution. In practice, the IEM
can be computed using a learned denoiser (analogous to generative diffusion
models) and solving a one-dimensional integral. To demonstrate the value of our
framework, we learn an IEM on the ImageNet database. Experiments show that this
IEM is competitive with or outperforms state-of-the-art supervised image
quality metrics in predicting human perceptual judgments.

</details>


### [60] [A UAV-Based VNIR Hyperspectral Benchmark Dataset for Landmine and UXO Detection](https://arxiv.org/abs/2510.02700)
*Sagar Lekhak,Emmett J. Ientilucci,Jasper Baur,Susmita Ghosh*

Main category: eess.IV

TL;DR: 本文介绍了一个用于地雷和未爆弹药检测研究的新型无人机可见光-近红外高光谱影像基准数据集，包含143个模拟目标，具有高光谱保真度。


<details>
  <summary>Details</summary>
Motivation: 填补无人机高光谱数据在地雷检测领域的公开数据空白，为可重复研究提供多传感器基准。

Method: 使用Headwall Nano-Hyperspec传感器在无人机平台上采集数据，通过经验线方法进行反射率反演，并进行辐射定标、正射校正和拼接处理。

Result: 交叉验证显示在400-900nm范围内RMSE低于1.0，SAM值在1-6度之间，证明了数据集的高光谱保真度。

Conclusion: 该数据集为地雷检测研究提供了重要的公开基准资源，可与先前发布的电磁感应数据结合使用。

Abstract: This paper introduces a novel benchmark dataset of Visible and Near-Infrared
(VNIR) hyperspectral imagery acquired via an unmanned aerial vehicle (UAV)
platform for landmine and unexploded ordnance (UXO) detection research. The
dataset was collected over a controlled test field seeded with 143 realistic
surrogate landmine and UXO targets, including surface, partially buried, and
fully buried configurations. Data acquisition was performed using a Headwall
Nano-Hyperspec sensor mounted on a multi-sensor drone platform, flown at an
altitude of approximately 20.6 m, capturing 270 contiguous spectral bands
spanning 398-1002 nm. Radiometric calibration, orthorectification, and
mosaicking were performed followed by reflectance retrieval using a two-point
Empirical Line Method (ELM), with reference spectra acquired using an SVC
spectroradiometer. Cross-validation against six reference objects yielded RMSE
values below 1.0 and SAM values between 1 and 6 degrees in the 400-900 nm
range, demonstrating high spectral fidelity. The dataset is released alongside
raw radiance cubes, GCP/AeroPoint data, and reference spectra to support
reproducible research. This contribution fills a critical gap in open-access
UAV-based hyperspectral data for landmine detection and offers a multi-sensor
benchmark when combined with previously published drone-based electromagnetic
induction (EMI) data from the same test field.

</details>


### [61] [Image Enhancement Based on Pigment Representation](https://arxiv.org/abs/2510.02713)
*Se-Ho Lee,Keunsoo Ko,Seung-Wook Kim*

Main category: eess.IV

TL;DR: 提出了一种基于颜料表示的高效图像增强方法，通过将RGB颜色动态转换为高维颜料空间，实现自适应内容增强，在图像修饰和色调映射任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统图像增强方法受限于预定义颜色空间（如RGB），缺乏对输入内容的动态适应能力。本文旨在开发一种更灵活、表达能力更强的颜色表示方法。

Method: 将输入RGB颜色转换为高维颜料空间，对颜料进行单独重投影和混合以优化颜色信息，然后转换回RGB生成增强图像。转换参数由视觉编码器根据输入图像内容自适应估计。

Result: 实验结果表明该方法在图像修饰和色调映射任务中优于现有最先进方法，同时保持较低计算复杂度和较小模型尺寸。

Conclusion: 颜料表示方法为图像增强提供了有效的自适应解决方案，在性能和效率方面均表现出色。

Abstract: This paper presents a novel and efficient image enhancement method based on
pigment representation. Unlike conventional methods where the color
transformation is restricted to pre-defined color spaces like RGB, our method
dynamically adapts to input content by transforming RGB colors into a
high-dimensional feature space referred to as \textit{pigments}. The proposed
pigment representation offers adaptability and expressiveness, achieving
superior image enhancement performance. The proposed method involves
transforming input RGB colors into high-dimensional pigments, which are then
reprojected individually and blended to refine and aggregate the information of
the colors in pigment spaces. Those pigments are then transformed back into RGB
colors to generate an enhanced output image. The transformation and
reprojection parameters are derived from the visual encoder which adaptively
estimates such parameters based on the content in the input image. Extensive
experimental results demonstrate the superior performance of the proposed
method over state-of-the-art methods in image enhancement tasks, including
image retouching and tone mapping, while maintaining relatively low
computational complexity and small model size.

</details>


### [62] [GCVAMD: A Modified CausalVAE Model for Causal Age-related Macular Degeneration Risk Factor Detection and Prediction](https://arxiv.org/abs/2510.02781)
*Daeyoung Kim*

Main category: eess.IV

TL;DR: 本文提出了一种新的因果性AMD分析模型GCVAMD，通过改进的CausalVAE方法从原始OCT图像中提取潜在因果因素，实现AMD检测和干预分析。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法主要关注预测性能，而忽略了AMD的病理学和潜在因果机制，这限制了特定因素的干预分析和决策可靠性。

Method: GCVAMD模型采用改进的CausalVAE方法，仅从原始OCT图像中提取潜在因果因素，支持针对主要风险因素（玻璃膜疣和新生血管）的治疗模拟和干预分析。

Result: 结果显示GCVAMD能够识别玻璃膜疣状态和新生血管状态，并在潜在空间中呈现AMD因果机制，可用于从AMD检测到干预分析的各种任务。

Conclusion: 通过引入因果性分析，GCVAMD不仅提高了AMD检测性能，还提供了可解释的因果特征，增强了医疗诊断的可靠性和实用性。

Abstract: Age Related Macular Degeneration(AMD) has been one of the most leading causes
of permanent vision impairment in ophthalmology. Though treatments, such as
anti VEGF drugs or photodynamic therapies, were developed to slow down the
degenerative process of AMD, there is still no specific cure to reverse vision
loss caused by AMD. Thus, for AMD, detecting existence of risk factors of AMD
or AMD itself within the patient retina in early stages is a crucial task to
reduce the possibility of vision impairment. Apart from traditional approaches,
deep learning based methods, especially attention mechanism based CNNs and
GradCAM based XAI analysis on OCT scans, exhibited successful performance in
distinguishing AMD retina from normal retinas, making it possible to use AI
driven models to aid medical diagnosis and analysis by ophthalmologists
regarding AMD. However, though having significant success, previous works
mostly focused on prediction performance itself, not pathologies or underlying
causal mechanisms of AMD, which can prohibit intervention analysis on specific
factors or even lead to less reliable decisions. Thus, this paper introduces a
novel causal AMD analysis model: GCVAMD, which incorporates a modified
CausalVAE approach that can extract latent causal factors from only raw OCT
images. By considering causality in AMD detection, GCVAMD enables causal
inference such as treatment simulation or intervention analysis regarding major
risk factors: drusen and neovascularization, while returning informative latent
causal features that can enhance downstream tasks. Results show that through
GCVAMD, drusen status and neovascularization status can be identified with AMD
causal mechanisms in GCVAMD latent spaces, which can in turn be used for
various tasks from AMD detection(classification) to intervention analysis.

</details>


### [63] [Wave-GMS: Lightweight Multi-Scale Generative Model for Medical Image Segmentation](https://arxiv.org/abs/2510.03216)
*Talha Ahmed,Nehal Ahmed Shaikh,Hassan Mohy-ud-Din*

Main category: eess.IV

TL;DR: Wave-GMS是一种轻量级高效的多尺度生成模型，用于医疗图像分割，具有参数量少、无需预训练大模型、支持有限内存GPU大批次训练的特点，在四个公开数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 为实现AI工具在医院和医疗设施的公平部署，需要能够在成本效益GPU上训练的高性能深度分割网络，这些GPU通常内存有限但需要大批次训练。

Method: 提出Wave-GMS模型，这是一个轻量级高效的多尺度生成模型，具有少量可训练参数，不依赖内存密集的预训练视觉基础模型，支持在有限内存GPU上进行大批次训练。

Result: 在四个公开数据集（BUS、BUSI、Kvasir-Instrument、HAM10000）上的实验表明，Wave-GMS仅需约260万可训练参数就能达到最先进的分割性能，并具有优异的跨域泛化能力。

Conclusion: Wave-GMS为医疗图像分割提供了一种高效实用的解决方案，能够在资源受限的环境中实现高性能分割，促进AI工具在医疗领域的公平部署。

Abstract: For equitable deployment of AI tools in hospitals and healthcare facilities,
we need Deep Segmentation Networks that offer high performance and can be
trained on cost-effective GPUs with limited memory and large batch sizes. In
this work, we propose Wave-GMS, a lightweight and efficient multi-scale
generative model for medical image segmentation. Wave-GMS has a substantially
smaller number of trainable parameters, does not require loading
memory-intensive pretrained vision foundation models, and supports training
with large batch sizes on GPUs with limited memory. We conducted extensive
experiments on four publicly available datasets (BUS, BUSI, Kvasir-Instrument,
and HAM10000), demonstrating that Wave-GMS achieves state-of-the-art
segmentation performance with superior cross-domain generalizability, while
requiring only ~2.6M trainable parameters. Code is available at
https://github.com/ATPLab-LUMS/Wave-GMS.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [64] [Words That Make Language Models Perceive](https://arxiv.org/abs/2510.02425)
*Sophie L. Wang,Phillip Isola,Brian Cheung*

Main category: cs.CL

TL;DR: 本文研究发现，通过简单的感官提示（如'看'或'听'）可以激活纯文本训练的LLMs中的多模态表征，使其与专业视觉和音频编码器的表征对齐。


<details>
  <summary>Details</summary>
Motivation: 探索纯文本训练的LLMs是否包含隐式的多模态结构，以及是否可以通过感官提示来激活这些潜在表征。

Method: 使用感官提示（如'看'或'听'）来引导LLMs在生成下一个token时，仿佛这些预测是基于未提供的视觉或听觉证据。

Result: 研究发现，轻量级的提示工程可以可靠地激活纯文本LLMs中与模态相适应的表征。

Conclusion: 纯文本训练的LLMs内部表征确实受到语言中编码的多模态规律的影响，感官提示可以有效激活这些潜在结构。

Abstract: Large language models (LLMs) trained purely on text ostensibly lack any
direct perceptual experience, yet their internal representations are implicitly
shaped by multimodal regularities encoded in language. We test the hypothesis
that explicit sensory prompting can surface this latent structure, bringing a
text-only LLM into closer representational alignment with specialist vision and
audio encoders. When a sensory prompt tells the model to 'see' or 'hear', it
cues the model to resolve its next-token predictions as if they were
conditioned on latent visual or auditory evidence that is never actually
supplied. Our findings reveal that lightweight prompt engineering can reliably
activate modality-appropriate representations in purely text-trained LLMs.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [65] [Neural Posterior Estimation with Autoregressive Tiling for Detecting Objects in Astronomical Images](https://arxiv.org/abs/2510.03074)
*Jeffrey Regier*

Main category: stat.AP

TL;DR: 提出了一种基于摊销变分推理的空间自回归变分分布方法，用于解决天文图像中的小目标检测问题，在斯隆数字巡天数据上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 即将到来的天文巡天将产生海量高分辨率星空图像，需要检测和表征其中的天文目标。由于大多数目标暗淡且视觉上相互重叠，这是一个具有挑战性的任务。

Method: 使用基于K色棋盘模式的空间自回归变分分布族，通过卷积神经网络参数化变分分布，采用神经后验估计最小化前向KL散度。

Result: 在斯隆数字巡天图像上实现了最先进的检测性能，并且自回归结构显著改善了后验校准。

Conclusion: 提出的空间自回归变分分布方法有效解决了天文图像小目标检测问题，其条件独立性结构与后验分布相匹配，提高了检测准确性和后验校准质量。

Abstract: Upcoming astronomical surveys will produce petabytes of high-resolution
images of the night sky, providing information about billions of stars and
galaxies. Detecting and characterizing the astronomical objects in these images
is a fundamental task in astronomy -- and a challenging one, as most of these
objects are faint and many visually overlap with other objects. We propose an
amortized variational inference procedure to solve this instance of
small-object detection. Our key innovation is a family of spatially
autoregressive variational distributions that partition and order the latent
space according to a $K$-color checkerboard pattern. By construction, the
conditional independencies of this variational family mirror those of the
posterior distribution. We fit the variational distribution, which is
parameterized by a convolutional neural network, using neural posterior
estimation (NPE) to minimize an expectation of the forward KL divergence. Using
images from the Sloan Digital Sky Survey, our method achieves state-of-the-art
performance. We further demonstrate that the proposed autoregressive structure
greatly improves posterior calibration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [66] [Dale meets Langevin: A Multiplicative Denoising Diffusion Model](https://arxiv.org/abs/2510.02730)
*Nishanth Shetty,Madhava Prasath,Chandra Sekhar Seelamantula*

Main category: cs.LG

TL;DR: 该论文提出了一种基于几何布朗运动的多重更新生成模型，将Dale定律与随机微分方程结合，开发出生物启发的生成方法。


<details>
  <summary>Details</summary>
Motivation: 传统梯度下降与生物学习机制不一致，需要开发更符合生物系统学习规律的方法。Dale定律指出抑制性和兴奋性突触在学习过程中不会互换角色，这启发了指数梯度下降方法。

Method: 从几何布朗运动的随机微分方程出发，离散化对应的时间反向SDE得到多重更新规则，提出多重去噪分数匹配形式化方法，适用于正数据分布。

Result: 在MNIST、Fashion MNIST和Kuzushiji数据集上的实验证明了新方案的生成能力，这是首个基于几何布朗运动的生物启发生成模型。

Conclusion: 该方法成功地将生物学习原理与生成模型相结合，为生物启发的机器学习提供了新思路，特别是在处理正数据分布时表现出天然优势。

Abstract: Gradient descent has proven to be a powerful and effective technique for
optimization in numerous machine learning applications. Recent advances in
computational neuroscience have shown that learning in standard gradient
descent optimization formulation is not consistent with learning in biological
systems. This has opened up interesting avenues for building biologically
inspired learning techniques. One such approach is inspired by Dale's law,
which states that inhibitory and excitatory synapses do not swap roles during
the course of learning. The resulting exponential gradient descent optimization
scheme leads to log-normally distributed synaptic weights. Interestingly, the
density that satisfies the Fokker-Planck equation corresponding to the
stochastic differential equation (SDE) with geometric Brownian motion (GBM) is
the log-normal density. Leveraging this connection, we start with the SDE
governing geometric Brownian motion, and show that discretizing the
corresponding reverse-time SDE yields a multiplicative update rule, which
surprisingly, coincides with the sampling equivalent of the exponential
gradient descent update founded on Dale's law. Furthermore, we propose a new
formalism for multiplicative denoising score-matching, subsuming the loss
function proposed by Hyvaerinen for non-negative data. Indeed, log-normally
distributed data is positive and the proposed score-matching formalism turns
out to be a natural fit. This allows for training of score-based models for
image data and results in a novel multiplicative update scheme for sample
generation starting from a log-normal density. Experimental results on MNIST,
Fashion MNIST, and Kuzushiji datasets demonstrate generative capability of the
new scheme. To the best of our knowledge, this is the first instance of a
biologically inspired generative model employing multiplicative updates,
founded on geometric Brownian motion.

</details>


### [67] [Confidence and Dispersity as Signals: Unsupervised Model Evaluation and Ranking](https://arxiv.org/abs/2510.02956)
*Weijian Deng,Weijie Tu,Ibrahim Radwan,Mohammad Abu Alsheikh,Stephen Gould,Liang Zheng*

Main category: cs.LG

TL;DR: 提出了一个统一的框架用于无监督模型评估和排名，通过结合模型预测的置信度和分散性来评估模型在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在真实世界部署中，当测试数据没有标签时，评估模型在分布偏移下的泛化能力至关重要。

Method: 系统性地比较了基于置信度、分散性以及混合的评估指标，并在多种模型架构、数据集和分布偏移类型下进行基准测试。

Result: 混合指标在数据集中心评估和模型中心评估设置中均优于单方面指标，特别是预测矩阵的核范数在各种任务中表现稳健准确。

Conclusion: 该研究为部署场景中的无监督模型评估提供了实用且可推广的基础。

Abstract: Assessing model generalization under distribution shift is essential for
real-world deployment, particularly when labeled test data is unavailable. This
paper presents a unified and practical framework for unsupervised model
evaluation and ranking in two common deployment settings: (1) estimating the
accuracy of a fixed model on multiple unlabeled test sets (dataset-centric
evaluation), and (2) ranking a set of candidate models on a single unlabeled
test set (model-centric evaluation). We demonstrate that two intrinsic
properties of model predictions, namely confidence (which reflects prediction
certainty) and dispersity (which captures the diversity of predicted classes),
together provide strong and complementary signals for generalization. We
systematically benchmark a set of confidence-based, dispersity-based, and
hybrid metrics across a wide range of model architectures, datasets, and
distribution shift types. Our results show that hybrid metrics consistently
outperform single-aspect metrics on both dataset-centric and model-centric
evaluation settings. In particular, the nuclear norm of the prediction matrix
provides robust and accurate performance across tasks, including real-world
datasets, and maintains reliability under moderate class imbalance. These
findings offer a practical and generalizable basis for unsupervised model
assessment in deployment scenarios.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [68] [Secure and Robust Watermarking for AI-generated Images: A Comprehensive Survey](https://arxiv.org/abs/2510.02384)
*Jie Cao,Qi Li,Zelin Zhang,Jianbing Ni*

Main category: cs.CR

TL;DR: 本文对AI生成图像水印技术进行了全面综述，涵盖了系统形式化、技术比较、评估方法、安全漏洞以及未来挑战等五个关键维度。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI的快速发展，AI生成图像在知识产权保护、真实性和责任追究方面引发了重要关切，水印技术成为区分AI生成内容与自然内容的关键解决方案。

Method: 采用系统综述方法，从五个维度分析AI生成图像水印技术：系统形式化、技术比较、评估方法（视觉质量、容量、可检测性）、安全漏洞分析以及挑战与未来方向。

Result: 提供了对当前AI生成图像水印技术的全面概述，为研究人员提供了该领域的整体理解框架。

Conclusion: 水印技术是确保AI生成图像可信度和可追溯性的重要手段，该综述旨在促进相关技术的持续发展。

Abstract: The rapid advancement of generative artificial intelligence (Gen-AI) has
facilitated the effortless creation of high-quality images, while
simultaneously raising critical concerns regarding intellectual property
protection, authenticity, and accountability. Watermarking has emerged as a
promising solution to these challenges by distinguishing AI-generated images
from natural content, ensuring provenance, and fostering trustworthy digital
ecosystems. This paper presents a comprehensive survey of the current state of
AI-generated image watermarking, addressing five key dimensions: (1)
formalization of image watermarking systems; (2) an overview and comparison of
diverse watermarking techniques; (3) evaluation methodologies with respect to
visual quality, capacity, and detectability; (4) vulnerabilities to malicious
attacks; and (5) prevailing challenges and future directions. The survey aims
to equip researchers with a holistic understanding of AI-generated image
watermarking technologies, thereby promoting their continued development.

</details>


### [69] [A Statistical Method for Attack-Agnostic Adversarial Attack Detection with Compressive Sensing Comparison](https://arxiv.org/abs/2510.02707)
*Chinthana Wimalasuriya,Spyros Tragoudas*

Main category: cs.CR

TL;DR: 提出一种基于压缩/未压缩神经网络对行为的统计方法，实现实时对抗攻击检测，在各种攻击类型上达到近乎完美的检测效果


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击检测方法无法有效检测未知攻击或准确检测不同攻击类型，存在检测能力不足的问题

Method: 通过比较压缩/未压缩神经网络对的行为，生成对抗攻击存在度量，在神经网络部署前建立检测基线

Result: 该方法在多种攻击类型上实现近乎完美的检测效果，显著降低误报率，经测试优于现有最先进技术

Conclusion: 该方法可靠且实用，适用于现实世界应用，为对抗攻击检测提供了有效的解决方案

Abstract: Adversarial attacks present a significant threat to modern machine learning
systems. Yet, existing detection methods often lack the ability to detect
unseen attacks or detect different attack types with a high level of accuracy.
In this work, we propose a statistical approach that establishes a detection
baseline before a neural network's deployment, enabling effective real-time
adversarial detection. We generate a metric of adversarial presence by
comparing the behavior of a compressed/uncompressed neural network pair. Our
method has been tested against state-of-the-art techniques, and it achieves
near-perfect detection across a wide range of attack types. Moreover, it
significantly reduces false positives, making it both reliable and practical
for real-world applications.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [70] [PyRadiomics-cuda: a GPU-accelerated 3D features extraction from medical images within PyRadiomics](https://arxiv.org/abs/2510.02894)
*Jakub Lisowski,Piotr Tyrakowski,Szymon Zyguła,Krzysztof Kaczmarski*

Main category: cs.DC

TL;DR: PyRadiomics-cuda是一个GPU加速的PyRadiomics库扩展，专门用于加速医学图像三维形状特征的提取计算。


<details>
  <summary>Details</summary>
Motivation: 解决从医学图像中提取三维形状特征时面临的计算挑战，特别是处理大型体积数据集时的处理时间问题。

Method: 通过将关键几何计算卸载到GPU硬件，实现计算加速，同时保持与原始PyRadiomics API的完全兼容性。

Result: 在典型计算集群、预算设备和家用设备上的测试证明，该系统在所有场景下都能显著减少处理时间。

Conclusion: PyRadiomics-cuda提供了透明的高效加速，支持可扩展的影像组学分析，对于高通量AI管道至关重要。

Abstract: PyRadiomics-cuda is a GPU-accelerated extension of the PyRadiomics library,
designed to address the computational challenges of extracting
three-dimensional shape features from medical images. By offloading key
geometric computations to GPU hardware it dramatically reduces processing times
for large volumetric datasets. The system maintains full compatibility with the
original PyRadiomics API, enabling seamless integration into existing AI
workflows without code modifications. This transparent acceleration facilitates
efficient, scalable radiomics analysis, supporting rapid feature extraction
essential for high-throughput AI pipeline. Tests performed on a typical
computational cluster, budget and home devices prove usefulness in all
scenarios. PyRadiomics-cuda is implemented in Python and C/CUDA and is freely
available under the BSD license at https://github.com/mis-wut/pyradiomics-CUDA
Additionally PyRadiomics-cuda test suite is available at
https://github.com/mis-wut/pyradiomics-cuda-data-gen. It provides detailed
handbook and sample scripts suited for different kinds of workflows plus
detailed installation instructions. The dataset used for testing is available
at Kaggle
https://www.kaggle.com/datasets/sabahesaraki/kidney-tumor-segmentation-challengekits-19

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [71] [SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting](https://arxiv.org/abs/2510.02469)
*Sung-Yeon Park,Adam Lee,Juanwu Lu,Can Cui,Luyang Jiang,Rohit Gupta,Kyungtae Han,Ahmadreza Moradipari,Ziran Wang*

Main category: cs.RO

TL;DR: SIMSplat是一个基于语言对齐高斯点云的预测性驾驶场景编辑器，能够通过自然语言提示直观地操作驾驶场景，支持详细的对象级编辑和预测性路径优化。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟驾驶模拟器在生成真实场景方面效率有限，现有框架的编辑能力不足，难以高效生成逼真的驾驶场景。

Method: 通过语言对齐的高斯点云重建技术，将自然语言与高斯重建场景对齐，支持直接查询道路对象，实现精确灵活的场景编辑。方法包括添加新对象、修改车辆和行人轨迹，并通过多智能体运动预测进行预测性路径优化。

Result: 在Waymo数据集上的实验表明，SIMSplat具有广泛的编辑能力和场景适应性，能够生成逼真的交互场景。

Conclusion: SIMSplat为驾驶场景操作提供了一个直观且强大的工具，通过语言控制和高斯点云技术的结合，显著提升了驾驶场景编辑的效率和真实感。

Abstract: Driving scene manipulation with sensor data is emerging as a promising
alternative to traditional virtual driving simulators. However, existing
frameworks struggle to generate realistic scenarios efficiently due to limited
editing capabilities. To address these challenges, we present SIMSplat, a
predictive driving scene editor with language-aligned Gaussian splatting. As a
language-controlled editor, SIMSplat enables intuitive manipulation using
natural language prompts. By aligning language with Gaussian-reconstructed
scenes, it further supports direct querying of road objects, allowing precise
and flexible editing. Our method provides detailed object-level editing,
including adding new objects and modifying the trajectories of both vehicles
and pedestrians, while also incorporating predictive path refinement through
multi-agent motion prediction to generate realistic interactions among all
agents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's
extensive editing capabilities and adaptability across a wide range of
scenarios. Project page: https://sungyeonparkk.github.io/simsplat/

</details>


### [72] [Work Zones challenge VLM Trajectory Planning: Toward Mitigation and Robust Autonomous Driving](https://arxiv.org/abs/2510.02803)
*Yifan Liao,Zhen Sun,Xiaoyun Qiu,Zixiao Zhao,Wenbing Tang,Xinlei He,Xinhu Zheng,Tianwei Zhang,Xinyi Huang,Xingshuo Han*

Main category: cs.RO

TL;DR: 本文首次系统研究了视觉语言模型在工作区轨迹规划中的能力，发现主流VLM在68%的情况下无法生成正确轨迹，并提出了REACT-Drive框架来提升规划性能。


<details>
  <summary>Details</summary>
Motivation: 工作区通常包含不规则布局、临时交通控制和动态变化的几何结构，但VLM在这些复杂环境中的轨迹规划能力尚未被探索。

Method: 通过子图挖掘和聚类分析识别失败模式，提出REACT-Drive框架，结合检索增强生成技术，将先前的失败案例转化为约束规则和可执行代码。

Result: 在ROADWork数据集上，REACT-Drive相比VLM基线将平均位移误差降低了约3倍，推理时间仅为0.58秒，远低于微调方法的17.90秒。

Conclusion: REACT-Drive在实际车辆测试中表现出强大的实用性，为VLM在自动驾驶工作区规划中的应用提供了有效解决方案。

Abstract: Visual Language Models (VLMs), with powerful multimodal reasoning
capabilities, are gradually integrated into autonomous driving by several
automobile manufacturers to enhance planning capability in challenging
environments. However, the trajectory planning capability of VLMs in work
zones, which often include irregular layouts, temporary traffic control, and
dynamically changing geometric structures, is still unexplored. To bridge this
gap, we conduct the \textit{first} systematic study of VLMs for work zone
trajectory planning, revealing that mainstream VLMs fail to generate correct
trajectories in $68.0%$ of cases. To better understand these failures, we first
identify candidate patterns via subgraph mining and clustering analysis, and
then confirm the validity of $8$ common failure patterns through human
verification. Building on these findings, we propose REACT-Drive, a trajectory
planning framework that integrates VLMs with Retrieval-Augmented Generation
(RAG). Specifically, REACT-Drive leverages VLMs to convert prior failure cases
into constraint rules and executable trajectory planning code, while RAG
retrieves similar patterns in new scenarios to guide trajectory generation.
Experimental results on the ROADWork dataset show that REACT-Drive yields a
reduction of around $3\times$ in average displacement error relative to VLM
baselines under evaluation with Qwen2.5-VL. In addition, REACT-Drive yields the
lowest inference time ($0.58$s) compared with other methods such as fine-tuning
($17.90$s). We further conduct experiments using a real vehicle in 15 work zone
scenarios in the physical world, demonstrating the strong practicality of
REACT-Drive.

</details>


### [73] [MM-Nav: Multi-View VLA Model for Robust Visual Navigation via Multi-Expert Learning](https://arxiv.org/abs/2510.03142)
*Tianyu Xu,Jiawei Chen,Jiazhao Zhang,Wenyao Zhang,Zekun Qi,Minghan Li,Zhizheng Zhang,He Wang*

Main category: cs.RO

TL;DR: 本文提出MM-Nav方法，利用视觉-语言-动作模型从合成专家数据中学习多样化导航能力，通过多视图VLA模型和动态平衡训练策略，在合成和真实环境中都展现出优异的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉导航策略虽然模仿人类使用视觉观察进行导航，但光学信息难以像LiDAR点云或深度图那样显式建模，需要智能模型和大规模数据支持。

Method: 基于预训练大语言模型和视觉基础模型实现多视图VLA模型MM-Nav，从三个具有不同导航能力的RL专家（到达、挤压、避障）收集合成专家数据，采用动态平衡训练比率的师生学习方式。

Result: 在合成环境中模型展现出强泛化能力，学生VLA模型甚至超越了RL教师模型，体现了多种能力整合的协同效应。真实世界实验进一步验证了方法的有效性。

Conclusion: 通过VLA模型整合多种导航能力的方法具有显著优势，能够实现超越传统RL专家的性能，为视觉导航提供了新的解决方案。

Abstract: Visual navigation policy is widely regarded as a promising direction, as it
mimics humans by using egocentric visual observations for navigation. However,
optical information of visual observations is difficult to be explicitly
modeled like LiDAR point clouds or depth maps, which subsequently requires
intelligent models and large-scale data. To this end, we propose to leverage
the intelligence of the Vision-Language-Action (VLA) model to learn diverse
navigation capabilities from synthetic expert data in a teacher-student manner.
Specifically, we implement the VLA model, MM-Nav, as a multi-view VLA (with 360
observations) based on pretrained large language models and visual foundation
models. For large-scale navigation data, we collect expert data from three
reinforcement learning (RL) experts trained with privileged depth information
in three challenging tailor-made environments for different navigation
capabilities: reaching, squeezing, and avoiding. We iteratively train our VLA
model using data collected online from RL experts, where the training ratio is
dynamically balanced based on performance on individual capabilities. Through
extensive experiments in synthetic environments, we demonstrate that our model
achieves strong generalization capability. Moreover, we find that our student
VLA model outperforms the RL teachers, demonstrating the synergistic effect of
integrating multiple capabilities. Extensive real-world experiments further
confirm the effectiveness of our method.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [74] [Representing Beauty: Towards a Participatory but Objective Latent Aesthetics](https://arxiv.org/abs/2510.02869)
*Alexander Michael Rusnak*

Main category: cs.CY

TL;DR: 论文探讨了神经网络识别美的能力，发现美丽图像在跨模型表示中产生更相似的对齐，表明美具有现实主义基础而非仅是社会建构的反映。


<details>
  <summary>Details</summary>
Motivation: 研究机器识别美的意义，探索深度学习系统如何建模审美判断，特别是面对形式多样性的美时神经网络的表征能力。

Method: 利用跨模型表征收敛的最新研究，分析不同数据和模态训练的模型对美与非美图像的表征对齐差异。

Result: 美丽图像在不同模型间产生更相似和一致的表征对齐，而非美图像则不会，表明美具有形式结构的现实主义基础。

Conclusion: 美在物理和文化基础上具有现实主义表征，人机共创不仅可能而且具有基础性意义，美在文化生产和机器感知中充当目的论吸引子。

Abstract: What does it mean for a machine to recognize beauty? While beauty remains a
culturally and experientially compelling but philosophically elusive concept,
deep learning systems increasingly appear capable of modeling aesthetic
judgment. In this paper, we explore the capacity of neural networks to
represent beauty despite the immense formal diversity of objects for which the
term applies. By drawing on recent work on cross-model representational
convergence, we show how aesthetic content produces more similar and aligned
representations between models which have been trained on distinct data and
modalities - while unaesthetic images do not produce more aligned
representations. This finding implies that the formal structure of beautiful
images has a realist basis - rather than only as a reflection of socially
constructed values. Furthermore, we propose that these realist representations
exist because of a joint grounding of aesthetic form in physical and cultural
substance. We argue that human perceptual and creative acts play a central role
in shaping these the latent spaces of deep learning systems, but that a realist
basis for aesthetics shows that machines are not mere creative parrots but can
produce novel creative insights from the unique vantage point of scale. Our
findings suggest that human-machine co-creation is not merely possible, but
foundational - with beauty serving as a teleological attractor in both cultural
production and machine perception.

</details>
