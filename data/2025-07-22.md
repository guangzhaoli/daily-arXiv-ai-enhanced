<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.IR](#cs.IR) [Total: 2]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [cs.LG](#cs.LG) [Total: 7]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.IV](#eess.IV) [Total: 17]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 5]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Comparative Analysis of Algorithms for the Fitting of Tessellations to 3D Image Data](https://arxiv.org/abs/2507.14268)
*Andreas Alpers,Orkun Furat,Christian Jung,Matthias Neumann,Claudia Redenbach,Aigerim Saken,Volker Schmidt*

Main category: cs.CV

TL;DR: 比较分析用于拟合三维图像数据的镶嵌模型算法策略，评估不同优化方法在生成近似体素结构时的效果。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，准确拟合三维图像数据（如多晶体和泡沫）的镶嵌模型是一个不断发展的领域，需要评估不同算法的适用性。

Method: 采用线性/非线性规划、交叉熵法的随机优化和梯度下降等方法，生成Voronoi、Laguerre和广义平衡功率图（GBPDs）来近似体素结构。

Result: 通过真实数据集评估拟合质量，结果显示模型复杂性、优化程序复杂性和近似质量之间存在权衡。

Conclusion: 研究结果为根据数据特征和应用需求选择合适方法提供了指导。

Abstract: This paper presents a comparative analysis of algorithmic strategies for
fitting tessellation models to 3D image data of materials such as polycrystals
and foams. In this steadily advancing field, we review and assess
optimization-based methods -- including linear and nonlinear programming,
stochastic optimization via the cross-entropy method, and gradient descent --
for generating Voronoi, Laguerre, and generalized balanced power diagrams
(GBPDs) that approximate voxelbased grain structures. The quality of fit is
evaluated on real-world datasets using discrepancy measures that quantify
differences in grain volume, surface area, and topology. Our results highlight
trade-offs between model complexity, the complexity of the optimization
routines involved, and the quality of approximation, providing guidance for
selecting appropriate methods based on data characteristics and application
needs.

</details>


### [2] [Semantic Segmentation based Scene Understanding in Autonomous Vehicles](https://arxiv.org/abs/2507.14303)
*Ehsan Rassekh*

Main category: cs.CV

TL;DR: 论文提出几种高效模型用于语义分割的场景理解，通过BDD100k数据集验证，并探讨不同主干网络对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 人工智能和深度学习在复杂任务中表现出色，尤其在自动驾驶领域。本文旨在通过语义分割提升场景理解能力。

Method: 使用BDD100k数据集，提出多种高效模型，并尝试不同主干网络作为编码器。

Result: 实验表明，选择合适的主干网络显著提升语义分割性能，模型在准确率、平均IoU和损失函数上均有改善。

Conclusion: 研究证实主干网络选择对语义分割至关重要，为场景理解提供了更优解决方案。

Abstract: In recent years, the concept of artificial intelligence (AI) has become a
prominent keyword because it is promising in solving complex tasks. The need
for human expertise in specific areas may no longer be needed because machines
have achieved successful results using artificial intelligence and can make the
right decisions in critical situations. This process is possible with the help
of deep learning (DL), one of the most popular artificial intelligence
technologies. One of the areas in which the use of DL is used is in the
development of self-driving cars, which is very effective and important. In
this work, we propose several efficient models to investigate scene
understanding through semantic segmentation. We use the BDD100k dataset to
investigate these models. Another contribution of this work is the usage of
several Backbones as encoders for models. The obtained results show that
choosing the appropriate backbone has a great effect on the performance of the
model for semantic segmentation. Better performance in semantic segmentation
allows us to understand better the scene and the environment around the agent.
In the end, we analyze and evaluate the proposed models in terms of accuracy,
mean IoU, and loss function, and the results show that these metrics are
improved.

</details>


### [3] [CLIPTTA: Robust Contrastive Vision-Language Test-Time Adaptation](https://arxiv.org/abs/2507.14312)
*Marc Lafon,Gustavo Adolfo Vargas Hakim,Clément Rambour,Christian Desrosier,Nicolas Thome*

Main category: cs.CV

TL;DR: CLIPTTA是一种基于梯度的测试时适应方法，通过软对比损失提升视觉语言模型在分布偏移下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（如CLIP）在零样本任务中表现良好，但在分布偏移下泛化能力不足。现有的测试时适应方法（如熵最小化）与CLIP的对比训练目标不一致，导致性能受限和失败模式。

Method: 提出CLIPTTA，利用与CLIP预训练目标一致的软对比损失进行梯度优化，并通过理论分析证明其抗崩溃性。扩展至开放集场景，使用OCE损失提升OOD检测。

Result: 在75个数据集上的实验表明，CLIPTTA优于基于熵的方法，并在多个数据集上超越现有TTA方法，表现更稳定。

Conclusion: CLIPTTA通过对齐预训练目标的软对比损失，显著提升了视觉语言模型在分布偏移下的适应能力，尤其在开放集场景中表现优异。

Abstract: Vision-language models (VLMs) like CLIP exhibit strong zero-shot capabilities
but often fail to generalize under distribution shifts. Test-time adaptation
(TTA) allows models to update at inference time without labeled data, typically
via entropy minimization. However, this objective is fundamentally misaligned
with the contrastive image-text training of VLMs, limiting adaptation
performance and introducing failure modes such as pseudo-label drift and class
collapse. We propose CLIPTTA, a new gradient-based TTA method for
vision-language models that leverages a soft contrastive loss aligned with
CLIP's pre-training objective. We provide a theoretical analysis of CLIPTTA's
gradients, showing how its batch-aware design mitigates the risk of collapse.
We further extend CLIPTTA to the open-set setting, where both in-distribution
(ID) and out-of-distribution (OOD) samples are encountered, using an Outlier
Contrastive Exposure (OCE) loss to improve OOD detection. Evaluated on 75
datasets spanning diverse distribution shifts, CLIPTTA consistently outperforms
entropy-based objectives and is highly competitive with state-of-the-art TTA
methods, outperforming them on a large number of datasets and exhibiting more
stable performance across diverse shifts.

</details>


### [4] [A Hidden Stumbling Block in Generalized Category Discovery: Distracted Attention](https://arxiv.org/abs/2507.14315)
*Qiyu Xu,Zhanxuan Hu,Yu Duan,Ercheng Pei,Yonghang Tai*

Main category: cs.CV

TL;DR: 论文提出了一种名为注意力聚焦（AF）的机制，通过剪除非信息性标记来优化广义类别发现（GCD）任务中的特征提取。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法在处理未标记数据时容易受到背景区域的干扰，导致特征提取不理想。

Method: AF由两个组件组成：标记重要性度量（TIME）和多尺度标记自适应剪枝（TAP），通过多尺度重要性评分剪除非信息性标记。

Result: 在SimGCD方法中，AF实现了高达15.4%的性能提升，且计算开销极小。

Conclusion: AF是一种轻量级、即插即用的模块，能显著提升GCD任务的性能。

Abstract: Generalized Category Discovery (GCD) aims to classify unlabeled data from
both known and unknown categories by leveraging knowledge from labeled known
categories. While existing methods have made notable progress, they often
overlook a hidden stumbling block in GCD: distracted attention. Specifically,
when processing unlabeled data, models tend to focus not only on key objects in
the image but also on task-irrelevant background regions, leading to suboptimal
feature extraction. To remove this stumbling block, we propose Attention
Focusing (AF), an adaptive mechanism designed to sharpen the model's focus by
pruning non-informative tokens. AF consists of two simple yet effective
components: Token Importance Measurement (TIME) and Token Adaptive Pruning
(TAP), working in a cascade. TIME quantifies token importance across multiple
scales, while TAP prunes non-informative tokens by utilizing the multi-scale
importance scores provided by TIME. AF is a lightweight, plug-and-play module
that integrates seamlessly into existing GCD methods with minimal computational
overhead. When incorporated into one prominent GCD method, SimGCD, AF achieves
up to 15.4% performance improvement over the baseline with minimal
computational overhead. The implementation code is provided in
https://github.com/Afleve/AFGCD.

</details>


### [5] [Hallucination Score: Towards Mitigating Hallucinations in Generative Image Super-Resolution](https://arxiv.org/abs/2507.14367)
*Weiming Ren,Raghav Goyal,Zhiming Hu,Tristan Ty Aumentado-Armstrong,Iqbal Mohomed,Alex Levinshtein*

Main category: cs.CV

TL;DR: 论文提出了一种新方法，通过多模态大语言模型（MLLM）评估生成超分辨率（GSR）中的幻觉问题，并提出了“幻觉分数”（HS）和基于深度特征的奖励函数来缓解幻觉。


<details>
  <summary>Details</summary>
Motivation: 生成超分辨率（GSR）模型在感知质量上表现优异，但存在幻觉问题（生成细节与低分辨率图像或真实图像不匹配），这一问题未被充分研究，限制了实际应用。

Method: 利用MLLM构建提示来评估幻觉元素并生成HS，同时发现某些深度特征距离与HS强相关，提出用这些特征作为可微奖励函数来对齐GSR模型。

Result: HS与人类评估高度一致，且为现有超分辨率指标提供了补充信息；深度特征奖励函数有效缓解了幻觉问题。

Conclusion: 通过HS和深度特征奖励函数，论文为GSR模型的幻觉问题提供了量化方法和解决方案，提升了模型的实用性和可靠性。

Abstract: Generative super-resolution (GSR) currently sets the state-of-the-art in
terms of perceptual image quality, overcoming the "regression-to-the-mean" blur
of prior non-generative models. However, from a human perspective, such models
do not fully conform to the optimal balance between quality and fidelity.
Instead, a different class of artifacts, in which generated details fail to
perceptually match the low resolution image (LRI) or ground-truth image (GTI),
is a critical but under studied issue in GSR, limiting its practical
deployments. In this work, we focus on measuring, analyzing, and mitigating
these artifacts (i.e., "hallucinations"). We observe that hallucinations are
not well-characterized with existing image metrics or quality models, as they
are orthogonal to both exact fidelity and no-reference quality. Instead, we
take advantage of a multimodal large language model (MLLM) by constructing a
prompt that assesses hallucinatory visual elements and generates a
"Hallucination Score" (HS). We find that our HS is closely aligned with human
evaluations, and also provides complementary insights to prior image metrics
used for super-resolution (SR) models. In addition, we find certain deep
feature distances have strong correlations with HS. We therefore propose to
align the GSR models by using such features as differentiable reward functions
to mitigate hallucinations.

</details>


### [6] [DUSTrack: Semi-automated point tracking in ultrasound videos](https://arxiv.org/abs/2507.14368)
*Praneeth Namburi,Roger Pallarès-López,Jessica Rosendorf,Duarte Folgado,Brian W. Anthony*

Main category: cs.CV

TL;DR: DUSTrack是一个结合深度学习和光流的半自动化工具包，用于B超视频中的点跟踪，解决了噪声和运动模糊问题，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: B超图像中的斑点噪声、低边缘对比度和平面外运动使得组织运动跟踪困难，影响临床和研究应用。

Method: 结合深度学习和光流技术，提供高质量跟踪，并包含图形界面支持数据生成和模型优化。

Result: DUSTrack在多种解剖结构和运动模式下表现优异，精度优于零样本跟踪器，与专用方法相当。

Conclusion: DUSTrack是一个开源工具，适用于临床和生物力学研究，具有广泛的应用潜力。

Abstract: Ultrasound technology enables safe, non-invasive imaging of dynamic tissue
behavior, making it a valuable tool in medicine, biomechanics, and sports
science. However, accurately tracking tissue motion in B-mode ultrasound
remains challenging due to speckle noise, low edge contrast, and out-of-plane
movement. These challenges complicate the task of tracking anatomical landmarks
over time, which is essential for quantifying tissue dynamics in many clinical
and research applications. This manuscript introduces DUSTrack (Deep learning
and optical flow-based toolkit for UltraSound Tracking), a semi-automated
framework for tracking arbitrary points in B-mode ultrasound videos. We combine
deep learning with optical flow to deliver high-quality and robust tracking
across diverse anatomical structures and motion patterns. The toolkit includes
a graphical user interface that streamlines the generation of high-quality
training data and supports iterative model refinement. It also implements a
novel optical-flow-based filtering technique that reduces high-frequency
frame-to-frame noise while preserving rapid tissue motion. DUSTrack
demonstrates superior accuracy compared to contemporary zero-shot point
trackers and performs on par with specialized methods, establishing its
potential as a general and foundational tool for clinical and biomechanical
research. We demonstrate DUSTrack's versatility through three use cases:
cardiac wall motion tracking in echocardiograms, muscle deformation analysis
during reaching tasks, and fascicle tracking during ankle plantarflexion. As an
open-source solution, DUSTrack offers a powerful, flexible framework for point
tracking to quantify tissue motion from ultrasound videos. DUSTrack is
available at https://github.com/praneethnamburi/DUSTrack.

</details>


### [7] [CRAFT: A Neuro-Symbolic Framework for Visual Functional Affordance Grounding](https://arxiv.org/abs/2507.14426)
*Zhou Chen,Joe Lin,Sathyanarayanan N. Aakur*

Main category: cs.CV

TL;DR: CRAFT是一个神经符号框架，用于可解释的affordance grounding，通过结合ConceptNet和CLIP的视觉证据，迭代优化预测。


<details>
  <summary>Details</summary>
Motivation: 解决场景理解中对象与动作关联的透明性和可解释性问题。

Method: 整合ConceptNet的常识先验和CLIP的视觉证据，通过基于能量的推理循环迭代优化预测。

Result: 在多对象、无标签设置中提高了准确性和可解释性。

Conclusion: CRAFT为稳健且可信的场景理解提供了新方向。

Abstract: We introduce CRAFT, a neuro-symbolic framework for interpretable affordance
grounding, which identifies the objects in a scene that enable a given action
(e.g., "cut"). CRAFT integrates structured commonsense priors from ConceptNet
and language models with visual evidence from CLIP, using an energy-based
reasoning loop to refine predictions iteratively. This process yields
transparent, goal-driven decisions to ground symbolic and perceptual
structures. Experiments in multi-object, label-free settings demonstrate that
CRAFT enhances accuracy while improving interpretability, providing a step
toward robust and trustworthy scene understanding.

</details>


### [8] [Adaptive 3D Gaussian Splatting Video Streaming](https://arxiv.org/abs/2507.14432)
*Han Gong,Qiyue Li,Zhi Liu,Hao Zhou,Peng Yuan Zhou,Zhu Li,Jie Li*

Main category: cs.CV

TL;DR: 提出了一种基于高斯变形场的3DGS视频流媒体框架，通过混合显著性分块和差异化质量建模，实现了高效压缩和带宽适应。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）视频的数据量大且传输复杂，传统流媒体方法难以应对。

Method: 设计了基于高斯变形场的3DGS视频构建方法，结合混合显著性分块和差异化质量建模。

Result: 实验表明，该方法在视频质量、压缩效率和传输速率上优于现有方法。

Conclusion: 提出的框架有效解决了3DGS视频流媒体的挑战，具有实际应用潜力。

Abstract: The advent of 3D Gaussian splatting (3DGS) has significantly enhanced the
quality of volumetric video representation. Meanwhile, in contrast to
conventional volumetric video, 3DGS video poses significant challenges for
streaming due to its substantially larger data volume and the heightened
complexity involved in compression and transmission. To address these issues,
we introduce an innovative framework for 3DGS volumetric video streaming.
Specifically, we design a 3DGS video construction method based on the Gaussian
deformation field. By employing hybrid saliency tiling and differentiated
quality modeling of 3DGS video, we achieve efficient data compression and
adaptation to bandwidth fluctuations while ensuring high transmission quality.
Then we build a complete 3DGS video streaming system and validate the
transmission performance. Through experimental evaluation, our method
demonstrated superiority over existing approaches in various aspects, including
video quality, compression effectiveness, and transmission rate.

</details>


### [9] [IRGPT: Understanding Real-world Infrared Image with Bi-cross-modal Curriculum on Large-scale Benchmark](https://arxiv.org/abs/2507.14449)
*Zhe Cao,Jin Zhang,Ruiheng Zhang*

Main category: cs.CV

TL;DR: IRGPT是首个针对真实红外图像的多模态大语言模型，基于大规模红外-文本数据集（IR-TD），通过双跨模态课程迁移学习策略，在9项任务中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 真实红外图像因缺乏对齐的文本数据和特定领域特性，现有方法依赖合成图像，无法捕捉红外模态的独特特征。

Method: 提出IR-TD数据集（26万真实红外-文本对），结合LLM生成和规则描述，并设计双跨模态课程迁移学习策略。

Result: 在9项任务（如识别、定位）中，IRGPT性能优于更大规模的模型。

Conclusion: IRGPT通过真实数据集和创新学习策略，解决了红外图像多模态任务的挑战，展现了卓越性能。

Abstract: Real-world infrared imagery presents unique challenges for vision-language
models due to the scarcity of aligned text data and domain-specific
characteristics. Although existing methods have advanced the field, their
reliance on synthetic infrared images generated through style transfer from
visible images, which limits their ability to capture the unique
characteristics of the infrared modality. To address this, we propose IRGPT,
the first multi-modal large language model for real-world infrared images,
built upon a large-scale InfraRed-Text Dataset (IR-TD) comprising over 260K
authentic image-text pairs. The proposed IR-TD dataset contains real infrared
images paired with meticulously handcrafted texts, where the initial drafts
originated from two complementary processes: (1) LLM-generated descriptions of
visible images, and (2) rule-based descriptions of annotations. Furthermore, we
introduce a bi-cross-modal curriculum transfer learning strategy that
systematically transfers knowledge from visible to infrared domains by
considering the difficulty scores of both infrared-visible and infrared-text.
Evaluated on a benchmark of 9 tasks (e.g., recognition, grounding), IRGPT
achieves state-of-the-art performance even compared with larger-scale models.

</details>


### [10] [GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration](https://arxiv.org/abs/2507.14452)
*Weikang Gu,Mingyue Han,Li Xue,Heng Dong,Changcai Yang,Riqing Chen,Lifang Wei*

Main category: cs.CV

TL;DR: 提出了一种基于Gestalt原则的并行交互网络（GPI-Net），用于点云配准中的高质量对应关系识别。


<details>
  <summary>Details</summary>
Motivation: 解决局部和全局特征融合中的冗余和复杂空间关系问题。

Method: 利用Gestalt原则设计正交几何一致性策略，结合自注意力和交叉注意力机制，以及双路径多粒度并行交互聚合块。

Result: 在多个挑战性任务中表现优于现有方法。

Conclusion: GPI-Net通过Gestalt原则有效提升了点云配准中对应关系的质量。

Abstract: The accurate identification of high-quality correspondences is a prerequisite
task in feature-based point cloud registration. However, it is extremely
challenging to handle the fusion of local and global features due to feature
redundancy and complex spatial relationships. Given that Gestalt principles
provide key advantages in analyzing local and global relationships, we propose
a novel Gestalt-guided Parallel Interaction Network via orthogonal geometric
consistency (GPI-Net) in this paper. It utilizes Gestalt principles to
facilitate complementary communication between local and global information.
Specifically, we introduce an orthogonal integration strategy to optimally
reduce redundant information and generate a more compact global structure for
high-quality correspondences. To capture geometric features in correspondences,
we leverage a Gestalt Feature Attention (GFA) block through a hybrid
utilization of self-attention and cross-attention mechanisms. Furthermore, to
facilitate the integration of local detail information into the global
structure, we design an innovative Dual-path Multi-Granularity parallel
interaction aggregation (DMG) block to promote information exchange across
different granularities. Extensive experiments on various challenging tasks
demonstrate the superior performance of our proposed GPI-Net in comparison to
existing methods. The code will be released at https://github.com/gwk/GPI-Net.

</details>


### [11] [Adaptive 3D Gaussian Splatting Video Streaming: Visual Saliency-Aware Tiling and Meta-Learning-Based Bitrate Adaptation](https://arxiv.org/abs/2507.14454)
*Han Gong,Qiyue Li,Jie Li,Zhi Liu*

Main category: cs.CV

TL;DR: 本文提出了一种自适应3D高斯泼溅视频流媒体解决方案，包括基于显著性的分块技术、质量评估框架和基于元学习的比特率自适应算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 3D高斯泼溅视频流媒体在提供沉浸式体验方面表现出色，但分块、质量评估和比特率自适应等基础问题仍需深入研究。

Method: 提出自适应分块技术（结合显著性和时空特征）、多质量版本编码、新型质量评估框架（联合评估3D表示和2D渲染质量）和基于元学习的比特率自适应算法。

Result: 实验表明，所提方法在性能上显著优于现有技术。

Conclusion: 本文为3D高斯泼溅视频流媒体提供了一套全面的解决方案，解决了关键挑战并提升了性能。

Abstract: 3D Gaussian splatting video (3DGS) streaming has recently emerged as a
research hotspot in both academia and industry, owing to its impressive ability
to deliver immersive 3D video experiences. However, research in this area is
still in its early stages, and several fundamental challenges, such as tiling,
quality assessment, and bitrate adaptation, require further investigation. In
this paper, we tackle these challenges by proposing a comprehensive set of
solutions. Specifically, we propose an adaptive 3DGS tiling technique guided by
saliency analysis, which integrates both spatial and temporal features. Each
tile is encoded into versions possessing dedicated deformation fields and
multiple quality levels for adaptive selection. We also introduce a novel
quality assessment framework for 3DGS video that jointly evaluates
spatial-domain degradation in 3DGS representations during streaming and the
quality of the resulting 2D rendered images. Additionally, we develop a
meta-learning-based adaptive bitrate algorithm specifically tailored for 3DGS
video streaming, achieving optimal performance across varying network
conditions. Extensive experiments demonstrate that our proposed approaches
significantly outperform state-of-the-art methods.

</details>


### [12] [GEMINUS: Dual-aware Global and Scene-Adaptive Mixture-of-Experts for End-to-End Autonomous Driving](https://arxiv.org/abs/2507.14456)
*Chi Wan,Yixin Cui,Jiatong Du,Shuo Yang,Yulong Bai,Yanjun Huang*

Main category: cs.CV

TL;DR: GEMINUS是一个混合专家端到端自动驾驶框架，通过全局专家和场景自适应专家组的耦合，实现多样场景下的自适应和鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 现有单模式规划方法难以处理多样场景，需要学习多样化驾驶技能。

Method: 提出GEMINUS框架，包含全局专家、场景自适应专家组和双感知路由器，动态激活专家模块。

Result: 在Bench2Drive基准测试中表现优异，驾驶分数和成功率均达到SOTA，单目视觉输入下仍有效。

Conclusion: GEMINUS通过混合专家方法显著提升自动驾驶性能，适用于复杂多样场景。

Abstract: End-to-end autonomous driving requires adaptive and robust handling of
complex and diverse traffic environments. However, prevalent single-mode
planning methods attempt to learn an overall policy while struggling to acquire
diversified driving skills to handle diverse scenarios. Therefore, this paper
proposes GEMINUS, a Mixture-of-Experts end-to-end autonomous driving framework
featuring a Global Expert, a Scene-Adaptive Experts Group, and equipped with a
Dual-aware Router. Specifically, the Global Expert is trained on the overall
dataset, possessing robust performance. The Scene-Adaptive Experts are trained
on corresponding scene subsets, achieving adaptive performance. The Dual-aware
Router simultaneously considers scenario-level features and routing uncertainty
to dynamically activate expert modules. Through the effective coupling of the
Global Expert and the Scene-Adaptive Experts Group via the Dual-aware Router,
GEMINUS achieves adaptive and robust performance in diverse scenarios. GEMINUS
outperforms existing methods in the Bench2Drive closed-loop benchmark and
achieves state-of-the-art performance in Driving Score and Success Rate, even
with only monocular vision input. Furthermore, ablation studies demonstrate
significant improvements over the original single-expert baseline: 7.67% in
Driving Score, 22.06% in Success Rate, and 19.41% in MultiAbility-Mean. The
code will be available at https://github.com/newbrains1/GEMINUS.

</details>


### [13] [VisGuard: Securing Visualization Dissemination through Tamper-Resistant Data Retrieval](https://arxiv.org/abs/2507.14459)
*Huayuan Ye,Juntong Chen,Shenzhuo Zhang,Yipeng Zhang,Changbo Wang,Chenhui Li*

Main category: cs.CV

TL;DR: VisGuard是一个抗篡改的可视化图像数据检索框架，通过嵌入元数据链接来保护可视化图像中的关键信息。


<details>
  <summary>Details</summary>
Motivation: 当前的可视化图像在传播过程中容易丢失关键信息（如源代码、交互功能和元数据），且现有方法对常见的图像篡改（如裁剪和编辑）缺乏抵抗力。

Method: VisGuard采用重复数据平铺、可逆信息广播和基于锚点的裁剪定位方案等技术，增强嵌入数据的鲁棒性。

Result: VisGuard在数据检索准确性、嵌入容量和抗篡改安全性方面表现优异，支持交互式图表重建、篡改检测和版权保护等应用。

Conclusion: VisGuard能有效保护和促进可视化传播及信息传递。

Abstract: The dissemination of visualizations is primarily in the form of raster
images, which often results in the loss of critical information such as source
code, interactive features, and metadata. While previous methods have proposed
embedding metadata into images to facilitate Visualization Image Data Retrieval
(VIDR), most existing methods lack practicability since they are fragile to
common image tampering during online distribution such as cropping and editing.
To address this issue, we propose VisGuard, a tamper-resistant VIDR framework
that reliably embeds metadata link into visualization images. The embedded data
link remains recoverable even after substantial tampering upon images. We
propose several techniques to enhance robustness, including repetitive data
tiling, invertible information broadcasting, and an anchor-based scheme for
crop localization. VisGuard enables various applications, including interactive
chart reconstruction, tampering detection, and copyright protection. We conduct
comprehensive experiments on VisGuard's superior performance in data retrieval
accuracy, embedding capacity, and security against tampering and steganalysis,
demonstrating VisGuard's competence in facilitating and safeguarding
visualization dissemination and information conveyance.

</details>


### [14] [OptiCorNet: Optimizing Sequence-Based Context Correlation for Visual Place Recognition](https://arxiv.org/abs/2507.14477)
*Zhenyu Li,Tianyi Shang,Pengjie Xu,Ruirui Zhang,Fanchen Kong*

Main category: cs.CV

TL;DR: OptiCorNet是一个新颖的序列建模框架，通过结合空间特征提取和时间差分，提升了动态和感知混淆环境中的视觉地点识别（VPR）性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的VPR方法主要关注单帧嵌入，忽略了图像序列中的时间连贯性。

Method: OptiCorNet采用轻量级1D卷积编码器和可学习的差分时间算子（DSD），结合LSTM和四元组损失，实现端到端的序列级嵌入学习。

Result: 在多个公开基准测试中，OptiCorNet在季节和视角变化下优于现有方法。

Conclusion: OptiCorNet通过端到端学习序列级嵌入，显著提升了动态环境中的地点识别效果。

Abstract: Visual Place Recognition (VPR) in dynamic and perceptually aliased
environments remains a fundamental challenge for long-term localization.
Existing deep learning-based solutions predominantly focus on single-frame
embeddings, neglecting the temporal coherence present in image sequences. This
paper presents OptiCorNet, a novel sequence modeling framework that unifies
spatial feature extraction and temporal differencing into a differentiable,
end-to-end trainable module. Central to our approach is a lightweight 1D
convolutional encoder combined with a learnable differential temporal operator,
termed Differentiable Sequence Delta (DSD), which jointly captures short-term
spatial context and long-range temporal transitions. The DSD module models
directional differences across sequences via a fixed-weight differencing
kernel, followed by an LSTM-based refinement and optional residual projection,
yielding compact, discriminative descriptors robust to viewpoint and appearance
shifts. To further enhance inter-class separability, we incorporate a
quadruplet loss that optimizes both positive alignment and multi-negative
divergence within each batch. Unlike prior VPR methods that treat temporal
aggregation as post-processing, OptiCorNet learns sequence-level embeddings
directly, enabling more effective end-to-end place recognition. Comprehensive
evaluations on multiple public benchmarks demonstrate that our approach
outperforms state-of-the-art baselines under challenging seasonal and viewpoint
variations.

</details>


### [15] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT提出了一种无需真实数据的ViT量化方法，通过合成高质量样本和激活校正矩阵，显著提升了量化模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成样本时未能充分平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: 提出DFQ-ViT，按难度递增顺序合成样本，并引入激活校正矩阵以对齐中间层激活分布。

Result: 实验表明DFQ-ViT优于现有DFQ方法，性能接近真实数据量化模型，如3位量化DeiT-T性能提升4.29%。

Conclusion: DFQ-ViT无需微调，降低计算开销和部署门槛，符合绿色学习原则，适用于资源受限环境。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [16] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，生成细粒度点云。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的挑战，现有方法局限于特定输入类别。

Method: 设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），结合双通道控制门和分层特征融合机制。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够处理稀疏数据和未见类别。

Conclusion: 该方法在生成细粒度点云和泛化能力方面表现出色。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [17] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型多模态大语言模型架构，通过令牌压缩技术解决病理学全切片图像（WSI）视觉问答（VQA）中的高计算资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 病理学全切片图像的高分辨率和长上下文长度对多模态大语言模型提出了巨大挑战，现有方法在生成能力和资源消耗方面存在不足。

Method: 提出TCP-LLaVA，通过可训练的压缩令牌聚合视觉和文本信息，仅将压缩后的令牌输入语言模型，显著减少计算成本。

Result: 在十种TCGA肿瘤亚型的实验中，TCP-LLaVA在VQA准确性上优于现有基线，同时大幅降低训练资源消耗。

Conclusion: TCP-LLaVA为病理学WSI的VQA提供了一种高效且准确的解决方案，具有实际应用潜力。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [18] [Motion Segmentation and Egomotion Estimation from Event-Based Normal Flow](https://arxiv.org/abs/2507.14500)
*Zhiyuan Hua,Dehao Yuan,Cornelia Fermüller*

Main category: cs.CV

TL;DR: 提出了一种基于事件法向流的运动分割和自运动估计框架，适用于神经形态视觉传感器，无需完整光流计算即可实现高精度分割和运动估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖光流或深度估计，而神经形态视觉传感器的事件数据稀疏且时间分辨率高，需要一种更高效的方法。

Method: 利用事件法向流、场景结构和惯性测量的几何约束，通过迭代过分割、残差分析和层次聚类实现运动分割和自运动估计。

Result: 在EVIMO2v2数据集上验证了方法的准确性，尤其在物体边界表现优异。

Conclusion: 该方法在实时机器人和导航应用中具有显著优势和潜力。

Abstract: This paper introduces a robust framework for motion segmentation and
egomotion estimation using event-based normal flow, tailored specifically for
neuromorphic vision sensors. In contrast to traditional methods that rely
heavily on optical flow or explicit depth estimation, our approach exploits the
sparse, high-temporal-resolution event data and incorporates geometric
constraints between normal flow, scene structure, and inertial measurements.
The proposed optimization-based pipeline iteratively performs event
over-segmentation, isolates independently moving objects via residual analysis,
and refines segmentations using hierarchical clustering informed by motion
similarity and temporal consistency. Experimental results on the EVIMO2v2
dataset validate that our method achieves accurate segmentation and
translational motion estimation without requiring full optical flow
computation. This approach demonstrates significant advantages at object
boundaries and offers considerable potential for scalable, real-time robotic
and navigation applications.

</details>


### [19] [Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey](https://arxiv.org/abs/2507.14501)
*Jiahui Zhang,Yuelei Li,Anpei Chen,Muyu Xu,Kunhao Liu,Jianyuan Wang,Xiao-Xiao Long,Hanxue Liang,Zexiang Xu,Hao Su,Christian Theobalt,Christian Rupprecht,Andrea Vedaldi,Hanspeter Pfister,Shijian Lu,Fangneng Zhan*

Main category: cs.CV

TL;DR: 该论文综述了基于前馈方法的3D重建和视图合成技术，分类了不同表示架构，并探讨了其应用、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算复杂且不适用于实时场景，深度学习驱动的快速、通用前馈方法成为研究热点。

Method: 通过分类点云、3D高斯泼溅、NeRF等表示架构，分析关键任务如无姿态重建、动态3D重建等。

Result: 总结了前馈方法在数字人、SLAM等领域的应用，并提供了数据集和评估协议的详细统计。

Conclusion: 前馈方法有望推动3D视觉领域的发展，但仍需解决开放研究挑战。

Abstract: 3D reconstruction and view synthesis are foundational problems in computer
vision, graphics, and immersive technologies such as augmented reality (AR),
virtual reality (VR), and digital twins. Traditional methods rely on
computationally intensive iterative optimization in a complex chain, limiting
their applicability in real-world scenarios. Recent advances in feed-forward
approaches, driven by deep learning, have revolutionized this field by enabling
fast and generalizable 3D reconstruction and view synthesis. This survey offers
a comprehensive review of feed-forward techniques for 3D reconstruction and
view synthesis, with a taxonomy according to the underlying representation
architectures including point cloud, 3D Gaussian Splatting (3DGS), Neural
Radiance Fields (NeRF), etc. We examine key tasks such as pose-free
reconstruction, dynamic 3D reconstruction, and 3D-aware image and video
synthesis, highlighting their applications in digital humans, SLAM, robotics,
and beyond. In addition, we review commonly used datasets with detailed
statistics, along with evaluation protocols for various downstream tasks. We
conclude by discussing open research challenges and promising directions for
future work, emphasizing the potential of feed-forward approaches to advance
the state of the art in 3D vision.

</details>


### [20] [DCHM: Depth-Consistent Human Modeling for Multiview Detection](https://arxiv.org/abs/2507.14505)
*Jiahao Ma,Tianyu Wang,Miaomiao Liu,David Ahmedt-Aristizabal,Chuong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种名为DCHM的框架，通过深度一致性建模和超像素高斯泼溅技术，在多视角、大规模和拥挤场景中实现精确的行人检测。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多视角行人检测中常引入噪声且精度低，依赖昂贵的3D标注且泛化能力差。

Method: 采用深度一致性建模（DCHM）框架，结合超像素高斯泼溅技术，实现多视角深度一致性和全局坐标融合。

Result: 显著减少了建模过程中的噪声，优于现有方法，并在挑战性场景中首次实现行人重建和多视角分割。

Conclusion: DCHM框架在多视角行人检测中表现出色，无需依赖人工标注，适用于复杂场景。

Abstract: Multiview pedestrian detection typically involves two stages: human modeling
and pedestrian localization. Human modeling represents pedestrians in 3D space
by fusing multiview information, making its quality crucial for detection
accuracy. However, existing methods often introduce noise and have low
precision. While some approaches reduce noise by fitting on costly multiview 3D
annotations, they often struggle to generalize across diverse scenes. To
eliminate reliance on human-labeled annotations and accurately model humans, we
propose Depth-Consistent Human Modeling (DCHM), a framework designed for
consistent depth estimation and multiview fusion in global coordinates.
Specifically, our proposed pipeline with superpixel-wise Gaussian Splatting
achieves multiview depth consistency in sparse-view, large-scaled, and crowded
scenarios, producing precise point clouds for pedestrian localization.
Extensive validations demonstrate that our method significantly reduces noise
during human modeling, outperforming previous state-of-the-art baselines.
Additionally, to our knowledge, DCHM is the first to reconstruct pedestrians
and perform multiview segmentation in such a challenging setting. Code is
available on the \href{https://jiahao-ma.github.io/DCHM/}{project page}.

</details>


### [21] [ArtiMuse: Fine-Grained Image Aesthetics Assessment with Joint Scoring and Expert-Level Understanding](https://arxiv.org/abs/2507.14533)
*Shuo Cao,Nan Ma,Jiayang Li,Xiaohui Li,Lihao Shao,Kaiwen Zhu,Yu Zhou,Yuandong Pu,Jiarui Wu,Jiaquan Wang,Bo Qu,Wenhai Wang,Yu Qiao,Dajuin Yao,Yihao Liu*

Main category: cs.CV

TL;DR: ArtiMuse是一个基于多模态大语言模型的图像美学评估模型，具备联合评分和专家级理解能力，并发布了首个专家策划的数据集ArtiMuse-10K。


<details>
  <summary>Details</summary>
Motivation: 随着教育应用、艺术创作和AI生成内容技术的快速发展，对全面的图像美学评估方法的需求增加，现有方法存在模态偏差和缺乏细粒度属性分解的问题。

Method: 提出了ArtiMuse模型，结合评分和专家级理解能力，并构建了ArtiMuse-10K数据集，包含10,000张图像，标注了8维属性和整体评分。

Result: ArtiMuse模型和ArtiMuse-10K数据集将公开，以推动该领域的发展。

Conclusion: ArtiMuse解决了现有方法的局限性，为图像美学评估提供了更全面的解决方案。

Abstract: The rapid advancement of educational applications, artistic creation, and
AI-generated content (AIGC) technologies has substantially increased practical
requirements for comprehensive Image Aesthetics Assessment (IAA), particularly
demanding methods capable of delivering both quantitative scoring and
professional understanding. Multimodal Large Language Model (MLLM)-based IAA
methods demonstrate stronger perceptual and generalization capabilities
compared to traditional approaches, yet they suffer from modality bias
(score-only or text-only) and lack fine-grained attribute decomposition,
thereby failing to support further aesthetic assessment. In this paper, we
present:(1) ArtiMuse, an innovative MLLM-based IAA model with Joint Scoring and
Expert-Level Understanding capabilities; (2) ArtiMuse-10K, the first
expert-curated image aesthetic dataset comprising 10,000 images spanning 5 main
categories and 15 subcategories, each annotated by professional experts with
8-dimensional attributes analysis and a holistic score. Both the model and
dataset will be made public to advance the field.

</details>


### [22] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，将手语实时翻译为字幕，帮助听障人士在视频会议中更顺畅地交流。


<details>
  <summary>Details</summary>
Motivation: 听障人士在视频会议中更倾向于使用手语而非打字，但普通人往往不理解手语，因此需要一种工具消除沟通障碍。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的实时翻译。

Result: 通过浏览器扩展实现手语到字幕的自动翻译，提升听障人士的沟通效率。

Conclusion: 该工具能有效减少听障人士与普通人之间的沟通障碍，尤其在视频会议场景中具有重要意义。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [23] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了使用Florence模型在胃肠道内窥镜视觉问答任务中的方法，通过领域特定增强提高泛化能力，并在KASVIR数据集上验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 探索大规模多模态基础模型在医学视觉问答中的潜力，为临床相关任务提供准确答案。

Method: 采用Florence模型作为VQA流程主干，结合视觉和文本编码器，并应用领域特定数据增强。

Result: 在KASVIR数据集上微调Florence模型后，在官方挑战指标上表现出色。

Conclusion: 展示了大型多模态模型在医学VQA中的潜力，为未来研究提供了基线，并公开了代码。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [24] [Synthesizing Images on Perceptual Boundaries of ANNs for Uncovering Human Perceptual Variability on Facial Expressions](https://arxiv.org/abs/2507.14549)
*Haotian Deng,Chi Zhang,Chen Wei,Quanying Liu*

Main category: cs.CV

TL;DR: 研究探讨了人工神经网络（ANN）与人类情感感知之间的关系，发现ANN分类模糊的刺激同样引发人类感知差异，并通过行为数据优化ANN模型。


<details>
  <summary>Details</summary>
Motivation: 解决情感认知科学中外部情感刺激与人类内部体验关系的建模问题，尤其是探索ANN在个体感知差异中的表现。

Method: 提出了一种新的感知边界采样方法，生成位于ANN决策边界的模糊面部表情刺激，构建varEmotion数据集，并通过大规模人类行为实验验证。

Result: 发现ANN分类模糊的刺激会引发人类感知不确定性，且通过行为数据微调ANN后，其预测与人类群体及个体感知模式一致。

Conclusion: 建立了ANN决策边界与人类感知变异性之间的系统性联系，为个性化情感解释建模提供了新思路。

Abstract: A fundamental challenge in affective cognitive science is to develop models
that accurately capture the relationship between external emotional stimuli and
human internal experiences. While ANNs have demonstrated remarkable accuracy in
facial expression recognition, their ability to model inter-individual
differences in human perception remains underexplored. This study investigates
the phenomenon of high perceptual variability-where individuals exhibit
significant differences in emotion categorization even when viewing the same
stimulus. Inspired by the similarity between ANNs and human perception, we
hypothesize that facial expression samples that are ambiguous for ANN
classifiers also elicit divergent perceptual judgments among human observers.
To examine this hypothesis, we introduce a novel perceptual boundary sampling
method to generate facial expression stimuli that lie along ANN decision
boundaries. These ambiguous samples form the basis of the varEmotion dataset,
constructed through large-scale human behavioral experiments. Our analysis
reveals that these ANN-confusing stimuli also provoke heightened perceptual
uncertainty in human participants, highlighting shared computational principles
in emotion perception. Finally, by fine-tuning ANN representations using
behavioral data, we achieve alignment between ANN predictions and both
group-level and individual-level human perceptual patterns. Our findings
establish a systematic link between ANN decision boundaries and human
perceptual variability, offering new insights into personalized modeling of
emotional interpretation.

</details>


### [25] [Clutter Detection and Removal by Multi-Objective Analysis for Photographic Guidance](https://arxiv.org/abs/2507.14553)
*Xiaoran Wu*

Main category: cs.CV

TL;DR: 论文提出了一种相机引导系统，帮助摄影爱好者识别和去除照片中的杂乱元素，提升照片美学质量。


<details>
  <summary>Details</summary>
Motivation: 摄影爱好者常因疏忽或经验不足在照片中留下杂乱元素，影响照片情感表达和故事传达。

Method: 系统通过算法评估对象对照片美学和内容的贡献，提供交互式杂乱识别工具，并结合生成对抗网络实现高分辨率图像修复。

Result: 用户研究表明，系统能帮助用户更高效地识别杂乱元素并提升照片质量。

Conclusion: 该系统通过灵活界面和精准算法，有效帮助用户改善摄影作品。

Abstract: Clutter in photos is a distraction preventing photographers from conveying
the intended emotions or stories to the audience. Photography amateurs
frequently include clutter in their photos due to unconscious negligence or the
lack of experience in creating a decluttered, aesthetically appealing scene for
shooting. We are thus motivated to develop a camera guidance system that
provides solutions and guidance for clutter identification and removal. We
estimate and visualize the contribution of objects to the overall aesthetics
and content of a photo, based on which users can interactively identify
clutter. Suggestions on getting rid of clutter, as well as a tool that removes
cluttered objects computationally, are provided to guide users to deal with
different kinds of clutter and improve their photographic work. Two technical
novelties underpin interactions in our system: a clutter distinguishment
algorithm with aesthetics evaluations for objects and an iterative image
inpainting algorithm based on generative adversarial nets that reconstructs
missing regions of removed objects for high-resolution images. User studies
demonstrate that our system provides flexible interfaces and accurate
algorithms that allow users to better identify distractions and take higher
quality images within less time.

</details>


### [26] [Descrip3D: Enhancing Large Language Model-based 3D Scene Understanding with Object-Level Text Descriptions](https://arxiv.org/abs/2507.14555)
*Jintang Xue,Ganning Zhao,Jie-En Yao,Hong-En Chen,Yue Hu,Meida Chen,Suya You,C. -C. Jay Kuo*

Main category: cs.CV

TL;DR: Descrip3D是一个新颖的框架，通过自然语言显式编码物体间关系，提升3D场景理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有3D场景-语言模型在关系理解上表现不足，视觉嵌入无法充分捕捉物体角色和交互。

Method: Descrip3D通过文本描述增强物体表示，结合嵌入融合和提示级注入实现双层次集成。

Result: 在五个基准数据集上，Descrip3D均优于基线模型。

Conclusion: 语言引导的关系表示能有效提升复杂室内场景的理解能力。

Abstract: Understanding 3D scenes goes beyond simply recognizing objects; it requires
reasoning about the spatial and semantic relationships between them. Current 3D
scene-language models often struggle with this relational understanding,
particularly when visual embeddings alone do not adequately convey the roles
and interactions of objects. In this paper, we introduce Descrip3D, a novel and
powerful framework that explicitly encodes the relationships between objects
using natural language. Unlike previous methods that rely only on 2D and 3D
embeddings, Descrip3D enhances each object with a textual description that
captures both its intrinsic attributes and contextual relationships. These
relational cues are incorporated into the model through a dual-level
integration: embedding fusion and prompt-level injection. This allows for
unified reasoning across various tasks such as grounding, captioning, and
question answering, all without the need for task-specific heads or additional
supervision. When evaluated on five benchmark datasets, including ScanRefer,
Multi3DRefer, ScanQA, SQA3D, and Scan2Cap, Descrip3D consistently outperforms
strong baseline models, demonstrating the effectiveness of language-guided
relational representation for understanding complex indoor scenes.

</details>


### [27] [LEAD: Exploring Logit Space Evolution for Model Selection](https://arxiv.org/abs/2507.14559)
*Zixuan Hu,Xiaotong Li,Shixiang Tang,Jun Liu,Yichun Hu,Ling-Yu Duan*

Main category: cs.CV

TL;DR: LEAD是一种基于网络输出的logits的微调对齐方法，通过ODE建模非线性优化过程，有效预测预训练模型在下游任务中的迁移性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型数量激增，如何高效选择最适合下游任务的模型成为挑战，关键在于准确预测模型迁移性。现有方法在特征空间中建模微调动态，未能捕捉优化的非线性特性。

Method: LEAD提出理论框架建模优化过程，推导ODE描述非线性演化，并设计类感知分解方法考虑不同类别的动态变化。

Result: 在24个预训练模型和10个下游数据集上的实验表明，LEAD在低数据场景下仍表现优异。

Conclusion: LEAD通过单步优化填补优化差距，避免了冗长的微调过程，展示了广泛的适应性。

Abstract: The remarkable success of pretrain-then-finetune paradigm has led to a
proliferation of available pre-trained models for vision tasks. This surge
presents a significant challenge in efficiently choosing the most suitable
pre-trained models for downstream tasks. The critical aspect of this challenge
lies in effectively predicting the model transferability by considering the
underlying fine-tuning dynamics. Existing methods often model fine-tuning
dynamics in feature space with linear transformations, which do not precisely
align with the fine-tuning objective and fail to grasp the essential
nonlinearity from optimization. To this end, we present LEAD, a
finetuning-aligned approach based on the network output of logits. LEAD
proposes a theoretical framework to model the optimization process and derives
an ordinary differential equation (ODE) to depict the nonlinear evolution
toward the final logit state. Additionally, we design a class-aware
decomposition method to consider the varying evolution dynamics across classes
and further ensure practical applicability. Integrating the closely aligned
optimization objective and nonlinear modeling capabilities derived from the
differential equation, our method offers a concise solution to effectively
bridge the optimization gap in a single step, bypassing the lengthy fine-tuning
process. The comprehensive experiments on 24 supervised and self-supervised
pre-trained models across 10 downstream datasets demonstrate impressive
performances and showcase its broad adaptability even in low-data scenarios.

</details>


### [28] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 本文提出了一个生成模型（GANs、扩散模型和流匹配技术）在T1w到T2w MRI图像转换中的综合基准测试，发现GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: MRI扫描中获取多种对比图像（如T1w和T2w）会增加时间和成本，因此需要研究跨模态合成的计算方法以减少扫描时间并保持诊断质量。

Method: 使用生成对抗网络（GANs）、扩散模型和流匹配（FM）技术进行T1w到T2w的2D MRI图像转换，并在三个公开的MRI数据集上进行评估。

Result: GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于扩散模型和FM-based方法，流匹配模型在小数据集上容易过拟合。

Conclusion: 研究结果为实际MRI工作流程中部署图像转换技术提供了实用指导，并指出了跨模态医学图像合成未来研究的方向。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [29] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX在血液细胞图像分类中的性能，重点关注推理时间和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对特定深度学习框架在血液图像分析中的详细性能分析。

Method: 使用BloodMNIST数据集，比较了TensorFlow、PyTorch和JAX在分类任务中的表现，特别是推理时间和不同图像尺寸的影响。

Result: 结果显示不同框架性能存在差异，JAX和PyTorch的分类准确性与当前基准相当。

Conclusion: JAX和PyTorch在医学图像分类中表现出高效性，适合相关应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [30] [DiSCO-3D : Discovering and segmenting Sub-Concepts from Open-vocabulary queries in NeRF](https://arxiv.org/abs/2507.14596)
*Doriand Petit,Steve Bourgeois,Vincent Gay-Bellile,Florian Chabot,Loïc Barthe*

Main category: cs.CV

TL;DR: DiSCO-3D是一种结合无监督分割和开放词汇引导的方法，首次解决了3D开放词汇子概念发现的问题，并在开放词汇和无监督分割的边缘案例中表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法仅适应特定任务目标或场景内容，而DiSCO-3D旨在提供一种同时适应场景和用户查询的3D语义分割方法。

Method: 基于神经场表示，结合无监督分割和弱开放词汇引导。

Result: 在开放词汇子概念发现中表现优异，并在开放词汇和无监督分割的边缘案例中达到最先进水平。

Conclusion: DiSCO-3D为3D语义分割提供了一种更灵活且适应性强的解决方案。

Abstract: 3D semantic segmentation provides high-level scene understanding for
applications in robotics, autonomous systems, \textit{etc}. Traditional methods
adapt exclusively to either task-specific goals (open-vocabulary segmentation)
or scene content (unsupervised semantic segmentation). We propose DiSCO-3D, the
first method addressing the broader problem of 3D Open-Vocabulary Sub-concepts
Discovery, which aims to provide a 3D semantic segmentation that adapts to both
the scene and user queries. We build DiSCO-3D on Neural Fields representations,
combining unsupervised segmentation with weak open-vocabulary guidance. Our
evaluations demonstrate that DiSCO-3D achieves effective performance in
Open-Vocabulary Sub-concepts Discovery and exhibits state-of-the-art results in
the edge cases of both open-vocabulary and unsupervised segmentation.

</details>


### [31] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: Exp-Graph是一种基于图模型的新框架，用于面部表情识别，结合视觉Transformer和图卷积网络，显著提升了识别准确率。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互中至关重要，但面部属性的结构变化需要被有效建模以提高识别效果。

Method: 使用面部关键点作为图的顶点，基于邻近性和局部外观相似性构建边，结合视觉Transformer和图卷积网络捕捉结构依赖关系。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别达到98.09%、79.01%和56.39%的准确率。

Conclusion: Exp-Graph在实验室和现实环境中均表现出强大的泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [32] [Depthwise-Dilated Convolutional Adapters for Medical Object Tracking and Segmentation Using the Segment Anything Model 2](https://arxiv.org/abs/2507.14613)
*Guoping Xu,Christopher Kabat,You Zhang*

Main category: cs.CV

TL;DR: DD-SAM2是一个高效适配框架，通过Depthwise-Dilated Adapter增强SAM2在医学视频分割中的多尺度特征提取，减少参数开销。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法多为模态特定设计，适应性差，且SAM2在医学视频场景中需要大规模数据集重新训练，计算成本高。

Method: 提出DD-SAM2框架，结合Depthwise-Dilated Adapter，实现有限数据下的高效微调，充分利用SAM2的流式记忆机制。

Result: 在TrackRad2025和EchoNet-Dynamic数据集上表现优异，Dice分数分别达到0.93和0.97。

Conclusion: DD-SAM2首次系统探索了基于适配器的SAM2微调方法，为医学视频分割与跟踪提供了高效解决方案。

Abstract: Recent advances in medical image segmentation have been driven by deep
learning; however, most existing methods remain limited by modality-specific
designs and exhibit poor adaptability to dynamic medical imaging scenarios. The
Segment Anything Model 2 (SAM2) and its related variants, which introduce a
streaming memory mechanism for real-time video segmentation, present new
opportunities for prompt-based, generalizable solutions. Nevertheless, adapting
these models to medical video scenarios typically requires large-scale datasets
for retraining or transfer learning, leading to high computational costs and
the risk of catastrophic forgetting. To address these challenges, we propose
DD-SAM2, an efficient adaptation framework for SAM2 that incorporates a
Depthwise-Dilated Adapter (DD-Adapter) to enhance multi-scale feature
extraction with minimal parameter overhead. This design enables effective
fine-tuning of SAM2 on medical videos with limited training data. Unlike
existing adapter-based methods focused solely on static images, DD-SAM2 fully
exploits SAM2's streaming memory for medical video object tracking and
segmentation. Comprehensive evaluations on TrackRad2025 (tumor segmentation)
and EchoNet-Dynamic (left ventricle tracking) datasets demonstrate superior
performance, achieving Dice scores of 0.93 and 0.97, respectively. To the best
of our knowledge, this work provides an initial attempt at systematically
exploring adapter-based SAM2 fine-tuning for medical video segmentation and
tracking. Code, datasets, and models will be publicly available at
https://github.com/apple1986/DD-SAM2.

</details>


### [33] [BusterX++: Towards Unified Cross-Modal AI-Generated Content Detection and Explanation with MLLM](https://arxiv.org/abs/2507.14632)
*Haiquan Wen,Tianxiao Li,Zhenglin Huang,Yiwei He,Guangliang Cheng*

Main category: cs.CV

TL;DR: BusterX++是一个用于跨模态检测和解释合成媒体的新框架，通过强化学习后训练策略和多阶段训练等方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的进步增加了虚假信息的风险，现有单模态检测方法无法应对多模态合成内容。

Method: 采用强化学习后训练策略，结合多阶段训练、思维奖励和混合推理。

Result: BusterX++在性能上取得显著提升，并提出了GenBuster++基准用于评估。

Conclusion: BusterX++在跨模态检测中表现出色，具有广泛适用性。

Abstract: Recent advances in generative AI have dramatically improved image and video
synthesis capabilities, significantly increasing the risk of misinformation
through sophisticated fake content. In response, detection methods have evolved
from traditional approaches to multimodal large language models (MLLMs),
offering enhanced transparency and interpretability in identifying synthetic
media. However, current detection systems remain fundamentally limited by their
single-modality design. These approaches analyze images or videos separately,
making them ineffective against synthetic content that combines multiple media
formats. To address these challenges, we introduce \textbf{BusterX++}, a novel
framework designed specifically for cross-modal detection and explanation of
synthetic media. Our approach incorporates an advanced reinforcement learning
(RL) post-training strategy that eliminates cold-start. Through Multi-stage
Training, Thinking Reward, and Hybrid Reasoning, BusterX++ achieves stable and
substantial performance improvements. To enable comprehensive evaluation, we
also present \textbf{GenBuster++}, a cross-modal benchmark leveraging
state-of-the-art image and video generation techniques. This benchmark
comprises 4,000 images and video clips, meticulously curated by human experts
using a novel filtering methodology to ensure high quality, diversity, and
real-world applicability. Extensive experiments demonstrate the effectiveness
and generalizability of our approach.

</details>


### [34] [Multispectral State-Space Feature Fusion: Bridging Shared and Cross-Parametric Interactions for Object Detection](https://arxiv.org/abs/2507.14643)
*Jifeng Shen,Haibo Zhan,Shaohua Dong,Xin Zuo,Wankou Yang,Haibin Ling*

Main category: cs.CV

TL;DR: MS2Fusion提出了一种基于状态空间模型的双路径参数交互机制，解决了多光谱特征融合中的局部互补特征偏好和计算复杂度问题，显著提升了目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现代多光谱特征融合方法存在对局部互补特征的过度偏好以及计算复杂度与感受野大小的权衡问题，影响了泛化性能和可扩展性。

Method: MS2Fusion通过双路径参数交互机制（互补信息挖掘和跨模态对齐）在状态空间模型中实现高效特征融合。

Result: 在FLIR、M3FD和LLVIP等主流基准测试中，MS2Fusion显著优于其他先进方法，并在RGB-T语义分割和RGBT显著目标检测任务中表现出通用性。

Conclusion: MS2Fusion通过统一框架实现了功能互补和共享语义空间，具有高效性和通用性。

Abstract: Modern multispectral feature fusion for object detection faces two critical
limitations: (1) Excessive preference for local complementary features over
cross-modal shared semantics adversely affects generalization performance; and
(2) The trade-off between the receptive field size and computational complexity
present critical bottlenecks for scalable feature modeling. Addressing these
issues, a novel Multispectral State-Space Feature Fusion framework, dubbed
MS2Fusion, is proposed based on the state space model (SSM), achieving
efficient and effective fusion through a dual-path parametric interaction
mechanism. More specifically, the first cross-parameter interaction branch
inherits the advantage of cross-attention in mining complementary information
with cross-modal hidden state decoding in SSM. The second shared-parameter
branch explores cross-modal alignment with joint embedding to obtain
cross-modal similar semantic features and structures through parameter sharing
in SSM. Finally, these two paths are jointly optimized with SSM for fusing
multispectral features in a unified framework, allowing our MS2Fusion to enjoy
both functional complementarity and shared semantic space. In our extensive
experiments on mainstream benchmarks including FLIR, M3FD and LLVIP, our
MS2Fusion significantly outperforms other state-of-the-art multispectral object
detection methods, evidencing its superiority. Moreover, MS2Fusion is general
and applicable to other multispectral perception tasks. We show that, even
without specific design, MS2Fusion achieves state-of-the-art results on RGB-T
semantic segmentation and RGBT salient object detection, showing its
generality. The source code will be available at
https://github.com/61s61min/MS2Fusion.git.

</details>


### [35] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，用于提升体育裁判的实时决策能力，特别是在跆拳道中实现头部踢击的实时检测和评分。


<details>
  <summary>Details</summary>
Motivation: 传统的人工裁判系统存在延迟、主观性和不一致的问题，影响了比赛的公平性和运动员的信任。

Method: 结合计算机视觉、深度学习和边缘推理，通过姿态估计、动作分类和影响分析实现自动化决策。

Result: 系统将决策时间从分钟级缩短到秒级，提高了裁判的一致性和透明度。

Conclusion: FST.ai不仅适用于跆拳道，还可扩展到其他需要动作检测的体育项目，展示了其鲁棒性和跨体育项目的潜力。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [36] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 提出了一种基于计算机视觉的经济高效框架，通过语义分割RGB图像来量化餐盘级食物浪费，适用于大规模餐饮环境。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的食物浪费对于支持数据驱动的可持续发展策略至关重要。

Method: 使用四种全监督模型（U-Net、U-Net++及其轻量版），结合动态逆频率损失和AdamW优化器，通过多种指标评估性能。

Result: 所有模型表现良好，部分模型对特定食物类型的DPA指标接近或超过90%，轻量模型实现实时推理。

Conclusion: 该框架为大规模餐饮环境中的实时浪费跟踪系统奠定了基础，并提供了可行的未来研究方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [37] [Gene-DML: Dual-Pathway Multi-Level Discrimination for Gene Expression Prediction from Histopathology Images](https://arxiv.org/abs/2507.14670)
*Yaxuan Song,Jianan Fan,Hang Chang,Weidong Cai*

Main category: cs.CV

TL;DR: Gene-DML通过双路径多级判别增强组织病理学图像与基因表达之间的跨模态对齐，提升基因表达预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分利用组织病理学图像与基因表达的多层次表征对齐，限制了预测性能。

Method: 提出Gene-DML框架，结合多尺度实例级判别和跨级实例-组判别路径，增强形态与转录模态的对应关系。

Result: 在公共空间转录组数据集上，Gene-DML实现了最先进的基因表达预测性能。

Conclusion: Gene-DML通过多层次跨模态对齐，显著提升了基因表达的预测准确性和泛化能力。

Abstract: Accurately predicting gene expression from histopathology images offers a
scalable and non-invasive approach to molecular profiling, with significant
implications for precision medicine and computational pathology. However,
existing methods often underutilize the cross-modal representation alignment
between histopathology images and gene expression profiles across multiple
representational levels, thereby limiting their prediction performance. To
address this, we propose Gene-DML, a unified framework that structures latent
space through Dual-pathway Multi-Level discrimination to enhance correspondence
between morphological and transcriptional modalities. The multi-scale
instance-level discrimination pathway aligns hierarchical histopathology
representations extracted at local, neighbor, and global levels with gene
expression profiles, capturing scale-aware morphological-transcriptional
relationships. In parallel, the cross-level instance-group discrimination
pathway enforces structural consistency between individual (image/gene)
instances and modality-crossed (gene/image, respectively) groups, strengthening
the alignment across modalities. By jointly modelling fine-grained and
structural-level discrimination, Gene-DML is able to learn robust cross-modal
representations, enhancing both predictive accuracy and generalization across
diverse biological contexts. Extensive experiments on public spatial
transcriptomics datasets demonstrate that Gene-DML achieves state-of-the-art
performance in gene expression prediction. The code and checkpoints will be
released soon.

</details>


### [38] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了Doc-750K数据集和Docopilot模型，解决了多模态大语言模型在复杂文档理解中的不足，无需依赖检索增强生成方法。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂、多页文档理解上表现不佳，主要由于缺乏高质量的文档级数据集，且现有检索增强生成方法存在上下文碎片化、多阶段错误累积等问题。

Method: 构建了高质量的文档级数据集Doc-750K，包含多样文档结构和跨页依赖关系，并基于此开发了原生多模态模型Docopilot。

Result: Docopilot在文档理解任务和多轮交互中表现出更高的连贯性、准确性和效率，为文档级多模态理解设定了新基准。

Conclusion: Doc-750K和Docopilot为复杂文档理解提供了有效解决方案，显著提升了多模态模型的性能。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [39] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种新型协作多代理系统，通过任务分配、验证机制和总结模块提升多模态WSI分析的准确性和多功能性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在WSI分析中表现不如任务专用模型，且多代理系统在病理学领域的潜力尚未充分探索。

Method: WSI-Agents包含三个组件：任务分配模块、验证机制和总结模块，结合专家代理和知识库。

Result: 实验表明WSI-Agents在多模态WSI基准测试中优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents通过协作多代理系统显著提升了WSI分析的准确性和多功能性。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [40] [From Semantics, Scene to Instance-awareness: Distilling Foundation Model for Open-vocabulary Situation Recognition](https://arxiv.org/abs/2507.14686)
*Chen Cai,Tianyi Liu,Jianjun Gao,Wenyang Liu,Kejun Wu,Ruoyu Wang,Yi Wang,Soo Chin Liew*

Main category: cs.CV

TL;DR: 论文提出了一种名为MIPD的新框架，通过从多模态大语言模型（MLLM）中蒸馏知识，提升小模型的泛化和零样本能力，解决了复杂场景识别（GSR）的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在复杂GSR任务中表现不佳且资源消耗大，而传统GSR模型泛化能力不足。论文旨在通过知识蒸馏提升小模型的性能。

Method: 提出MIPD框架，利用LLM生成正负样本的理性解释，通过场景感知和实例感知提示对齐视觉信息，最终蒸馏到学生模型中。

Result: 在Ov-SWiG和HICO-DET数据集上，MIPD在已知、罕见和未知场景中表现优异，提升了检测能力。

Conclusion: MIPD通过知识蒸馏有效提升了小模型的泛化和零样本能力，为复杂场景识别提供了新思路。

Abstract: Recent Multimodal Large Language Models (MLLMs) exhibit strong zero-shot
abilities but struggle with complex Grounded Situation Recognition (GSR) and
are resource-intensive for edge device deployment. Meanwhile, conventional GSR
models often lack generalization ability, falling short in recognizing unseen
and rare situations. In this paper, we exploit transferring knowledge from a
teacher MLLM to a small GSR model to enhance its generalization and zero-shot
abilities, thereby introducing the task of Open-vocabulary Grounded Situation
Recognition (Ov-GSR). To achieve this, we propose Multimodal Interactive Prompt
Distillation (MIPD), a novel framework that distills enriched multimodal
knowledge from the foundation model, enabling the student Ov-GSR model to
recognize unseen situations and be better aware of rare situations.
Specifically, the MIPD framework first leverages the LLM-based Judgmental
Rationales Generator (JRG) to construct positive and negative glimpse and gaze
rationales enriched with contextual semantic information. The proposed
scene-aware and instance-perception prompts are then introduced to align
rationales with visual information from the MLLM teacher via the
Negative-Guided Multimodal Prompting Alignment (NMPA) module, effectively
capturing holistic and perceptual multimodal knowledge. Finally, the aligned
multimodal knowledge is distilled into the student Ov-GSR model, providing a
stronger foundation for generalization that enhances situation understanding,
bridges the gap between seen and unseen scenarios, and mitigates prediction
bias in rare cases. We evaluate MIPD on the refined Ov-SWiG dataset, achieving
superior performance on seen, rare, and unseen situations, and further
demonstrate improved unseen detection on the HICO-DET dataset.

</details>


### [41] [GTPBD: A Fine-Grained Global Terraced Parcel and Boundary Dataset](https://arxiv.org/abs/2507.14697)
*Zhiwei Zhang,Zi Ye,Yibin Wen,Shuai Yuan,Haohuan Fu,Jianxi Huang,Juepeng Zheng*

Main category: cs.CV

TL;DR: 本文介绍了全球梯田地块和边界数据集（GTPBD），这是首个覆盖全球主要梯田区域的细粒度数据集，包含超过20万个复杂梯田地块的手动标注，适用于多种遥感任务。


<details>
  <summary>Details</summary>
Motivation: 现有农业地块提取研究主要关注中分辨率或平坦农田，缺乏对复杂梯田地形的精细表达，而GTPBD填补了这一空白，为精细农业地形分析和跨场景知识迁移提供了基础设施。

Method: GTPBD包含47,537张高分辨率图像，带有三级标签（像素级边界标签、掩码标签和地块标签），覆盖中国七大地理区域和全球跨气候区域。

Result: GTPBD在八种语义分割方法、四种边缘提取方法、三种地块提取方法和五种无监督域自适应方法上进行了基准测试，并提出了多维评估框架。

Conclusion: GTPBD填补了梯田遥感研究的关键空白，为精细农业地形分析和跨场景知识迁移提供了重要基础设施。

Abstract: Agricultural parcels serve as basic units for conducting agricultural
practices and applications, which is vital for land ownership registration,
food security assessment, soil erosion monitoring, etc. However, existing
agriculture parcel extraction studies only focus on mid-resolution mapping or
regular plain farmlands while lacking representation of complex terraced
terrains due to the demands of precision agriculture.In this paper, we
introduce a more fine-grained terraced parcel dataset named GTPBD (Global
Terraced Parcel and Boundary Dataset), which is the first fine-grained dataset
covering major worldwide terraced regions with more than 200,000 complex
terraced parcels with manual annotation. GTPBD comprises 47,537 high-resolution
images with three-level labels, including pixel-level boundary labels, mask
labels, and parcel labels. It covers seven major geographic zones in China and
transcontinental climatic regions around the world.Compared to the existing
datasets, the GTPBD dataset brings considerable challenges due to the: (1)
terrain diversity; (2) complex and irregular parcel objects; and (3) multiple
domain styles. Our proposed GTPBD dataset is suitable for four different tasks,
including semantic segmentation, edge detection, terraced parcel extraction,
and unsupervised domain adaptation (UDA) tasks.Accordingly, we benchmark the
GTPBD dataset on eight semantic segmentation methods, four edge extraction
methods, three parcel extraction methods, and five UDA methods, along with a
multi-dimensional evaluation framework integrating pixel-level and object-level
metrics. GTPBD fills a critical gap in terraced remote sensing research,
providing a basic infrastructure for fine-grained agricultural terrain analysis
and cross-scenario knowledge transfer.

</details>


### [42] [MultiRetNet: A Multimodal Vision Model and Deferral System for Staging Diabetic Retinopathy](https://arxiv.org/abs/2507.14738)
*Jeannie She,Katie Spivakovsky*

Main category: cs.CV

TL;DR: MultiRetNet结合视网膜成像、社会经济因素和共病情况，通过多模态融合和临床延迟系统提高糖尿病视网膜病变分期的准确性，尤其服务于低收入人群。


<details>
  <summary>Details</summary>
Motivation: 低收入社区人群因筛查机会有限，糖尿病视网膜病变（DR）进展风险更高，共病情况加速病情发展，需改进早期检测方法。

Method: 提出MultiRetNet管道，结合三种多模态融合方法，通过全连接层融合数据，并利用对比学习训练延迟系统识别需临床复查的样本。

Result: 系统在低质量图像上保持诊断准确性，整合关键健康数据，提升早期检测率，尤其对服务不足人群有效。

Conclusion: MultiRetNet可降低医疗成本，提高早期检测率，减少医疗资源分配不均，促进医疗公平。

Abstract: Diabetic retinopathy (DR) is a leading cause of preventable blindness,
affecting over 100 million people worldwide. In the United States, individuals
from lower-income communities face a higher risk of progressing to advanced
stages before diagnosis, largely due to limited access to screening. Comorbid
conditions further accelerate disease progression. We propose MultiRetNet, a
novel pipeline combining retinal imaging, socioeconomic factors, and
comorbidity profiles to improve DR staging accuracy, integrated with a clinical
deferral system for a clinical human-in-the-loop implementation. We experiment
with three multimodal fusion methods and identify fusion through a fully
connected layer as the most versatile methodology. We synthesize adversarial,
low-quality images and use contrastive learning to train the deferral system,
guiding the model to identify out-of-distribution samples that warrant
clinician review. By maintaining diagnostic accuracy on suboptimal images and
integrating critical health data, our system can improve early detection,
particularly in underserved populations where advanced DR is often first
identified. This approach may reduce healthcare costs, increase early detection
rates, and address disparities in access to care, promoting healthcare equity.

</details>


### [43] [InterAct-Video: Reasoning-Rich Video QA for Urban Traffic](https://arxiv.org/abs/2507.14743)
*Joseph Raj Vishal,Rutuja Patil,Manas Srinivas Gowda,Katha Naik,Yezhou Yang,Bharatesh Chakravarthi*

Main category: cs.CV

TL;DR: InterAct VideoQA是一个专为交通监控任务设计的视频问答数据集，旨在提升视频问答模型在复杂交通场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答模型难以处理真实交通场景中的多事件并发问题，需要专门的数据集来提升性能。

Method: 收集了8小时的真实交通视频，分割为10秒片段，并标注了25,000多个问答对，涵盖时空动态、车辆交互等关键属性。

Result: 在InterAct VideoQA上评估的先进模型显示出对细粒度时空依赖的推理挑战，但微调后性能显著提升。

Conclusion: InterAct VideoQA作为公开数据集，有助于推动智能交通系统中视频问答模型的研发。

Abstract: Traffic monitoring is crucial for urban mobility, road safety, and
intelligent transportation systems (ITS). Deep learning has advanced
video-based traffic monitoring through video question answering (VideoQA)
models, enabling structured insight extraction from traffic videos. However,
existing VideoQA models struggle with the complexity of real-world traffic
scenes, where multiple concurrent events unfold across spatiotemporal
dimensions. To address these challenges, this paper introduces \textbf{InterAct
VideoQA}, a curated dataset designed to benchmark and enhance VideoQA models
for traffic monitoring tasks. The InterAct VideoQA dataset comprises 8 hours of
real-world traffic footage collected from diverse intersections, segmented into
10-second video clips, with over 25,000 question-answer (QA) pairs covering
spatiotemporal dynamics, vehicle interactions, incident detection, and other
critical traffic attributes. State-of-the-art VideoQA models are evaluated on
InterAct VideoQA, exposing challenges in reasoning over fine-grained
spatiotemporal dependencies within complex traffic scenarios. Additionally,
fine-tuning these models on InterAct VideoQA yields notable performance
improvements, demonstrating the necessity of domain-specific datasets for
VideoQA. InterAct VideoQA is publicly available as a benchmark dataset to
facilitate future research in real-world deployable VideoQA models for
intelligent transportation systems. GitHub Repo:
https://github.com/joe-rabbit/InterAct_VideoQA

</details>


### [44] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，显著提升了视频问答的复杂推理能力，并在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频问答方法在处理长视频时存在任务无关采样和启发式检索的缺陷，无法有效捕捉关键事件和因果时序结构。

Method: LeAdQA利用LLM优化问题-选项对以消除因果歧义，并通过时序定位模型精准检索关键片段，结合自适应融合机制整合证据。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上，LeAdQA在复杂推理任务中表现最优，同时保持计算效率。

Conclusion: LeAdQA通过因果感知和视觉定位的协同作用，显著提升了视频问答的准确性和效率。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [45] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架首次实现了对冻结Vision Transformers（ViTs）的可靠且高效的空间-光谱可解释性，解决了高光谱数据中ViTs解释的两大挑战。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）数据的高维性和现有显著性方法难以捕捉有意义的光谱线索，导致ViTs在此领域的解释性不足。

Method: FOCUS引入类特定光谱提示和可学习的[SINK]令牌，通过吸引损失吸收噪声或冗余注意力，无需梯度反向传播或主干修改。

Result: FOCUS将波段级IoU提高15%，减少注意力崩溃40%以上，且显著性结果与专家标注高度一致。

Conclusion: FOCUS以不到1%的参数开销，为高光谱应用提供了实用的高分辨率ViT可解释性，填补了黑盒建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [46] [A Novel Downsampling Strategy Based on Information Complementarity for Medical Image Segmentation](https://arxiv.org/abs/2507.14790)
*Wenbo Yue,Chang Li,Guoping Xu*

Main category: cs.CV

TL;DR: 提出了一种基于信息互补的下采样方法HPD，用于解决传统下采样方法在语义分割任务中丢失关键空间信息的问题。


<details>
  <summary>Details</summary>
Motivation: 传统下采样方法（如最大池化和跨行卷积）在特征聚合、感受野扩展和计算减少方面表现良好，但在语义分割任务中可能导致关键空间信息丢失，影响像素级预测精度。

Method: 提出Hybrid Pooling Downsampling (HPD)，用MinMaxPooling替代传统方法，通过提取局部区域的最大值信息，有效保留图像的明暗对比和细节特征。

Result: 在ACDC和Synapse数据集上的实验表明，HPD在分割性能上优于传统方法，平均DSC系数提高了0.5%。

Conclusion: HPD模块为语义分割任务提供了一种高效的解决方案。

Abstract: In convolutional neural networks (CNNs), downsampling operations are crucial
to model performance. Although traditional downsampling methods (such as
maximum pooling and cross-row convolution) perform well in feature aggregation,
receptive field expansion, and computational reduction, they may lead to the
loss of key spatial information in semantic segmentation tasks, thereby
affecting the pixel-by-pixel prediction accuracy.To this end, this study
proposes a downsampling method based on information complementarity - Hybrid
Pooling Downsampling (HPD). The core is to replace the traditional method with
MinMaxPooling, and effectively retain the light and dark contrast and detail
features of the image by extracting the maximum value information of the local
area.Experiment on various CNN architectures on the ACDC and Synapse datasets
show that HPD outperforms traditional methods in segmentation performance, and
increases the DSC coefficient by 0.5% on average. The results show that the HPD
module provides an efficient solution for semantic segmentation tasks.

</details>


### [47] [Distilling Parallel Gradients for Fast ODE Solvers of Diffusion Models](https://arxiv.org/abs/2507.14797)
*Beier Zhu,Ruoyu Wang,Tong Zhao,Hanwang Zhang,Chi Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为EPD的新型ODE求解器，通过并行梯度评估减少截断误差，实现高质量和低延迟的采样。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（DMs）在生成性能上表现优异，但因其顺序去噪特性导致采样延迟高。现有基于求解器的加速方法在低延迟预算下常面临图像质量下降的问题。

Method: EPD通过在每个ODE步骤中引入多个并行梯度评估来减少截断误差，且这些计算可完全并行化以保持低延迟。方法通过蒸馏方式优化少量可学习参数，训练开销小。

Result: 在多个图像合成基准测试中，EPD在5 NFE的延迟水平下，FID得分显著优于现有基于学习的求解器（如CIFAR-10上4.47，FFHQ上7.97等）。

Conclusion: EPD作为一种插件式方法，能有效提升现有ODE采样器的性能，实现高质量和低延迟的采样。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance
but suffer from high sampling latency due to their sequential denoising nature.
Existing solver-based acceleration methods often face image quality degradation
under a low-latency budget. In this paper, we propose the Ensemble Parallel
Direction solver (dubbed as \ours), a novel ODE solver that mitigates
truncation errors by incorporating multiple parallel gradient evaluations in
each ODE step. Importantly, since the additional gradient computations are
independent, they can be fully parallelized, preserving low-latency sampling.
  Our method optimizes a small set of learnable parameters in a distillation
fashion, ensuring minimal training overhead.
  In addition, our method can serve as a plugin to improve existing ODE
samplers. Extensive experiments on various image synthesis benchmarks
demonstrate the effectiveness of our \ours~in achieving high-quality and
low-latency sampling. For example, at the same latency level of 5 NFE, EPD
achieves an FID of 4.47 on CIFAR-10, 7.97 on FFHQ, 8.17 on ImageNet, and 8.26
on LSUN Bedroom, surpassing existing learning-based solvers by a significant
margin. Codes are available in https://github.com/BeierZhu/EPD.

</details>


### [48] [An Evaluation of DUSt3R/MASt3R/VGGT 3D Reconstruction on Photogrammetric Aerial Blocks](https://arxiv.org/abs/2507.14798)
*Xinyi Wu,Steven Landgraf,Markus Ulrich,Rongjun Qin*

Main category: cs.CV

TL;DR: 论文评估了DUSt3R、MASt3R和VGGT等3D重建基础模型在稀疏航拍图像上的表现，发现它们在低分辨率、稀疏场景中优于传统方法，但在高分辨率和大规模图像集上存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究这些模型在航拍图像上的潜力，填补其在摄影测量领域应用的空白。

Method: 在UseGeo数据集的航拍图像块上对预训练模型进行姿态估计和密集3D重建的全面评估。

Result: 这些方法能从极稀疏图像集（少于10张图像，分辨率最高518像素）准确重建密集点云，完整性比COLMAP提高50%。VGGT在计算效率和姿态估计上表现更优。

Conclusion: 基于Transformer的方法无法完全取代传统SfM和MVS，但在低分辨率、稀疏场景中可作为补充方案。

Abstract: State-of-the-art 3D computer vision algorithms continue to advance in
handling sparse, unordered image sets. Recently developed foundational models
for 3D reconstruction, such as Dense and Unconstrained Stereo 3D Reconstruction
(DUSt3R), Matching and Stereo 3D Reconstruction (MASt3R), and Visual Geometry
Grounded Transformer (VGGT), have attracted attention due to their ability to
handle very sparse image overlaps. Evaluating DUSt3R/MASt3R/VGGT on typical
aerial images matters, as these models may handle extremely low image overlaps,
stereo occlusions, and textureless regions. For redundant collections, they can
accelerate 3D reconstruction by using extremely sparsified image sets. Despite
tests on various computer vision benchmarks, their potential on photogrammetric
aerial blocks remains unexplored. This paper conducts a comprehensive
evaluation of the pre-trained DUSt3R/MASt3R/VGGT models on the aerial blocks of
the UseGeo dataset for pose estimation and dense 3D reconstruction. Results
show these methods can accurately reconstruct dense point clouds from very
sparse image sets (fewer than 10 images, up to 518 pixels resolution), with
completeness gains up to +50% over COLMAP. VGGT also demonstrates higher
computational efficiency, scalability, and more reliable camera pose
estimation. However, all exhibit limitations with high-resolution images and
large sets, as pose reliability declines with more images and geometric
complexity. These findings suggest transformer-based methods cannot fully
replace traditional SfM and MVS, but offer promise as complementary approaches,
especially in challenging, low-resolution, and sparse scenarios.

</details>


### [49] [Exploring Scalable Unified Modeling for General Low-Level Vision](https://arxiv.org/abs/2507.14801)
*Xiangyu Chen,Kaiwen Zhu,Yuandong Pu,Shuo Cao,Xiaohui Li,Wenlong Zhang,Yihao Liu,Yu Qiao,Jiantao Zhou,Chao Dong*

Main category: cs.CV

TL;DR: 提出了一种基于视觉提示的统一低层次视觉任务处理框架VPIP，通过多任务训练提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决低层次视觉任务多样性和输出域差异带来的统一建模挑战。

Method: 设计了包含图像处理主干、提示编码器和交互模块的VPIP框架，支持多任务联合训练。

Result: 在100多个任务上验证了性能，多任务训练提升了泛化能力，尤其在数据有限的任务中表现突出。

Conclusion: VPIP框架具有高效性、可扩展性和潜力，可作为通用低层次视觉建模的基础。

Abstract: Low-level vision involves a wide spectrum of tasks, including image
restoration, enhancement, stylization, and feature extraction, which differ
significantly in both task formulation and output domains. To address the
challenge of unified modeling across such diverse tasks, we propose a Visual
task Prompt-based Image Processing (VPIP) framework that leverages input-target
image pairs as visual prompts to guide the model in performing a variety of
low-level vision tasks. The framework comprises an end-to-end image processing
backbone, a prompt encoder, and a prompt interaction module, enabling flexible
integration with various architectures and effective utilization of
task-specific visual representations. Based on this design, we develop a
unified low-level vision model, GenLV, and evaluate its performance across
multiple representative tasks. To explore the scalability of this approach, we
extend the framework along two dimensions: model capacity and task diversity.
We construct a large-scale benchmark consisting of over 100 low-level vision
tasks and train multiple versions of the model with varying scales.
Experimental results show that the proposed method achieves considerable
performance across a wide range of tasks. Notably, increasing the number of
training tasks enhances generalization, particularly for tasks with limited
data, indicating the model's ability to learn transferable representations
through joint training. Further evaluations in zero-shot generalization,
few-shot transfer, and task-specific fine-tuning scenarios demonstrate the
model's strong adaptability, confirming the effectiveness, scalability, and
potential of the proposed framework as a unified foundation for general
low-level vision modeling.

</details>


### [50] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 提出了一种基于人类认知的多脸深度伪造视频检测方法HICOM，通过分析人类依赖的关键线索，显著提升了检测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单脸检测中表现良好，但在多脸场景中因缺乏上下文线索而表现不佳。

Method: 通过人类研究识别出四种关键线索，并基于此设计了HICOM框架，结合LLM提供可解释性。

Result: HICOM在基准数据集上平均准确率提升3.3%，在未见数据集上优于现有方法5.8%。

Conclusion: 通过引入人类因素，HICOM显著提升了多脸深度伪造视频的检测效果和解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [51] [Light Future: Multimodal Action Frame Prediction via InstructPix2Pix](https://arxiv.org/abs/2507.14809)
*Zesen Zhong,Duomin Zhang,Yijia Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级机器人动作预测方法，利用InstructPix2Pix模型进行未来视觉帧预测，显著降低计算成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 预测未来运动轨迹在机器人、自动驾驶等领域至关重要，但传统视频预测模型计算成本高且延迟大。

Method: 通过微调InstructPix2Pix模型，结合视觉和文本输入，实现多模态未来帧预测。

Result: 在RoboTWin数据集上，SSIM和PSNR优于现有方法，且仅需单帧图像和文本输入。

Conclusion: 该方法轻量高效，适用于机器人运动轨迹分析等优先考虑精度而非视觉保真度的场景。

Abstract: Predicting future motion trajectories is a critical capability across domains
such as robotics, autonomous systems, and human activity forecasting, enabling
safer and more intelligent decision-making. This paper proposes a novel,
efficient, and lightweight approach for robot action prediction, offering
significantly reduced computational cost and inference latency compared to
conventional video prediction models. Importantly, it pioneers the adaptation
of the InstructPix2Pix model for forecasting future visual frames in robotic
tasks, extending its utility beyond static image editing. We implement a deep
learning-based visual prediction framework that forecasts what a robot will
observe 100 frames (10 seconds) into the future, given a current image and a
textual instruction. We repurpose and fine-tune the InstructPix2Pix model to
accept both visual and textual inputs, enabling multimodal future frame
prediction. Experiments on the RoboTWin dataset (generated based on real-world
scenarios) demonstrate that our method achieves superior SSIM and PSNR compared
to state-of-the-art baselines in robot action prediction tasks. Unlike
conventional video prediction models that require multiple input frames, heavy
computation, and slow inference latency, our approach only needs a single image
and a text prompt as input. This lightweight design enables faster inference,
reduced GPU demands, and flexible multimodal control, particularly valuable for
applications like robotics and sports motion trajectory analytics, where motion
trajectory precision is prioritized over visual fidelity.

</details>


### [52] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一个统一的量化框架，通过自适应结合互补技术提升跨模型通用性，解决了扩散模型量化中的通用性和工业部署问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在资源受限或延迟敏感环境中部署时面临计算密集型问题，现有后训练量化方法通用性不足。

Method: SegQuant结合了SegLinear（分段感知、基于图的量化策略）和DualScale（双尺度量化方案），保留极性不对称激活。

Result: SegQuant在多种扩散模型中表现优异，且与主流部署工具兼容。

Conclusion: SegQuant为扩散模型量化提供了通用且高效的解决方案。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [53] [FinChart-Bench: Benchmarking Financial Chart Comprehension in Vision-Language Models](https://arxiv.org/abs/2507.14823)
*Dong Shu,Haoyang Yuan,Yuchen Wang,Yanguang Liu,Huopu Zhang,Haiyan Zhao,Mengnan Du*

Main category: cs.CV

TL;DR: FinChart-Bench是首个专注于真实世界金融图表的基准测试，包含1,200张金融图表和7,016个问题，评估了25个先进的大视觉语言模型（LVLM），揭示了当前模型在金融图表理解中的局限性。


<details>
  <summary>Details</summary>
Motivation: 金融图表因其复杂的时间结构和领域特定术语而未被充分研究，现有的大视觉语言模型（LVLM）在此类任务中的表现尚不明确。

Method: 构建了FinChart-Bench基准测试，包含1,200张金融图表和7,016个问题（TF、MC、QA），并对25个LVLM进行了全面评估。

Result: 评估结果显示：开源与闭源模型性能差距缩小；模型升级后性能下降；指令遵循能力不足；空间推理能力有限；当前LVLM不适合作为自动评估器。

Conclusion: 当前LVLM在金融图表理解中存在显著局限性，FinChart-Bench为未来研究提供了重要基准。

Abstract: Large vision-language models (LVLMs) have made significant progress in chart
understanding. However, financial charts, characterized by complex temporal
structures and domain-specific terminology, remain notably underexplored. We
introduce FinChart-Bench, the first benchmark specifically focused on
real-world financial charts. FinChart-Bench comprises 1,200 financial chart
images collected from 2015 to 2024, each annotated with True/False (TF),
Multiple Choice (MC), and Question Answering (QA) questions, totaling 7,016
questions. We conduct a comprehensive evaluation of 25 state-of-the-art LVLMs
on FinChart-Bench. Our evaluation reveals critical insights: (1) the
performance gap between open-source and closed-source models is narrowing, (2)
performance degradation occurs in upgraded models within families, (3) many
models struggle with instruction following, (4) both advanced models show
significant limitations in spatial reasoning abilities, and (5) current LVLMs
are not reliable enough to serve as automated evaluators. These findings
highlight important limitations in current LVLM capabilities for financial
chart understanding. The FinChart-Bench dataset is available at
https://huggingface.co/datasets/Tizzzzy/FinChart-Bench.

</details>


### [54] [PHATNet: A Physics-guided Haze Transfer Network for Domain-adaptive Real-world Image Dehazing](https://arxiv.org/abs/2507.14826)
*Fu-Jen Tsai,Yan-Tsung Peng,Yen-Yu Lin,Chia-Wen Lin*

Main category: cs.CV

TL;DR: PHATNet通过将目标域的雾霾模式转移到源域的无雾图像上，创建特定领域的微调数据集，从而提升去雾模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有去雾模型在未见过的真实雾霾图像上表现不佳，原因是训练数据有限。

Method: 提出PHATNet，利用物理引导的雾霾转移网络，结合Haze-Transfer-Consistency损失和Content-Leakage损失，增强模型的解耦能力。

Result: 实验表明，PHATNet显著提升了现有去雾模型在真实图像数据集上的性能。

Conclusion: PHATNet通过灵活的领域适应方法，有效解决了去雾模型在未见数据上的性能下降问题。

Abstract: Image dehazing aims to remove unwanted hazy artifacts in images. Although
previous research has collected paired real-world hazy and haze-free images to
improve dehazing models' performance in real-world scenarios, these models
often experience significant performance drops when handling unseen real-world
hazy images due to limited training data. This issue motivates us to develop a
flexible domain adaptation method to enhance dehazing performance during
testing. Observing that predicting haze patterns is generally easier than
recovering clean content, we propose the Physics-guided Haze Transfer Network
(PHATNet) which transfers haze patterns from unseen target domains to
source-domain haze-free images, creating domain-specific fine-tuning sets to
update dehazing models for effective domain adaptation. Additionally, we
introduce a Haze-Transfer-Consistency loss and a Content-Leakage Loss to
enhance PHATNet's disentanglement ability. Experimental results demonstrate
that PHATNet significantly boosts state-of-the-art dehazing models on benchmark
real-world image dehazing datasets.

</details>


### [55] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种无需外部条件的配对图像生成方法，用于解决乳腺断层扫描图像中病灶分割任务的数据标注不足问题。


<details>
  <summary>Details</summary>
Motivation: 高密度乳腺组织导致病灶隐蔽，手动标注困难且耗时，导致标注数据缺乏，现有扩散模型生成质量低且无法生成标注。

Method: 通过训练额外的扩散引导器，实现条件扩散模型的配对图像生成，生成乳腺断层扫描切片和病灶掩码。

Result: 实验表明，该方法无需外部条件即可提高生成质量，并缓解标注数据短缺，提升下游任务性能。

Conclusion: 该方法有效解决了病灶分割任务中的数据标注不足问题，提高了生成图像的质量和实用性。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [56] [Training Self-Supervised Depth Completion Using Sparse Measurements and a Single Image](https://arxiv.org/abs/2507.14845)
*Rizhao Fan,Zhigen Li,Heping Li,Ning An*

Main category: cs.CV

TL;DR: 提出一种仅需稀疏深度测量和对应图像的自监督深度补全方法，无需密集标签或多帧图像。


<details>
  <summary>Details</summary>
Motivation: 解决密集标注成本高和多帧依赖限制自监督方法在静态或单帧场景应用的问题。

Method: 利用深度分布特性设计新损失函数，结合视觉基础模型生成的语义分割图增强深度估计。

Result: 实验证明该方法有效。

Conclusion: 新方法在自监督深度补全中表现优异，适用于更广泛场景。

Abstract: Depth completion is an important vision task, and many efforts have been made
to enhance the quality of depth maps from sparse depth measurements. Despite
significant advances, training these models to recover dense depth from sparse
measurements remains a challenging problem. Supervised learning methods rely on
dense depth labels to predict unobserved regions, while self-supervised
approaches require image sequences to enforce geometric constraints and
photometric consistency between frames. However, acquiring dense annotations is
costly, and multi-frame dependencies limit the applicability of self-supervised
methods in static or single-frame scenarios. To address these challenges, we
propose a novel self-supervised depth completion paradigm that requires only
sparse depth measurements and their corresponding image for training. Unlike
existing methods, our approach eliminates the need for dense depth labels or
additional images captured from neighboring viewpoints. By leveraging the
characteristics of depth distribution, we design novel loss functions that
effectively propagate depth information from observed points to unobserved
regions. Additionally, we incorporate segmentation maps generated by vision
foundation models to further enhance depth estimation. Extensive experiments
demonstrate the effectiveness of our proposed method.

</details>


### [57] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言的多任务视频修复框架，无需预知退化类型，性能领先。


<details>
  <summary>Details</summary>
Motivation: 现有方法需预知退化类型，缺乏灵活性和可解释性。

Method: 利用基础模型学习退化感知语义上下文，无需训练或测试时退化知识。

Result: 在多个基准测试中达到最优性能。

Conclusion: 框架灵活高效，呼吁标准化多任务视频修复基准。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [58] [An Uncertainty-aware DETR Enhancement Framework for Object Detection](https://arxiv.org/abs/2507.14855)
*Xingshu Chen,Sicheng Yu,Chong Cheng,Hao Wang,Ting Tian*

Main category: cs.CV

TL;DR: 本文提出了一种基于DETR的不确定性感知增强框架，通过建模边界框为多元高斯分布并引入Gromov-Wasserstein距离，提高了定位精度和预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统检测器依赖确定性边界框回归，忽略了预测中的不确定性，限制了模型的鲁棒性。

Method: 将边界框建模为多元高斯分布，在损失函数中引入Gromov-Wasserstein距离，并提出贝叶斯风险公式过滤高风险信息。

Result: 在COCO基准测试中有效提升了DETR变体的性能，并在白细胞检测任务中取得了最先进的结果。

Conclusion: 该框架在通用和特定领域检测任务中均表现出良好的可扩展性。

Abstract: This paper investigates the problem of object detection with a focus on
improving both the localization accuracy of bounding boxes and explicitly
modeling prediction uncertainty. Conventional detectors rely on deterministic
bounding box regression, ignoring uncertainty in predictions and limiting model
robustness. In this paper, we propose an uncertainty-aware enhancement
framework for DETR-based object detectors. We model bounding boxes as
multivariate Gaussian distributions and incorporate the Gromov-Wasserstein
distance into the loss function to better align the predicted and ground-truth
distributions. Building on this, we derive a Bayes Risk formulation to filter
high-risk information and improve detection reliability. We also propose a
simple algorithm to quantify localization uncertainty via confidence intervals.
Experiments on the COCO benchmark show that our method can be effectively
integrated into existing DETR variants, enhancing their performance. We further
extend our framework to leukocyte detection tasks, achieving state-of-the-art
results on the LISC and WBCDD datasets. These results confirm the scalability
of our framework across both general and domain-specific detection tasks. Code
page:
https://github.com/ParadiseforAndaChen/An-Uncertainty-aware-DETR-Enhancement-Framework-for-Object-Detection.

</details>


### [59] [Hybrid-supervised Hypergraph-enhanced Transformer for Micro-gesture Based Emotion Recognition](https://arxiv.org/abs/2507.14867)
*Zhaoqiang Xia,Hexiang Huang,Haoyu Chen,Xiaoyi Feng,Guoying Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于超图增强Transformer的混合监督框架，用于通过微手势识别情绪状态，并在公开数据集上取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 微手势作为无意识的身体动作，能够传达人类情绪状态，但在情绪建模方面尚未充分探索。

Method: 采用超图增强Transformer的编码器-解码器结构，结合自监督和监督学习，通过超图自注意力模块和多尺度时间卷积模块捕捉微手势的细微运动。

Result: 在iMiGUE和SMG数据集上表现优于现有方法。

Conclusion: 该方法通过混合监督框架有效建模微手势与情绪状态的关系，为情绪识别提供了新思路。

Abstract: Micro-gestures are unconsciously performed body gestures that can convey the
emotion states of humans and start to attract more research attention in the
fields of human behavior understanding and affective computing as an emerging
topic. However, the modeling of human emotion based on micro-gestures has not
been explored sufficiently. In this work, we propose to recognize the emotion
states based on the micro-gestures by reconstructing the behavior patterns with
a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the
framework, hypergraph Transformer based encoder and decoder are separately
designed by stacking the hypergraph-enhanced self-attention and multiscale
temporal convolution modules. Especially, to better capture the subtle motion
of micro-gestures, we construct a decoder with additional upsampling operations
for a reconstruction task in a self-supervised learning manner. We further
propose a hypergraph-enhanced self-attention module where the hyperedges
between skeleton joints are gradually updated to present the relationships of
body joints for modeling the subtle local motion. Lastly, for exploiting the
relationship between the emotion states and local motion of micro-gestures, an
emotion recognition head from the output of encoder is designed with a shallow
architecture and learned in a supervised way. The end-to-end framework is
jointly trained in a one-stage way by comprehensively utilizing
self-reconstruction and supervision information. The proposed method is
evaluated on two publicly available datasets, namely iMiGUE and SMG, and
achieves the best performance under multiple metrics, which is superior to the
existing methods.

</details>


### [60] [Region-aware Depth Scale Adaptation with Sparse Measurements](https://arxiv.org/abs/2507.14879)
*Rizhao Fan,Tianfang Ma,Zhigen Li,Ning An,Jian Cheng*

Main category: cs.CV

TL;DR: 提出了一种无需学习的方法，利用稀疏深度测量将基础模型的相对尺度深度预测转换为度量尺度深度，避免了额外的计算成本或泛化能力的损失。


<details>
  <summary>Details</summary>
Motivation: 现有基础模型在零样本单目深度估计中表现出色，但其输出为相对尺度，限制了实际应用。现有尺度适应方法成本高且可能损害模型的泛化能力。

Method: 提出一种非基于学习的方法，利用稀疏深度测量将相对尺度深度预测转换为度量尺度深度，无需重新训练或微调。

Result: 实验结果表明，该方法有效且能够在不增加计算成本或牺牲泛化能力的情况下实现相对尺度到度量尺度的转换。

Conclusion: 该方法为实际应用中基础模型的度量尺度深度预测提供了一种高效且通用的解决方案。

Abstract: In recent years, the emergence of foundation models for depth prediction has
led to remarkable progress, particularly in zero-shot monocular depth
estimation. These models generate impressive depth predictions; however, their
outputs are often in relative scale rather than metric scale. This limitation
poses challenges for direct deployment in real-world applications. To address
this, several scale adaptation methods have been proposed to enable foundation
models to produce metric depth. However, these methods are typically costly, as
they require additional training on new domains and datasets. Moreover,
fine-tuning these models often compromises their original generalization
capabilities, limiting their adaptability across diverse scenes. In this paper,
we introduce a non-learning-based approach that leverages sparse depth
measurements to adapt the relative-scale predictions of foundation models into
metric-scale depth. Our method requires neither retraining nor fine-tuning,
thereby preserving the strong generalization ability of the original foundation
models while enabling them to produce metric depth. Experimental results
demonstrate the effectiveness of our approach, high-lighting its potential to
bridge the gap between relative and metric depth without incurring additional
computational costs or sacrificing generalization ability.

</details>


### [61] [BeatFormer: Efficient motion-robust remote heart rate estimation through unsupervised spectral zoomed attention filters](https://arxiv.org/abs/2507.14885)
*Joaquim Comas,Federico Sukno*

Main category: cs.CV

TL;DR: BeatFormer是一种轻量级的光谱注意力模型，结合了手工方法和深度学习的优势，用于远程光电容积描记术（rPPG）估计，无需PPG或HR标签即可训练。


<details>
  <summary>Details</summary>
Motivation: 当前rPPG估计中，深度学习方法依赖大数据集，而手工方法在未见场景中泛化能力有限，因此需要结合两者优势的混合方法。

Method: 提出BeatFormer模型，结合了放大的正交复数注意力和频域能量测量，并引入光谱对比学习（SCL）进行无监督训练。

Result: 在PURE、UBFC-rPPG和MMPD数据集上验证了BeatFormer的鲁棒性和性能，尤其在运动场景下的跨数据集评估中表现优异。

Conclusion: BeatFormer通过结合手工方法和深度学习的优势，提供了一种高效且泛化能力强的rPPG估计解决方案。

Abstract: Remote photoplethysmography (rPPG) captures cardiac signals from facial
videos and is gaining attention for its diverse applications. While deep
learning has advanced rPPG estimation, it relies on large, diverse datasets for
effective generalization. In contrast, handcrafted methods utilize
physiological priors for better generalization in unseen scenarios like motion
while maintaining computational efficiency. However, their linear assumptions
limit performance in complex conditions, where deep learning provides superior
pulsatile information extraction. This highlights the need for hybrid
approaches that combine the strengths of both methods. To address this, we
present BeatFormer, a lightweight spectral attention model for rPPG estimation,
which integrates zoomed orthonormal complex attention and frequency-domain
energy measurement, enabling a highly efficient model. Additionally, we
introduce Spectral Contrastive Learning (SCL), which allows BeatFormer to be
trained without any PPG or HR labels. We validate BeatFormer on the PURE,
UBFC-rPPG, and MMPD datasets, demonstrating its robustness and performance,
particularly in cross-dataset evaluations under motion scenarios.

</details>


### [62] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种基于统一2D预训练多模态网络的3D视觉定位方法，简化架构并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖多模态分离编码器，导致模型复杂且训练低效。

Method: 利用2D CLIP双模态模型，通过适配器微调适应三模态设置，设计GARF模块融合几何特征，并引入多模态解码器。

Result: 参数减少58%，3D检测任务提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 统一特征提取与融合方法显著简化模型并提升性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [63] [Semantic-Aware Representation Learning for Multi-label Image Classification](https://arxiv.org/abs/2507.14918)
*Ren-Dong Xie,Zhi-Fen He,Bo Li,Bin Liu,Jin-Yan Hu*

Main category: cs.CV

TL;DR: 提出了一种语义感知表示学习方法（SARL），用于多标签图像分类，通过语义相关特征学习和最优传输注意力机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如注意力机制或GCN）的表示可能包含噪声且无法精确定位对象，因此需要更精确的语义表示。

Method: 1. 语义相关特征学习模块提取特征；2. 最优传输注意力机制对齐语义表示；3. 区域分数聚合策略进行多标签预测。

Result: 在PASCAL VOC 2007和MS-COCO数据集上优于现有方法。

Conclusion: SARL通过语义对齐和区域聚合显著提升了多标签分类性能。

Abstract: Multi-label image classification, an important research area in computer
vision, focuses on identifying multiple labels or concepts within an image.
Existing approaches often employ attention mechanisms or graph convolutional
networks (GCNs) to learn image representation. However, this representation may
contain noise and may not locate objects precisely. Therefore, this paper
proposes a Semantic-Aware Representation Learning (SARL) for multi-label image
classification. First, a label semantic-related feature learning module is
utilized to extract semantic-related features. Then, an optimal transport-based
attention mechanism is designed to obtain semantically aligned image
representation. Finally, a regional score aggregation strategy is used for
multi-label prediction. Experimental results on two benchmark datasets, PASCAL
VOC 2007 and MS-COCO, demonstrate the superiority of SARL over existing
methods.

</details>


### [64] [Stereo-GS: Multi-View Stereo Vision Model for Generalizable 3D Gaussian Splatting Reconstruction](https://arxiv.org/abs/2507.14921)
*Xiufeng Huang,Ka Chun Cheung,Runmin Cong,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: 提出了一种解耦框架，用于高效预测3D高斯，通过立体视觉和全局注意力块提取特征，生成几何和外观的高斯特征，实现无姿态的高质量3D重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法在3D高斯几何和外观预测上耦合度高，依赖数据驱动先验且回归速度慢，计算资源需求大。

Method: 使用立体视觉骨干网络提取局部图像对特征，通过全局注意力块融合，生成几何的多视点点云和外观的高斯特征，结合为GS-maps，并通过细化网络提升重建质量。

Result: 实现了无相机参数依赖的高质量3D重建，提高了鲁棒性和实用性，同时降低了资源需求。

Conclusion: 该方法为现实世界的3D内容生成提供了高效、可扩展的解决方案。

Abstract: Generalizable 3D Gaussian Splatting reconstruction showcases advanced
Image-to-3D content creation but requires substantial computational resources
and large datasets, posing challenges to training models from scratch. Current
methods usually entangle the prediction of 3D Gaussian geometry and appearance,
which rely heavily on data-driven priors and result in slow regression speeds.
To address this, we propose \method, a disentangled framework for efficient 3D
Gaussian prediction. Our method extracts features from local image pairs using
a stereo vision backbone and fuses them via global attention blocks. Dedicated
point and Gaussian prediction heads generate multi-view point-maps for geometry
and Gaussian features for appearance, combined as GS-maps to represent the 3DGS
object. A refinement network enhances these GS-maps for high-quality
reconstruction. Unlike existing methods that depend on camera parameters, our
approach achieves pose-free 3D reconstruction, improving robustness and
practicality. By reducing resource demands while maintaining high-quality
outputs, \method provides an efficient, scalable solution for real-world 3D
content generation.

</details>


### [65] [3-Dimensional CryoEM Pose Estimation and Shift Correction Pipeline](https://arxiv.org/abs/2507.14924)
*Kaishva Chintan Shah,Virajith Boddapati,Karthik S. Gurumoorthy,Sandip Kaledhonkar,Ajit Rajwade*

Main category: cs.CV

TL;DR: 提出了一种基于多维尺度分析（MDS）的稳健方法，用于低温电子显微镜（cryo-EM）中的姿态估计和位移校正，显著提高了3D重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 低温电子显微镜中极低的信噪比（SNR）导致姿态估计和位移校正困难，直接影响3D重建的保真度。

Method: 利用MDS技术从二面角对估计3D旋转矩阵，并通过稳健的联合优化框架（基于ℓ₁范数）和迭代位移校正算法实现精确的姿态估计。

Result: 该方法在欧拉角精度和重建保真度（通过傅里叶壳层相关性FSC衡量）上均优于现有方法。

Conclusion: 提出的方法通过稳健的优化和全局一致性约束，显著提升了低SNR条件下的姿态估计和3D重建效果。

Abstract: Accurate pose estimation and shift correction are key challenges in cryo-EM
due to the very low SNR, which directly impacts the fidelity of 3D
reconstructions. We present an approach for pose estimation in cryo-EM that
leverages multi-dimensional scaling (MDS) techniques in a robust manner to
estimate the 3D rotation matrix of each particle from pairs of dihedral angles.
We express the rotation matrix in the form of an axis of rotation and a unit
vector in the plane perpendicular to the axis. The technique leverages the
concept of common lines in 3D reconstruction from projections. However, common
line estimation is ridden with large errors due to the very low SNR of cryo-EM
projection images. To address this challenge, we introduce two complementary
components: (i) a robust joint optimization framework for pose estimation based
on an $\ell_1$-norm objective or a similar robust norm, which simultaneously
estimates rotation axes and in-plane vectors while exactly enforcing unit norm
and orthogonality constraints via projected coordinate descent; and (ii) an
iterative shift correction algorithm that estimates consistent in-plane
translations through a global least-squares formulation. While prior approaches
have leveraged such embeddings and common-line geometry for orientation
recovery, existing formulations typically rely on $\ell_2$-based objectives
that are sensitive to noise, and enforce geometric constraints only
approximately. These choices, combined with a sequential pipeline structure,
can lead to compounding errors and suboptimal reconstructions in low-SNR
regimes. Our pipeline consistently outperforms prior methods in both Euler
angle accuracy and reconstruction fidelity, as measured by the Fourier Shell
Correlation (FSC).

</details>


### [66] [Probabilistic smooth attention for deep multiple instance learning in medical imaging](https://arxiv.org/abs/2507.14932)
*Francisco M. Castro-Macías,Pablo Morales-Álvarez,Yunan Wu,Rafael Molina,Aggelos K. Katsaggelos*

Main category: cs.CV

TL;DR: 提出了一种新的概率框架，用于多实例学习（MIL）中的注意力机制，通过估计注意力值的概率分布来捕捉全局和局部交互，并在医学图像分类中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的深度MIL方法通常以确定性方式处理注意力值，忽略了单个实例贡献的不确定性。

Method: 提出了一种概率框架，估计注意力值的概率分布，并同时考虑全局和局部交互。

Result: 在三个医学数据集和十一个基线方法的综合评估中，该方法在不同指标上均表现出最佳预测性能。

Conclusion: 该方法不仅提升了预测性能，还提供了可解释的不确定性图，有助于疾病定位。

Abstract: The Multiple Instance Learning (MIL) paradigm is attracting plenty of
attention in medical imaging classification, where labeled data is scarce. MIL
methods cast medical images as bags of instances (e.g. patches in whole slide
images, or slices in CT scans), and only bag labels are required for training.
Deep MIL approaches have obtained promising results by aggregating
instance-level representations via an attention mechanism to compute the
bag-level prediction. These methods typically capture both local interactions
among adjacent instances and global, long-range dependencies through various
mechanisms. However, they treat attention values deterministically, potentially
overlooking uncertainty in the contribution of individual instances. In this
work we propose a novel probabilistic framework that estimates a probability
distribution over the attention values, and accounts for both global and local
interactions. In a comprehensive evaluation involving {\color{review} eleven}
state-of-the-art baselines and three medical datasets, we show that our
approach achieves top predictive performance in different metrics. Moreover,
the probabilistic treatment of the attention provides uncertainty maps that are
interpretable in terms of illness localization.

</details>


### [67] [Open-set Cross Modal Generalization via Multimodal Unified Representation](https://arxiv.org/abs/2507.14935)
*Hai Huang,Yan Xia,Shulei Wang,Hanting Wang,Minghui Fang,Shengpeng Ji,Sashuai Zhou,Tao Jin,Zhou Zhao*

Main category: cs.CV

TL;DR: 本文提出了开放集跨模态泛化（OSCMG）任务，扩展了跨模态泛化（CMG）到开放集环境，并提出了MICU方法，通过FCMI和CUJP组件提升模型在开放集条件下的性能。


<details>
  <summary>Details</summary>
Motivation: 现有跨模态统一表示方法未考虑开放集环境，而实际应用中常遇到未见类别和新模态的情况，因此需要更鲁棒的泛化能力。

Method: 提出MICU方法，包括FCMI（细粒度与粗粒度掩码多模态对比学习）和CUJP（跨模态统一拼图），分别增强多模态对齐和特征多样性。

Result: 在CMG和新提出的OSCMG任务上的实验验证了方法的有效性。

Conclusion: MICU通过结合对比学习和自监督学习，显著提升了开放集跨模态泛化能力。

Abstract: This paper extends Cross Modal Generalization (CMG) to open-set environments
by proposing the more challenging Open-set Cross Modal Generalization (OSCMG)
task. This task evaluates multimodal unified representations in open-set
conditions, addressing the limitations of prior closed-set cross-modal
evaluations. OSCMG requires not only cross-modal knowledge transfer but also
robust generalization to unseen classes within new modalities, a scenario
frequently encountered in real-world applications. Existing multimodal unified
representation work lacks consideration for open-set environments. To tackle
this, we propose MICU, comprising two key components: Fine-Coarse Masked
multimodal InfoNCE (FCMI) and Cross modal Unified Jigsaw Puzzles (CUJP). FCMI
enhances multimodal alignment by applying contrastive learning at both holistic
semantic and temporal levels, incorporating masking to enhance generalization.
CUJP enhances feature diversity and model uncertainty by integrating
modality-agnostic feature selection with self-supervised learning, thereby
strengthening the model's ability to handle unknown categories in open-set
tasks. Extensive experiments on CMG and the newly proposed OSCMG validate the
effectiveness of our approach. The code is available at
https://github.com/haihuangcode/CMG.

</details>


### [68] [Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices](https://arxiv.org/abs/2507.14959)
*Saeid Ghafouri,Mohsen Fayyaz,Xiangchen Li,Deepu John,Bo Ji,Dimitrios Nikolopoulos,Hans Vandierendonck*

Main category: cs.CV

TL;DR: Polymorph是一种上下文感知框架，通过动态激活轻量级低秩适配器（LoRA）来优化嵌入式设备上的实时多标签视频分类，显著降低能耗并提高性能。


<details>
  <summary>Details</summary>
Motivation: 嵌入式设备在实时多标签视频分类中面临计算和能源预算的限制，而视频流的结构特性（如标签稀疏性、时间连续性和标签共现）可用于更高效的推理。

Method: Polymorph框架动态激活每帧所需的轻量级LoRA适配器，这些适配器专用于基于共现模式的子集类别，并作为共享主干的LoRA权重实现。

Result: Polymorph在TAO数据集上实现了能耗降低40%，mAP提高9个点。

Conclusion: Polymorph通过模块化策略显著提升了嵌入式设备上实时多标签视频分类的效率和性能。

Abstract: Real-time multi-label video classification on embedded devices is constrained
by limited compute and energy budgets. Yet, video streams exhibit structural
properties such as label sparsity, temporal continuity, and label co-occurrence
that can be leveraged for more efficient inference. We introduce Polymorph, a
context-aware framework that activates a minimal set of lightweight Low Rank
Adapters (LoRA) per frame. Each adapter specializes in a subset of classes
derived from co-occurrence patterns and is implemented as a LoRA weight over a
shared backbone. At runtime, Polymorph dynamically selects and composes only
the adapters needed to cover the active labels, avoiding full-model switching
and weight merging. This modular strategy improves scalability while reducing
latency and energy overhead. Polymorph achieves 40% lower energy consumption
and improves mAP by 9 points over strong baselines on the TAO dataset.
Polymorph is open source at https://github.com/inference-serving/polymorph/.

</details>


### [69] [Decision PCR: Decision version of the Point Cloud Registration task](https://arxiv.org/abs/2507.14965)
*Yaojie Zhang,Tianlun Huang,Weijun Wang,Wei Feng*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的低重叠点云配准（PCR）评估方法，显著提升了现有配准方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统评估指标在极低内点比例下失效，论文重新审视了配准结果评估问题，提出了决策版本的PCR任务。

Method: 构建了基于3DMatch的数据集，训练深度学习分类器评估配准质量，并将其集成到标准PCR流程中。

Result: 结合GeoTransformer等方法，在3DLoMatch基准上达到86.97%的配准召回率，并在ETH数据集上表现出强泛化能力。

Conclusion: 论文首次通过深度学习框架解决了决策PCR任务，显著提升了配准性能，具有广泛适用性。

Abstract: Low-overlap point cloud registration (PCR) remains a significant challenge in
3D vision. Traditional evaluation metrics, such as Maximum Inlier Count, become
ineffective under extremely low inlier ratios. In this paper, we revisit the
registration result evaluation problem and identify the Decision version of the
PCR task as the fundamental problem. To address this Decision PCR task, we
propose a data-driven approach. First, we construct a corresponding dataset
based on the 3DMatch dataset. Then, a deep learning-based classifier is trained
to reliably assess registration quality, overcoming the limitations of
traditional metrics. To our knowledge, this is the first comprehensive study to
address this task through a deep learning framework. We incorporate this
classifier into standard PCR pipelines. When integrated with our approach,
existing state-of-the-art PCR methods exhibit significantly enhanced
registration performance. For example, combining our framework with
GeoTransformer achieves a new SOTA registration recall of 86.97\% on the
challenging 3DLoMatch benchmark. Our method also demonstrates strong
generalization capabilities on the unseen outdoor ETH dataset.

</details>


### [70] [Hierarchical Cross-modal Prompt Learning for Vision-Language Models](https://arxiv.org/abs/2507.14976)
*Hao Zheng,Shunzhi Yang,Zhuoxin He,Jinfeng Yang,Zhenhua Huang*

Main category: cs.CV

TL;DR: HiCroPL是一种分层跨模态提示学习框架，通过双向知识流提升视觉语言模型的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 预训练的视觉语言模型（如CLIP）在下游任务中保持泛化能力仍具挑战性，现有提示学习方法存在模态隔离和层次语义衰减问题。

Method: HiCroPL通过分层知识映射器和轻量级知识代理，实现文本和视觉模态的双向知识流动，增强语义表示。

Result: 在四个任务和11个基准测试中，HiCroPL表现优异，达到最先进水平。

Conclusion: HiCroPL通过分层跨模态交互有效解决了模态隔离和语义衰减问题，显著提升了模型的泛化能力。

Abstract: Pre-trained Vision-Language Models (VLMs) such as CLIP have shown excellent
generalization abilities. However, adapting these large-scale models to
downstream tasks while preserving their generalization capabilities remains
challenging. Although prompt learning methods have shown promise, they suffer
from two fundamental bottlenecks that limit generalization: (a) modality
isolation, and (b) hierarchical semantic decay. To address these limitations,
we propose HiCroPL, a Hierarchical Cross-modal Prompt Learning framework that
establishes bidirectional knowledge flow between text and vision modalities,
enabling them to refine their semantics mutually. HiCroPL routes knowledge
flows by leveraging the complementary strengths of text and vision. In early
layers, text prompts inject relatively clear semantics into visual prompts
through a hierarchical knowledge mapper, enhancing the representation of
low-level visual semantics. In later layers, visual prompts encoding specific
task-relevant objects flow back to refine text prompts, enabling deeper
alignment. Crucially, our hierarchical knowledge mapper allows representations
at multi-scales to be fused, ensuring that deeper representations retain
transferable shallow semantics thereby enhancing generalization. We further
introduce a lightweight layer-specific knowledge proxy to enable efficient
cross-modal interactions. Extensive evaluations across four tasks demonstrate
HiCroPL's superior performance, achieving state-of-the-art results on 11
benchmarks with significant improvements. Code is available at:
https://github.com/zzeoZheng/HiCroPL.

</details>


### [71] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: MLLMs在图像回归任务中表现不佳，预设词汇和通用提示无效。提出RvTC方法，通过灵活分箱和语义提示提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能利用文本输入的语义理解，性能与纯图像模型相当。

Method: 提出RvTC，用分箱替代预设词汇，结合数据特异性语义提示。

Result: 在四个数据集上达到SOTA，语义提示显著提升性能（如AVA数据集从0.83到0.90）。

Conclusion: 语义提示对多模态回归任务至关重要，RvTC方法有效。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [72] [Axis-Aligned Document Dewarping](https://arxiv.org/abs/2507.15000)
*Chaoyun Wang,I-Chao Shen,Takeo Igarashi,Nanning Zheng,Caigui Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于轴对齐几何约束的文档去扭曲方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖标注数据，未充分利用文档的几何特性。

Method: 训练阶段引入轴对齐几何约束，推理阶段采用轴对齐预处理策略。

Result: 在多个基准测试中达到SOTA，AAD指标提升18.2%~34.5%。

Conclusion: 结合几何特性能有效提升文档去扭曲效果。

Abstract: Document dewarping is crucial for many applications. However, existing
learning-based methods primarily rely on supervised regression with annotated
data without leveraging the inherent geometric properties in physical documents
to the dewarping process. Our key insight is that a well-dewarped document is
characterized by transforming distorted feature lines into axis-aligned ones.
This property aligns with the inherent axis-aligned nature of the discrete grid
geometry in planar documents. In the training phase, we propose an axis-aligned
geometric constraint to enhance document dewarping. In the inference phase, we
propose an axis alignment preprocessing strategy to reduce the dewarping
difficulty. In the evaluation phase, we introduce a new metric, Axis-Aligned
Distortion (AAD), that not only incorporates geometric meaning and aligns with
human visual perception but also demonstrates greater robustness. As a result,
our method achieves SOTA results on multiple existing benchmarks and achieves
18.2%~34.5% improvements on the AAD metric.

</details>


### [73] [FastSmoothSAM: A Fast Smooth Method For Segment Anything Model](https://arxiv.org/abs/2507.15008)
*Jiasheng Xu,Yewang Chen*

Main category: cs.CV

TL;DR: 论文提出了一种基于B-Spline曲线拟合的优化方法，用于提升FastSAM的边缘质量，解决了FastSAM生成的锯齿边缘问题。


<details>
  <summary>Details</summary>
Motivation: FastSAM虽然实现了实时分割，但其生成的边缘存在锯齿状问题，影响了分割的视觉质量和分析准确性。

Method: 采用B-Spline曲线拟合技术，通过四阶段细化过程（包括两轮曲线拟合）来平滑锯齿边缘。

Result: 该方法显著提升了边缘的视觉质量和分析准确性，同时保持了实时处理能力。

Conclusion: 提出的方法增强了FastSAM的实用性，使其在工业自动化、医疗影像和自主系统等场景中更具潜力。

Abstract: Accurately identifying and representing object edges is a challenging task in
computer vision and image processing. The Segment Anything Model (SAM) has
significantly influenced the field of image segmentation, but suffers from high
memory consumption and long inference times, limiting its efficiency in
real-time applications. To address these limitations, Fast Segment Anything
(FastSAM) was proposed, achieving real-time segmentation. However, FastSAM
often generates jagged edges that deviate from the true object shapes.
Therefore, this paper introduces a novel refinement approach using B-Spline
curve fitting techniques to enhance the edge quality in FastSAM. Leveraging the
robust shape control and flexible geometric construction of B-Splines, a
four-stage refining process involving two rounds of curve fitting is employed
to effectively smooth jagged edges. This approach significantly improves the
visual quality and analytical accuracy of object edges without compromising
critical geometric information. The proposed method improves the practical
utility of FastSAM by improving segmentation accuracy while maintaining
real-time processing capabilities. This advancement unlocks greater potential
for FastSAM technology in various real-world scenarios, such as industrial
automation, medical imaging, and autonomous systems, where precise and
efficient edge recognition is crucial.

</details>


### [74] [Towards Video Thinking Test: A Holistic Benchmark for Advanced Video Reasoning and Understanding](https://arxiv.org/abs/2507.15028)
*Yuanhan Zhang,Yunice Chew,Yuhao Dong,Aria Leo,Bo Hu,Ziwei Liu*

Main category: cs.CV

TL;DR: Video-TT是一个新基准测试，用于评估视频大语言模型在理解和解释真实世界视频时的正确性和鲁棒性，结果显示与人类表现存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试未能充分反映视频大语言模型与人类在视频理解中的正确性和鲁棒性差距，因此需要更全面的评估工具。

Method: 通过构建包含1000个YouTube Shorts视频的数据集，每个视频配有一个开放式问题和四个对抗性问题，测试模型的视觉和叙事理解能力。

Result: 评估显示视频大语言模型在复杂视觉叙事理解和对抗性问题上的表现显著低于人类。

Conclusion: Video-TT揭示了视频大语言模型在真实世界视频理解中的局限性，为未来研究提供了改进方向。

Abstract: Human intelligence requires correctness and robustness, with the former being
foundational for the latter. In video understanding, correctness ensures the
accurate interpretation of visual content, and robustness maintains consistent
performance in challenging conditions. Despite advances in video large language
models (video LLMs), existing benchmarks inadequately reflect the gap between
these models and human intelligence in maintaining correctness and robustness
in video interpretation. We introduce the Video Thinking Test (Video-TT), to
assess if video LLMs can interpret real-world videos as effectively as humans.
Video-TT reflects genuine gaps in understanding complex visual narratives, and
evaluates robustness against natural adversarial questions. Video-TT comprises
1,000 YouTube Shorts videos, each with one open-ended question and four
adversarial questions that probe visual and narrative complexity. Our
evaluation shows a significant gap between video LLMs and human performance.

</details>


### [75] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，用于桥接理论方程与实际成像应用，包含8000个解剖学真实的人体乳房模型和1600万次频域波模拟。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程数值求解器计算量大且不稳定，限制了准实时图像重建的实际应用。神经算子虽能加速求解，但现有数据集过于简化现实复杂性。

Method: 提出OpenBreastUS数据集，包含8000个真实乳房模型和1600万次频域波模拟，用于评估神经算子在正向模拟和逆向成像任务中的性能。

Result: 首次展示了神经算子求解器在人体乳房活体成像中的高效应用。

Conclusion: OpenBreastUS为开发创新神经PDE求解器提供了平台，并促进其在现实医学成像问题中的部署。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [76] [EBA-AI: Ethics-Guided Bias-Aware AI for Efficient Underwater Image Enhancement and Coral Reef Monitoring](https://arxiv.org/abs/2507.15036)
*Lyes Saad Saoud,Irfan Hussain*

Main category: cs.CV

TL;DR: EBA-AI是一个基于伦理和偏见的AI框架，用于水下图像增强，解决了数据集偏见、高计算成本和透明度问题。


<details>
  <summary>Details</summary>
Motivation: 水下图像增强对海洋保护至关重要，但现有AI模型存在数据集偏见、高计算成本和透明度不足的问题。

Method: EBA-AI利用CLIP嵌入检测和缓解数据集偏见，并采用自适应处理优化能效。

Result: 实验表明，PSNR仅下降1.0 dB，但计算成本显著降低，适用于大规模海洋监测。

Conclusion: EBA-AI在效率、公平性和可解释性方面优于现有方法，为可持续海洋保护提供了解决方案。

Abstract: Underwater image enhancement is vital for marine conservation, particularly
coral reef monitoring. However, AI-based enhancement models often face dataset
bias, high computational costs, and lack of transparency, leading to potential
misinterpretations. This paper introduces EBA-AI, an ethics-guided bias-aware
AI framework to address these challenges. EBA-AI leverages CLIP embeddings to
detect and mitigate dataset bias, ensuring balanced representation across
varied underwater environments. It also integrates adaptive processing to
optimize energy efficiency, significantly reducing GPU usage while maintaining
competitive enhancement quality. Experiments on LSUI400, Oceanex, and UIEB100
show that while PSNR drops by a controlled 1.0 dB, computational savings enable
real-time feasibility for large-scale marine monitoring. Additionally,
uncertainty estimation and explainability techniques enhance trust in AI-driven
environmental decisions. Comparisons with CycleGAN, FunIEGAN, RAUNENet,
WaterNet, UGAN, PUGAN, and UTUIE validate EBA-AI's effectiveness in balancing
efficiency, fairness, and interpretability in underwater image processing. By
addressing key limitations of AI-driven enhancement, this work contributes to
sustainable, bias-aware, and computationally efficient marine conservation
efforts. For interactive visualizations, animations, source code, and access to
the preprint, visit: https://lyessaadsaoud.github.io/EBA-AI/

</details>


### [77] [OmniVTON: Training-Free Universal Virtual Try-On](https://arxiv.org/abs/2507.15037)
*Zhaotong Yang,Yuhui Li,Shengfeng He,Xinzhe Li,Yangyang Xu,Junyu Dong,Yong Du*

Main category: cs.CV

TL;DR: OmniVTON是一个无需训练的统一虚拟试穿框架，通过解耦服装和姿势条件，实现跨场景的高保真纹理和姿势一致性。


<details>
  <summary>Details</summary>
Motivation: 现有虚拟试穿技术要么依赖监督学习（高保真但泛化性差），要么依赖无监督学习（适应性强但受数据偏差限制），缺乏统一的解决方案。

Method: OmniVTON通过服装先验生成机制和连续边界缝合技术保留细节，利用DDIM反演实现姿势对齐，解耦服装和姿势约束。

Result: 实验表明，OmniVTON在多样化数据集、服装类型和应用场景中表现优异，首次实现多人物虚拟试穿。

Conclusion: OmniVTON通过解耦条件，消除了扩散模型的偏差，成为首个跨场景、多人物虚拟试穿的统一框架。

Abstract: Image-based Virtual Try-On (VTON) techniques rely on either supervised
in-shop approaches, which ensure high fidelity but struggle with cross-domain
generalization, or unsupervised in-the-wild methods, which improve adaptability
but remain constrained by data biases and limited universality. A unified,
training-free solution that works across both scenarios remains an open
challenge. We propose OmniVTON, the first training-free universal VTON
framework that decouples garment and pose conditioning to achieve both texture
fidelity and pose consistency across diverse settings. To preserve garment
details, we introduce a garment prior generation mechanism that aligns clothing
with the body, followed by continuous boundary stitching technique to achieve
fine-grained texture retention. For precise pose alignment, we utilize DDIM
inversion to capture structural cues while suppressing texture interference,
ensuring accurate body alignment independent of the original image textures. By
disentangling garment and pose constraints, OmniVTON eliminates the bias
inherent in diffusion models when handling multiple conditions simultaneously.
Experimental results demonstrate that OmniVTON achieves superior performance
across diverse datasets, garment types, and application scenarios. Notably, it
is the first framework capable of multi-human VTON, enabling realistic garment
transfer across multiple individuals in a single scene. Code is available at
https://github.com/Jerome-Young/OmniVTON

</details>


### [78] [Rethinking Pan-sharpening: Principled Design, Unified Training, and a Universal Loss Surpass Brute-Force Scaling](https://arxiv.org/abs/2507.15059)
*Ran Zhang,Xuanhua He,Li Xueheng,Ke Cao,Liu Liu,Wenbo Xu,Fang Jiabin,Yang Qize,Jie Zhang*

Main category: cs.CV

TL;DR: PanTiny是一个轻量级、单步的pan-sharpening框架，通过多数据集联合训练和复合损失函数，实现了高效且泛化性能强的模型。


<details>
  <summary>Details</summary>
Motivation: 当前pan-sharpening领域倾向于使用大型复杂模型，但存在计算开销大和泛化能力差的问题。本文旨在挑战这一范式。

Method: 提出PanTiny框架，采用多数据集联合训练（WV2、WV3、GF2）和复合损失函数，优化模型设计和训练策略。

Result: PanTiny在性能和效率上优于大多数大型专用模型，并显著提升了全分辨率数据的泛化能力。

Conclusion: 通过模型设计、训练范式和损失函数的优化，可以超越暴力扩展，推动pan-sharpening领域向高效、泛化和数据意识的方向发展。

Abstract: The field of pan-sharpening has recently seen a trend towards increasingly
large and complex models, often trained on single, specific satellite datasets.
This approach, however, leads to high computational overhead and poor
generalization on full resolution data, a paradigm we challenge in this paper.
In response to this issue, we propose PanTiny, a lightweight, single-step
pan-sharpening framework designed for both efficiency and robust performance.
More critically, we introduce multiple-in-one training paradigm, where a
single, compact model is trained simultaneously on three distinct satellite
datasets (WV2, WV3, and GF2) with different resolution and spectral
information. Our experiments show that this unified training strategy not only
simplifies deployment but also significantly boosts generalization on
full-resolution data. Further, we introduce a universally powerful composite
loss function that elevates the performance of almost all of models for
pan-sharpening, pushing state-of-the-art metrics into a new era. Our PanTiny
model, benefiting from these innovations, achieves a superior
performance-to-efficiency balance, outperforming most larger, specialized
models. Through extensive ablation studies, we validate that principled
engineering in model design, training paradigms, and loss functions can surpass
brute-force scaling. Our work advocates for a community-wide shift towards
creating efficient, generalizable, and data-conscious models for
pan-sharpening. The code is available at
https://github.com/Zirconium233/PanTiny .

</details>


### [79] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种基于视频扩散模型的身份保持框架，通过可学习的姿态对齐和分布感知的身份适配器，显著提升了身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性。

Method: 使用可学习层预测相似变换矩阵对齐姿态，结合全局内容感知的面部编码器和分布感知的身份适配器，并在推理阶段引入HJB面部优化。

Result: 在基准测试中，StableAnimator++ 在定性和定量上均表现出色。

Conclusion: StableAnimator++ 通过创新的姿态对齐和身份保持机制，显著提升了动画生成的质量和身份一致性。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [80] [Aesthetics is Cheap, Show me the Text: An Empirical Evaluation of State-of-the-Art Generative Models for OCR](https://arxiv.org/abs/2507.15085)
*Peirong Zhang,Haowei Xu,Jiaxin Zhang,Guitao Xu,Xuhan Zheng,Zhenhua Yang,Junle Liu,Yuyi Zhang,Lianwen Jin*

Main category: cs.CV

TL;DR: 评估当前最先进的生成模型在文本图像生成和编辑方面的能力，提出将逼真文本图像生成和编辑作为通用生成模型的基础技能。


<details>
  <summary>Details</summary>
Motivation: 探讨专业图像生成器和统一生成模型是否能掌握文本图像生成和编辑的复杂性。

Method: 选择33个代表性任务，分为五类，评估六种模型在闭源和开源领域的表现。

Result: 识别当前生成模型在OCR任务中的弱点，提出改进方向。

Conclusion: 逼真文本图像生成和编辑应作为通用生成模型的基础技能，而非依赖专业解决方案。

Abstract: Text image is a unique and crucial information medium that integrates visual
aesthetics and linguistic semantics in modern e-society. Due to their subtlety
and complexity, the generation of text images represents a challenging and
evolving frontier in the image generation field. The recent surge of
specialized image generators (\emph{e.g.}, Flux-series) and unified generative
models (\emph{e.g.}, GPT-4o), which demonstrate exceptional fidelity, raises a
natural question: can they master the intricacies of text image generation and
editing? Motivated by this, we assess current state-of-the-art generative
models' capabilities in terms of text image generation and editing. We
incorporate various typical optical character recognition (OCR) tasks into our
evaluation and broaden the concept of text-based generation tasks into OCR
generative tasks. We select 33 representative tasks and categorize them into
five categories: document, handwritten text, scene text, artistic text, and
complex \& layout-rich text. For comprehensive evaluation, we examine six
models across both closed-source and open-source domains, using tailored,
high-quality image inputs and prompts. Through this evaluation, we draw crucial
observations and identify the weaknesses of current generative models for OCR
tasks. We argue that photorealistic text image generation and editing should be
internalized as foundational skills into general-domain generative models,
rather than being delegated to specialized solutions, and we hope this
empirical analysis can provide valuable insights for the community to achieve
this goal. This evaluation is online and will be continuously updated at our
GitHub repository.

</details>


### [81] [Visual Place Recognition for Large-Scale UAV Applications](https://arxiv.org/abs/2507.15089)
*Ioannis Tsampikos Papapetros,Ioannis Kansizoglou,Antonios Gasteratos*

Main category: cs.CV

TL;DR: 论文提出了LASED数据集和可转向CNN，用于解决无人机视觉地点识别中的数据集不足和旋转模糊问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 无人机视觉地点识别面临大规模高空数据集稀缺和图像旋转模糊的挑战，限制了模型的泛化能力。

Method: 引入LASED大规模数据集，并提出使用可转向CNN处理旋转模糊问题。

Result: 实验表明，LASED训练模型召回率显著提升，可转向CNN比传统CNN平均提高12%召回率。

Conclusion: 结合大规模数据集和旋转等变网络，显著提升了无人机视觉地点识别的鲁棒性和泛化能力。

Abstract: Visual Place Recognition (vPR) plays a crucial role in Unmanned Aerial
Vehicle (UAV) navigation, enabling robust localization across diverse
environments. Despite significant advancements, aerial vPR faces unique
challenges due to the limited availability of large-scale, high-altitude
datasets, which limits model generalization, along with the inherent rotational
ambiguity in UAV imagery. To address these challenges, we introduce LASED, a
large-scale aerial dataset with approximately one million images,
systematically sampled from 170,000 unique locations throughout Estonia over a
decade, offering extensive geographic and temporal diversity. Its structured
design ensures clear place separation significantly enhancing model training
for aerial scenarios. Furthermore, we propose the integration of steerable
Convolutional Neural Networks (CNNs) to explicitly handle rotational variance,
leveraging their inherent rotational equivariance to produce robust,
orientation-invariant feature representations. Our extensive benchmarking
demonstrates that models trained on LASED achieve significantly higher recall
compared to those trained on smaller, less diverse datasets, highlighting the
benefits of extensive geographic coverage and temporal diversity. Moreover,
steerable CNNs effectively address rotational ambiguity inherent in aerial
imagery, consistently outperforming conventional convolutional architectures,
achieving on average 12\% recall improvement over the best-performing
non-steerable network. By combining structured, large-scale datasets with
rotation-equivariant neural networks, our approach significantly enhances model
robustness and generalization for aerial vPR.

</details>


### [82] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 提出了首个ESD出血源数据集BleedOrigin-Bench和新框架BleedOrigin-Net，用于实时定位和跟踪出血源，显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前AI方法仅关注出血区域分割，缺乏对出血源的精确定位和动态跟踪，且缺乏专用数据集。

Method: 引入BleedOrigin-Bench数据集和BleedOrigin-Net双阶段检测-跟踪框架，结合检测和跟踪技术。

Result: 在出血起始检测、初始源定位和点跟踪任务中分别达到96.85%、70.24%和96.11%的准确率。

Conclusion: BleedOrigin-Net在ESD出血源定位和跟踪中表现优异，填补了现有技术空白。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [83] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet是一种基于多任务ResNet架构的方法，用于实时SLAM系统中的闭环检测，通过在线学习和DISK描述符提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测的准确性和嵌入式硬件实时计算的挑战。

Method: 采用多任务ResNet架构，结合在线学习和DISK描述符，优化嵌入式设备性能。

Result: LoopNet在动态视觉数据集上表现优于传统方法，并提供了新的闭环检测数据集LoopDB。

Conclusion: LoopNet通过创新架构和在线学习，显著提升了闭环检测的准确性和实时性。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [84] [Enhancing Visual Planning with Auxiliary Tasks and Multi-token Prediction](https://arxiv.org/abs/2507.15130)
*Ce Zhang,Yale Song,Ruta Desai,Michael Louis Iuzzolino,Joseph Tighe,Gedas Bertasius,Satwik Kottur*

Main category: cs.CV

TL;DR: VPA通过视频预测用户行为序列，提出辅助任务增强和多令牌预测方法，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决长时视觉规划中数据稀缺和动作空间建模不足的问题。

Method: 采用辅助任务增强和多令牌预测技术。

Result: 在COIN和CrossTask数据集上分别提升7.3%和3.4%。

Conclusion: VideoPlan方法在VPA任务中表现优异，且适用于其他挑战性任务。

Abstract: Visual Planning for Assistance (VPA) aims to predict a sequence of user
actions required to achieve a specified goal based on a video showing the
user's progress. Although recent advances in multimodal large language models
(MLLMs) have shown promising results in video understanding, long-horizon
visual planning remains a challenging problem. We identify two challenges in
training large MLLMs for video-based planning tasks: (1) scarcity of procedural
annotations, limiting the model's ability to learn procedural task dynamics
effectively, and (2) inefficiency of next-token prediction objective to
explicitly capture the structured action space for visual planning when
compared to free-form, natural language. To tackle data scarcity, we introduce
Auxiliary Task Augmentation. We design and train our model on auxiliary tasks
relevant to long-horizon video-based planning (e.g., goal prediction) to
augment the model's planning ability. To more explicitly model the structured
action space unique to visual planning tasks, we leverage Multi-token
Prediction, extending traditional next-token prediction by using multiple heads
to predict multiple future tokens during training. Our approach, VideoPlan,
achieves state-of-the-art VPA performance on the COIN and CrossTask datasets,
surpassing prior methods by 7.3% and 3.4%, respectively, when predicting 3
future actions. We further extend our method to the challenging Ego4D Long-term
Action Anticipation task, and show that it is on par with the state-of-the-art
approaches despite not using specialized egocentric features. Code will be made
available.

</details>


### [85] [Event-based Graph Representation with Spatial and Motion Vectors for Asynchronous Object Detection](https://arxiv.org/abs/2507.15150)
*Aayush Atul Verma,Arpitsinh Vaghela,Bharatesh Chakravarthi,Kaustav Chanda,Yezhou Yang*

Main category: cs.CV

TL;DR: 提出了一种新颖的时空多图表示方法，用于更好地捕捉事件传感器的稀疏异步数据，显著提升了检测精度和效率。


<details>
  <summary>Details</summary>
Motivation: 事件传感器数据稀疏且异步，传统方法将其转换为密集张量会丧失其优势，而现有图表示方法在时空动态建模上表现不佳。

Method: 构建了两个解耦的图：空间图（利用B样条基函数建模全局结构）和时间图（利用运动向量注意力建模局部动态变化），以替代计算昂贵的3D核。

Result: 在Gen1和eTraM数据集上，检测精度提升6%，速度提升5倍，参数减少且计算成本不变。

Conclusion: 结构化图建模在异步视觉任务中表现出色，为事件传感器数据处理提供了高效解决方案。

Abstract: Event-based sensors offer high temporal resolution and low latency by
generating sparse, asynchronous data. However, converting this irregular data
into dense tensors for use in standard neural networks diminishes these
inherent advantages, motivating research into graph representations. While such
methods preserve sparsity and support asynchronous inference, their performance
on downstream tasks remains limited due to suboptimal modeling of
spatiotemporal dynamics. In this work, we propose a novel spatiotemporal
multigraph representation to better capture spatial structure and temporal
changes. Our approach constructs two decoupled graphs: a spatial graph
leveraging B-spline basis functions to model global structure, and a temporal
graph utilizing motion vector-based attention for local dynamic changes. This
design enables the use of efficient 2D kernels in place of computationally
expensive 3D kernels. We evaluate our method on the Gen1 automotive and eTraM
datasets for event-based object detection, achieving over a 6% improvement in
detection accuracy compared to previous graph-based works, with a 5x speedup,
reduced parameter count, and no increase in computational cost. These results
highlight the effectiveness of structured graph modeling for asynchronous
vision. Project page: eventbasedvision.github.io/eGSMV.

</details>


### [86] [MeshMamba: State Space Models for Articulated 3D Mesh Generation and Reconstruction](https://arxiv.org/abs/2507.15212)
*Yusuke Yoshiyasu,Leyuan Sun,Ryusuke Sagawa*

Main category: cs.CV

TL;DR: MeshMamba利用Mamba-SSMs高效处理大规模3D网格数据，生成和重建包含衣物和手部细节的密集人体网格。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理大规模3D网格数据时效率低下，MeshMamba旨在通过Mamba-SSMs提升处理能力。

Method: 通过序列化网格顶点并排序，结合Mamba-SSMs，设计了MambaDiff3D和Mamba-HMR两个模型。

Result: MambaDiff3D在生成3D人体网格任务中表现优异，Mamba-HMR实现了近实时全身重建。

Conclusion: MeshMamba展示了Mamba-SSMs在3D网格处理中的潜力，为高效生成和重建提供了新思路。

Abstract: In this paper, we introduce MeshMamba, a neural network model for learning 3D
articulated mesh models by employing the recently proposed Mamba State Space
Models (Mamba-SSMs). MeshMamba is efficient and scalable in handling a large
number of input tokens, enabling the generation and reconstruction of body mesh
models with more than 10,000 vertices, capturing clothing and hand geometries.
The key to effectively learning MeshMamba is the serialization technique of
mesh vertices into orderings that are easily processed by Mamba. This is
achieved by sorting the vertices based on body part annotations or the 3D
vertex locations of a template mesh, such that the ordering respects the
structure of articulated shapes. Based on MeshMamba, we design 1) MambaDiff3D,
a denoising diffusion model for generating 3D articulated meshes and 2)
Mamba-HMR, a 3D human mesh recovery model that reconstructs a human body shape
and pose from a single image. Experimental results showed that MambaDiff3D can
generate dense 3D human meshes in clothes, with grasping hands, etc., and
outperforms previous approaches in the 3D human shape generation task.
Additionally, Mamba-HMR extends the capabilities of previous non-parametric
human mesh recovery approaches, which were limited to handling body-only poses
using around 500 vertex tokens, to the whole-body setting with face and hands,
while achieving competitive performance in (near) real-time.

</details>


### [87] [Improving Joint Embedding Predictive Architecture with Diffusion Noise](https://arxiv.org/abs/2507.15216)
*Yuping Qiu,Rui Zhu,Ying-cong Chen*

Main category: cs.CV

TL;DR: 论文提出了一种结合自监督学习（SSL）和生成模型的方法，通过引入扩散噪声增强表示能力，并在下游分类任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 自监督学习在特征学习中表现优异，但在图像生成和细节增强方面不如生成模型。结合两者的优势可以进一步提升SSL的表示能力。

Method: 提出N-JEPA方法，将扩散噪声通过掩码标记的位置嵌入引入到掩码图像建模（MIM）中，并采用多级噪声调度增强模型鲁棒性。

Result: 在分类任务中验证了方法的有效性。

Conclusion: 结合扩散噪声的SSL方法能够提升模型的表示能力，为下游任务提供更优的性能。

Abstract: Self-supervised learning has become an incredibly successful method for
feature learning, widely applied to many downstream tasks. It has proven
especially effective for discriminative tasks, surpassing the trending
generative models. However, generative models perform better in image
generation and detail enhancement. Thus, it is natural for us to find a
connection between SSL and generative models to further enhance the
representation capacity of SSL. As generative models can create new samples by
approximating the data distribution, such modeling should also lead to a
semantic understanding of the raw visual data, which is necessary for
recognition tasks. This enlightens us to combine the core principle of the
diffusion model: diffusion noise, with SSL to learn a competitive recognition
model. Specifically, diffusion noise can be viewed as a particular state of
mask that reveals a close relationship between masked image modeling (MIM) and
diffusion models. In this paper, we propose N-JEPA (Noise-based JEPA) to
incorporate diffusion noise into MIM by the position embedding of masked
tokens. The multi-level noise schedule is a series of feature augmentations to
further enhance the robustness of our model. We perform a comprehensive study
to confirm its effectiveness in the classification of downstream tasks. Codes
will be released soon in public.

</details>


### [88] [Hierarchical Part-based Generative Model for Realistic 3D Blood Vessel](https://arxiv.org/abs/2507.15223)
*Siqi Chen,Guoqing Zhang,Jiahao Lai,Bingzhi Shen,Sihong Zhang,Caixia Dong,Xuejin Chen,Yang Li*

Main category: cs.CV

TL;DR: 提出了一种基于分层的3D血管生成框架，通过分离全局拓扑和局部几何细节，显著提升了复杂血管网络的建模效果。


<details>
  <summary>Details</summary>
Motivation: 由于血管的复杂分支、弯曲和不规则形状，准确建模其几何和拓扑结构仍具挑战性。

Method: 采用三阶段方法：生成关键图建模全局结构，基于几何属性生成血管段，最后整合局部段到全局图中。

Result: 在真实数据集上验证，性能优于现有方法，首次成功应用基于部分的生成方法。

Conclusion: 为3D血管建模设定了新基准，代码已开源。

Abstract: Advancements in 3D vision have increased the impact of blood vessel modeling
on medical applications. However, accurately representing the complex geometry
and topology of blood vessels remains a challenge due to their intricate
branching patterns, curvatures, and irregular shapes. In this study, we propose
a hierarchical part-based frame work for 3D vessel generation that separates
the global binary tree-like topology from local geometric details. Our approach
proceeds in three stages: (1) key graph generation to model the overall
hierarchical struc ture, (2) vessel segment generation conditioned on geometric
properties, and (3) hierarchical vessel assembly by integrating the local
segments according to the global key graph. We validate our framework on real
world datasets, demonstrating superior performance over existing methods in
modeling complex vascular networks. This work marks the first successful
application of a part-based generative approach for 3D vessel modeling, setting
a new benchmark for vascular data generation. The code is available at:
https://github.com/CybercatChen/PartVessel.git.

</details>


### [89] [Mammo-SAE: Interpreting Breast Cancer Concept Learning with Sparse Autoencoders](https://arxiv.org/abs/2507.15227)
*Krishna Kanth Nakka*

Main category: cs.CV

TL;DR: 论文提出了一种基于稀疏自编码器（SAE）的可解释性方法，用于分析乳腺影像中的基础模型Mammo-CLIP，揭示了模型决策的潜在特征和混淆因素。


<details>
  <summary>Details</summary>
Motivation: 在乳腺影像等高风险领域，模型决策的可解释性对临床采用至关重要。

Method: 通过训练一个基于Mammo-CLIP的SAE（Mammo-SAE），识别和探究与临床相关乳腺概念（如肿块和可疑钙化）相关的潜在特征。

Result: 研究发现SAE潜在空间中激活的神经元与真实区域对齐，并揭示了影响模型决策的混淆因素。

Conclusion: 研究展示了SAE潜在表示在深入理解乳腺影像基础模型内部工作机制中的潜力。

Abstract: Interpretability is critical in high-stakes domains such as medical imaging,
where understanding model decisions is essential for clinical adoption. In this
work, we introduce Sparse Autoencoder (SAE)-based interpretability to breast
imaging by analyzing {Mammo-CLIP}, a vision--language foundation model
pretrained on large-scale mammogram image--report pairs. We train a patch-level
\texttt{Mammo-SAE} on Mammo-CLIP to identify and probe latent features
associated with clinically relevant breast concepts such as \textit{mass} and
\textit{suspicious calcification}. Our findings reveal that top activated class
level latent neurons in the SAE latent space often tend to align with ground
truth regions, and also uncover several confounding factors influencing the
model's decision-making process. Additionally, we analyze which latent neurons
the model relies on during downstream finetuning for improving the breast
concept prediction. This study highlights the promise of interpretable SAE
latent representations in providing deeper insight into the internal workings
of foundation models at every layer for breast imaging.

</details>


### [90] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: 提出了一种新的Coalescent Projection（CP）方法，结合伪类生成和自监督变换（SSTs），在极端领域偏移场景中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本学习中因参数过多导致的过拟合问题，并提升模型对未见领域的适应能力。

Method: 引入CP作为软提示的替代，结合伪类生成和SSTs，仅依赖基础领域数据。

Result: 在BSCD-FSL基准测试中表现优于现有SOTA方法。

Conclusion: CP和SSTs的组合有效解决了跨域少样本学习的挑战。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [91] [FreeCus: Free Lunch Subject-driven Customization in Diffusion Transformers](https://arxiv.org/abs/2507.15249)
*Yanbing Zhang,Zhe Wang,Qin Zhou,Mengping Yang*

Main category: cs.CV

TL;DR: FreeCus是一个无需训练的框架，通过注意力共享机制和动态分析，激活扩散变换器（DiT）的零样本能力，实现高质量主题驱动合成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖训练过程，限制了实际应用，且未能充分利用扩散变换器的零样本潜力。

Method: 提出注意力共享机制、动态分析改进特征提取，并整合多模态大语言模型（MLLMs）。

Result: 在多样场景中实现一致的主题合成，性能与需训练的方法相当或更优。

Conclusion: FreeCus展示了与现有工具的兼容性，为设计工作流和娱乐应用提供了新可能。

Abstract: In light of recent breakthroughs in text-to-image (T2I) generation,
particularly with diffusion transformers (DiT), subject-driven technologies are
increasingly being employed for high-fidelity customized production that
preserves subject identity from reference inputs, enabling thrilling design
workflows and engaging entertainment. Existing alternatives typically require
either per-subject optimization via trainable text embeddings or training
specialized encoders for subject feature extraction on large-scale datasets.
Such dependencies on training procedures fundamentally constrain their
practical applications. More importantly, current methodologies fail to fully
leverage the inherent zero-shot potential of modern diffusion transformers
(e.g., the Flux series) for authentic subject-driven synthesis. To bridge this
gap, we propose FreeCus, a genuinely training-free framework that activates
DiT's capabilities through three key innovations: 1) We introduce a pivotal
attention sharing mechanism that captures the subject's layout integrity while
preserving crucial editing flexibility. 2) Through a straightforward analysis
of DiT's dynamic shifting, we propose an upgraded variant that significantly
improves fine-grained feature extraction. 3) We further integrate advanced
Multimodal Large Language Models (MLLMs) to enrich cross-modal semantic
representations. Extensive experiments reflect that our method successfully
unlocks DiT's zero-shot ability for consistent subject synthesis across diverse
contexts, achieving state-of-the-art or comparable results compared to
approaches that require additional training. Notably, our framework
demonstrates seamless compatibility with existing inpainting pipelines and
control modules, facilitating more compelling experiences. Our code is
available at: https://github.com/Monalissaa/FreeCus.

</details>


### [92] [MinCD-PnP: Learning 2D-3D Correspondences with Approximate Blind PnP](https://arxiv.org/abs/2507.15257)
*Pei An,Jiaqi Yang,Muyao Peng,You Yang,Qiong Liu,Xiaolin Wu,Liangliang Nan*

Main category: cs.CV

TL;DR: 提出了一种基于近似盲PnP的对应学习方法MinCD-PnP，通过最小化2D和3D关键点之间的Chamfer距离来简化盲PnP，并设计了轻量级多任务学习模块MinCD-Net，显著提升了I2P配准的性能。


<details>
  <summary>Details</summary>
Motivation: 传统差分PnP对噪声和异常值敏感，影响了对应学习的有效性，而盲PnP对此具有鲁棒性，但计算成本高。

Method: 提出MinCD-PnP方法，简化盲PnP为最小化Chamfer距离的任务，并设计MinCD-Net模块集成到现有I2P配准架构中。

Result: 在多个数据集上实验表明，MinCD-Net在跨场景和跨数据集设置中均优于现有方法，实现了更高的内点率和配准召回率。

Conclusion: MinCD-PnP和MinCD-Net有效解决了传统PnP的敏感性问题，提升了I2P配准的鲁棒性和性能。

Abstract: Image-to-point-cloud (I2P) registration is a fundamental problem in computer
vision, focusing on establishing 2D-3D correspondences between an image and a
point cloud. The differential perspective-n-point (PnP) has been widely used to
supervise I2P registration networks by enforcing the projective constraints on
2D-3D correspondences. However, differential PnP is highly sensitive to noise
and outliers in the predicted correspondences. This issue hinders the
effectiveness of correspondence learning. Inspired by the robustness of blind
PnP against noise and outliers in correspondences, we propose an approximated
blind PnP based correspondence learning approach. To mitigate the high
computational cost of blind PnP, we simplify blind PnP to an amenable task of
minimizing Chamfer distance between learned 2D and 3D keypoints, called
MinCD-PnP. To effectively solve MinCD-PnP, we design a lightweight multi-task
learning module, named as MinCD-Net, which can be easily integrated into the
existing I2P registration architectures. Extensive experiments on 7-Scenes,
RGBD-V2, ScanNet, and self-collected datasets demonstrate that MinCD-Net
outperforms state-of-the-art methods and achieves a higher inlier ratio (IR)
and registration recall (RR) in both cross-scene and cross-dataset settings.

</details>


### [93] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过生成模型从稀疏信号中重建视频，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐重建方面的优势，优化视频压缩的感知质量。

Method: 1. 多粒度条件捕捉静态和动态信息；2. 设计高效传输的紧凑表示；3. 多条件训练增强鲁棒性。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下。

Conclusion: 条件扩散模型为视频压缩提供了感知优化的有效途径。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [94] [In-context Learning of Vision Language Models for Detection of Physical and Digital Attacks against Face Recognition Systems](https://arxiv.org/abs/2507.15285)
*Lazaro Janier Gonzalez-Soler,Maciej Salwowski,Christoph Busch*

Main category: cs.CV

TL;DR: 该论文提出了一种基于视觉语言模型（VLM）的上下文学习框架，用于检测生物识别系统中的物理呈现攻击和数字变形攻击，无需大量训练数据即可实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在生物识别攻击检测中表现良好但适应性不足，且需要大量训练数据。隐私和多样性数据收集的挑战促使研究者探索更高效的替代方案。

Method: 采用开源视觉语言模型，通过上下文学习技术建立首个系统性定量评估框架，用于安全关键场景中的攻击检测。

Result: 实验表明，该框架在物理和数字攻击检测中表现优异，优于部分传统CNN模型，且无需资源密集型训练。

Conclusion: 提出的框架是一种有前景的工具，可提高攻击检测的泛化能力，为生物识别安全提供新思路。

Abstract: Recent advances in biometric systems have significantly improved the
detection and prevention of fraudulent activities. However, as detection
methods improve, attack techniques become increasingly sophisticated. Attacks
on face recognition systems can be broadly divided into physical and digital
approaches. Traditionally, deep learning models have been the primary defence
against such attacks. While these models perform exceptionally well in
scenarios for which they have been trained, they often struggle to adapt to
different types of attacks or varying environmental conditions. These
subsystems require substantial amounts of training data to achieve reliable
performance, yet biometric data collection faces significant challenges,
including privacy concerns and the logistical difficulties of capturing diverse
attack scenarios under controlled conditions. This work investigates the
application of Vision Language Models (VLM) and proposes an in-context learning
framework for detecting physical presentation attacks and digital morphing
attacks in biometric systems. Focusing on open-source models, the first
systematic framework for the quantitative evaluation of VLMs in
security-critical scenarios through in-context learning techniques is
established. The experimental evaluation conducted on freely available
databases demonstrates that the proposed subsystem achieves competitive
performance for physical and digital attack detection, outperforming some of
the traditional CNNs without resource-intensive training. The experimental
results validate the proposed framework as a promising tool for improving
generalisation in attack detection.

</details>


### [95] [Minutiae-Anchored Local Dense Representation for Fingerprint Matching](https://arxiv.org/abs/2507.15297)
*Zhiyu Pan,Xiongjun Guan,Yongjie Duan,Jianjiang Feng,Jie Zhou*

Main category: cs.CV

TL;DR: 提出了一种名为DMD的指纹匹配方法，通过局部密集表示结合细粒度纹理和细节特征，提升多样捕获条件下的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 指纹匹配在多样捕获条件下仍具挑战性，需兼顾鲁棒性和准确性。

Method: 基于细节点的局部密集表示，提取三维张量描述符，结合空间结构和语义特征。

Result: 在多种指纹数据集上表现优异，达到SOTA精度且计算高效。

Conclusion: DMD方法在指纹识别中具有高效性和通用性，适合大规模应用。

Abstract: Fingerprint matching under diverse capture conditions remains a fundamental
challenge in biometric recognition. To achieve robust and accurate performance
in such scenarios, we propose DMD, a minutiae-anchored local dense
representation which captures both fine-grained ridge textures and
discriminative minutiae features in a spatially structured manner.
Specifically, descriptors are extracted from local patches centered and
oriented on each detected minutia, forming a three-dimensional tensor, where
two dimensions represent spatial locations on the fingerprint plane and the
third encodes semantic features. This representation explicitly captures
abstract features of local image patches, enabling a multi-level, fine-grained
description that aggregates information from multiple minutiae and their
surrounding ridge structures. Furthermore, thanks to its strong spatial
correspondence with the patch image, DMD allows for the use of foreground
segmentation masks to identify valid descriptor regions. During matching,
comparisons are then restricted to overlapping foreground areas, improving
efficiency and robustness. Extensive experiments on rolled, plain, parital,
contactless, and latent fingerprint datasets demonstrate the effectiveness and
generalizability of the proposed method. It achieves state-of-the-art accuracy
across multiple benchmarks while maintaining high computational efficiency,
showing strong potential for large-scale fingerprint recognition. Corresponding
code is available at https://github.com/Yu-Yy/DMD.

</details>


### [96] [Few-Shot Object Detection via Spatial-Channel State Space Model](https://arxiv.org/abs/2507.15308)
*Zhimeng Xin,Tianxu Wu,Yixiong Zou,Shiming Chen,Dingjie Fu,Xinge You*

Main category: cs.CV

TL;DR: 提出了一种基于Mamba的空间-通道状态建模模块（SCSM），用于提升少样本目标检测中通道特征的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前少样本目标检测方法在通道特征提取上存在不足，高权重通道未必有效，低权重通道可能仍有价值。

Method: 设计了SCSM模块，包含空间特征建模（SFM）和基于Mamba的通道状态建模（CSM）。

Result: 在VOC和COCO数据集上实现了最先进的性能。

Conclusion: SCSM模块能有效提升通道特征表示质量。

Abstract: Due to the limited training samples in few-shot object detection (FSOD), we
observe that current methods may struggle to accurately extract effective
features from each channel. Specifically, this issue manifests in two aspects:
i) channels with high weights may not necessarily be effective, and ii)
channels with low weights may still hold significant value. To handle this
problem, we consider utilizing the inter-channel correlation to facilitate the
novel model's adaptation process to novel conditions, ensuring the model can
correctly highlight effective channels and rectify those incorrect ones. Since
the channel sequence is also 1-dimensional, its similarity with the temporal
sequence inspires us to take Mamba for modeling the correlation in the channel
sequence. Based on this concept, we propose a Spatial-Channel State Space
Modeling (SCSM) module for spatial-channel state modeling, which highlights the
effective patterns and rectifies those ineffective ones in feature channels. In
SCSM, we design the Spatial Feature Modeling (SFM) module to balance the
learning of spatial relationships and channel relationships, and then introduce
the Channel State Modeling (CSM) module based on Mamba to learn correlation in
channels. Extensive experiments on the VOC and COCO datasets show that the SCSM
module enables the novel detector to improve the quality of focused feature
representation in channels and achieve state-of-the-art performance.

</details>


### [97] [BenchDepth: Are We on the Right Way to Evaluate Depth Foundation Models?](https://arxiv.org/abs/2507.15321)
*Zhenyu Li,Haotong Lin,Jiashi Feng,Peter Wonka,Bingyi Kang*

Main category: cs.CV

TL;DR: 提出了BenchDepth，一种新的深度基础模型评估基准，通过五个下游代理任务来评估模型的实用性。


<details>
  <summary>Details</summary>
Motivation: 现有深度评估协议存在不一致性，传统基准依赖对齐指标，导致偏见和不公平比较。

Method: 设计了五个下游代理任务（如深度补全、立体匹配等），绕过对齐问题，直接评估模型在实际应用中的表现。

Result: 评估了八个最先进的深度基础模型，并提供了关键发现和分析。

Conclusion: BenchDepth为深度模型评估提供了新标准，有望推动深度估计领域的进一步研究和讨论。

Abstract: Depth estimation is a fundamental task in computer vision with diverse
applications. Recent advancements in deep learning have led to powerful depth
foundation models (DFMs), yet their evaluation remains challenging due to
inconsistencies in existing protocols. Traditional benchmarks rely on
alignment-based metrics that introduce biases, favor certain depth
representations, and complicate fair comparisons. In this work, we propose
BenchDepth, a new benchmark that evaluates DFMs through five carefully selected
downstream proxy tasks: depth completion, stereo matching, monocular
feed-forward 3D scene reconstruction, SLAM, and vision-language spatial
understanding. Unlike conventional evaluation protocols, our approach assesses
DFMs based on their practical utility in real-world applications, bypassing
problematic alignment procedures. We benchmark eight state-of-the-art DFMs and
provide an in-depth analysis of key findings and observations. We hope our work
sparks further discussion in the community on best practices for depth model
evaluation and paves the way for future research and advancements in depth
estimation.

</details>


### [98] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式建模双特征分布，解决了工业缺陷检测中单类异常检测的局限性，利用潜在扩散模型生成合成缺陷数据，并通过邻域感知评分机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测系统在单类异常检测范式下面临数据稀缺和异常分布不均匀的问题，限制了其实际应用效果。

Method: 提出ExDD框架，通过并行记忆库建模正常和异常特征分布，结合潜在扩散模型生成合成缺陷数据，并采用邻域感知评分机制融合距离度量。

Result: 在KSDD2数据集上取得优异性能（94.2% I-AUROC, 97.7% P-AUROC），最佳增强效果为100个合成样本。

Conclusion: ExDD通过显式建模双分布和生成合成数据，有效解决了工业缺陷检测中的数据稀缺和异常分布问题，显著提升了检测性能。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [99] [RoadFusion: Latent Diffusion Model for Pavement Defect Detection](https://arxiv.org/abs/2507.15346)
*Muhammad Aqeel,Kidus Dagnaw Bellete,Francesco Setti*

Main category: cs.CV

TL;DR: RoadFusion框架通过合成异常生成和双路径特征适应，解决了路面缺陷检测中的数据稀缺、领域偏移和缺陷多样性问题。


<details>
  <summary>Details</summary>
Motivation: 路面缺陷检测面临标注数据有限、训练与部署环境间的领域偏移以及不同道路条件下缺陷外观的高变异性等挑战。

Method: RoadFusion利用潜在扩散模型通过文本提示和空间掩码合成多样且真实的缺陷，并通过双路径特征适应器分别优化正常和异常输入的特征表示，同时使用轻量级判别器在补丁级别区分细粒度缺陷模式。

Result: 在六个基准数据集上的评估表明，RoadFusion在分类和定位任务中均表现优异，并在多个实际道路检查相关指标上达到新的最先进水平。

Conclusion: RoadFusion通过合成数据和特征适应策略，显著提升了路面缺陷检测的鲁棒性和性能，为实际应用提供了有效解决方案。

Abstract: Pavement defect detection faces critical challenges including limited
annotated data, domain shift between training and deployment environments, and
high variability in defect appearances across different road conditions. We
propose RoadFusion, a framework that addresses these limitations through
synthetic anomaly generation with dual-path feature adaptation. A latent
diffusion model synthesizes diverse, realistic defects using text prompts and
spatial masks, enabling effective training under data scarcity. Two separate
feature adaptors specialize representations for normal and anomalous inputs,
improving robustness to domain shift and defect variability. A lightweight
discriminator learns to distinguish fine-grained defect patterns at the patch
level. Evaluated on six benchmark datasets, RoadFusion achieves consistently
strong performance across both classification and localization tasks, setting
new state-of-the-art in multiple metrics relevant to real-world road
inspection.

</details>


### [100] [DAViD: Data-efficient and Accurate Vision Models from Synthetic Data](https://arxiv.org/abs/2507.15365)
*Fatemeh Saleh,Sadegh Aliakbarian,Charlie Hewitt,Lohit Petikam,Xiao-Xian,Antonio Criminisi,Thomas J. Cashman,Tadas Baltrušaitis*

Main category: cs.CV

TL;DR: 论文提出了一种使用高保真合成数据集训练模型的方法，在保持高精度的同时降低了训练和推理成本。


<details>
  <summary>Details</summary>
Motivation: 解决现有高参数模型对大数据集、高计算资源的需求问题，同时确保数据来源、使用权利和用户同意的可控性。

Method: 利用合成训练数据，提供高细节和完美标签，并通过程序化数据合成控制数据多样性。

Result: 在三个密集预测任务（深度估计、表面法线估计和软前景分割）上展示了高精度，且训练和推理成本大幅降低。

Conclusion: 合成数据集方法在保持精度的同时提高了效率和公平性，为人类中心计算机视觉提供了新思路。

Abstract: The state of the art in human-centric computer vision achieves high accuracy
and robustness across a diverse range of tasks. The most effective models in
this domain have billions of parameters, thus requiring extremely large
datasets, expensive training regimes, and compute-intensive inference. In this
paper, we demonstrate that it is possible to train models on much smaller but
high-fidelity synthetic datasets, with no loss in accuracy and higher
efficiency. Using synthetic training data provides us with excellent levels of
detail and perfect labels, while providing strong guarantees for data
provenance, usage rights, and user consent. Procedural data synthesis also
provides us with explicit control on data diversity, that we can use to address
unfairness in the models we train. Extensive quantitative assessment on real
input images demonstrates accuracy of our models on three dense prediction
tasks: depth estimation, surface normal estimation, and soft foreground
segmentation. Our models require only a fraction of the cost of training and
inference when compared with foundational models of similar accuracy. Our
human-centric synthetic dataset and trained models are available at
https://aka.ms/DAViD.

</details>


### [101] [Rethinking Occlusion in FER: A Semantic-Aware Perspective and Go Beyond](https://arxiv.org/abs/2507.15401)
*Huiyu Zhai,Xingxing Yang,Yalan Ye,Chenyang Li,Bin Fan,Changze Li*

Main category: cs.CV

TL;DR: ORSANet通过多模态语义引导、多尺度交互模块和动态对抗排斥损失，显著提升了遮挡条件下的面部表情识别性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有FER模型在面部遮挡和数据集偏差下的性能不足问题。

Method: 引入多模态语义引导（语义分割图和面部关键点）、多尺度交互模块（MCM）和动态对抗排斥损失（DARELoss）。

Result: 在公开基准和自建Occlu-FER数据集上达到SOTA性能。

Conclusion: ORSANet在遮挡条件下表现出色，为FER领域提供了新的解决方案。

Abstract: Facial expression recognition (FER) is a challenging task due to pervasive
occlusion and dataset biases. Especially when facial information is partially
occluded, existing FER models struggle to extract effective facial features,
leading to inaccurate classifications. In response, we present ORSANet, which
introduces the following three key contributions: First, we introduce auxiliary
multi-modal semantic guidance to disambiguate facial occlusion and learn
high-level semantic knowledge, which is two-fold: 1) we introduce semantic
segmentation maps as dense semantics prior to generate semantics-enhanced
facial representations; 2) we introduce facial landmarks as sparse geometric
prior to mitigate intrinsic noises in FER, such as identity and gender biases.
Second, to facilitate the effective incorporation of these two multi-modal
priors, we customize a Multi-scale Cross-interaction Module (MCM) to adaptively
fuse the landmark feature and semantics-enhanced representations within
different scales. Third, we design a Dynamic Adversarial Repulsion Enhancement
Loss (DARELoss) that dynamically adjusts the margins of ambiguous classes,
further enhancing the model's ability to distinguish similar expressions. We
further construct the first occlusion-oriented FER dataset to facilitate
specialized robustness analysis on various real-world occlusion conditions,
dubbed Occlu-FER. Extensive experiments on both public benchmarks and Occlu-FER
demonstrate that our proposed ORSANet achieves SOTA recognition performance.
Code is publicly available at https://github.com/Wenyuzhy/ORSANet-master.

</details>


### [102] [SurgX: Neuron-Concept Association for Explainable Surgical Phase Recognition](https://arxiv.org/abs/2507.15418)
*Ka Young Kim,Hyeon Bae Kim,Seong Tae Kim*

Main category: cs.CV

TL;DR: SurgX是一个基于概念的解释框架，旨在提高手术阶段识别模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习模型在手术阶段识别中缺乏可解释性，阻碍了信任和调试。

Method: 通过选择代表性示例序列、构建概念集、关联神经元与概念，并识别关键神经元。

Result: 在两个手术阶段识别模型上的实验验证了方法的有效性。

Conclusion: SurgX展示了在解释手术阶段识别方面的潜力。

Abstract: Surgical phase recognition plays a crucial role in surgical workflow
analysis, enabling various applications such as surgical monitoring, skill
assessment, and workflow optimization. Despite significant advancements in deep
learning-based surgical phase recognition, these models remain inherently
opaque, making it difficult to understand how they make decisions. This lack of
interpretability hinders trust and makes it challenging to debug the model. To
address this challenge, we propose SurgX, a novel concept-based explanation
framework that enhances the interpretability of surgical phase recognition
models by associating neurons with relevant concepts. In this paper, we
introduce the process of selecting representative example sequences for
neurons, constructing a concept set tailored to the surgical video dataset,
associating neurons with concepts and identifying neurons crucial for
predictions. Through extensive experiments on two surgical phase recognition
models, we validate our method and analyze the explanation for prediction. This
highlights the potential of our method in explaining surgical phase
recognition. The code is available at https://github.com/ailab-kyunghee/SurgX

</details>


### [103] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种无需训练的分词剪枝方法，专为自我运动视频推理设计，通过关键帧选择、视角感知冗余过滤和基于MMR的分词选择，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 自我运动视频是具身AI代理的主要视觉输入，现有方法无法有效利用其时空连续性和运动约束，导致计算成本高昂。

Method: EgoPrune包含三个组件：关键帧选择器、视角感知冗余过滤器和基于MMR的分词选择器，共同优化视频推理效率。

Result: 在多个基准测试中，EgoPrune优于现有方法，显著减少FLOPs、内存使用和延迟，并在边缘设备上验证了其实用性。

Conclusion: EgoPrune为自我运动视频推理提供了一种高效、实用的解决方案，适用于具身AI代理的实时部署。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [104] [One Last Attention for Your Vision-Language Model](https://arxiv.org/abs/2507.15480)
*Liang Chen,Ghazi Shazan Ahmad,Tianjun Yao,Lingqiao Liu,Zhiqiang Shen*

Main category: cs.CV

TL;DR: RAda是一种简单有效的微调方法，通过动态校准视觉语言模型（VLM）的融合表示来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了融合表示在决策中的关键作用，RAda旨在填补这一空白。

Method: RAda使用轻量级注意力层生成学习掩码，动态调整融合表示中每个元素的贡献。

Result: 实验表明，RAda在多种设置下均能提升基线性能，且代码简洁。

Conclusion: RAda是一种通用的微调技术，性能与当前先进方法相当。

Abstract: Pretrained vision-language models (VLMs), such as CLIP, achieve remarkable
zero-shot performance, yet their downstream potential hinges on effective
fine-tuning. Most adaptation methods typically focus on refining representation
from separate modalities (text or vision) but neglect the critical role of
their fused representations in the decision-making process, \emph{\ie} rational
matrix that drives the final prediction. To bridge the gap, we propose a simple
yet effective \textbf{R}ational \textbf{Ada}ptaion ({RAda}) to explicitly
exploit the final fused representation during fine-tuning. RAda employs a
learned mask, obtained from a lightweight attention layer attached at the end
of a VLM, to dynamically calibrate the contribution of each element in the
rational matrix, enabling targeted adjustments to the final cross-modal
interactions without incurring costly modifications to intermediate features.
Experiments in different settings (i.e., updating, or freezing pretrained
encoders in adaptation, and test-time training that can only access the
unlabeled test data) show that RAda serves as a versatile fine-tuning
technique, improving the baseline with minimal code and performing comparably
against current arts in most settings. Code is available at
\href{https://github.com/khufia/RAda/tree/main}{github.com/khufia/RAda}.

</details>


### [105] [An aerial color image anomaly dataset for search missions in complex forested terrain](https://arxiv.org/abs/2507.15492)
*Rakesh John Amala Arokia Nathan,Matthias Gessner,Nurullah Özkan,Marius Bock,Mohamed Youssef,Maximilian Mews,Björn Piltz,Ralf Berger,Oliver Bimber*

Main category: cs.CV

TL;DR: 论文通过一起德国农村谋杀案，利用航空影像和众包搜索创建了一个遮挡条件下难以检测的异常数据集，用于改进复杂森林环境中的异常检测方法。


<details>
  <summary>Details</summary>
Motivation: 由于茂密植被遮挡导致自动分析失效，需要开发更有效的异常检测方法以支持搜捕和救援行动。

Method: 通过研究飞机获取高分辨率航空影像，结合众包搜索生成标注数据集，并开发交互式网络界面支持动态标注。

Result: 现有方法在初始基准测试中表现不佳，凸显了需要上下文感知方法的需求。

Conclusion: 该数据集可作为复杂森林环境中异常检测的基准，支持离线处理和在线动态标注，推动相关技术发展。

Abstract: After a family murder in rural Germany, authorities failed to locate the
suspect in a vast forest despite a massive search. To aid the search, a
research aircraft captured high-resolution aerial imagery. Due to dense
vegetation obscuring small clues, automated analysis was ineffective, prompting
a crowd-search initiative. This effort produced a unique dataset of labeled,
hard-to-detect anomalies under occluded, real-world conditions. It can serve as
a benchmark for improving anomaly detection approaches in complex forest
environments, supporting manhunts and rescue operations. Initial benchmark
tests showed existing methods performed poorly, highlighting the need for
context-aware approaches. The dataset is openly accessible for offline
processing. An additional interactive web interface supports online viewing and
dynamic growth by allowing users to annotate and submit new findings.

</details>


### [106] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种新颖的LiDAR-视觉里程计框架，结合LiDAR点云和图像，通过深度补全和多尺度特征提取网络实现高精度和鲁棒的姿态估计。


<details>
  <summary>Details</summary>
Motivation: 里程计是自主系统定位和导航的关键任务，现有方法在遮挡区域和动态环境中表现不佳，需要更鲁棒的解决方案。

Method: 利用深度补全生成稠密深度图，结合多尺度特征提取网络和注意力机制，通过层次化姿态优化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中，该方法在精度和鲁棒性上达到或优于现有最先进的视觉和LiDAR里程计方法。

Conclusion: 提出的LiDAR-视觉里程计框架在复杂环境中表现出色，为自主系统提供了更可靠的定位和导航能力。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [107] [Quantifying and Narrowing the Unknown: Interactive Text-to-Video Retrieval via Uncertainty Minimization](https://arxiv.org/abs/2507.15504)
*Bingqing Zhang,Zhuo Cao,Heming Du,Yang Li,Xue Li,Jiajun Liu,Sen Wang*

Main category: cs.CV

TL;DR: UMIVR是一个基于不确定性最小化的交互式文本-视频检索框架，通过量化文本模糊性、映射不确定性和帧不确定性，生成针对性问题以优化检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有交互式文本-视频检索方法依赖启发式策略，未明确量化不确定性，限制了效果。UMIVR填补了这一空白。

Method: UMIVR通过语义熵（TAS）、Jensen-Shannon散度（MUS）和时序质量采样器（TQFS）量化不确定性，并生成针对性问题。

Result: 在MSR-VTT-1k数据集上，UMIVR在10轮交互后Recall@1达到69.2%，显著优于现有方法。

Conclusion: UMIVR为交互式文本-视频检索提供了不确定性最小化的基础，显著提升了检索效果。

Abstract: Despite recent advances, Text-to-video retrieval (TVR) is still hindered by
multiple inherent uncertainties, such as ambiguous textual queries, indistinct
text-video mappings, and low-quality video frames. Although interactive systems
have emerged to address these challenges by refining user intent through
clarifying questions, current methods typically rely on heuristic or ad-hoc
strategies without explicitly quantifying these uncertainties, limiting their
effectiveness. Motivated by this gap, we propose UMIVR, an
Uncertainty-Minimizing Interactive Text-to-Video Retrieval framework that
explicitly quantifies three critical uncertainties-text ambiguity, mapping
uncertainty, and frame uncertainty-via principled, training-free metrics:
semantic entropy-based Text Ambiguity Score (TAS), Jensen-Shannon
divergence-based Mapping Uncertainty Score (MUS), and a Temporal Quality-based
Frame Sampler (TQFS). By adaptively generating targeted clarifying questions
guided by these uncertainty measures, UMIVR iteratively refines user queries,
significantly reducing retrieval ambiguity. Extensive experiments on multiple
benchmarks validate UMIVR's effectiveness, achieving notable gains in Recall@1
(69.2\% after 10 interactive rounds) on the MSR-VTT-1k dataset, thereby
establishing an uncertainty-minimizing foundation for interactive TVR.

</details>


### [108] [SAIGFormer: A Spatially-Adaptive Illumination-Guided Network for Low-Light Image Enhancement](https://arxiv.org/abs/2507.15520)
*Hanting Li,Fei Zhou,Xin Sun,Yang Hua,Jungong Han,Liang-Jie Zhang*

Main category: cs.CV

TL;DR: SAIGFormer是一种基于Transformer的低光增强方法，通过动态积分图像表示和光照引导的多头自注意力机制，有效解决非均匀光照场景下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有Transformer方法在非均匀光照场景（如背光和阴影）中表现不佳，导致过曝或亮度恢复不足。

Method: 提出动态积分图像表示建模空间变化的光照，构建SAI²E估计器，并引入IG-MSA机制校准亮度相关特征。

Result: 在五个低光数据集和跨域基准测试中，SAIGFormer在定量和定性指标上显著优于现有方法。

Conclusion: SAIGFormer在非均匀光照增强中表现优异，并展示出强大的跨数据集泛化能力。

Abstract: Recent Transformer-based low-light enhancement methods have made promising
progress in recovering global illumination. However, they still struggle with
non-uniform lighting scenarios, such as backlit and shadow, appearing as
over-exposure or inadequate brightness restoration. To address this challenge,
we present a Spatially-Adaptive Illumination-Guided Transformer (SAIGFormer)
framework that enables accurate illumination restoration. Specifically, we
propose a dynamic integral image representation to model the spatially-varying
illumination, and further construct a novel Spatially-Adaptive Integral
Illumination Estimator ($\text{SAI}^2\text{E}$). Moreover, we introduce an
Illumination-Guided Multi-head Self-Attention (IG-MSA) mechanism, which
leverages the illumination to calibrate the lightness-relevant features toward
visual-pleased illumination enhancement. Extensive experiments on five standard
low-light datasets and a cross-domain benchmark (LOL-Blur) demonstrate that our
SAIGFormer significantly outperforms state-of-the-art methods in both
quantitative and qualitative metrics. In particular, our method achieves
superior performance in non-uniform illumination enhancement while exhibiting
strong generalization capabilities across multiple datasets. Code is available
at https://github.com/LHTcode/SAIGFormer.git.

</details>


### [109] [Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport](https://arxiv.org/abs/2507.15540)
*Syed Ahmed Mahmood,Ali Shah Ali,Umer Ahmed,Fawad Javed Fateh,M. Zeeshan Zia,Quoc-Huy Tran*

Main category: cs.CV

TL;DR: 提出了一种自监督程序学习框架，通过融合Gromov-Wasserstein最优传输和对比正则化，解决了顺序变化、背景冗余和动作重复的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在顺序变化、背景冗余和动作重复情况下性能不佳。

Method: 结合Gromov-Wasserstein最优传输和对比正则化，避免嵌入空间退化。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: 提出的框架有效提升了程序学习的性能。

Abstract: We study the problem of self-supervised procedure learning, which discovers
key steps and establishes their order from a set of unlabeled procedural
videos. Previous procedure learning methods typically learn frame-to-frame
correspondences between videos before determining key steps and their order.
However, their performance often suffers from order variations,
background/redundant frames, and repeated actions. To overcome these
challenges, we propose a self-supervised procedure learning framework, which
utilizes a fused Gromov-Wasserstein optimal transport formulation with a
structural prior for computing frame-to-frame mapping between videos. However,
optimizing exclusively for the above temporal alignment term may lead to
degenerate solutions, where all frames are mapped to a small cluster in the
embedding space and hence every video is associated with only one key step. To
address that limitation, we further integrate a contrastive regularization
term, which maps different frames to different points in the embedding space,
avoiding the collapse to trivial solutions. Finally, we conduct extensive
experiments on large-scale egocentric (i.e., EgoProceL) and third-person (i.e.,
ProceL and CrossTask) benchmarks to demonstrate superior performance by our
approach against previous methods, including OPEL which relies on a traditional
Kantorovich optimal transport formulation with an optimality prior.

</details>


### [110] [Towards Holistic Surgical Scene Graph](https://arxiv.org/abs/2507.15541)
*Jongmin Shin,Enki Cho,Ka Yong Kim,Jung Yong Kim,Seong Tae Kim,Namkee Oh*

Main category: cs.CV

TL;DR: 论文提出了一种基于图的方法SSG-Com和数据集Endoscapes-SG201，用于增强手术场景理解，特别是工具-动作-目标组合和手部身份。


<details>
  <summary>Details</summary>
Motivation: 手术场景理解需要建模工具、解剖结构及其交互，但现有图表示方法未充分探索工具-动作-目标组合和手部身份。

Method: 提出SSG-Com方法，结合Endoscapes-SG201数据集，标注了工具-动作-目标组合和手部身份。

Result: 实验证明，这些关键元素对手术场景理解任务（如安全评估和动作识别）有显著贡献。

Conclusion: 通过整合工具-动作-目标组合和手部身份，显著提升了手术场景理解的性能。

Abstract: Surgical scene understanding is crucial for computer-assisted intervention
systems, requiring visual comprehension of surgical scenes that involves
diverse elements such as surgical tools, anatomical structures, and their
interactions. To effectively represent the complex information in surgical
scenes, graph-based approaches have been explored to structurally model
surgical entities and their relationships. Previous surgical scene graph
studies have demonstrated the feasibility of representing surgical scenes using
graphs. However, certain aspects of surgical scenes-such as diverse
combinations of tool-action-target and the identity of the hand operating the
tool-remain underexplored in graph-based representations, despite their
importance. To incorporate these aspects into graph representations, we propose
Endoscapes-SG201 dataset, which includes annotations for tool-action-target
combinations and hand identity. We also introduce SSG-Com, a graph-based method
designed to learn and represent these critical elements. Through experiments on
downstream tasks such as critical view of safety assessment and action triplet
recognition, we demonstrated the importance of integrating these essential
scene graph components, highlighting their significant contribution to surgical
scene understanding. The code and dataset are available at
https://github.com/ailab-kyunghee/SSG-Com

</details>


### [111] [HOLa: Zero-Shot HOI Detection with Low-Rank Decomposed VLM Feature Adaptation](https://arxiv.org/abs/2507.15542)
*Qinqian Lei,Bo Wang,Robby T. Tan*

Main category: cs.CV

TL;DR: HOLa通过低秩分解VLM特征增强零样本HOI检测的泛化能力和动作区分能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在零样本HOI检测中难以区分相同对象的不同动作或泛化能力有限的问题。

Method: 使用低秩分解VLM文本特征，生成类共享基特征和可调权重，结合LLM动作正则化优化权重。

Result: 在HICO-DET上实现了27.91的未见类mAP，达到新SOTA。

Conclusion: HOLa显著提升了零样本HOI检测的性能，尤其在泛化和动作区分方面。

Abstract: Zero-shot human-object interaction (HOI) detection remains a challenging
task, particularly in generalizing to unseen actions. Existing methods address
this challenge by tapping Vision-Language Models (VLMs) to access knowledge
beyond the training data. However, they either struggle to distinguish actions
involving the same object or demonstrate limited generalization to unseen
classes. In this paper, we introduce HOLa (Zero-Shot HOI Detection with
Low-Rank Decomposed VLM Feature Adaptation), a novel approach that both
enhances generalization to unseen classes and improves action distinction. In
training, HOLa decomposes VLM text features for given HOI classes via low-rank
factorization, producing class-shared basis features and adaptable weights.
These features and weights form a compact HOI representation that preserves
shared information across classes, enhancing generalization to unseen classes.
Subsequently, we refine action distinction by adapting weights for each HOI
class and introducing human-object tokens to enrich visual interaction
representations. To further distinguish unseen actions, we guide the weight
adaptation with LLM-derived action regularization. Experimental results show
that our method sets a new state-of-the-art across zero-shot HOI settings on
HICO-DET, achieving an unseen-class mAP of 27.91 in the unseen-verb setting.
Our code is available at https://github.com/ChelsieLei/HOLa.

</details>


### [112] [DynImg: Key Frames with Visual Prompts are Good Representation for Multi-Modal Video Understanding](https://arxiv.org/abs/2507.15569)
*Xiaoyi Bao,Chenwei Xie,Hao Tang,Tingyu Weng,Xiaofeng Wang,Yun Zheng,Xingang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为Dynamic-Image（DynImg）的创新视频表示方法，通过非关键帧作为时间提示，增强对快速移动物体的空间特征提取，提升视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 传统方法将空间和时间信息分开处理，导致快速移动物体的空间信息难以准确表示，影响时空交互和视频理解。

Method: 引入非关键帧作为时间提示，指导模型关注快速移动物体的细粒度空间特征，并使用4D视频旋转位置嵌入保持时空顺序。

Result: 实验表明，DynImg在多个视频理解基准上优于现有方法约2%。

Conclusion: DynImg通过时间提示有效提升了视频理解的准确性，证明了其在时空信息整合中的优势。

Abstract: In recent years, the introduction of Multi-modal Large Language Models
(MLLMs) into video understanding tasks has become increasingly prevalent.
However, how to effectively integrate temporal information remains a critical
research focus. Traditional approaches treat spatial and temporal information
separately. Due to issues like motion blur, it is challenging to accurately
represent the spatial information of rapidly moving objects. This can lead to
temporally important regions being underemphasized during spatial feature
extraction, which in turn hinders accurate spatio-temporal interaction and
video understanding. To address this limitation, we propose an innovative video
representation method called Dynamic-Image (DynImg). Specifically, we introduce
a set of non-key frames as temporal prompts to highlight the spatial areas
containing fast-moving objects. During the process of visual feature
extraction, these prompts guide the model to pay additional attention to the
fine-grained spatial features corresponding to these regions. Moreover, to
maintain the correct sequence for DynImg, we employ a corresponding 4D video
Rotary Position Embedding. This retains both the temporal and spatial adjacency
of DynImg, helping MLLM understand the spatio-temporal order within this
combined format. Experimental evaluations reveal that DynImg surpasses the
state-of-the-art methods by approximately 2% across multiple video
understanding benchmarks, proving the effectiveness of our temporal prompts in
enhancing video comprehension.

</details>


### [113] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于条件GAN的两阶段图像增强框架，用于替代传统的像素级mixup，提高医学图像分类的准确性和语义保真度。


<details>
  <summary>Details</summary>
Motivation: 传统的像素级mixup在医学图像分类中可能生成不真实的图像，影响学习效果，特别是在高风险应用中。

Method: GeMix使用StyleGAN2-ADA生成器，通过Dirichlet先验和Beta分布系数混合标签向量，生成视觉连贯的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据显著提高了所有骨干网络的macro-F1，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种即插即用的mixup替代方案，提供更强的正则化和语义保真度，且不破坏现有训练流程。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [114] [Compress-Align-Detect: onboard change detection from unregistered images](https://arxiv.org/abs/2507.15578)
*Gabriele Inzerillo,Diego Valsesia,Aniello Fiengo,Enrico Magli*

Main category: cs.CV

TL;DR: 提出了一种卫星上实时变化检测的新框架，通过端到端深度学习网络解决数据存储、图像配准和变化检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统卫星图像变化检测因数据传输和处理延迟无法满足实时或近实时应用需求。

Method: 采用包含三个子模块的深度学习网络：图像压缩、轻量级配准和高效变化检测模型。

Result: 在低功耗硬件上实现0.7 Mpixel/s的吞吐量，F1分数随压缩率变化表现优异。

Conclusion: 该框架首次在卫星上实现端到端变化检测，为实时应用提供了可行方案。

Abstract: Change detection from satellite images typically incurs a delay ranging from
several hours up to days because of latency in downlinking the acquired images
and generating orthorectified image products at the ground stations; this may
preclude real- or near real-time applications. To overcome this limitation, we
propose shifting the entire change detection workflow onboard satellites. This
requires to simultaneously solve challenges in data storage, image registration
and change detection with a strict complexity constraint. In this paper, we
present a novel and efficient framework for onboard change detection that
addresses the aforementioned challenges in an end-to-end fashion with a deep
neural network composed of three interlinked submodules: (1) image compression,
tailored to minimize onboard data storage resources; (2) lightweight
co-registration of non-orthorectified multi-temporal image pairs; and (3) a
novel temporally-invariant and computationally efficient change detection
model. This is the first approach in the literature combining all these tasks
in a single end-to-end framework with the constraints dictated by onboard
processing. Experimental results compare each submodule with the current
state-of-the-art, and evaluate the performance of the overall integrated system
in realistic setting on low-power hardware. Compelling change detection results
are obtained in terms of F1 score as a function of compression rate, sustaining
a throughput of 0.7 Mpixel/s on a 15W accelerator.

</details>


### [115] [SegDT: A Diffusion Transformer-Based Segmentation Model for Medical Imaging](https://arxiv.org/abs/2507.15595)
*Salah Eddine Bekhouche,Gaby Maroun,Fadi Dornaika,Abdenour Hadid*

Main category: cs.CV

TL;DR: SegDT是一种基于扩散变压器的新型分割模型，适用于低成本硬件，通过Rectified Flow提升生成质量，并在三个基准数据集上取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 皮肤病变分割对皮肤癌诊断和患者监测至关重要，但现有方法在性能和速度上存在不足。

Method: 提出SegDT模型，结合扩散变压器和Rectified Flow技术，优化推理步骤并保持灵活性。

Result: 在三个基准数据集上达到最先进水平，同时保持快速推理速度。

Conclusion: SegDT为医学图像分析提供了高效、准确的工具，适合实际医疗应用。

Abstract: Medical image segmentation is crucial for many healthcare tasks, including
disease diagnosis and treatment planning. One key area is the segmentation of
skin lesions, which is vital for diagnosing skin cancer and monitoring
patients. In this context, this paper introduces SegDT, a new segmentation
model based on diffusion transformer (DiT). SegDT is designed to work on
low-cost hardware and incorporates Rectified Flow, which improves the
generation quality at reduced inference steps and maintains the flexibility of
standard diffusion models. Our method is evaluated on three benchmarking
datasets and compared against several existing works, achieving
state-of-the-art results while maintaining fast inference speeds. This makes
the proposed model appealing for real-world medical applications. This work
advances the performance and capabilities of deep learning models in medical
image analysis, enabling faster, more accurate diagnostic tools for healthcare
professionals. The code is made publicly available at
\href{https://github.com/Bekhouche/SegDT}{GitHub}.

</details>


### [116] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一种基于人类视频训练的高灵巧性视觉-语言-动作模型，通过物理指令调优和毫米级运动标记化方法，解决了现有模型在复杂操作任务中的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言-动作模型在复杂操作任务中表现不佳，主要依赖合成数据或有限的遥操作演示，导致泛化能力差。

Method: 提出物理指令调优范式，结合大规模人类视频预训练、物理空间对齐和机器人任务后适应，并引入毫米级运动标记化方法。

Result: Being-H0在手部运动生成和指令跟随方面表现优异，并在真实机器人操作中展现出预期增益。

Conclusion: 通过物理指令调优和大规模数据训练，Being-H0显著提升了复杂操作任务的性能和泛化能力。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [117] [SurfaceSplat: Connecting Surface Reconstruction and Gaussian Splatting](https://arxiv.org/abs/2507.15602)
*Zihui Gao,Jia-Wang Bian,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: 提出了一种结合SDF和3DGS的混合方法，用于稀疏视图图像中的表面重建和新视角渲染。


<details>
  <summary>Details</summary>
Motivation: 解决SDF方法在细节捕捉和3DGS方法在全局几何一致性上的不足。

Method: 通过SDF捕捉粗粒度几何以增强3DGS渲染，同时利用3DGS生成的新图像细化SDF的细节。

Result: 在DTU和MobileBrick数据集上超越了现有方法。

Conclusion: 该方法在表面重建和新视角合成中表现出色，代码将开源。

Abstract: Surface reconstruction and novel view rendering from sparse-view images are
challenging. Signed Distance Function (SDF)-based methods struggle with fine
details, while 3D Gaussian Splatting (3DGS)-based approaches lack global
geometry coherence. We propose a novel hybrid method that combines the
strengths of both approaches: SDF captures coarse geometry to enhance
3DGS-based rendering, while newly rendered images from 3DGS refine the details
of SDF for accurate surface reconstruction. As a result, our method surpasses
state-of-the-art approaches in surface reconstruction and novel view synthesis
on the DTU and MobileBrick datasets. Code will be released at
https://github.com/Gaozihui/SurfaceSplat.

</details>


### [118] [CylinderPlane: Nested Cylinder Representation for 3D-aware Image Generation](https://arxiv.org/abs/2507.15606)
*Ru Jia,Xiaozhuang Ma,Jianji Wang,Nanning Zheng*

Main category: cs.CV

TL;DR: 提出了一种基于圆柱坐标系的CylinderPlane表示方法，解决了Tri-plane表示中的多视角一致性问题，实现了高质量、无伪影的360°图像合成。


<details>
  <summary>Details</summary>
Motivation: Tri-plane表示在对称区域共享特征导致多面伪影，限制了360°视图图像的生成能力。

Method: 采用圆柱坐标系分离不同角度的特征，引入嵌套圆柱表示以处理复杂几何和多分辨率。

Result: 在合成数据集和真实图像上表现优于现有方法。

Conclusion: CylinderPlane是一种高效且通用的表示方法，适用于任何神经渲染流程。

Abstract: While the proposal of the Tri-plane representation has advanced the
development of the 3D-aware image generative models, problems rooted in its
inherent structure, such as multi-face artifacts caused by sharing the same
features in symmetric regions, limit its ability to generate 360$^\circ$ view
images. In this paper, we propose CylinderPlane, a novel implicit
representation based on Cylindrical Coordinate System, to eliminate the feature
ambiguity issue and ensure multi-view consistency in 360$^\circ$. Different
from the inevitable feature entanglement in Cartesian coordinate-based
Tri-plane representation, the cylindrical coordinate system explicitly
separates features at different angles, allowing our cylindrical representation
possible to achieve high-quality, artifacts-free 360$^\circ$ image synthesis.
We further introduce the nested cylinder representation that composites
multiple cylinders at different scales, thereby enabling the model more
adaptable to complex geometry and varying resolutions. The combination of
cylinders with different resolutions can effectively capture more critical
locations and multi-scale features, greatly facilitates fine detail learning
and robustness to different resolutions. Moreover, our representation is
agnostic to implicit rendering methods and can be easily integrated into any
neural rendering pipeline. Extensive experiments on both synthetic dataset and
unstructured in-the-wild images demonstrate that our proposed representation
achieves superior performance over previous methods.

</details>


### [119] [A Survey on Efficiency Optimization Techniques for DNN-based Video Analytics: Process Systems, Algorithms, and Applications](https://arxiv.org/abs/2507.15628)
*Shanjiang Tang,Rui Huang,Hsinyu Luo,Chunjiang Wang,Ce Yu,Yusen Li,Hao Fu,Chao Sun,and Jian Xiao*

Main category: cs.CV

TL;DR: 本文综述了深度神经网络（DNN）在视频分析中的效率优化技术，填补了现有研究主要关注准确性优化的空白。


<details>
  <summary>Details</summary>
Motivation: 视频数据的爆炸性增长对视频分析的准确性和效率提出了更高要求，而现有研究多集中于准确性优化，缺乏对效率优化的系统总结。

Method: 采用自底向上的方式组织现有方法，涵盖硬件支持、数据处理、操作部署等多方面视角。

Result: 提出了一个优化框架，并基于现有工作分析了DNN在视频分析中性能优化的问题与挑战。

Conclusion: 本文为DNN在视频分析中的效率优化提供了全面综述，并指出了未来研究的潜在方向。

Abstract: The explosive growth of video data in recent years has brought higher demands
for video analytics, where accuracy and efficiency remain the two primary
concerns. Deep neural networks (DNNs) have been widely adopted to ensure
accuracy; however, improving their efficiency in video analytics remains an
open challenge. Different from existing surveys that make summaries of
DNN-based video mainly from the accuracy optimization aspect, in this survey,
we aim to provide a thorough review of optimization techniques focusing on the
improvement of the efficiency of DNNs in video analytics. We organize existing
methods in a bottom-up manner, covering multiple perspectives such as hardware
support, data processing, operational deployment, etc. Finally, based on the
optimization framework and existing works, we analyze and discuss the problems
and challenges in the performance optimization of DNN-based video analytics.

</details>


### [120] [Experimenting active and sequential learning in a medieval music manuscript](https://arxiv.org/abs/2507.15633)
*Sachin Sharma,Federico Simonetta,Michele Flammini*

Main category: cs.CV

TL;DR: 本文探讨了在光学音乐识别（OMR）中应用主动学习（AL）和顺序学习（SL）的方法，以减少标注数据需求并提升性能。


<details>
  <summary>Details</summary>
Motivation: 由于历史音乐手稿的复杂性和标注数据的稀缺性，光学音乐识别（OMR）面临挑战。本文旨在通过主动学习和顺序学习优化标注效率。

Method: 使用YOLOv8模型，选择预测置信度最低的样本进行迭代标注和重新训练，从单张标注图像开始逐步提升性能。

Result: 实验表明，该方法在显著减少标注样本的情况下，能达到与全监督训练相当的准确率。但不确定性主动学习在特定手稿中效果不佳。

Conclusion: 研究强调了在数据稀缺场景下开发更实用方法的重要性，并为社区提供了一个新的数据集。

Abstract: Optical Music Recognition (OMR) is a cornerstone of music digitization
initiatives in cultural heritage, yet it remains limited by the scarcity of
annotated data and the complexity of historical manuscripts. In this paper, we
present a preliminary study of Active Learning (AL) and Sequential Learning
(SL) tailored for object detection and layout recognition in an old medieval
music manuscript. Leveraging YOLOv8, our system selects samples with the
highest uncertainty (lowest prediction confidence) for iterative labeling and
retraining. Our approach starts with a single annotated image and successfully
boosts performance while minimizing manual labeling. Experimental results
indicate that comparable accuracy to fully supervised training can be achieved
with significantly fewer labeled examples. We test the methodology as a
preliminary investigation on a novel dataset offered to the community by the
Anonymous project, which studies laude, a poetical-musical genre spread across
Italy during the 12th-16th Century. We show that in the manuscript at-hand,
uncertainty-based AL is not effective and advocates for more usable methods in
data-scarcity scenarios.

</details>


### [121] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 研究探讨了彩票假设（LTH）在深度伪造检测中的应用，发现关键子网络在高度稀疏下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确。

Method: 通过MesoNet、CNN-5和ResNet-18架构，结合LTH进行迭代幅度剪枝，分析关键特征。

Result: MesoNet在80%稀疏度下保持56.2%准确率，LTH方法优于一次性剪枝。

Conclusion: LTH方法可高效剪枝模型并保持性能，具有跨数据集迁移潜力。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [122] [Extracting Visual Facts from Intermediate Layers for Mitigating Hallucinations in Multimodal Large Language Models](https://arxiv.org/abs/2507.15652)
*Haoran Zhou,Zihan Zhang,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为EVA的训练无关方法，通过动态选择中间层提取视觉事实信息，显著减少多模态大语言模型中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在视觉识别和语言理解方面取得了进展，但仍存在对象幻觉问题，即模型生成看似合理但实际错误的输出。研究发现先验知识会抑制视觉信息，但中间层的具体机制尚不明确。

Method: 提出EVA方法，通过对比原始输入和纯文本输入在中间层的输出分布，提取视觉事实知识并整合到最终层，以修正输出。

Result: 实验表明，EVA显著降低了幻觉率，优于基线方法。

Conclusion: EVA是一种模型无关、无需训练的方法，能有效减少MLLMs中的幻觉问题。

Abstract: Multimodal Large Language Models (MLLMs) have made significant strides by
combining visual recognition and language understanding to generate content
that is both coherent and contextually accurate. However, MLLMs continue to
struggle with object hallucinations, where models produce seemingly plausible
but factually incorrect outputs, including objects that do not exist in the
image. Recent work has revealed that the prior knowledge in MLLMs significantly
suppresses visual information in deep layers, causing hallucinatory outputs.
However, how these priors suppress visual information at the intermediate layer
stage in MLLMs remains unclear. We observe that visual factual knowledge and
the differences between intermediate-layer prior/original probability
distributions show similar evolutionary trends in intermediate layers.
Motivated by this, we introduce Decoding by Extracting Visual Facts (EVA), a
simple, training-free method that dynamically selects intermediate layers with
the most significant visual factual information. By contrasting the output
distributions of the selected layer derived from the original input and
pure-text input, EVA extracts visual factual knowledge and proportionally
incorporates it into the final layer to correct the output logits. Importantly,
EVA is model-agnostic, seamlessly integrates with various classic decoding
strategies, and is applicable across different MLLMs. We validate EVA on
widely-used benchmarks, and the results show that it significantly reduces
hallucination rates compared to baseline methods, underscoring its
effectiveness in mitigating hallucinations.

</details>


### [123] [HW-MLVQA: Elucidating Multilingual Handwritten Document Understanding with a Comprehensive VQA Benchmark](https://arxiv.org/abs/2507.15655)
*Aniket Pal,Ajoy Mondal,Minesh Mathew,C. V. Jawahar*

Main category: cs.CV

TL;DR: HW-MLVQA是一个新的多语言手写文档视觉问答基准，旨在解决现有模型在手写文档理解上的不足，包含1600页手写文档和2400个问答对。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言视觉问答模型在处理多样化的手写文档时表现不佳，缺乏真实的多语言手写文档理解基准。

Method: HW-MLVQA通过整合文本、图像及图文结合三种模态，并评估专有和开源OCR模型，模拟真实场景。

Result: 基准提供了1600页手写文档和2400个问答对，支持多模态评估。

Conclusion: HW-MLVQA旨在推动多语言手写文档理解的研究和创新。

Abstract: The proliferation of MultiLingual Visual Question Answering (MLVQA)
benchmarks augments the capabilities of large language models (LLMs) and
multi-modal LLMs, thereby enabling them to adeptly capture the intricate
linguistic subtleties and visual complexities inherent across diverse
languages. Despite its potential, the current MLVQA model struggles to fully
utilize its capabilities when dealing with the extensive variety of handwritten
documents. This article delineates HW-MLVQA, an avant-garde VQA benchmark
meticulously crafted to mitigate the dearth of authentic Multilingual
Handwritten document comprehension. HW-MLVQA encompasses an extensive
collection of 1,600 handwritten Pages complemented by 2,400 question-answers.
Furthermore, it provides a robust benchmark evaluation framework spanning three
distinct modalities: text, image, and an integrated image & text modality. To
simulate authentic real-world contexts devoid of ground truth textual
transcriptions, we facilitates a rigorous assessment of proprietary and
open-source OCR models. The benchmark aspires to facilitate pivotal
advancements in multilingual handwritten document interpretation, fostering
innovation and scholarly inquiry within this specialized domain.

</details>


### [124] [Visual-Language Model Knowledge Distillation Method for Image Quality Assessment](https://arxiv.org/abs/2507.15680)
*Yongkang Hou,Jiarun Song*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型知识蒸馏的方法，用于图像质量评估任务，显著降低模型复杂度并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在图像质量评估任务中参数负担过重和局部失真特征识别能力不足的问题。

Method: 设计了质量分级提示模板，微调CLIP，并提出模态自适应知识蒸馏策略。

Result: 在多个数据集上实验表明，该方法显著降低复杂度且性能优于现有方法。

Conclusion: 该方法展示了实际部署的潜力，为图像质量评估任务提供了高效解决方案。

Abstract: Image Quality Assessment (IQA) is a core task in computer vision. Multimodal
methods based on vision-language models, such as CLIP, have demonstrated
exceptional generalization capabilities in IQA tasks. To address the issues of
excessive parameter burden and insufficient ability to identify local distorted
features in CLIP for IQA, this study proposes a visual-language model knowledge
distillation method aimed at guiding the training of models with architectural
advantages using CLIP's IQA knowledge. First, quality-graded prompt templates
were designed to guide CLIP to output quality scores. Then, CLIP is fine-tuned
to enhance its capabilities in IQA tasks. Finally, a modality-adaptive
knowledge distillation strategy is proposed to achieve guidance from the CLIP
teacher model to the student model. Our experiments were conducted on multiple
IQA datasets, and the results show that the proposed method significantly
reduces model complexity while outperforming existing IQA methods,
demonstrating strong potential for practical deployment.

</details>


### [125] [Hi^2-GSLoc: Dual-Hierarchical Gaussian-Specific Visual Relocalization for Remote Sensing](https://arxiv.org/abs/2507.15683)
*Boni Hu,Zhenyu Xia,Lin Chen,Pengcheng Han,Shuhui Bu*

Main category: cs.CV

TL;DR: 论文提出了一种基于3D高斯散射（3DGS）的双层次视觉重定位框架Hi²-GSLoc，解决了现有方法在精度和计算复杂度上的不足，适用于大规模遥感场景。


<details>
  <summary>Details</summary>
Motivation: 现有视觉重定位方法在精度和计算复杂度之间存在固有矛盾，尤其在遥感场景中因大规模、高海拔变化和领域差距问题更为突出。

Method: 采用3DGS作为场景表示，提出双层次框架（稀疏到密集、粗到细），结合分区高斯训练、GPU并行匹配和动态内存管理。

Result: 在仿真数据、公开数据集和实际飞行实验中表现出高定位精度、召回率和计算效率，并能有效过滤不可靠估计。

Conclusion: Hi²-GSLoc框架在遥感应用中表现出色，解决了现有方法的局限性。

Abstract: Visual relocalization, which estimates the 6-degree-of-freedom (6-DoF) camera
pose from query images, is fundamental to remote sensing and UAV applications.
Existing methods face inherent trade-offs: image-based retrieval and pose
regression approaches lack precision, while structure-based methods that
register queries to Structure-from-Motion (SfM) models suffer from
computational complexity and limited scalability. These challenges are
particularly pronounced in remote sensing scenarios due to large-scale scenes,
high altitude variations, and domain gaps of existing visual priors. To
overcome these limitations, we leverage 3D Gaussian Splatting (3DGS) as a novel
scene representation that compactly encodes both 3D geometry and appearance. We
introduce $\mathrm{Hi}^2$-GSLoc, a dual-hierarchical relocalization framework
that follows a sparse-to-dense and coarse-to-fine paradigm, fully exploiting
the rich semantic information and geometric constraints inherent in Gaussian
primitives. To handle large-scale remote sensing scenarios, we incorporate
partitioned Gaussian training, GPU-accelerated parallel matching, and dynamic
memory management strategies. Our approach consists of two stages: (1) a sparse
stage featuring a Gaussian-specific consistent render-aware sampling strategy
and landmark-guided detector for robust and accurate initial pose estimation,
and (2) a dense stage that iteratively refines poses through coarse-to-fine
dense rasterization matching while incorporating reliability verification.
Through comprehensive evaluation on simulation data, public datasets, and real
flight experiments, we demonstrate that our method delivers competitive
localization accuracy, recall rate, and computational efficiency while
effectively filtering unreliable pose estimates. The results confirm the
effectiveness of our approach for practical remote sensing applications.

</details>


### [126] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经表示（INR）的无损点云几何压缩方法LINR-PCGC，解决了现有方法依赖训练数据分布和编码时间限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有AI点云压缩方法依赖特定训练数据分布，限制了实际应用；INR方法虽能解决分布问题，但仅支持有损压缩。

Method: 设计了点云级编码框架和高效网络初始化策略，减少60%编码时间；提出基于多尺度SparseConv的轻量级编码网络。

Result: 在MVUB数据集上，比特流比G-PCC TMC13v23减少21.21%，比SparsePCGC减少21.95%。

Conclusion: LINR-PCGC首次实现基于INR的无损点云几何压缩，显著提升压缩效率和实用性。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [127] [DWTGS: Rethinking Frequency Regularization for Sparse-view 3D Gaussian Splatting](https://arxiv.org/abs/2507.15690)
*Hung Nguyen,Runfa Li,An Le,Truong Nguyen*

Main category: cs.CV

TL;DR: DWTGS提出了一种基于小波空间损失的新型频率正则化方法，通过监督低频子带并自监督高频子带，显著提升了稀疏视图3D高斯泼溅的重建质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3D高斯泼溅（3DGS）在重建高质量新视图时容易过拟合训练视图的高频细节，现有基于傅里叶变换的频率正则化方法参数调优困难且偏向有害的高频学习。

Method: DWTGS利用小波空间损失提供额外的空间监督，仅监督多级DWT的低频LL子带，并以自监督方式对高频HH子带施加稀疏性。

Result: 实验表明，DWTGS在多个基准测试中优于基于傅里叶的方法，低频中心策略提高了泛化能力并减少了高频幻觉。

Conclusion: DWTGS通过小波空间损失重新思考频率正则化，为稀疏视图3DGS提供了一种更有效的解决方案。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) presents significant challenges in
reconstructing high-quality novel views, as it often overfits to the
widely-varying high-frequency (HF) details of the sparse training views. While
frequency regularization can be a promising approach, its typical reliance on
Fourier transforms causes difficult parameter tuning and biases towards
detrimental HF learning. We propose DWTGS, a framework that rethinks frequency
regularization by leveraging wavelet-space losses that provide additional
spatial supervision. Specifically, we supervise only the low-frequency (LF) LL
subbands at multiple DWT levels, while enforcing sparsity on the HF HH subband
in a self-supervised manner. Experiments across benchmarks show that DWTGS
consistently outperforms Fourier-based counterparts, as this LF-centric
strategy improves generalization and reduces HF hallucinations.

</details>


### [128] [Efficient Face Image Quality Assessment via Self-training and Knowledge Distillation](https://arxiv.org/abs/2507.15709)
*Wei Sun,Weixia Zhang,Linhan Cao,Jun Jia,Xiangyang Zhu,Dandan Zhu,Xiongkuo Min,Guangtao Zhai*

Main category: cs.CV

TL;DR: 提出了一种高效的人脸图像质量评估方法，通过教师-学生模型蒸馏技术降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决现有FIQA算法计算复杂度高的问题，以提升实际部署的可行性。

Method: 采用两阶段方法：先训练强大的教师模型，再通过自训练和知识蒸馏生成轻量级学生模型。

Result: 学生模型性能接近教师模型，计算开销极低，并在ICCV 2025 VQualA FIQA挑战赛中夺冠。

Conclusion: 该方法在保持高性能的同时显著降低了计算成本，适合实际应用部署。

Abstract: Face image quality assessment (FIQA) is essential for various face-related
applications. Although FIQA has been extensively studied and achieved
significant progress, the computational complexity of FIQA algorithms remains a
key concern for ensuring scalability and practical deployment in real-world
systems. In this paper, we aim to develop a computationally efficient FIQA
method that can be easily deployed in real-world applications. Specifically,
our method consists of two stages: training a powerful teacher model and
distilling a lightweight student model from it. To build a strong teacher
model, we adopt a self-training strategy to improve its capacity. We first
train the teacher model using labeled face images, then use it to generate
pseudo-labels for a set of unlabeled images. These pseudo-labeled samples are
used in two ways: (1) to distill knowledge into the student model, and (2) to
combine with the original labeled images to further enhance the teacher model
through self-training. The enhanced teacher model is used to further
pseudo-label another set of unlabeled images for distilling the student models.
The student model is trained using a combination of labeled images,
pseudo-labeled images from the original teacher model, and pseudo-labeled
images from the enhanced teacher model. Experimental results demonstrate that
our student model achieves comparable performance to the teacher model with an
extremely low computational overhead. Moreover, our method achieved first place
in the ICCV 2025 VQualA FIQA Challenge. The code is available at
https://github.com/sunwei925/Efficient-FIQA.git.

</details>


### [129] [A Practical Investigation of Spatially-Controlled Image Generation with Transformers](https://arxiv.org/abs/2507.15724)
*Guoxuan Xia,Harleen Hanspal,Petru-Daniel Tudosiu,Shifeng Zhang,Sarah Parisot*

Main category: cs.CV

TL;DR: 论文探讨了如何通过空间控制改进图像生成模型，比较了不同生成范式，并提出了一种简单且高效的基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决现有模型在空间控制生成中缺乏详细科学比较的问题，并为实践者提供清晰的指导。

Method: 在ImageNet上进行了扩散模型、流模型和自回归模型的对比实验，提出了控制标记预填充方法，并研究了采样时间增强技术。

Result: 控制标记预填充是一种高效基线方法，分类器自由指导和softmax截断对控制生成一致性有显著影响。

Conclusion: 适配器方法在有限数据下能保持生成质量，但在控制一致性上不如完整训练。

Abstract: Enabling image generation models to be spatially controlled is an important
area of research, empowering users to better generate images according to their
own fine-grained specifications via e.g. edge maps, poses. Although this task
has seen impressive improvements in recent times, a focus on rapidly producing
stronger models has come at the cost of detailed and fair scientific
comparison. Differing training data, model architectures and generation
paradigms make it difficult to disentangle the factors contributing to
performance. Meanwhile, the motivations and nuances of certain approaches
become lost in the literature. In this work, we aim to provide clear takeaways
across generation paradigms for practitioners wishing to develop
transformer-based systems for spatially-controlled generation, clarifying the
literature and addressing knowledge gaps. We perform controlled experiments on
ImageNet across diffusion-based/flow-based and autoregressive (AR) models.
First, we establish control token prefilling as a simple, general and
performant baseline approach for transformers. We then investigate previously
underexplored sampling time enhancements, showing that extending
classifier-free guidance to control, as well as softmax truncation, have a
strong impact on control-generation consistency. Finally, we re-clarify the
motivation of adapter-based approaches, demonstrating that they mitigate
"forgetting" and maintain generation quality when trained on limited downstream
data, but underperform full training in terms of generation-control
consistency. Code will be released upon publication.

</details>


### [130] [TokensGen: Harnessing Condensed Tokens for Long Video Generation](https://arxiv.org/abs/2507.15728)
*Wenqi Ouyang,Zeqi Xiao,Danni Yang,Yifan Zhou,Shuai Yang,Lei Yang,Jianlou Si,Xingang Pan*

Main category: cs.CV

TL;DR: TokensGen是一个两阶段框架，通过使用浓缩的token解决长视频生成中的内存瓶颈和长期不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成短视频时表现良好，但在生成长视频时面临内存和一致性挑战。

Method: 方法分为两阶段：1) 训练To2V模型生成短视频；2) 使用T2To模型生成全局一致的token，并通过自适应FIFO-Diffusion策略平滑连接片段。

Result: 实验表明，该方法显著提升了长视频的时间一致性和内容连贯性，且计算开销可控。

Conclusion: TokensGen为长视频生成提供了可扩展的模块化解决方案，适用于影视制作和沉浸式模拟等领域。

Abstract: Generating consistent long videos is a complex challenge: while
diffusion-based generative models generate visually impressive short clips,
extending them to longer durations often leads to memory bottlenecks and
long-term inconsistency. In this paper, we propose TokensGen, a novel two-stage
framework that leverages condensed tokens to address these issues. Our method
decomposes long video generation into three core tasks: (1) inner-clip semantic
control, (2) long-term consistency control, and (3) inter-clip smooth
transition. First, we train To2V (Token-to-Video), a short video diffusion
model guided by text and video tokens, with a Video Tokenizer that condenses
short clips into semantically rich tokens. Second, we introduce T2To
(Text-to-Token), a video token diffusion transformer that generates all tokens
at once, ensuring global consistency across clips. Finally, during inference,
an adaptive FIFO-Diffusion strategy seamlessly connects adjacent clips,
reducing boundary artifacts and enhancing smooth transitions. Experimental
results demonstrate that our approach significantly enhances long-term temporal
and content coherence without incurring prohibitive computational overhead. By
leveraging condensed tokens and pre-trained short video models, our method
provides a scalable, modular solution for long video generation, opening new
possibilities for storytelling, cinematic production, and immersive
simulations. Please see our project page at
https://vicky0522.github.io/tokensgen-webpage/ .

</details>


### [131] [Appearance Harmonization via Bilateral Grid Prediction with Transformers for 3DGS](https://arxiv.org/abs/2507.15748)
*Jisu Shin,Richard Shaw,Seunghyun Shin,Anton Pelykh,Zhensong Zhang,Hae-Gon Jeon,Eduardo Perez-Pellitero*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的方法，通过预测空间自适应的双边网格来校正多视角的光度变化，无需场景特定重训练即可实现跨场景的泛化。


<details>
  <summary>Details</summary>
Motivation: 现代相机流水线中的处理（如曝光调整、白平衡等）会导致多视角间的光度不一致，影响新视角合成的质量。现有方法通过联合优化场景表示和每张图像的外观嵌入来解决，但增加了计算复杂度和训练时间。

Method: 采用Transformer预测空间自适应的双边网格，校正光度变化，并将其集成到3D高斯泼溅流水线中。

Result: 实验表明，该方法在重建质量和收敛速度上优于或匹配现有的场景特定优化方法。

Conclusion: 该方法在多视角一致性和训练效率上表现优异，适用于跨场景的泛化。

Abstract: Modern camera pipelines apply extensive on-device processing, such as
exposure adjustment, white balance, and color correction, which, while
beneficial individually, often introduce photometric inconsistencies across
views. These appearance variations violate multi-view consistency and degrade
the quality of novel view synthesis. Joint optimization of scene
representations and per-image appearance embeddings has been proposed to
address this issue, but at the cost of increased computational complexity and
slower training. In this work, we propose a transformer-based method that
predicts spatially adaptive bilateral grids to correct photometric variations
in a multi-view consistent manner, enabling robust cross-scene generalization
without the need for scene-specific retraining. By incorporating the learned
grids into the 3D Gaussian Splatting pipeline, we improve reconstruction
quality while maintaining high training efficiency. Extensive experiments show
that our approach outperforms or matches existing scene-specific optimization
methods in reconstruction fidelity and convergence speed.

</details>


### [132] [Learning from Heterogeneity: Generalizing Dynamic Facial Expression Recognition via Distributionally Robust Optimization](https://arxiv.org/abs/2507.15765)
*Feng-Qi Cui,Anyang Tong,Jinyang Huang,Jie Zhang,Dan Guo,Zhi Liu,Meng Wang*

Main category: cs.CV

TL;DR: 提出了一种名为HDF的新框架，通过时间-频率分布注意力模块和自适应优化模块，解决了动态面部表情识别中的样本异质性和优化不平衡问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多源数据和个体表达变异性导致的样本异质性下性能下降，需要更鲁棒的解决方案。

Method: 设计了时间-频率分布注意力模块（DAM）和分布感知缩放模块（DSM），分别增强时间-频率建模和优化平衡。

Result: 在DFEW和FERV39k数据集上显著提高了识别准确性和鲁棒性，取得了更高的WAR和UAR。

Conclusion: HDF框架有效解决了样本异质性和优化不平衡问题，具有广泛的应用潜力。

Abstract: Dynamic Facial Expression Recognition (DFER) plays a critical role in
affective computing and human-computer interaction. Although existing methods
achieve comparable performance, they inevitably suffer from performance
degradation under sample heterogeneity caused by multi-source data and
individual expression variability. To address these challenges, we propose a
novel framework, called Heterogeneity-aware Distributional Framework (HDF), and
design two plug-and-play modules to enhance time-frequency modeling and
mitigate optimization imbalance caused by hard samples. Specifically, the
Time-Frequency Distributional Attention Module (DAM) captures both temporal
consistency and frequency robustness through a dual-branch attention design,
improving tolerance to sequence inconsistency and visual style shifts. Then,
based on gradient sensitivity and information bottleneck principles, an
adaptive optimization module Distribution-aware Scaling Module (DSM) is
introduced to dynamically balance classification and contrastive losses,
enabling more stable and discriminative representation learning. Extensive
experiments on two widely used datasets, DFEW and FERV39k, demonstrate that HDF
significantly improves both recognition accuracy and robustness. Our method
achieves superior weighted average recall (WAR) and unweighted average recall
(UAR) while maintaining strong generalization across diverse and imbalanced
scenarios. Codes are released at https://github.com/QIcita/HDF_DFER.

</details>


### [133] [Label tree semantic losses for rich multi-class medical image segmentation](https://arxiv.org/abs/2507.15777)
*Junwen Wang,Oscar MacCormac,William Rochford,Aaron Kujawa,Jonathan Shapey,Tom Vercauteren*

Main category: cs.CV

TL;DR: 提出两种基于树结构的语义损失函数，利用标签层次结构改进医学图像分割，在稀疏标注和全监督任务中均达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割方法对所有错误同等惩罚，忽略了标签空间的语义层次关系，尤其在标签丰富且类别差异细微时表现不佳。

Method: 提出两种树基语义损失函数，结合稀疏标注训练方法，适用于层次化标签组织。

Result: 在全脑分割和神经外科高光谱图像分割任务中达到SOTA性能。

Conclusion: 树基语义损失函数能有效利用标签层次信息，提升医学图像分割的准确性和鲁棒性。

Abstract: Rich and accurate medical image segmentation is poised to underpin the next
generation of AI-defined clinical practice by delineating critical anatomy for
pre-operative planning, guiding real-time intra-operative navigation, and
supporting precise post-operative assessment. However, commonly used learning
methods for medical and surgical imaging segmentation tasks penalise all errors
equivalently and thus fail to exploit any inter-class semantics in the labels
space. This becomes particularly problematic as the cardinality and richness of
labels increases to include subtly different classes. In this work, we propose
two tree-based semantic loss functions which take advantage of a hierarchical
organisation of the labels. We further incorporate our losses in a recently
proposed approach for training with sparse, background-free annotations to
extend the applicability of our proposed losses. Extensive experiments are
reported on two medical and surgical image segmentation tasks, namely head MRI
for whole brain parcellation (WBP) with full supervision and neurosurgical
hyperspectral imaging (HSI) for scene understanding with sparse annotations.
Results demonstrate that our proposed method reaches state-of-the-art
performance in both cases.

</details>


### [134] [Regularized Low-Rank Adaptation for Few-Shot Organ Segmentation](https://arxiv.org/abs/2507.15793)
*Ghassen Baklouti,Julio Silva-Rodríguez,Jose Dolz,Houda Bahig,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 提出了一种动态调整低秩适应（LoRA）秩的方法，用于医学图像分割，通过稀疏正则化自动选择任务适应的秩。


<details>
  <summary>Details</summary>
Motivation: LoRA在医学图像任务中需要固定秩，难以适应不同任务的复杂性。

Method: 引入l1稀疏正则化到损失函数中，通过近端优化器动态调整秩。

Result: 在少样本微调设置中显著优于标准LoRA和其他PEFT方法。

Conclusion: 该方法高效且对秩初始化不敏感，代码已开源。

Abstract: Parameter-efficient fine-tuning (PEFT) of pre-trained foundation models is
increasingly attracting interest in medical imaging due to its effectiveness
and computational efficiency. Among these methods, Low-Rank Adaptation (LoRA)
is a notable approach based on the assumption that the adaptation inherently
occurs in a low-dimensional subspace. While it has shown good performance, its
implementation requires a fixed and unalterable rank, which might be
challenging to select given the unique complexities and requirements of each
medical imaging downstream task. Inspired by advancements in natural image
processing, we introduce a novel approach for medical image segmentation that
dynamically adjusts the intrinsic rank during adaptation. Viewing the low-rank
representation of the trainable weight matrices as a singular value
decomposition, we introduce an l_1 sparsity regularizer to the loss function,
and tackle it with a proximal optimizer. The regularizer could be viewed as a
penalty on the decomposition rank. Hence, its minimization enables to find
task-adapted ranks automatically. Our method is evaluated in a realistic
few-shot fine-tuning setting, where we compare it first to the standard LoRA
and then to several other PEFT methods across two distinguishable tasks: base
organs and novel organs. Our extensive experiments demonstrate the significant
performance improvements driven by our method, highlighting its efficiency and
robustness against suboptimal rank initialization. Our code is publicly
available: https://github.com/ghassenbaklouti/ARENA

</details>


### [135] [Exploring Superposition and Interference in State-of-the-Art Low-Parameter Vision Models](https://arxiv.org/abs/2507.15798)
*Lilian Hollard,Lucas Mohimont,Nathalie Gaveau,Luiz-Angelo Steffenel*

Main category: cs.CV

TL;DR: 论文研究了低参数深度神经网络在计算机视觉中的性能，重点关注瓶颈架构及其在使用超线性激活函数时的行为。研究发现限制特征图中的干扰可以提升小规模网络的扩展性和准确性。


<details>
  <summary>Details</summary>
Motivation: 探讨低参数深度神经网络中瓶颈架构的性能，尤其是特征图中的干扰现象及其对网络扩展性和准确性的影响。

Method: 通过分析不同瓶颈架构，识别减少干扰的关键设计元素，并提出名为NoDepth Bottleneck的概念验证架构。

Result: 提出的NoDepth Bottleneck架构在ImageNet数据集上表现出稳健的扩展性和准确性。

Conclusion: 研究为低参数范围的神经网络提供了更高效和可扩展的设计思路，并深化了对计算机视觉中瓶颈架构的理解。

Abstract: The paper investigates the performance of state-of-the-art low-parameter deep
neural networks for computer vision, focusing on bottleneck architectures and
their behavior using superlinear activation functions. We address interference
in feature maps, a phenomenon associated with superposition, where neurons
simultaneously encode multiple characteristics. Our research suggests that
limiting interference can enhance scaling and accuracy in very low-scaled
networks (under 1.5M parameters). We identify key design elements that reduce
interference by examining various bottleneck architectures, leading to a more
efficient neural network. Consequently, we propose a proof-of-concept
architecture named NoDepth Bottleneck built on mechanistic insights from our
experiments, demonstrating robust scaling accuracy on the ImageNet dataset.
These findings contribute to more efficient and scalable neural networks for
the low-parameter range and advance the understanding of bottlenecks in
computer vision. https://caiac.pubpub.org/pub/3dh6rsel

</details>


### [136] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM生成未标注数据的预测掩码，并通过不确定性校准和自依赖训练策略提升半监督语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务中标注数据稀缺的问题，利用基础分割模型的泛化能力。

Method: 提出ConformalSAM框架，通过不确定性校准和自依赖训练策略优化SEEM生成的掩码。

Result: 在三个标准基准测试中优于现有半监督语义分割方法。

Conclusion: ConformalSAM能有效利用基础模型提升半监督分割性能，并可作为插件增强其他方法。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [137] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 当前多模态大语言模型（MLLMs）在多模态上下文学习（MICL）中过度依赖文本信息，忽视视觉线索。本文提出动态注意力重分配（DARA）和TrueMICL数据集，以提升模型对视觉信息的利用能力。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在MICL中忽视视觉信息，导致性能受限。本文旨在解决这一问题，提升模型的多模态适应能力。

Method: 提出动态注意力重分配（DARA）策略，调整视觉和文本标记的注意力权重；构建TrueMICL数据集，明确要求整合多模态信息。

Result: 实验表明，DARA和TrueMICL显著提升了模型的多模态上下文学习能力。

Conclusion: 本文提出的方法有效解决了MLLMs在MICL中的视觉信息利用问题，为多模态学习提供了可靠评估和改进方案。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [138] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现出色，通过改进扩散后验采样方法，显著提升了统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在多元地下建模和概率反演中的应用，以解决变分自编码器和生成对抗网络在此类任务中的局限性。

Method: 提出对扩散后验采样方法的改进，包括引入考虑噪声污染的似然近似，并在多元地质场景中评估性能。

Result: 实验显示，改进方法在统计稳健性、后验概率密度采样和计算成本方面优于原始方法。

Conclusion: 改进的扩散模型方法适用于硬数据和间接条件数据，且反演速度更快，优于传统方法。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [139] [Can Your Model Separate Yolks with a Water Bottle? Benchmarking Physical Commonsense Understanding in Video Generation Models](https://arxiv.org/abs/2507.15824)
*Enes Sanli,Baris Sarper Tezcan,Aykut Erdem,Erkut Erdem*

Main category: cs.CV

TL;DR: PhysVidBench是一个评估文本到视频生成模型物理常识能力的基准，包含383个精心设计的提示，通过三阶段评估流程间接测试模型的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频生成模型在物理常识方面表现不足，导致输出违反直觉的因果关系和物体行为。

Method: 设计包含工具使用、材料属性和程序交互的提示，通过生成视频、视觉语言模型标注和语言模型回答问题三阶段评估。

Result: PhysVidBench提供了一个结构化、可解释的框架，用于评估生成视频模型的物理常识能力。

Conclusion: 该基准填补了当前文本到视频评估中物理合理性测试的空白，为模型改进提供了方向。

Abstract: Recent progress in text-to-video (T2V) generation has enabled the synthesis
of visually compelling and temporally coherent videos from natural language.
However, these models often fall short in basic physical commonsense, producing
outputs that violate intuitive expectations around causality, object behavior,
and tool use. Addressing this gap, we present PhysVidBench, a benchmark
designed to evaluate the physical reasoning capabilities of T2V systems. The
benchmark includes 383 carefully curated prompts, emphasizing tool use,
material properties, and procedural interactions, and domains where physical
plausibility is crucial. For each prompt, we generate videos using diverse
state-of-the-art models and adopt a three-stage evaluation pipeline: (1)
formulate grounded physics questions from the prompt, (2) caption the generated
video with a vision-language model, and (3) task a language model to answer
several physics-involved questions using only the caption. This indirect
strategy circumvents common hallucination issues in direct video-based
evaluation. By highlighting affordances and tool-mediated actions, areas
overlooked in current T2V evaluations, PhysVidBench provides a structured,
interpretable framework for assessing physical commonsense in generative video
models.

</details>


### [140] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: SeC是一种基于概念驱动的视频对象分割框架，利用大型视觉语言模型构建对象中心表示，显著提升了复杂场景下的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象分割方法依赖外观匹配，缺乏人类对对象的概念理解，导致在剧烈视觉变化和复杂场景中表现不佳。

Method: SeC通过大型视觉语言模型整合视觉线索，构建对象的概念先验，并在推理时结合语义推理和特征匹配。

Result: 在SeCVOS基准测试中，SeC比SAM 2.1提升了11.8分，成为概念感知视频对象分割的新标杆。

Conclusion: SeC通过概念驱动的表示和动态计算调整，显著提升了复杂场景下的分割性能，为未来研究提供了新方向。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


### [141] [Latent Denoising Makes Good Visual Tokenizers](https://arxiv.org/abs/2507.15856)
*Jiawei Yang,Tianhong Li,Lijie Fan,Yonglong Tian,Yue Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新的视觉分词器（l-DeTok），通过直接与下游去噪目标对齐，提升了生成模型的效果。


<details>
  <summary>Details</summary>
Motivation: 现代生成模型的训练目标（如去噪）与分词器的潜在嵌入重建能力之间存在关联，因此希望通过改进分词器设计来提升生成模型性能。

Method: 提出了Latent Denoising Tokenizer（l-DeTok），通过训练分词器从被噪声和掩码污染的潜在嵌入中重建干净图像。

Result: 在ImageNet 256x256上，l-DeTok在六种代表性生成模型中均优于标准分词器。

Conclusion: 去噪是分词器设计的基本原则，未来分词器设计可以以此为出发点。

Abstract: Despite their fundamental role, it remains unclear what properties could make
visual tokenizers more effective for generative modeling. We observe that
modern generative models share a conceptually similar training objective --
reconstructing clean signals from corrupted inputs such as Gaussian noise or
masking -- a process we term denoising. Motivated by this insight, we propose
aligning tokenizer embeddings directly with the downstream denoising objective,
encouraging latent embeddings to be more easily reconstructed even when heavily
corrupted. To achieve this, we introduce the Latent Denoising Tokenizer
(l-DeTok), a simple yet effective tokenizer trained to reconstruct clean images
from latent embeddings corrupted by interpolative noise and random masking.
Extensive experiments on ImageNet 256x256 demonstrate that our tokenizer
consistently outperforms standard tokenizers across six representative
generative models. Our findings highlight denoising as a fundamental design
principle for tokenizer development, and we hope it could motivate new
perspectives for future tokenizer design.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [142] [Hyper-spectral Unmixing algorithms for remote compositional surface mapping: a review of the state of the art](https://arxiv.org/abs/2507.14260)
*Alfredo Gimenez Zapiola,Andrea Boselli,Alessandra Menafoglio,Simone Vantini*

Main category: astro-ph.IM

TL;DR: 综述了用于遥感图像的数据分析方法，重点讨论了高光谱图像中材料推断及其丰度和空间分布估计问题，比较了相关方法，并探讨了公开数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱图像中材料识别及其分布估计的问题，为相关研究提供方法比较和数据集参考。

Method: 综述和比较了高光谱解混方法，并分析了最新技术。

Result: 总结了成功的高光谱解混方法，并探讨了公开数据集的应用。

Conclusion: 指出了开放性问题，并提出了未来研究的具体建议。

Abstract: This work concerns a detailed review of data analysis methods used for
remotely sensed images of large areas of the Earth and of other solid
astronomical objects. In detail, it focuses on the problem of inferring the
materials that cover the surfaces captured by hyper-spectral images and
estimating their abundances and spatial distributions within the region. The
most successful and relevant hyper-spectral unmixing methods are reported as
well as compared, as an addition to analysing the most recent methodologies.
The most important public data-sets in this setting, which are vastly used in
the testing and validation of the former, are also systematically explored.
Finally, open problems are spotlighted and concrete recommendations for future
research are provided.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [143] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO是一个高效处理大规模视频数据中复杂对象查询的系统，通过预训练视觉编码器生成紧凑特征，结合倒排多索引结构，显著降低搜索延迟并提高准确性。


<details>
  <summary>Details</summary>
Motivation: 视频数据激增带来查询挑战，现有方法难以适应新对象类别或高延迟问题。

Method: LOVO使用预训练视觉编码器提取特征，构建倒排多索引结构，支持快速近似最近邻搜索和跨模态重排序。

Result: LOVO在复杂查询中表现优异，搜索延迟降低85倍，索引构建成本显著减少。

Conclusion: LOVO为视频分析中的对象查询设定了新标准，提供高效、可扩展的解决方案。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [144] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: 该论文提出了一种名为U-MARVEL的统一框架，用于多模态检索任务，通过系统分析嵌入学习和训练策略的关键因素，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的多模态检索方法虽然成功，但其检索机制未充分探索，可能导致性能不佳和泛化能力有限。

Method: 通过实现通用的MLLM嵌入学习流程，分析关键因素，并探索嵌入生成和训练策略的细节（如渐进过渡、硬负样本挖掘和重排器蒸馏）。

Result: U-MARVEL框架在M-BEIR基准测试中大幅领先现有方法，并在零样本任务中表现优异。

Conclusion: U-MARVEL框架在多模态检索任务中表现出强大的泛化能力，为嵌入学习提供了新的研究方向。

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [145] [On Splitting Lightweight Semantic Image Segmentation for Wireless Communications](https://arxiv.org/abs/2507.14199)
*Ebrahim Abu-Helalah,Jordi Serra,Jordi Perez-Romero*

Main category: cs.NI

TL;DR: 该论文提出了一种新颖的语义通信方法，通过在资源受限的发射器和接收器之间分割语义图像分割过程，以降低带宽需求和计算负载。


<details>
  <summary>Details</summary>
Motivation: 语义通信在图像分割中能降低通信成本，但在资源受限环境和变化信道条件下，计算效率和带宽需求之间缺乏平衡。

Method: 将语义图像分割过程分割到资源受限的发射器和接收器上，减少传输数据量并保持分割精度。

Result: 实验显示，与全部分割在发射器相比，传输比特率降低72%，发射器计算负载减少19%以上。

Conclusion: 该方法适用于通信系统，尤其是未来的6G系统，能有效平衡带宽需求和计算效率。

Abstract: Semantic communication represents a promising technique towards reducing
communication costs, especially when dealing with image segmentation, but it
still lacks a balance between computational efficiency and bandwidth
requirements while maintaining high image segmentation accuracy, particularly
in resource-limited environments and changing channel conditions. On the other
hand, the more complex and larger semantic image segmentation models become,
the more stressed the devices are when processing data. This paper proposes a
novel approach to implementing semantic communication based on splitting the
semantic image segmentation process between a resource constrained transmitter
and the receiver. This allows saving bandwidth by reducing the transmitted data
while maintaining the accuracy of the semantic image segmentation.
Additionally, it reduces the computational requirements at the resource
constrained transmitter compared to doing all the semantic image segmentation
in the transmitter. The proposed approach is evaluated by means of
simulation-based experiments in terms of different metrics such as
computational resource usage, required bit rate and segmentation accuracy. The
results when comparing the proposal with the full semantic image segmentation
in the transmitter show that up to 72% of the bit rate was reduced in the
transmission process. In addition, the computational load of the transmitter is
reduced by more than 19%. This reflects the interest of this technique for its
application in communication systems, particularly in the upcoming 6G systems.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [146] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: APTx Neuron是一种新型的统一神经计算单元，将非线性和线性变换集成到单个可训练表达式中，提高了计算效率和架构简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统神经元设计需要独立的激活层，导致计算效率低下和架构复杂。APTx Neuron旨在通过统一设计解决这一问题。

Method: APTx Neuron基于APTx激活函数，形式为$y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$，所有参数均可训练。

Result: 在MNIST数据集上验证，仅用20个epoch和约332K可训练参数，测试准确率达到96.69%。

Conclusion: APTx Neuron展示了比传统神经元更强的表达能力和计算效率，为统一神经元设计提供了新方向。

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [147] [Prompt-aware of Frame Sampling for Efficient Text-Video Retrieval](https://arxiv.org/abs/2507.15491)
*Deyu Zhang,Tingting Long,Jinrui Zhang,Ligeng Chen,Ju Ren,Yaoxue Zhang*

Main category: cs.MM

TL;DR: ProCLIP是一个用户中心的框架，通过动态帧采样和两阶段候选剪枝策略，显著提升了文本-视频检索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡准确性和计算效率方面存在挑战，均匀帧采样计算成本高，而显著帧采样则因查询无关的选择导致结果偏差。

Method: 设计了提示感知的帧采样策略和两阶段候选剪枝策略，结合轻量级特征提取器和CLIP重排序。

Result: 在基准测试中，ProCLIP实现了75.3%的延迟降低，同时在MSR-VTT数据集上保持R@1=49.0的准确性。

Conclusion: ProCLIP在保持高准确性的同时显著提升了效率，适用于边缘端设备的实时应用。

Abstract: Enabling efficient text-video retrieval on edge-end devices is critical for
real-world applications. Yet, existing methods face a critical challenge in
balancing accuracy and computational efficiency: uniform frame sampling methods
ensure content coverage but incur prohibitive computational costs, while
salient-frame sampling methods reduce overhead but suffer from query-agnostic
frame selection that biases retrieval results. To address this, we propose
ProCLIP, a user-centric framework that achieves state-of-the-art accuracy with
significantly improved efficiency. We design a prompt-aware frame sampling
strategy that dynamically guides lightweight feature extractors using textual
prompts to select semantically relevant frames, overcoming the limitations of
existing salient-frame sampling methods which rely on static, query-agnostic
selection criteria. Moreover, we adopt a two-stage candidate pruning strategy
that combines rapid coarse filtering via a lightweight module with CLIP-powered
fine-grained re-ranking, enhancing retrieval efficiency while preserving
accuracy. Experiments across benchmarks show ProCLIP achieves 75.3% latency
reduction versus baselines while maintaining competitive accuracy, i.e.,
R@1=49.0 in MSR-VTT dataset. Code is available at
https://github.com/tiffylong/ProCLIP.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [148] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估网络代理行为风险的数据集，旨在开发安全措施。初步评估显示当前LLMs在预测高风险行为时表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLM驱动的自主网络代理快速发展，其潜在风险亟需有效安全措施。WebGuard旨在填补这一空白，提供评估和开发安全护栏的数据支持。

Method: WebGuard包含4,939个人工标注的行为，覆盖193个网站和22个领域，采用三级风险分类（SAFE、LOW、HIGH）。通过微调Qwen2.5VL-7B模型提升性能。

Result: 微调模型将准确率从37%提升至80%，高风险行为召回率从20%提升至76%，但仍未达到高可靠性部署要求。

Conclusion: WebGuard为网络代理安全提供了重要数据集，但当前模型性能仍需进一步提升以满足高可靠性需求。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [149] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型多模态模型（LMM）的代理框架InsightX Agent，用于X射线无损检测（NDT），通过结合稀疏可变形多尺度检测器（SDMSD）和基于证据的反思工具（EGR），提高了检测的可靠性、可解释性和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent以LMM为核心协调器，结合SDMSD进行多尺度缺陷检测和EGR工具进行反思验证，实现主动推理。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的目标检测F1分数，并显著提高了分析的可解释性和可信度。

Conclusion: 该框架展示了代理式LLM在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [150] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的图表领域视觉语言模型，通过两阶段训练策略（Chart-COT和Chart-RFT）提升复杂图表推理能力，实验结果显示其优于现有图表领域方法，甚至可与GPT-4o等大型模型媲美。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在通用多模态数据（尤其是图表领域）上的优势，解决图表推理中缺乏高质量数据的问题。

Method: 提出程序化数据合成技术生成高质量图表推理数据，并采用两阶段训练策略：Chart-COT（逐步监督）和Chart-RFT（数值敏感的强化微调）。

Result: Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异，优于现有图表领域方法，接近GPT-4o等大型模型。

Conclusion: Chart-R1通过创新的数据合成和训练策略，显著提升了图表推理能力，为多模态推理研究提供了新思路。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [151] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件生成的知识蒸馏框架GenDD，通过Split Tokenization和Distribution Contraction技术解决了高维优化和标签语义监督不足的问题，在无监督和监督设置下均取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在高维优化和缺乏标签语义监督时效果不佳，需要一种更高效的框架来解决这些问题。

Method: 提出GenDD框架，结合Split Tokenization策略实现稳定的无监督知识蒸馏，并通过Distribution Contraction技术将标签监督融入重建目标。

Result: 在无监督设置下，GenDD比KL基线在ImageNet验证集上提升了16.29%；在监督设置下，ResNet-50在600轮训练后达到82.28%的top-1准确率。

Conclusion: GenDD框架通过创新的技术解决了知识蒸馏中的关键挑战，并在实验中验证了其高效性和优越性。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [152] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文通过追溯自注意力机制的概念起源，将其与更广泛的基于亲和矩阵的计算范式联系起来，强调了Inf-FS作为基础方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 探索自注意力机制在不同领域（如计算机视觉、自然语言处理和图学习）中的共同数学基础，揭示其与Inf-FS方法的联系。

Method: 通过分析亲和矩阵的定义和应用，比较自注意力机制与Inf-FS方法的异同，揭示其共同的计算原理。

Result: 自注意力机制可视为Inf-FS的特例，两者均基于对成对关系的推理，但亲和矩阵的定义和应用方式不同。

Conclusion: 通过将自注意力机制置于基于亲和矩阵的计算范式中，本文统一了多种机器学习研究，并强调了其共同的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [153] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种新型多模态框架，通过整合稀疏的胸部X光片和临床数据，预测ICU患者的异常X光结果。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法捕捉胸部X光的动态变化，限制了其在ICU中的应用。

Method: CXR-TFT结合视觉编码器的潜在嵌入和每小时临床数据，通过变压器模型预测未来X光结果。

Result: 在20,000名ICU患者的回顾性研究中，CXR-TFT能提前12小时高精度预测异常X光结果。

Conclusion: CXR-TFT提供了高时间分辨率的预测能力，有望改善急性呼吸窘迫综合征等时间敏感病症的管理。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [154] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文提出了一种将等变性网络理论扩展到时间参数化序列变换的方法，解决了传统RNN在流动对称性上的不足。


<details>
  <summary>Details</summary>
Motivation: 数据以连续流的形式到达感官，其平滑变换可视为环境的连续对称性。现有等变性网络仅适用于静态变换和前馈网络，限制了其在序列模型中的应用。

Method: 扩展等变性网络理论到时间参数化的序列变换（流动），提出了一种使RNN具有流动等变性的方法。

Result: 实验表明，流动等变性模型在训练速度、长度泛化和速度泛化上显著优于非等变性模型。

Conclusion: 该研究为构建尊重时间参数化对称性的序列模型迈出了第一步。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [155] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM是一个统一的、可解释的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数描述AL轨迹，并预测未来性能。


<details>
  <summary>Details</summary>
Motivation: 传统评估方法仅关注最终准确性，无法捕捉学习过程的完整动态，PALM旨在填补这一空白。

Method: PALM通过四个参数（可实现准确性、覆盖效率、早期表现和可扩展性）描述AL轨迹，并提供预测性描述。

Result: PALM在CIFAR和ImageNet数据集上验证，能有效预测学习曲线，并揭示AL方法的学习效率、数据覆盖和可扩展性。

Conclusion: PALM为AL的评估提供了系统性、可重复的方法，适用于研究和实际应用。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [156] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: GUI-G² introduces Gaussian rewards for GUI grounding, outperforming UI-TARS-72B by 24.7%.


<details>
  <summary>Details</summary>
Motivation: Current binary rewards in GUI grounding ignore continuous spatial interactions, while human clicking behavior forms Gaussian distributions.

Method: GUI-G² uses Gaussian point rewards and coverage rewards, with adaptive variance for diverse element scales.

Result: GUI-G² outperforms UI-TARS-72B by 24.7% on ScreenSpot-Pro, showing superior robustness and generalization.

Conclusion: Continuous Gaussian modeling improves GUI grounding, setting a new paradigm for spatial reasoning.

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [157] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限情况下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在数据受限场景下的优势，填补其与自回归模型比较的研究空白。

Method: 系统研究掩码扩散模型在数据受限环境中的表现，分析其隐式数据增强特性。

Result: 扩散模型在计算资源充足时显著优于自回归模型，验证损失更低且下游任务表现更好。

Conclusion: 当数据成为瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [158] [Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks](https://arxiv.org/abs/2507.14694)
*Yue Ma,Kanglei Zhou,Fuyang Yu,Frederick W. B. Li,Xiaohui Liang*

Main category: cs.RO

TL;DR: ProbHMI是一种用于3D人体运动预测的方法，通过可逆网络和潜在空间建模实现不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 在安全关键场景（如人机协作）中，量化预测的不确定性至关重要，但现有方法难以实现。

Method: 使用可逆网络将姿态参数化到解耦的潜在空间，并显式预测未来潜在分布。

Result: 在基准测试中表现优异，验证了不确定性校准的有效性。

Conclusion: ProbHMI在确定性和多样性预测中均表现良好，适用于风险感知决策。

Abstract: 3D human motion forecasting aims to enable autonomous applications.
Estimating uncertainty for each prediction (i.e., confidence based on
probability density or quantile) is essential for safety-critical contexts like
human-robot collaboration to minimize risks. However, existing diverse motion
forecasting approaches struggle with uncertainty quantification due to implicit
probabilistic representations hindering uncertainty modeling. We propose
ProbHMI, which introduces invertible networks to parameterize poses in a
disentangled latent space, enabling probabilistic dynamics modeling. A
forecasting module then explicitly predicts future latent distributions,
allowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI
achieves strong performance for both deterministic and diverse prediction while
validating uncertainty calibration, critical for risk-aware decision making.

</details>


### [159] [Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe](https://arxiv.org/abs/2507.15444)
*Leonard Bauersfeld,Davide Scaramuzza*

Main category: cs.RO

TL;DR: 提出了一种基于实时流场测量的四旋翼无人机闭环控制系统，用于在狭窄管道中悬停，解决了气流扰动问题。


<details>
  <summary>Details</summary>
Motivation: 在狭窄管道中悬停的四旋翼无人机面临气流扰动挑战，现有方法依赖持续运动或稳定性不足。

Method: 开发了低延迟事件型烟雾测速法，结合循环卷积神经网络的扰动估计器和强化学习控制器。

Result: 系统在管道横截面横向移动时表现优异，能有效抵消瞬态气动效应，避免碰撞。

Conclusion: 首次展示了基于实时流场测量的闭环控制无人机，为复杂气动环境飞行研究开辟新方向。

Abstract: Autonomous quadrotor flight in confined spaces such as pipes and tunnels
presents significant challenges due to unsteady, self-induced aerodynamic
disturbances. Very recent advances have enabled flight in such conditions, but
they either rely on constant motion through the pipe to mitigate airflow
recirculation effects or suffer from limited stability during hovering. In this
work, we present the first closed-loop control system for quadrotors for
hovering in narrow pipes that leverages real-time flow field measurements. We
develop a low-latency, event-based smoke velocimetry method that estimates
local airflow at high temporal resolution. This flow information is used by a
disturbance estimator based on a recurrent convolutional neural network, which
infers force and torque disturbances in real time. The estimated disturbances
are integrated into a learning-based controller trained via reinforcement
learning. The flow-feedback control proves particularly effective during
lateral translation maneuvers in the pipe cross-section. There, the real-time
disturbance information enables the controller to effectively counteract
transient aerodynamic effects, thereby preventing collisions with the pipe
wall. To the best of our knowledge, this work represents the first
demonstration of an aerial robot with closed-loop control informed by real-time
flow field measurements. This opens new directions for research on flight in
aerodynamically complex environments. In addition, our work also sheds light on
the characteristic flow structures that emerge during flight in narrow,
circular pipes, providing new insights at the intersection of robotics and
fluid dynamics.

</details>


### [160] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3是一个大规模视觉-语言-动作模型，展示了在泛化到新对象、环境和抽象指令方面的卓越能力，并能高效微调以适应新场景。


<details>
  <summary>Details</summary>
Motivation: 开发通用机器人策略，以构建能够辅助人类日常生活的通用机器人。

Method: 通过多方面的训练方法，包括与网络规模视觉语言数据的共同训练、通过VR设备收集的人类轨迹数据的高效微调，以及机器人轨迹数据的有效模仿学习。

Result: GR-3在多种挑战性任务上超越了最先进的基线方法π₀，展示了鲁棒和可靠的性能。

Conclusion: GR-3是构建通用机器人能力的重要一步，有望在人类日常生活中发挥作用。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [161] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 论文探讨了将人类主动注视机制引入机器人视觉系统，以提高效率和性能。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过注视和注意力机制高效处理任务相关区域，而机器人系统通常被动处理图像，缺乏这种效率。

Method: 结合人类注视数据，提出了一种基于ViT的注视引导图像处理框架，并探索了两种注视预测方法。

Result: 该方法显著降低了计算开销，同时提高了任务精度和对干扰的鲁棒性。

Conclusion: 人类视觉机制为机器人视觉系统提供了有益的启发。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [162] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种优化的大型视觉语言模型，用于深入理解多种图表类型，通过高效数据生成和双路径训练策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在图表理解任务中存在泛化能力不足和数据对齐训练缺失的问题。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: 实验表明ChartScope显著提升多种图表类型的理解能力。

Conclusion: ChartScope通过新方法和基准测试解决了现有局限性，提升了图表理解能力。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [163] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 论文提出了一种基于上下文学习（ICL）和视觉语言模型（VLM）的太赫兹（THz）图像分类方法，无需微调即可提升分类效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安全筛查和材料分类等应用中具有潜力，但受限于标注数据少、分辨率低和视觉模糊，传统分类方法效果不佳。

Method: 采用模态对齐的提示框架，将两种开放权重的VLM适配到THz领域，并在零样本和单样本设置下评估。

Result: 实验表明，ICL在低数据量情况下提升了分类性能和可解释性。

Conclusion: 这是ICL增强的VLM首次应用于THz成像，为资源受限的科学领域提供了新方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [164] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: CALIN是一种推理时校准方法，旨在减少多模态大语言模型（MLLMs）在医学图像分类中的校准偏差和人口不公平性。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医学图像分析中具有巨大潜力，但其预测准确性和校准误差在不同人口亚组中的表现需要深入分析，以确保临床实践中的安全部署。

Method: CALIN通过双层程序（从总体水平到亚组水平）估计校准需求，并在推理时应用校准矩阵来调整预测置信度。

Result: 在三个医学影像数据集上的实验表明，CALIN能有效确保预测的公平校准，同时提高整体预测准确性，且公平性与效用之间的权衡最小。

Conclusion: CALIN为MLLMs在医学图像分类中的公平性和校准问题提供了一种有效的解决方案。

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [165] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: MiDeSeC数据集包含25名患者的H&E染色乳腺癌切片，用于研究有丝分裂形态。


<details>
  <summary>Details</summary>
Motivation: 研究乳腺癌有丝分裂形态需要覆盖多种可能形状的大规模数据集。

Method: 从25名患者的50个1024*1024像素区域中提取数据，使用3D Histech和Olympus设备扫描。

Result: 数据集包含500多个有丝分裂样本，2/3用于训练，1/3用于测试。

Conclusion: MiDeSeC数据集为乳腺癌有丝分裂研究提供了丰富资源。

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [166] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: NuSeC数据集包含100张1024*1024像素的图像，分为75%训练集和25%测试集，用于未来方法比较。


<details>
  <summary>Details</summary>
Motivation: 为研究人员提供一个标准化的数据集，以便未来开发的方法可以进行一致的比较分析。

Method: 从25位患者的每张幻灯片中选取4张图像，随机选择1张作为测试集，其余作为训练集。

Result: 训练集包含75张图像（约30000个核结构），测试集包含25张图像（约6000个核结构）。

Conclusion: NuSeC数据集的设计支持未来研究方法的标准化比较。

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [167] [Self-Supervised Joint Reconstruction and Denoising of T2-Weighted PROPELLER MRI of the Lungs at 0.55T](https://arxiv.org/abs/2507.14308)
*Jingjia Chen,Haoyang Pei,Christoph Maier,Mary Bruno,Qiuting Wen,Seon-Hi Shin,William Moore,Hersh Chandarana,Li Feng*

Main category: eess.IV

TL;DR: 提出了一种自监督联合重建和去噪模型，用于改进0.55T T2加权PROPELLER肺部MRI图像质量。


<details>
  <summary>Details</summary>
Motivation: 提高低场强（0.55T）T2加权肺部MRI的图像清晰度和结构完整性，同时减少扫描时间。

Method: 使用自监督学习框架，将PROPELLER采集的每个叶片沿读出方向分成两个子集，分别用于训练重建网络和计算损失，无需干净目标数据。

Result: 模型显著提升了图像质量，与CT图像对齐良好，且仅需一半叶片数量即可完成扫描。读者评估显示优于传统方法（p<0.001）。

Conclusion: 通过利用k空间子集的结构冗余性，自监督模型有效重建图像并抑制噪声，适用于0.55T T2加权肺部MRI。

Abstract: Purpose: This study aims to improve 0.55T T2-weighted PROPELLER lung MRI
through a self-supervised joint reconstruction and denoising model.
  Methods: T2-weighted 0.55T lung MRI dataset including 44 patients with
previous covid infection were used. A self-supervised learning framework was
developed, where each blade of the PROPELLER acquisition was split along the
readout direction into two partitions. One subset trains the unrolled
reconstruction network, while the other subset is used for loss calculation,
enabling self-supervised training without clean targets and leveraging matched
noise statistics for denoising. For comparison, Marchenko-Pastur Principal
Component Analysis (MPPCA) was performed along the coil dimension, followed by
conventional parallel imaging reconstruction. The quality of the reconstructed
lung MRI was assessed visually by two experienced radiologists independently.
  Results: The proposed self-supervised model improved the clarity and
structural integrity of the lung images. For cases with available CT scans, the
reconstructed images demonstrated strong alignment with corresponding CT
images. Additionally, the proposed model enables further scan time reduction by
requiring only half the number of blades. Reader evaluations confirmed that the
proposed method outperformed MPPCA-denoised images across all categories
(Wilcoxon signed-rank test, p<0.001), with moderate inter-reader agreement
(weighted Cohen's kappa=0.55; percentage of exact and within +/-1 point
agreement=91%).
  Conclusion: By leveraging intrinsic structural redundancies between two
disjoint splits of k-space subsets, the proposed self-supervised learning model
effectively reconstructs the image while suppressing the noise for 0.55T
T2-weighted lung MRI with PROPELLER sampling.

</details>


### [168] [Classification of Histopathology Slides with Persistence Homology Convolutions](https://arxiv.org/abs/2507.14378)
*Shrunal Pothagoni,Benjamin Schweinhart*

Main category: eess.IV

TL;DR: 提出了一种名为Persistent Homology Convolutions的新方法，通过局部持久同调数据改进CNN在医学图像分类中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统CNN可能丢失拓扑信息，而医学图像（如病理切片）中拓扑信息对疾病诊断至关重要。现有方法使用全局拓扑摘要，缺乏局部特征信息。

Method: 开发了Persistent Homology Convolutions方法，通过改进的卷积算子生成局部持久同调数据，捕捉拓扑特征的局部性和平移不变性。

Result: 实验表明，使用该方法训练的模型性能优于传统方法，且对超参数不敏感，表明其能有效提取病理切片的几何信息。

Conclusion: Persistent Homology Convolutions能显著提升医学图像分类的准确性，尤其在需要拓扑信息的领域。

Abstract: Convolutional neural networks (CNNs) are a standard tool for computer vision
tasks such as image classification. However, typical model architectures may
result in the loss of topological information. In specific domains such as
histopathology, topology is an important descriptor that can be used to
distinguish between disease-indicating tissue by analyzing the shape
characteristics of cells. Current literature suggests that reintroducing
topological information using persistent homology can improve medical
diagnostics; however, previous methods utilize global topological summaries
which do not contain information about the locality of topological features. To
address this gap, we present a novel method that generates local persistent
homology-based data using a modified version of the convolution operator called
Persistent Homology Convolutions. This method captures information about the
locality and translation invariance of topological features. We perform a
comparative study using various representations of histopathology slides and
find that models trained with persistent homology convolutions outperform
conventionally trained models and are less sensitive to hyperparameters. These
results indicate that persistent homology convolutions extract meaningful
geometric information from the histopathology slides.

</details>


### [169] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: QUTCC是一种非线性、非均匀的量化不确定性训练和校准技术，用于提高MRI和显微镜去噪等任务的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在科学和医学逆问题中可能产生幻觉，准确性比感知质量更重要。现有方法使用线性缩放因子导致不确定性边界较大且信息量少。

Method: 提出QUTCC技术，通过U-Net架构和量化嵌入预测完整的条件分布，并通过迭代查询上下分位数逐步细化不确定性边界。

Result: QUTCC能准确定位图像估计中的幻觉，并在保持统计覆盖的同时提供更紧的不确定性区间。

Conclusion: QUTCC在去噪和MRI重建任务中表现优于现有方法，提供了更精确的不确定性估计。

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [170] [PET Image Reconstruction Using Deep Diffusion Image Prior](https://arxiv.org/abs/2507.15078)
*Fumio Hashimoto,Kuang Gong*

Main category: eess.IV

TL;DR: 提出了一种基于扩散模型的解剖学先验引导PET图像重建方法，适用于不同示踪剂，计算高效且泛化能力强。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在医学图像去噪和重建中表现优异，但在PET成像中受限于示踪剂对比度差异和高计算需求。

Method: 结合扩散采样和模型微调，利用HQS算法提高计算效率，实现跨示踪剂的高质量重建。

Result: 在模拟和临床数据上验证了方法的有效性，展示了跨示踪剂和扫描仪类型的鲁棒性。

Conclusion: 该方法为低剂量PET成像提供了高效且通用的重建框架。

Abstract: Diffusion models have shown great promise in medical image denoising and
reconstruction, but their application to Positron Emission Tomography (PET)
imaging remains limited by tracer-specific contrast variability and high
computational demands. In this work, we proposed an anatomical prior-guided PET
image reconstruction method based on diffusion models, inspired by the deep
diffusion image prior (DDIP) framework. The proposed method alternated between
diffusion sampling and model fine-tuning guided by the PET sinogram, enabling
the reconstruction of high-quality images from various PET tracers using a
score function pretrained on a dataset of another tracer. To improve
computational efficiency, the half-quadratic splitting (HQS) algorithm was
adopted to decouple network optimization from iterative PET reconstruction. The
proposed method was evaluated using one simulation and two clinical datasets.
For the simulation study, a model pretrained on [$^{18}$F]FDG data was tested
on amyloid-negative PET data to assess out-of-distribution (OOD) performance.
For the clinical-data validation, ten low-dose [$^{18}$F]FDG datasets and one
[$^{18}$F]Florbetapir dataset were tested on a model pretrained on data from
another tracer. Experiment results show that the proposed PET reconstruction
method can generalize robustly across tracer distributions and scanner types,
providing an efficient and versatile reconstruction framework for low-dose PET
imaging.

</details>


### [171] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: 利用深度学习模型通过结膜苍白检测贫血，使用MobileNet架构，在CP-AnemiC数据集上取得高精度，并探索量化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统贫血检测方法成本高且依赖专家知识，深度学习可提供低成本、高效的替代方案。

Method: 使用MobileNet架构，结合数据增强和交叉验证策略，对模型进行端到端微调，并测试不同量化方案。

Result: 模型准确率0.9313，精度0.9374，F1分数0.9773；FP16量化性能稳定，INT8和INT4量化性能下降明显。

Conclusion: 研究支持进一步探索量化方案和硬件优化，以平衡模型大小、推理时间和诊断准确性。

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [172] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: 本研究通过系统评估解剖学先验知识，提出了一种基于器官特异性解剖先验的多类标注策略，显著提高了深度学习模型对嗜铬细胞瘤（PCC）的分割精度。


<details>
  <summary>Details</summary>
Motivation: 准确的PCC分割对肿瘤负荷估计、预后和治疗规划至关重要，同时可减少对昂贵基因检测的依赖。

Method: 使用nnU-Net框架评估11种标注策略，引入基于邻近器官解剖先验的多类标注方案，并在105例CT扫描上进行训练和测试。

Result: Tumor + Kidney + Aorta (TKA)标注策略在DSC、NSD和F1分数上显著优于传统Tumor + Body (TB)标注，且在不同基因亚型中表现稳健。

Conclusion: 相关解剖学先验的引入可显著提升PCC分割精度，支持临床评估和长期监测。

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [173] [Personalized 3D Myocardial Infarct Geometry Reconstruction from Cine MRI with Explicit Cardiac Motion Modeling](https://arxiv.org/abs/2507.15194)
*Yilin Lyu,Fan Yang,Xiaoyue Liu,Zichen Jiang,Joshua Dillon,Debbie Zhao,Martyn Nash,Charlene Mauger,Alistair Young,Ching-Hui Sia,Mark YY Chan,Lei Li*

Main category: eess.IV

TL;DR: 提出了一种从2D标准电影MRI自动重建高保真3D心肌梗死几何的新方法，无需对比剂。


<details>
  <summary>Details</summary>
Motivation: LGE MRI需要对比剂且有副作用，且2D切片重建空间分辨率有限。

Method: 通过自动深度形状拟合模型biv-me重建4D双心室网格，再设计CMotion2Infarct-Net利用动态几何中的运动模式定位梗死区域。

Result: 在205个MRI扫描中与手动描绘结果一致。

Conclusion: 证明了无对比剂、心脏运动驱动的3D梗死重建的可行性，为MI数字孪生铺路。

Abstract: Accurate representation of myocardial infarct geometry is crucial for
patient-specific cardiac modeling in MI patients. While Late gadolinium
enhancement (LGE) MRI is the clinical gold standard for infarct detection, it
requires contrast agents, introducing side effects and patient discomfort.
Moreover, infarct reconstruction from LGE often relies on sparsely sampled 2D
slices, limiting spatial resolution and accuracy. In this work, we propose a
novel framework for automatically reconstructing high-fidelity 3D myocardial
infarct geometry from 2D clinically standard cine MRI, eliminating the need for
contrast agents. Specifically, we first reconstruct the 4D biventricular mesh
from multi-view cine MRIs via an automatic deep shape fitting model, biv-me.
Then, we design a infarction reconstruction model, CMotion2Infarct-Net, to
explicitly utilize the motion patterns within this dynamic geometry to localize
infarct regions. Evaluated on 205 cine MRI scans from 126 MI patients, our
method shows reasonable agreement with manual delineation. This study
demonstrates the feasibility of contrast-free, cardiac motion-driven 3D infarct
reconstruction, paving the way for efficient digital twin of MI.

</details>


### [174] [Personalized 4D Whole Heart Geometry Reconstruction from Cine MRI for Cardiac Digital Twins](https://arxiv.org/abs/2507.15203)
*Xiaoyue Liu,Xicheng Sheng,Xiahai Zhuang,Vicente Grau,Mark YY Chan,Ching-Hui Sia,Lei Li*

Main category: eess.IV

TL;DR: 提出了一种弱监督学习模型，直接从多视角2D心脏电影MRI重建4D心脏网格，为精准医学提供个性化心脏模型。


<details>
  <summary>Details</summary>
Motivation: 现有全心脏数字孪生模型在模拟四个心腔的电机械活动方面仍有限，需要更高效的方法。

Method: 通过自监督学习将电影MRI与4D心脏网格映射，生成与输入MRI紧密对应的个性化心脏模型。

Result: 生成的4D心脏网格可高分辨率自动提取关键心脏变量，如射血分数和动态心腔容积变化。

Conclusion: 证明了从心脏MRI推断个性化4D心脏模型的可行性，为精准医学的高效数字孪生平台铺平道路。

Abstract: Cardiac digital twins (CDTs) provide personalized in-silico cardiac
representations and hold great potential for precision medicine in cardiology.
However, whole-heart CDT models that simulate the full organ-scale
electromechanics of all four heart chambers remain limited. In this work, we
propose a weakly supervised learning model to reconstruct 4D (3D+t) heart mesh
directly from multi-view 2D cardiac cine MRIs. This is achieved by learning a
self-supervised mapping between cine MRIs and 4D cardiac meshes, enabling the
generation of personalized heart models that closely correspond to input cine
MRIs. The resulting 4D heart meshes can facilitate the automatic extraction of
key cardiac variables, including ejection fraction and dynamic chamber volume
changes with high temporal resolution. It demonstrates the feasibility of
inferring personalized 4D heart models from cardiac MRIs, paving the way for an
efficient CDT platform for precision medicine. The code will be publicly
released once the manuscript is accepted.

</details>


### [175] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: EndoControlMag是一种无需训练的、基于拉格朗日的方法，用于内窥镜手术中血管运动的放大，通过周期性参考重置和分层组织感知放大模块，显著提升了运动放大的准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术中血管运动的可视化对手术精度和决策至关重要，但由于手术场景的复杂性和动态性，这一任务极具挑战性。

Method: EndoControlMag采用周期性参考重置（PRR）和分层组织感知放大（HTM）框架，结合双模式掩模扩张，通过运动或距离自适应的软化策略放大血管运动。

Result: 在EndoVMM24数据集上的实验表明，EndoControlMag在放大准确性和视觉质量上显著优于现有方法，并在各种挑战性手术条件下保持鲁棒性。

Conclusion: EndoControlMag为内窥镜手术中的血管运动可视化提供了一种高效且鲁棒的解决方案，具有广泛的应用潜力。

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [176] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: TVSRN-V2是一种基于Transformer的超分辨率框架，用于低剂量CT体积的高分辨率重建，显著提升了肺部CT分析的临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 高分辨率体积CT对胸部疾病的准确诊断和治疗规划至关重要，但受限于辐射剂量和硬件成本。

Method: TVSRN-V2采用可扩展组件（如Through-Plane Attention Blocks和Swin Transformer V2），并结合伪低分辨率增强以提高鲁棒性。

Result: 在多个临床队列中，TVSRN-V2显著提高了分割准确性（+4% Dice）、放射组学特征的可重复性以及预测性能（+0.06 C-index和AUC）。

Conclusion: TVSRN-V2是一种工程化良好、临床可行的系统，可用于现实世界CT工作流程中的剂量高效成像和定量分析。

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [177] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff结合文本引导的合成数据生成和高效扩散分割，解决医学图像分割数据稀缺问题，提升分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割（如息肉检测）面临数据稀缺问题，标注需要专业知识。

Method: 使用潜在扩散模型通过文本条件修复生成临床真实的合成息肉，结合单步推理加速计算。

Result: 在CVC-ClinicDB上达到96.0% Dice和92.9% IoU，保持实时性。

Conclusion: SynDiff通过合成数据增强提升分割鲁棒性，适用于资源有限的医疗场景。

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [178] [A Steel Surface Defect Detection Method Based on Lightweight Convolution Optimization](https://arxiv.org/abs/2507.15476)
*Cong Chen,Ming Chen,Hoileong Lee,Yan Li,Jiyang Yu*

Main category: eess.IV

TL;DR: 该论文提出了一种基于YOLOv9s的深度学习框架，结合C3Ghost模块、SCConv模块和CARAFE上采样算子，以提高钢表面多尺度缺陷检测的准确性和模型性能。


<details>
  <summary>Details</summary>
Motivation: 钢表面缺陷检测在工业制造中是一个重要挑战，传统方法在复杂环境中对小目标缺陷的检测精度不足且漏检率高。

Method: 通过SCConv模块优化特征表示，C3Ghost模块增强特征提取能力，CARAFE上采样算子精细化重组特征图。

Result: 实验结果表明，该方法在钢表面缺陷检测任务中具有更高的准确性和鲁棒性。

Conclusion: 该研究有效解决了钢表面缺陷检测问题，提升了检测性能。

Abstract: Surface defect detection of steel, especially the recognition of multi-scale
defects, has always been a major challenge in industrial manufacturing. Steel
surfaces not only have defects of various sizes and shapes, which limit the
accuracy of traditional image processing and detection methods in complex
environments. However, traditional defect detection methods face issues of
insufficient accuracy and high miss-detection rates when dealing with small
target defects. To address this issue, this study proposes a detection
framework based on deep learning, specifically YOLOv9s, combined with the
C3Ghost module, SCConv module, and CARAFE upsampling operator, to improve
detection accuracy and model performance. First, the SCConv module is used to
reduce feature redundancy and optimize feature representation by reconstructing
the spatial and channel dimensions. Second, the C3Ghost module is introduced to
enhance the model's feature extraction ability by reducing redundant
computations and parameter volume, thereby improving model efficiency. Finally,
the CARAFE upsampling operator, which can more finely reorganize feature maps
in a content-aware manner, optimizes the upsampling process and ensures
detailed restoration of high-resolution defect regions. Experimental results
demonstrate that the proposed model achieves higher accuracy and robustness in
steel surface defect detection tasks compared to other methods, effectively
addressing defect detection problems.

</details>


### [179] [DeSamba: Decoupled Spectral Adaptive Framework for 3D Multi-Sequence MRI Lesion Classification](https://arxiv.org/abs/2507.15487)
*Dezhen Wang,Sheng Miao,Rongxin Chai,Jiufa Cui*

Main category: eess.IV

TL;DR: DeSamba是一种新型框架，通过解耦表示学习和自适应频谱空间特征融合，显著提升了多序列MRI数据的3D病变分类性能。


<details>
  <summary>Details</summary>
Motivation: 多序列MRI数据在3D病变分类中的有效整合仍具挑战性，需要一种能够自适应融合空间和频谱特征的方法。

Method: DeSamba框架包含解耦表示学习模块（DRLM）和频谱自适应调制块（SAMB），通过自重建和交叉重建解耦特征，并动态融合频谱与空间信息。

Result: 在脊柱转移和脊柱炎数据集上，DeSamba在多项指标上优于现有方法，如Top-1准确率62.10%和AUC 87.71%。

Conclusion: DeSamba是一种通用且有效的解决方案，适用于多序列医学影像中的3D病变分类。

Abstract: Magnetic Resonance Imaging (MRI) sequences provide rich spatial and frequency
domain information, which is crucial for accurate lesion classification in
medical imaging. However, effectively integrating multi-sequence MRI data for
robust 3D lesion classification remains a challenge. In this paper, we propose
DeSamba (Decoupled Spectral Adaptive Network and Mamba-Based Model), a novel
framework designed to extract decoupled representations and adaptively fuse
spatial and spectral features for lesion classification. DeSamba introduces a
Decoupled Representation Learning Module (DRLM) that decouples features from
different MRI sequences through self-reconstruction and cross-reconstruction,
and a Spectral Adaptive Modulation Block (SAMB) within the proposed SAMNet,
enabling dynamic fusion of spectral and spatial information based on lesion
characteristics. We evaluate DeSamba on two clinically relevant 3D datasets. On
a six-class spinal metastasis dataset (n=1,448), DeSamba achieves 62.10% Top-1
accuracy, 63.62% F1-score, 87.71% AUC, and 93.55% Top-3 accuracy on an external
validation set (n=372), outperforming all state-of-the-art (SOTA) baselines. On
a spondylitis dataset (n=251) involving a challenging binary classification
task, DeSamba achieves 70.00%/64.52% accuracy and 74.75/73.88 AUC on internal
and external validation sets, respectively. Ablation studies demonstrate that
both DRLM and SAMB significantly contribute to overall performance, with over
10% relative improvement compared to the baseline. Our results highlight the
potential of DeSamba as a generalizable and effective solution for 3D lesion
classification in multi-sequence medical imaging.

</details>


### [180] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: RARE-UNet是一种分辨率感知的多尺度分割架构，动态适应输入分辨率，在低分辨率数据下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有模型假设固定高分辨率输入，在低分辨率数据下性能下降，限制了实际应用。

Method: 提出RARE-UNet，集成多尺度块、分辨率感知路由机制和一致性驱动训练。

Result: 在脑成像任务中，RARE-UNet的平均Dice分数最高（0.84和0.65），且推理时间显著减少。

Conclusion: RARE-UNet实现了分辨率鲁棒的分割，具有高效性和可扩展性。

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [181] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 提出了一种基于GRU神经网络和迁移学习的边缘流处理自动扩展方法，以应对负载波动和资源分配问题。


<details>
  <summary>Details</summary>
Motivation: 边缘流处理面临快速负载波动，现有方法（如阈值策略和强化学习）存在反应滞后或计算成本高的问题。

Method: 使用GRU预测上游负载，结合迁移学习处理离线与在线数据差异，并通过动态并行度调整实现资源扩展。

Result: GRU模型在真实数据集上SMAPE值低至1.3%，优于CNN、ARIMA和Prophet模型，且训练时间短。

Conclusion: 该方法有效解决了边缘流处理的资源分配问题，兼具高精度和低计算成本。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [182] [Real-Time Scene Reconstruction using Light Field Probes](https://arxiv.org/abs/2507.14624)
*Yaru Liu,Derek Nowrouzezahri,Morgan Mcguire*

Main category: cs.GR

TL;DR: 提出了一种基于探针数据结构的神经渲染方法，用于高效重建大规模复杂场景，避免了显式依赖场景几何，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法在大规模场景中难以平衡场景大小、保真度和渲染速度，而显式几何重建方法则因数据量大而成本高昂。

Method: 利用探针数据结构重建场景的中间多尺度隐式几何表示，避免显式几何依赖，并通过探针数据实现高效渲染。

Result: 该方法能够高效重建复杂场景，渲染成本与场景复杂度无关，适用于VR和AR应用。

Conclusion: 通过探针数据结构，结合几何重建和新视角合成，为大规模场景的神经渲染提供了高效解决方案。

Abstract: Reconstructing photo-realistic large-scale scenes from images, for example at
city scale, is a long-standing problem in computer graphics. Neural rendering
is an emerging technique that enables photo-realistic image synthesis from
previously unobserved viewpoints; however, state-of-the-art neural rendering
methods have difficulty efficiently rendering a high complex large-scale scene
because these methods typically trade scene size, fidelity, and rendering speed
for quality. The other stream of techniques utilizes scene geometries for
reconstruction. But the cost of building and maintaining a large set of
geometry data increases as scene size grows. Our work explores novel view
synthesis methods that efficiently reconstruct complex scenes without explicit
use of scene geometries. Specifically, given sparse images of the scene
(captured from the real world), we reconstruct intermediate, multi-scale,
implicit representations of scene geometries. In this way, our method avoids
explicitly relying on scene geometry, significantly reducing the computational
cost of maintaining large 3D data. Unlike current methods, we reconstruct the
scene using a probe data structure. Probe data hold highly accurate depth
information of dense data points, enabling the reconstruction of highly complex
scenes. By reconstructing the scene using probe data, the rendering cost is
independent of the complexity of the scene. As such, our approach combines
geometry reconstruction and novel view synthesis. Moreover, when rendering
large-scale scenes, compressing and streaming probe data is more efficient than
using explicit scene geometry. Therefore, our neural representation approach
can potentially be applied to virtual reality (VR) and augmented reality (AR)
applications.

</details>


### [183] [Towards Geometric and Textural Consistency 3D Scene Generation via Single Image-guided Model Generation and Layout Optimization](https://arxiv.org/abs/2507.14841)
*Xiang Tang,Ruotong Li,Xiaopeng Fan*

Main category: cs.GR

TL;DR: 提出了一种三阶段框架，通过单图像引导生成和空间布局优化，解决从单RGB图像生成3D场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前方法在多对象场景中难以同时保证生成质量和场景一致性，需改进。

Method: 1. 图像实例分割和修复；2. 伪立体视角构建；3. 模型参数化和布局优化。

Result: 在几何精度和纹理保真度上优于现有方法，且在场景布局合成中表现突出。

Conclusion: 该方法显著提升了3D场景生成的完整性和一致性。

Abstract: In recent years, 3D generation has made great strides in both academia and
industry. However, generating 3D scenes from a single RGB image remains a
significant challenge, as current approaches often struggle to ensure both
object generation quality and scene coherence in multi-object scenarios. To
overcome these limitations, we propose a novel three-stage framework for 3D
scene generation with explicit geometric representations and high-quality
textural details via single image-guided model generation and spatial layout
optimization. Our method begins with an image instance segmentation and
inpainting phase, which recovers missing details of occluded objects in the
input images, thereby achieving complete generation of foreground 3D assets.
Subsequently, our approach captures the spatial geometry of reference image by
constructing pseudo-stereo viewpoint for camera parameter estimation and scene
depth inference, while employing a model selection strategy to ensure optimal
alignment between the 3D assets generated in the previous step and the input.
Finally, through model parameterization and minimization of the Chamfer
distance between point clouds in 3D and 2D space, our approach optimizes layout
parameters to produce an explicit 3D scene representation that maintains
precise alignment with input guidance image. Extensive experiments on
multi-object scene image sets have demonstrated that our approach not only
outperforms state-of-the-art methods in terms of geometric accuracy and texture
fidelity of individual generated 3D models, but also has significant advantages
in scene layout synthesis.

</details>


### [184] [Blended Point Cloud Diffusion for Localized Text-guided Shape Editing](https://arxiv.org/abs/2507.15399)
*Etai Sella,Noam Atia,Ron Mokady,Hadar Averbuch-Elor*

Main category: cs.GR

TL;DR: 提出了一种基于修复的点云编辑框架，结合3D扩散模型和坐标混合算法，实现局部精细编辑并保持全局一致性。


<details>
  <summary>Details</summary>
Motivation: 自然语言为3D形状的局部精细编辑提供了直观接口，但现有方法难以在局部修改时保持全局一致性。

Method: 利用基础3D扩散模型进行局部形状编辑，通过部分条件形状提供结构指导，并提出推理时坐标混合算法平衡重建与修复。

Result: 实验表明，该方法在保真度和文本描述一致性上优于其他技术。

Conclusion: 该方法无需昂贵且不准确的反演，实现了精细编辑并保持形状身份。

Abstract: Natural language offers a highly intuitive interface for enabling localized
fine-grained edits of 3D shapes. However, prior works face challenges in
preserving global coherence while locally modifying the input 3D shape. In this
work, we introduce an inpainting-based framework for editing shapes represented
as point clouds. Our approach leverages foundation 3D diffusion models for
achieving localized shape edits, adding structural guidance in the form of a
partial conditional shape, ensuring that other regions correctly preserve the
shape's identity. Furthermore, to encourage identity preservation also within
the local edited region, we propose an inference-time coordinate blending
algorithm which balances reconstruction of the full shape with inpainting at a
progression of noise levels during the inference process. Our coordinate
blending algorithm seamlessly blends the original shape with its edited
version, enabling a fine-grained editing of 3D shapes, all while circumventing
the need for computationally expensive and often inaccurate inversion.
Extensive experiments show that our method outperforms alternative techniques
across a wide range of metrics that evaluate both fidelity to the original
shape and also adherence to the textual description.

</details>


### [185] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个对象感知框架，将3D场景重建与语义理解结合，通过局部锚点和对象ID实现精确的对象级重建。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting缺乏语义理解，限制了对象级感知能力。

Method: ObjectGS将场景中的对象建模为局部锚点，生成神经高斯并共享对象ID，动态调整锚点并优化特征，使用分类损失强化语义约束。

Result: ObjectGS在开放词汇和全景分割任务中优于现有方法，并能无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS通过语义增强的3D重建，显著提升了对象级感知能力，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


### [186] [Gaussian Splatting with Discretized SDF for Relightable Assets](https://arxiv.org/abs/2507.15629)
*Zuo-Liang Zhu,Jian Yang,Beibei Wang*

Main category: cs.GR

TL;DR: 3D高斯泼溅（3DGS）在逆渲染任务中通过离散化SDF提升几何约束，实现高质量重光照效果。


<details>
  <summary>Details</summary>
Motivation: 解决3DGS在逆渲染中因离散性难以应用几何约束的问题。

Method: 引入离散化SDF，通过采样值编码到高斯中，结合投影一致性损失优化。

Result: 实验表明方法优于现有高斯逆渲染方法，且无需额外内存。

Conclusion: 离散化SDF有效提升逆渲染质量，简化优化过程。

Abstract: 3D Gaussian splatting (3DGS) has shown its detailed expressive ability and
highly efficient rendering speed in the novel view synthesis (NVS) task. The
application to inverse rendering still faces several challenges, as the
discrete nature of Gaussian primitives makes it difficult to apply geometry
constraints. Recent works introduce the signed distance field (SDF) as an extra
continuous representation to regularize the geometry defined by Gaussian
primitives. It improves the decomposition quality, at the cost of increasing
memory usage and complicating training. Unlike these works, we introduce a
discretized SDF to represent the continuous SDF in a discrete manner by
encoding it within each Gaussian using a sampled value. This approach allows us
to link the SDF with the Gaussian opacity through an SDF-to-opacity
transformation, enabling rendering the SDF via splatting and avoiding the
computational cost of ray marching.The key challenge is to regularize the
discrete samples to be consistent with the underlying SDF, as the discrete
representation can hardly apply the gradient-based constraints (\eg Eikonal
loss). For this, we project Gaussians onto the zero-level set of SDF and
enforce alignment with the surface from splatting, namely a projection-based
consistency loss. Thanks to the discretized SDF, our method achieves higher
relighting quality, while requiring no extra memory beyond GS and avoiding
complex manually designed optimization. The experiments reveal that our method
outperforms existing Gaussian-based inverse rendering methods. Our code is
available at https://github.com/NK-CS-ZZL/DiscretizedSDF.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [187] [Self-Supervised Distillation of Legacy Rule-Based Methods for Enhanced EEG-Based Decision-Making](https://arxiv.org/abs/2507.14542)
*Yipeng Zhang,Yuanyi Ding,Chenda Duan,Atsuro Daida,Hiroki Nariai,Vwani Roychowdhury*

Main category: cs.CE

TL;DR: SS2LD框架利用自监督学习改进传统HFO检测器，通过变分自编码器和聚类生成弱监督，提升病理HFO检测精度。


<details>
  <summary>Details</summary>
Motivation: 传统HFO检测器精度低且依赖专家标注，标注困难且不一致，需改进方法。

Method: 使用变分自编码器预训练和聚类生成弱监督，结合分类器优化检测边界。

Result: 在多机构数据集上表现优于现有方法。

Conclusion: SS2LD提供高效、可扩展的病理HFO检测策略。

Abstract: High-frequency oscillations (HFOs) in intracranial Electroencephalography
(iEEG) are critical biomarkers for localizing the epileptogenic zone in
epilepsy treatment. However, traditional rule-based detectors for HFOs suffer
from unsatisfactory precision, producing false positives that require
time-consuming manual review. Supervised machine learning approaches have been
used to classify the detection results, yet they typically depend on labeled
datasets, which are difficult to acquire due to the need for specialized
expertise. Moreover, accurate labeling of HFOs is challenging due to low
inter-rater reliability and inconsistent annotation practices across
institutions. The lack of a clear consensus on what constitutes a pathological
HFO further challenges supervised refinement approaches. To address this, we
leverage the insight that legacy detectors reliably capture clinically relevant
signals despite their relatively high false positive rates. We thus propose the
Self-Supervised to Label Discovery (SS2LD) framework to refine the large set of
candidate events generated by legacy detectors into a precise set of
pathological HFOs. SS2LD employs a variational autoencoder (VAE) for
morphological pre-training to learn meaningful latent representation of the
detected events. These representations are clustered to derive weak supervision
for pathological events. A classifier then uses this supervision to refine
detection boundaries, trained on real and VAE-augmented data. Evaluated on
large multi-institutional interictal iEEG datasets, SS2LD outperforms
state-of-the-art methods. SS2LD offers a scalable, label-efficient, and
clinically effective strategy to identify pathological HFOs using legacy
detectors.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [188] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: 本文提出了一种便携式、边缘支持的电子健康记录平台，针对资源有限环境优化，支持离线操作、安全数据管理和模块化诊断集成。


<details>
  <summary>Details</summary>
Motivation: 现有数字健康解决方案在资源有限环境中存在互操作性差、缺乏离线支持和依赖昂贵基础设施的问题，限制了其在欠发达地区的有效性。

Method: 平台基于小型嵌入式设备，提供AES-256加密本地存储和可选云同步，并集成了非侵入性贫血筛查模块（使用随机森林模型和YOLOv8n量化技术）。

Result: 贫血筛查模块在测试中表现良好（RMSE为1.969 g/dL，MAE为1.490 g/dL），量化后的YOLOv8n检测器延迟降低至21.50 ms，同时保持高精度。

Conclusion: 该系统展示了在资源有限环境中增强便携式健康信息系统的可扩展方法，支持欠发达地区的前线医疗保健。

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [189] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: AdViT攻击方法能同时欺骗视觉Transformer模型及其解释模型，实验显示在黑白盒场景下均高效。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型在结合解释模型时仍存在的对抗攻击漏洞。

Method: 提出AdViT攻击方法，生成能同时误导模型和解释模型的对抗样本。

Result: 在多种Transformer模型和解释模型上，AdViT攻击成功率达100%，且生成准确的解释。

Conclusion: AdViT揭示了Transformer模型在安全关键领域的潜在风险，需进一步防御研究。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>
