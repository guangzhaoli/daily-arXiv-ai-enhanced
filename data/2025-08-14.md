<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 122]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 11]
- [cs.CR](#cs.CR) [Total: 1]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.IR](#cs.IR) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 7]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A Context-aware Attention and Graph Neural Network-based Multimodal Framework for Misogyny Detection](https://arxiv.org/abs/2508.09175)
*Mohammad Zia Ur Rehman,Sufyaan Zahoor,Areeb Manzoor,Musharaf Maqbool,Nagendra Kumar*

Main category: cs.CV

TL;DR: 提出了一种多模态框架，用于检测社交媒体上的厌女和性别歧视内容，通过三个模块（MANM、GFRM、CFLM）提升检测效果，并在两个数据集上验证了性能提升。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上针对女性的攻击性内容较多，现有方法难以有效检测此类内容，需要针对性解决方案。

Method: 采用多模态框架，包括注意力模块（MANM）、图特征重构模块（GFRM）和内容特定特征学习模块（CFLM），并结合词汇表评分和测试时增强技术。

Result: 在MAMI和MMHS150K数据集上，平均宏F1分数分别提高了10.17%和8.88%。

Conclusion: 提出的多模态框架显著提升了厌女和性别歧视内容的检测效果，为社交媒体内容审核提供了有效工具。

Abstract: A substantial portion of offensive content on social media is directed
towards women. Since the approaches for general offensive content detection
face a challenge in detecting misogynistic content, it requires solutions
tailored to address offensive content against women. To this end, we propose a
novel multimodal framework for the detection of misogynistic and sexist
content. The framework comprises three modules: the Multimodal Attention module
(MANM), the Graph-based Feature Reconstruction Module (GFRM), and the
Content-specific Features Learning Module (CFLM). The MANM employs adaptive
gating-based multimodal context-aware attention, enabling the model to focus on
relevant visual and textual information and generating contextually relevant
features. The GFRM module utilizes graphs to refine features within individual
modalities, while the CFLM focuses on learning text and image-specific features
such as toxicity features and caption features. Additionally, we curate a set
of misogynous lexicons to compute the misogyny-specific lexicon score from the
text. We apply test-time augmentation in feature space to better generalize the
predictions on diverse inputs. The performance of the proposed approach has
been evaluated on two multimodal datasets, MAMI and MMHS150K, with 11,000 and
13,494 samples, respectively. The proposed method demonstrates an average
improvement of 10.17% and 8.88% in macro-F1 over existing methods on the MAMI
and MMHS150K datasets, respectively.

</details>


### [2] [IAD-R1: Reinforcing Consistent Reasoning in Industrial Anomaly Detection](https://arxiv.org/abs/2508.09178)
*Yanhui Li,Yunkang Cao,Chengliang Liu,Yuan Xiong,Xinghui Dong,Chao Huang*

Main category: cs.CV

TL;DR: IAD-R1是一个通用的后训练框架，显著提升了不同架构和参数规模的视觉语言模型（VLMs）在工业异常检测中的性能。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中缺陷样本稀缺，传统方法受限，而现有VLMs的性能不足。

Method: 采用两阶段训练策略：PA-SFT阶段使用高质量Chain-of-Thought数据集增强异常感知能力；SC-GRPO阶段通过奖励函数实现从异常感知到异常解释的飞跃。

Result: 在7个VLMs和6个基准数据集上，平均准确率提升高达43.3%，0.5B参数模型在零样本设置下超越GPT-4.1等商业模型。

Conclusion: IAD-R1展示了在工业异常检测中的有效性和优越性，相关资源将开源。

Abstract: Industrial anomaly detection is a critical component of modern manufacturing,
yet the scarcity of defective samples restricts traditional detection methods
to scenario-specific applications. Although Vision-Language Models (VLMs)
demonstrate significant advantages in generalization capabilities, their
performance in industrial anomaly detection remains limited. To address this
challenge, we propose IAD-R1, a universal post-training framework applicable to
VLMs of different architectures and parameter scales, which substantially
enhances their anomaly detection capabilities. IAD-R1 employs a two-stage
training strategy: the Perception Activation Supervised Fine-Tuning (PA-SFT)
stage utilizes a meticulously constructed high-quality Chain-of-Thought dataset
(Expert-AD) for training, enhancing anomaly perception capabilities and
establishing reasoning-to-answer correlations; the Structured Control Group
Relative Policy Optimization (SC-GRPO) stage employs carefully designed reward
functions to achieve a capability leap from "Anomaly Perception" to "Anomaly
Interpretation". Experimental results demonstrate that IAD-R1 achieves
significant improvements across 7 VLMs, attaining up to 43.3% enhancement in
average accuracy on 6 industrial anomaly detection benchmark datasets. Notably,
the 0.5B parameter model trained with IAD-R1 surpasses commercial models
including GPT-4.1 and Claude-Sonnet-4 in zero-shot settings, demonstrating the
effectiveness and superiority of IAD-R1. The dataset, code, and all model
weights will be publicly available at https://github.com/Yanhui-Lee/IAD-R1.

</details>


### [3] [A Neurosymbolic Framework for Interpretable Cognitive Attack Detection in Augmented Reality](https://arxiv.org/abs/2508.09185)
*Rongqian Chen,Allison Andreyev,Yanming Xiu,Mahdi Imani,Bin Li,Maria Gorlatova,Gang Tan,Tian Lan*

Main category: cs.CV

TL;DR: CADAR是一种新型的神经符号方法，用于检测AR中的认知攻击，结合了视觉语言模型和粒子滤波技术，提高了检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉变化检测或依赖预训练视觉语言模型时存在局限性，缺乏语义推理能力或可解释性。

Method: CADAR融合多模态视觉语言输入，生成符号感知图，并利用粒子滤波进行统计推理。

Result: 在扩展的AR认知攻击数据集上，CADAR比基线方法准确率提高了10.7%。

Conclusion: 神经符号方法在AR认知攻击检测中表现出高效性和可解释性，具有广阔应用前景。

Abstract: Augmented Reality (AR) enriches perception by overlaying virtual elements on
the physical world. Due to its growing popularity, cognitive attacks that alter
AR content to manipulate users' semantic perception have received increasing
attention. Existing detection methods often focus on visual changes, which are
restricted to pixel- or image-level processing and lack semantic reasoning
capabilities, or they rely on pre-trained vision-language models (VLMs), which
function as black-box approaches with limited interpretability. In this paper,
we present CADAR, a novel neurosymbolic approach for cognitive attack detection
in AR. It fuses multimodal vision-language inputs using neural VLMs to obtain a
symbolic perception-graph representation, incorporating prior knowledge,
salience weighting, and temporal correlations. The model then enables
particle-filter based statistical reasoning -- a sequential Monte Carlo method
-- to detect cognitive attacks. Thus, CADAR inherits the adaptability of
pre-trained VLM and the interpretability and reasoning rigor of particle
filtering. Experiments on an extended AR cognitive attack dataset show accuracy
improvements of up to 10.7% over strong baselines on challenging AR attack
scenarios, underscoring the promise of neurosymbolic methods for effective and
interpretable cognitive attack detection.

</details>


### [4] [RL-MoE: An Image-Based Privacy Preserving Approach In Intelligent Transportation System](https://arxiv.org/abs/2508.09186)
*Abdolazim Rezaei,Mehdi Sookhak,Mahboobeh Haghparast*

Main category: cs.CV

TL;DR: RL-MoE框架通过将敏感视觉数据转换为隐私保护的文本描述，解决了智能交通系统中隐私与数据效用之间的冲突。


<details>
  <summary>Details</summary>
Motivation: 智能交通系统中AI摄像头的普及导致隐私与数据需求之间的冲突，现有隐私保护机制（如模糊或加密）效果不佳。

Method: 结合Mixture-of-Experts（MoE）架构和强化学习（RL）代理，将视觉数据转换为语义准确且隐私保护的文本描述。

Result: 在CFP-FP数据集上，RL-MoE将重放攻击成功率降至9.4%，同时生成比基线方法更丰富的文本内容。

Conclusion: RL-MoE为隐私敏感领域提供了实用且可扩展的解决方案，推动了更安全的智能城市和自动驾驶网络的发展。

Abstract: The proliferation of AI-powered cameras in Intelligent Transportation Systems
(ITS) creates a severe conflict between the need for rich visual data and the
fundamental right to privacy. Existing privacy-preserving mechanisms, such as
blurring or encryption, are often insufficient, creating an undesirable
trade-off where either privacy is compromised against advanced reconstruction
attacks or data utility is critically degraded. To resolve this impasse, we
propose RL-MoE, a novel framework that transforms sensitive visual data into
privacy-preserving textual descriptions, eliminating the need for direct image
transmission. RL-MoE uniquely combines a Mixture-of-Experts (MoE) architecture
for nuanced, multi-aspect scene decomposition with a Reinforcement Learning
(RL) agent that optimizes the generated text for a dual objective of semantic
accuracy and privacy preservation. Extensive experiments demonstrate that
RL-MoE provides superior privacy protection, reducing the success rate of
replay attacks to just 9.4\% on the CFP-FP dataset, while simultaneously
generating richer textual content than baseline methods. Our work provides a
practical and scalable solution for building trustworthy AI systems in
privacy-sensitive domains, paving the way for more secure smart city and
autonomous vehicle networks.

</details>


### [5] [Synthetic Data Generation for Emotional Depth Faces: Optimizing Conditional DCGANs via Genetic Algorithms in the Latent Space and Stabilizing Training with Knowledge Distillation](https://arxiv.org/abs/2508.09188)
*Seyed Muhammad Hossein Mousavi,S. Younes Mirinezhad*

Main category: cs.CV

TL;DR: 提出了一种基于GAN和知识蒸馏的合成深度人脸生成框架，结合遗传算法提升多样性和质量，在情感分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决情感计算中高质量、多样化深度人脸数据集缺乏的问题。

Method: 使用优化的GAN和知识蒸馏（EMA教师模型）稳定训练，结合遗传算法优化潜在向量，提取多种特征进行分类。

Result: 在多样性和质量上优于GAN、VAE、GMM和KDE，分类准确率达94%和96%。

Conclusion: 该方法在生成和分类任务中均优于现有技术，为情感计算提供了有效解决方案。

Abstract: Affective computing faces a major challenge: the lack of high-quality,
diverse depth facial datasets for recognizing subtle emotional expressions. We
propose a framework for synthetic depth face generation using an optimized GAN
with Knowledge Distillation (EMA teacher models) to stabilize training, improve
quality, and prevent mode collapse. We also apply Genetic Algorithms to evolve
GAN latent vectors based on image statistics, boosting diversity and visual
quality for target emotions. The approach outperforms GAN, VAE, GMM, and KDE in
both diversity and quality. For classification, we extract and concatenate LBP,
HOG, Sobel edge, and intensity histogram features, achieving 94% and 96%
accuracy with XGBoost. Evaluation using FID, IS, SSIM, and PSNR shows
consistent improvement over state-of-the-art methods.

</details>


### [6] [$Δ$-AttnMask: Attention-Guided Masked Hidden States for Efficient Data Selection and Augmentation](https://arxiv.org/abs/2508.09199)
*Jucheng Hu,Suorong Yang,Dongzhan Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种名为Δ-AttnMask的数据高效框架，通过注意力引导的隐藏状态掩码量化样本质量，显著提升了视觉指令微调的性能。


<details>
  <summary>Details</summary>
Motivation: 视觉指令微调（VIF）需要多模态数据以实现视觉和文本的联合理解，但数据选择问题尚未充分研究。

Method: 提出Δ-AttnMask框架，通过计算原始状态与高注意力区域掩码状态之间的损失差异来评估样本质量。

Result: 实验表明，Δ-AttnMask仅需20%的数据即可实现最先进性能，训练速度提升5倍，整体准确率提高10.1%。

Conclusion: Δ-AttnMask的模型无关和数据无关设计使其在多模态和架构中具有广泛适用性。

Abstract: Visual Instruction Finetuning (VIF) is pivotal for post-training
Vision-Language Models (VLMs). Unlike unimodal instruction finetuning in
plain-text large language models, which mainly requires instruction datasets to
enable model instruction-following ability, VIF also requires multimodal data
to enable joint visual and textual understanding; therefore, it typically
requires more data. Consequently, VIF imposes stricter data selection
challenges: the method must scale efficiently to handle larger data demands
while ensuring the quality of both visual and textual content, as well as their
alignment. Despite its critical impact on performance, data selection for VIF
remains an understudied area. In this paper, we propose $\Delta$-AttnMask. This
data-efficient framework quantifies sample quality through attention-guided
masking of the model's hidden states, jointly evaluating image-text pairs
without requiring domain labels, auxiliary models, or extra training. By
computing loss differences ($\Delta$) between the original states and states
masked using high-attention regions, $\Delta$-AttnMask intrinsically assesses
sample quality. Experiments across multiple VLMs and datasets show that
$\Delta$-AttnMask achieves state-of-the-art performance with just 20% of data,
accelerating training by 5x while surpassing full-dataset baselines by +10.1%
in overall accuracy. Its model-agnostic and data-agnostic design ensures broad
applicability across modalities and architectures.

</details>


### [7] [Personalized Feature Translation for Expression Recognition: An Efficient Source-Free Domain Adaptation Method](https://arxiv.org/abs/2508.09202)
*Masoumeh Sharafi,Soufiane Belharbi,Houssem Ben Salem,Ali Etemad,Alessandro Lameiras Koerich,Marco Pedersoli,Simon Bacon,Eric Granger*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级的个性化特征翻译（PFT）方法，用于源自由域适应（SFDA），以解决仅使用中性表情的目标数据时的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的SFDA方法通常不适用于仅使用单一类别的目标数据，且生成非中性表情的图像可能不稳定且计算量大。

Method: PFT在潜在空间中操作，通过预训练翻译器在源域数据上转换特定主体的风格特征，并在中性目标数据上进行适应，无需源数据或图像合成。

Result: PFT避免了面部表情生成的复杂性和噪声，生成了优化的分类嵌入，减少了计算开销，并仅适应模型的一部分。

Conclusion: PFT是一种高效的方法，适用于仅使用中性表情的目标数据的SFDA场景。

Abstract: Facial expression recognition (FER) models are employed in many video-based
affective computing applications, such as human-computer interaction and
healthcare monitoring. However, deep FER models often struggle with subtle
expressions and high inter-subject variability, limiting their performance in
real-world applications. To improve their performance, source-free domain
adaptation (SFDA) methods have been proposed to personalize a pretrained source
model using only unlabeled target domain data, thereby avoiding data privacy,
storage, and transmission constraints. This paper addresses a challenging
scenario where source data is unavailable for adaptation, and only unlabeled
target data consisting solely of neutral expressions is available. SFDA methods
are not typically designed to adapt using target data from only a single class.
Further, using models to generate facial images with non-neutral expressions
can be unstable and computationally intensive. In this paper, personalized
feature translation (PFT) is proposed for SFDA. Unlike current image
translation methods for SFDA, our lightweight method operates in the latent
space. We first pre-train the translator on the source domain data to transform
the subject-specific style features from one source subject into another.
Expression information is preserved by optimizing a combination of expression
consistency and style-aware objectives. Then, the translator is adapted on
neutral target data, without using source data or image synthesis. By
translating in the latent space, PFT avoids the complexity and noise of face
expression generation, producing discriminative embeddings optimized for
classification. Using PFT eliminates the need for image synthesis, reduces
computational overhead (using a lightweight translator), and only adapts part
of the model, making the method efficient compared to image-based translation.

</details>


### [8] [GANime: Generating Anime and Manga Character Drawings from Sketches with Deep Learning](https://arxiv.org/abs/2508.09207)
*Tai Vu,Robert Yang*

Main category: cs.CV

TL;DR: 研究比较了多种图像转换模型，发现C-GAN在动漫角色草图着色任务中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决动漫行业中草图着色成本高的问题。

Method: 评估了Neural Style Transfer、C-GAN和CycleGAN等模型。

Result: C-GAN能生成接近人类创作的高质量、高分辨率图像。

Conclusion: C-GAN是动漫草图着色任务中最有效的模型。

Abstract: The process of generating fully colorized drawings from sketches is a large,
usually costly bottleneck in the manga and anime industry. In this study, we
examine multiple models for image-to-image translation between anime characters
and their sketches, including Neural Style Transfer, C-GAN, and CycleGAN. By
assessing them qualitatively and quantitatively, we find that C-GAN is the most
effective model that is able to produce high-quality and high-resolution images
close to those created by humans.

</details>


### [9] [MME-Emotion: A Holistic Evaluation Benchmark for Emotional Intelligence in Multimodal Large Language Models](https://arxiv.org/abs/2508.09210)
*Fan Zhang,Zebang Cheng,Chong Deng,Haoxuan Li,Zheng Lian,Qian Chen,Huadai Liu,Wen Wang,Yi-Fan Zhang,Renrui Zhang,Ziyu Guo,Zhihong Zhu,Hao Wu,Haixin Wang,Yefeng Zheng,Xiaojiang Peng,Xian Wu,Kun Wang,Xiangang Li,Jieping Ye,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: MME-Emotion是一个系统性基准测试，用于评估多模态大语言模型（MLLMs）的情感情感和推理能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前情感基准测试在泛化能力和推理能力方面存在不足，需要更全面的评估工具。

Method: 提出了MME-Emotion基准，包含6000多个视频片段和任务特定的问答对，涵盖八种情感任务，并采用混合指标和多代理系统框架进行评估。

Result: 评估了20个先进MLLMs，发现其情感情感表现不佳，最佳模型识别得分仅为39.3%，推理得分为56.0%。

Conclusion: MME-Emotion为未来提升MLLMs的情感情感能力提供了基础。

Abstract: Recent advances in multimodal large language models (MLLMs) have catalyzed
transformative progress in affective computing, enabling models to exhibit
emergent emotional intelligence. Despite substantial methodological progress,
current emotional benchmarks remain limited, as it is still unknown: (a) the
generalization abilities of MLLMs across distinct scenarios, and (b) their
reasoning capabilities to identify the triggering factors behind emotional
states. To bridge these gaps, we present \textbf{MME-Emotion}, a systematic
benchmark that assesses both emotional understanding and reasoning capabilities
of MLLMs, enjoying \textit{scalable capacity}, \textit{diverse settings}, and
\textit{unified protocols}. As the largest emotional intelligence benchmark for
MLLMs, MME-Emotion contains over 6,000 curated video clips with task-specific
questioning-answering (QA) pairs, spanning broad scenarios to formulate eight
emotional tasks. It further incorporates a holistic evaluation suite with
hybrid metrics for emotion recognition and reasoning, analyzed through a
multi-agent system framework. Through a rigorous evaluation of 20 advanced
MLLMs, we uncover both their strengths and limitations, yielding several key
insights: \ding{182} Current MLLMs exhibit unsatisfactory emotional
intelligence, with the best-performing model achieving only $39.3\%$
recognition score and $56.0\%$ Chain-of-Thought (CoT) score on our benchmark.
\ding{183} Generalist models (\emph{e.g.}, Gemini-2.5-Pro) derive emotional
intelligence from generalized multimodal understanding capabilities, while
specialist models (\emph{e.g.}, R1-Omni) can achieve comparable performance
through domain-specific post-training adaptation. By introducing MME-Emotion,
we hope that it can serve as a foundation for advancing MLLMs' emotional
intelligence in the future.

</details>


### [10] [Towards Effective MLLM Jailbreaking Through Balanced On-Topicness and OOD-Intensity](https://arxiv.org/abs/2508.09218)
*Zuoou Li,Weitong Zhang,Jingyuan Wang,Shuyuan Zhang,Wenjia Bai,Bernhard Kainz,Mengyun Qiao*

Main category: cs.CV

TL;DR: 论文提出了一种四轴评估框架和改进的递归重写策略（BSD），用于更准确地评估和提升对抗性提示在多模态大语言模型中的攻击效果。


<details>
  <summary>Details</summary>
Motivation: 现有评估标准可能高估了对抗性提示的成功率，许多被标记为“成功”的回应实际上是良性的或无关的，因此需要更精确的评估方法。

Method: 引入四轴评估框架（输入主题性、输入OOD强度、输出危害性、输出拒绝率），并提出BSD策略，通过语义对齐的子任务和OOD信号优化提示。

Result: BSD在13个商业和开源MLLMs中显著提高了攻击成功率和危害性，分别提升了67%和21%。

Conclusion: 当前多模态安全系统存在未被充分认识的弱点，BSD策略揭示了平衡相关性和新颖性的提示更易绕过过滤器并触发有害输出。

Abstract: Multimodal large language models (MLLMs) are widely used in vision-language
reasoning tasks. However, their vulnerability to adversarial prompts remains a
serious concern, as safety mechanisms often fail to prevent the generation of
harmful outputs. Although recent jailbreak strategies report high success
rates, many responses classified as "successful" are actually benign, vague, or
unrelated to the intended malicious goal. This mismatch suggests that current
evaluation standards may overestimate the effectiveness of such attacks. To
address this issue, we introduce a four-axis evaluation framework that
considers input on-topicness, input out-of-distribution (OOD) intensity, output
harmfulness, and output refusal rate. This framework identifies truly effective
jailbreaks. In a substantial empirical study, we reveal a structural trade-off:
highly on-topic prompts are frequently blocked by safety filters, whereas those
that are too OOD often evade detection but fail to produce harmful content.
However, prompts that balance relevance and novelty are more likely to evade
filters and trigger dangerous output. Building on this insight, we develop a
recursive rewriting strategy called Balanced Structural Decomposition (BSD).
The approach restructures malicious prompts into semantically aligned
sub-tasks, while introducing subtle OOD signals and visual cues that make the
inputs harder to detect. BSD was tested across 13 commercial and open-source
MLLMs, where it consistently led to higher attack success rates, more harmful
outputs, and fewer refusals. Compared to previous methods, it improves success
rates by $67\%$ and harmfulness by $21\%$, revealing a previously
underappreciated weakness in current multimodal safety systems.

</details>


### [11] [Towards Scalable Training for Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.09220)
*Haoyang Li,Jiaqing Li,Jialun Cao,Zongyuan Yang,Yongping Xiong*

Main category: cs.CV

TL;DR: 提出了一种结合手写公式和大规模LaTeX渲染公式的新方法，构建了最大的公式数据集Tex80M，并训练了首个大规模HMER模型TexTeller，取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: HMER领域因数据稀缺而受限，主要由于手动标注成本高昂。

Method: 开发可扩展的数据引擎生成复杂且一致的LaTeX序列，结合Tex80M和少量HME数据集进行混合训练。

Result: 构建了包含8000万高质量训练样本的Tex80M数据集，TexTeller模型在几乎所有基准测试中达到SOTA性能。

Conclusion: 公开模型、数据集和代码库，推动HMER领域的进一步研究。

Abstract: Large foundation models have achieved significant performance gains through
scalable training on massive datasets. However, the field of
\textbf{H}andwritten \textbf{M}athematical \textbf{E}xpression
\textbf{R}ecognition (HMER) has been impeded by the scarcity of data, primarily
due to the arduous and costly process of manual annotation. To bridge this gap,
we propose a novel method integrating limited handwritten formulas with
large-scale LaTeX-rendered formulas by developing a scalable data engine to
generate complex and consistent LaTeX sequences. With this engine, we built the
largest formula dataset to date, termed \texttt{Tex80M}, comprising over 80
million high-quality training instances. Then we propose \texttt{TexTeller},
the first HMER model trained at scale, by mix-training \texttt{Tex80M} with a
relatively small HME dataset. The expansive training dataset and our refined
pipeline have equipped \texttt{TexTeller} with state-of-the-art (SOTA)
performance across nearly all benchmarks. To advance the field, we will openly
release our complete model, entire dataset, and full codebase, enabling further
research building upon our contributions.

</details>


### [12] [Gradient-Direction-Aware Density Control for 3D Gaussian Splatting](https://arxiv.org/abs/2508.09239)
*Zheng Zhou,Yu-Jie Xiong,Chun-Ming Xia,Jia-Chen Zhang,Hong-Jian Zhan*

Main category: cs.CV

TL;DR: GDAGS提出了一种基于梯度方向感知的自适应密度控制框架，解决了3DGS在复杂场景中的过重建和过密集问题，显著提升了渲染质量并减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在复杂场景中存在过重建和过密集问题，导致内存开销增加和渲染质量下降。

Method: GDAGS通过梯度一致性比率（GCR）和非线性动态加权机制，实现梯度方向感知的密度控制。

Result: GDAGS在多种真实场景基准测试中表现出色，渲染质量更高，内存消耗减少50%。

Conclusion: GDAGS通过优化高斯分布，有效解决了3DGS的局限性，为实时渲染提供了更高效的解决方案。

Abstract: The emergence of 3D Gaussian Splatting (3DGS) has significantly advanced
novel view synthesis through explicit scene representation, enabling real-time
photorealistic rendering. However, existing approaches manifest two critical
limitations in complex scenarios: (1) Over-reconstruction occurs when
persistent large Gaussians cannot meet adaptive splitting thresholds during
density control. This is exacerbated by conflicting gradient directions that
prevent effective splitting of these Gaussians; (2) Over-densification of
Gaussians occurs in regions with aligned gradient aggregation, leading to
redundant component proliferation. This redundancy significantly increases
memory overhead due to unnecessary data retention. We present
Gradient-Direction-Aware Gaussian Splatting (GDAGS), a gradient-direction-aware
adaptive density control framework to address these challenges. Our key
innovations: the gradient coherence ratio (GCR), computed through normalized
gradient vector norms, which explicitly discriminates Gaussians with concordant
versus conflicting gradient directions; and a nonlinear dynamic weighting
mechanism leverages the GCR to enable gradient-direction-aware density control.
Specifically, GDAGS prioritizes conflicting-gradient Gaussians during splitting
operations to enhance geometric details while suppressing redundant
concordant-direction Gaussians. Conversely, in cloning processes, GDAGS
promotes concordant-direction Gaussian densification for structural completion
while preventing conflicting-direction Gaussian overpopulation. Comprehensive
evaluations across diverse real-world benchmarks demonstrate that GDAGS
achieves superior rendering quality while effectively mitigating
over-reconstruction, suppressing over-densification, and constructing compact
scene representations with 50\% reduced memory consumption through optimized
Gaussians utilization.

</details>


### [13] [FineState-Bench: A Comprehensive Benchmark for Fine-Grained State Control in GUI Agents](https://arxiv.org/abs/2508.09241)
*Fengxian Ji,Jingpu Yang,Zirui Song,Yuanxi Wang,Zhexuan Cui,Yuke Li,Qian Jiang,Miao Fang,Xiuying Chen*

Main category: cs.CV

TL;DR: FineState-Bench是首个用于评估GUI代理细粒度控制能力的标准，通过多平台任务基准和视觉诊断助手（VDA）量化分析，发现当前先进模型的交互准确率仅为32.8%。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理评估框架过于关注粗粒度任务完成度，忽视了细粒度控制能力的重要性。

Method: 开发了FineState-Bench，包含2257个多平台任务基准和四阶段指标，并设计了VDA进行视觉定位能力的定量解耦分析。

Result: 实验显示，先进模型的细粒度交互准确率为32.8%，理想视觉定位可提升Gemini-2.5-Flash成功率14.9%。

Conclusion: 当前GUI代理的主要瓶颈是基础视觉定位能力，所有资源已开源。

Abstract: With the rapid advancement of generative artificial intelligence technology,
Graphical User Interface (GUI) agents have demonstrated tremendous potential
for autonomously managing daily tasks through natural language instructions.
However, current evaluation frameworks for GUI agents suffer from fundamental
flaws: existing benchmarks overly focus on coarse-grained task completion while
neglecting fine-grained control capabilities crucial for real-world
applications. To address this, we introduce FineState-Bench, the first
evaluation and diagnostic standard for fine-grained GUI proxy operations,
designed to quantify fine-grained control. This multi-platform (desktop, Web,
mobile) framework includes 2257 task benchmarks in four components and uses a
four-phase indicator for comprehensive perception-to-control assessment. To
analyze perception and positioning for refined operations, we developed the
plug-and-play Visual Diagnostic Assistant (VDA), enabling the first
quantitative decoupling analysis of these capabilities. Experimental results on
our benchmark show that the most advanced models achieve only 32.8%
fine-grained interaction accuracy. Using our VDA in controlled experiments,
quantifying the impact of visual capabilities, we showed that ideal visual
localization boosts Gemini-2.5-Flash's success rate by 14.9\%. Our diagnostic
framework confirms for the first time that the primary bottleneck for current
GUI proxies is basic visual positioning capability.All resources are fully
open-source. github: https://github.com/AnonymousThewarehouse/FineState-Bench
huggingface: https://huggingface.co/datasets/Willtime2006/Static-FineBench

</details>


### [14] [Beyond Blanket Masking: Examining Granularity for Privacy Protection in Images Captured by Blind and Low Vision Users](https://arxiv.org/abs/2508.09245)
*Jeffri Murrugarra-LLerena,Haoran Niu,K. Suzanne Barber,Hal Daumé III,Yang Trista Cao,Paola Cascante-Bonilla*

Main category: cs.CV

TL;DR: FiGPriv是一种细粒度隐私保护框架，通过选择性屏蔽高风险隐私信息，保留低风险信息，提升视觉语言模型的可用性。


<details>
  <summary>Details</summary>
Motivation: 针对视觉助手系统中盲人和低视力用户可能无意中捕捉隐私信息的问题，现有方法因粗粒度屏蔽导致可用性下降。

Method: 结合细粒度分割与数据驱动的风险评分机制，选择性屏蔽高风险隐私信息。

Result: 在BIV-Priv-Seg数据集上，FiGPriv保留26%的图像内容，提升VLM响应能力11%，识别能力45%。

Conclusion: FiGPriv在保护隐私的同时显著提升了视觉语言模型的可用性和识别能力。

Abstract: As visual assistant systems powered by visual language models (VLMs) become
more prevalent, concerns over user privacy have grown, particularly for blind
and low vision users who may unknowingly capture personal private information
in their images. Existing privacy protection methods rely on coarse-grained
segmentation, which uniformly masks entire private objects, often at the cost
of usability. In this work, we propose FiGPriv, a fine-grained privacy
protection framework that selectively masks only high-risk private information
while preserving low-risk information. Our approach integrates fine-grained
segmentation with a data-driven risk scoring mechanism. We evaluate our
framework using the BIV-Priv-Seg dataset and show that FiG-Priv preserves +26%
of image content, enhancing the ability of VLMs to provide useful responses by
11% and identify the image content by 45%, while ensuring privacy protection.
Project Page: https://artcs1.github.io/VLMPrivacy/

</details>


### [15] [Harnessing Input-Adaptive Inference for Efficient VLN](https://arxiv.org/abs/2508.09262)
*Dongwoo Kang,Akhil Perincherry,Zachary Coalson,Aiden Gabriel,Stefan Lee,Sanghyun Hong*

Main category: cs.CV

TL;DR: 提出了一种新型输入自适应导航方法，通过三种算法提升视觉与语言导航（VLN）模型的效率，显著减少计算量。


<details>
  <summary>Details</summary>
Motivation: 现有输入自适应机制在减少计算量时会导致性能显著下降，因此需要更高效的解决方案。

Method: 提出三种自适应算法：选择性处理全景视图、基于重要性的自适应阈值和缓存机制。

Result: 在七个VLN基准测试中，计算量减少超过2倍，且性能无明显下降。

Conclusion: 该方法显著提升了VLN模型的效率，适用于计算资源有限的场景。

Abstract: An emerging paradigm in vision-and-language navigation (VLN) is the use of
history-aware multi-modal transformer models. Given a language instruction,
these models process observation and navigation history to predict the most
appropriate action for an agent. While they have significantly improved
performance, the scale of these models can be a bottleneck in practical
settings with limited computational resources. In this work, we propose a novel
input-adaptive navigation method to enhance VLN model efficiency. We first show
that existing input-adaptive mechanisms fail to reduce computations without
substantial performance degradation. To address this, we introduce three
adaptive algorithms, each deployed at a different level: (1) To improve spatial
efficiency, we selectively process panoramic views at each observation of an
agent. (2) To improve intra-model efficiency, we propose importance-based
adaptive thresholding for the early-exit methods. (3) To improve temporal
efficiency, we implement a caching mechanism that prevents reprocessing of
views previously seen by the agent. In evaluations on seven VLN benchmarks, we
demonstrate over a 2$\times$ reduction in computation across three
off-the-shelf agents in both standard and continuous environments. Our code is
publicly available at
https://github.com/secure-ai-systems-group/adaptive-vision-and-language-navigation.

</details>


### [16] [SegDAC: Segmentation-Driven Actor-Critic for Visual Reinforcement Learning](https://arxiv.org/abs/2508.09325)
*Alexandre Brown,Glen Berseth*

Main category: cs.CV

TL;DR: SegDAC是一种基于分割的视觉强化学习方法，通过结合SAM和YOLO-World实现对象中心分解和语义标注，显著提升了视觉泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 视觉强化学习面临高维输入和噪声奖励的挑战，现有大模型在RL中的整合效果不明确，需要一种更有效的方法。

Method: SegDAC结合SAM进行对象中心分解，YOLO-World通过文本提示语义标注，并采用新型Transformer架构动态选择关注的分割区域。

Result: 在Maniskill3基准测试中，SegDAC在视觉泛化能力上表现优异，最难任务性能翻倍，样本效率也优于或匹配现有方法。

Conclusion: SegDAC通过对象中心分割和动态注意力机制，显著提升了视觉RL的泛化能力和效率，无需人工标注。

Abstract: Visual reinforcement learning (RL) is challenging due to the need to learn
both perception and actions from high-dimensional inputs and noisy rewards.
Although large perception models exist, integrating them effectively into RL
for visual generalization and improved sample efficiency remains unclear. We
propose SegDAC, a Segmentation-Driven Actor-Critic method. SegDAC uses Segment
Anything (SAM) for object-centric decomposition and YOLO-World to ground
segments semantically via text prompts. It includes a novel transformer-based
architecture that supports a dynamic number of segments at each time step and
effectively learns which segments to focus on using online RL, without using
human labels. By evaluating SegDAC over a challenging visual generalization
benchmark using Maniskill3, which covers diverse manipulation tasks under
strong visual perturbations, we demonstrate that SegDAC achieves significantly
better visual generalization, doubling prior performance on the hardest setting
and matching or surpassing prior methods in sample efficiency across all
evaluated tasks.

</details>


### [17] [Lung-DDPM+: Efficient Thoracic CT Image Synthesis using Diffusion Probabilistic Model](https://arxiv.org/abs/2508.09327)
*Yifan Jiang,Ahmad Shariftabrizi,Venkata SK. Manem*

Main category: cs.CV

TL;DR: Lung-DDPM+是一种改进的生成模型，用于高效生成高保真度的肺部CT图像，显著提升了计算效率和临床适用性。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在肺癌诊断中存在效率低和解剖学不精确的问题，限制了其临床应用。

Method: 提出Lung-DDPM+，一种基于结节语义布局的去噪扩散概率模型，通过肺部DPM-solver加速，专注于病变区域。

Result: 在LIDC-IDRI数据集上，Lung-DDPM+实现了8倍FLOPs减少、6.8倍GPU内存降低和14倍采样加速，同时保持样本质量。

Conclusion: Lung-DDPM+能高效生成高质量肺部CT图像，具有广泛医学影像应用的潜力。

Abstract: Generative artificial intelligence (AI) has been playing an important role in
various domains. Leveraging its high capability to generate high-fidelity and
diverse synthetic data, generative AI is widely applied in diagnostic tasks,
such as lung cancer diagnosis using computed tomography (CT). However, existing
generative models for lung cancer diagnosis suffer from low efficiency and
anatomical imprecision, which limit their clinical applicability. To address
these drawbacks, we propose Lung-DDPM+, an improved version of our previous
model, Lung-DDPM. This novel approach is a denoising diffusion probabilistic
model (DDPM) guided by nodule semantic layouts and accelerated by a pulmonary
DPM-solver, enabling the method to focus on lesion areas while achieving a
better trade-off between sampling efficiency and quality. Evaluation results on
the public LIDC-IDRI dataset suggest that the proposed method achieves
8$\times$ fewer FLOPs (floating point operations per second), 6.8$\times$ lower
GPU memory consumption, and 14$\times$ faster sampling compared to Lung-DDPM.
Moreover, it maintains comparable sample quality to both Lung-DDPM and other
state-of-the-art (SOTA) generative models in two downstream segmentation tasks.
We also conducted a Visual Turing Test by an experienced radiologist, showing
the advanced quality and fidelity of synthetic samples generated by the
proposed method. These experimental results demonstrate that Lung-DDPM+ can
effectively generate high-quality thoracic CT images with lung nodules,
highlighting its potential for broader applications, such as general tumor
synthesis and lesion generation in medical imaging. The code and pretrained
models are available at https://github.com/Manem-Lab/Lung-DDPM-PLUS.

</details>


### [18] [UltraLight Med-Vision Mamba for Classification of Neoplastic Progression in Tubular Adenomas](https://arxiv.org/abs/2508.09339)
*Aqsa Sultana,Nordin Abouzahra,Ahmed Rahu,Brian Shula,Brandon Combs,Derrick Forchetti,Theus Aspiras,Vijayan K. Asari*

Main category: cs.CV

TL;DR: Ultralight Med-Vision Mamba（SSM模型）在结肠镜筛查中通过深度学习算法精确分类腺瘤，优化风险评估和个性化监测方案。


<details>
  <summary>Details</summary>
Motivation: 识别癌前息肉对降低结直肠癌风险至关重要，需要高效准确的分类方法。

Method: 采用Ultralight Med-Vision Mamba（SSM模型），利用其长短期依赖建模和图像泛化能力分析全切片图像。

Result: 该模型在计算速度和可扩展性上表现优异，适合实时临床部署。

Conclusion: Ultralight Med-Vision Mamba是一种有前景的工具，可提升结肠镜筛查的效率和准确性。

Abstract: Identification of precancerous polyps during routine colonoscopy screenings
is vital for their excision, lowering the risk of developing colorectal cancer.
Advanced deep learning algorithms enable precise adenoma classification and
stratification, improving risk assessment accuracy and enabling personalized
surveillance protocols that optimize patient outcomes. Ultralight Med-Vision
Mamba, a state-space based model (SSM), has excelled in modeling long- and
short-range dependencies and image generalization, critical factors for
analyzing whole slide images. Furthermore, Ultralight Med-Vision Mamba's
efficient architecture offers advantages in both computational speed and
scalability, making it a promising tool for real-time clinical deployment.

</details>


### [19] [Blink-to-code: real-time Morse code communication via eye blink detection and classification](https://arxiv.org/abs/2508.09344)
*Anushka Bhatt*

Main category: cs.CV

TL;DR: 提出了一种将眨眼动作转化为摩尔斯电码的实时系统，帮助严重运动障碍者进行交流。


<details>
  <summary>Details</summary>
Motivation: 为严重运动障碍者提供一种低成本、易实现的辅助交流方法。

Method: 使用标准摄像头和计算机视觉技术检测眨眼动作，并将其分类为短（点）或长（划），再解码为字母数字字符。

Result: 实验显示，系统解码准确率为62%，响应时间为18-20秒。

Conclusion: 该系统是一种可行的低成本辅助交流方法。

Abstract: This study proposes a real-time system that translates voluntary eye blinks
into Morse code, enabling communication for individuals with severe motor
impairments. Using a standard webcam and computer vision, the system detects
and classifies blinks as short (dot) or long (dash), then decodes them into
alphanumeric characters. Experiments with five participants show 62% decoding
accuracy and 18-20 seconds response times, demonstrating a viable, low-cost
assistive communication method.

</details>


### [20] [FusionEnsemble-Net: An Attention-Based Ensemble of Spatiotemporal Networks for Multimodal Sign Language Recognition](https://arxiv.org/abs/2508.09362)
*Md. Milon Islam,Md Rezwanul Haque,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种名为FusionEnsemble-Net的新型注意力集成时空网络，用于动态融合视觉和运动数据，显著提升了手语识别的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医疗沟通中复杂多模态手势识别的挑战。

Method: 通过四种不同的时空网络同步处理RGB视频和雷达数据，使用注意力融合模块动态融合特征，并通过集成分类器增强鲁棒性。

Result: 在意大利手语的大规模数据集MultiMeDaLIS上达到了99.44%的测试准确率，优于现有方法。

Conclusion: 注意力融合的多样化时空网络集成能够为复杂多模态手势识别提供鲁棒且准确的框架。

Abstract: Accurate recognition of sign language in healthcare communication poses a
significant challenge, requiring frameworks that can accurately interpret
complex multimodal gestures. To deal with this, we propose FusionEnsemble-Net,
a novel attention-based ensemble of spatiotemporal networks that dynamically
fuses visual and motion data to enhance recognition accuracy. The proposed
approach processes RGB video and range Doppler map radar modalities
synchronously through four different spatiotemporal networks. For each network,
features from both modalities are continuously fused using an attention-based
fusion module before being fed into an ensemble of classifiers. Finally, the
outputs of these four different fused channels are combined in an ensemble
classification head, thereby enhancing the model's robustness. Experiments
demonstrate that FusionEnsemble-Net outperforms state-of-the-art approaches
with a test accuracy of 99.44% on the large-scale MultiMeDaLIS dataset for
Italian Sign Language. Our findings indicate that an ensemble of diverse
spatiotemporal networks, unified by attention-based fusion, yields a robust and
accurate framework for complex, multimodal isolated gesture recognition tasks.
The source code is available at:
https://github.com/rezwanh001/Multimodal-Isolated-Italian-Sign-Language-Recognition.

</details>


### [21] [A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition](https://arxiv.org/abs/2508.09372)
*Md Rezwanul Haque,Md. Milon Islam,S M Taslim Uddin Raju,Fakhri Karray*

Main category: cs.CV

TL;DR: 提出了一种双架构框架，分别针对手语识别中的独立于手语者和未见句子问题，通过特定任务网络显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决连续手语识别中手语者独立性和未见句子结构泛化能力不足的问题。

Method: 使用Signer-Invariant Conformer和Multi-Scale Fusion Transformer分别处理手语者独立性和未见句子任务。

Result: 在Isharah-1000数据集上，SI任务WER降低13.53%，US任务WER达到47.78%，在SignEval 2025挑战赛中表现优异。

Conclusion: 特定任务网络设计显著提升了连续手语识别的性能，为后续研究设定了新基准。

Abstract: Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

</details>


### [22] [What Can We Learn from Inter-Annotator Variability in Skin Lesion Segmentation?](https://arxiv.org/abs/2508.09381)
*Kumar Abhishek,Jeremy Kawahara,Ghassan Hamarneh*

Main category: cs.CV

TL;DR: IMA++数据集研究了皮肤病变分割中的标注者间差异，发现恶性病变与标注一致性显著相关，并利用多任务学习提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究医学图像分割中标注者间差异的原因及其与恶性病变的关联。

Method: 构建IMA++数据集，分析标注者、恶性程度等因素对一致性的影响，并利用多任务学习结合一致性作为特征。

Result: 恶性病变与标注一致性显著相关（p<0.001），多任务学习使平衡准确率提升4.2%。

Conclusion: 标注一致性可作为临床特征提升模型性能，尤其在恶性病变检测中。

Abstract: Medical image segmentation exhibits intra- and inter-annotator variability
due to ambiguous object boundaries, annotator preferences, expertise, and
tools, among other factors. Lesions with ambiguous boundaries, e.g., spiculated
or infiltrative nodules, or irregular borders per the ABCD rule, are
particularly prone to disagreement and are often associated with malignancy. In
this work, we curate IMA++, the largest multi-annotator skin lesion
segmentation dataset, on which we conduct an in-depth study of variability due
to annotator, malignancy, tool, and skill factors. We find a statistically
significant (p<0.001) association between inter-annotator agreement (IAA),
measured using Dice, and the malignancy of skin lesions. We further show that
IAA can be accurately predicted directly from dermoscopic images, achieving a
mean absolute error of 0.108. Finally, we leverage this association by
utilizing IAA as a "soft" clinical feature within a multi-task learning
objective, yielding a 4.2% improvement in balanced accuracy averaged across
multiple model architectures and across IMA++ and four public dermoscopic
datasets. The code is available at https://github.com/sfu-mial/skin-IAV.

</details>


### [23] [X-UniMotion: Animating Human Images with Expressive, Unified and Identity-Agnostic Motion Latents](https://arxiv.org/abs/2508.09383)
*Guoxian Song,Hongyi Xu,Xiaochen Zhao,You Xie,Tianpei Gu,Zenan Li,Chenxu Zhang,Linjie Luo*

Main category: cs.CV

TL;DR: X-UniMotion是一种统一的隐式潜在表示方法，用于捕捉全身人体运动，包括面部表情、身体姿势和手势，支持高保真跨身份运动转移。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖显式骨骼姿势和启发式跨身份调整，缺乏对多粒度运动的统一表达。X-UniMotion旨在通过隐式潜在表示解决这一问题。

Method: 提出自监督端到端框架，联合学习运动编码器和潜在表示，结合DiT生成模型，并通过空间、颜色增强及合成3D渲染实现运动-身份解耦。

Result: 实验表明，X-UniMotion在运动保真度和身份保持方面优于现有方法。

Conclusion: X-UniMotion通过隐式潜在表示实现了高保真、跨身份的全身运动转移，为运动生成领域提供了新思路。

Abstract: We present X-UniMotion, a unified and expressive implicit latent
representation for whole-body human motion, encompassing facial expressions,
body poses, and hand gestures. Unlike prior motion transfer methods that rely
on explicit skeletal poses and heuristic cross-identity adjustments, our
approach encodes multi-granular motion directly from a single image into a
compact set of four disentangled latent tokens -- one for facial expression,
one for body pose, and one for each hand. These motion latents are both highly
expressive and identity-agnostic, enabling high-fidelity, detailed
cross-identity motion transfer across subjects with diverse identities, poses,
and spatial configurations. To achieve this, we introduce a self-supervised,
end-to-end framework that jointly learns the motion encoder and latent
representation alongside a DiT-based video generative model, trained on
large-scale, diverse human motion datasets. Motion-identity disentanglement is
enforced via 2D spatial and color augmentations, as well as synthetic 3D
renderings of cross-identity subject pairs under shared poses. Furthermore, we
guide motion token learning with auxiliary decoders that promote fine-grained,
semantically aligned, and depth-aware motion embeddings. Extensive experiments
show that X-UniMotion outperforms state-of-the-art methods, producing highly
expressive animations with superior motion fidelity and identity preservation.

</details>


### [24] [DenoDet V2: Phase-Amplitude Cross Denoising for SAR Object Detection](https://arxiv.org/abs/2508.09392)
*Kang Ni,Minrui Zou,Yuxuan Li,Xiang Li,Kehua Guo,Ming-Ming Cheng,Yimian Dai*

Main category: cs.CV

TL;DR: DenoDet V2提出了一种新颖的变换域特征解构与调制方法，通过注意力架构和振幅-相位互补机制显著提升了SAR目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决SAR目标检测中相干噪声的普遍影响，现有方法多依赖空间域特征分析或增强，缺乏对变换域特征的直接调制。

Method: 设计了基于注意力架构的变换域特征解构与调制方法，利用振幅和相位信息的互补性进行互调制。

Result: 在多个SAR数据集上表现优异，SARDet-100K上比DenoDet V1提升0.8%，模型复杂度减半。

Conclusion: DenoDet V2通过变换域特征调制和互补机制，显著提升了SAR目标检测的性能和效率。

Abstract: One of the primary challenges in Synthetic Aperture Radar (SAR) object
detection lies in the pervasive influence of coherent noise. As a common
practice, most existing methods, whether handcrafted approaches or deep
learning-based methods, employ the analysis or enhancement of object
spatial-domain characteristics to achieve implicit denoising. In this paper, we
propose DenoDet V2, which explores a completely novel and different perspective
to deconstruct and modulate the features in the transform domain via a
carefully designed attention architecture. Compared to DenoDet V1, DenoDet V2
is a major advancement that exploits the complementary nature of amplitude and
phase information through a band-wise mutual modulation mechanism, which
enables a reciprocal enhancement between phase and amplitude spectra. Extensive
experiments on various SAR datasets demonstrate the state-of-the-art
performance of DenoDet V2. Notably, DenoDet V2 achieves a significant 0.8\%
improvement on SARDet-100K dataset compared to DenoDet V1, while reducing the
model complexity by half. The code is available at
https://github.com/GrokCV/GrokSAR.

</details>


### [25] [Skyshield: Event-Driven Submillimetre Thin Obstacle Detection for Drone Flight Safety](https://arxiv.org/abs/2508.09397)
*Zhengli Zhang,Xinyu Luo,Yuchen Sun,Wenhua Ding,Dongyu Huang,Xinlei Chen*

Main category: cs.CV

TL;DR: SkyShield是一个事件驱动的端到端框架，用于检测无人机在复杂环境中遇到的亚毫米级障碍物，如钢丝和风筝线。


<details>
  <summary>Details</summary>
Motivation: 传统传感器（如RGB相机、LiDAR和深度相机）难以检测亚毫米级障碍物，这对无人机的安全构成威胁。

Method: 利用事件流中薄障碍物的独特特征，采用轻量级U-Net架构和创新的Dice-Contour正则化损失函数进行精确检测。

Result: 实验结果显示，该方法平均F1分数为0.7088，延迟仅为21.2毫秒，适合边缘和移动平台部署。

Conclusion: SkyShield为无人机在复杂环境中的安全飞行提供了高效且低延迟的解决方案。

Abstract: Drones operating in complex environments face a significant threat from thin
obstacles, such as steel wires and kite strings at the submillimeter level,
which are notoriously difficult for conventional sensors like RGB cameras,
LiDAR, and depth cameras to detect. This paper introduces SkyShield, an
event-driven, end-to-end framework designed for the perception of submillimeter
scale obstacles. Drawing upon the unique features that thin obstacles present
in the event stream, our method employs a lightweight U-Net architecture and an
innovative Dice-Contour Regularization Loss to ensure precise detection.
Experimental results demonstrate that our event-based approach achieves mean F1
Score of 0.7088 with a low latency of 21.2 ms, making it ideal for deployment
on edge and mobile platforms.

</details>


### [26] [Autonomous AI Bird Feeder for Backyard Biodiversity Monitoring](https://arxiv.org/abs/2508.09398)
*El Mustapha Mansouri*

Main category: cs.CV

TL;DR: 低成本、本地化的自主后院鸟类监测系统，适用于比利时城市花园。


<details>
  <summary>Details</summary>
Motivation: 开发一种低成本、隐私保护的鸟类监测系统，避免云服务费用，适合公民科学项目。

Method: 使用运动触发的IP摄像头上传视频片段，通过Detectron2定位鸟类，EfficientNet-B3模型分类。

Result: 验证集准确率约99.5%，实际场景中top-1准确率约88%。

Conclusion: 该系统适合家庭生物多样性记录，具有公民科学应用的可行性。

Abstract: This paper presents a low cost, on premise system for autonomous backyard
bird monitoring in Belgian urban gardens. A motion triggered IP camera uploads
short clips via FTP to a local server, where frames are sampled and birds are
localized with Detectron2; cropped regions are then classified by an
EfficientNet-B3 model fine tuned on a 40-species Belgian subset derived from a
larger Kaggle corpus. All processing runs on commodity hardware without a
discrete GPU, preserving privacy and avoiding cloud fees. The physical feeder
uses small entry ports (30 mm) to exclude pigeons and reduce nuisance triggers.
Detector-guided cropping improves classification accuracy over raw-frame
classification. The classifier attains high validation performance on the
curated subset (about 99.5 percent) and delivers practical field accuracy
(top-1 about 88 percent) on held-out species, demonstrating feasibility for
citizen-science-grade biodiversity logging at home.

</details>


### [27] [Waymo-3DSkelMo: A Multi-Agent 3D Skeletal Motion Dataset for Pedestrian Interaction Modeling in Autonomous Driving](https://arxiv.org/abs/2508.09404)
*Guangxun Zhu,Shiyu Fan,Hang Dai,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: Waymo-3DSkelMo是一个大规模高质量3D运动数据集，专注于多行人交互，为自动驾驶中的细粒度行人行为理解提供支持。


<details>
  <summary>Details</summary>
Motivation: 现有数据集依赖单目RGB视频帧估计3D姿态，存在遮挡和时间不连续问题，导致运动质量低。

Method: 利用3D人体形状和运动先验，从LiDAR点云中提取高质量、时间连贯的3D骨骼运动。

Result: 数据集覆盖800多个真实驾驶场景，包含丰富的交互语义，并建立了3D姿态预测基准。

Conclusion: Waymo-3DSkelMo是复杂城市环境中细粒度人类行为研究的宝贵资源。

Abstract: Large-scale high-quality 3D motion datasets with multi-person interactions
are crucial for data-driven models in autonomous driving to achieve
fine-grained pedestrian interaction understanding in dynamic urban
environments. However, existing datasets mostly rely on estimating 3D poses
from monocular RGB video frames, which suffer from occlusion and lack of
temporal continuity, thus resulting in unrealistic and low-quality human
motion. In this paper, we introduce Waymo-3DSkelMo, the first large-scale
dataset providing high-quality, temporally coherent 3D skeletal motions with
explicit interaction semantics, derived from the Waymo Perception dataset. Our
key insight is to utilize 3D human body shape and motion priors to enhance the
quality of the 3D pose sequences extracted from the raw LiDRA point clouds. The
dataset covers over 14,000 seconds across more than 800 real driving scenarios,
including rich interactions among an average of 27 agents per scene (with up to
250 agents in the largest scene). Furthermore, we establish 3D pose forecasting
benchmarks under varying pedestrian densities, and the results demonstrate its
value as a foundational resource for future research on fine-grained human
behavior understanding in complex urban environments. The dataset and code will
be available at https://github.com/GuangxunZhu/Waymo-3DSkelMo

</details>


### [28] [RampNet: A Two-Stage Pipeline for Bootstrapping Curb Ramp Detection in Streetscape Images from Open Government Metadata](https://arxiv.org/abs/2508.09415)
*John S. O'Meara,Jared Hwang,Zeyu Wang,Michael Saugstad,Jon E. Froehlich*

Main category: cs.CV

TL;DR: 提出RampNet两阶段流程，用于大规模生成高质量路缘坡道数据集并提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 路缘坡道对城市无障碍至关重要，但缺乏大规模高质量数据集限制了检测能力。

Method: 第一阶段通过自动转换政府数据生成21万张标注的街景图像；第二阶段训练改进的ConvNeXt V2模型。

Result: 生成数据集精度94.0%，召回率92.5%；检测模型AP达0.9236，超越现有工作。

Conclusion: 首次贡献了大规模高质量路缘坡道数据集、基准和模型。

Abstract: Curb ramps are critical for urban accessibility, but robustly detecting them
in images remains an open problem due to the lack of large-scale, high-quality
datasets. While prior work has attempted to improve data availability with
crowdsourced or manually labeled data, these efforts often fall short in either
quality or scale. In this paper, we introduce and evaluate a two-stage pipeline
called RampNet to scale curb ramp detection datasets and improve model
performance. In Stage 1, we generate a dataset of more than 210,000 annotated
Google Street View (GSV) panoramas by auto-translating government-provided curb
ramp location data to pixel coordinates in panoramic images. In Stage 2, we
train a curb ramp detection model (modified ConvNeXt V2) from the generated
dataset, achieving state-of-the-art performance. To evaluate both stages of our
pipeline, we compare to manually labeled panoramas. Our generated dataset
achieves 94.0% precision and 92.5% recall, and our detection model reaches
0.9236 AP -- far exceeding prior work. Our work contributes the first
large-scale, high-quality curb ramp detection dataset, benchmark, and model.

</details>


### [29] [Distilling LLM Prior to Flow Model for Generalizable Agent's Imagination in Object Goal Navigation](https://arxiv.org/abs/2508.09423)
*Badi Li,Ren-jie Lu,Yu Zhou,Jingke Meng,Wei-shi Zheng*

Main category: cs.CV

TL;DR: GOAL是一种基于生成流的框架，通过结合LLM增强的全场景语义地图，在未见环境中实现物体导航任务，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖确定性模型，忽略了室内布局的不确定性，限制了泛化能力。GOAL旨在通过生成模型和LLM知识提升泛化性能。

Method: 使用生成流模型结合LLM推断的空间先验（二维高斯场），增强语义地图的补全能力。

Result: 在MP3D和Gibson数据集上达到SOTA性能，并在HM3D上表现出强泛化能力。

Conclusion: GOAL通过生成模型和LLM知识提升了物体导航任务的泛化性能，适用于未见环境。

Abstract: The Object Goal Navigation (ObjectNav) task challenges agents to locate a
specified object in an unseen environment by imagining unobserved regions of
the scene. Prior approaches rely on deterministic and discriminative models to
complete semantic maps, overlooking the inherent uncertainty in indoor layouts
and limiting their ability to generalize to unseen environments. In this work,
we propose GOAL, a generative flow-based framework that models the semantic
distribution of indoor environments by bridging observed regions with
LLM-enriched full-scene semantic maps. During training, spatial priors inferred
from large language models (LLMs) are encoded as two-dimensional Gaussian
fields and injected into target maps, distilling rich contextual knowledge into
the flow model and enabling more generalizable completions. Extensive
experiments demonstrate that GOAL achieves state-of-the-art performance on MP3D
and Gibson, and shows strong generalization in transfer settings to HM3D. Codes
and pretrained models are available at https://github.com/Badi-Li/GOAL.

</details>


### [30] [What-Meets-Where: Unified Learning of Action and Contact Localization in a New Dataset](https://arxiv.org/abs/2508.09428)
*Yuxiao Wang,Yu Lei,Wolin Liang,Weiying Xue,Zhenao Wei,Nan Zhuang,Qi Liu*

Main category: cs.CV

TL;DR: 论文提出PaIR-Net框架，结合动作语义和身体部位接触区域预测，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能同时建模动作语义和空间上下文，需填补这一空白。

Method: 提出PaIR-Net框架，包含CPAM、PGCS和IIM三个模块，用于动作语义和接触区域预测。

Result: 实验表明PaIR-Net显著优于基线，消融研究验证各模块有效性。

Conclusion: PaIR-Net成功解决了动作语义与空间上下文的联合建模问题，代码和数据集将公开。

Abstract: People control their bodies to establish contact with the environment. To
comprehensively understand actions across diverse visual contexts, it is
essential to simultaneously consider \textbf{what} action is occurring and
\textbf{where} it is happening. Current methodologies, however, often
inadequately capture this duality, typically failing to jointly model both
action semantics and their spatial contextualization within scenes. To bridge
this gap, we introduce a novel vision task that simultaneously predicts
high-level action semantics and fine-grained body-part contact regions. Our
proposed framework, PaIR-Net, comprises three key components: the Contact Prior
Aware Module (CPAM) for identifying contact-relevant body parts, the
Prior-Guided Concat Segmenter (PGCS) for pixel-wise contact segmentation, and
the Interaction Inference Module (IIM) responsible for integrating global
interaction relationships. To facilitate this task, we present PaIR (Part-aware
Interaction Representation), a comprehensive dataset containing 13,979 images
that encompass 654 actions, 80 object categories, and 17 body parts.
Experimental evaluation demonstrates that PaIR-Net significantly outperforms
baseline approaches, while ablation studies confirm the efficacy of each
architectural component. The code and dataset will be released upon
publication.

</details>


### [31] [MPT: Motion Prompt Tuning for Micro-Expression Recognition](https://arxiv.org/abs/2508.09446)
*Jiateng Liu,Hengcan Shi,Feng Chen,Zhiwen Shao,Yaonan Wang,Jianfei Cai,Wenming Zheng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Motion Prompt Tuning (MPT)的新方法，通过微调大型预训练模型（LMs）来改进微表情识别（MER），解决了现有方法难以捕捉细微面部运动的局限性。


<details>
  <summary>Details</summary>
Motivation: 微表情识别在医疗诊断、测谎等领域有广泛应用，但由于标注困难，数据集样本稀缺，限制了模型学习。现有LMs无法直接用于MER，因为它们难以捕捉短暂且细微的面部运动。

Method: MPT方法包括运动提示生成（运动放大和高斯标记化）和组适配器设计，以增强LMs在MER领域的表现。

Result: 在三个广泛使用的MER数据集上的实验表明，MPT方法优于现有最先进方法。

Conclusion: MPT方法通过运动提示生成和组适配器设计，显著提升了MER的性能，为细微运动识别提供了新思路。

Abstract: Micro-expression recognition (MER) is crucial in the affective computing
field due to its wide application in medical diagnosis, lie detection, and
criminal investigation. Despite its significance, obtaining micro-expression
(ME) annotations is challenging due to the expertise required from
psychological professionals. Consequently, ME datasets often suffer from a
scarcity of training samples, severely constraining the learning of MER models.
While current large pre-training models (LMs) offer general and discriminative
representations, their direct application to MER is hindered by an inability to
capture transitory and subtle facial movements-essential elements for effective
MER. This paper introduces Motion Prompt Tuning (MPT) as a novel approach to
adapting LMs for MER, representing a pioneering method for subtle motion prompt
tuning. Particularly, we introduce motion prompt generation, including motion
magnification and Gaussian tokenization, to extract subtle motions as prompts
for LMs. Additionally, a group adapter is carefully designed and inserted into
the LM to enhance it in the target MER domain, facilitating a more nuanced
distinction of ME representation. Furthermore, extensive experiments conducted
on three widely used MER datasets demonstrate that our proposed MPT
consistently surpasses state-of-the-art approaches and verifies its
effectiveness.

</details>


### [32] [RASR: Retrieval-Augmented Super Resolution for Practical Reference-based Image Restoration](https://arxiv.org/abs/2508.09449)
*Jiaqi Yan,Shuning Xu,Xiangyu Chen,Dell Zhang,Jie Tang,Gangshan Wu,Jie Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的检索增强超分辨率（RASR）方法，通过自动从参考数据库中检索相关高分辨率图像来提升低质量输入图像的分辨率，解决了传统RefSR依赖手动配对的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有RefSR方法依赖手动配对的参考图像，限制了实际应用。论文旨在通过自动检索相关参考图像，提升RefSR的实用性和扩展性。

Method: 提出了RASRNet，结合语义参考检索器和基于扩散的RefSR生成器，通过语义相似性检索参考图像，并利用扩散模型生成高分辨率图像。

Result: 在RASR-Flickr30数据集上，RASRNet比SISR基线提升了0.38 dB PSNR和-0.0131 LPIPS，生成了更真实的纹理。

Conclusion: 检索增强是缩小RefSR研究与实际应用差距的有前景的方向。

Abstract: Reference-based Super Resolution (RefSR) improves upon Single Image Super
Resolution (SISR) by leveraging high-quality reference images to enhance
texture fidelity and visual realism. However, a critical limitation of existing
RefSR approaches is their reliance on manually curated target-reference image
pairs, which severely constrains their practicality in real-world scenarios. To
overcome this, we introduce Retrieval-Augmented Super Resolution (RASR), a new
and practical RefSR paradigm that automatically retrieves semantically relevant
high-resolution images from a reference database given only a low-quality
input. This enables scalable and flexible RefSR in realistic use cases, such as
enhancing mobile photos taken in environments like zoos or museums, where
category-specific reference data (e.g., animals, artworks) can be readily
collected or pre-curated. To facilitate research in this direction, we
construct RASR-Flickr30, the first benchmark dataset designed for RASR. Unlike
prior datasets with fixed target-reference pairs, RASR-Flickr30 provides
per-category reference databases to support open-world retrieval. We further
propose RASRNet, a strong baseline that combines a semantic reference retriever
with a diffusion-based RefSR generator. It retrieves relevant references based
on semantic similarity and employs a diffusion-based generator enhanced with
semantic conditioning. Experiments on RASR-Flickr30 demonstrate that RASRNet
consistently improves over SISR baselines, achieving +0.38 dB PSNR and -0.0131
LPIPS, while generating more realistic textures. These findings highlight
retrieval augmentation as a promising direction to bridge the gap between
academic RefSR research and real-world applicability.

</details>


### [33] [HyperKD: Distilling Cross-Spectral Knowledge in Masked Autoencoders via Inverse Domain Shift with Spatial-Aware Masking and Specialized Loss](https://arxiv.org/abs/2508.09453)
*Abdul Matin,Tanjim Bin Faruk,Shrideep Pallickara,Sangmi Lee Pallickara*

Main category: cs.CV

TL;DR: HyperKD是一种新颖的知识蒸馏框架，用于将预训练基础模型的知识迁移到高光谱遥感图像中，解决了光谱差异和观测稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱遥感中直接应用基础模型存在光谱差异和观测稀缺的挑战，需要一种有效的方法来迁移学习表示。

Method: HyperKD通过特征策略（包括光谱范围通道对齐、空间特征引导掩码和增强损失函数）实现知识蒸馏，从Prithvi基础模型到EnMAP高光谱图像的学生模型。

Result: 实验表明，HyperKD显著提升了表示学习效果，增强了重建保真度和下游任务（如土地覆盖分类、作物类型识别和土壤有机碳预测）的鲁棒性。

Conclusion: HyperKD展示了知识蒸馏框架在高光谱遥感分析中的潜力，能够有效弥合光谱域差距。

Abstract: The proliferation of foundation models, pretrained on large-scale unlabeled
datasets, has emerged as an effective approach in creating adaptable and
reusable architectures that can be leveraged for various downstream tasks using
satellite observations. However, their direct application to hyperspectral
remote sensing remains challenging due to inherent spectral disparities and the
scarcity of available observations. In this work, we present HyperKD, a novel
knowledge distillation framework that enables transferring learned
representations from a teacher model into a student model for effective
development of a foundation model on hyperspectral images. Unlike typical
knowledge distillation frameworks, which use a complex teacher to guide a
simpler student, HyperKD enables an inverse form of knowledge transfer across
different types of spectral data, guided by a simpler teacher model. Building
upon a Masked Autoencoder, HyperKD distills knowledge from the Prithvi
foundational model into a student tailored for EnMAP hyperspectral imagery.
HyperKD addresses the inverse domain adaptation problem with spectral gaps by
introducing a feature-based strategy that includes spectral range-based channel
alignment, spatial feature-guided masking, and an enhanced loss function
tailored for hyperspectral images. HyperKD bridges the substantial spectral
domain gap, enabling the effective use of pretrained foundation models for
geospatial applications. Extensive experiments show that HyperKD significantly
improves representation learning in MAEs, leading to enhanced reconstruction
fidelity and more robust performance on downstream tasks such as land cover
classification, crop type identification, and soil organic carbon prediction,
underpinning the potential of knowledge distillation frameworks in remote
sensing analytics with hyperspectral imagery.

</details>


### [34] [Animate-X++: Universal Character Image Animation with Dynamic Backgrounds](https://arxiv.org/abs/2508.09454)
*Shuai Tan,Biao Gong,Zhuoxin Liu,Yan Wang,Xi Chen,Yifan Feng,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Animate-X++ 是一个基于 DiT 的通用动画框架，适用于包括拟人角色在内的多种角色类型，解决了现有方法在拟人角色和动态背景上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要适用于人类角色，且只能生成静态背景视频，限制了在游戏和娱乐行业中的应用。Animate-X++ 旨在解决这些问题。

Method: 通过 Pose Indicator 增强运动表示，结合 CLIP 视觉特征和模拟输入；采用多任务训练策略，联合训练动画和 TI2V 任务。

Result: Animate-X++ 在拟人角色和动态背景生成上表现优异，实验证明了其优越性和有效性。

Conclusion: Animate-X++ 是一个通用且高效的动画框架，适用于多种角色类型和动态背景，提升了视频的真实感。

Abstract: Character image animation, which generates high-quality videos from a
reference image and target pose sequence, has seen significant progress in
recent years. However, most existing methods only apply to human figures, which
usually do not generalize well on anthropomorphic characters commonly used in
industries like gaming and entertainment. Furthermore, previous methods could
only generate videos with static backgrounds, which limits the realism of the
videos. For the first challenge, our in-depth analysis suggests to attribute
this limitation to their insufficient modeling of motion, which is unable to
comprehend the movement pattern of the driving video, thus imposing a pose
sequence rigidly onto the target character. To this end, this paper proposes
Animate-X++, a universal animation framework based on DiT for various character
types, including anthropomorphic characters. To enhance motion representation,
we introduce the Pose Indicator, which captures comprehensive motion pattern
from the driving video through both implicit and explicit manner. The former
leverages CLIP visual features of a driving video to extract its gist of
motion, like the overall movement pattern and temporal relations among motions,
while the latter strengthens the generalization of DiT by simulating possible
inputs in advance that may arise during inference. For the second challenge, we
introduce a multi-task training strategy that jointly trains the animation and
TI2V tasks. Combined with the proposed partial parameter training, this
approach achieves not only character animation but also text-driven background
dynamics, making the videos more realistic. Moreover, we introduce a new
Animated Anthropomorphic Benchmark (A2Bench) to evaluate the performance of
Animate-X++ on universal and widely applicable animation images. Extensive
experiments demonstrate the superiority and effectiveness of Animate-X++.

</details>


### [35] [IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding](https://arxiv.org/abs/2508.09456)
*Junxian Li,Beining Xu,Di Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对视觉语言模型的新型输入感知后门攻击方法IAG，通过自适应触发器生成器操纵模型的视觉定位行为。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在视觉定位任务中的安全问题，尤其是后门攻击的潜在威胁。

Method: 使用文本条件U-Net将攻击目标的语义信息嵌入原始图像，并通过重建损失确保攻击的隐蔽性。

Result: 在InternVL-2.5-8B上ASR@0.5超过65%，且在Ferret-7B和LlaVA-1.5-7B上表现良好。

Conclusion: IAG攻击方法具有高效性、隐蔽性和可迁移性，为视觉语言模型的安全性研究提供了新视角。

Abstract: Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

</details>


### [36] [RelayFormer: A Unified Local-Global Attention Framework for Scalable Image and Video Manipulation Localization](https://arxiv.org/abs/2508.09459)
*Wen Huang,Jiarui Yang,Tao Dai,Jiawei Li,Shaoxiong Zhan,Bin Wang,Shu-Tao Xia*

Main category: cs.CV

TL;DR: RelayFormer是一种统一的模块化架构，用于跨图像和视频的视觉篡改定位，通过灵活的局部单元和全局-局部中继注意力机制实现可扩展、分辨率无关的处理。


<details>
  <summary>Details</summary>
Motivation: 现有方法在跨模态泛化和高效处理高分辨率或长时间输入方面存在不足。

Method: 采用灵活的局部单元和GLoRA机制，与现有Transformer骨干网络无缝集成，并通过轻量级查询式掩码解码器支持线性复杂度的视频序列推理。

Result: 在多个基准测试中实现了最先进的定位性能。

Conclusion: RelayFormer为可扩展和模态无关的视觉篡改定位设定了新的基准。

Abstract: Visual manipulation localization (VML) -- across both images and videos -- is
a crucial task in digital forensics that involves identifying tampered regions
in visual content. However, existing methods often lack cross-modal
generalization and struggle to handle high-resolution or long-duration inputs
efficiently.
  We propose RelayFormer, a unified and modular architecture for visual
manipulation localization across images and videos. By leveraging flexible
local units and a Global-Local Relay Attention (GLoRA) mechanism, it enables
scalable, resolution-agnostic processing with strong generalization. Our
framework integrates seamlessly with existing Transformer-based backbones, such
as ViT and SegFormer, via lightweight adaptation modules that require only
minimal architectural changes, ensuring compatibility without disrupting
pretrained representations.
  Furthermore, we design a lightweight, query-based mask decoder that supports
one-shot inference across video sequences with linear complexity. Extensive
experiments across multiple benchmarks demonstrate that our approach achieves
state-of-the-art localization performance, setting a new baseline for scalable
and modality-agnostic VML. Code is available at:
https://github.com/WenOOI/RelayFormer.

</details>


### [37] [Gen-AFFECT: Generation of Avatar Fine-grained Facial Expressions with Consistent identiTy](https://arxiv.org/abs/2508.09461)
*Hao Yu,Rupayan Mallick,Margrit Betke,Sarah Adel Bargal*

Main category: cs.CV

TL;DR: GEN-AFFECT是一种新型个性化头像生成框架，通过多模态扩散变换器实现身份一致性和多样化表情生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以捕捉细粒度面部表情并保持身份一致性。

Method: 利用多模态扩散变换器提取身份-表情表示，并通过一致注意力机制保持身份一致性。

Result: 在生成表情准确性、身份保持和一致性方面优于现有方法。

Conclusion: GEN-AFFECT在个性化头像生成中表现出色，解决了现有方法的局限性。

Abstract: Different forms of customized 2D avatars are widely used in gaming
applications, virtual communication, education, and content creation. However,
existing approaches often fail to capture fine-grained facial expressions and
struggle to preserve identity across different expressions. We propose
GEN-AFFECT, a novel framework for personalized avatar generation that generates
expressive and identity-consistent avatars with a diverse set of facial
expressions. Our framework proposes conditioning a multimodal diffusion
transformer on an extracted identity-expression representation. This enables
identity preservation and representation of a wide range of facial expressions.
GEN-AFFECT additionally employs consistent attention at inference for
information sharing across the set of generated expressions, enabling the
generation process to maintain identity consistency over the array of generated
fine-grained expressions. GEN-AFFECT demonstrates superior performance compared
to previous state-of-the-art methods on the basis of the accuracy of the
generated expressions, the preservation of the identity and the consistency of
the target identity across an array of fine-grained facial expressions.

</details>


### [38] [Event-driven Robust Fitting on Neuromorphic Hardware](https://arxiv.org/abs/2508.09466)
*Tam Ngoc-Bang Nguyen,Anh-Dzung Doan,Zhipeng Cai,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 论文提出了一种基于神经形态计算的能量高效鲁棒拟合方法，使用新型脉冲神经网络在Intel Loihi 2硬件上实现，能耗仅为传统CPU方法的15%。


<details>
  <summary>Details</summary>
Motivation: 鲁棒拟合在计算机视觉中至关重要，但现有方法忽视了能源效率问题，而AI的高能耗已成为关注焦点。

Method: 设计了新型脉冲神经网络，结合事件驱动的模型估计方法，适应Intel Loihi 2硬件架构，并优化算法以应对硬件限制。

Result: 实验表明，该方法能耗仅为传统CPU方法的15%，且精度相当。

Conclusion: 神经形态计算为鲁棒拟合提供了能源高效的解决方案，展示了其在AI应用中的潜力。

Abstract: Robust fitting of geometric models is a fundamental task in many computer
vision pipelines. Numerous innovations have been produced on the topic, from
improving the efficiency and accuracy of random sampling heuristics to
generating novel theoretical insights that underpin new approaches with
mathematical guarantees. However, one aspect of robust fitting that has
received little attention is energy efficiency. This performance metric has
become critical as high energy consumption is a growing concern for AI
adoption. In this paper, we explore energy-efficient robust fitting via the
neuromorphic computing paradigm. Specifically, we designed a novel spiking
neural network for robust fitting on real neuromorphic hardware, the Intel
Loihi 2. Enabling this are novel event-driven formulations of model estimation
that allow robust fitting to be implemented in the unique architecture of Loihi
2, and algorithmic strategies to alleviate the current limited precision and
instruction set of the hardware. Results show that our neuromorphic robust
fitting consumes only a fraction (15%) of the energy required to run the
established robust fitting algorithm on a standard CPU to equivalent accuracy.

</details>


### [39] [CitySeg: A 3D Open Vocabulary Semantic Segmentation Foundation Model in City-scale Scenarios](https://arxiv.org/abs/2508.09470)
*Jialei Xu,Zizhuang Wei,Weikang You,Linyun Li,Weijian Sun*

Main category: cs.CV

TL;DR: CitySeg是一个用于城市规模点云语义分割的基础模型，通过引入文本模态实现开放词汇分割和零样本推理，解决了数据分布不均和语义标签差异问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型受限于3D数据规模小和数据集间的领域差距，导致泛化能力不足。

Method: 提出局部-全局交叉注意力网络增强感知能力，采用分层分类策略解决语义标签差异，并使用两阶段训练策略和铰链损失。

Result: 在九个封闭集基准测试中达到SOTA性能，首次实现不依赖视觉信息的零样本泛化。

Conclusion: CitySeg通过创新方法显著提升了城市规模点云语义分割的性能和泛化能力。

Abstract: Semantic segmentation of city-scale point clouds is a critical technology for
Unmanned Aerial Vehicle (UAV) perception systems, enabling the classification
of 3D points without relying on any visual information to achieve comprehensive
3D understanding. However, existing models are frequently constrained by the
limited scale of 3D data and the domain gap between datasets, which lead to
reduced generalization capability. To address these challenges, we propose
CitySeg, a foundation model for city-scale point cloud semantic segmentation
that incorporates text modality to achieve open vocabulary segmentation and
zero-shot inference. Specifically, in order to mitigate the issue of
non-uniform data distribution across multiple domains, we customize the data
preprocessing rules, and propose a local-global cross-attention network to
enhance the perception capabilities of point networks in UAV scenarios. To
resolve semantic label discrepancies across datasets, we introduce a
hierarchical classification strategy. A hierarchical graph established
according to the data annotation rules consolidates the data labels, and the
graph encoder is used to model the hierarchical relationships between
categories. In addition, we propose a two-stage training strategy and employ
hinge loss to increase the feature separability of subcategories. Experimental
results demonstrate that the proposed CitySeg achieves state-of-the-art (SOTA)
performance on nine closed-set benchmarks, significantly outperforming existing
approaches. Moreover, for the first time, CitySeg enables zero-shot
generalization in city-scale point cloud scenarios without relying on visual
information.

</details>


### [40] [Leveraging Failed Samples: A Few-Shot and Training-Free Framework for Generalized Deepfake Detection](https://arxiv.org/abs/2508.09475)
*Shibo Yao,Renshuai Tao,Xiaolong Zheng,Chao Liang,Chunjie Zhang*

Main category: cs.CV

TL;DR: FTNet是一种无需训练的方法，用于少样本深度伪造检测，通过利用少量样本显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决深度伪造检测中模型对未知样本泛化能力差的问题，提出少样本任务视角。

Method: 提出Few-shot Training-free Network (FTNet)，仅需一个伪造样本和真实样本进行比较分类。

Result: 在29种生成模型上实现新SoTA性能，平均提升8.7%。

Conclusion: 利用失败样本可以提升少样本深度伪造检测性能，为实际应用提供新思路。

Abstract: Recent deepfake detection studies often treat unseen sample detection as a
``zero-shot" task, training on images generated by known models but
generalizing to unknown ones. A key real-world challenge arises when a model
performs poorly on unknown samples, yet these samples remain available for
analysis. This highlights that it should be approached as a ``few-shot" task,
where effectively utilizing a small number of samples can lead to significant
improvement. Unlike typical few-shot tasks focused on semantic understanding,
deepfake detection prioritizes image realism, which closely mirrors real-world
distributions. In this work, we propose the Few-shot Training-free Network
(FTNet) for real-world few-shot deepfake detection. Simple yet effective, FTNet
differs from traditional methods that rely on large-scale known data for
training. Instead, FTNet uses only one fake samplefrom an evaluation set,
mimicking the scenario where new samples emerge in the real world and can be
gathered for use, without any training or parameter updates. During evaluation,
each test sample is compared to the known fake and real samples, and it is
classified based on the category of the nearest sample. We conduct a
comprehensive analysis of AI-generated images from 29 different generative
models and achieve a new SoTA performance, with an average improvement of 8.7\%
compared to existing methods. This work introduces a fresh perspective on
real-world deepfake detection: when the model struggles to generalize on a
few-shot sample, leveraging the failed samples leads to better performance.

</details>


### [41] [From Large Angles to Consistent Faces: Identity-Preserving Video Generation via Mixture of Facial Experts](https://arxiv.org/abs/2508.09476)
*Yuji Wang,Moran Li,Xiaobin Hu,Ran Yi,Jiangning Zhang,Chengming Xu,Weijian Cao,Yabiao Wang,Chengjie Wang,Lizhuang Ma*

Main category: cs.CV

TL;DR: 提出了一种动态混合专家模型（MoFE）和专门的数据处理流程，以解决视频生成中身份保持和大面部角度的问题，并创建了LFA数据集。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在大面部角度下难以保持身份一致性，且缺乏相关数据集。

Method: 引入MoFE模型，结合三个专家模块（身份、语义、细节），并设计数据处理流程（面部约束和身份一致性）以构建LFA数据集。

Result: 在LFA基准测试中，方法在面部相似性、FID和CLIP语义对齐上显著优于现有方法。

Conclusion: MoFE和LFA数据集有效解决了大面部角度下的身份保持问题，提升了视频生成质量。

Abstract: Current video generation models struggle with identity preservation under
large facial angles, primarily facing two challenges: the difficulty in
exploring an effective mechanism to integrate identity features into DiT
structure, and the lack of targeted coverage of large facial angles in existing
open-source video datasets. To address these, we present two key innovations.
First, we introduce a Mixture of Facial Experts (MoFE) that dynamically
combines complementary cues from three specialized experts, each designed to
capture distinct but mutually reinforcing aspects of facial attributes. The
identity expert captures cross-pose identity-sensitive features, the semantic
expert extracts high-level visual semantxics, and the detail expert preserves
pixel-level features (e.g., skin texture, color gradients). Furthermore, to
mitigate dataset limitations, we have tailored a data processing pipeline
centered on two key aspects: Face Constraints and Identity Consistency. Face
Constraints ensure facial angle diversity and a high proportion of facial
regions, while Identity Consistency preserves coherent person-specific features
across temporal sequences, collectively addressing the scarcity of large facial
angles and identity-stable training data in existing datasets. Leveraging this
pipeline, we have curated and refined a Large Face Angles (LFA) Dataset from
existing open-source human video datasets, comprising 460K video clips with
annotated facial angles. Experimental results on the LFA benchmark demonstrate
that our method, empowered by the LFA dataset, significantly outperforms prior
SOTA methods in face similarity, face FID, and CLIP semantic alignment. The
code and dataset will be made publicly available at
https://github.com/rain152/LFA-Video-Generation.

</details>


### [42] [CLIP-Flow: A Universal Discriminator for AI-Generated Images Inspired by Anomaly Detection](https://arxiv.org/abs/2508.09477)
*Zhipeng Yuan,Kai Wang,Weize Quan,Dong-Ming Yan,Tieru Wu*

Main category: cs.CV

TL;DR: 提出了一种基于异常检测的通用AI生成图像检测器，无需访问任何AI生成图像，通过无监督学习实现泛化。


<details>
  <summary>Details</summary>
Motivation: AI生成图像质量接近自然图像，现有检测器对未见过的生成模型性能有限。

Method: 使用预训练CLIP编码器提取特征，设计类似归一化流的无监督模型，利用代理图像训练。

Result: 在多种图像生成器生成的图像上验证了方法的有效性。

Conclusion: 该方法通过异常检测和无监督学习，实现了对未见生成模型的通用检测。

Abstract: With the rapid advancement of AI generative models, the visual quality of
AI-generated images (AIIs) has become increasingly close to natural images,
which inevitably raises security concerns. Most AII detectors often employ the
conventional image classification pipeline with natural images and AIIs
(generated by a generative model), which can result in limited detection
performance for AIIs from unseen generative models. To solve this, we proposed
a universal AI-generated image detector from the perspective of anomaly
detection. Our discriminator does not need to access any AIIs and learn a
generalizable representation with unsupervised learning. Specifically, we use
the pre-trained CLIP encoder as the feature extractor and design a normalizing
flow-like unsupervised model. Instead of AIIs, proxy images, e.g., obtained by
applying a spectral modification operation on natural images, are used for
training. Our models are trained by minimizing the likelihood of proxy images,
optionally combined with maximizing the likelihood of natural images. Extensive
experiments demonstrate the effectiveness of our method on AIIs produced by
various image generators.

</details>


### [43] [GazeLT: Visual attention-guided long-tailed disease classification in chest radiographs](https://arxiv.org/abs/2508.09478)
*Moinak Bhattacharya,Gagandeep Singh,Shubham Jain,Prateek Prasanna*

Main category: cs.CV

TL;DR: GazeLT是一种结合放射科医生视觉注意力的方法，用于长尾疾病分类，显著提升了分类准确率。


<details>
  <summary>Details</summary>
Motivation: 放射科医生的视觉注意力模式包含细粒度和粗粒度疾病信息，将其融入深度学习框架可提升自动化图像解读能力。

Method: 通过整合和解构视觉搜索过程的时序特征，GazeLT改进了长尾疾病分类。

Result: 在NIH-CXR-LT和MIMIC-CXR-LT数据集上，GazeLT比最佳长尾损失方法提升4.1%，比视觉注意力基线提升21.7%。

Conclusion: GazeLT通过利用视觉注意力时序特征，显著提升了长尾疾病分类的性能。

Abstract: In this work, we present GazeLT, a human visual attention
integration-disintegration approach for long-tailed disease classification. A
radiologist's eye gaze has distinct patterns that capture both fine-grained and
coarser level disease related information. While interpreting an image, a
radiologist's attention varies throughout the duration; it is critical to
incorporate this into a deep learning framework to improve automated image
interpretation. Another important aspect of visual attention is that apart from
looking at major/obvious disease patterns, experts also look at
minor/incidental findings (few of these constituting long-tailed classes)
during the course of image interpretation. GazeLT harnesses the temporal aspect
of the visual search process, via an integration and disintegration mechanism,
to improve long-tailed disease classification. We show the efficacy of GazeLT
on two publicly available datasets for long-tailed disease classification,
namely the NIH-CXR-LT (n=89237) and the MIMIC-CXR-LT (n=111898) datasets.
GazeLT outperforms the best long-tailed loss by 4.1% and the visual
attention-based baseline by 21.7% in average accuracy metrics for these
datasets. Our code is available at https://github.com/lordmoinak1/gazelt.

</details>


### [44] [SkySplat: Generalizable 3D Gaussian Splatting from Multi-Temporal Sparse Satellite Images](https://arxiv.org/abs/2508.09479)
*Xuejun Huang,Xinyi Liu,Yi Wan,Zhi Zheng,Bin Zhang,Mingtao Xiong,Yingying Pei,Yongjun Zhang*

Main category: cs.CV

TL;DR: SkySplat是一种自监督框架，通过将RPC模型集成到通用3DGS流程中，显著提升了稀疏卫星图像的三维重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在卫星图像上表现不佳，主要由于与RPC模型不兼容以及泛化能力有限。

Method: SkySplat结合了RPC模型和通用3DGS流程，引入跨自一致性模块（CSCM）和多视角一致性聚合策略。

Result: 在DFC19数据集上，MAE从13.18米降至1.80米，速度提升86倍，并在MVS3D基准测试中表现出强泛化能力。

Conclusion: SkySplat通过自监督和几何约束优化，显著提升了稀疏卫星图像的三维重建效率和精度。

Abstract: Three-dimensional scene reconstruction from sparse-view satellite images is a
long-standing and challenging task. While 3D Gaussian Splatting (3DGS) and its
variants have recently attracted attention for its high efficiency, existing
methods remain unsuitable for satellite images due to incompatibility with
rational polynomial coefficient (RPC) models and limited generalization
capability. Recent advances in generalizable 3DGS approaches show potential,
but they perform poorly on multi-temporal sparse satellite images due to
limited geometric constraints, transient objects, and radiometric
inconsistencies. To address these limitations, we propose SkySplat, a novel
self-supervised framework that integrates the RPC model into the generalizable
3DGS pipeline, enabling more effective use of sparse geometric cues for
improved reconstruction. SkySplat relies only on RGB images and
radiometric-robust relative height supervision, thereby eliminating the need
for ground-truth height maps. Key components include a Cross-Self Consistency
Module (CSCM), which mitigates transient object interference via
consistency-based masking, and a multi-view consistency aggregation strategy
that refines reconstruction results. Compared to per-scene optimization
methods, SkySplat achieves an 86 times speedup over EOGS with higher accuracy.
It also outperforms generalizable 3DGS baselines, reducing MAE from 13.18 m to
1.80 m on the DFC19 dataset significantly, and demonstrates strong
cross-dataset generalization on the MVS3D benchmark.

</details>


### [45] [Episodic Memory Representation for Long-form Video Understanding](https://arxiv.org/abs/2508.09486)
*Yun Wang,Long Zhang,Jingren Liu,Jiaqi Yan,Zhanjie Zhang,Jiahao Zheng,Xun Yang,Dapeng Wu,Xiangyu Chen,Xuelong Li*

Main category: cs.CV

TL;DR: Video-EM是一种无需训练的框架，通过模拟人类情景记忆，解决长视频理解中关键帧冗余和时空关系缺失的问题，显著提升视频问答性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理长视频时因上下文窗口限制，仅关注静态关键帧匹配，忽略了时空关系和场景连续性，导致信息冗余和关键线索丢失。

Method: Video-EM将关键帧建模为时序事件，结合空间关系和动态时序，并利用LLM的思维链迭代筛选最小但信息丰富的记忆子集。

Result: 在多个基准测试中，Video-EM性能提升4-9%，且使用更少帧数。

Conclusion: Video-EM通过情景记忆和思维链优化，显著提升了长视频理解的准确性和效率。

Abstract: Video Large Language Models (Video-LLMs) excel at general video understanding
but struggle with long-form videos due to context window limits. Consequently,
recent approaches focus on keyframe retrieval, condensing lengthy videos into a
small set of informative frames. Despite their practicality, these methods
simplify the problem to static text image matching, overlooking spatio temporal
relationships crucial for capturing scene transitions and contextual
continuity, and may yield redundant keyframes with limited information,
diluting salient cues essential for accurate video question answering. To
address these limitations, we introduce Video-EM, a training free framework
inspired by the principles of human episodic memory, designed to facilitate
robust and contextually grounded reasoning. Rather than treating keyframes as
isolated visual entities, Video-EM explicitly models them as temporally ordered
episodic events, capturing both spatial relationships and temporal dynamics
necessary for accurately reconstructing the underlying narrative. Furthermore,
the framework leverages chain of thought (CoT) thinking with LLMs to
iteratively identify a minimal yet highly informative subset of episodic
memories, enabling efficient and accurate question answering by Video-LLMs.
Extensive evaluations on the Video-MME, EgoSchema, HourVideo, and LVBench
benchmarks confirm the superiority of Video-EM, which achieves highly
competitive results with performance gains of 4-9 percent over respective
baselines while utilizing fewer frames.

</details>


### [46] [SARE: Semantic-Aware Reconstruction Error for Generalizable Diffusion-Generated Image Detection](https://arxiv.org/abs/2508.09487)
*Ju Yeon Kang,Jaehong Park,Semin Kim,Ji Won Yoon,Nam Soo Kim*

Main category: cs.CV

TL;DR: 提出了一种基于语义感知重建误差（SARE）的方法，用于检测扩散生成的图像，解决了现有方法在面对未见过的生成模型时性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法依赖模型特定的伪影，在面对未见过的生成模型时性能显著下降。假图像与描述文本的相似性高于真实图像，这一现象启发了SARE的提出。

Method: 通过测量图像与其描述文本引导重建之间的语义差异（SARE），量化语义变化，作为检测特征。

Result: 在GenImage和CommunityForensics基准测试中，SARE表现出强大的泛化能力，优于现有基线方法。

Conclusion: SARE是一种鲁棒的检测特征，能够有效区分真实与假图像，尤其在面对未见过的生成模型时表现优异。

Abstract: Recently, diffusion-generated image detection has gained increasing
attention, as the rapid advancement of diffusion models has raised serious
concerns about their potential misuse. While existing detection methods have
achieved promising results, their performance often degrades significantly when
facing fake images from unseen, out-of-distribution (OOD) generative models,
since they primarily rely on model-specific artifacts. To address this
limitation, we explore a fundamental property commonly observed in fake images.
Motivated by the observation that fake images tend to exhibit higher similarity
to their captions than real images, we propose a novel representation, namely
Semantic-Aware Reconstruction Error (SARE), that measures the semantic
difference between an image and its caption-guided reconstruction. The
hypothesis behind SARE is that real images, whose captions often fail to fully
capture their complex visual content, may undergo noticeable semantic shifts
during the caption-guided reconstruction process. In contrast, fake images,
which closely align with their captions, show minimal semantic changes. By
quantifying these semantic shifts, SARE can be utilized as a discriminative
feature for robust detection across diverse generative models. We empirically
demonstrate that the proposed method exhibits strong generalization,
outperforming existing baselines on benchmarks including GenImage and
CommunityForensics.

</details>


### [47] [CWFBind: Geometry-Awareness for Fast and Accurate Protein-Ligand Docking](https://arxiv.org/abs/2508.09499)
*Liyan Jia,Chuan-Xian Ren,Hong Yan*

Main category: cs.CV

TL;DR: CWFBind是一种基于局部曲率特征的快速准确对接方法，通过几何信息增强和动态半径策略提升小分子配体与蛋白质结合的预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖图表示和语言模型编码器，忽略了关键几何信息，导致口袋定位和结合构象不准确。

Method: CWFBind整合局部曲率描述符，增强蛋白质和配体的几何表示，并嵌入度感知加权机制，改进消息传递过程。

Result: CWFBind在多个对接基准测试中表现优异，平衡了准确性和效率。

Conclusion: CWFBind通过几何信息增强和动态策略，显著提升了小分子对接的预测性能。

Abstract: Accurately predicting the binding conformation of small-molecule ligands to
protein targets is a critical step in rational drug design. Although recent
deep learning-based docking surpasses traditional methods in speed and
accuracy, many approaches rely on graph representations and language
model-inspired encoders while neglecting critical geometric information,
resulting in inaccurate pocket localization and unrealistic binding
conformations. In this study, we introduce CWFBind, a weighted, fast, and
accurate docking method based on local curvature features. Specifically, we
integrate local curvature descriptors during the feature extraction phase to
enrich the geometric representation of both proteins and ligands, complementing
existing chemical, sequence, and structural features. Furthermore, we embed
degree-aware weighting mechanisms into the message passing process, enhancing
the model's ability to capture spatial structural distinctions and interaction
strengths. To address the class imbalance challenge in pocket prediction,
CWFBind employs a ligand-aware dynamic radius strategy alongside an enhanced
loss function, facilitating more precise identification of binding regions and
key residues. Comprehensive experimental evaluations demonstrate that CWFBind
achieves competitive performance across multiple docking benchmarks, offering a
balanced trade-off between accuracy and efficiency.

</details>


### [48] [Generation of Indian Sign Language Letters, Numbers, and Words](https://arxiv.org/abs/2508.09522)
*Ajeet Kumar Yadav,Nishant Kumar,Rathna G N*

Main category: cs.CV

TL;DR: 提出了一种结合ProGAN和SAGAN的GAN变体，用于生成高质量、高分辨率的印度手语图像，并在IS和FID指标上优于传统ProGAN。


<details>
  <summary>Details</summary>
Motivation: 手语生成领域仍需探索，现有模型在分辨率和细节平衡上存在不足。

Method: 结合ProGAN和SAGAN的优点，开发了一种改进的基于注意力的GAN模型。

Result: 在印度手语字母、数字和单词图像生成上，IS和FID分别提高了3.2和30.12。

Conclusion: 提出的模型在手语图像生成上表现优异，并发布了一个大型印度手语数据集。

Abstract: Sign language, which contains hand movements, facial expressions and bodily
gestures, is a significant medium for communicating with hard-of-hearing
people. A well-trained sign language community communicates easily, but those
who don't know sign language face significant challenges. Recognition and
generation are basic communication methods between hearing and hard-of-hearing
individuals. Despite progress in recognition, sign language generation still
needs to be explored. The Progressive Growing of Generative Adversarial Network
(ProGAN) excels at producing high-quality images, while the Self-Attention
Generative Adversarial Network (SAGAN) generates feature-rich images at medium
resolutions. Balancing resolution and detail is crucial for sign language image
generation. We are developing a Generative Adversarial Network (GAN) variant
that combines both models to generate feature-rich, high-resolution, and
class-conditional sign language images. Our modified Attention-based model
generates high-quality images of Indian Sign Language letters, numbers, and
words, outperforming the traditional ProGAN in Inception Score (IS) and
Fr\'echet Inception Distance (FID), with improvements of 3.2 and 30.12,
respectively. Additionally, we are publishing a large dataset incorporating
high-quality images of Indian Sign Language alphabets, numbers, and 129 words.

</details>


### [49] [SOI is the Root of All Evil: Quantifying and Breaking Similar Object Interference in Single Object Tracking](https://arxiv.org/abs/2508.09524)
*Yipei Wang,Shiyu Hu,Shukun Jia,Panxi Xu,Hongfei Ma,Yiping Ma,Jing Zhang,Xiaobo Lu,Xin Zhao*

Main category: cs.CV

TL;DR: 本文首次系统研究了单目标跟踪中的相似物体干扰（SOI），并提出了一种利用自然语言作为外部认知指导的新方法，显著提升了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 相似物体干扰（SOI）是单目标跟踪（SOT）中长期被忽视的关键瓶颈，本文旨在量化其影响并探索外部认知指导的可行性。

Method: 通过在线干扰掩蔽（OIM）实验量化SOI的影响，构建SOIBench基准测试，并提出基于大规模视觉语言模型（VLM）的新方法。

Result: 消除干扰源显著提升跟踪性能（AUC增益达4.35），新方法在语义认知指导下AUC增益达0.93，优于现有方法。

Conclusion: SOIBench可作为标准化评估平台，推动语义认知跟踪研究，并为跟踪领域提供新见解。

Abstract: In this paper, we present the first systematic investigation and
quantification of Similar Object Interference (SOI), a long-overlooked yet
critical bottleneck in Single Object Tracking (SOT). Through controlled Online
Interference Masking (OIM) experiments, we quantitatively demonstrate that
eliminating interference sources leads to substantial performance improvements
(AUC gains up to 4.35) across all SOTA trackers, directly validating SOI as a
primary constraint for robust tracking and highlighting the feasibility of
external cognitive guidance. Building upon these insights, we adopt natural
language as a practical form of external guidance, and construct SOIBench-the
first semantic cognitive guidance benchmark specifically targeting SOI
challenges. It automatically mines SOI frames through multi-tracker collective
judgment and introduces a multi-level annotation protocol to generate precise
semantic guidance texts. Systematic evaluation on SOIBench reveals a striking
finding: existing vision-language tracking (VLT) methods fail to effectively
exploit semantic cognitive guidance, achieving only marginal improvements or
even performance degradation (AUC changes of -0.26 to +0.71). In contrast, we
propose a novel paradigm employing large-scale vision-language models (VLM) as
external cognitive engines that can be seamlessly integrated into arbitrary RGB
trackers. This approach demonstrates substantial improvements under semantic
cognitive guidance (AUC gains up to 0.93), representing a significant
advancement over existing VLT methods. We hope SOIBench will serve as a
standardized evaluation platform to advance semantic cognitive tracking
research and contribute new insights to the tracking research community.

</details>


### [50] [Learning Spatial Decay for Vision Transformers](https://arxiv.org/abs/2508.09525)
*Yuxin Mao,Zhen Qin,Jinxing Zhou,Bin Fan,Jing Zhang,Yiran Zhong,Yuchao Dai*

Main category: cs.CV

TL;DR: SDT引入了一种新的上下文感知门控机制，通过动态、数据依赖的空间衰减优化视觉Transformer的空间注意力。


<details>
  <summary>Details</summary>
Motivation: ViTs的自注意力机制缺乏显式的空间归纳偏置，导致在空间结构化任务上表现不佳。现有方法基于固定距离度量的空间衰减限制了适应性。

Method: 提出了SDT，采用上下文感知门控机制（CAG），结合曼哈顿距离的空间先验和学习的内容表示，动态调整空间注意力。

Result: 在ImageNet-1K分类和生成任务上表现优于基线模型。

Conclusion: SDT为视觉Transformer的空间注意力优化提供了新范式。

Abstract: Vision Transformers (ViTs) have revolutionized computer vision, yet their
self-attention mechanism lacks explicit spatial inductive biases, leading to
suboptimal performance on spatially-structured tasks. Existing approaches
introduce data-independent spatial decay based on fixed distance metrics,
applying uniform attention weighting regardless of image content and limiting
adaptability to diverse visual scenarios. Inspired by recent advances in large
language models where content-aware gating mechanisms (e.g., GLA, HGRN2, FOX)
significantly outperform static alternatives, we present the first successful
adaptation of data-dependent spatial decay to 2D vision transformers. We
introduce \textbf{Spatial Decay Transformer (SDT)}, featuring a novel
Context-Aware Gating (CAG) mechanism that generates dynamic, data-dependent
decay for patch interactions. Our approach learns to modulate spatial attention
based on both content relevance and spatial proximity. We address the
fundamental challenge of 1D-to-2D adaptation through a unified spatial-content
fusion framework that integrates manhattan distance-based spatial priors with
learned content representations. Extensive experiments on ImageNet-1K
classification and generation tasks demonstrate consistent improvements over
strong baselines. Our work establishes data-dependent spatial decay as a new
paradigm for enhancing spatial attention in vision transformers.

</details>


### [51] [Physics-guided Deep Unfolding Network for Enhanced Kronecker Compressive sensing](https://arxiv.org/abs/2508.09528)
*Gang Qu,Ping Wang,Siming Zheng,Xin Yuan*

Main category: cs.CV

TL;DR: 提出了一种新的非对称Kronecker压缩感知（AKCS）模型和测量感知交叉注意力（MACA）机制，结合展开网络架构，显著提升了图像压缩感知的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在测量阶段缺乏非相干性，重建阶段缺乏显式测量表示，限制了整体性能。

Method: 提出AKCS模型提高测量非相干性，MACA机制学习隐式测量表示，并集成到展开网络中。

Result: MEUNet在重建精度和推理速度上达到最优性能。

Conclusion: AKCS和MACA的结合显著提升了压缩感知任务的性能。

Abstract: Deep networks have achieved remarkable success in image compressed sensing
(CS) task, namely reconstructing a high-fidelity image from its compressed
measurement. However, existing works are deficient inincoherent compressed
measurement at sensing phase and implicit measurement representations at
reconstruction phase, limiting the overall performance. In this work, we answer
two questions: 1) how to improve the measurement incoherence for decreasing the
ill-posedness; 2) how to learn informative representations from measurements.
To this end, we propose a novel asymmetric Kronecker CS (AKCS) model and
theoretically present its better incoherence than previous Kronecker CS with
minimal complexity increase. Moreover, we reveal that the unfolding networks'
superiority over non-unfolding ones result from sufficient gradient descents,
called explicit measurement representations. We propose a measurement-aware
cross attention (MACA) mechanism to learn implicit measurement representations.
We integrate AKCS and MACA into widely-used unfolding architecture to get a
measurement-enhanced unfolding network (MEUNet). Extensive experiences
demonstrate that our MEUNet achieves state-of-the-art performance in
reconstruction accuracy and inference speed.

</details>


### [52] [COXNet: Cross-Layer Fusion with Adaptive Alignment and Scale Integration for RGBT Tiny Object Detection](https://arxiv.org/abs/2508.09533)
*Peiran Peng,Tingfa Xu,Liqiang Song,Mengqi Zhu,Yuqiang Fang,Jianan Li*

Main category: cs.CV

TL;DR: COXNet是一种用于RGBT微小目标检测的新框架，通过跨层融合、动态对齐和优化标签分配策略，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 在无人机场景中，RGBT图像中的微小目标检测面临空间错位、低光照和复杂背景等挑战，现有方法难以有效利用多模态信息。

Method: COXNet提出三个核心创新：跨层融合模块、动态对齐与尺度细化模块，以及基于GeoShape相似性度量的标签分配策略。

Result: 在RGBTDronePerson数据集上，COXNet比现有方法提升了3.32%的mAP50。

Conclusion: COXNet在复杂环境中表现出色，为RGBT微小目标检测提供了有效解决方案。

Abstract: Detecting tiny objects in multimodal Red-Green-Blue-Thermal (RGBT) imagery is
a critical challenge in computer vision, particularly in surveillance, search
and rescue, and autonomous navigation. Drone-based scenarios exacerbate these
challenges due to spatial misalignment, low-light conditions, occlusion, and
cluttered backgrounds. Current methods struggle to leverage the complementary
information between visible and thermal modalities effectively. We propose
COXNet, a novel framework for RGBT tiny object detection, addressing these
issues through three core innovations: i) the Cross-Layer Fusion Module, fusing
high-level visible and low-level thermal features for enhanced semantic and
spatial accuracy; ii) the Dynamic Alignment and Scale Refinement module,
correcting cross-modal spatial misalignments and preserving multi-scale
features; and iii) an optimized label assignment strategy using the GeoShape
Similarity Measure for better localization. COXNet achieves a 3.32\% mAP$_{50}$
improvement on the RGBTDronePerson dataset over state-of-the-art methods,
demonstrating its effectiveness for robust detection in complex environments.

</details>


### [53] [Iterative Volume Fusion for Asymmetric Stereo Matching](https://arxiv.org/abs/2508.09543)
*Yuanting Gao,Linghao Shen*

Main category: cs.CV

TL;DR: 提出了一种用于非对称立体匹配的两阶段迭代体积融合网络（IVF-AStereo），通过融合两种成本体积来解决视觉不对称问题。


<details>
  <summary>Details</summary>
Motivation: 传统立体匹配算法假设双目视觉对称，但非对称多相机系统（如长焦-广角相机）的兴起打破了这一假设，导致匹配困难。

Method: 首先通过聚合拼接体积优化相关体积，随后融合两种体积以增强细节。

Result: 在非对称场景下表现优异，对显著的视觉不对称具有鲁棒性。

Conclusion: 实验证明该方法在分辨率和颜色退化情况下有效解决了非对称立体匹配问题。

Abstract: Stereo matching is vital in 3D computer vision, with most algorithms assuming
symmetric visual properties between binocular visions. However, the rise of
asymmetric multi-camera systems (e.g., tele-wide cameras) challenges this
assumption and complicates stereo matching. Visual asymmetry disrupts stereo
matching by affecting the crucial cost volume computation. To address this, we
explore the matching cost distribution of two established cost volume
construction methods in asymmetric stereo. We find that each cost volume
experiences distinct information distortion, indicating that both should be
comprehensively utilized to solve the issue. Based on this, we propose the
two-phase Iterative Volume Fusion network for Asymmetric Stereo matching
(IVF-AStereo). Initially, the aggregated concatenation volume refines the
correlation volume. Subsequently, both volumes are fused to enhance fine
details. Our method excels in asymmetric scenarios and shows robust performance
against significant visual asymmetry. Extensive comparative experiments on
benchmark datasets, along with ablation studies, confirm the effectiveness of
our approach in asymmetric stereo with resolution and color degradation.

</details>


### [54] [GoViG: Goal-Conditioned Visual Navigation Instruction Generation](https://arxiv.org/abs/2508.09547)
*Fengyi Wu,Yifei Dong,Zhi-Qi Cheng,Yilong Dai,Guangyu Chen,Hang Wang,Qi Dai,Alexander G. Hauptmann*

Main category: cs.CV

TL;DR: GoViG is a new task for generating navigation instructions from raw egocentric visual data, improving adaptability to unseen environments. It decomposes the task into visual forecasting and instruction generation, integrated via a multimodal model, and shows superior performance.


<details>
  <summary>Details</summary>
Motivation: To autonomously generate precise navigation instructions from visual observations without relying on structured inputs like maps or annotations.

Method: Decomposes the task into visual forecasting and instruction generation, integrated in an autoregressive multimodal model with tailored objectives. Introduces one-pass and interleaved reasoning strategies.

Result: Achieves superior BLEU-4 and CIDEr scores, with robust cross-domain generalization.

Conclusion: GoViG effectively generates coherent navigation instructions from raw visual data, outperforming existing methods.

Abstract: We introduce Goal-Conditioned Visual Navigation Instruction Generation
(GoViG), a new task that aims to autonomously generate precise and contextually
coherent navigation instructions solely from egocentric visual observations of
initial and goal states. Unlike conventional approaches that rely on structured
inputs such as semantic annotations or environmental maps, GoViG exclusively
leverages raw egocentric visual data, substantially improving its adaptability
to unseen and unstructured environments. Our method addresses this task by
decomposing it into two interconnected subtasks: (1) visual forecasting, which
predicts intermediate visual states bridging the initial and goal views; and
(2) instruction generation, which synthesizes linguistically coherent
instructions grounded in both observed and anticipated visuals. These subtasks
are integrated within an autoregressive multimodal large language model trained
with tailored objectives to ensure spatial accuracy and linguistic clarity.
Furthermore, we introduce two complementary multimodal reasoning strategies,
one-pass and interleaved reasoning, to mimic incremental human cognitive
processes during navigation. To evaluate our method, we propose the R2R-Goal
dataset, combining diverse synthetic and real-world trajectories. Empirical
results demonstrate significant improvements over state-of-the-art methods,
achieving superior BLEU-4 and CIDEr scores along with robust cross-domain
generalization.

</details>


### [55] [Exploring the Equivalence of Closed-Set Generative and Real Data Augmentation in Image Classification](https://arxiv.org/abs/2508.09550)
*Haowen Wang,Guowei Zhang,Xiang Zhang,Zeyuan Chen,Haiyang Xu,Dou Hoon Kwark,Zhuowen Tu*

Main category: cs.CV

TL;DR: 论文探讨了在图像分类任务中，使用生成模型增强分类性能的有效性，并量化了合成数据与真实数据在增强中的等效性。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中的一个关键问题：是否可以通过生成模型在训练集上生成合成数据来提升分类性能。

Method: 通过实验比较真实图像与生成模型生成的合成图像，系统分析了合成数据在增强中的使用效果。

Result: 实证确定了合成图像在增强中的等效规模，并展示了合成数据与真实数据增强的定量等价性。

Conclusion: 研究结果为合成数据增强提供了量化指导，并展示了其在自然和医学图像数据集上的应用效果。

Abstract: In this paper, we address a key scientific problem in machine learning: Given
a training set for an image classification task, can we train a generative
model on this dataset to enhance the classification performance? (i.e.,
closed-set generative data augmentation). We start by exploring the
distinctions and similarities between real images and closed-set synthetic
images generated by advanced generative models. Through extensive experiments,
we offer systematic insights into the effective use of closed-set synthetic
data for augmentation. Notably, we empirically determine the equivalent scale
of synthetic images needed for augmentation. In addition, we also show
quantitative equivalence between the real data augmentation and open-set
generative augmentation (generative models trained using data beyond the given
training set). While it aligns with the common intuition that real images are
generally preferred, our empirical formulation also offers a guideline to
quantify the increased scale of synthetic data augmentation required to achieve
comparable image classification performance. Our results on natural and medical
image datasets further illustrate how this effect varies with the baseline
training set size and the amount of synthetic data incorporated.

</details>


### [56] [Topological Invariant-Based Iris Identification via Digital Homology and Machine Learning](https://arxiv.org/abs/2508.09555)
*Ahmet Öztel,İsmet Karaca*

Main category: cs.CV

TL;DR: 提出了一种基于拓扑不变量的虹膜识别方法，性能优于传统CNN，且具有可解释性。


<details>
  <summary>Details</summary>
Motivation: 探索一种基于形式化数字同调的拓扑不变量方法，以提供紧凑、可解释且准确的虹膜识别替代方案。

Method: 将归一化虹膜图像分块，计算每块的Betti0、Betti1及其比率，形成特征矩阵，结合逻辑回归等分类器。

Result: 逻辑回归准确率达97.78%，优于CNN（96.44%），且特征方差低。

Conclusion: 该方法首次将形式化数字同调用于虹膜识别，适用于多种领域，尤其在需要可解释性或数据有限时表现优异。

Abstract: Objective - This study presents a biometric identification method based on
topological invariants from 2D iris images, representing iris texture via
formally defined digital homology and evaluating classification performance.
  Methods - Each normalized iris image (48x482 pixels) is divided into grids
(e.g., 6x54 or 3x27). For each subregion, we compute Betti0, Betti1, and their
ratio using a recent algorithm for homology groups in 2D digital images. The
resulting invariants form a feature matrix used with logistic regression, KNN,
and SVM (with PCA and 100 randomized repetitions). A convolutional neural
network (CNN) is trained on raw images for comparison.
  Results - Logistic regression achieved 97.78 +/- 0.82% accuracy,
outperforming CNN (96.44 +/- 1.32%) and other feature-based models. The
topological features showed high accuracy with low variance.
  Conclusion - This is the first use of topological invariants from formal
digital homology for iris recognition. The method offers a compact,
interpretable, and accurate alternative to deep learning, useful when
explainability or limited data is important. Beyond iris recognition, it can
apply to other biometrics, medical imaging, materials science, remote sensing,
and interpretable AI. It runs efficiently on CPU-only systems and produces
robust, explainable features valuable for security-critical domains.

</details>


### [57] [WeatherPrompt: Multi-modality Representation Learning for All-Weather Drone Visual Geo-Localization](https://arxiv.org/abs/2508.09560)
*Jiahao Wen,Hang Yu,Zhedong Zheng*

Main category: cs.CV

TL;DR: WeatherPrompt通过多模态学习范式，融合图像嵌入与文本上下文，建立天气不变的表示，显著提升了无人机在复杂天气条件下的地理定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在天气扰动（如雨、雾）下性能显著下降，主要受限于对有限天气类别的依赖以及场景-天气特征解耦不足。

Method: 提出Training-free Weather Reasoning机制和动态门控多模态框架，通过文本嵌入自适应重加权和融合视觉特征，并结合跨模态目标优化。

Result: 在多样天气条件下，Recall@1在夜间提升13.37%，在雾和雪条件下提升18.69%。

Conclusion: WeatherPrompt通过多模态融合和动态门控机制，有效解决了天气扰动下的地理定位问题，性能优于现有方法。

Abstract: Visual geo-localization for drones faces critical degradation under weather
perturbations, \eg, rain and fog, where existing methods struggle with two
inherent limitations: 1) Heavy reliance on limited weather categories that
constrain generalization, and 2) Suboptimal disentanglement of entangled
scene-weather features through pseudo weather categories. We present
WeatherPrompt, a multi-modality learning paradigm that establishes
weather-invariant representations through fusing the image embedding with the
text context. Our framework introduces two key contributions: First, a
Training-free Weather Reasoning mechanism that employs off-the-shelf large
multi-modality models to synthesize multi-weather textual descriptions through
human-like reasoning. It improves the scalability to unseen or complex weather,
and could reflect different weather strength. Second, to better disentangle the
scene and weather feature, we propose a multi-modality framework with the
dynamic gating mechanism driven by the text embedding to adaptively reweight
and fuse visual features across modalities. The framework is further optimized
by the cross-modal objectives, including image-text contrastive learning and
image-text matching, which maps the same scene with different weather
conditions closer in the respresentation space. Extensive experiments validate
that, under diverse weather conditions, our method achieves competitive recall
rates compared to state-of-the-art drone geo-localization methods. Notably, it
improves Recall@1 by +13.37\% under night conditions and by 18.69\% under fog
and snow conditions.

</details>


### [58] [WEC-DG: Multi-Exposure Wavelet Correction Method Guided by Degradation Description](https://arxiv.org/abs/2508.09565)
*Ming Zhao,Pingping Liu,Tongshun Zhang,Zhe Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于小波的曝光校正方法（WEC-DG），通过退化描述符和小波变换解决现有方法在复杂光照条件下的适应性不足问题。


<details>
  <summary>Details</summary>
Motivation: 当前多曝光校正方法在处理单一曝光图像时，因光照、环境和天气多样性导致类内变异性问题，影响校正效果。

Method: 引入退化描述符确保曝光一致性，并利用小波变换的光-细节解耦特性设计模块，分别处理低频和高频信息。

Result: 在多个公开数据集上验证，性能优于现有算法，显著提升了校正效果和细节恢复。

Conclusion: WEC-DG方法在复杂光照条件下具有高效性和实用性，解决了现有方法的局限性。

Abstract: Multi-exposure correction technology is essential for restoring images
affected by insufficient or excessive lighting, enhancing the visual experience
by improving brightness, contrast, and detail richness. However, current
multi-exposure correction methods often encounter challenges in addressing
intra-class variability caused by diverse lighting conditions, shooting
environments, and weather factors, particularly when processing images captured
at a single exposure level. To enhance the adaptability of these models under
complex imaging conditions, this paper proposes a Wavelet-based Exposure
Correction method with Degradation Guidance (WEC-DG). Specifically, we
introduce a degradation descriptor within the Exposure Consistency Alignment
Module (ECAM) at both ends of the processing pipeline to ensure exposure
consistency and achieve final alignment. This mechanism effectively addresses
miscorrected exposure anomalies caused by existing methods' failure to
recognize 'blurred' exposure degradation. Additionally, we investigate the
light-detail decoupling properties of the wavelet transform to design the
Exposure Restoration and Detail Reconstruction Module (EDRM), which processes
low-frequency information related to exposure enhancement before utilizing
high-frequency information as a prior guide for reconstructing spatial domain
details. This serial processing strategy guarantees precise light correction
and enhances detail recovery. Extensive experiments conducted on multiple
public datasets demonstrate that the proposed method outperforms existing
algorithms, achieving significant performance improvements and validating its
effectiveness and practical applicability.

</details>


### [59] [A Chain of Diagnosis Framework for Accurate and Explainable Radiology Report Generation](https://arxiv.org/abs/2508.09566)
*Haibo Jin,Haoxuan Che,Sunan He,Hao Chen*

Main category: cs.CV

TL;DR: 论文提出了一种名为诊断链（CoD）的框架，旨在提高放射学报告生成（RRG）的临床准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有RRG方法在临床效果（尤其是病变属性描述）和结果可解释性方面表现不佳，难以获得放射科医生的信任。

Method: CoD框架通过生成诊断对话的问答对提取关键发现，并利用大语言模型生成准确报告，同时设计了诊断和病变定位模块以增强可解释性。

Result: CoD在多个RRG基准测试中优于专家和通用模型，并展示了通过问答诊断和图像定位生成句子的能力。

Conclusion: CoD框架显著提升了RRG的临床准确性和可解释性，为放射科医生提供了更高效的工具。

Abstract: Despite the progress of radiology report generation (RRG), existing works
face two challenges: 1) The performances in clinical efficacy are
unsatisfactory, especially for lesion attributes description; 2) the generated
text lacks explainability, making it difficult for radiologists to trust the
results. To address the challenges, we focus on a trustworthy RRG model, which
not only generates accurate descriptions of abnormalities, but also provides
basis of its predictions. To this end, we propose a framework named chain of
diagnosis (CoD), which maintains a chain of diagnostic process for clinically
accurate and explainable RRG. It first generates question-answer (QA) pairs via
diagnostic conversation to extract key findings, then prompts a large language
model with QA diagnoses for accurate generation. To enhance explainability, a
diagnosis grounding module is designed to match QA diagnoses and generated
sentences, where the diagnoses act as a reference. Moreover, a lesion grounding
module is designed to locate abnormalities in the image, further improving the
working efficiency of radiologists. To facilitate label-efficient training, we
propose an omni-supervised learning strategy with clinical consistency to
leverage various types of annotations from different datasets. Our efforts lead
to 1) an omni-labeled RRG dataset with QA pairs and lesion boxes; 2) a
evaluation tool for assessing the accuracy of reports in describing lesion
location and severity; 3) extensive experiments to demonstrate the
effectiveness of CoD, where it outperforms both specialist and generalist
models consistently on two RRG benchmarks and shows promising explainability by
accurately grounding generated sentences to QA diagnoses and images.

</details>


### [60] [Dual Recursive Feedback on Generation and Appearance Latents for Pose-Robust Text-to-Image Diffusion](https://arxiv.org/abs/2508.09575)
*Jiwon Kim,Pureum Kim,SeonHwa Kim,Soobin Park,Eunju Cha,Kyong Hwan Jin*

Main category: cs.CV

TL;DR: 提出了一种无需训练的双递归反馈（DRF）系统，用于改进可控文本到图像（T2I）模型的空间和外观控制。


<details>
  <summary>Details</summary>
Motivation: 现有可控T2I模型在保留空间结构和捕捉细粒度条件（如物体姿态和场景布局）方面表现不佳。

Method: 通过外观反馈和生成反馈的双递归机制，优化中间潜在表示，以更好地反映外观信息和用户意图。

Result: 实验表明，该方法能生成高质量、语义一致且结构一致的图像，例如将人类动作转移到老虎形态上。

Conclusion: DRF系统有效整合了结构和外观属性，提升了可控T2I模型的性能。

Abstract: Recent advancements in controllable text-to-image (T2I) diffusion models,
such as Ctrl-X and FreeControl, have demonstrated robust spatial and appearance
control without requiring auxiliary module training. However, these models
often struggle to accurately preserve spatial structures and fail to capture
fine-grained conditions related to object poses and scene layouts. To address
these challenges, we propose a training-free Dual Recursive Feedback (DRF)
system that properly reflects control conditions in controllable T2I models.
The proposed DRF consists of appearance feedback and generation feedback that
recursively refines the intermediate latents to better reflect the given
appearance information and the user's intent. This dual-update mechanism guides
latent representations toward reliable manifolds, effectively integrating
structural and appearance attributes. Our approach enables fine-grained
generation even between class-invariant structure-appearance fusion, such as
transferring human motion onto a tiger's form. Extensive experiments
demonstrate the efficacy of our method in producing high-quality, semantically
coherent, and structurally consistent image generations. Our source code is
available at https://github.com/jwonkm/DRF.

</details>


### [61] [SHALE: A Scalable Benchmark for Fine-grained Hallucination Evaluation in LVLMs](https://arxiv.org/abs/2508.09584)
*Bei Yan,Zhiyuan Chen,Yuecong Min,Jie Zhang,Jiahao Wang,Xiaozhen Wang,Shiguang Shan*

Main category: cs.CV

TL;DR: SHALE是一个可扩展的幻觉评估基准，用于细粒度评估大型视觉语言模型中的幻觉问题，包括忠实性和事实性幻觉。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）存在幻觉问题，现有评估方法在细粒度分析和数据扩展性方面不足。

Method: 提出自动化数据构建流程和分层幻觉诱导框架，构建包含30K图像-指令对的SHALE基准。

Result: 实验表明主流LVLMs存在显著事实性幻觉，并对语义扰动高度敏感。

Conclusion: SHALE为LVLMs的幻觉问题提供了细粒度、可扩展的评估方案。

Abstract: Despite rapid advances, Large Vision-Language Models (LVLMs) still suffer
from hallucinations, i.e., generating content inconsistent with input or
established world knowledge, which correspond to faithfulness and factuality
hallucinations, respectively. Prior studies primarily evaluate faithfulness
hallucination at a coarse level (e.g., object-level) and lack fine-grained
analysis. Additionally, existing benchmarks rely on costly manual curation or
reused public datasets, raising concerns about scalability and data leakage. To
address these limitations, we propose an automated data construction pipeline
that produces scalable, controllable, and diverse evaluation data. We also
design a hierarchical hallucination induction framework with input
perturbations to simulate realistic noisy scenarios. Integrating these designs,
we construct SHALE, a Scalable HALlucination Evaluation benchmark designed to
assess both faithfulness and factuality hallucinations via a fine-grained
hallucination categorization scheme. SHALE comprises over 30K image-instruction
pairs spanning 12 representative visual perception aspects for faithfulness and
6 knowledge domains for factuality, considering both clean and noisy scenarios.
Extensive experiments on over 20 mainstream LVLMs reveal significant factuality
hallucinations and high sensitivity to semantic perturbations.

</details>


### [62] [Offline Auto Labeling: BAAS](https://arxiv.org/abs/2508.09585)
*Stefan Haag,Bharanidhar Duraisamy,Felix Govaers,Wolfgang Koch,Martin Fritzsche,Juergen Dickmann*

Main category: cs.CV

TL;DR: BAAS是一个基于贝叶斯跟踪和融合的雷达检测标注框架，用于自动驾驶中的扩展目标跟踪和标注。


<details>
  <summary>Details</summary>
Motivation: 解决自动驾驶中雷达检测的精确轨迹和形状估计问题，提供多级监督下的标注标签。

Method: 利用贝叶斯跟踪、平滑和融合方法，结合手动标注数据进行模块化分析和闭环改进。

Result: 在复杂城市场景中验证了跟踪性能和标注误差，适用于多种动态目标和类别。

Conclusion: BAAS框架能有效提升自动驾驶中雷达检测的标注和跟踪性能。

Abstract: This paper introduces BAAS, a new Extended Object Tracking (EOT) and
fusion-based label annotation framework for radar detections in autonomous
driving. Our framework utilizes Bayesian-based tracking, smoothing and
eventually fusion methods to provide veritable and precise object trajectories
along with shape estimation to provide annotation labels on the detection level
under various supervision levels. Simultaneously, the framework provides
evaluation of tracking performance and label annotation. If manually labeled
data is available, each processing module can be analyzed independently or
combined with other modules to enable closed-loop continuous improvements. The
framework performance is evaluated in a challenging urban real-world scenario
in terms of tracking performance and the label annotation errors. We
demonstrate the functionality of the proposed approach for varying dynamic
objects and class types

</details>


### [63] [Hierarchical Brain Structure Modeling for Predicting Genotype of Glioma](https://arxiv.org/abs/2508.09593)
*Haotian Tang,Jianwei Chen,Xinrui Tang,Yunjia Wu,Zhengyang Miao,Chao Li*

Main category: cs.CV

TL;DR: Hi-SMGNN框架通过整合结构和形态连接组，结合多模态交互和多尺度特征融合，提升了IDH突变预测的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于功能MRI的低可用性和噪声，且忽略了大脑的层次结构和多尺度交互。

Method: 提出Hi-SMGNN框架，包含多模态交互模块、多尺度特征融合机制和个性化模块划分策略。

Result: 在UCSF-PDGM数据集上，Hi-SMGNN优于基线模型和现有最佳模型。

Conclusion: Hi-SMGNN在IDH突变预测中表现出更高的鲁棒性和有效性。

Abstract: Isocitrate DeHydrogenase (IDH) mutation status is a crucial biomarker for
glioma prognosis. However, current prediction methods are limited by the low
availability and noise of functional MRI. Structural and morphological
connectomes offer a non-invasive alternative, yet existing approaches often
ignore the brain's hierarchical organisation and multiscale interactions. To
address this, we propose Hi-SMGNN, a hierarchical framework that integrates
structural and morphological connectomes from regional to modular levels. It
features a multimodal interaction module with a Siamese network and cross-modal
attention, a multiscale feature fusion mechanism for reducing redundancy, and a
personalised modular partitioning strategy to enhance individual specificity
and interpretability. Experiments on the UCSF-PDGM dataset demonstrate that
Hi-SMGNN outperforms baseline and state-of-the-art models, showing improved
robustness and effectiveness in IDH mutation prediction.

</details>


### [64] [SVG-Head: Hybrid Surface-Volumetric Gaussians for High-Fidelity Head Reconstruction and Real-Time Editing](https://arxiv.org/abs/2508.09597)
*Heyi Sun,Cong Wang,Tian-Xing Xu,Jingwei Huang,Di Kang,Chunchao Guo,Song-Hai Zhang*

Main category: cs.CV

TL;DR: SVG-Head提出了一种新颖的混合表示方法，结合表面和体积高斯模型，实现了高保真渲染和实时外观编辑的头像生成。


<details>
  <summary>Details</summary>
Motivation: 当前头像编辑技术难以实现实时外观编辑，主要由于隐式表示和几何与外观的耦合建模。

Method: 使用3D高斯模型绑定FLAME网格，分离纹理图像捕获全局外观，并通过网格感知的UV映射优化纹理空间。

Result: 在NeRSemble数据集上，SVG-Head实现了高保真渲染，并首次支持实时外观编辑。

Conclusion: SVG-Head为头像编辑提供了高效且灵活的解决方案，推动了AR/VR应用的发展。

Abstract: Creating high-fidelity and editable head avatars is a pivotal challenge in
computer vision and graphics, boosting many AR/VR applications. While recent
advancements have achieved photorealistic renderings and plausible animation,
head editing, especially real-time appearance editing, remains challenging due
to the implicit representation and entangled modeling of the geometry and
global appearance. To address this, we propose Surface-Volumetric Gaussian Head
Avatar (SVG-Head), a novel hybrid representation that explicitly models the
geometry with 3D Gaussians bound on a FLAME mesh and leverages disentangled
texture images to capture the global appearance. Technically, it contains two
types of Gaussians, in which surface Gaussians explicitly model the appearance
of head avatars using learnable texture images, facilitating real-time texture
editing, while volumetric Gaussians enhance the reconstruction quality of
non-Lambertian regions (e.g., lips and hair). To model the correspondence
between 3D world and texture space, we provide a mesh-aware Gaussian UV mapping
method, which leverages UV coordinates given by the FLAME mesh to obtain sharp
texture images and real-time rendering speed. A hierarchical optimization
strategy is further designed to pursue the optimal performance in both
reconstruction quality and editing flexibility. Experiments on the NeRSemble
dataset show that SVG-Head not only generates high-fidelity rendering results,
but also is the first method to obtain explicit texture images for Gaussian
head avatars and support real-time appearance editing.

</details>


### [65] [Images Speak Louder Than Scores: Failure Mode Escape for Enhancing Generative Quality](https://arxiv.org/abs/2508.09598)
*Jie Shao,Ke Zhu,Minghao Fu,Guo-hua Wang,Jianxin Wu*

Main category: cs.CV

TL;DR: FaME是一种无需训练、高效推理的方法，通过识别低质量生成图像并利用其采样轨迹作为负引导，提升生成图像的感知质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在类别到图像生成方面取得了显著进展，但现有模型在某些类别中仍会生成扭曲或低质量的图像。FID指标仅评估全局分布对齐，而忽略单个样本的感知质量。

Method: FaME利用图像质量评估模型识别低质量生成图像，存储其采样轨迹，并将这些失败模式作为负引导，避免未来采样进入低质量区域。

Result: 在ImageNet上的实验表明，FaME在不影响FID的情况下，显著提升了视觉质量。

Conclusion: FaME是一种有效的方法，能够提升扩散模型的感知质量，并具备扩展到文本到图像生成的潜力。

Abstract: Diffusion models have achieved remarkable progress in class-to-image
generation. However, we observe that despite impressive FID scores,
state-of-the-art models often generate distorted or low-quality images,
especially in certain classes. This gap arises because FID evaluates global
distribution alignment, while ignoring the perceptual quality of individual
samples. We further examine the role of CFG, a common technique used to enhance
generation quality. While effective in improving metrics and suppressing
outliers, CFG can introduce distribution shift and visual artifacts due to its
misalignment with both training objectives and user expectations. In this work,
we propose FaME, a training-free and inference-efficient method for improving
perceptual quality. FaME uses an image quality assessment model to identify
low-quality generations and stores their sampling trajectories. These failure
modes are then used as negative guidance to steer future sampling away from
poor-quality regions. Experiments on ImageNet demonstrate that FaME brings
consistent improvements in visual quality without compromising FID. FaME also
shows the potential to be extended to improve text-to-image generation.

</details>


### [66] [BridgeTA: Bridging the Representation Gap in Knowledge Distillation via Teacher Assistant for Bird's Eye View Map Segmentation](https://arxiv.org/abs/2508.09599)
*Beomjun Kim,Suhan Woo,Sejong Heo,Euntai Kim*

Main category: cs.CV

TL;DR: BridgeTA是一种成本效益高的蒸馏框架，通过教师助理（TA）网络缩小LiDAR-Camera融合与纯相机模型之间的表示差距，同时保持学生模型的架构和推理成本不变。


<details>
  <summary>Details</summary>
Motivation: 纯相机方法在自动驾驶的BEV地图分割任务中表现不如LiDAR-Camera融合方法，现有知识蒸馏方法通过模仿教师模型架构增加了学生模型的推理成本。

Method: 引入轻量级TA网络，结合教师和学生的BEV表示，创建共享潜在空间，并使用Young不等式推导蒸馏损失，分解为教师-TA和TA-学生双路径。

Result: 在nuScenes数据集上，BridgeTA比纯相机基线提高了4.2% mIoU，比其他最先进KD方法的改进高出45%。

Conclusion: BridgeTA通过TA网络和理论驱动的蒸馏损失，有效提升了纯相机模型的性能，同时保持了低成本。

Abstract: Bird's-Eye-View (BEV) map segmentation is one of the most important and
challenging tasks in autonomous driving. Camera-only approaches have drawn
attention as cost-effective alternatives to LiDAR, but they still fall behind
LiDAR-Camera (LC) fusion-based methods. Knowledge Distillation (KD) has been
explored to narrow this gap, but existing methods mainly enlarge the student
model by mimicking the teacher's architecture, leading to higher inference
cost. To address this issue, we introduce BridgeTA, a cost-effective
distillation framework to bridge the representation gap between LC fusion and
Camera-only models through a Teacher Assistant (TA) network while keeping the
student's architecture and inference cost unchanged. A lightweight TA network
combines the BEV representations of the teacher and student, creating a shared
latent space that serves as an intermediate representation. To ground the
framework theoretically, we derive a distillation loss using Young's
Inequality, which decomposes the direct teacher-student distillation path into
teacher-TA and TA-student dual paths, stabilizing optimization and
strengthening knowledge transfer. Extensive experiments on the challenging
nuScenes dataset demonstrate the effectiveness of our method, achieving an
improvement of 4.2% mIoU over the Camera-only baseline, up to 45% higher than
the improvement of other state-of-the-art KD methods.

</details>


### [67] [MInDI-3D: Iterative Deep Learning in 3D for Sparse-view Cone Beam Computed Tomography](https://arxiv.org/abs/2508.09616)
*Daniel Barco,Marc Stadelmann,Martin Oswald,Ivo Herzig,Lukas Lichtensteiger,Pascal Paysan,Igor Peterlik,Michal Walczak,Bjoern Menze,Frank-Peter Schilling*

Main category: cs.CV

TL;DR: MInDI-3D是首个基于3D条件扩散的模型，用于稀疏视图CBCT伪影去除，显著降低辐射暴露。


<details>
  <summary>Details</summary>
Motivation: 减少医学成像中的辐射暴露，提升稀疏视图CBCT图像质量。

Method: 将2D InDI扩展到3D，通过迭代去噪直接从稀疏输入优化CBCT体积，并使用大型伪CBCT数据集训练。

Result: 在CT-RATE测试集上PSNR提升12.96 dB，辐射暴露降低8倍，性能与3D U-Net相当。

Conclusion: MInDI-3D在临床评估中表现优异，适用于多种解剖部位，并能泛化到新CBCT扫描仪。

Abstract: We present MInDI-3D (Medical Inversion by Direct Iteration in 3D), the first
3D conditional diffusion-based model for real-world sparse-view Cone Beam
Computed Tomography (CBCT) artefact removal, aiming to reduce imaging radiation
exposure. A key contribution is extending the "InDI" concept from 2D to a full
3D volumetric approach for medical images, implementing an iterative denoising
process that refines the CBCT volume directly from sparse-view input. A further
contribution is the generation of a large pseudo-CBCT dataset (16,182) from
chest CT volumes of the CT-RATE public dataset to robustly train MInDI-3D. We
performed a comprehensive evaluation, including quantitative metrics,
scalability analysis, generalisation tests, and a clinical assessment by 11
clinicians. Our results show MInDI-3D's effectiveness, achieving a 12.96 (6.10)
dB PSNR gain over uncorrected scans with only 50 projections on the CT-RATE
pseudo-CBCT (independent real-world) test set and enabling an 8x reduction in
imaging radiation exposure. We demonstrate its scalability by showing that
performance improves with more training data. Importantly, MInDI-3D matches the
performance of a 3D U-Net on real-world scans from 16 cancer patients across
distortion and task-based metrics. It also generalises to new CBCT scanner
geometries. Clinicians rated our model as sufficient for patient positioning
across all anatomical sites and found it preserved lung tumour boundaries well.

</details>


### [68] [Plane Detection and Ranking via Model Information Optimization](https://arxiv.org/abs/2508.09625)
*Daoxin Zhong,Jun Li,Meng Yee Michael Chuah*

Main category: cs.CV

TL;DR: 提出了一种基于模型信息优化的平面检测框架，解决了RANSAC方法在复杂场景中易产生误检的问题。


<details>
  <summary>Details</summary>
Motivation: RANSAC方法在复杂场景中因阈值模糊易产生误检，需一种更鲁棒的平面检测方法。

Method: 将深度数据视为离散随机变量，通过模型信息优化选择最可能的地面真实平面。

Result: 在合成数据中表现优于Open3D RANSAC，并通过神经网络加速提升了真实数据中的性能。

Conclusion: 该框架能更准确地估计平面参数，减少误检，适用于复杂场景。

Abstract: Plane detection from depth images is a crucial subtask with broad robotic
applications, often accomplished by iterative methods such as Random Sample
Consensus (RANSAC). While RANSAC is a robust strategy with strong probabilistic
guarantees, the ambiguity of its inlier threshold criterion makes it
susceptible to false positive plane detections. This issue is particularly
prevalent in complex real-world scenes, where the true number of planes is
unknown and multiple planes coexist. In this paper, we aim to address this
limitation by proposing a generalised framework for plane detection based on
model information optimization. Building on previous works, we treat the
observed depth readings as discrete random variables, with their probability
distributions constrained by the ground truth planes. Various models containing
different candidate plane constraints are then generated through repeated
random sub-sampling to explain our observations. By incorporating the physics
and noise model of the depth sensor, we can calculate the information for each
model, and the model with the least information is accepted as the most likely
ground truth. This information optimization process serves as an objective
mechanism for determining the true number of planes and preventing false
positive detections. Additionally, the quality of each detected plane can be
ranked by summing the information reduction of inlier points for each plane. We
validate these properties through experiments with synthetic data and find that
our algorithm estimates plane parameters more accurately compared to the
default Open3D RANSAC plane segmentation. Furthermore, we accelerate our
algorithm by partitioning the depth map using neural network segmentation,
which enhances its ability to generate more realistic plane parameters in
real-world data.

</details>


### [69] [Semantic-aware DropSplat: Adaptive Pruning of Redundant Gaussians for 3D Aerial-View Segmentation](https://arxiv.org/abs/2508.09626)
*Xu Tang,Junan Jia,Yijing Wang,Jingjing Ma,Xiangrong Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为SAD-Splat的新方法，用于解决3D航空场景语义分割中的语义模糊问题，通过高斯点丢弃模块和高置信度伪标签生成管道提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在3D航空场景语义分割中因尺度变化和遮挡导致语义模糊，限制了分割精度和一致性。

Method: 引入高斯点丢弃模块（结合语义置信度估计和可学习稀疏机制）和高置信度伪标签生成管道（利用2D基础模型增强监督）。

Result: SAD-Splat在分割精度和表示紧凑性之间取得了良好平衡，并在新数据集3D-AS上表现优异。

Conclusion: SAD-Splat为3D航空场景理解提供了高效且可扩展的解决方案。

Abstract: In the task of 3D Aerial-view Scene Semantic Segmentation (3D-AVS-SS),
traditional methods struggle to address semantic ambiguity caused by scale
variations and structural occlusions in aerial images. This limits their
segmentation accuracy and consistency. To tackle these challenges, we propose a
novel 3D-AVS-SS approach named SAD-Splat. Our method introduces a Gaussian
point drop module, which integrates semantic confidence estimation with a
learnable sparsity mechanism based on the Hard Concrete distribution. This
module effectively eliminates redundant and semantically ambiguous Gaussian
points, enhancing both segmentation performance and representation compactness.
Furthermore, SAD-Splat incorporates a high-confidence pseudo-label generation
pipeline. It leverages 2D foundation models to enhance supervision when
ground-truth labels are limited, thereby further improving segmentation
accuracy. To advance research in this domain, we introduce a challenging
benchmark dataset: 3D Aerial Semantic (3D-AS), which encompasses diverse
real-world aerial scenes with sparse annotations. Experimental results
demonstrate that SAD-Splat achieves an excellent balance between segmentation
accuracy and representation compactness. It offers an efficient and scalable
solution for 3D aerial scene understanding.

</details>


### [70] [Enhancing Monocular 3D Hand Reconstruction with Learned Texture Priors](https://arxiv.org/abs/2508.09629)
*Giorgos Karvounas,Nikolaos Kyriazis,Iason Oikonomidis,Georgios Pavlakos,Antonis A. Argyros*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级纹理模块，通过纹理对齐提升单目3D手部重建的准确性和真实感。


<details>
  <summary>Details</summary>
Motivation: 现有高精度模型在手部几何与图像外观的对齐上存在不足，纹理对齐可能是一个未被充分利用的监督信号。

Method: 提出一个轻量级纹理模块，将像素观测嵌入UV纹理空间，并引入密集对齐损失函数，结合可微分渲染和已知拓扑的3D手部网格模型。

Result: 在HaMeR架构上增强后，系统在准确性和真实感上均有提升。

Conclusion: 纹理对齐监督信号在手部重建中具有重要价值。

Abstract: We revisit the role of texture in monocular 3D hand reconstruction, not as an
afterthought for photorealism, but as a dense, spatially grounded cue that can
actively support pose and shape estimation. Our observation is simple: even in
high-performing models, the overlay between predicted hand geometry and image
appearance is often imperfect, suggesting that texture alignment may be an
underused supervisory signal. We propose a lightweight texture module that
embeds per-pixel observations into UV texture space and enables a novel dense
alignment loss between predicted and observed hand appearances. Our approach
assumes access to a differentiable rendering pipeline and a model that maps
images to 3D hand meshes with known topology, allowing us to back-project a
textured hand onto the image and perform pixel-based alignment. The module is
self-contained and easily pluggable into existing reconstruction pipelines. To
isolate and highlight the value of texture-guided supervision, we augment
HaMeR, a high-performing yet unadorned transformer architecture for 3D hand
pose estimation. The resulting system improves both accuracy and realism,
demonstrating the value of appearance-guided alignment in hand reconstruction.

</details>


### [71] [Preacher: Paper-to-Video Agentic System](https://arxiv.org/abs/2508.09632)
*Jingwei Liu,Ling Yang,Hao Luo,Fan Wang Hongyan Li,Mengdi Wang*

Main category: cs.CV

TL;DR: Preacher是一个将论文转化为结构化视频摘要的代理系统，通过分解、总结和重组论文内容，生成高质量的视频摘要。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型受限于上下文窗口、视频时长、风格多样性和领域知识表示不足，Preacher旨在解决这些问题。

Method: 采用自上而下的分解、总结和重组方法，结合自下而上的视频生成，使用渐进式思维链（P-CoT）进行迭代规划。

Result: 在五个研究领域成功生成了高质量的视频摘要，展示了超越现有视频生成模型的专业能力。

Conclusion: Preacher通过创新的方法解决了现有模型的局限性，为论文到视频的转换提供了高效解决方案。

Abstract: The paper-to-video task converts a research paper into a structured video
abstract, distilling key concepts, methods, and conclusions into an accessible,
well-organized format. While state-of-the-art video generation models
demonstrate potential, they are constrained by limited context windows, rigid
video duration constraints, limited stylistic diversity, and an inability to
represent domain-specific knowledge. To address these limitations, we introduce
Preacher, the first paper-to-video agentic system. Preacher employs a top-down
approach to decompose, summarize, and reformulate the paper, followed by
bottom-up video generation, synthesizing diverse video segments into a coherent
abstract. To align cross-modal representations, we define key scenes and
introduce a Progressive Chain of Thought (P-CoT) for granular, iterative
planning. Preacher successfully generates high-quality video abstracts across
five research fields, demonstrating expertise beyond current video generation
models. Code will be released at: https://github.com/GenVerse/Paper2Video

</details>


### [72] [Multi-Contrast Fusion Module: An attention mechanism integrating multi-contrast features for fetal torso plane classification](https://arxiv.org/abs/2508.09644)
*Shengjun Zhu,Siyu Liu,Runqing Xiong,Liping Zheng,Duo Ma,Rongshang Chen,Jiaxin Cai*

Main category: cs.CV

TL;DR: 提出了一种多对比度融合模块（MCFM），用于提升超声图像中胎儿躯干平面的识别准确性，方法简单且参数开销小。


<details>
  <summary>Details</summary>
Motivation: 超声图像的低对比度和模糊纹理细节限制了胎儿解剖结构的精细识别，影响诊断准确性。

Method: MCFM模块通过在不同对比条件下分配注意力权重，增强特征建模能力，同时保持低参数开销。

Result: 实验表明，MCFM显著提升了识别性能，且模型复杂度增加极小。

Conclusion: MCFM为超声图像中胎儿躯干平面识别提供了有效解决方案，具有临床应用的潜力。

Abstract: Purpose: Prenatal ultrasound is a key tool in evaluating fetal structural
development and detecting abnormalities, contributing to reduced perinatal
complications and improved neonatal survival. Accurate identification of
standard fetal torso planes is essential for reliable assessment and
personalized prenatal care. However, limitations such as low contrast and
unclear texture details in ultrasound imaging pose significant challenges for
fine-grained anatomical recognition. Methods: We propose a novel Multi-Contrast
Fusion Module (MCFM) to enhance the model's ability to extract detailed
information from ultrasound images. MCFM operates exclusively on the lower
layers of the neural network, directly processing raw ultrasound data. By
assigning attention weights to image representations under different contrast
conditions, the module enhances feature modeling while explicitly maintaining
minimal parameter overhead. Results: The proposed MCFM was evaluated on a
curated dataset of fetal torso plane ultrasound images. Experimental results
demonstrate that MCFM substantially improves recognition performance, with a
minimal increase in model complexity. The integration of multi-contrast
attention enables the model to better capture subtle anatomical structures,
contributing to higher classification accuracy and clinical reliability.
Conclusions: Our method provides an effective solution for improving fetal
torso plane recognition in ultrasound imaging. By enhancing feature
representation through multi-contrast fusion, the proposed approach supports
clinicians in achieving more accurate and consistent diagnoses, demonstrating
strong potential for clinical adoption in prenatal screening. The codes are
available at https://github.com/sysll/MCFM.

</details>


### [73] [Multi-Sequence Parotid Gland Lesion Segmentation via Expert Text-Guided Segment Anything Model](https://arxiv.org/abs/2508.09645)
*Zhongyuan Wu,Chuan-Xian Ren,Yu Wang,Xiaohua Ban,Jianning Xiao,Xiaohui Duan*

Main category: cs.CV

TL;DR: PG-SAM模型结合专家诊断文本引导，用于跨序列腮腺病变分割，显著提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 解决SAM模型依赖精确提示和忽略医学专家知识的局限性。

Method: 提出专家诊断报告引导的提示生成模块和跨序列注意力模块，结合多模态信息。

Result: 在三个独立临床中心验证了PG-SAM的先进性能和临床适用性。

Conclusion: PG-SAM通过结合专家知识，显著提升了腮腺病变分割的准确性和实用性。

Abstract: Parotid gland lesion segmentation is essential for the treatment of parotid
gland diseases. However, due to the variable size and complex lesion
boundaries, accurate parotid gland lesion segmentation remains challenging.
Recently, the Segment Anything Model (SAM) fine-tuning has shown remarkable
performance in the field of medical image segmentation. Nevertheless, SAM's
interaction segmentation model relies heavily on precise lesion prompts
(points, boxes, masks, etc.), which are very difficult to obtain in real-world
applications. Besides, current medical image segmentation methods are
automatically generated, ignoring the domain knowledge of medical experts when
performing segmentation. To address these limitations, we propose the parotid
gland segment anything model (PG-SAM), an expert diagnosis text-guided SAM
incorporating expert domain knowledge for cross-sequence parotid gland lesion
segmentation. Specifically, we first propose an expert diagnosis report guided
prompt generation module that can automatically generate prompt information
containing the prior domain knowledge to guide the subsequent lesion
segmentation process. Then, we introduce a cross-sequence attention module,
which integrates the complementary information of different modalities to
enhance the segmentation effect. Finally, the multi-sequence image features and
generated prompts are feed into the decoder to get segmentation result.
Experimental results demonstrate that PG-SAM achieves state-of-the-art
performance in parotid gland lesion segmentation across three independent
clinical centers, validating its clinical applicability and the effectiveness
of diagnostic text for enhancing image segmentation in real-world clinical
settings.

</details>


### [74] [The Brain Resection Multimodal Image Registration (ReMIND2Reg) 2025 Challenge](https://arxiv.org/abs/2508.09649)
*Reuben Dorent,Laura Rigolo,Colin P. Galvin,Junyu Chen,Mattias P. Heinrich,Aaron Carass,Olivier Colliot,Demian Wassermann,Alexandra Golby,Tina Kapur,William Wells*

Main category: cs.CV

TL;DR: ReMIND2Reg 2025 Challenge提供了一个公开基准，用于评估脑肿瘤手术中术前MRI与术后超声图像配准的算法，旨在解决脑移位问题。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤手术中，基于术前MRI的导航系统因脑移位导致准确性下降，需要开发多模态配准算法以恢复空间准确性。

Method: 挑战赛提供99个训练案例、5个验证案例和10个测试案例，包含配对的3D ceT1 MRI、T2 MRI和术后3D超声图像，通过手动标注的解剖标志评估性能。

Result: 评估指标包括目标配准误差（TRE）、对最坏情况标志错位的鲁棒性（TRE30）和运行时间。

Conclusion: ReMIND2Reg旨在推动开发鲁棒、通用且可临床部署的多模态配准算法，以提升神经外科手术的导航准确性。

Abstract: Accurate intraoperative image guidance is critical for achieving maximal safe
resection in brain tumor surgery, yet neuronavigation systems based on
preoperative MRI lose accuracy during the procedure due to brain shift.
Aligning post-resection intraoperative ultrasound (iUS) with preoperative MRI
can restore spatial accuracy by estimating brain shift deformations, but it
remains a challenging problem given the large anatomical and topological
changes and substantial modality intensity gap. The ReMIND2Reg 2025 Challenge
provides the largest public benchmark for this task, built upon the ReMIND
dataset. It offers 99 training cases, 5 validation cases, and 10 private test
cases comprising paired 3D ceT1 MRI, T2 MRI, and post-resection 3D iUS volumes.
Data are provided without annotations for training, while validation and test
performance are evaluated on manually annotated anatomical landmarks. Metrics
include target registration error (TRE), robustness to worst-case landmark
misalignment (TRE30), and runtime. By establishing a standardized evaluation
framework for this clinically critical and technically complex problem,
ReMIND2Reg aims to accelerate the development of robust, generalizable, and
clinically deployable multimodal registration algorithms for image-guided
neurosurgery.

</details>


### [75] [TOTNet: Occlusion-Aware Temporal Tracking for Robust Ball Detection in Sports Videos](https://arxiv.org/abs/2508.09650)
*Hao Xu,Arbind Agrahari Baniya,Sam Wells,Mohamed Reda Bouadjenek,Richard Dazely,Sunil Aryal*

Main category: cs.CV

TL;DR: TOTNet是一种用于体育视频中球体跟踪的时序遮挡网络，通过3D卷积和遮挡增强技术显著提升了遮挡情况下的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 体育视频分析中，球体在遮挡情况下的跟踪是一个关键挑战，影响了事件检测和裁判决策。

Method: TOTNet结合了3D卷积、可见性加权损失和遮挡增强技术，并使用了新的遮挡丰富的数据集TTA进行训练。

Result: 在四个数据集上，TOTNet将RMSE从37.30降至7.19，全遮挡帧的准确率从0.63提升至0.80。

Conclusion: TOTNet在快速体育场景中表现出色，适用于离线体育分析。

Abstract: Robust ball tracking under occlusion remains a key challenge in sports video
analysis, affecting tasks like event detection and officiating. We present
TOTNet, a Temporal Occlusion Tracking Network that leverages 3D convolutions,
visibility-weighted loss, and occlusion augmentation to improve performance
under partial and full occlusions. Developed in collaboration with Paralympics
Australia, TOTNet is designed for real-world sports analytics. We introduce
TTA, a new occlusion-rich table tennis dataset collected from
professional-level Paralympic matches, comprising 9,159 samples with 1,996
occlusion cases. Evaluated on four datasets across tennis, badminton, and table
tennis, TOTNet significantly outperforms prior state-of-the-art methods,
reducing RMSE from 37.30 to 7.19 and improving accuracy on fully occluded
frames from 0.63 to 0.80. These results demonstrate TOTNets effectiveness for
offline sports analytics in fast-paced scenarios. Code and data
access:\href{https://github.com/AugustRushG/TOTNet}{AugustRushG/TOTNet}.

</details>


### [76] [Noise-adapted Neural Operator for Robust Non-Line-of-Sight Imaging](https://arxiv.org/abs/2508.09655)
*Lianfang Wang,Kuilin Qin,Xueying Liu,Huibin Chang,Yong Wang,Yuping Duan*

Main category: cs.CV

TL;DR: 提出了一种基于参数化逆问题框架的3D成像重建方法，结合噪声估计和神经算子学习，显著提升了非视距成像的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 非视距成像中，间接光信号弱且易受噪声干扰，需要结合物理过程实现精确重建。

Method: 采用噪声估计模块和参数化神经算子，通过深度算法展开构建3D重建框架，并结合全局与局部时空数据特征融合方法。

Result: 在模拟和真实数据集上验证了方法的有效性，尤其在快速扫描和稀疏光照点数据中表现优异。

Conclusion: 该方法为复杂场景下的非视距成像提供了可行解决方案，兼具高准确性和鲁棒性。

Abstract: Computational imaging, especially non-line-of-sight (NLOS) imaging, the
extraction of information from obscured or hidden scenes is achieved through
the utilization of indirect light signals resulting from multiple reflections
or scattering. The inherently weak nature of these signals, coupled with their
susceptibility to noise, necessitates the integration of physical processes to
ensure accurate reconstruction. This paper presents a parameterized inverse
problem framework tailored for large-scale linear problems in 3D imaging
reconstruction. Initially, a noise estimation module is employed to adaptively
assess the noise levels present in transient data. Subsequently, a
parameterized neural operator is developed to approximate the inverse mapping,
facilitating end-to-end rapid image reconstruction. Our 3D image reconstruction
framework, grounded in operator learning, is constructed through deep algorithm
unfolding, which not only provides commendable model interpretability but also
enables dynamic adaptation to varying noise levels in the acquired data,
thereby ensuring consistently robust and accurate reconstruction outcomes.
Furthermore, we introduce a novel method for the fusion of global and local
spatiotemporal data features. By integrating structural and detailed
information, this method significantly enhances both accuracy and robustness.
Comprehensive numerical experiments conducted on both simulated and real
datasets substantiate the efficacy of the proposed method. It demonstrates
remarkable performance with fast scanning data and sparse illumination point
data, offering a viable solution for NLOS imaging in complex scenarios.

</details>


### [77] [NegFaceDiff: The Power of Negative Context in Identity-Conditioned Diffusion for Synthetic Face Generation](https://arxiv.org/abs/2508.09661)
*Eduarda Caldeira,Naser Damer,Fadi Boutros*

Main category: cs.CV

TL;DR: NegFaceDiff是一种新的采样方法，通过在身份条件扩散过程中引入负条件，显著提高了生成数据的身份一致性和可分性。


<details>
  <summary>Details</summary>
Motivation: 解决现有身份条件扩散模型在生成数据时缺乏明确的类间分离机制，导致身份重叠和FR性能不佳的问题。

Method: 提出NegFaceDiff方法，利用负条件在扩散过程中明确引导模型远离不需要的特征，同时保持类内一致性。

Result: 实验表明，NegFaceDiff将身份可分性（FDR）从2.427提高到5.687，并显著提升了FR系统的性能。

Conclusion: NegFaceDiff通过引入负条件，有效提升了生成数据的质量，从而改善了FR模型的训练效果。

Abstract: The use of synthetic data as an alternative to authentic datasets in face
recognition (FR) development has gained significant attention, addressing
privacy, ethical, and practical concerns associated with collecting and using
authentic data. Recent state-of-the-art approaches have proposed
identity-conditioned diffusion models to generate identity-consistent face
images, facilitating their use in training FR models. However, these methods
often lack explicit sampling mechanisms to enforce inter-class separability,
leading to identity overlap in the generated data and, consequently, suboptimal
FR performance. In this work, we introduce NegFaceDiff, a novel sampling method
that incorporates negative conditions into the identity-conditioned diffusion
process. NegFaceDiff enhances identity separation by leveraging negative
conditions that explicitly guide the model away from unwanted features while
preserving intra-class consistency. Extensive experiments demonstrate that
NegFaceDiff significantly improves the identity consistency and separability of
data generated by identity-conditioned diffusion models. Specifically, identity
separability, measured by the Fisher Discriminant Ratio (FDR), increases from
2.427 to 5.687. These improvements are reflected in FR systems trained on the
NegFaceDiff dataset, which outperform models trained on data generated without
negative conditions across multiple benchmarks.

</details>


### [78] [GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video Diffusion Priors](https://arxiv.org/abs/2508.09667)
*Xingyilang Yin,Qi Zhang,Jiahao Chang,Ying Feng,Qingnan Fan,Xi Yang,Chi-Man Pun,Huaqi Zhang,Xiaodong Cun*

Main category: cs.CV

TL;DR: GSFixer是一个新框架，通过参考引导的视频修复模型提升稀疏视图3D高斯泼溅（3DGS）重建的质量，结合2D语义和3D几何特征增强一致性。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS重建因信息不足导致明显伪影，现有生成方法难以保持与输入观察的一致性。

Method: 基于DiT的视频扩散模型，利用视觉几何基础模型提取参考视图的2D语义和3D几何特征。

Result: GSFixer在3DGS伪影修复和稀疏视图3D重建中优于现有方法。

Conclusion: GSFixer通过多特征整合有效提升3DGS重建质量，并提供了新的评估基准DL3DV-Res。

Abstract: Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views
is an ill-posed problem due to insufficient information, often resulting in
noticeable artifacts. While recent approaches have sought to leverage
generative priors to complete information for under-constrained regions, they
struggle to generate content that remains consistent with input observations.
To address this challenge, we propose GSFixer, a novel framework designed to
improve the quality of 3DGS representations reconstructed from sparse inputs.
The core of our approach is the reference-guided video restoration model, built
upon a DiT-based video diffusion model trained on paired artifact 3DGS renders
and clean frames with additional reference-based conditions. Considering the
input sparse views as references, our model integrates both 2D semantic
features and 3D geometric features of reference views extracted from the visual
geometry foundation model, enhancing the semantic coherence and 3D consistency
when fixing artifact novel views. Furthermore, considering the lack of suitable
benchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which
contains artifact frames rendered using low-quality 3DGS. Extensive experiments
demonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS
artifact restoration and sparse-view 3D reconstruction. Project page:
https://github.com/GVCLab/GSFixer.

</details>


### [79] [Surg-InvNeRF: Invertible NeRF for 3D tracking and reconstruction in surgical vision](https://arxiv.org/abs/2508.09681)
*Gerardo Loza,Junlei Hu,Dominic Jones,Sharib Ali,Pietro Valdastri*

Main category: cs.CV

TL;DR: 提出了一种基于NeRF的新型测试时优化（TTO）方法，用于长期3D点跟踪，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪方法难以实现一致运动或仅限于2D运动，需改进。

Method: 使用可逆NeRF（InvNeRF）架构，结合多尺度HexPlanes和高效像素采样算法。

Result: 在2D和3D点跟踪中表现优异，2D精度提升50%，3D首次实现TTO方法。

Conclusion: 该方法在手术场景中实现了高效、精确的点跟踪，结合了可变形NeRF的优势。

Abstract: We proposed a novel test-time optimisation (TTO) approach framed by a
NeRF-based architecture for long-term 3D point tracking. Most current methods
in point tracking struggle to obtain consistent motion or are limited to 2D
motion. TTO approaches frame the solution for long-term tracking as optimising
a function that aggregates correspondences from other specialised
state-of-the-art methods. Unlike the state-of-the-art on TTO, we propose
parametrising such a function with our new invertible Neural Radiance Field
(InvNeRF) architecture to perform both 2D and 3D tracking in surgical
scenarios. Our approach allows us to exploit the advantages of a
rendering-based approach by supervising the reprojection of pixel
correspondences. It adapts strategies from recent rendering-based methods to
obtain a bidirectional deformable-canonical mapping, to efficiently handle a
defined workspace, and to guide the rays' density. It also presents our
multi-scale HexPlanes for fast inference and a new algorithm for efficient
pixel sampling and convergence criteria. We present results in the STIR and
SCARE datasets, for evaluating point tracking and testing the integration of
kinematic data in our pipeline, respectively. In 2D point tracking, our
approach surpasses the precision and accuracy of the TTO state-of-the-art
methods by nearly 50% on average precision, while competing with other
approaches. In 3D point tracking, this is the first TTO approach, surpassing
feed-forward methods while incorporating the benefits of a deformable
NeRF-based reconstruction.

</details>


### [80] [PaCo-FR: Patch-Pixel Aligned End-to-End Codebook Learning for Facial Representation Pre-training](https://arxiv.org/abs/2508.09691)
*Yin Xie,Zhichao Chen,Xiaoze Yu,Yongle Zhao,Xiang An,Kaicheng Yang,Zimin Ran,Jia Guo,Ziyong Feng,Jiankang Deng*

Main category: cs.CV

TL;DR: PaCo-FR是一种无监督框架，通过结合掩码图像建模和像素对齐，解决了现有面部表示预训练方法的三个关键挑战，并在多个面部分析任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉面部特征、空间结构和利用有限标注数据方面存在不足，需要一种更高效的面部表示预训练方法。

Method: PaCo-FR结合了结构化掩码策略、基于补丁的代码书和空间一致性约束，以增强特征区分和几何关系保留。

Result: PaCo-FR仅需200万未标注图像进行预训练，在多种面部分析任务中表现优异，尤其在姿态、遮挡和光照变化场景中。

Conclusion: PaCo-FR推动了面部表示学习的发展，提供了一种可扩展且高效的解决方案，减少了对昂贵标注数据的依赖。

Abstract: Facial representation pre-training is crucial for tasks like facial
recognition, expression analysis, and virtual reality. However, existing
methods face three key challenges: (1) failing to capture distinct facial
features and fine-grained semantics, (2) ignoring the spatial structure
inherent to facial anatomy, and (3) inefficiently utilizing limited labeled
data. To overcome these, we introduce PaCo-FR, an unsupervised framework that
combines masked image modeling with patch-pixel alignment. Our approach
integrates three innovative components: (1) a structured masking strategy that
preserves spatial coherence by aligning with semantically meaningful facial
regions, (2) a novel patch-based codebook that enhances feature discrimination
with multiple candidate tokens, and (3) spatial consistency constraints that
preserve geometric relationships between facial components. PaCo-FR achieves
state-of-the-art performance across several facial analysis tasks with just 2
million unlabeled images for pre-training. Our method demonstrates significant
improvements, particularly in scenarios with varying poses, occlusions, and
lighting conditions. We believe this work advances facial representation
learning and offers a scalable, efficient solution that reduces reliance on
expensive annotated datasets, driving more effective facial analysis systems.

</details>


### [81] [Slot Attention-based Feature Filtering for Few-Shot Learning](https://arxiv.org/abs/2508.09699)
*Javier Rodenas,Eduardo Aguilar,Petia Radeva*

Main category: cs.CV

TL;DR: SAFF利用Slot Attention机制过滤无关特征，提升小样本学习性能。


<details>
  <summary>Details</summary>
Motivation: 无关特征会显著降低小样本学习的性能，导致混淆和误分类。

Method: 提出SAFF方法，结合Slot Attention和patch embeddings，通过相似性矩阵量化特征相关性。

Result: 在多个小样本学习基准数据集上表现优于现有方法。

Conclusion: SAFF能有效过滤无关特征，提升分类性能。

Abstract: Irrelevant features can significantly degrade few-shot learn ing performance.
This problem is used to match queries and support images based on meaningful
similarities despite the limited data. However, in this process, non-relevant
fea tures such as background elements can easily lead to confu sion and
misclassification. To address this issue, we pro pose Slot Attention-based
Feature Filtering for Few-Shot Learning (SAFF) that leverages slot attention
mechanisms to discriminate and filter weak features, thereby improving few-shot
classification performance. The key innovation of SAFF lies in its integration
of slot attention with patch em beddings, unifying class-aware slots into a
single attention mechanism to filter irrelevant features effectively. We intro
duce a similarity matrix that computes across support and query images to
quantify the relevance of filtered embed dings for classification. Through
experiments, we demon strate that Slot Attention performs better than other
atten tion mechanisms, capturing discriminative features while reducing
irrelevant information. We validate our approach through extensive experiments
on few-shot learning bench marks: CIFAR-FS, FC100, miniImageNet and tieredIma
geNet, outperforming several state-of-the-art methods.

</details>


### [82] [MangaDiT: Reference-Guided Line Art Colorization with Hierarchical Attention in Diffusion Transformers](https://arxiv.org/abs/2508.09709)
*Qianru Qiu,Jiafeng Mao,Kento Masui,Xueting Wang*

Main category: cs.CV

TL;DR: MangaDiT利用扩散变换器和分层注意力机制，提升了线稿着色中的区域级颜色一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在参考图像和目标图像姿态或动作不同时，区域级颜色一致性表现不佳，因此提出通过内部注意力机制隐式发现语义对应关系。

Method: 提出MangaDiT模型，结合线稿和参考图像作为条件输入，采用分层注意力机制和动态注意力权重策略，增强颜色对齐。

Result: 在两个基准数据集上显著优于现有方法，定性和定量评估均表现优异。

Conclusion: MangaDiT通过改进注意力机制，有效提升了参考引导线稿着色的性能。

Abstract: Recent advances in diffusion models have significantly improved the
performance of reference-guided line art colorization. However, existing
methods still struggle with region-level color consistency, especially when the
reference and target images differ in character pose or motion. Instead of
relying on external matching annotations between the reference and target, we
propose to discover semantic correspondences implicitly through internal
attention mechanisms. In this paper, we present MangaDiT, a powerful model for
reference-guided line art colorization based on Diffusion Transformers (DiT).
Our model takes both line art and reference images as conditional inputs and
introduces a hierarchical attention mechanism with a dynamic attention
weighting strategy. This mechanism augments the vanilla attention with an
additional context-aware path that leverages pooled spatial features,
effectively expanding the model's receptive field and enhancing region-level
color alignment. Experiments on two benchmark datasets demonstrate that our
method significantly outperforms state-of-the-art approaches, achieving
superior performance in both qualitative and quantitative evaluations.

</details>


### [83] [NEURAL: Attention-Guided Pruning for Unified Multimodal Resource-Constrained Clinical Evaluation](https://arxiv.org/abs/2508.09715)
*Devvrat Joshi,Islem Rekik*

Main category: cs.CV

TL;DR: NEURAL是一种新颖的多模态医学图像压缩框架，通过语义引导的数据压缩和图形表示，显著减小数据大小同时保持高诊断性能。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限临床环境中多模态医学图像数据的存储和传输挑战。

Method: 利用生成视觉语言模型的交叉注意力分数对胸部X光进行结构性修剪，生成压缩的图形表示，并与临床报告的知识图融合。

Result: 在MIMIC-CXR和CheXpert Plus数据集上，NEURAL将图像数据大小减少93.4-97.7%，同时保持0.88-0.95 AUC的高诊断性能。

Conclusion: NEURAL通过任务无关的数据资产解决了数据大小与临床效用之间的权衡，支持高效工作流程和远程放射学。

Abstract: The rapid growth of multimodal medical imaging data presents significant
storage and transmission challenges, particularly in resource-constrained
clinical settings. We propose NEURAL, a novel framework that addresses this by
using semantics-guided data compression. Our approach repurposes
cross-attention scores between the image and its radiological report from a
fine-tuned generative vision-language model to structurally prune chest X-rays,
preserving only diagnostically critical regions. This process transforms the
image into a highly compressed, graph representation. This unified graph-based
representation fuses the pruned visual graph with a knowledge graph derived
from the clinical report, creating a universal data structure that simplifies
downstream modeling. Validated on the MIMIC-CXR and CheXpert Plus dataset for
pneumonia detection, NEURAL achieves a 93.4-97.7\% reduction in image data size
while maintaining a high diagnostic performance of 0.88-0.95 AUC, outperforming
other baseline models that use uncompressed data. By creating a persistent,
task-agnostic data asset, NEURAL resolves the trade-off between data size and
clinical utility, enabling efficient workflows and teleradiology without
sacrificing performance. Our NEURAL code is available at
https://github.com/basiralab/NEURAL.

</details>


### [84] [Multimodal Sheaf-based Network for Glioblastoma Molecular Subtype Prediction](https://arxiv.org/abs/2508.09717)
*Shekhnaz Idrissova,Islem Rekik*

Main category: cs.CV

TL;DR: 提出了一种基于sheaf的框架，用于MRI和组织病理学数据的结构感知和一致性融合，以解决现有方法在保留跨模态共享结构信息和处理缺失数据方面的不足。


<details>
  <summary>Details</summary>
Motivation: 胶质母细胞瘤的分子亚型分类需要侵入性组织提取，现有多模态方法在保留共享结构信息和处理缺失数据方面存在局限。

Method: 提出了一种新颖的sheaf-based框架，用于MRI和组织病理学数据的结构感知和一致性融合。

Result: 模型在基线方法上表现优越，并在缺失数据场景中表现出鲁棒性。

Conclusion: 该框架有助于开发虚拟活检工具，用于快速诊断。

Abstract: Glioblastoma is a highly invasive brain tumor with rapid progression rates.
Recent studies have shown that glioblastoma molecular subtype classification
serves as a significant biomarker for effective targeted therapy selection.
However, this classification currently requires invasive tissue extraction for
comprehensive histopathological analysis. Existing multimodal approaches
combining MRI and histopathology images are limited and lack robust mechanisms
for preserving shared structural information across modalities. In particular,
graph-based models often fail to retain discriminative features within
heterogeneous graphs, and structural reconstruction mechanisms for handling
missing or incomplete modality data are largely underexplored. To address these
limitations, we propose a novel sheaf-based framework for structure-aware and
consistent fusion of MRI and histopathology data. Our model outperforms
baseline methods and demonstrates robustness in incomplete or missing data
scenarios, contributing to the development of virtual biopsy tools for rapid
diagnostics. Our source code is available at
https://github.com/basiralab/MMSN/.

</details>


### [85] [Predictive Uncertainty for Runtime Assurance of a Real-Time Computer Vision-Based Landing System](https://arxiv.org/abs/2508.09732)
*Romeo Valentin,Sydney M. Katz,Artur B. Carneiro,Don Walker,Mykel J. Kochenderfer*

Main category: cs.CV

TL;DR: 提出了一种基于视觉的飞机姿态估计方法，通过创新的神经网络架构、校准的预测不确定性和故障检测机制，提升了安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动的计算机视觉在航空导航中取得了进展，但确保系统满足航空应用的安全性和鲁棒性要求仍具挑战性。

Method: 采用基于空间Soft Argmax的神经网络架构、校准的损失函数和RAIM改进的故障检测机制。

Result: 模型在跑道图像数据集上表现优于基线架构，提供高精度和校准良好的不确定性估计。

Conclusion: 该方法为安全关键航空应用中的系统认证提供了可行路径。

Abstract: Recent advances in data-driven computer vision have enabled robust autonomous
navigation capabilities for civil aviation, including automated landing and
runway detection. However, ensuring that these systems meet the robustness and
safety requirements for aviation applications remains a major challenge. In
this work, we present a practical vision-based pipeline for aircraft pose
estimation from runway images that represents a step toward the ability to
certify these systems for use in safety-critical aviation applications. Our
approach features three key innovations: (i) an efficient, flexible neural
architecture based on a spatial Soft Argmax operator for probabilistic keypoint
regression, supporting diverse vision backbones with real-time inference; (ii)
a principled loss function producing calibrated predictive uncertainties, which
are evaluated via sharpness and calibration metrics; and (iii) an adaptation of
Residual-based Receiver Autonomous Integrity Monitoring (RAIM), enabling
runtime detection and rejection of faulty model outputs. We implement and
evaluate our pose estimation pipeline on a dataset of runway images. We show
that our model outperforms baseline architectures in terms of accuracy while
also producing well-calibrated uncertainty estimates with sub-pixel precision
that can be used downstream for fault detection.

</details>


### [86] [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)
*Lin Long,Yichen He,Wentao Ye,Yiyuan Pan,Yuan Lin,Hang Li,Junbo Zhao,Wei Li*

Main category: cs.CV

TL;DR: M3-Agent是一个具有长期记忆的多模态代理框架，能够处理实时视觉和听觉输入，并通过强化学习在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种更接近人类长期记忆的多模态代理，以提升其在复杂任务中的理解和推理能力。

Method: M3-Agent采用实体中心的多模态记忆组织方式，结合强化学习进行训练，并通过M3-Bench基准测试评估其性能。

Result: M3-Agent在M3-Bench-robot、M3-Bench-web和VideoMME-long基准测试中分别比基线模型高出6.7%、7.7%和5.3%的准确率。

Conclusion: M3-Agent在多模态代理的长期记忆和推理能力方面取得了显著进展，为实际应用提供了设计参考。

Abstract: We introduce M3-Agent, a novel multimodal agent framework equipped with
long-term memory. Like humans, M3-Agent can process real-time visual and
auditory inputs to build and update its long-term memory. Beyond episodic
memory, it also develops semantic memory, enabling it to accumulate world
knowledge over time. Its memory is organized in an entity-centric, multimodal
format, allowing deeper and more consistent understanding of the environment.
Given an instruction, M3-Agent autonomously performs multi-turn, iterative
reasoning and retrieves relevant information from memory to accomplish the
task. To evaluate memory effectiveness and memory-based reasoning in multimodal
agents, we develop M3-Bench, a new long-video question answering benchmark.
M3-Bench comprises 100 newly recorded real-world videos captured from a robot's
perspective (M3-Bench-robot) and 929 web-sourced videos across diverse
scenarios (M3-Bench-web). We annotate question-answer pairs designed to test
key capabilities essential for agent applications, such as human understanding,
general knowledge extraction, and cross-modal reasoning. Experimental results
show that M3-Agent, trained via reinforcement learning, outperforms the
strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,
achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web
and VideoMME-long, respectively. Our work advances the multimodal agents toward
more human-like long-term memory and provides insights into their practical
design. Model, code and data are available at
https://github.com/bytedance-seed/m3-agent

</details>


### [87] [Region-to-Region: Enhancing Generative Image Harmonization with Adaptive Regional Injection](https://arxiv.org/abs/2508.09746)
*Zhiqiu Zhang,Dongqi Fan,Mingjie Wang,Qiang Tang,Jian Yang,Zili Yi*

Main category: cs.CV

TL;DR: 提出了一种基于区域到区域变换的图像协调方法R2R，通过Clear-VAE和Harmony Controller提升细节保留和协调能力，并构建了新的合成数据集RPHarmony。


<details>
  <summary>Details</summary>
Motivation: 现有基于LDM的协调方法在细节保留和协调能力上存在不足，且合成数据集缺乏真实光照变化。

Method: 设计了Clear-VAE和Harmony Controller（含MACA模块），并提出Random Poisson Blending构建新数据集RPHarmony。

Result: 实验表明，该方法在定量指标和视觉协调性上优于其他方法，且数据集能生成更真实的图像。

Conclusion: R2R方法显著提升了图像协调效果，公开的代码和数据集为后续研究提供了支持。

Abstract: The goal of image harmonization is to adjust the foreground in a composite
image to achieve visual consistency with the background. Recently, latent
diffusion model (LDM) are applied for harmonization, achieving remarkable
results. However, LDM-based harmonization faces challenges in detail
preservation and limited harmonization ability. Additionally, current synthetic
datasets rely on color transfer, which lacks local variations and fails to
capture complex real-world lighting conditions. To enhance harmonization
capabilities, we propose the Region-to-Region transformation. By injecting
information from appropriate regions into the foreground, this approach
preserves original details while achieving image harmonization or, conversely,
generating new composite data. From this perspective, We propose a novel model
R2R. Specifically, we design Clear-VAE to preserve high-frequency details in
the foreground using Adaptive Filter while eliminating disharmonious elements.
To further enhance harmonization, we introduce the Harmony Controller with
Mask-aware Adaptive Channel Attention (MACA), which dynamically adjusts the
foreground based on the channel importance of both foreground and background
regions. To address the limitation of existing datasets, we propose Random
Poisson Blending, which transfers color and lighting information from a
suitable region to the foreground, thereby generating more diverse and
challenging synthetic images. Using this method, we construct a new synthetic
dataset, RPHarmony. Experiments demonstrate the superiority of our method over
other methods in both quantitative metrics and visual harmony. Moreover, our
dataset helps the model generate more realistic images in real examples. Our
code, dataset, and model weights have all been released for open access.

</details>


### [88] [MoIIE: Mixture of Intra- and Inter-Modality Experts for Large Vision Language Models](https://arxiv.org/abs/2508.09779)
*Dianyi Wang,Siyuan Wang,Zejun Li,Yikun Wang,Yitong Li,Duyu Tang,Xiaoyu Shen,Xuanjing Huang,Zhongyu Wei*

Main category: cs.CV

TL;DR: 提出了一种混合模态内和模态间专家的稀疏架构（MoIIE），用于提升大型视觉语言模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型视觉语言模型（LVLMs）在多模态任务中表现优异，但其计算成本高昂，稀疏的混合专家（MoE）架构成为探索方向。然而，如何在LVLMs中有效应用MoE以同时建模模态内特征和跨模态关联仍具挑战性。

Method: 提出MoIIE架构，通过模态引导的路由机制将令牌分配到模态内专家和共享的模态间专家池中，并结合两阶段训练策略激活MoE和多模态能力。

Result: 实验表明，MoIIE模型在5.5B和11.3B激活参数下，性能匹配甚至超越现有开源MoE-LLMs多模态模型，且效率更高。

Conclusion: MoIIE架构有效提升了LVLMs的效率和性能，为稀疏多模态模型的发展提供了新思路。

Abstract: Large Vision-Language Models (LVLMs) have demonstrated remarkable performance
across multi-modal tasks by scaling model size and training data. However,
these dense LVLMs incur significant computational costs and motivate the
exploration of sparse Mixture of Experts (MoE) architectures. While MoE improve
parameter efficiency, effectively applying MoE to simultaneously model
modality-specific features and cross-modal associations in LVLMs remains
challenging. In this work, we propose to incorporate Mixture of Intra- and
Inter-Modality Experts (MoIIE) to LVLMs. For each token, expert routing is
guided by its modality, directing tokens to their respective intra-modality
experts as well as a shared pool of inter-modality experts, enabling the model
to jointly learn rich intra-modal features and cross-modal interactions. We
further introduce an effective and straightforward two-stage training strategy,
which facilitates the direct activation of both MoE and multi-modal
capabilities. Extensive experiments across different data scales and LLM
backbone demonstrate the effectiveness, efficiency and generality of our
approach. Notably, our MoIIE models with 5.5B and 11.3B activated parameters
match or even surpass the performance of existing advanced open-source MoE-LLMs
based multi-modal models that involve more activated parameters. The code is
available at https://github.com/AlenjandroWang/MoIIE.

</details>


### [89] [Combinative Matching for Geometric Shape Assembly](https://arxiv.org/abs/2508.09780)
*Nahyuk Lee,Juhong Min,Junhong Lee,Chunghyun Park,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种新的形状匹配方法（组合匹配），用于几何形状装配中的互锁部件组合。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖于通过寻找相同表面来对齐部件，而本文明确建模了互锁形状的两个特性：'相同表面形状'和'相反体积占用'。

Method: 通过学习在表面形状相同但体积占用相反的区域建立对应关系，并通过等变神经网络估计形状方向以对齐旋转区域。

Result: 实验结果表明，该方法显著减少了匹配中的局部模糊性，并在几何装配基准测试中优于现有技术。

Conclusion: 提出的方法在几何形状装配中表现出高效性和鲁棒性。

Abstract: This paper introduces a new shape-matching methodology, combinative matching,
to combine interlocking parts for geometric shape assembly. Previous methods
for geometric assembly typically rely on aligning parts by finding identical
surfaces between the parts as in conventional shape matching and registration.
In contrast, we explicitly model two distinct properties of interlocking
shapes: 'identical surface shape' and 'opposite volume occupancy.' Our method
thus learns to establish correspondences across regions where their surface
shapes appear identical but their volumes occupy the inverted space to each
other. To facilitate this process, we also learn to align regions in rotation
by estimating their shape orientations via equivariant neural networks. The
proposed approach significantly reduces local ambiguities in matching and
allows a robust combination of parts in assembly. Experimental results on
geometric assembly benchmarks demonstrate the efficacy of our method,
consistently outperforming the state of the art. Project page:
https://nahyuklee.github.io/cmnet.

</details>


### [90] [DSS-Prompt: Dynamic-Static Synergistic Prompting for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2508.09785)
*Linpu He,Yanan Li,Bingze Li,Elvis Han Cui,Donghui Wang*

Main category: cs.CV

TL;DR: DSS-Prompt利用静态和动态提示将预训练Vision Transformer转化为强FSCIL分类器，无需增量任务训练即可超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索预训练模型在少样本类增量学习（FSCIL）中的应用，解决从有限样本学习新概念且不遗忘旧知识的问题。

Method: 结合静态提示（适应预训练与下游数据域差异）和动态提示（捕获实例感知语义），利用多模态模型生成输入感知提示。

Result: 在四个基准测试中表现优于现有方法，有效缓解灾难性遗忘问题。

Conclusion: DSS-Prompt是一种简单有效的FSCIL方法，通过提示机制显著提升性能。

Abstract: Learning from large-scale pre-trained models with strong generalization
ability has shown remarkable success in a wide range of downstream tasks
recently, but it is still underexplored in the challenging few-shot
class-incremental learning (FSCIL) task. It aims to continually learn new
concepts from limited training samples without forgetting the old ones at the
same time. In this paper, we introduce DSS-Prompt, a simple yet effective
approach that transforms the pre-trained Vision Transformer with minimal
modifications in the way of prompts into a strong FSCIL classifier. Concretely,
we synergistically utilize two complementary types of prompts in each
Transformer block: static prompts to bridge the domain gap between the
pre-training and downstream datasets, thus enabling better adaption; and
dynamic prompts to capture instance-aware semantics, thus enabling easy
transfer from base to novel classes. Specially, to generate dynamic prompts, we
leverage a pre-trained multi-modal model to extract input-related diverse
semantics, thereby generating complementary input-aware prompts, and then
adaptively adjust their importance across different layers. In this way, on top
of the prompted visual embeddings, a simple prototype classifier can beat
state-of-the-arts without further training on the incremental tasks. We conduct
extensive experiments on four benchmarks to validate the effectiveness of our
DSS-Prompt and show that it consistently achieves better performance than
existing approaches on all datasets and can alleviate the catastrophic
forgetting issue as well.

</details>


### [91] [MeMoSORT: Memory-Assisted Filtering and Motion-Adaptive Association Metric for Multi-Person Tracking](https://arxiv.org/abs/2508.09796)
*Yingjie Wang,Zhixing Wang,Le Zheng,Tianxiao Liu,Roujing Li,Xueyao Hu*

Main category: cs.CV

TL;DR: MeMoSORT是一种简单、在线、实时的多目标跟踪器，通过记忆辅助卡尔曼滤波器和运动自适应IoU解决传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统跟踪方法依赖卡尔曼滤波器和刚性IoU关联，在复杂运动和遮挡情况下表现不佳，导致跟踪错误和目标丢失。

Method: 提出记忆辅助卡尔曼滤波器（MeKF）补偿运动模型不匹配，以及运动自适应IoU（Mo-IoU）扩展匹配空间并引入高度相似性。

Result: 在DanceTrack和SportsMOT数据集上，MeMoSORT分别达到67.9%和82.1%的HOTA分数，表现优异。

Conclusion: MeMoSORT通过创新设计解决了传统方法的不足，实现了高性能的多目标跟踪。

Abstract: Multi-object tracking (MOT) in human-dominant scenarios, which involves
continuously tracking multiple people within video sequences, remains a
significant challenge in computer vision due to targets' complex motion and
severe occlusions. Conventional tracking-by-detection methods are fundamentally
limited by their reliance on Kalman filter (KF) and rigid Intersection over
Union (IoU)-based association. The motion model in KF often mismatches
real-world object dynamics, causing filtering errors, while rigid association
struggles under occlusions, leading to identity switches or target loss. To
address these issues, we propose MeMoSORT, a simple, online, and real-time MOT
tracker with two key innovations. First, the Memory-assisted Kalman filter
(MeKF) uses memory-augmented neural networks to compensate for mismatches
between assumed and actual object motion. Second, the Motion-adaptive IoU
(Mo-IoU) adaptively expands the matching space and incorporates height
similarity to reduce the influence of detection errors and association
failures, while remaining lightweight. Experiments on DanceTrack and SportsMOT
show that MeMoSORT achieves state-of-the-art performance, with HOTA scores of
67.9\% and 82.1\%, respectively.

</details>


### [92] [MUJICA: Reforming SISR Models for PBR Material Super-Resolution via Cross-Map Attention](https://arxiv.org/abs/2508.09802)
*Xin Du,Maoyuan Xu,Zhi Ying*

Main category: cs.CV

TL;DR: MUJICA是一种用于PBR材质超分辨率的适配器，通过跨图注意力机制提升预训练SISR模型的性能，解决了现有方法在跨图一致性、模态特征建模和数据分布偏移上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有SISR方法在处理PBR材质超分辨率时存在跨图不一致、模态特征建模不足和数据分布偏移导致的泛化能力有限的问题。

Method: 提出MUJICA适配器，基于预训练的Swin-transformer SISR模型，通过跨图注意力机制融合特征，保持预训练模型的优秀重建能力。

Result: 在SwinIR、DRCT和HMANet等SISR模型上，MUJICA提升了PSNR、SSIM和LPIPS分数，同时保持了跨图一致性。

Conclusion: MUJICA在有限资源下实现高效训练，并在PBR材质数据集上达到最先进性能。

Abstract: Physically Based Rendering (PBR) materials are typically characterized by
multiple 2D texture maps such as basecolor, normal, metallic, and roughness
which encode spatially-varying bi-directional reflectance distribution function
(SVBRDF) parameters to model surface reflectance properties and microfacet
interactions. Upscaling SVBRDF material is valuable for modern 3D graphics
applications. However, existing Single Image Super-Resolution (SISR) methods
struggle with cross-map inconsistency, inadequate modeling of modality-specific
features, and limited generalization due to data distribution shifts. In this
work, we propose Multi-modal Upscaling Joint Inference via Cross-map Attention
(MUJICA), a flexible adapter that reforms pre-trained Swin-transformer-based
SISR models for PBR material super-resolution. MUJICA is seamlessly attached
after the pre-trained and frozen SISR backbone. It leverages cross-map
attention to fuse features while preserving remarkable reconstruction ability
of the pre-trained SISR model. Applied to SISR models such as SwinIR, DRCT, and
HMANet, MUJICA improves PSNR, SSIM, and LPIPS scores while preserving cross-map
consistency. Experiments demonstrate that MUJICA enables efficient training
even with limited resources and delivers state-of-the-art performance on PBR
material datasets.

</details>


### [93] [Automated Segmentation of Coronal Brain Tissue Slabs for 3D Neuropathology](https://arxiv.org/abs/2508.09805)
*Jonathan Williams Ramirez,Dina Zemlyanker,Lucas Deden-Binder,Rogeny Herisse,Erendira Garcia Pallares,Karthik Gopinath,Harshvardhan Gazula,Christopher Mount,Liana N. Kozanno,Michael S. Marshall,Theresa R. Connors,Matthew P. Frosch,Mark Montine,Derek H. Oakley,Christine L. Mac Donald,C. Dirk Keene,Bradley T. Hyman,Juan Eugenio Iglesias*

Main category: cs.CV

TL;DR: 提出了一种基于U-Net的深度学习模型，用于自动化分割脑组织照片，性能接近人工标注水平。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要昂贵的人工干预，自动化分割工具可提高效率。

Method: 使用U-Net架构，结合1,414张手动标注图像和2,000张合成图像训练。

Result: 模型在未见过的照片上表现优异，Dice分数中位数超过0.98。

Conclusion: 该工具公开可用，性能接近人工标注，适用于脑库和神经病理学实验室。

Abstract: Advances in image registration and machine learning have recently enabled
volumetric analysis of \emph{postmortem} brain tissue from conventional
photographs of coronal slabs, which are routinely collected in brain banks and
neuropathology laboratories worldwide. One caveat of this methodology is the
requirement of segmentation of the tissue from photographs, which currently
requires costly manual intervention. In this article, we present a deep
learning model to automate this process. The automatic segmentation tool relies
on a U-Net architecture that was trained with a combination of
\textit{(i)}1,414 manually segmented images of both fixed and fresh tissue,
from specimens with varying diagnoses, photographed at two different sites; and
\textit{(ii)}~2,000 synthetic images with randomized contrast and corresponding
masks generated from MRI scans for improved generalizability to unseen
photographic setups. Automated model predictions on a subset of photographs not
seen in training were analyzed to estimate performance compared to manual
labels -- including both inter- and intra-rater variability. Our model achieved
a median Dice score over 0.98, mean surface distance under 0.4~mm, and 95\%
Hausdorff distance under 1.60~mm, which approaches inter-/intra-rater levels.
Our tool is publicly available at surfer.nmr.mgh.harvard.edu/fswiki/PhotoTools.

</details>


### [94] [TRACE: Learning 3D Gaussian Physical Dynamics from Multi-view Videos](https://arxiv.org/abs/2508.09811)
*Jinxi Li,Ziyang Song,Bo Yang*

Main category: cs.CV

TL;DR: TRACE框架通过将3D点建模为刚性粒子，直接学习每个粒子的平移旋转动力学系统，无需人工标签即可建模动态3D场景的物理运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过物理约束或简单物理模型难以学习复杂运动物理，且需要额外标签。

Method: 将每个3D点视为具有大小和方向的刚性粒子，学习其平移旋转动力学系统，并估计物理参数。

Result: 在多个动态数据集上表现优异，尤其在未来帧外推任务中，且能通过聚类物理参数轻松分割对象。

Conclusion: TRACE框架在无监督学习复杂3D场景物理运动方面表现出色，并具有对象分割的潜力。

Abstract: In this paper, we aim to model 3D scene geometry, appearance, and physical
information just from dynamic multi-view videos in the absence of any human
labels. By leveraging physics-informed losses as soft constraints or
integrating simple physics models into neural nets, existing works often fail
to learn complex motion physics, or doing so requires additional labels such as
object types or masks. We propose a new framework named TRACE to model the
motion physics of complex dynamic 3D scenes. The key novelty of our method is
that, by formulating each 3D point as a rigid particle with size and
orientation in space, we directly learn a translation rotation dynamics system
for each particle, explicitly estimating a complete set of physical parameters
to govern the particle's motion over time. Extensive experiments on three
existing dynamic datasets and one newly created challenging synthetic datasets
demonstrate the extraordinary performance of our method over baselines in the
task of future frame extrapolation. A nice property of our framework is that
multiple objects or parts can be easily segmented just by clustering the
learned physical parameters.

</details>


### [95] [Poaching Hotspot Identification Using Satellite Imagery](https://arxiv.org/abs/2508.09812)
*Aryan Pandhi,Shrey Baid,Sanjali Jha*

Main category: cs.CV

TL;DR: 论文提出利用计算机视觉模型识别非洲大象偷猎热点区域，通过地理和环境因素动态预测偷猎活动，以优化反偷猎资源部署。


<details>
  <summary>Details</summary>
Motivation: 非洲大象偷猎问题日益严重，传统反偷猎方法效率低下，需要一种能动态监测和预测偷猎热点的新技术。

Method: 结合计算机视觉模型和卫星图像，分析地理和环境因素（如巡逻密度、水源、季节等）来识别偷猎热点。

Result: 模型能够动态追踪偷猎热点，减少对人工巡逻的依赖，并避免干扰当地生态或跨境航空限制。

Conclusion: 计算机视觉模型为反偷猎工作提供了高效、动态的解决方案，有望显著降低偷猎活动。

Abstract: Elephant Poaching in African countries has been a decade-old problem. So much
so that African Forest Elephants are now listed as an endangered species, and
African Savannah Elephants as critically endangered by the IUCN (International
Union for Conservation of Nature). [1] Elephants are hunted primarily for their
ivory tusks which caused many elephants to be born tuskless as a genetic
modification for survival. [2] Data gathered by recent studies shows that
though poaching methods remain the same, the poaching grounds are rather
dynamic. Poachers have shifted to areas with less ranger patrols and several
other factors like watering holes, seasons, altitude etc. cause constant shifts
in poaching hotspot locations. [3] After a period of low poaching from
2000-2014, poaching numbers in African countries are now on the rise again --
WWF (World Wildlife Foundation) says there are 20,000 elephants poached
annually [4]. In African countries, anti-poaching efforts are concentrated near
towns, while a majority of poaching occurs in the deserted regions. All of
these factors result in the need for a Computer Vision Model to identify
poaching hotspots through locating the geographic indicators of favorable
poaching regions. A CV model eliminates the need to manually track poachers and
account for the environmental factors to deploy resources and its combination
with satellite imagery allows us to survey large areas without disturbing local
species or cross border aviation restrictions.

</details>


### [96] [Evolution of Low-Level and Texture Human-CLIP Alignment](https://arxiv.org/abs/2508.09814)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Jesus Malo,Valero Laparra*

Main category: cs.CV

TL;DR: CLIP模型训练早期与低层次人类图像质量评估相关性高，后期下降，原因与形状-纹理偏差和噪声下分类准确性有关。


<details>
  <summary>Details</summary>
Motivation: 研究CLIP训练过程中与低层次人类感知相关性变化的原因。

Method: 分析形状-纹理偏差和噪声下的分类准确性。

Result: CLIP早期学习低层次视觉特征，后期转向抽象形状表示，影响感知对齐与鲁棒性。

Conclusion: 揭示了感知对齐与鲁棒性的权衡机制，为优化视觉语言模型提供新思路。

Abstract: During the training of multi-modal models like CLIP, we observed an
intriguing phenomenon: the correlation with low-level human image quality
assessments peaks in the early epochs before gradually declining. This study
investigates this observation and seeks to understand its causes through two
key factors: shape-texture bias alignment and classification accuracy drop
under noise. Our findings suggest that CLIP initially learn low-level visual
features, enhancing its alignment with low-level human perception but also
increasing its sensitivity to noise and its texture bias. As training
progresses, the model shifts toward more abstract shape-based representations,
improving noise robustness but reducing alignment with low-level human
perception. These results suggest that these factors shared an underlying
learning mechanism and provide new insights into optimizing the trade-off
between perceptual alignment and robustness in vision-language models.

</details>


### [97] [ViMoNet: A Multimodal Vision-Language Framework for Human Behavior Understanding from Motion and Video](https://arxiv.org/abs/2508.09818)
*Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Abir Ahmed,Liew Tze Hui*

Main category: cs.CV

TL;DR: ViMoNet结合视频和运动数据，通过联合训练策略提升对人类行为的理解，并引入新数据集VIMOS和评估基准ViMoNet-Bench，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅关注单一数据类型（运动或视频），无法全面捕捉人类行为的细微差别，因此需要结合两者。

Method: 提出ViMoNet框架，采用联合训练策略，结合精确的运动-文本数据和通用的视频-文本数据。

Result: ViMoNet在字幕生成、运动理解和行为解释任务中表现优于现有方法。

Conclusion: 结合多模态数据能更全面地理解人类行为，ViMoNet为相关研究提供了新方向。

Abstract: This study investigates how large language models (LLMs) can be used to
understand human behavior using motion and video data. We think that mixing
both types is essential to completely capture the nuanced movements and
meanings of human actions, in contrast to recent models that simply concentrate
on motion data or films. To address this, we provide ViMoNet, a straightforward
yet effective framework for comprehending, characterizing, and deducing human
action. ViMoNet employs a joint training strategy that leverages the advantages
of two data types: detailed motion-text data, which is more exact, and generic
video-text data, which is more comprehensive but less detailed. This aids in
the model's acquisition of rich data regarding time and space in human
behavior. Additionally, we provide a brand new dataset named VIMOS that
contains a variety of films, motion sequences, instructions, and subtitles. We
developed ViMoNet-Bench, a standardized benchmark with carefully labeled
samples, to evaluate how well models understand human behavior. Our tests show
that ViMoNet outperforms existing methods in caption generation, motion
understanding, and behavior interpretation.

</details>


### [98] [Physical Autoregressive Model for Robotic Manipulation without Action Pretraining](https://arxiv.org/abs/2508.09822)
*Zijian Song,Sihan Qin,Tianshui Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: 提出了一种物理自回归模型（PAR），利用视频预训练中的世界知识进行机器人操作，无需动作预训练即可实现准确的视频预测和一致的动作轨迹。


<details>
  <summary>Details</summary>
Motivation: 机器人操作数据稀缺，促使研究者利用其他模态的预训练大模型。

Method: PAR结合帧和动作作为物理令牌，采用DiT-based去令牌化器建模连续令牌，并引入因果掩码、逆运动学、并行训练和KV缓存机制。

Result: 在ManiSkill基准测试中，PAR在PushCube任务上达到100%成功率，其他任务表现与动作预训练基线相当，并能准确预测未来视频。

Conclusion: 通过从自回归视频预训练中迁移世界知识，PAR为机器人操作提供了一种有前景的方向。

Abstract: The scarcity of manipulation data has motivated the use of pretrained large
models from other modalities in robotics. In this work, we build upon
autoregressive video generation models to propose a Physical Autoregressive
Model (PAR), where physical tokens combine frames and actions to represent the
joint evolution of the robot and its environment. PAR leverages the world
knowledge embedded in video pretraining to understand physical dynamics without
requiring action pretraining, enabling accurate video prediction and consistent
action trajectories. It also adopts a DiT-based de-tokenizer to model frames
and actions as continuous tokens, mitigating quantization errors and
facilitating mutual enhancement. Furthermore, we incorporate a causal mask with
inverse kinematics, parallel training, and the KV-cache mechanism to further
improve performance and efficiency. Experiments on the ManiSkill benchmark show
that PAR achieves a 100\% success rate on the PushCube task, matches the
performance of action-pretrained baselines on other tasks, and accurately
predicts future videos with tightly aligned action trajectories. These findings
underscore a promising direction for robotic manipulation by transferring world
knowledge from autoregressive video pretraining.

</details>


### [99] [KonfAI: A Modular and Fully Configurable Framework for Deep Learning in Medical Imaging](https://arxiv.org/abs/2508.09823)
*Valentin Boussot,Jean-Louis Dillenseger*

Main category: cs.CV

TL;DR: KonfAI是一个模块化、可扩展且完全可配置的深度学习框架，专为医学影像任务设计，通过YAML配置文件实现训练、推理和评估工作流，提升可重复性和开发效率。


<details>
  <summary>Details</summary>
Motivation: 设计KonfAI的目的是为了简化医学影像任务的深度学习流程，通过声明式配置增强实验的可重复性和透明度，同时减少开发时间。

Method: KonfAI采用模块化和可扩展架构，支持通过YAML文件定义工作流，并提供高级策略如基于补丁的学习、测试时增强、模型集成等。

Result: KonfAI已成功应用于分割、配准和图像合成任务，并在多个国际医学影像挑战中取得领先成果。

Conclusion: KonfAI是一个开源框架，通过其灵活性和高级功能，显著提升了医学影像任务的效率和效果。

Abstract: KonfAI is a modular, extensible, and fully configurable deep learning
framework specifically designed for medical imaging tasks. It enables users to
define complete training, inference, and evaluation workflows through
structured YAML configuration files, without modifying the underlying code.
This declarative approach enhances reproducibility, transparency, and
experimental traceability while reducing development time. Beyond the
capabilities of standard pipelines, KonfAI provides native abstractions for
advanced strategies including patch-based learning, test-time augmentation,
model ensembling, and direct access to intermediate feature representations for
deep supervision. It also supports complex multi-model training setups such as
generative adversarial architectures. Thanks to its modular and extensible
architecture, KonfAI can easily accommodate custom models, loss functions, and
data processing components. The framework has been successfully applied to
segmentation, registration, and image synthesis tasks, and has contributed to
top-ranking results in several international medical imaging challenges. KonfAI
is open source and available at
\href{https://github.com/vboussot/KonfAI}{https://github.com/vboussot/KonfAI}.

</details>


### [100] [Reverse Convolution and Its Applications to Image Restoration](https://arxiv.org/abs/2508.09824)
*Xuhong Huang,Shiqi Liu,Kai Zhang,Ying Tai,Jian Yang,Hui Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的深度反向卷积算子，用于有效反转深度卷积，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统转置卷积并非卷积的数学逆运算，缺乏标准反向卷积算子，阻碍了神经网络架构的发展。

Method: 通过正则化最小二乘优化问题设计深度反向卷积算子，并结合层归一化、1×1卷积和GELU激活构建反向卷积块。

Result: 实验表明，该算子能有效替代传统卷积和转置卷积层，在图像去噪、超分辨率和去模糊任务中表现优异。

Conclusion: 该研究为深度模型设计中的新算子开发提供了基础，有望推动相关应用的发展。

Abstract: Convolution and transposed convolution are fundamental operators widely used
in neural networks. However, transposed convolution (a.k.a. deconvolution) does
not serve as a true inverse of convolution due to inherent differences in their
mathematical formulations. To date, no reverse convolution operator has been
established as a standard component in neural architectures. In this paper, we
propose a novel depthwise reverse convolution operator as an initial attempt to
effectively reverse depthwise convolution by formulating and solving a
regularized least-squares optimization problem. We thoroughly investigate its
kernel initialization, padding strategies, and other critical aspects to ensure
its effective implementation. Building upon this operator, we further construct
a reverse convolution block by combining it with layer normalization,
1$\times$1 convolution, and GELU activation, forming a Transformer-like
structure. The proposed operator and block can directly replace conventional
convolution and transposed convolution layers in existing architectures,
leading to the development of ConverseNet. Corresponding to typical image
restoration models such as DnCNN, SRResNet and USRNet, we train three variants
of ConverseNet for Gaussian denoising, super-resolution and deblurring,
respectively. Extensive experiments demonstrate the effectiveness of the
proposed reverse convolution operator as a basic building module. We hope this
work could pave the way for developing new operators in deep model design and
applications.

</details>


### [101] [RayletDF: Raylet Distance Fields for Generalizable 3D Surface Reconstruction from Point Clouds or Gaussians](https://arxiv.org/abs/2508.09830)
*Shenxing Wei,Jinxi Li,Yafei Yang,Siyuan Zhou,Bo Yang*

Main category: cs.CV

TL;DR: 提出了一种名为RayletDF的新方法，用于从点云或3D高斯中重建3D表面，具有高效和泛化能力强的特点。


<details>
  <summary>Details</summary>
Motivation: 现有基于坐标的方法在渲染显式表面时计算量大，需要一种更高效且泛化能力强的表面重建方法。

Method: 通过raylet距离场技术，结合射线特征提取器、距离场预测器和多射线混合器，直接预测表面点。

Result: 在多个公开数据集上表现优异，尤其在泛化能力上，能够在单次前向传递中重建未见数据集的3D表面。

Conclusion: RayletDF方法在3D表面重建中表现出高效性和强大的泛化能力，适用于多种输入数据。

Abstract: In this paper, we present a generalizable method for 3D surface
reconstruction from raw point clouds or pre-estimated 3D Gaussians by 3DGS from
RGB images. Unlike existing coordinate-based methods which are often
computationally intensive when rendering explicit surfaces, our proposed
method, named RayletDF, introduces a new technique called raylet distance
field, which aims to directly predict surface points from query rays. Our
pipeline consists of three key modules: a raylet feature extractor, a raylet
distance field predictor, and a multi-raylet blender. These components work
together to extract fine-grained local geometric features, predict raylet
distances, and aggregate multiple predictions to reconstruct precise surface
points. We extensively evaluate our method on multiple public real-world
datasets, demonstrating superior performance in surface reconstruction from
point clouds or 3D Gaussians. Most notably, our method achieves exceptional
generalization ability, successfully recovering 3D surfaces in a single-forward
pass across unseen datasets in testing.

</details>


### [102] [Hierarchical Graph Attention Network for No-Reference Omnidirectional Image Quality Assessment](https://arxiv.org/abs/2508.09843)
*Hao Yang,Xu Zhang,Jiaqi Ma,Linwei Zhu,Yun Zhang,Huan Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于图神经网络的OIQA框架，通过建模视口间的结构关系来提升对空间失真非均匀性的感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有OIQA方法难以评估局部非均匀失真，因未能有效建模质量的空间变化和特征表示。

Method: 采用斐波那契球采样生成视口，用图节点表示，结合GAT和图变换器建模局部和全局空间依赖关系。

Result: 在两个大型OIQA数据库上的实验表明，该方法显著优于现有方法。

Conclusion: 该方法有效且具有强泛化能力。

Abstract: Current Omnidirectional Image Quality Assessment (OIQA) methods struggle to
evaluate locally non-uniform distortions due to inadequate modeling of spatial
variations in quality and ineffective feature representation capturing both
local details and global context. To address this, we propose a graph neural
network-based OIQA framework that explicitly models structural relationships
between viewports to enhance perception of spatial distortion non-uniformity.
Our approach employs Fibonacci sphere sampling to generate viewports with
well-structured topology, representing each as a graph node. Multi-stage
feature extraction networks then derive high-dimensional node representation.
To holistically capture spatial dependencies, we integrate a Graph Attention
Network (GAT) modeling fine-grained local distortion variations among adjacent
viewports, and a graph transformer capturing long-range quality interactions
across distant regions. Extensive experiments on two large-scale OIQA databases
with complex spatial distortions demonstrate that our method significantly
outperforms existing approaches, confirming its effectiveness and strong
generalization capability.

</details>


### [103] [Enhancing Diffusion Face Generation with Contrastive Embeddings and SegFormer Guidance](https://arxiv.org/abs/2508.09847)
*Dhruvraj Singh Rawat,Enggen Sherpa,Rishikesan Kirupanantha,Tin Hoang*

Main category: cs.CV

TL;DR: 论文提出了一个在小规模CelebAMask-HQ数据集上评估扩散模型生成人脸的基准，比较了UNet和DiT架构的无条件生成，并探索了基于LoRA的预训练Stable Diffusion模型微调。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提升属性引导合成中的语义对齐和可控性，特别是在有限数据环境下。

Method: 集成了InfoNCE损失用于属性嵌入，并采用SegFormer-based分割编码器，改进了多条件方法。

Result: 结果表明对比嵌入学习和高级分割编码在有限数据环境下对控制人脸生成有效。

Conclusion: 通过对比嵌入和先进分割编码的集成，提升了属性引导合成的语义对齐和可控性。

Abstract: We present a benchmark of diffusion models for human face generation on a
small-scale CelebAMask-HQ dataset, evaluating both unconditional and
conditional pipelines. Our study compares UNet and DiT architectures for
unconditional generation and explores LoRA-based fine-tuning of pretrained
Stable Diffusion models as a separate experiment. Building on the
multi-conditioning approach of Giambi and Lisanti, which uses both attribute
vectors and segmentation masks, our main contribution is the integration of an
InfoNCE loss for attribute embedding and the adoption of a SegFormer-based
segmentation encoder. These enhancements improve the semantic alignment and
controllability of attribute-guided synthesis. Our results highlight the
effectiveness of contrastive embedding learning and advanced segmentation
encoding for controlled face generation in limited data settings.

</details>


### [104] [ARI3D: A Software for Interactive Quantification of Regions in X-Ray CT 3D Images](https://arxiv.org/abs/2508.09849)
*Jan Phillipp Albrecht,Jose R. A. Godinho,Christina Hübers,Deborah Schmidt*

Main category: cs.CV

TL;DR: ARI3D是一个用于交互式分析3D X射线CT图像区域的软件工具，旨在改进相位识别、考虑部分体积效应、提高检测限和准确性，并统一不同科学领域的定量3D分析。


<details>
  <summary>Details</summary>
Motivation: X射线CT成像技术存在固有的成像伪影（如束硬化和部分体积效应），使得用户需要基于体素灰度值对微结构进行分割和分类，这增加了分析的复杂性。

Method: 开发了ARI3D软件工具，通过交互式分析3D CT图像区域，辅助用户完成分类和量化协议中的各个步骤。

Result: ARI3D能够改进相位识别、处理部分体积效应、提高检测限和量化准确性，并促进跨领域的统一分析。

Conclusion: ARI3D为3D X射线CT图像的定量分析提供了一种有效的交互式工具，有助于克服现有技术中的挑战并提升分析效率。

Abstract: X-ray computed tomography (CT) is the main 3D technique for imaging the
internal microstructures of materials. Quantitative analysis of the
microstructures is usually achieved by applying a sequence of steps that are
implemented to the entire 3D image. This is challenged by various imaging
artifacts inherent from the technique, e.g., beam hardening and partial volume.
Consequently, the analysis requires users to make a number of decisions to
segment and classify the microstructures based on the voxel gray-values. In
this context, a software tool, here called ARI3D, is proposed to interactively
analyze regions in three-dimensional X-ray CT images, assisting users through
the various steps of a protocol designed to classify and quantify objects
within regions of a three-dimensional image. ARI3D aims to 1) Improve phase
identification; 2) Account for partial volume effect; 3) Increase the detection
limit and accuracy of object quantification; and 4) Harmonize quantitative 3D
analysis that can be implemented in different fields of science.

</details>


### [105] [Do Vision Transformers See Like Humans? Evaluating their Perceptual Alignment](https://arxiv.org/abs/2508.09850)
*Pablo Hernández-Cámara,Jose Manuel Jaén-Lorites,Jorge Vila-Tomás,Valero Laparra,Jesus Malo*

Main category: cs.CV

TL;DR: 论文研究了Vision Transformers（ViTs）在图像识别任务中的表现与人类感知的对齐情况，发现模型大小、数据集多样性、数据增强和正则化等因素会影响对齐效果。


<details>
  <summary>Details</summary>
Motivation: 探索ViTs在图像识别任务中的表现与人类感知的对齐情况，以了解模型复杂性和训练策略如何影响这种对齐。

Method: 通过系统分析模型大小、数据集大小、数据增强和正则化对ViT在TID2013数据集上与人类感知对齐的影响。

Result: 发现更大的模型表现出更低的对齐性；增加数据集多样性影响较小，但重复训练会降低对齐性；更强的数据增强和正则化进一步降低对齐性。

Conclusion: 研究揭示了模型复杂性、训练策略与人类感知对齐之间的权衡，对需要类人视觉理解的应用提出了重要考虑。

Abstract: Vision Transformers (ViTs) achieve remarkable performance in image
recognition tasks, yet their alignment with human perception remains largely
unexplored. This study systematically analyzes how model size, dataset size,
data augmentation and regularization impact ViT perceptual alignment with human
judgments on the TID2013 dataset. Our findings confirm that larger models
exhibit lower perceptual alignment, consistent with previous works. Increasing
dataset diversity has a minimal impact, but exposing models to the same images
more times reduces alignment. Stronger data augmentation and regularization
further decrease alignment, especially in models exposed to repeated training
cycles. These results highlight a trade-off between model complexity, training
strategies, and alignment with human perception, raising important
considerations for applications requiring human-like visual understanding.

</details>


### [106] [OneVAE: Joint Discrete and Continuous Optimization Helps Discrete Video VAE Train Better](https://arxiv.org/abs/2508.09857)
*Yupeng Zhou,Zhen Li,Ziheng Ouyang,Yuming Chen,Ruoyi Du,Daquan Zhou,Bin Fu,Yihao Liu,Peng Gao,Ming-Ming Cheng,Qibin Hou*

Main category: cs.CV

TL;DR: OneVAE enhances discrete video VAEs by leveraging continuous VAE priors, achieving faster convergence and superior performance through multi-token quantization and improved first-frame reconstruction.


<details>
  <summary>Details</summary>
Motivation: To address unstable training, long training time, and degraded quality in discrete video VAEs by leveraging continuous VAE priors and improving structural designs.

Method: Utilizes FSQ for preserving continuous VAE priors, introduces multi-token quantization, strengthens first-frame reconstruction, and proposes a joint discrete-continuous optimization scheme.

Result: Achieves faster convergence, nearly 1 dB PSNR improvement, and competitive performance in both continuous and discrete representations.

Conclusion: OneVAE successfully unifies continuous and discrete video representations, offering a robust and efficient solution for video encoding.

Abstract: Encoding videos into discrete tokens could align with text tokens to
facilitate concise and unified multi-modal LLMs, yet introducing significant
spatiotemporal compression compared to continuous video representation.
Previous discrete video VAEs experienced unstable training, long training time,
and degraded reconstruction quality. Given the easier training and superior
performance of continuous VAEs, an intuitive idea is to enhance discrete video
VAEs by leveraging continuous VAEs. After rethinking the intrinsic link between
discrete and continuous representations, we found that FSQ could effectively
preserve pre-trained continuous VAE priors compared to other quantization
methods. By leveraging continuous VAE priors, it converges several times faster
than training from scratch and achieves superior performance at convergence.
Meanwhile, two structural improvements are proposed. First, inspired by how
continuous VAEs enhance reconstruction via enlarged latent dimensions, we
introduce a multi-token quantization mechanism, which achieves nearly a 1 dB
improvement in PSNR without compromising the token compression ratio. Second,
to tackle reconstruction challenges in high-compression video VAEs, we
strengthen first-frame reconstruction, enabling the causal VAE to leverage this
information in subsequent frames and markedly improving the performance of 4 x
16 x 16 discrete VAEs. Furthermore, we propose a joint discrete-continuous
optimization scheme that unifies the two paradigms and, for the first time,
achieves competitive performance on both continuous and discrete
representations within a single network. We name our method OneVAE to reflect
this connection.

</details>


### [107] [HumanGenesis: Agent-Based Geometric and Generative Modeling for Synthetic Human Dynamics](https://arxiv.org/abs/2508.09858)
*Weiqi Li,Zehao Zhang,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: HumanGenesis是一个结合几何与生成建模的框架，通过四个协作代理解决合成人类动态中的几何不一致和运动泛化问题。


<details>
  <summary>Details</summary>
Motivation: 当前方法在几何一致性和运动泛化方面存在不足，HumanGenesis旨在通过多代理协作提升重建保真度和视频合成质量。

Method: 框架包含Reconstructor、Critique Agent、Pose Guider和Video Harmonizer四个代理，分别负责3D重建、区域优化、运动生成和视频合成。

Result: HumanGenesis在文本引导合成、视频重演和新姿势泛化等任务中表现优异，显著提升了表达力、几何保真度和场景整合。

Conclusion: HumanGenesis通过多代理协作有效解决了合成人类动态中的核心挑战，实现了高质量的3D重建和视频合成。

Abstract: \textbf{Synthetic human dynamics} aims to generate photorealistic videos of
human subjects performing expressive, intention-driven motions. However,
current approaches face two core challenges: (1) \emph{geometric inconsistency}
and \emph{coarse reconstruction}, due to limited 3D modeling and detail
preservation; and (2) \emph{motion generalization limitations} and \emph{scene
inharmonization}, stemming from weak generative capabilities. To address these,
we present \textbf{HumanGenesis}, a framework that integrates geometric and
generative modeling through four collaborative agents: (1)
\textbf{Reconstructor} builds 3D-consistent human-scene representations from
monocular video using 3D Gaussian Splatting and deformation decomposition. (2)
\textbf{Critique Agent} enhances reconstruction fidelity by identifying and
refining poor regions via multi-round MLLM-based reflection. (3) \textbf{Pose
Guider} enables motion generalization by generating expressive pose sequences
using time-aware parametric encoders. (4) \textbf{Video Harmonizer} synthesizes
photorealistic, coherent video via a hybrid rendering pipeline with diffusion,
refining the Reconstructor through a Back-to-4D feedback loop. HumanGenesis
achieves state-of-the-art performance on tasks including text-guided synthesis,
video reenactment, and novel-pose generalization, significantly improving
expressiveness, geometric fidelity, and scene integration.

</details>


### [108] [COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets](https://arxiv.org/abs/2508.09886)
*Lingyu Chen,Yawen Zeng,Yue Wang,Peng Wan,Guo-chen Ning,Hongen Liao,Daoqiang Zhang,Fang Chen*

Main category: cs.CV

TL;DR: 提出了一种名为COME的通用协作异构专家混合框架，用于解决超声图像分析中多数据集训练的挑战，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 传统单数据集训练在新数据分布下表现不佳，尤其是在超声图像分析中，由于数据有限、声学阴影和斑点噪声等问题，构建多异构数据集的通用框架至关重要。

Method: COME通过建立双结构-语义共享专家，创建通用表示空间，并与源特定专家协作提取判别性特征，利用跨数据集经验分布和通用超声先验知识。

Result: 在三种评估模式下（单数据集、器官内和器官间集成数据集），COME显著优于现有方法，平均AP提升明显。

Conclusion: COME框架通过协作异构专家混合，有效解决了多数据集训练中的干扰问题，并提升了超声图像分析的鲁棒性和泛化能力。

Abstract: Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

</details>


### [109] [E-4DGS: High-Fidelity Dynamic Reconstruction from the Multi-view Event Cameras](https://arxiv.org/abs/2508.09912)
*Chaoran Feng,Zhenyu Tang,Wangbo Yu,Yatian Pang,Yian Zhao,Jianbin Zhao,Li Yuan,Yonghong Tian*

Main category: cs.CV

TL;DR: E-4DGS是一种基于事件相机的动态高斯泼溅方法，用于多视角事件流的新视角合成，解决了高速运动和低光场景的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统RGB相机在光照不足、运动模糊和动态范围有限等方面存在局限性，而事件相机具有低功耗、高时间分辨率和高动态范围的优势。

Method: 提出事件驱动的初始化方案、事件自适应切片泼溅、强度重要性剪枝和自适应对比度阈值优化。

Result: E-4DGS在合成多视角事件流数据集上表现优于仅事件和事件-RGB融合基线方法。

Conclusion: E-4DGS为多视角事件流重建提供了一种新方法，推动了快速场景捕捉的探索。

Abstract: Novel view synthesis and 4D reconstruction techniques predominantly rely on
RGB cameras, thereby inheriting inherent limitations such as the dependence on
adequate lighting, susceptibility to motion blur, and a limited dynamic range.
Event cameras, offering advantages of low power, high temporal resolution and
high dynamic range, have brought a new perspective to addressing the scene
reconstruction challenges in high-speed motion and low-light scenes. To this
end, we propose E-4DGS, the first event-driven dynamic Gaussian Splatting
approach, for novel view synthesis from multi-view event streams with
fast-moving cameras. Specifically, we introduce an event-based initialization
scheme to ensure stable training and propose event-adaptive slicing splatting
for time-aware reconstruction. Additionally, we employ intensity importance
pruning to eliminate floating artifacts and enhance 3D consistency, while
incorporating an adaptive contrast threshold for more precise optimization. We
design a synthetic multi-view camera setup with six moving event cameras
surrounding the object in a 360-degree configuration and provide a benchmark
multi-view event stream dataset that captures challenging motion scenarios. Our
approach outperforms both event-only and event-RGB fusion baselines and paves
the way for the exploration of multi-view event-based reconstruction as a novel
approach for rapid scene capture.

</details>


### [110] [SpeechForensics: Audio-Visual Speech Representation Learning for Face Forgery Detection](https://arxiv.org/abs/2508.09913)
*Yachao Liang,Min Yu,Gang Li,Jianguo Jiang,Boquan Li,Feng Yu,Ning Zhang,Xiang Meng,Weiqing Huang*

Main category: cs.CV

TL;DR: 提出了一种基于音频-视觉语音表示学习的方法，用于检测伪造视频，无需训练数据中的伪造视频即可实现跨数据集泛化和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 音频信号富含语音内容，能精确反映面部运动，为伪造视频检测提供了新思路。

Method: 通过自监督掩码预测任务学习音频-视觉语音表示，并将其直接迁移到伪造检测任务。

Result: 在跨数据集泛化和鲁棒性方面优于现有方法。

Conclusion: 该方法展示了音频-视觉协同在伪造检测中的潜力，且无需伪造视频训练数据。

Abstract: Detection of face forgery videos remains a formidable challenge in the field
of digital forensics, especially the generalization to unseen datasets and
common perturbations. In this paper, we tackle this issue by leveraging the
synergy between audio and visual speech elements, embarking on a novel approach
through audio-visual speech representation learning. Our work is motivated by
the finding that audio signals, enriched with speech content, can provide
precise information effectively reflecting facial movements. To this end, we
first learn precise audio-visual speech representations on real videos via a
self-supervised masked prediction task, which encodes both local and global
semantic information simultaneously. Then, the derived model is directly
transferred to the forgery detection task. Extensive experiments demonstrate
that our method outperforms the state-of-the-art methods in terms of
cross-dataset generalization and robustness, without the participation of any
fake video in model training. Code is available at
https://github.com/Eleven4AI/SpeechForensics.

</details>


### [111] [Towards Comprehensive Cellular Characterisation of H&E slides](https://arxiv.org/abs/2508.09926)
*Benjamin Adjadj,Pierre-Antoine Bannier,Guillaume Horent,Sebastien Mandela,Aurore Lyon,Kathryn Schutte,Ulysse Marteau,Valentin Gaury,Laura Dumont,Thomas Mathieu,Reda Belbahri,Benoît Schmauch,Eric Durand,Katharina Von Loga,Lucie Gillet*

Main category: cs.CV

TL;DR: HistoPLUS是一种先进的细胞分析模型，在检测质量和分类性能上显著优于现有方法，并支持对罕见细胞类型的研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在罕见细胞类型和跨域泛化上的性能不足问题。

Method: 基于一个包含108,722个细胞核的新数据集训练，覆盖13种细胞类型。

Result: 在外部验证中，检测质量提升5.2%，分类F1分数提升23.7%，且参数减少5倍。

Conclusion: HistoPLUS显著提升细胞分析性能，支持更广泛的肿瘤微环境研究。

Abstract: Cell detection, segmentation and classification are essential for analyzing
tumor microenvironments (TME) on hematoxylin and eosin (H&E) slides. Existing
methods suffer from poor performance on understudied cell types (rare or not
present in public datasets) and limited cross-domain generalization. To address
these shortcomings, we introduce HistoPLUS, a state-of-the-art model for cell
analysis, trained on a novel curated pan-cancer dataset of 108,722 nuclei
covering 13 cell types. In external validation across 4 independent cohorts,
HistoPLUS outperforms current state-of-the-art models in detection quality by
5.2% and overall F1 classification score by 23.7%, while using 5x fewer
parameters. Notably, HistoPLUS unlocks the study of 7 understudied cell types
and brings significant improvements on 8 of 13 cell types. Moreover, we show
that HistoPLUS robustly transfers to two oncology indications unseen during
training. To support broader TME biomarker research, we release the model
weights and inference code at https://github.com/owkin/histoplus/.

</details>


### [112] [Quo Vadis Handwritten Text Generation for Handwritten Text Recognition?](https://arxiv.org/abs/2508.09936)
*Vittorio Pippi,Konstantina Nikolaidou,Silvia Cascianelli,George Retsinas,Giorgos Sfikas,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 本文系统比较了三种手写文本生成（HTG）模型，评估其对低资源手写文本识别（HTR）性能的影响，并提供了选择最有效模型的定量指南。


<details>
  <summary>Details</summary>
Motivation: 历史手稿数字化对HTR系统提出了挑战，尤其是处理与训练数据分布不同的作者特定小规模数据集时。HTG技术有望通过生成特定手写风格的合成数据来解决这些问题。

Method: 系统比较了三种最先进的HTG模型（生成对抗、扩散和自回归范式），分析合成数据的视觉和语言特征对HTR微调的影响。

Result: 研究结果揭示了当前HTG方法的能力，并指出了在低资源HTR应用中需要改进的关键领域。

Conclusion: HTG模型在提升低资源HTR性能方面具有潜力，但仍需进一步改进。

Abstract: The digitization of historical manuscripts presents significant challenges
for Handwritten Text Recognition (HTR) systems, particularly when dealing with
small, author-specific collections that diverge from the training data
distributions. Handwritten Text Generation (HTG) techniques, which generate
synthetic data tailored to specific handwriting styles, offer a promising
solution to address these challenges. However, the effectiveness of various HTG
models in enhancing HTR performance, especially in low-resource transcription
settings, has not been thoroughly evaluated. In this work, we systematically
compare three state-of-the-art styled HTG models (representing the generative
adversarial, diffusion, and autoregressive paradigms for HTG) to assess their
impact on HTR fine-tuning. We analyze how visual and linguistic characteristics
of synthetic data influence fine-tuning outcomes and provide quantitative
guidelines for selecting the most effective HTG model. The results of our
analysis provide insights into the current capabilities of HTG methods and
highlight key areas for further improvement in their application to
low-resource HTR.

</details>


### [113] [AST-n: A Fast Sampling Approach for Low-Dose CT Reconstruction using Diffusion Models](https://arxiv.org/abs/2508.09943)
*Tomás de la Sotta,José M. Saavedra,Héctor Henríquez,Violeta Chang,Aline Xavier*

Main category: cs.CV

TL;DR: AST-n框架通过从中间噪声水平启动反向扩散并集成高阶ODE求解器，显著加速了LDCT去噪的推理过程，同时保持了图像质量。


<details>
  <summary>Details</summary>
Motivation: 低剂量CT（LDCT）协议减少了辐射暴露，但增加了图像噪声，影响诊断信心。扩散生成模型通过学习图像先验和迭代细化，显示出在LDCT去噪中的潜力。

Method: 提出AST-n框架，从中间噪声水平启动反向扩散，并在条件模型中集成高阶ODE求解器以减少采样步骤。评估了两种加速范式：AST-n采样和标准调度与高阶求解器。

Result: AST-25（仅25步）在低剂量CT数据集上实现了PSNR超过38 dB和SSIM超过0.95，接近标准基线，同时将推理时间从约16秒缩短到每切片1秒以下。无条件采样质量显著下降。

Conclusion: AST-n与高阶采样器结合，能够在快速LDCT重建中不显著损失图像保真度，推动了扩散方法在临床工作流程中的可行性。

Abstract: Low-dose CT (LDCT) protocols reduce radiation exposure but increase image
noise, compromising diagnostic confidence. Diffusion-based generative models
have shown promise for LDCT denoising by learning image priors and performing
iterative refinement. In this work, we introduce AST-n, an accelerated
inference framework that initiates reverse diffusion from intermediate noise
levels, and integrate high-order ODE solvers within conditioned models to
further reduce sampling steps. We evaluate two acceleration paradigms--AST-n
sampling and standard scheduling with high-order solvers -- on the Low Dose CT
Grand Challenge dataset, covering head, abdominal, and chest scans at 10-25 %
of standard dose. Conditioned models using only 25 steps (AST-25) achieve peak
signal-to-noise ratio (PSNR) above 38 dB and structural similarity index (SSIM)
above 0.95, closely matching standard baselines while cutting inference time
from ~16 seg to under 1 seg per slice. Unconditional sampling suffers
substantial quality loss, underscoring the necessity of conditioning. We also
assess DDIM inversion, which yields marginal PSNR gains at the cost of doubling
inference time, limiting its clinical practicality. Our results demonstrate
that AST-n with high-order samplers enables rapid LDCT reconstruction without
significant loss of image fidelity, advancing the feasibility of
diffusion-based methods in clinical workflows.

</details>


### [114] [Stable Diffusion Models are Secretly Good at Visual In-Context Learning](https://arxiv.org/abs/2508.09949)
*Trevine Oorloff,Vishwanath Sindagi,Wele Gedara Chaminda Bandara,Ali Shafahi,Amin Ghiasi,Charan Prakash,Reza Ardekani*

Main category: cs.CV

TL;DR: Off-the-shelf Stable Diffusion models can be repurposed for visual in-context learning (V-ICL) without additional fine-tuning, achieving competitive performance across multiple vision tasks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for in-context learning in computer vision require specialized training or additional data, limiting generalizability. This work aims to simplify the process by leveraging Stable Diffusion models.

Method: The authors propose an in-place attention re-computation within the self-attention layers of Stable Diffusion to incorporate context between query and example prompts.

Result: The method improves performance on six vision tasks, e.g., boosting mIoU for foreground segmentation by 8.9% and 3.2% over recent methods. Ensembling multiple prompts further enhances performance.

Conclusion: The study demonstrates that Stable Diffusion models can be effectively adapted for V-ICL, offering a simpler and more generalizable approach compared to existing methods.

Abstract: Large language models (LLM) in natural language processing (NLP) have
demonstrated great potential for in-context learning (ICL) -- the ability to
leverage a few sets of example prompts to adapt to various tasks without having
to explicitly update the model weights. ICL has recently been explored for
computer vision tasks with promising early outcomes. These approaches involve
specialized training and/or additional data that complicate the process and
limit its generalizability. In this work, we show that off-the-shelf Stable
Diffusion models can be repurposed for visual in-context learning (V-ICL).
Specifically, we formulate an in-place attention re-computation within the
self-attention layers of the Stable Diffusion architecture that explicitly
incorporates context between the query and example prompts. Without any
additional fine-tuning, we show that this repurposed Stable Diffusion model is
able to adapt to six different tasks: foreground segmentation, single object
detection, semantic segmentation, keypoint detection, edge detection, and
colorization. For example, the proposed approach improves the mean intersection
over union (mIoU) for the foreground segmentation task on Pascal-5i dataset by
8.9% and 3.2% over recent methods such as Visual Prompting and IMProv,
respectively. Additionally, we show that the proposed method is able to
effectively leverage multiple prompts through ensembling to infer the task
better and further improve the performance.

</details>


### [115] [LIA-X: Interpretable Latent Portrait Animator](https://arxiv.org/abs/2508.09959)
*Yaohui Wang,Di Yang,Xinyuan Chen,Francois Bremond,Yu Qiao,Antitza Dantcheva*

Main category: cs.CV

TL;DR: LIA-X是一种新型可解释的肖像动画生成器，通过稀疏运动字典实现精细控制的面部动态迁移。


<details>
  <summary>Details</summary>
Motivation: 传统方法在面部动态迁移中缺乏可解释性和精细控制，LIA-X旨在解决这一问题。

Method: 采用自编码器框架，通过稀疏运动字典将面部动态分解为可解释因子，支持“编辑-变形-渲染”策略。

Result: 在多个基准测试中，LIA-X在自重现和跨重现任务上优于现有方法，并支持用户引导的精细编辑。

Conclusion: LIA-X通过可解释和可控的设计，为肖像动画和视频编辑提供了高效且灵活的解决方案。

Abstract: We introduce LIA-X, a novel interpretable portrait animator designed to
transfer facial dynamics from a driving video to a source portrait with
fine-grained control. LIA-X is an autoencoder that models motion transfer as a
linear navigation of motion codes in latent space. Crucially, it incorporates a
novel Sparse Motion Dictionary that enables the model to disentangle facial
dynamics into interpretable factors. Deviating from previous 'warp-render'
approaches, the interpretability of the Sparse Motion Dictionary allows LIA-X
to support a highly controllable 'edit-warp-render' strategy, enabling precise
manipulation of fine-grained facial semantics in the source portrait. This
helps to narrow initial differences with the driving video in terms of pose and
expression. Moreover, we demonstrate the scalability of LIA-X by successfully
training a large-scale model with approximately 1 billion parameters on
extensive datasets. Experimental results show that our proposed method
outperforms previous approaches in both self-reenactment and cross-reenactment
tasks across several benchmarks. Additionally, the interpretable and
controllable nature of LIA-X supports practical applications such as
fine-grained, user-guided image and video editing, as well as 3D-aware portrait
video manipulation.

</details>


### [116] [January Food Benchmark (JFB): A Public Benchmark Dataset and Evaluation Suite for Multimodal Food Analysis](https://arxiv.org/abs/2508.09966)
*Amir Hosseinian,Ashkan Dehghani Zahedani,Umer Mansoor,Noosheen Hashemi,Mark Woodward*

Main category: cs.CV

TL;DR: 论文提出了一个名为JFB的公开食品图像数据集、一个全面的基准测试框架，以及一个专用模型，显著提升了自动营养分析的性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动营养分析领域缺乏标准化的评估方法和高质量的真实世界数据集，阻碍了研究进展。

Method: 引入JFB数据集（1000张食品图像）、设计基准测试框架（包括新指标和综合评分），并提供专用模型baseline。

Result: 专用模型在综合评分上达到86.2分，比通用模型提高了12.1分。

Conclusion: 该研究为自动营养分析领域提供了新的数据集和评估框架，有助于未来研究的发展。

Abstract: Progress in AI for automated nutritional analysis is critically hampered by
the lack of standardized evaluation methodologies and high-quality, real-world
benchmark datasets. To address this, we introduce three primary contributions.
First, we present the January Food Benchmark (JFB), a publicly available
collection of 1,000 food images with human-validated annotations. Second, we
detail a comprehensive benchmarking framework, including robust metrics and a
novel, application-oriented overall score designed to assess model performance
holistically. Third, we provide baseline results from both general-purpose
Vision-Language Models (VLMs) and our own specialized model,
january/food-vision-v1. Our evaluation demonstrates that the specialized model
achieves an Overall Score of 86.2, a 12.1-point improvement over the
best-performing general-purpose configuration. This work offers the research
community a valuable new evaluation dataset and a rigorous framework to guide
and benchmark future developments in automated nutritional analysis.

</details>


### [117] [MOC: Meta-Optimized Classifier for Few-Shot Whole Slide Image Classification](https://arxiv.org/abs/2508.09967)
*Tianqi Xiang,Yi Li,Qixiang Zhang,Xiaomeng Li*

Main category: cs.CV

TL;DR: 提出了一种元优化分类器（MOC），通过元学习自动优化分类器配置，显著提升了少样本学习下的WSI分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有少样本学习方法在数据稀缺时表现不佳，需要改进分类器设计以提升性能。

Method: MOC包含元学习器和分类器库，前者优化分类器配置，后者提供多样化的候选分类器。

Result: 在多个少样本基准测试中表现优异，TCGA-NSCLC基准上AUC提升10.4%，1-shot条件下提升26.25%。

Conclusion: MOC为临床部署提供了重要进展，尤其在诊断训练数据严重受限的情况下。

Abstract: Recent advances in histopathology vision-language foundation models (VLFMs)
have shown promise in addressing data scarcity for whole slide image (WSI)
classification via zero-shot adaptation. However, these methods remain
outperformed by conventional multiple instance learning (MIL) approaches
trained on large datasets, motivating recent efforts to enhance VLFM-based WSI
classification through fewshot learning paradigms. While existing few-shot
methods improve diagnostic accuracy with limited annotations, their reliance on
conventional classifier designs introduces critical vulnerabilities to data
scarcity. To address this problem, we propose a Meta-Optimized Classifier (MOC)
comprising two core components: (1) a meta-learner that automatically optimizes
a classifier configuration from a mixture of candidate classifiers and (2) a
classifier bank housing diverse candidate classifiers to enable a holistic
pathological interpretation. Extensive experiments demonstrate that MOC
outperforms prior arts in multiple few-shot benchmarks. Notably, on the
TCGA-NSCLC benchmark, MOC improves AUC by 10.4% over the state-of-the-art
few-shot VLFM-based methods, with gains up to 26.25% under 1-shot conditions,
offering a critical advancement for clinical deployments where diagnostic
training data is severely limited. Code is available at
https://github.com/xmed-lab/MOC.

</details>


### [118] [PERSONA: Personalized Whole-Body 3D Avatar with Pose-Driven Deformations from a Single Image](https://arxiv.org/abs/2508.09973)
*Geonhee Sim,Gyeongsik Moon*

Main category: cs.CV

TL;DR: PERSONA结合了3D和扩散模型的方法，从单张图像创建具有姿态驱动变形的高质量3D人体化身。


<details>
  <summary>Details</summary>
Motivation: 现有方法在姿态驱动变形和身份保持上存在不足，3D方法需要大量姿态丰富的视频，扩散方法难以保持身份一致性。

Method: PERSONA利用扩散模型从单张图像生成姿态丰富的视频，并通过平衡采样和几何加权优化优化3D化身。

Result: PERSONA能够从单张图像生成高质量且身份一致的3D化身，支持多样姿态。

Conclusion: PERSONA结合两种方法的优势，解决了单图像3D化身生成中的身份保持和姿态变形问题。

Abstract: Two major approaches exist for creating animatable human avatars. The first,
a 3D-based approach, optimizes a NeRF- or 3DGS-based avatar from videos of a
single person, achieving personalization through a disentangled identity
representation. However, modeling pose-driven deformations, such as non-rigid
cloth deformations, requires numerous pose-rich videos, which are costly and
impractical to capture in daily life. The second, a diffusion-based approach,
learns pose-driven deformations from large-scale in-the-wild videos but
struggles with identity preservation and pose-dependent identity entanglement.
We present PERSONA, a framework that combines the strengths of both approaches
to obtain a personalized 3D human avatar with pose-driven deformations from a
single image. PERSONA leverages a diffusion-based approach to generate
pose-rich videos from the input image and optimizes a 3D avatar based on them.
To ensure high authenticity and sharp renderings across diverse poses, we
introduce balanced sampling and geometry-weighted optimization. Balanced
sampling oversamples the input image to mitigate identity shifts in
diffusion-generated training videos. Geometry-weighted optimization prioritizes
geometry constraints over image loss, preserving rendering quality in diverse
poses.

</details>


### [119] [A Survey on 3D Gaussian Splatting Applications: Segmentation, Editing, and Generation](https://arxiv.org/abs/2508.09977)
*Shuting He,Peilin Ji,Yitong Yang,Changshuo Wang,Jiayi Ji,Yinglin Wang,Henghui Ding*

Main category: cs.CV

TL;DR: 3D高斯泼溅（3DGS）作为NeRF的替代方案，提供高保真实时渲染，并支持多种下游应用。本文综述了3DGS在分割、编辑、生成等任务中的进展。


<details>
  <summary>Details</summary>
Motivation: 探索3DGS在几何和语义理解方面的潜力，并总结其在多种应用中的最新进展。

Method: 综述了支持3DGS语义理解的2D基础模型、NeRF方法，并分类总结了3DGS应用的代表性方法。

Result: 总结了常用数据集、评估协议，并在公开基准上对比了最新方法。

Conclusion: 3DGS在多种任务中表现出色，未来研究可通过持续更新的资源库进一步推进。

Abstract: 3D Gaussian Splatting (3DGS) has recently emerged as a powerful alternative
to Neural Radiance Fields (NeRF) for 3D scene representation, offering
high-fidelity photorealistic rendering with real-time performance. Beyond novel
view synthesis, the explicit and compact nature of 3DGS enables a wide range of
downstream applications that require geometric and semantic understanding. This
survey provides a comprehensive overview of recent progress in 3DGS
applications. It first introduces 2D foundation models that support semantic
understanding and control in 3DGS applications, followed by a review of
NeRF-based methods that inform their 3DGS counterparts. We then categorize 3DGS
applications into segmentation, editing, generation, and other functional
tasks. For each, we summarize representative methods, supervision strategies,
and learning paradigms, highlighting shared design principles and emerging
trends. Commonly used datasets and evaluation protocols are also summarized,
along with comparative analyses of recent methods across public benchmarks. To
support ongoing research and development, a continually updated repository of
papers, code, and resources is maintained at
https://github.com/heshuting555/Awesome-3DGS-Applications.

</details>


### [120] [LLMC+: Benchmarking Vision-Language Model Compression with a Plug-and-play Toolkit](https://arxiv.org/abs/2508.09981)
*Chengtao Lv,Bilang Zhang,Yang Yong,Ruihao Gong,Yushi Huang,Shiqiao Gu,Jiajun Wu,Yumeng Shi,Jinyang Guo,Wenya Wang*

Main category: cs.CV

TL;DR: LLMC+是一个全面的VLM压缩基准测试工具包，支持20多种算法，揭示了空间和时间冗余需要不同策略，以及多轮对话中令牌减少方法的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有VLM压缩方法存在三个主要问题：技术模块不可比、评估局限于单轮任务、未探索联合压缩潜力。

Method: 提出LLMC+，一个支持多种算法和VLM家族的压缩基准测试工具包，系统研究令牌级和模型级压缩。

Result: 发现空间和时间冗余需不同策略，令牌减少方法在多轮对话中性能下降，结合令牌和模型压缩可实现极端压缩。

Conclusion: LLMC+有助于公平评估和激发高效VLM的未来研究。

Abstract: Large Vision-Language Models (VLMs) exhibit impressive multi-modal
capabilities but suffer from prohibitive computational and memory demands, due
to their long visual token sequences and massive parameter sizes. To address
these issues, recent works have proposed training-free compression methods.
However, existing efforts often suffer from three major limitations: (1)
Current approaches do not decompose techniques into comparable modules,
hindering fair evaluation across spatial and temporal redundancy. (2)
Evaluation confined to simple single-turn tasks, failing to reflect performance
in realistic scenarios. (3) Isolated use of individual compression techniques,
without exploring their joint potential. To overcome these gaps, we introduce
LLMC+, a comprehensive VLM compression benchmark with a versatile,
plug-and-play toolkit. LLMC+ supports over 20 algorithms across five
representative VLM families and enables systematic study of token-level and
model-level compression. Our benchmark reveals that: (1) Spatial and temporal
redundancies demand distinct technical strategies. (2) Token reduction methods
degrade significantly in multi-turn dialogue and detail-sensitive tasks. (3)
Combining token and model compression achieves extreme compression with minimal
performance loss. We believe LLMC+ will facilitate fair evaluation and inspire
future research in efficient VLM. Our code is available at
https://github.com/ModelTC/LightCompress.

</details>


### [121] [Story2Board: A Training-Free Approach for Expressive Storyboard Generation](https://arxiv.org/abs/2508.09983)
*David Dinkevich,Matan Levy,Omri Avrahami,Dvir Samuel,Dani Lischinski*

Main category: cs.CV

TL;DR: Story2Board是一个无需训练的框架，用于从自然语言生成富有表现力的故事板，通过轻量级一致性框架提升视觉连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有方法过于关注主体身份，忽略了视觉叙事的关键方面（如空间构图、背景演变和叙事节奏）。

Method: 引入Latent Panel Anchoring和Reciprocal Attention Value Mixing两个组件，结合预训练扩散模型生成故事板。

Result: 在Rich Storyboard Benchmark上表现优异，生成的故事板更具动态性、连贯性和叙事吸引力。

Conclusion: Story2Board在无需架构调整或微调的情况下，显著提升了故事板生成的视觉多样性和一致性。

Abstract: We present Story2Board, a training-free framework for expressive storyboard
generation from natural language. Existing methods narrowly focus on subject
identity, overlooking key aspects of visual storytelling such as spatial
composition, background evolution, and narrative pacing. To address this, we
introduce a lightweight consistency framework composed of two components:
Latent Panel Anchoring, which preserves a shared character reference across
panels, and Reciprocal Attention Value Mixing, which softly blends visual
features between token pairs with strong reciprocal attention. Together, these
mechanisms enhance coherence without architectural changes or fine-tuning,
enabling state-of-the-art diffusion models to generate visually diverse yet
consistent storyboards. To structure generation, we use an off-the-shelf
language model to convert free-form stories into grounded panel-level prompts.
To evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain
narratives designed to assess layout diversity and background-grounded
storytelling, in addition to consistency. We also introduce a new Scene
Diversity metric that quantifies spatial and pose variation across storyboards.
Our qualitative and quantitative results, as well as a user study, show that
Story2Board produces more dynamic, coherent, and narratively engaging
storyboards than existing baselines.

</details>


### [122] [Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation](https://arxiv.org/abs/2508.09987)
*Junyan Ye,Dongzhi Jiang,Zihao Wang,Leqi Zhu,Zhenghao Hu,Zilong Huang,Jun He,Zhiyuan Yan,Jinghua Yu,Hongsheng Li,Conghui He,Weijia Li*

Main category: cs.CV

TL;DR: 论文探讨了GPT-4o生成的合成图像数据在补充真实数据集不足和提供更清晰监督信号方面的优势，并提出了Echo-4o-Image数据集和两个新评估基准。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决真实图像数据集在罕见场景和文本-图像对齐方面的局限性，探索合成数据的潜在价值。

Method: 通过GPT-4o生成180K规模的合成数据集Echo-4o-Image，并基于此微调多模态生成模型Bagel。同时提出GenEval++和Imagine-Bench两个新评估基准。

Result: Echo-4o在标准基准测试中表现优异，且Echo-4o-Image数据集在其他基础模型上也带来一致性能提升。

Conclusion: 合成图像数据能有效补充真实数据集的不足，提升模型性能，具有广泛的应用潜力。

Abstract: Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [123] [Real-time deep learning phase imaging flow cytometer reveals blood cell aggregate biomarkers for haematology diagnostics](https://arxiv.org/abs/2508.09215)
*Kerem Delikoyun,Qianyu Chen,Liu Wei,Si Ko Myo,Johannes Krell,Martin Schlegel,Win Sen Kuan,John Tshon Yit Soong,Gerhard Schneider,Clarissa Prazeres da Costa,Percy A. Knolle,Laurent Renia,Matthew Edward Cove,Hwee Kuan Lee,Klaus Diepold,Oliver Hayden*

Main category: q-bio.QM

TL;DR: RT-HAD是一个基于深度学习的端到端图像和数据处理框架，用于离轴数字全息显微镜（DHM），能够实时处理大量图像数据，显著提高血细胞聚集体的检测效率。


<details>
  <summary>Details</summary>
Motivation: 传统流式细胞仪无法识别血细胞聚集体，需要人工复查，而定量相位成像流式细胞术虽然能捕捉聚集体形态，但数据存储和处理问题限制了其临床应用。

Method: RT-HAD结合物理一致的全息重建和检测，将每个血细胞表示为图以识别聚集体，实时处理超过30 GB的图像数据。

Result: RT-HAD在血小板聚集体检测中的错误率为8.9%，与可接受的实验室错误率相当，且处理时间少于1.5分钟。

Conclusion: RT-HAD解决了即时诊断中的大数据挑战，显著提升了血细胞聚集体的检测效率和准确性。

Abstract: While analysing rare blood cell aggregates remains challenging in automated
haematology, they could markedly advance label-free functional diagnostics.
Conventional flow cytometers efficiently perform cell counting with leukocyte
differentials but fail to identify aggregates with flagged results, requiring
manual reviews. Quantitative phase imaging flow cytometry captures detailed
aggregate morphologies, but clinical use is hampered by massive data storage
and offline processing. Incorporating hidden biomarkers into routine
haematology panels would significantly improve diagnostics without flagged
results. We present RT-HAD, an end-to-end deep learning-based image and data
processing framework for off-axis digital holographic microscopy (DHM), which
combines physics-consistent holographic reconstruction and detection,
representing each blood cell in a graph to recognize aggregates. RT-HAD
processes >30 GB of image data on-the-fly with turnaround time of <1.5 min and
error rate of 8.9% in platelet aggregate detection, which matches acceptable
laboratory error rates of haematology biomarkers and solves the big data
challenge for point-of-care diagnostics.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [124] [Generative Artificial Intelligence in Medical Imaging: Foundations, Progress, and Clinical Translation](https://arxiv.org/abs/2508.09177)
*Xuanru Zhou,Cheng Li,Shuqiang Wang,Ye Li,Tao Tan,Hairong Zheng,Shanshan Wang*

Main category: eess.IV

TL;DR: 综述探讨了生成式AI在医学影像中的广泛应用，包括数据合成、图像增强等，并提出了评估框架和未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 解决医学影像中的数据稀缺、标准化和多模态整合等长期挑战。

Method: 系统回顾了GANs、VAEs、扩散模型等生成模型在医学影像工作流中的应用。

Result: 提出了一个三层次评估框架，并探讨了生成式AI与基础模型的结合潜力。

Conclusion: 生成式AI有望推动医学影像的下一代发展，但需解决泛化性、隐私等现实障碍。

Abstract: Generative artificial intelligence (AI) is rapidly transforming medical
imaging by enabling capabilities such as data synthesis, image enhancement,
modality translation, and spatiotemporal modeling. This review presents a
comprehensive and forward-looking synthesis of recent advances in generative
modeling including generative adversarial networks (GANs), variational
autoencoders (VAEs), diffusion models, and emerging multimodal foundation
architectures and evaluates their expanding roles across the clinical imaging
continuum. We systematically examine how generative AI contributes to key
stages of the imaging workflow, from acquisition and reconstruction to
cross-modality synthesis, diagnostic support, and treatment planning. Emphasis
is placed on both retrospective and prospective clinical scenarios, where
generative models help address longstanding challenges such as data scarcity,
standardization, and integration across modalities. To promote rigorous
benchmarking and translational readiness, we propose a three-tiered evaluation
framework encompassing pixel-level fidelity, feature-level realism, and
task-level clinical relevance. We also identify critical obstacles to
real-world deployment, including generalization under domain shift,
hallucination risk, data privacy concerns, and regulatory hurdles. Finally, we
explore the convergence of generative AI with large-scale foundation models,
highlighting how this synergy may enable the next generation of scalable,
reliable, and clinically integrated imaging systems. By charting technical
progress and translational pathways, this review aims to guide future research
and foster interdisciplinary collaboration at the intersection of AI, medicine,
and biomedical engineering.

</details>


### [125] [HiFi-Mamba: Dual-Stream W-Laplacian Enhanced Mamba for High-Fidelity MRI Reconstruction](https://arxiv.org/abs/2508.09179)
*Hongli Chen,Pengcheng Fang,Yuxia Chen,Yingxuan Ren,Jing Hao,Fangfang Tang,Xiaohao Cai,Shanshan Shan,Feng Liu*

Main category: eess.IV

TL;DR: HiFi-Mamba是一种新型双流Mamba架构，用于高保真MRI重建，通过频谱解耦和自适应状态空间调制提升重建精度。


<details>
  <summary>Details</summary>
Motivation: 现有Mamba变体在MRI重建中存在对高频细节不敏感和多方向扫描冗余的问题。

Method: HiFi-Mamba采用双流结构（WL块和HiFi-Mamba块），通过频谱解耦和单向遍历策略优化计算效率。

Result: 实验表明，HiFi-Mamba在重建精度上优于现有CNN、Transformer和其他Mamba模型。

Conclusion: HiFi-Mamba在保持高效设计的同时，显著提升了MRI重建的准确性和频谱细节保留能力。

Abstract: Reconstructing high-fidelity MR images from undersampled k-space data remains
a challenging problem in MRI. While Mamba variants for vision tasks offer
promising long-range modeling capabilities with linear-time complexity, their
direct application to MRI reconstruction inherits two key limitations: (1)
insensitivity to high-frequency anatomical details; and (2) reliance on
redundant multi-directional scanning. To address these limitations, we
introduce High-Fidelity Mamba (HiFi-Mamba), a novel dual-stream Mamba-based
architecture comprising stacked W-Laplacian (WL) and HiFi-Mamba blocks.
Specifically, the WL block performs fidelity-preserving spectral decoupling,
producing complementary low- and high-frequency streams. This separation
enables the HiFi-Mamba block to focus on low-frequency structures, enhancing
global feature modeling. Concurrently, the HiFi-Mamba block selectively
integrates high-frequency features through adaptive state-space modulation,
preserving comprehensive spectral details. To eliminate the scanning
redundancy, the HiFi-Mamba block adopts a streamlined unidirectional traversal
strategy that preserves long-range modeling capability with improved
computational efficiency. Extensive experiments on standard MRI reconstruction
benchmarks demonstrate that HiFi-Mamba consistently outperforms
state-of-the-art CNN-based, Transformer-based, and other Mamba-based models in
reconstruction accuracy while maintaining a compact and efficient model design.

</details>


### [126] [MedPatch: Confidence-Guided Multi-Stage Fusion for Multimodal Clinical Data](https://arxiv.org/abs/2508.09182)
*Baraa Al Jorf,Farah Shamout*

Main category: eess.IV

TL;DR: MedPatch是一种多阶段多模态融合架构，通过置信度引导的补丁技术整合多种医疗数据模态，显著提升临床预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 医疗数据具有异构性、规模小和稀疏性（缺失模态）的特点，限制了模型在临床预测任务中的表现。

Method: MedPatch包含多阶段融合策略、缺失感知模块和联合融合模块，通过置信度引导的补丁技术整合多模态数据。

Result: 在MIMIC数据集上的实验表明，MedPatch在院内死亡率预测和临床状况分类任务中达到最先进性能。

Conclusion: MedPatch通过多阶段融合有效解决了多模态数据的异构性问题，为临床预测任务设定了新的基准。

Abstract: Clinical decision-making relies on the integration of information across
various data modalities, such as clinical time-series, medical images and
textual reports. Compared to other domains, real-world medical data is
heterogeneous in nature, limited in size, and sparse due to missing modalities.
This significantly limits model performance in clinical prediction tasks.
Inspired by clinical workflows, we introduce MedPatch, a multi-stage multimodal
fusion architecture, which seamlessly integrates multiple modalities via
confidence-guided patching. MedPatch comprises three main components: (i) a
multi-stage fusion strategy that leverages joint and late fusion
simultaneously, (ii) a missingness-aware module that handles sparse samples
with missing modalities, (iii) a joint fusion module that clusters latent token
patches based on calibrated unimodal token-level confidence. We evaluated
MedPatch using real-world data consisting of clinical time-series data, chest
X-ray images, radiology reports, and discharge notes extracted from the
MIMIC-IV, MIMIC-CXR, and MIMIC-Notes datasets on two benchmark tasks, namely
in-hospital mortality prediction and clinical condition classification.
Compared to existing baselines, MedPatch achieves state-of-the-art performance.
Our work highlights the effectiveness of confidence-guided multi-stage fusion
in addressing the heterogeneity of multimodal data, and establishes new
state-of-the-art benchmark results for clinical prediction tasks.

</details>


### [127] [Hybrid(Transformer+CNN)-based Polyp Segmentation](https://arxiv.org/abs/2508.09189)
*Madan Baduwal*

Main category: eess.IV

TL;DR: 提出了一种结合Transformer和CNN的混合模型，用于提高结肠息肉分割的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 结肠息肉分割面临形状、光照、成像协议等多变性和边界模糊的挑战，现有方法难以应对。

Method: 采用Transformer和CNN结合的混合架构，引入边界感知注意力机制和鲁棒特征提取。

Result: 在分割准确性和抗干扰能力上显著优于现有方法（召回率提升1.76%，准确率提升0.07%）。

Conclusion: 混合模型在复杂环境下表现出色，为结肠息肉分割提供了更可靠的解决方案。

Abstract: Colonoscopy is still the main method of detection and segmentation of colonic
polyps, and recent advancements in deep learning networks such as U-Net,
ResUNet, Swin-UNet, and PraNet have made outstanding performance in polyp
segmentation. Yet, the problem is extremely challenging due to high variation
in size, shape, endoscopy types, lighting, imaging protocols, and ill-defined
boundaries (fluid, folds) of the polyps, rendering accurate segmentation a
challenging and problematic task. To address these critical challenges in polyp
segmentation, we introduce a hybrid (Transformer + CNN) model that is crafted
to enhance robustness against evolving polyp characteristics. Our hybrid
architecture demonstrates superior performance over existing solutions,
particularly in addressing two critical challenges: (1) accurate segmentation
of polyps with ill-defined margins through boundary-aware attention mechanisms,
and (2) robust feature extraction in the presence of common endoscopic
artifacts, including specular highlights, motion blur, and fluid occlusions.
Quantitative evaluations reveal significant improvements in segmentation
accuracy (Recall improved by 1.76%, i.e., 0.9555, accuracy improved by 0.07%,
i.e., 0.9849) and artifact resilience compared to state-of-the-art polyp
segmentation methods.

</details>


### [128] [impuTMAE: Multi-modal Transformer with Masked Pre-training for Missing Modalities Imputation in Cancer Survival Prediction](https://arxiv.org/abs/2508.09195)
*Maria Boyko,Aleksandra Beliaeva,Dmitriy Kornilov,Alexander Bernstein,Maxim Sharaev*

Main category: eess.IV

TL;DR: impuTMAE是一种基于Transformer的多模态预训练方法，通过重建掩码补丁同时学习模态间和模态内交互，并在缺失模态数据的情况下进行预测，显著提升了胶质瘤患者生存预测的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态数据（如组学、医学影像和临床数据）可以提高预后模型的性能并加深对疾病机制的理解，但医学数据复杂且常缺失模态，因此需要有效处理缺失数据的方法。

Method: 提出impuTMAE，一种基于Transformer的端到端方法，通过预训练策略学习模态交互并重建缺失模态数据，预训练后用于胶质瘤生存预测。

Result: 在TCGA-GBM/LGG和BraTS数据集上，impuTMAE在多模态胶质瘤生存预测中达到最优性能。

Conclusion: impuTMAE通过处理缺失数据和高效资源利用，在多模态医学数据分析中表现出色，为疾病预测提供了新方法。

Abstract: The use of diverse modalities, such as omics, medical images, and clinical
data can not only improve the performance of prognostic models but also deepen
an understanding of disease mechanisms and facilitate the development of novel
treatment approaches. However, medical data are complex, often incomplete, and
contains missing modalities, making effective handling its crucial for training
multimodal models. We introduce impuTMAE, a novel transformer-based end-to-end
approach with an efficient multimodal pre-training strategy. It learns inter-
and intra-modal interactions while simultaneously imputing missing modalities
by reconstructing masked patches. Our model is pre-trained on heterogeneous,
incomplete data and fine-tuned for glioma survival prediction using
TCGA-GBM/LGG and BraTS datasets, integrating five modalities: genetic (DNAm,
RNA-seq), imaging (MRI, WSI), and clinical data. By addressing missing data
during pre-training and enabling efficient resource utilization, impuTMAE
surpasses prior multimodal approaches, achieving state-of-the-art performance
in glioma patient survival prediction. Our code is available at
https://github.com/maryjis/mtcp

</details>


### [129] [FIVA: Federated Inverse Variance Averaging for Universal CT Segmentation with Uncertainty Estimation](https://arxiv.org/abs/2508.09196)
*Asim Ukaye,Numan Saeed,Karthik Nandakumar*

Main category: eess.IV

TL;DR: 提出了一种新颖的联邦学习方法，通过利用模型不确定性进行聚合和预测不确定性进行推理，实现跨多样腹部CT数据集的通用分割。


<details>
  <summary>Details</summary>
Motivation: 解决不同CT分割数据集因扫描器和捕获设置不同导致的异质性问题，同时保护患者隐私。

Method: 利用随机小批量梯度下降中的噪声估计模型权重分布，采用贝叶斯逆方差聚合方案进行参数聚合，并通过传播模型权重不确定性量化预测不确定性。

Result: 实验证明该方法在联邦聚合质量和不确定性加权推理方面优于现有基线。

Conclusion: 该方法有效提升了跨数据集分割的性能，并为临床决策提供了置信度支持。

Abstract: Different CT segmentation datasets are typically obtained from different
scanners under different capture settings and often provide segmentation labels
for a limited and often disjoint set of organs. Using these heterogeneous data
effectively while preserving patient privacy can be challenging. This work
presents a novel federated learning approach to achieve universal segmentation
across diverse abdominal CT datasets by utilizing model uncertainty for
aggregation and predictive uncertainty for inference. Our approach leverages
the inherent noise in stochastic mini-batch gradient descent to estimate a
distribution over the model weights to provide an on-the-go uncertainty over
the model parameters at the client level. The parameters are then aggregated at
the server using the additional uncertainty information using a
Bayesian-inspired inverse-variance aggregation scheme. Furthermore, the
proposed method quantifies prediction uncertainty by propagating the
uncertainty from the model weights, providing confidence measures essential for
clinical decision-making. In line with recent work shown, predictive
uncertainty is utilized in the inference stage to improve predictive
performance. Experimental evaluations demonstrate the effectiveness of this
approach in improving both the quality of federated aggregation and
uncertainty-weighted inference compared to previously established baselines.
The code for this work is made available at: https://github.com/asimukaye/fiva

</details>


### [130] [Zero-shot self-supervised learning of single breath-hold magnetic resonance cholangiopancreatography (MRCP) reconstruction](https://arxiv.org/abs/2508.09200)
*Jinho Kim,Marcel Dominik Nickel,Florian Knoll*

Main category: eess.IV

TL;DR: 零样本自监督学习重建技术可显著减少磁共振胰胆管造影（MRCP）的屏气时间，同时保持高图像质量。


<details>
  <summary>Details</summary>
Motivation: 研究目的是探索零样本自监督学习重建技术在减少MRCP屏气时间中的可行性，以优化临床工作流程。

Method: 通过零样本重建技术与并行成像和压缩感知重建对比，并采用预训练网络减少训练时间。

Result: 零样本重建显著提升了图像质量，接近呼吸触发采集的效果，且浅层训练大幅缩短了计算时间。

Conclusion: 零样本学习为MRCP提供了高质量的快速重建方案，浅层训练使其更适用于临床。

Abstract: Purpose: To investigate the feasibility of applying zero-shot self-supervised
learning reconstruction to reduce breath-hold times in magnetic resonance
cholangiopancreatography (MRCP). Methods: Breath-hold MRCP was acquired from 11
healthy volunteers on a 3T scanner using an incoherent k-space sampling pattern
leading to a breath-hold duration of 14s. We evaluated zero-shot reconstruction
of breath-hold MRCP against parallel imaging of respiratory-triggered MRCP
acquired in 338s on average and compressed sensing reconstruction of
breath-hold MRCP. To address the long computation times of zero-shot trainings,
we used a training approach that leverages a pretrained network to reduce
backpropagation depth during training. Results: Zero-shot learning
reconstruction significantly improved visual image quality compared to
compressed sensing reconstruction, particularly in terms of signal-to-noise
ratio and ductal delineation, and reached a level of quality comparable to that
of successful respiratory-triggered acquisitions with regular breathing
patterns. Shallow training provided nearly equivalent reconstruction
performance with a training time of 11 minutes in comparison to 271 minutes for
a conventional zero-shot training. Conclusion: Zero-shot learning delivers
high-fidelity MRCP reconstructions with reduced breath-hold times, and shallow
training offers a practical solution for translation to time-constrained
clinical workflows.

</details>


### [131] [From Explainable to Explained AI: Ideas for Falsifying and Quantifying Explanations](https://arxiv.org/abs/2508.09205)
*Yoni Schirris,Eric Marcus,Jonas Teuwen,Hugo Horlings,Efstratios Gavves*

Main category: eess.IV

TL;DR: 提出了一种人机-VLM交互系统，用于解释计算病理学中的分类器，结合滑动窗口实验和视觉语言模型量化解释的预测性。


<details>
  <summary>Details</summary>
Motivation: 解释深度学习模型对医学图像分析系统的临床整合至关重要，以避免模型依赖虚假特征或发现新生物学见解。

Method: 提出AI集成的幻灯片查看器进行滑动窗口实验，并使用通用视觉语言模型量化解释的预测性。

Result: 系统能定性测试解释的合理性，并量化区分竞争性解释。

Conclusion: 为从可解释AI到解释AI提供了实用路径，适用于数字病理学等领域。

Abstract: Explaining deep learning models is essential for clinical integration of
medical image analysis systems. A good explanation highlights if a model
depends on spurious features that undermines generalization and harms a subset
of patients or, conversely, may present novel biological insights. Although
techniques like GradCAM can identify influential features, they are measurement
tools that do not themselves form an explanation. We propose a
human-machine-VLM interaction system tailored to explaining classifiers in
computational pathology, including multi-instance learning for whole-slide
images. Our proof of concept comprises (1) an AI-integrated slide viewer to run
sliding-window experiments to test claims of an explanation, and (2)
quantification of an explanation's predictiveness using general-purpose
vision-language models. The results demonstrate that this allows us to
qualitatively test claims of explanations and can quantifiably distinguish
competing explanations. This offers a practical path from explainable AI to
explained AI in digital pathology and beyond. Code and prompts are available at
https://github.com/nki-ai/x2x.

</details>


### [132] [AMRG: Extend Vision Language Models for Automatic Mammography Report Generation](https://arxiv.org/abs/2508.09225)
*Nak-Jun Sung,Donghyun Lee,Bo Hwa Choi,Chae Jung Park*

Main category: eess.IV

TL;DR: AMRG是首个基于大型视觉语言模型（VLM）的端到端框架，用于生成乳腺X光检查报告，通过参数高效微调（PEFT）策略实现高性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺X光检查报告生成在医学AI中是一个重要但未被充分探索的任务，面临多视图图像推理、高分辨率视觉线索和非结构化放射学语言等挑战。

Method: 基于MedGemma-4B-it模型，采用低秩适应（LoRA）进行参数高效微调，训练和评估在公开数据集DMID上进行。

Result: AMRG在语言生成和临床指标上表现优异，ROUGE-L为0.5691，METEOR为0.6152，CIDEr为0.5818，BI-RADS准确率为0.5582。

Conclusion: AMRG为放射学报告生成提供了可扩展和适应性强的基础，并为未来多模态医学AI研究铺平了道路。

Abstract: Mammography report generation is a critical yet underexplored task in medical
AI, characterized by challenges such as multiview image reasoning,
high-resolution visual cues, and unstructured radiologic language. In this
work, we introduce AMRG (Automatic Mammography Report Generation), the first
end-to-end framework for generating narrative mammography reports using large
vision-language models (VLMs). Building upon MedGemma-4B-it-a
domain-specialized, instruction-tuned VLM-we employ a parameter-efficient
fine-tuning (PEFT) strategy via Low-Rank Adaptation (LoRA), enabling
lightweight adaptation with minimal computational overhead. We train and
evaluate AMRG on DMID, a publicly available dataset of paired high-resolution
mammograms and diagnostic reports. This work establishes the first reproducible
benchmark for mammography report generation, addressing a longstanding gap in
multimodal clinical AI. We systematically explore LoRA hyperparameter
configurations and conduct comparative experiments across multiple VLM
backbones, including both domain-specific and general-purpose models under a
unified tuning protocol. Our framework demonstrates strong performance across
both language generation and clinical metrics, achieving a ROUGE-L score of
0.5691, METEOR of 0.6152, CIDEr of 0.5818, and BI-RADS accuracy of 0.5582.
Qualitative analysis further highlights improved diagnostic consistency and
reduced hallucinations. AMRG offers a scalable and adaptable foundation for
radiology report generation and paves the way for future research in multimodal
medical AI.

</details>


### [133] [Dynamic Survival Prediction using Longitudinal Images based on Transformer](https://arxiv.org/abs/2508.09328)
*Bingfan Liu,Haolun Shi,Jiguo Cao*

Main category: eess.IV

TL;DR: SurLonFormer是一种基于Transformer的神经网络，整合纵向医学影像和结构化数据用于生存预测，解决了现有方法在利用截尾数据、时序相关性及可解释性方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用截尾数据、时序相关性及可解释性方面存在不足，SurLonFormer旨在解决这些问题。

Method: SurLonFormer包含三个关键组件：Vision Encoder提取空间特征，Sequence Encoder聚合时序信息，Survival Encoder基于Cox比例风险模型。

Result: 在模拟和阿尔茨海默病分析中，SurLonFormer表现出优越的预测性能，并成功识别疾病相关影像生物标志物。

Conclusion: SurLonFormer通过整合纵向影像和结构化数据，显著提升了生存预测的性能和可解释性。

Abstract: Survival analysis utilizing multiple longitudinal medical images plays a
pivotal role in the early detection and prognosis of diseases by providing
insight beyond single-image evaluations. However, current methodologies often
inadequately utilize censored data, overlook correlations among longitudinal
images measured over multiple time points, and lack interpretability. We
introduce SurLonFormer, a novel Transformer-based neural network that
integrates longitudinal medical imaging with structured data for survival
prediction. Our architecture comprises three key components: a Vision Encoder
for extracting spatial features, a Sequence Encoder for aggregating temporal
information, and a Survival Encoder based on the Cox proportional hazards
model. This framework effectively incorporates censored data, addresses
scalability issues, and enhances interpretability through occlusion sensitivity
analysis and dynamic survival prediction. Extensive simulations and a
real-world application in Alzheimer's disease analysis demonstrate that
SurLonFormer achieves superior predictive performance and successfully
identifies disease-related imaging biomarkers.

</details>


### [134] [T-CACE: A Time-Conditioned Autoregressive Contrast Enhancement Multi-Task Framework for Contrast-Free Liver MRI Synthesis, Segmentation, and Diagnosis](https://arxiv.org/abs/2508.09919)
*Xiaojiao Xiao,Jianfeng Zhao,Qinmin Vivian Hu,Guanghui Wang*

Main category: eess.IV

TL;DR: T-CACE框架通过合成多期对比增强MRI，解决了传统MRI的局限性，提升了肝脏病变的诊断效率和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统MRI存在对比剂风险、耗时的手动评估和有限标注数据集的问题，需要一种更安全、高效的方法。

Method: 提出T-CACE框架，结合条件令牌编码（CTE）、动态时间感知注意力掩码（DTAM）和时间分类一致性约束（TCC）。

Result: 在两个独立肝脏MRI数据集上，T-CACE在图像合成、分割和病变分类方面优于现有方法。

Conclusion: T-CACE为肝脏病变评估提供了一种临床相关且高效的替代方案，提升了安全性和诊断可靠性。

Abstract: Magnetic resonance imaging (MRI) is a leading modality for the diagnosis of
liver cancer, significantly improving the classification of the lesion and
patient outcomes. However, traditional MRI faces challenges including risks
from contrast agent (CA) administration, time-consuming manual assessment, and
limited annotated datasets. To address these limitations, we propose a
Time-Conditioned Autoregressive Contrast Enhancement (T-CACE) framework for
synthesizing multi-phase contrast-enhanced MRI (CEMRI) directly from
non-contrast MRI (NCMRI). T-CACE introduces three core innovations: a
conditional token encoding (CTE) mechanism that unifies anatomical priors and
temporal phase information into latent representations; and a dynamic
time-aware attention mask (DTAM) that adaptively modulates inter-phase
information flow using a Gaussian-decayed attention mechanism, ensuring smooth
and physiologically plausible transitions across phases. Furthermore, a
constraint for temporal classification consistency (TCC) aligns the lesion
classification output with the evolution of the physiological signal, further
enhancing diagnostic reliability. Extensive experiments on two independent
liver MRI datasets demonstrate that T-CACE outperforms state-of-the-art methods
in image synthesis, segmentation, and lesion classification. This framework
offers a clinically relevant and efficient alternative to traditional
contrast-enhanced imaging, improving safety, diagnostic efficiency, and
reliability for the assessment of liver lesion. The implementation of T-CACE is
publicly available at: https://github.com/xiaojiao929/T-CACE.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [135] [Learning to Detect Unknown Jailbreak Attacks in Large Vision-Language Models: A Unified and Accurate Approach](https://arxiv.org/abs/2508.09201)
*Shuang Liang,Zhihao Xu,Jialing Tao,Hui Xue,Xiting Wang*

Main category: cs.CR

TL;DR: LoD是一种无监督框架，通过异常检测识别大型视觉语言模型的越狱攻击，表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 尽管现有方法尝试检测越狱攻击，但依赖启发式规则导致性能不佳，需要更优的无监督解决方案。

Method: 提出LoD框架，结合多模态安全概念激活向量（MSCAV）和安全模式自动编码器，通过重建误差检测异常。

Result: 在三个LVLM和五个基准测试中，LoD平均AUROC达0.9951，性能提升显著。

Conclusion: LoD通过无监督异常检测有效识别越狱攻击，为模型安全提供了新思路。

Abstract: Despite extensive alignment efforts, Large Vision-Language Models (LVLMs)
remain vulnerable to jailbreak attacks, posing serious safety risks. Although
recent detection works have shifted to internal representations due to their
rich cross-modal information, most methods rely on heuristic rules rather than
principled objectives, resulting in suboptimal performance. To address these
limitations, we propose Learning to Detect (LoD), a novel unsupervised
framework that formulates jailbreak detection as anomaly detection. LoD
introduces two key components: Multi-modal Safety Concept Activation Vectors
(MSCAV), which capture layer-wise safety-related representations across
modalities, and the Safety Pattern Auto-Encoder, which models the distribution
of MSCAV derived from safe inputs and detects anomalies via reconstruction
errors. By training the auto-encoder (AE) solely on safe samples without attack
labels, LoD naturally identifies jailbreak inputs as distributional anomalies,
enabling accurate and unified detection of jailbreak attacks. Comprehensive
experiments on three different LVLMs and five benchmarks demonstrate that LoD
achieves state-of-the-art performance, with an average AUROC of 0.9951 and an
improvement of up to 38.89% in the minimum AUROC over the strongest baselines.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [136] [Robustness analysis of Deep Sky Objects detection models on HPC](https://arxiv.org/abs/2508.09831)
*Olivier Parisot,Diogo Ramalho Fernandes*

Main category: astro-ph.IM

TL;DR: 论文探讨了利用计算机视觉和深度学习技术自动检测深空天体（如星系、星云和星团）的方法，并比较了不同检测模型（YOLO、RET-DETR）在智能望远镜图像上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着天文观测和业余天文学家的参与增加，需要更准确和鲁棒的自动化处理方法来解决深空天体检测的挑战。

Method: 使用高性能计算（HPC）并行化计算，训练和比较了YOLO和RET-DETR等检测模型。

Result: 通过HPC加速的模型训练和测试，提高了检测的效率和鲁棒性。

Conclusion: 计算机视觉和深度学习的结合为深空天体检测提供了有效的自动化解决方案。

Abstract: Astronomical surveys and the growing involvement of amateur astronomers are
producing more sky images than ever before, and this calls for automated
processing methods that are accurate and robust. Detecting Deep Sky Objects --
such as galaxies, nebulae, and star clusters -- remains challenging because of
their faint signals and complex backgrounds. Advances in Computer Vision and
Deep Learning now make it possible to improve and automate this process. In
this paper, we present the training and comparison of different detection
models (YOLO, RET-DETR) on smart telescope images, using High-Performance
Computing (HPC) to parallelise computations, in particular for robustness
testing.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [137] [Perceptual Reality Transformer: Neural Architectures for Simulating Neurological Perception Conditions](https://arxiv.org/abs/2508.09852)
*Baihan Lin*

Main category: q-bio.NC

TL;DR: 提出了Perceptual Reality Transformer框架，通过六种神经网络架构模拟八种神经感知条件，用于医学教育和辅助技术开发。


<details>
  <summary>Details</summary>
Motivation: 解决神经感知条件导致的体验差异，帮助医疗专业人员和家属理解患者感受。

Method: 使用Vision Transformer架构，学习从自然图像到特定感知状态的映射，并通过ImageNet和CIFAR-10数据集评估性能。

Result: Vision Transformer表现最优，超越传统CNN和生成方法，建立了首个神经感知模拟基准。

Conclusion: 该框架在医学教育和辅助技术中有直接应用，同时推进了对神经网络模拟非典型人类感知的理解。

Abstract: Neurological conditions affecting visual perception create profound
experiential divides between affected individuals and their caregivers,
families, and medical professionals. We present the Perceptual Reality
Transformer, a comprehensive framework employing six distinct neural
architectures to simulate eight neurological perception conditions with
scientifically-grounded visual transformations. Our system learns mappings from
natural images to condition-specific perceptual states, enabling others to
experience approximations of simultanagnosia, prosopagnosia, ADHD attention
deficits, visual agnosia, depression-related changes, anxiety tunnel vision,
and Alzheimer's memory effects. Through systematic evaluation across ImageNet
and CIFAR-10 datasets, we demonstrate that Vision Transformer architectures
achieve optimal performance, outperforming traditional CNN and generative
approaches. Our work establishes the first systematic benchmark for
neurological perception simulation, contributes novel condition-specific
perturbation functions grounded in clinical literature, and provides
quantitative metrics for evaluating simulation fidelity. The framework has
immediate applications in medical education, empathy training, and assistive
technology development, while advancing our fundamental understanding of how
neural networks can model atypical human perception.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [138] [DAgger Diffusion Navigation: DAgger Boosted Diffusion Policy for Vision-Language Navigation](https://arxiv.org/abs/2508.09444)
*Haoxiang Shi,Xiang Deng,Zaijing Li,Gongwei Chen,Yaowei Wang,Liqiang Nie*

Main category: cs.RO

TL;DR: 提出了一种名为DifNav的端到端优化VLN-CE策略，通过扩散策略统一了传统的两阶段导航方法，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有两阶段导航框架中的全局次优化问题和性能瓶颈，通过端到端优化提升导航效率。

Method: 使用条件扩散策略直接建模连续导航空间中的多模态动作分布，结合DAgger进行在线策略训练和专家轨迹增强。

Result: 在基准数据集上的实验表明，该方法显著优于现有的两阶段导航模型。

Conclusion: DifNav通过端到端优化和扩散策略，有效解决了传统导航框架的局限性，提升了导航性能和鲁棒性。

Abstract: Vision-Language Navigation in Continuous Environments (VLN-CE) requires
agents to follow natural language instructions through free-form 3D spaces.
Existing VLN-CE approaches typically use a two-stage waypoint planning
framework, where a high-level waypoint predictor generates the navigable
waypoints, and then a navigation planner suggests the intermediate goals in the
high-level action space. However, this two-stage decomposition framework
suffers from: (1) global sub-optimization due to the proxy objective in each
stage, and (2) a performance bottleneck caused by the strong reliance on the
quality of the first-stage predicted waypoints. To address these limitations,
we propose DAgger Diffusion Navigation (DifNav), an end-to-end optimized VLN-CE
policy that unifies the traditional two stages, i.e. waypoint generation and
planning, into a single diffusion policy. Notably, DifNav employs a conditional
diffusion policy to directly model multi-modal action distributions over future
actions in continuous navigation space, eliminating the need for a waypoint
predictor while enabling the agent to capture multiple possible
instruction-following behaviors. To address the issues of compounding error in
imitation learning and enhance spatial reasoning in long-horizon navigation
tasks, we employ DAgger for online policy training and expert trajectory
augmentation, and use the aggregated data to further fine-tune the policy. This
approach significantly improves the policy's robustness and its ability to
recover from error states. Extensive experiments on benchmark datasets
demonstrate that, even without a waypoint predictor, the proposed method
substantially outperforms previous state-of-the-art two-stage waypoint-based
models in terms of navigation performance. Our code is available at:
https://github.com/Tokishx/DifNav.

</details>


### [139] [Toward Human-Robot Teaming: Learning Handover Behaviors from 3D Scenes](https://arxiv.org/abs/2508.09855)
*Yuekun Wu,Yik Lung Pang,Andrea Cavallaro,Changjae Oh*

Main category: cs.RO

TL;DR: 提出了一种仅通过RGB图像训练人机协作策略的方法，无需真实机器人数据，利用稀疏视图高斯泼溅重建场景生成演示。


<details>
  <summary>Details</summary>
Motivation: 解决真实机器人训练成本高和视觉域差距问题，实现稳定的人机交接任务。

Method: 利用稀疏视图高斯泼溅重建场景生成图像-动作对，模拟相机姿态变化转化为夹爪姿态变化。

Result: 在重建场景和真实人机交接实验中验证了方法的有效性和鲁棒性。

Conclusion: 该方法为人机交接任务提供了新的有效表示，促进了更无缝和鲁棒的人机协作。

Abstract: Human-robot teaming (HRT) systems often rely on large-scale datasets of human
and robot interactions, especially for close-proximity collaboration tasks such
as human-robot handovers. Learning robot manipulation policies from raw,
real-world image data requires a large number of robot-action trials in the
physical environment. Although simulation training offers a cost-effective
alternative, the visual domain gap between simulation and robot workspace
remains a major limitation. We introduce a method for training HRT policies,
focusing on human-to-robot handovers, solely from RGB images without the need
for real-robot training or real-robot data collection. The goal is to enable
the robot to reliably receive objects from a human with stable grasping while
avoiding collisions with the human hand. The proposed policy learner leverages
sparse-view Gaussian Splatting reconstruction of human-to-robot handover scenes
to generate robot demonstrations containing image-action pairs captured with a
camera mounted on the robot gripper. As a result, the simulated camera pose
changes in the reconstructed scene can be directly translated into gripper pose
changes. Experiments in both Gaussian Splatting reconstructed scene and
real-world human-to-robot handover experiments demonstrate that our method
serves as a new and effective representation for the human-to-robot handover
task, contributing to more seamless and robust HRT.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [140] [Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations](https://arxiv.org/abs/2508.09789)
*Marco De Nadai,Andreas Damianou,Mounia Lalmas*

Main category: cs.IR

TL;DR: 论文提出了一种利用多模态大语言模型（MLLM）生成视频自然语言描述的方法，以提升视频推荐系统的语义理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频推荐系统依赖低层次特征或用户定义的元数据，缺乏对视频深层语义（如意图、幽默等）的捕捉，影响个性化推荐效果。

Method: 通过零微调框架，利用MLLM生成视频的丰富自然语言描述，结合文本编码器和标准推荐模型。

Result: 在MicroLens-100K数据集上，该方法在五种代表性模型中均优于传统特征。

Conclusion: MLLM可作为动态知识提取器，帮助构建更符合用户意图的视频推荐系统。

Abstract: Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [141] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)
*Weigao Sun,Jiaxi Hu,Yucheng Zhou,Jusen Du,Disen Lan,Kexin Wang,Tong Zhu,Xiaoye Qu,Yu Zhang,Xiaoyu Mo,Daizong Liu,Yuxuan Liang,Wenliang Chen,Guoqi Li,Yu Cheng*

Main category: cs.CL

TL;DR: 本文综述了针对传统Transformer架构计算量大、难以大规模训练和部署的问题，探讨了多种高效LLM架构的创新方法。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer架构在计算资源消耗和实际部署方面存在显著限制，因此需要研究更高效的LLM架构。

Method: 系统分析了线性与稀疏序列建模、高效全注意力变体、稀疏混合专家、混合模型架构及新兴扩散LLM等技术。

Result: 提出了现代高效LLM架构的蓝图，为未来研究更高效、多功能的AI系统提供参考。

Conclusion: 通过分类总结现有研究，本文为开发高效、资源感知的基础模型提供了方向，并激励未来研究。

Abstract: Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

</details>


### [142] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)
*Lingjie Jiang,Shaohan Huang,Xun Wu,Yixia Li,Dongdong Zhang,Furu Wei*

Main category: cs.CL

TL;DR: VisCodex框架通过融合视觉和编码语言模型，提升多模态大语言模型的代码生成能力，并引入新数据集和基准测试。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在多模态代码生成方面能力有限，需要更强大的视觉与编码结合模型。

Method: 采用任务向量模型合并技术，将编码LLM集成到视觉语言骨干中，同时保留视觉理解和编码能力。

Result: VisCodex在开源MLLMs中表现最佳，接近GPT-4o等专有模型。

Conclusion: 模型合并策略和新数据集有效提升了多模态代码生成能力。

Abstract: Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [MoLAN: A Unified Modality-Aware Noise Dynamic Editing Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2508.09145)
*Xingle Xu,Yongkang Liu,Dexian Cai,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.LG

TL;DR: MoLAN是一个多模态感知的动态噪声编辑框架，通过分块处理和多粒度去噪，提升多模态情感分析的性能。MoLAN+在此基础上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析常因无关或误导性的视觉和听觉信息而表现不佳，现有方法通常以整个模态为单位处理，容易丢失关键信息。

Method: MoLAN将每个模态的特征分块，根据噪声水平和语义相关性动态分配去噪强度，实现细粒度噪声抑制。

Result: 在五个模型和四个数据集上的实验表明，MoLAN框架广泛有效，MoLAN+达到了最先进的性能。

Conclusion: MoLAN是一个统一且灵活的框架，可无缝集成到多模态模型中，显著提升多模态情感分析的效果。

Abstract: Multimodal Sentiment Analysis aims to integrate information from various
modalities, such as audio, visual, and text, to make complementary predictions.
However, it often struggles with irrelevant or misleading visual and auditory
information. Most existing approaches typically treat the entire modality
information (e.g., a whole image, audio segment, or text paragraph) as an
independent unit for feature enhancement or denoising. They often suppress the
redundant and noise information at the risk of losing critical information. To
address this challenge, we propose MoLAN, a unified ModaLity-aware noise
dynAmic editiNg framework. Specifically, MoLAN performs modality-aware blocking
by dividing the features of each modality into multiple blocks. Each block is
then dynamically assigned a distinct denoising strength based on its noise
level and semantic relevance, enabling fine-grained noise suppression while
preserving essential multimodal information. Notably, MoLAN is a unified and
flexible framework that can be seamlessly integrated into a wide range of
multimodal models. Building upon this framework, we further introduce MoLAN+, a
new multimodal sentiment analysis approach. Experiments across five models and
four datasets demonstrate the broad effectiveness of the MoLAN framework.
Extensive evaluations show that MoLAN+ achieves the state-of-the-art
performance. The code is publicly available at
https://github.com/betterfly123/MoLAN-Framework.

</details>


### [144] [Masked Training for Robust Arrhythmia Detection from Digitalized Multiple Layout ECG Images](https://arxiv.org/abs/2508.09165)
*Shanwei Zhang,Deyun Zhang,Yirao Tao,Kexin Wang,Shijia Geng,Jun Li,Qinghao Zhao,Xingpeng Liu,Yuxi Zhou,Shenda Hong*

Main category: cs.LG

TL;DR: PatchECG框架通过自适应可变块缺失表示学习，解决了不同ECG布局导致的信号异步和部分缺失问题，显著提升了心律失常识别的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 不同医院的ECG布局差异导致信号异步和部分缺失，现有模型难以处理，需开发适应性强的解决方案。

Method: 提出PatchECG框架，基于掩码训练策略，自动聚焦关键块并利用导联间协作依赖关系，实现不同布局ECG的关键识别。

Result: 在PTB-XL数据集和模拟数据上，AUROC达0.835；外部验证中，房颤诊断AUROC为0.778，12x1布局下提升至0.893。

Conclusion: PatchECG在多种布局下表现稳定且优于基线方法，显著提升了ECG心律失常诊断的准确性和鲁棒性。

Abstract: Electrocardiogram (ECG) as an important tool for diagnosing cardiovascular
diseases such as arrhythmia. Due to the differences in ECG layouts used by
different hospitals, the digitized signals exhibit asynchronous lead time and
partial blackout loss, which poses a serious challenge to existing models. To
address this challenge, the study introduced PatchECG, a framework for adaptive
variable block count missing representation learning based on a masking
training strategy, which automatically focuses on key patches with
collaborative dependencies between leads, thereby achieving key recognition of
arrhythmia in ECGs with different layouts. Experiments were conducted on the
PTB-XL dataset and 21388 asynchronous ECG images generated using ECG image kit
tool, using the 23 Subclasses as labels. The proposed method demonstrated
strong robustness under different layouts, with average Area Under the Receiver
Operating Characteristic Curve (AUROC) of 0.835 and remained stable (unchanged
with layout changes). In external validation based on 400 real ECG images data
from Chaoyang Hospital, the AUROC for atrial fibrillation diagnosis reached
0.778; On 12 x 1 layout ECGs, AUROC reaches 0.893. This result is superior to
various classic interpolation and baseline methods, and compared to the current
optimal large-scale pre-training model ECGFounder, it has improved by 0.111 and
0.19.

</details>


### [145] [SVGen: Interpretable Vector Graphics Generation with Large Language Models](https://arxiv.org/abs/2508.09168)
*Feiyu Wang,Zhiyuan Zhao,Yuandong Liu,Da Zhang,Junyu Gao,Hao Sun,Xuelong Li*

Main category: cs.LG

TL;DR: SVG-1M数据集和SVGen模型解决了自然语言生成SVG代码的挑战，提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决将创意转化为精确矢量图形的耗时问题。

Method: 通过SVG-1M数据集和SVGen模型，结合课程学习和强化学习优化。

Result: SVGen在效果和效率上优于通用大模型和传统渲染方法。

Conclusion: SVGen为自然语言生成SVG代码提供了高效准确的解决方案。

Abstract: Scalable Vector Graphics (SVG) is widely used in front-end development and
UI/UX design due to its scalability, editability, and rendering efficiency.
However, turning creative ideas into precise vector graphics remains a
time-consuming challenge. To address this, we introduce SVG-1M, a large-scale
dataset of high-quality SVGs paired with natural language descriptions. Through
advanced data augmentation and annotation, we create well-aligned Text to SVG
training pairs, including a subset with Chain of Thought annotations for
enhanced semantic guidance. Based on this dataset, we propose SVGen, an
end-to-end model that generates SVG code from natural language inputs. Our
approach ensures semantic accuracy and structural completeness, supported by
curriculum learning and reinforcement learning optimization. Experiments show
that SVGen outperforms general large models and traditional rendering methods
in both effectiveness and efficiency. Code, model, and dataset are available on
GitHub.

</details>


### [146] [Multimodal RAG Enhanced Visual Description](https://arxiv.org/abs/2508.09170)
*Amit Kumar Jaiswal,Haiming Liu,Ingo Frommholz*

Main category: cs.LG

TL;DR: 提出了一种轻量级、无需训练的方法，利用检索增强生成（RAG）跨模态扩展，通过线性映射高效计算，显著提升了多模态数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大型多模态模型（LMMs）中的模态间隙问题，避免昂贵的微调需求。

Method: 采用检索增强生成（RAG）和线性映射，结合语言模型生成新文本描述，并通过迭代技术优化映射。

Result: 在两个基准多模态数据集上表现显著提升。

Conclusion: 该方法高效且无需训练，有效解决了模态间隙问题。

Abstract: Textual descriptions for multimodal inputs entail recurrent refinement of
queries to produce relevant output images. Despite efforts to address
challenges such as scaling model size and data volume, the cost associated with
pre-training and fine-tuning remains substantial. However, pre-trained large
multimodal models (LMMs) encounter a modality gap, characterised by a
misalignment between textual and visual representations within a common
embedding space. Although fine-tuning can potentially mitigate this gap, it is
typically expensive and impractical due to the requirement for extensive
domain-driven data. To overcome this challenge, we propose a lightweight
training-free approach utilising Retrieval-Augmented Generation (RAG) to extend
across the modality using a linear mapping, which can be computed efficiently.
During inference, this mapping is applied to images embedded by an LMM enabling
retrieval of closest textual descriptions from the training set. These textual
descriptions, in conjunction with an instruction, cater as an input prompt for
the language model to generate new textual descriptions. In addition, we
introduce an iterative technique for distilling the mapping by generating
synthetic descriptions via the language model facilitating optimisation for
standard utilised image description measures. Experimental results on two
benchmark multimodal datasets demonstrate significant improvements.

</details>


### [147] [MoQE: Improve Quantization Model performance via Mixture of Quantization Experts](https://arxiv.org/abs/2508.09204)
*Jinhao Zhang,Yunquan Zhang,Boyang Zhang,Zeyu Liu,Daning Cheng*

Main category: cs.LG

TL;DR: MoQE是一种基于MoE架构的量化推理框架，通过动态路由输入数据到最合适的量化专家模型，缓解单一量化模型的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 量化方法在提高模型效率和降低部署成本方面至关重要，但量化过程会导致精度下降。MoQE旨在通过结合多个量化变体作为专家模型，共同提升量化模型的性能。

Method: MoQE将全精度模型的多个量化变体作为专门的“量化专家”，并基于输入数据的特征动态路由到最合适的专家。设计了轻量级、结构感知的路由器模型，适用于CV和NLP任务。

Result: 在ResNet、LLaMA和Qwen模型家族上的实验表明，MoQE在ImageNet、WikiText、C4和OpenWebText等基准数据集上达到了与SOTA量化模型相当的性能，且未显著增加推理延迟。

Conclusion: MoQE通过动态路由和专家模型的结合，有效缓解了量化模型的性能下降问题，适用于多种任务，且不影响推理效率。

Abstract: Quantization method plays a crucial role in improving model efficiency and
reducing deployment costs, enabling the widespread application of deep learning
models on resource-constrained devices. However, the quantization process
inevitably introduces accuracy degradation. In this paper, we propose Mixture
of Quantization Experts( abbr. MoQE), a quantization inference framework based
on the Mixture-of-Experts (MoE) architecture, aiming to jointly improve the
performance of quantization models. MoQE combines multiple quantization
variants of one full-precision model as specialized "quantization experts" and
dynamically routes input data to the most suitable expert based on its
characteristics. MoQE alleviates the performance degradation commonly seen in
single quantization models through specialization quantization expert models.
We design lightweight, structure-aware router models tailored for both CV and
NLP tasks. Experimental evaluations on ResNet, LLaMA, and Qwen model families
across benchmark datasets including ImageNet, WikiText, C4, and OpenWebText
demonstrate that MoQE achieves performance comparable to SOTA quantization
model, without incurring significant increases in inference latency.

</details>


### [148] [Combating Noisy Labels via Dynamic Connection Masking](https://arxiv.org/abs/2508.09697)
*Xinlei Zhang,Fan Liu,Chuanyi Zhang,Fan Cheng,Yuhui Zheng*

Main category: cs.LG

TL;DR: 提出动态连接掩码（DCM）机制，增强分类器对噪声标签的鲁棒性，适用于MLPs和KANs。


<details>
  <summary>Details</summary>
Motivation: 噪声标签在实际场景中不可避免，现有研究主要关注鲁棒损失函数和样本选择，对模型架构正则化的探索较少。

Method: 通过评估边的信息承载能力，动态掩码不重要连接，减少梯度误差。

Result: 在合成和真实数据集上优于现有方法，并首次验证KANs在噪声标签下的优越性。

Conclusion: DCM机制可无缝集成多种噪声鲁棒方法，提升模型性能，KANs在噪声场景中表现优于MLPs。

Abstract: Noisy labels are inevitable in real-world scenarios. Due to the strong
capacity of deep neural networks to memorize corrupted labels, these noisy
labels can cause significant performance degradation. Existing research on
mitigating the negative effects of noisy labels has mainly focused on robust
loss functions and sample selection, with comparatively limited exploration of
regularization in model architecture. Inspired by the sparsity regularization
used in Kolmogorov-Arnold Networks (KANs), we propose a Dynamic Connection
Masking (DCM) mechanism for both Multi-Layer Perceptron Networks (MLPs) and
KANs to enhance the robustness of classifiers against noisy labels. The
mechanism can adaptively mask less important edges during training by
evaluating their information-carrying capacity. Through theoretical analysis,
we demonstrate its efficiency in reducing gradient error. Our approach can be
seamlessly integrated into various noise-robust training methods to build more
robust deep networks, including robust loss functions, sample selection
strategies, and regularization techniques. Extensive experiments on both
synthetic and real-world benchmarks demonstrate that our method consistently
outperforms state-of-the-art (SOTA) approaches. Furthermore, we are also the
first to investigate KANs as classifiers against noisy labels, revealing their
superior noise robustness over MLPs in real-world noisy scenarios. Our code
will soon be publicly available.

</details>


### [149] [Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models](https://arxiv.org/abs/2508.09968)
*Luca Eyring,Shyamgopal Karthik,Alexey Dosovitskiy,Nataniel Ruiz,Zeynep Akata*

Main category: cs.LG

TL;DR: 提出了一种通过噪声超网络替代奖励引导的测试时噪声优化的方法，以减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 测试时缩放虽然提升了模型性能，但计算时间大幅增加，限制了其实际应用。

Method: 使用噪声超网络调制初始输入噪声，通过可处理的噪声空间目标学习奖励倾斜分布。

Result: 在保持基础模型保真度的同时，显著降低了计算成本，恢复了大部分质量增益。

Conclusion: 该方法有效平衡了性能与计算效率，适用于实际应用。

Abstract: The new paradigm of test-time scaling has yielded remarkable breakthroughs in
Large Language Models (LLMs) (e.g. reasoning models) and in generative vision
models, allowing models to allocate additional computation during inference to
effectively tackle increasingly complex problems. Despite the improvements of
this approach, an important limitation emerges: the substantial increase in
computation time makes the process slow and impractical for many applications.
Given the success of this paradigm and its growing usage, we seek to preserve
its benefits while eschewing the inference overhead. In this work we propose
one solution to the critical problem of integrating test-time scaling knowledge
into a model during post-training. Specifically, we replace reward guided
test-time noise optimization in diffusion models with a Noise Hypernetwork that
modulates initial input noise. We propose a theoretically grounded framework
for learning this reward-tilted distribution for distilled generators, through
a tractable noise-space objective that maintains fidelity to the base model
while optimizing for desired characteristics. We show that our approach
recovers a substantial portion of the quality gains from explicit test-time
optimization at a fraction of the computational cost. Code is available at
https://github.com/ExplainableML/HyperNoise

</details>
