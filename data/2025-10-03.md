<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CR](#cs.CR) [Total: 1]
- [physics.optics](#physics.optics) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [q-bio.QM](#q-bio.QM) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个基于视频一致性模型（VCMs）的零样本高分辨率视频修复求解器，通过绕过自动微分需求，在保证测量一致性和时间平滑性的同时，以少量神经网络评估实现最先进的视频重建质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像潜在扩散模型（LDMs）的零样本图像逆求解器在视频修复中面临挑战，因为逐帧应用图像先验会导致时间不一致的重建结果。需要一种能够同时恢复精细空间细节并捕捉微妙时间依赖性的视频修复方法。

Method: 利用视频一致性模型（VCMs）将视频潜在扩散模型蒸馏为快速生成器，显式捕捉时间因果关系。提出条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量视频重建。

Result: 在多种视频逆问题上的广泛实验表明，相比当前最先进的逐帧应用图像LDMs的方法，LVTINO在重建保真度和计算效率方面都取得了显著感知改进，建立了新的基准。

Conclusion: LVTINO成功解决了高分辨率视频修复中的时间一致性问题，通过VCMs先验实现了既高效又高质量的视频重建，为视频逆问题求解提供了新的解决方案。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出一种基于风格提取的三阶段训练图像生成方法，通过风格编码器和风格投影层实现细粒度文本引导的风格化图像生成


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度风格难以用自然语言精确描述和控制的问题，以及风格参考图像难以与文本条件直接对齐的挑战

Method: 使用风格编码器和风格投影层提取单张风格参考图像的细粒度风格表示，并将其注入预训练生成模型而不改变其结构框架

Result: 构建了Style30k-captions数据集，包含图像、风格标签和文本描述的三元组，用于训练风格编码器和风格投影层

Conclusion: 该方法能够充分利用预训练生成模型的生成能力，实现基于文本提示的细粒度风格控制图像生成

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文提出了一个用于技能学习中挣扎行为检测的数据集EvoStruggle，包含61.68小时视频记录和5,385个标注的挣扎时间段，涵盖四种不同活动。研究表明时间动作定位模型能够有效检测挣扎行为，并具有一定的跨任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 技能学习过程中识别用户何时遇到困难对于优化人类学习和开发辅助系统至关重要。现有数据集未关注挣扎行为随时间演变的过程，需要构建专门的数据集来研究这一现象。

Method: 收集了76名参与者完成18个任务的视频数据，每个任务重复5次以捕捉技能演变。将挣扎检测问题定义为时间动作定位任务，使用时间动作定位模型来识别和定位挣扎时间段。

Result: 模型在跨任务泛化时达到34.56%的平均mAP，跨活动泛化时达到19.24%的mAP，表明挣扎是一个可跨任务迁移的概念，但检测性能仍有提升空间。

Conclusion: 挣扎行为检测是一个可行的研究方向，时间动作定位模型能够学习到挣扎的通用特征，但需要进一步改进检测精度。数据集为研究技能学习过程中的困难识别提供了重要资源。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一个基于轻量级残差U-Net架构的偏微分方程求解基础模型，相比基于Transformer的现有方法具有更高的参数效率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的偏微分方程基础模型主要基于大型复杂Transformer架构，存在计算和参数开销高的问题，需要探索更轻量高效的架构。

Method: 采用轻量级残差U-Net架构，使用自回归预训练策略模拟数值求解器行为，在多样化流体动力学PDE数据集上进行预训练。

Result: 在6个未见过的下游PDE任务上实现了最先进的泛化性能，同时显著减少了参数数量和微调数据需求。

Conclusion: SPUS展示了作为参数高效基础模型解决多样化PDE系统的潜力，残差U-Net架构在该领域具有显著优势。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过多样性约束直接优化多人生成中的身份多样性，解决了文本到图像模型在多人提示下重复面部、合并身份和计数错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像模型在现实主义方面表现出色，但在多人提示下表现不佳，容易出现面部重复、身份合并和个体计数错误的问题。

Method: DisCo使用组相对策略优化（GRPO）对流动匹配模型进行微调，采用组合奖励函数：(i)惩罚图像内面部相似性，(ii)阻止跨样本身份重复，(iii)强制执行准确的人数计数，(iv)通过人类偏好分数保持视觉保真度。采用单阶段课程来稳定训练复杂度扩展，无需额外标注。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的独特面部准确率和接近完美的全局身份分布，超越了开源和专有方法（如Gemini、GPT-Image），同时保持了竞争力的感知质量。

Conclusion: DisCo作为一个可扩展、无需标注的解决方案，解决了生成模型中长期存在的身份危机问题，为组合多人生成设定了新基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 提出了一种新的视觉地理定位方法，通过层次化地理嵌入表示和视觉-语义特征融合，在多个基准数据集上实现了显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视觉地理定位方法在学习地理表示方面仍有改进空间，需要更有效地对齐查询图像的视觉表示与地理表示。

Method: 将世界建模为层次化地理嵌入表示，并引入外观特征与语义分割图的高效融合方法，构建鲁棒的视觉表示。

Result: 在5个基准数据集的25个指标中，22个指标超越了现有SOTA方法和大型视觉语言模型，创造了新的最佳性能。

Conclusion: 性能提升主要归功于地理表示与视觉表示的有效结合，证明了该方法的有效性。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是首个针对大型视觉语言模型（LVLM）的数据高效指令调优方法，通过聚类注意力矩阵轨迹来去除训练数据冗余，能在减少50%-85%数据量的同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在LVLM上无法超越随机选择，且该领域研究不足，需要开发专门针对LVLM的数据高效学习方法。

Method: 基于理论证明相似注意力矩阵对应相似梯度，提出XMAS方法：通过在小代理LVLM上微调获得注意力矩阵的top奇异值轨迹，聚类相似样本，从各簇中平衡采样。

Result: 在LLaVA-665k数据集上减少50%数据，Vision-Flan数据集上减少85%数据，完全保持LLaVA-1.5-7B在10个下游基准上的性能，训练速度提升1.2倍，数据减少量比最佳基线多30%。

Conclusion: XMAS是首个有效的LVLM数据选择方法，通过注意力矩阵分析成功实现了数据高效训练，为大规模LVLM训练提供了实用解决方案。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种变分流匹配方法，用于向量量化图像生成，通过结合连续传输动力学和离散分类监督，提高了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在连续方法和离散方法之间存在权衡：连续方法具有几何感知但缺乏离散监督，离散方法有明确监督但缺乏连续动力学。Purrception旨在结合两者的优势。

Method: 将变分流匹配适应于向量量化潜在空间，通过学习码本索引的分类后验，同时在连续嵌入空间中计算速度场。

Result: 在ImageNet-1k 256x256生成任务上，训练收敛速度比连续和离散流匹配基线更快，同时达到与最先进模型竞争的FID分数。

Conclusion: 变分流匹配能够有效桥接连续传输和离散监督，为图像生成带来改进的训练效率。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出了一种统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描生成合成对比增强CT图像，并同时分割主动脉腔和血栓，解决了传统多阶段方法的误差累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT检查需要碘造影剂，存在肾毒性、患者过敏和环境危害等风险。现有深度学习方法采用多阶段流程，首先生成图像再进行分割，导致误差累积且无法共享语义和解剖结构。

Method: 提出统一框架，整合条件扩散模型与多任务学习，实现图像合成和解剖分割的端到端联合优化。无需初始预测，共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据。

Result: 在264名患者队列中评估，模型在图像合成方面PSNR达到25.61 dB（优于单任务CDM的23.80 dB），分割方面腔体Dice分数提升至0.89（从0.87），血栓Dice分数提升至0.53（从0.48），临床测量误差显著降低。

Conclusion: 该方法在减少造影剂使用的同时，通过联合优化实现了更准确的图像合成和分割性能，为临床AAA评估提供了更安全有效的解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出了一个用于多模态内容分析的高效原型框架，将预训练模型与视频数据结合，生成可查询的知识图谱表示


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析通常计算成本高、工程实现复杂，现有预训练模型与视频等复杂数据的融合具有挑战性

Method: 设计候选流程配方，结合预训练模型将视频转换为时序半结构化数据，再转化为帧级索引知识图谱表示

Result: 创建了可查询的知识图谱表示，支持持续学习，能够通过交互方式动态融入新的领域知识

Conclusion: 该框架为多模态内容分析提供了高效的原型设计解决方案，支持动态知识更新和查询功能

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过逆向工程网站功能创建可重用工具，让Web代理调用高级操作（如搜索、过滤、排序）而非低级UI交互，实现更高效、稳健的浏览器自动化。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理方法依赖逐步UI交互和大量LLM推理，在动态布局和长任务中容易失败，而人类则利用网站提供的高级功能进行操作。

Method: WALT逆向工程网站的潜在功能，将其转化为可调用的工具，抽象掉低级执行细节，让代理直接调用search(query)、create(listing)等高级操作。

Result: 在VisualWebArena和WebArena测试中，WALT实现了更高的成功率、更少的步骤和更少的LLM依赖推理。

Conclusion: WALT建立了一个稳健且可泛化的浏览器自动化范式，将计算负担从脆弱的逐步推理转移到可靠的工具调用。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一个半监督分割框架，通过拓扑一致性机制来识别和保留组织病理学图像中的语义结构，特别适用于密集分布的对象分割。


<details>
  <summary>Details</summary>
Motivation: 解决半监督分割中从无标签数据中捕获有意义的语义结构的挑战，特别是在组织病理学图像分析中，对象密集分布且难以区分生物意义结构与噪声伪影。

Method: 利用随机dropout和时间训练快照获得多个扰动预测，通过整合空间重叠和全局结构对齐的新型匹配策略来强制拓扑一致性，最小化预测间的差异。

Result: 大量实验表明该方法有效减少了拓扑错误，产生了更鲁棒和准确的分割结果，对可靠的下游分析至关重要。

Conclusion: 提出的半监督分割框架通过拓扑一致性机制成功解决了组织病理学图像中密集对象分割的挑战，代码已开源。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，将列表式偏好优化应用于扩散模型，通过Plackett-Luce模型扩展DPO目标，利用用户反馈的排名信息来更精确地对齐人类偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的DPO方法主要依赖成对偏好，但人类对图像的偏好反馈往往包含隐含的排名信息，这些信息比成对比较能更精确地表达人类偏好。列表式偏好的精确优化在扩散模型中尚未得到充分解决。

Method: 提出Diffusion-LPO框架：1）将用户反馈聚合成图像排名列表；2）在Plackett-Luce模型下推导DPO目标的列表式扩展；3）通过鼓励每个样本优于其所有排名较低的替代品来确保整个排名的一致性。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等任务上的实验表明，Diffusion-LPO在视觉质量和偏好对齐方面始终优于成对DPO基线方法。

Conclusion: Diffusion-LPO是一个简单有效的列表式偏好优化框架，能够更好地利用人类反馈中的排名信息来提升扩散模型的性能和对齐效果。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯自回归的统一多模态大语言模型，通过Mixture-of-Transformers架构增强预训练视觉理解模型的生成能力，在单个next-token预测框架内实现图像理解和生成。


<details>
  <summary>Details</summary>
Motivation: 构建既能理解又能生成图像的统一MLLM具有挑战性：混合方法结合连续嵌入和扩散目标产生高质量图像但破坏自回归范式，而纯自回归方法在语义对齐和像素级保真度之间存在权衡。

Method: 提出语义到像素的离散表示，整合紧凑语义标记和细粒度像素标记，仅增加7.9%序列长度即可实现强语言对齐和精确视觉细节描述。

Result: 在多样化多模态基准测试中，Bridge在理解和生成基准上均取得竞争性或更优结果，同时需要更少的训练数据和更短的训练时间。

Conclusion: Bridge证明了纯自回归方法可以在统一框架内有效实现多模态理解和生成，为构建更高效的多模态AI系统提供了新思路。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出一种结合CNN和贝叶斯深度学习的混合模型，用于小数据集下的口腔癌分类，通过不确定性量化提高模型可靠性


<details>
  <summary>Details</summary>
Motivation: 口腔癌是全球高发癌症，早期诊断对降低死亡率至关重要。传统深度学习模型需要大数据集且存在过度自信问题，在医疗资源匮乏地区难以应用

Method: 使用卷积神经网络结合贝叶斯深度学习，采用变分推断进行不确定性量化，使用智能手机拍摄的彩色图像进行训练

Result: 在相似分布测试集上达到94%准确率，与传统CNN相当；在真实世界图像数据上达到88%准确率，显著优于传统CNN的72.94%，且对正确分类样本显示低不确定性，错误分类样本显示高不确定性

Conclusion: 贝叶斯推断在数据稀缺环境下能有效提高口腔癌早期诊断的模型可靠性和泛化能力

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CADTrans的无源域自适应方法，通过构建辅助域和一致性策略来解决目标域中难以获取不变特征的问题，显著提升了域自适应性能。


<details>
  <summary>Details</summary>
Motivation: 现有的无源域自适应方法主要关注评估目标域中与源域相似的不变特征，但这些方法容易受到困难样本和域偏差的影响，无法充分表示多样性。

Method: CADTrans方法包含三个核心部分：1）辅助域模块从中间聚合的全局注意力中获得多样化表示；2）通过多一致性策略获取不变特征表示来区分简单和困难样本；3）构建条件多核最大均值差异策略来对齐困难样本到相应的简单样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准数据集上进行了广泛实验，证明了所提方法取得了显著的性能提升。

Conclusion: CADTrans通过构建域一致性的不变特征表示，有效解决了无源域自适应中的挑战，特别是在处理困难样本和域偏差方面表现出色。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 本文提出了一种利用历史BLV用户问题来指导多模态大语言模型生成更相关描述的系统，相比默认的全面描述方法，能更有效地满足用户需求。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在为盲人和低视力用户提供视觉解释时，往往生成冗长且不相关的描述，导致用户体验不佳。

Method: 开发了一个系统，利用VizWiz-LF数据集中的历史BLV用户问题，通过识别相似视觉上下文来指导MLLM生成更相关的描述。

Result: 评估显示，上下文感知描述在76.1%的情况下能预测并回答用户问题，并在54.4%的比较中被用户偏好。

Conclusion: 利用历史用户问题指导MLLM能显著提高描述的针对性和用户体验。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: ImageNet-Think是一个多模态推理数据集，基于ImageNet21k的25万张图像构建，包含结构化思维标记和对应答案，旨在开发具有显式推理能力的视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型缺乏显式推理能力，需要专门的数据集来训练和评估多模态推理模型，以促进对多模态推理机制的理解。

Method: 使用两个最先进的视觉语言模型（GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506）生成合成数据集，每张图像配备两对思维-答案序列，捕捉逐步推理过程和最终描述性答案。

Result: 创建了一个包含25万张图像的多模态推理数据集，提供结构化思维标记和对应答案，为训练和评估推理型视觉语言模型提供资源。

Conclusion: ImageNet-Think数据集将公开发布，有助于推动多模态推理视觉语言模型的研究发展，促进对推理机制的深入理解。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过在感知矩阵的零空间上施加低维投影约束，而不是传统的图像域结构约束，来解决成像逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法通常忽略感知矩阵零空间的特定结构信息，而零空间包含了感知过程无法捕捉的与信号正交的信息，这些信息对于解决逆问题的模糊性至关重要。

Method: 使用神经网络学习感知矩阵零空间的低维投影，将解空间约束在该投影上，该方法可与现有重建框架兼容，并作为传统图像域先验的补充。

Result: 理论分析证明了在即插即用方法中的收敛性和重建精度保证。实验结果表明NPN先验在多种成像逆问题中都能显著提升重建质量。

Conclusion: NPN方法通过关注感知矩阵零空间的结构，提供了一种可解释且灵活的正则化策略，能够有效提升各种成像逆问题的重建性能。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种自动化基因组解释模块，将原始DNA序列转化为可解释的决策，用于医疗自动化和机器人系统。结合混沌游戏表示和概念瓶颈模型，通过生物学概念进行预测，并加入多种可靠性增强技术。


<details>
  <summary>Details</summary>
Motivation: 旨在建立可靠的基因组医学自动化和机器人系统基础，弥合可解释基因组建模与自动化决策之间的差距。

Method: 结合混沌游戏表示和概念瓶颈模型，通过GC含量、CpG密度、k-mer基序等生物学概念进行预测，加入概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准等可靠性技术。

Result: 在HIV亚型分类任务上达到最先进性能，具有优越的概念预测保真度和更好的成本效益权衡。

Conclusion: 该工作为基因组医学中的机器人和临床自动化建立了可靠基础，能够减少不必要的重测并提高效率。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1是一个增强推理能力的视觉-语言-动作模型，通过强化学习验证奖励和群体相对策略优化来系统优化推理和执行能力，在泛化性和真实世界性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏明确的逐步推理能力，忽视可用性约束和几何关系，后训练流程也缺乏对推理质量的强化，主要依赖监督微调和弱奖励设计。

Method: 集成强化学习验证奖励（RLVR）和群体相对策略优化（GRPO），设计基于RLVR的后训练策略，包含区域对齐、轨迹一致性和输出格式化的可验证奖励，并开发了VLA-CoT-13K高质量数据集。

Result: 在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比现有VLA方法实现了更优的泛化性和真实世界性能。

Conclusion: VLA-R1通过系统化的推理增强方法有效解决了当前VLA模型的局限性，为具身AI提供了更强大的视觉-语言-动作统一框架。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于微距摄影的联合去模糊和3D重建方法，从多视角模糊图像中联合优化物体的清晰3D模型和每个像素的散焦模糊核，通过可微分渲染实现自监督优化。


<details>
  <summary>Details</summary>
Motivation: 微距摄影具有高分辨率和大放大倍率的优势，但散焦模糊问题严重阻碍了物体的清晰成像和高质量3D重建。传统方法需要大量图像和标注，且目前没有专门针对微距摄影的多视角3D重建方法。

Method: 从多视角模糊图像出发，联合优化物体的清晰3D模型和每个像素的散焦模糊核。整个框架采用可微分渲染方法，自监督优化3D模型和散焦模糊核。

Result: 大量实验表明，从少量多视角图像中，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观。

Conclusion: 该方法成功解决了微距摄影中的散焦模糊问题，实现了从模糊图像到清晰3D模型的高质量重建。

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个单步扩散模型，通过重新定义运动去模糊为扩散过程，训练一致性模型实现高效高保真图像去模糊


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在图像去模糊任务中推理时间过长和保真度不足的问题，充分发挥预训练扩散模型在真实世界建模方面的优势

Method: 将运动去模糊重新表述为扩散过程，每个时间步对应逐渐模糊的图像；训练一致性模型将所有时间步对齐到同一清晰图像；集成Kernel ControlNet进行模糊核估计；引入自适应时间步预测

Result: 在全参考指标上取得优异性能，超越之前的扩散方法，与其他最先进模型性能相当

Conclusion: FideDiff为将预训练扩散模型应用于高保真图像恢复任务提供了新方向，为实际工业应用中的扩散模型建立了强大基准

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 本文提出了一种用于青铜器铭文识别的两阶段检测识别流程，并开发了LadderMoE方法来处理多域变异性问题，在大型数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文是早期中国文字的重要阶段，但自动识别面临视觉退化严重、多域变异性（照片、拓片、描摹）和长尾字符分布等挑战。

Method: 构建大规模青铜器铭文数据集（22454张全页图像，198598个标注字符），开发两阶段检测识别流程，使用LadderMoE增强预训练CLIP编码器，实现动态专家专业化和更强鲁棒性。

Result: 在单字符和全页识别任务上的综合实验表明，该方法显著优于最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上都取得了优异的准确率。

Conclusion: 这些结果为青铜器铭文识别和下游考古分析奠定了坚实基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA是一种参数高效的领域自适应方法，通过视觉重编程层生成视觉提示来调整输入图像的纹理风格，而不需要微调整个骨干网络，显著减少了训练参数和存储需求。


<details>
  <summary>Details</summary>
Motivation: 现有的UDA方法为每个新的源-目标对微调整个骨干网络参数，导致训练参数和存储内存随新对线性增长，且无法重用训练好的骨干参数。

Method: VirDA在骨干网络前添加一个领域特定的视觉重编程层，生成视觉提示作为纹理偏置，通过优化多个目标函数来减少域内和域间分布差异，而不修改骨干网络参数。

Result: 在Office-31数据集上，VirDA达到92.8%的平均准确率，仅需1.5M可训练参数，优于现有参数高效的UDA基线PDA（+1.6%准确率，仅用46%参数），且相比全骨干微调方法仅需1.7%-2.8%的参数。

Conclusion: VirDA通过视觉重编程实现了参数高效的领域自适应，在保持竞争力的准确率的同时大幅减少了参数需求，支持骨干网络的跨域重用。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 本文提出了一种名为离散面部编码（DFE）的无监督方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，以替代传统的人工标注FACS系统。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统如FACS存在覆盖范围有限和人工标注成本高的问题，需要一种更精确、可扩展的替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，然后通过RVQ-VAE进行编码，生成来自共享码本的离散标记序列，每个标记捕获特定的可重用面部变形模式。

Result: DFE在压力检测、人格预测和抑郁检测三个心理学任务中，使用简单的词袋模型就优于FACS基线和强图像视频表示学习模型，且覆盖更广泛的面部表情。

Conclusion: DFE作为FACS的可扩展有效替代方案，在心理学和情感计算应用中具有重要潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM是一种用于处理共形变形的非刚性结构恢复方法，通过图优化框架进行点对点重建，无需严格假设即可准确计算局部共形尺度，并在深度估计和共形尺度解耦方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有NRSfM方法依赖严格假设（如局部平面表面或局部线性变形），无法恢复共形尺度，且深度和共形尺度约束耦合，限制了重建精度。

Method: 采用图优化框架进行点对点重建，通过并行可分离迭代优化策略处理问题敏感性，并结合自监督学习的编码器-解码器网络生成带纹理的密集3D点云。

Result: 在合成和真实数据集上的仿真和实验结果表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM方法消除了现有方法的约束，实现了更精确的深度估计和共形尺度计算，为单目视觉可变形SLAM提供了有效的解决方案。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个用于鲁棒3D重建的统一框架，通过将不一致的多视图图像转换为视频，使用视频扩散模型恢复一致性，然后进行3D重建。


<details>
  <summary>Details</summary>
Motivation: 解决从不一致的多视图图像进行3D重建的挑战，现有方法依赖密集观测且优化困难。

Method: 将鲁棒重建分解为恢复和重建两个子任务：首先将不一致图像转换为初始视频，使用视频扩散模型恢复一致性，然后从恢复后的图像重建3D场景。

Result: 在合成和真实数据集上的实验表明，该方法具有强大的泛化能力和优越性能，并能控制重建3D场景的风格。

Conclusion: UniVerse通过利用扩散模型学习的大规模场景先验，能够处理多样化的图像不一致性，简化了优化过程并提高了重建质量。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出轻量级端到端模板匹配框架PoseMatch-TDCM，将模板匹配重构为联合定位和几何回归问题，输出中心坐标、旋转角度和独立缩放比例，在复杂背景下实现高效精确的几何姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要穷举角度和尺度导致效率低下，深度学习方法仅估计相似度分数而无法显式建模几何姿态，难以满足工业应用需求。

Method: 使用Template-Aware Dynamic Convolution Module动态注入模板特征，结合深度可分离卷积和像素洗牌实现高效匹配，采用旋转-剪切增强策略和结构感知伪标签进行无几何标注训练，轻量级优化模块提升角度和尺度精度。

Result: 3.07M参数模型在复合变换下达到高精度和14ms推理速度，在小模板和多目标场景中表现出强鲁棒性。

Conclusion: 该方法非常适合实时工业应用部署，在精度和效率方面优于传统方法和现有深度学习方法。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了首个自适应像素推理框架，通过动态确定必要的像素级操作来解决视觉语言模型在细粒度视觉理解任务中的效率问题


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在处理需要精确理解细粒度视觉元素的任务时存在困难，主要由于图像编码过程中的信息丢失或对关键区域关注不足。虽然引入像素级视觉信息有所帮助，但容易过度使用导致效率低下和无关视觉细节干扰

Method: 提出自适应像素推理框架：首先进行操作感知的监督微调建立文本推理和视觉操作的基础能力，然后设计基于模型自身响应反馈的rollout引导强化学习框架，使VLM能够根据查询难度动态决定何时调用像素操作

Result: 在广泛的多模态推理基准测试中，模型实现了73.4%的HR-Bench 4K准确率，同时仅使用20.1%的工具使用率，相比之前方法在提高准确率的同时将工具使用率降低了66.5%

Conclusion: 该框架有效解决了VLM在细粒度视觉任务中的效率问题，实现了性能提升与计算资源消耗的平衡

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了一种基于增强敏感性的风险评分框架（ASRS），用于识别胸部X光片中容易出错的病例，通过测量图像旋转后的嵌入变化来评估模型稳定性，提高医疗AI的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸部X光片解读中表现优异，但在不同患者亚组中存在准确率不均的问题，现有错误检测方法难以处理分布内的细微错误，需要一种无标签的可靠错误检测方法。

Method: ASRS框架应用临床合理的旋转（±15°/±30°），使用RAD-DINO编码器测量嵌入变化，通过敏感性评分将样本分为稳定性四分位数。

Result: 高度敏感的病例显示出显著较低的召回率（-0.2到-0.3），尽管具有较高的AUROC和置信度，证明ASRS能够有效识别易错病例。

Conclusion: ASRS提供了一种无需标签的选择性预测和临床医生审查方法，能够改善医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的I2V模型，实现高保真度和强时序一致性的视频风格化。


<details>
  <summary>Details</summary>
Motivation: 现有的视频风格化方法存在时序不一致、风格丰富度不足的问题，且训练专用模型需要配对视频数据和大量计算资源。

Method: 整合多个风格化参考到预训练I2V模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索保持低显著性区域的风格纹理。

Result: FreeViS在风格化保真度和时序一致性方面优于现有基线方法，获得强烈的人类偏好。

Conclusion: 该训练免费框架为高质量、时序一致的视频风格化提供了实用且经济的解决方案。

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench是一个用于评估多模态大语言模型在医学图像质量评估中的感知和推理能力的综合基准，包含两个互补任务：MedQ-Perception（低层次感知能力）和MedQ-Reasoning（高层次推理能力），涵盖5种成像模态和40多个质量属性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法受限于基于标量分数的度量，无法反映专家评估中基于描述性、类人推理的过程，因此需要建立新的评估范式。

Method: 提出MedQ-Bench基准，包含2,600个感知查询和708个推理评估，使用多维度评判协议从四个互补维度评估模型输出，并进行严格的人机对齐验证。

Result: 对14个最先进MLLMs的评估显示，模型表现出初步但不稳定的感知和推理技能，准确率不足以可靠临床使用。

Conclusion: 研究强调了针对医学IQA优化MLLMs的必要性，MedQ-Bench有望促进进一步探索并释放MLLMs在医学图像质量评估中的潜力。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一个能够通过单次前向传播预测场景中所有实例的完整遮挡和深度顺序的网络，无需昂贵的输入格式和二次推断成本。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉模型在理解实例级几何关系方面存在挑战，且依赖昂贵的输入格式（如类别标签、分割掩码）和二次推断成本。

Method: InstaFormer通过对象查询和潜在掩码描述符之间的交互来实现整体顺序预测，这些描述符语义上表示相同对象但携带互补信息。

Result: 该方法在基准测试和消融实验中表现出有效性，代码和模型已开源。

Conclusion: InstaFormer通过单次前向传播有效预测场景中所有实例的遮挡和深度顺序，解决了现有方法的局限性。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过金字塔位置编码和强化学习优化，实现了高质量、实时的艺术风格渲染。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时存在计算效率低下的问题，需要更高效的风格迁移方法。

Method: 提出PyramidStyler框架，采用金字塔位置编码(PPE)来捕获多尺度特征，并结合强化学习动态优化风格化过程，加速收敛。

Result: 在COCO和WikiArt数据集上训练4000轮后，内容损失降低62.6%(至2.07)，风格损失降低57.4%(至0.86)，推理时间1.39秒；使用RL后进一步优化(内容2.03，风格0.75)，推理时间1.40秒。

Conclusion: 该方法实现了实时高质量的艺术风格渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一种针对大规模3D高斯泼溅的负载平衡高效框架，解决了现有方法在大型无边界场景中的负载不平衡和低效问题，将预处理时间从小时级缩短到分钟级，训练速度提升2倍。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在处理大规模无边界场景（如城市街区）时存在严重问题：分区方法存在负载不平衡，均匀或启发式分割无法反映实际计算需求；粗到细流水线未能有效利用粗阶段，导致高开销。

Method: 提出深度感知分区方法减少预处理时间；基于优化的策略平衡可见高斯分布（计算负载的强代理）；两种轻量级技术：可见性裁剪和选择性稠密化，进一步降低训练成本。

Result: 在大规模城市和户外数据集上的评估显示，LoBE-GS比最先进基线实现了2倍的端到端训练速度提升，同时保持重建质量，并能够扩展到传统3DGS无法处理的场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS流水线，有效解决了负载平衡和效率问题，为大规模3D场景重建提供了可行的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出了MemoryPack和Direct Forcing两个创新方法来解决长视频生成中的长期依赖建模和错误累积问题


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕捉长期依赖关系，同时防止自回归解码中固有的错误累积

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖关系；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐以减少推理过程中的错误传播

Result: 该方法实现了分钟级的时间一致性，计算复杂度线性增长，保持了计算效率

Conclusion: MemoryPack和Direct Forcing显著提升了长视频生成的上下文一致性和可靠性，推进了自回归视频模型的实用化

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D目标检测器分类任务置信度校准的方法，通过引入两个正则化损失项来改善主导预测和完整预测向量的校准，并发现结合全类别预测校准损失和等渗回归在CenterPoint和PillarNet上效果最佳。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，精确的目标检测和不确定性估计对于自感知和安全操作至关重要。当前需要关注完整预测置信度分布在所有类别上的校准问题。

Method: 提出了两个辅助正则化损失项：一个用于主导预测的校准，另一个用于完整预测向量的校准。评估了多种后处理和训练时方法，包括等渗回归。

Result: 实验表明，结合全类别预测校准损失和等渗回归在CenterPoint和PillarNet上实现了主导和次要类别预测的最佳校准效果。但DSVT-Pillar无法使用相同方法同时校准主导和次要预测。

Conclusion: 该方法有效改善了3D目标检测器的置信度校准，但不同检测器架构需要针对性的校准策略。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个新颖的人员搜索框架，利用预训练的扩散模型来解决现有方法中检测和重识别任务优化冲突的问题，通过三个专门模块实现更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有人员搜索方法主要使用ImageNet预训练主干网络，可能无法充分捕捉复杂空间上下文和细粒度身份线索。同时，检测和重识别任务共享主干特征会导致优化目标冲突。

Method: 提出DiffPS框架，利用预训练扩散模型的先验知识，包含三个模块：扩散引导区域提议网络(DGRPN)用于改进人员定位，多尺度频率细化网络(MSFRN)减轻形状偏差，语义自适应特征聚合网络(SFAN)利用文本对齐的扩散特征。

Result: DiffPS在CUHK-SYSU和PRW数据集上达到了新的最先进性能。

Conclusion: 扩散模型的先验知识可以有效解决人员搜索中检测和重识别任务的优化冲突问题，提出的三个专门模块能够充分利用扩散特征的优势。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU是首个将流匹配与HSI重建结合的展开网络，通过嵌入生成先验和引入平均速度损失来提升重建质量


<details>
  <summary>Details</summary>
Motivation: 高光谱成像成本高且重建困难，现有压缩感知系统在严重退化和光谱细节丢失方面存在挑战

Method: 将流匹配的生成先验嵌入深度展开框架，引入平均速度损失来增强流动态的全局一致性

Result: 在模拟和真实数据集上的广泛实验表明，FMU在重建质量上显著优于现有方法

Conclusion: FMU成功结合了基于优化方法的可解释性和流匹配的生成能力，实现了更鲁棒准确的HSI重建

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的双列直插式封装（DIP）自动缺陷检测系统，使用ConSinGAN生成训练数据，YOLOv7模型在准确率和检测时间上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统工业组件缺陷检测耗时耗力，给质检人员带来负担且难以管理产品质量。

Method: 使用数字相机光学和深度学习模型，采用ConSinGAN生成缺陷图像数据集，比较YOLOv3、v4、v7、v9四种模型性能。

Result: YOLOv7结合ConSinGAN在准确率达到95.50%，检测时间285毫秒，显著优于阈值方法。

Conclusion: 该系统可轻松应用于多种缺陷类型或缺陷数据不足的情况，并开发了SCADA系统进行监控。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 本文提出了一种名为FoundAD的少样本异常检测方法，利用预训练视觉编码器学习图像嵌入差异来检测异常，在多类检测中表现出色且参数更少。


<details>
  <summary>Details</summary>
Motivation: 少样本异常检测在工业安全检测中很重要，但有限样本使得正常和异常特征难以区分，特别是在类别不可知条件下。预训练视觉编码器通过学习大量数据来理解正常图像的分布，这为异常检测提供了基础。

Method: FoundAD通过学习非线性投影算子到自然图像流形上，利用图像中异常数量与学习嵌入差异之间的相关性来检测异常。该方法支持多类检测，参数使用量显著少于现有方法。

Result: 大量实验表明，该方法在多类检测中具有竞争力，并且在使用DINOv3等多种基础编码器的评估中表现良好。

Conclusion: 这种方法拓宽了对基础特征的理解，并推动了少样本异常检测领域的发展。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT通过可训练的聚类模块和再生器模块，在保持分割精度的同时显著降低了Vision Transformer的计算复杂度


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在真实机器人系统中应用受限，主要因为其二次注意力复杂度。现有的token合并方法适合分类任务但不适合密集预测任务如语义分割

Method: 扩展Vision Transformer架构，引入可训练的Cluster模块根据分割掩码的伪聚类指导合并相似token，然后通过Regenerator模块恢复细节信息

Result: 在三个不同数据集上实现了2.18倍更少的GFLOPs和1.64倍更快的推理速度，同时保持可比较的分割精度

Conclusion: ClustViT为Vision Transformer在语义分割任务中的高效应用提供了可行方案，代码和模型将公开

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT提出了一种统一的多模态大语言模型范式，通过视觉参考令牌（VRTs）直接生成文本和视觉输出，解决了现有方法在密集预测任务上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在视觉任务中依赖间接表示（如将坐标生成为文本），限制了性能并阻碍了分割等密集预测任务。需要一种能直接生成多样化视觉输出的方法。

Method: 提出Patch-as-Decodable Token（PaDT）范式，使用视觉参考令牌（VRTs）与LLM输出令牌交织，通过轻量级解码器生成检测、分割和定位预测。采用动态扩展嵌入表和专门的训练策略。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终达到最先进性能，甚至优于显著更大的MLLM模型。

Conclusion: PaDT为MLLMs提供了一种有效的统一范式，能够直接处理多样化视觉输出任务，在多个任务上表现出色。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本文提出TriAlignXA可解释AI框架，通过三重优化引擎解决农产品电商中的'信任三角'问题，实现从算法决策到消费者信任的转化


<details>
  <summary>Details</summary>
Motivation: 解决在线果蔬电商因无法提供直接感官体验而产生的'信任赤字'问题

Method: 构建'信任金字塔'模型，提出'三角信任指数'，设计TriAlignXA可解释AI框架，包含生物自适应引擎、时效优化引擎和经济优化引擎，并采用'预映射机制'将过程数据编码为QR码

Result: 在分级任务上的实验显示显著高于基线模型的准确率，实证和理论分析验证了框架在解决'不可能三角'问题上的平衡能力

Conclusion: 该研究为构建可信赖的在线农产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个一致且交互式的4D高斯溅射编辑框架，通过4D感知的InstructPix2Pix模型、多视图网格模块和Gaussian选择机制解决视图、时间和非编辑区域一致性问题，并利用LLM模块实现用户意图理解。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯溅射编辑方法在视图一致性、时间一致性和非编辑区域一致性方面存在挑战，且难以处理复杂的文本指令。

Method: 1. 引入4D感知的InstructPix2Pix模型，集成4D VGGT几何特征；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出Gaussian选择机制仅优化编辑区域的Gaussian；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 相比现有方法，该框架实现了更一致和可控的4D场景编辑，能够处理复杂的用户指令。

Conclusion: 4DGS-Craft通过多技术融合有效解决了4D高斯溅射编辑中的一致性和交互性问题，为复杂4D场景编辑提供了可行方案。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP)是一种像素级掩码机制，通过固定颜色中心点对像素分类，在图像超分辨率任务中识别纯像素并免除其昂贵计算，集成到ATD-light模型后实现了更好的重建质量和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer虽然尝试根据内容恢复难度路由不同复杂度的token mixer，但仍存在适应性差、掩码粒度粗、空间灵活性不足等问题。

Method: 提出Pure-Pass像素级掩码机制，利用固定颜色中心点将像素分类到不同类别，识别纯像素并免除其复杂计算，实现细粒度、空间灵活的自适应掩码。

Result: 将PP集成到ATD-light模型后，PP-ATD-light在节省相似计算量的情况下，在重建质量和参数效率方面均优于CAMixer-ATD-light。

Conclusion: Pure-Pass机制通过像素级精细掩码有效提升了轻量级超分辨率模型的性能，证明了细粒度自适应计算在图像重建任务中的重要性。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o多模态能力自动生成牙科全景X光片的下颌囊肿诊断报告，提出了自校正循环结构化输出(SLSO)框架，相比传统思维链方法在多个评估指标上显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 传统AI诊断系统在牙科影像分析中存在输出不一致、幻觉现象等问题，需要开发更可靠的自动化诊断报告生成方法。

Method: 构建SLSO框架，包含10步处理流程：图像输入分析、结构化数据生成、牙位编号提取与一致性检查、不一致时迭代重新生成、诊断报告生成与结构化验证。

Result: SLSO框架相比CoT方法在牙位编号、牙齿移动和牙根吸收三个指标上分别提高了66.9%、33.3%和28.6%的准确率，成功案例最多经过5次重新生成即可获得一致结构化输出。

Conclusion: SLSO框架能有效抑制幻觉、提高牙位识别准确性，但对于跨越多牙的大范围病变识别仍有限制，需要进一步优化以实现实用化诊断报告生成系统。

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net是一个3D自动编码器架构，专门用于从LiDAR点云数据中提取高效特征，通过简化的编码器层和跳连连接实现高效重建，并在保持性能的同时具有良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有最先进的3D自动编码器架构需要大量计算资源，而本文旨在开发一种轻量级但高效的架构，仅使用LiDAR点云数据就能从真实交通环境中提取有效特征。

Method: 提出LiLa-Net架构，采用减少编码器层数和简化跳连连接的方法，在保持潜在空间代表性的同时降低计算复杂度。系统利用跳连连接概念来提升性能而不需要大量资源。

Result: 模型在保持准确重建原始点云的同时，实现了跳连连接信息和潜在编码之间的有效平衡，提高了重建质量。模型还展示了强大的泛化能力，能够成功重建与原始交通环境无关的物体。

Conclusion: LiLa-Net通过简化的架构设计实现了高效的3D点云重建，在计算资源有限的情况下仍能保持良好性能，并具有优秀的泛化能力，为实时交通环境分析提供了可行的解决方案。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 开发了kabr-tools开源工具包，通过无人机视频和机器学习实现多物种行为自动监测，相比传统方法显著提升行为数据采集效率和准确性


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法范围有限、耗时耗力，难以大规模评估动物行为响应，需要可扩展的自动化解决方案

Method: 集成无人机视频与机器学习系统，使用目标检测、追踪和行为分类技术提取行为、社交和空间指标

Result: 验证了969个行为序列，无人机观察减少15%可见性损失，捕获更多行为转换；发现不同斑马物种在警戒行为、栖息地影响和空间隔离方面的差异

Conclusion: kabr-tools为生态系统范围研究提供了强大的自动化行为监测工具，推进了保护、生物多样性研究和生态监测

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个基于3D高斯泼溅的语义感知3D形状和纹理变形框架，通过网格引导实现高保真几何和外观建模，无需预定义同胚映射或标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖点云或需要预定义同胚映射的限制，实现无标签数据的语义感知3D变形。

Method: 使用网格引导的3D高斯泼溅技术，将3D高斯锚定到重建的网格面片上，通过统一的变形策略和拓扑感知约束确保几何一致性和纹理保真度。

Result: 在TexMorph基准测试中，颜色一致性误差(ΔE)降低22.2%，EI指标降低26.2%，显著优于现有2D/3D方法。

Conclusion: 该框架能够同时保持局部细节和全局语义一致性，为3D形状和纹理变形提供了有效的无监督解决方案。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，通过扩散模型仅使用旋转测量数据实现零样本泛化的人体姿态估计


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法在跨用户泛化方面表现不佳，主要因为位置测量高度依赖用户体型，需要解决这一泛化问题

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅以旋转测量为条件，通过似然项引导模型生成与测量位置一致的姿态序列

Result: 提出的InPose方法能够在零样本设置下，为任意用户生成与稀疏体表测量数据最匹配的高概率姿态序列

Conclusion: 通过仅依赖旋转测量并利用扩散模型先验，该方法实现了更好的跨用户泛化能力，解决了位置测量对体型敏感的问题

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: 提出VGDM框架，结合视觉Transformer和扩散模型进行脑肿瘤检测与分割，相比传统U-Net在Dice相似度和Hausdorff距离指标上均有提升


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面存在局限，限制了在复杂肿瘤结构上的性能表现。扩散模型在医学图像生成和分割边界细化方面展现出强大潜力

Method: VGDM框架在扩散过程核心嵌入视觉Transformer，利用全局上下文推理和迭代去噪增强体积精度和边界精度。Transformer骨干网络有效建模整个MRI体积的空间关系，扩散细化缓解体素级误差并恢复细粒度肿瘤细节

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似度和Hausdorff距离指标上取得一致增益

Conclusion: 这种混合设计为神经肿瘤学提供了改进鲁棒性和可扩展性的途径，超越了传统U-Net基线，展示了Transformer引导扩散模型在推进肿瘤分割技术前沿的潜力

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发了一个可扩展的深度学习管道，从历史地图中提取法国1925-1950年的城市足迹数据，填补了1970年代前全国性数字城市数据空白。


<details>
  <summary>Details</summary>
Motivation: 1970年代前法国历史城市扩张的定量分析因缺乏全国性数字城市足迹数据而受限，需要解决这一数据缺口。

Method: 采用双通道U-Net方法处理历史地图的复杂辐射和风格特征，第一通道生成初步地图识别混淆区域，第二通道使用精炼数据集减少辐射噪声。

Result: 处理了941个高分辨率图块，覆盖整个法国大陆，最终镶嵌图总体准确率达到73%，有效捕捉了多样化城市模式。

Conclusion: 成功创建了首个该时期的全国性城市足迹数据集，并公开了代码、训练数据和结果，支持长期城市化动态研究。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文系统分析了腹腔镜胆囊切除术视频中点追踪的失败模式，发现点追踪对于手术工具表现良好，但对于解剖目标（如胆囊）表现不佳，主要由于组织相似性和模糊边界导致失败。


<details>
  <summary>Details</summary>
Motivation: 虽然像SAM2这样的视频对象分割模型在手术视频中展现出有前景的零样本追踪能力，但点追踪这种高效低成本的输入方式在复杂手术环境中的可靠性和失败案例尚未得到充分理解。

Method: 研究聚焦于胆囊、抓钳和L型电钩三个手术目标，在腹腔镜胆囊切除术视频中系统比较了点追踪与分割掩模初始化的性能表现。

Result: 结果显示点追踪对于手术工具具有竞争力，但对于解剖目标持续表现不佳，主要失败原因是组织相似性和模糊边界。通过定性分析揭示了影响追踪结果的关键因素。

Conclusion: 研究提供了多个可操作的建议，包括如何选择和放置追踪点，以改善手术视频分析中的性能表现。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出了FFREEDG新任务和FRIEREN框架，解决联邦学习中语义分割任务在无标签客户端数据上的领域适应问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法要么不切实际地假设客户端有标签数据，要么未能充分利用现代视觉基础模型的能力，而真实场景中客户端数据通常是无标签的

Method: 使用视觉-语言解码器结合CLIP文本嵌入进行语义消歧，采用弱到强一致性学习策略在伪标签上进行鲁棒的本地训练

Result: 在合成到真实和清晰到恶劣天气基准测试中表现优异，与现有领域泛化和适应方法相比具有竞争力

Conclusion: FRIEREN框架有效解决了FFREEDG任务，为未来研究建立了强基线

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个基于动作知识的提示框架，通过结构化提示让冻结的视觉语言模型更好地进行视频异常检测，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法过于抽象，忽略了细粒度的人-物交互和动作语义，无法有效检测监控视频中的复杂异常行为。

Method: 提出ASK-Hint框架，将提示按语义分组（如暴力、财产犯罪、公共安全），并制定细粒度指导问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上的实验显示，ASK-Hint显著提升AUC指标，优于现有微调和无训练方法，并展现出良好的可解释性和泛化性。

Conclusion: 研究证明了提示粒度的重要性，ASK-Hint为可解释的视频异常检测提供了一种无需训练且可泛化的解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify是一种解决2D视觉语言模型特征向3D语义分割迁移时存在权衡问题的方法，通过利用潜在几何信息和学习亲和力网络，在仅使用约1.5%训练数据的情况下达到或超越最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的2D到3D特征迁移方法存在权衡：直接投影2D特征会产生噪声和碎片化预测，而强制几何一致性需要昂贵的训练流程和大规模标注3D数据。这种限制源于分割-匹配范式未能将2D语义与3D几何结构相协调。

Method: 提出GeoPurify方法，应用小型学生亲和力网络，利用从3D自监督教师模型提取的几何先验来纯化2D VLM生成的3D点特征。在推理时，设计几何引导池化模块进一步去噪点云并确保语义和结构一致性。

Result: 在主要3D基准测试上的广泛实验表明，GeoPurify在仅使用约1.5%训练数据的情况下，达到或超越了最先进的性能。

Conclusion: GeoPurify通过利用潜在几何信息和学习亲和力网络，有效缓解了2D到3D特征迁移的权衡问题，实现了卓越的数据效率。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出一种基于猪耳静脉模式的非侵入性生物识别方法，使用智能手机采集图像，通过多阶段计算机视觉流程和机器学习模型实现98.12%的识别准确率，为小规模农户提供低成本、无压力的动物识别方案。


<details>
  <summary>Details</summary>
Motivation: 传统猪只识别方法（如耳标和微芯片）不可靠、成本高且主要针对纯种猪，不适用于小规模农户。需要一种非侵入性、成本效益高的识别方案。

Method: 收集20只混种猪的800张耳部图像，使用标准智能手机和简单背光拍摄。开发多阶段计算机视觉流程增强静脉可见性，提取结构和空间特征生成生物特征签名，使用支持向量机等机器学习模型进行分类。

Result: 支持向量机模型在混种猪群中达到98.12%的识别准确率，从图像处理到分类平均耗时8.3秒，证明可实现实时农场部署。

Conclusion: 该系统用永久性生物标记替代脆弱的物理标识，为农民提供成本效益高且无压力的动物识别方法，证实了耳静脉生物识别技术在数字化畜牧管理中的实用性，有助于将精准农业效益扩展到资源有限的农业社区。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一个多类别计数框架，使用Twins金字塔视觉Transformer骨干网络和专门的多类计数头，通过两任务设计减少类别间干扰，在密集场景中优于传统检测方法。


<details>
  <summary>Details</summary>
Motivation: 解决密集和遮挡场景中物体计数问题，传统基于检测的方法在这些场景中容易失效，需要更有效的密度图估计方法。

Method: 采用Twins金字塔视觉Transformer作为骨干网络，构建专门的多类计数头，结合多尺度解码方法，并添加基于分割的类别聚焦模块来抑制训练时的类别间干扰。

Result: 在VisDrone和iSAID基准测试中表现优异，MAE指标分别降低了33%、43%和64%，相比YOLOv11在密集场景中更具优势。

Conclusion: 该方法在密集场景计数方面具有显著优势，区域损失设计使其可扩展到新领域，如生物多样性监测，为保护工作和生态研究提供可扩展的解决方案。

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过优化跨注意力图来实现视频生成中的时间控制，允许用户指定视觉元素在生成序列中的出现时间。


<details>
  <summary>Details</summary>
Motivation: 现有的生成视频模型缺乏细粒度的时间控制能力，无法让用户指定特定视觉元素在生成序列中的出现时机。

Method: TempoControl利用文本到视频扩散模型中的跨注意力图，通过新颖的优化方法引导概念的时间安排。该方法采用三个互补原则：通过相关性对齐时间形状、通过能量放大可见性区域、通过熵保持空间焦点。

Result: TempoControl能够在确保高质量和多样性的同时，实现对视频生成时间的精确控制，适用于时间重排序、多对象控制以及动作和音频对齐生成等多种应用场景。

Conclusion: 该方法有效解决了生成视频模型中时间控制不足的问题，为视频生成提供了更精细的时间控制能力。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: RewardMap是一个多阶段强化学习框架，通过难度感知奖励设计和多阶段训练策略，解决了多模态大语言模型在细粒度视觉推理任务中的稀疏奖励和不稳定优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在结构化信息丰富的场景（如交通地图）中的空间推理能力不足，标准强化学习面临稀疏奖励和优化不稳定的挑战。

Method: 构建ReasonMap-Plus数据集提供密集奖励信号，提出RewardMap框架包含难度感知奖励设计和多阶段RL方案，从简单感知任务逐步训练到复杂推理任务。

Result: 在ReasonMap和ReasonMap-Plus上的实验表明，RewardMap各组件均带来性能提升，组合使用效果最佳，在6个基准测试上平均提升3.47%。

Conclusion: RewardMap有效提升了MLLMs的视觉理解和推理能力，特别是在细粒度视觉推理任务中表现出色，具有很好的泛化性。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个利用FLUX强大先验进行拖拽式图像编辑的框架，通过区域级编辑和仿射变换解决了DiT特征结构不足的问题，显著提升了编辑质量。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet-based DDPMs转向更可扩展的DiT with flow matching（如SD3.5、FLUX），生成先验显著增强，但拖拽式编辑尚未受益于这些更强的先验。现有方法在目标区域存在严重失真问题。

Method: 提出区域级编辑范式，使用仿射变换实现更丰富一致的特征监督；集成预训练开放域个性化适配器（如IP-Adapter）增强主体一致性；采用基于梯度掩码的硬约束保护背景保真度；利用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的大量实验表明，DragFlow超越了基于点和基于区域的基线方法，在拖拽式图像编辑中达到了新的最先进水平。

Conclusion: DragFlow成功利用FLUX的丰富先验，通过区域级编辑和多项技术创新，显著提升了拖拽式图像编辑的质量和效果，为该领域设立了新的标杆。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 该论文提出F2C方法，通过从关键帧选择扩展到关键片段选择来提升视频理解，同时采用自适应分辨率策略保持计算预算不变。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型面临"大海捞针"问题：原始视频帧产生的大量视觉标记会耗尽模型上下文窗口。现有解决方案通过选择稀疏帧来减少标记数量，但这种方法丢弃了关键的时间动态信息，导致对运动和事件连续性的推理效果不佳。

Method: 提出F2C方法，将选择范围从孤立的关键帧扩展到关键片段（短时、时间连贯的片段）。为在固定计算预算下处理片段带来的更大标记占用，采用自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试（Video-MME、LongVideoBench和MLVU）上的实验表明，这种无需训练的方法相比均匀采样分别提升了8.1%、5.6%和10.3%。

Conclusion: 结果强调了在帧选择中保持时间连贯性的重要性，并为将视频大语言模型扩展到现实世界视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 该研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元（IMU）在13种日常活动中的性能，发现MotionAGFormer表现最佳，揭示了两种技术在成本、可及性和精度方面的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和可穿戴传感器的发展，需要在真实条件下准确评估人体运动，这对远程医疗、运动科学和康复等领域至关重要。

Method: 使用VIDIMU数据集，比较了MotionAGFormer、MotionBERT、MMPose 2D-to-3D姿态提升和NVIDIA BodyTrack等深度学习框架与基于IMU的OpenSim逆运动学方法，评估了17个关键点的关节角度。

Result: MotionAGFormer表现最优，总体RMSE为9.27°±4.80°，MAE为7.86°±4.18°，Pearson相关系数为0.86±0.15，决定系数R²为0.67±0.28。

Conclusion: 两种技术都适用于实验室外的运动学评估，但各有优劣。研究为开发稳健、经济高效且用户友好的远程医疗解决方案提供了宝贵指南。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift提出了一种新的跨被试视觉刺激重建方法，通过集成AutoKL和CLIP适配器，仅需少量参数微调即可实现高性能的fMRI数据解码。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试视觉刺激重建中的挑战，包括被试间神经表征差异和大脑对复杂视觉输入的抽象语义编码问题。

Method: 集成互补适配器：AutoKL处理低级特征，CLIP处理语义特征；CLIP适配器在Stable Diffusion生成图像上训练；采用预训练+微调策略，仅微调17%参数。

Result: 在轻量级GPU（三块RTX 4090）上仅需1小时训练即可达到最先进性能，优于现有方法。

Conclusion: NeuroSwift为跨被试视觉重建提供了一种高效且性能优越的解决方案，显著降低了计算需求。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一种自训练框架，通过细粒度线索联合优化CLIP的视觉和文本表示，在13个细粒度基准测试上平均提升2.90%准确率


<details>
  <summary>Details</summary>
Motivation: CLIP依赖粗粒度全局特征限制了其在细粒度分类任务上的性能，现有方法缺乏空间精度

Method: 使用Saliency-Oriented Attention Pooling构建细粒度token，结合双头LLM分类器和动态知识聚合进行伪标签迭代优化

Result: 在13个细粒度基准测试上平均获得2.90%的准确率提升，仅需轻量级适配

Conclusion: microCLIP能够有效挖掘CLIP中的潜在细粒度信号，实现细粒度分类性能的显著提升

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个基于多模态大语言模型(MLLM)的视频真实性检测器，采用群体相对策略优化(GRPO)进行微调，能够提供高准确度的分类和可解释的推理。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频技术的快速发展，急需有效的检测工具来应对错误信息和声誉损害等社会风险，同时检测模型需要提供可解释的说明以确保监管机构和最终用户的透明度。

Method: 使用包含14万真实和AI生成视频的挑战性数据集，通过GRPO方法微调Qwen-VL模型，采用两个专门针对时间伪影和生成复杂度的奖励模型。

Result: VidGuard-R1在现有基准测试中实现了最先进的零样本性能，额外训练后准确率超过95%，案例研究显示其预测具有精确且可解释的推理。

Conclusion: VidGuard-R1通过结合多模态大语言模型和GRPO优化，成功实现了高准确度的视频真实性检测，并提供了可解释的决策依据，代码已公开可用。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频教师监督或重新训练长视频数据集。


<details>
  <summary>Details</summary>
Motivation: 扩散模型依赖Transformer架构导致计算成本过高，现有自回归方法在生成超出训练范围的长视频时会出现质量显著下降的问题。

Method: 利用教师模型的丰富知识，通过从自生成长视频中提取的片段为学生模型提供指导，保持时间一致性同时将视频长度扩展到教师能力的20倍。

Result: 方法能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上，在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效解决了长视频生成中的质量退化问题，展示了在标准基准和改进基准上的优越性能。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种基于物理引导的视频生成方法，通过两阶段训练策略和对象掩码技术，显著改善了视频生成中的物体交互真实性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物体交互的物理合理性方面存在不足，缺乏基于物理的控制机制。KineMask旨在解决这一限制，为机器人和具身决策提供更可靠的世界模拟器。

Method: 提出两阶段训练策略：首先使用合成场景训练视频扩散模型，通过对象掩码逐步移除未来运动监督；结合低级运动控制（物体速度）和高级文本条件（预测性场景描述）来实现复杂动力学现象的合成。

Result: 在真实场景中显著改善了物体交互质量，相比同规模模型取得了明显提升。消融研究验证了低级和高级条件在视频扩散模型中的互补作用。

Conclusion: KineMask通过物理引导的视频生成方法，成功实现了更真实的刚性物体控制、交互和效果，为视频生成提供了有效的物理基础控制机制。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 该论文提出了一种用于精细控制的多模态动作模拟方法，通过整合本体感觉、动觉、力触觉和肌肉激活等感官信息，提高了模拟精度并减少了时间漂移。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法作为世界模型。通用家庭机器人需要实时精细运动控制来处理精细任务和紧急情况。

Method: 开发了特征学习范式来对齐多模态感官信息，同时保持每个模态的独特信息；提出了正则化方案来增强动作轨迹特征在表示复杂交互动态中的因果性。

Result: 实验表明，整合多模态感官提高了模拟精度，减少了时间漂移。广泛的消融研究和下游应用证明了该方法的有效性和实用性。

Conclusion: 多模态感官的引入能够有效模拟精细交互，为机器人精细控制提供了可行的解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过将原生稀疏注意力（NSA）适配到视频语言模型中，解决了多模态语言模型因上下文长度限制而难以理解长视频的问题。该方法在Qwen2.5-VL模型上进行端到端训练，采用硬件感知的混合注意力机制，在文本上保持密集注意力，在视频上使用NSA。


<details>
  <summary>Details</summary>
Motivation: 多模态语言模型在视频理解方面受限于上下文长度，容易错过关键过渡帧，难以在长时间尺度上保持连贯性。

Method: 将原生稀疏注意力（NSA）适配到视频语言模型中，通过端到端训练在216K视频指令数据集上训练Qwen2.5-VL模型，采用硬件感知的混合注意力方法（文本用密集注意力，视频用NSA）。

Result: 与token压缩和无训练稀疏基线相比，VideoNSA在长视频理解、时序推理和空间基准测试上表现更好，能够可靠扩展到128K tokens，并发现全局-局部注意力分配、任务依赖的分支使用模式等关键发现。

Conclusion: VideoNSA有效解决了视频理解中的上下文长度限制问题，通过稀疏注意力机制提升了长视频理解能力，并揭示了注意力机制在视频语言模型中的动态特性。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过重新校准噪声调度器来解决扩散模型在不同分辨率下的泛化问题，显著提升低分辨率图像生成质量


<details>
  <summary>Details</summary>
Motivation: 现有的高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供经济高效的替代方案，扩散模型在固定分辨率训练后难以泛化到其他分辨率，特别是低分辨率

Method: 提出NoiseShift方法，识别噪声调度器在不同分辨率下具有不同的感知效果，通过根据分辨率大小重新校准去噪器的噪声水平，无需改变模型架构或采样计划

Result: 在Stable Diffusion 3、3.5和Flux-Dev上应用NoiseShift后，低分辨率生成质量显著提升：LAION-COCO数据集上FID分别提升15.89%、8.56%和2.44%；CelebA数据集上FID分别提升10.36%、5.19%和3.02%

Conclusion: NoiseShift有效缓解了分辨率相关的伪影，提高了低分辨率图像生成质量，为扩散模型提供了更好的分辨率泛化能力

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该论文研究从视频中预测动态物理属性的任务，包括弹性、粘度和动态摩擦，通过收集新数据集并探索三种推断方法，发现视频基础模型表现良好但不及Oracle方法，MLLMs性能较差但可通过提示改进。


<details>
  <summary>Details</summary>
Motivation: 研究从视频中推断需要时间信息才能判断的动态物理属性，如弹性、粘度和动态摩擦，填补现有研究在动态物理属性预测方面的空白。

Method: 收集三个物理属性的新视频数据集；探索三种推断方法：Oracle方法（使用经典计算机视觉技术）、基于预训练视频生成和自监督模型的简单读取机制、多模态大语言模型的提示策略。

Result: 视频基础模型（生成式或自监督训练）表现相似但不及Oracle方法；多模态大语言模型目前性能较差，但通过合适的提示可以改进。

Conclusion: 视频基础模型在动态物理属性预测方面具有潜力，但Oracle方法仍是最佳；多模态大语言模型需要进一步优化提示策略来提升性能。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出声音对象检测任务，通过多模态对象感知框架从第一人称视角视频中学习，利用自动分割管道和slot attention视觉编码器实现最先进性能


<details>
  <summary>Details</summary>
Motivation: 日常物体交互产生独特声音，但现有模型难以区分声音来源（如勺子敲击木地板与地毯的声音差异），需要开发能够将声音与直接参与的物体联系起来的能力

Method: 开发自动管道计算物体分割掩码，引导模型关注交互信息最丰富的区域；使用slot attention视觉编码器强化物体先验；基于第一人称视角视频进行多模态对象感知学习

Result: 在新任务和现有多模态动作理解任务上均达到最先进性能

Conclusion: 提出的声音对象检测任务和对象感知框架能够有效学习物体与声音的关联，为多模态理解提供了新思路

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯泼溅（3DGS）的密度引导毒化攻击方法，通过在低密度区域注入高斯点来嵌入视角相关的虚幻物体，并引入自适应噪声策略破坏多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法的普及，研究其安全漏洞变得至关重要。本文旨在分析3DGS对图像级毒化攻击的鲁棒性。

Method: 1. 使用核密度估计（KDE）识别低密度区域
2. 在低密度区域战略性地注入高斯点
3. 引入自适应噪声策略破坏多视角一致性
4. 提出基于KDE的攻击难度评估协议

Result: 大量实验表明，该方法在攻击效果上优于现有最先进技术，能够从毒化视角清晰显示虚幻物体，同时对无辜视角影响最小。

Conclusion: 该研究揭示了3DGS的安全漏洞，提出的密度引导毒化攻击方法具有显著效果，为未来相关安全研究提供了客观的基准评估框架。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 该论文提出了首个理论框架和优化目标，通过随机最优控制方法解决文本到图像模型在多主体描述中的属性泄漏、身份纠缠和主体遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在单实体提示上表现优秀，但在处理多主体描述时存在属性泄漏、身份纠缠和主体遗漏等问题，需要开发专门的方法来提高多主体保真度。

Method: 通过将流匹配与随机最优控制相结合，提出了两种架构无关的算法：1）无需训练的单次更新测试时控制器；2）轻量级微调方法Adjoint Matching，通过回归控制网络到后向伴随信号来保持基础模型能力。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL等模型上，两种算法都显著提高了多主体对齐度，同时保持了基础模型的风格。测试时控制可在普通GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见过的提示。

Conclusion: 提出的FOCUS方法在多主体保真度方面达到了最先进的性能，为多主体文本到图像生成提供了统一的框架和有效的解决方案。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [78] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音频-视频生成范式，通过双DiT模块的块级跨模态融合，将两种模态建模为单一生成过程，实现了自然同步，无需单独的流水线或后处理对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的音频-视频生成方法通常依赖于复杂的多阶段架构或声音和视觉的顺序合成，存在同步困难的问题。

Method: 使用块级跨模态融合的双DiT模块，初始化音频塔与预训练视频模型相同的架构，通过在大量视频语料上联合训练视频和音频塔，实现时序和语义的双向跨模态融合。

Result: 模型能够生成具有自然语音和准确、上下文匹配音效的电影级视频片段，支持电影叙事。

Conclusion: Ovi提供了一个统一的音频-视频生成框架，通过端到端的训练实现了高质量的跨模态生成效果。

Abstract: Audio-video generation has often relied on complex multi-stage architectures
or sequential synthesis of sound and visuals. We introduce Ovi, a unified
paradigm for audio-video generation that models the two modalities as a single
generative process. By using blockwise cross-modal fusion of twin-DiT modules,
Ovi achieves natural synchronization and removes the need for separate
pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion
modeling, we initialize an audio tower with an architecture identical to that
of a strong pretrained video model. Trained from scratch on hundreds of
thousands of hours of raw audio, the audio tower learns to generate realistic
sound effects, as well as speech that conveys rich speaker identity and
emotion. Fusion is obtained by jointly training the identical video and audio
towers via blockwise exchange of timing (via scaled-RoPE embeddings) and
semantics (through bidirectional cross-attention) on a vast video corpus. Our
model enables cinematic storytelling with natural speech and accurate,
context-matched sound effects, producing movie-grade video clips. All the
demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [79] [Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare](https://arxiv.org/abs/2510.01194)
*Juan Barrientos,Michaelle Pérez,Douglas González,Favio Reyna,Julio Fajardo,Andrea Lara*

Main category: cs.HC

TL;DR: 开发了一个人机协同AI系统，帮助助产士在资源匮乏地区使用盲扫协议获取诊断相关的胎儿图像，通过AI识别关键帧并支持专家异步审查，旨在扩大产前影像服务的可及性。


<details>
  <summary>Details</summary>
Motivation: 在低收入和中等收入国家的农村地区，产科超声检查资源有限，需要一种能够帮助非专业人士获取有效胎儿图像的技术解决方案。

Method: 构建了一个包含分类模型和基于网络平台的AI系统，使用盲扫协议采集视频，AI识别关键诊断帧，专家可异步审查关键帧而非整个视频。

Result: 系统在非专业人士采集的盲扫视频中成功识别标准胎儿平面，现场评估显示系统可用性好且认知负荷低。

Conclusion: 该系统有望在资源匮乏地区扩大产前影像服务的覆盖范围，提高超声检查的可及性。

Abstract: Access to obstetric ultrasound is often limited in low-resource settings,
particularly in rural areas of low- and middle-income countries. This work
proposes a human-in-the-loop artificial intelligence (AI) system designed to
assist midwives in acquiring diagnostically relevant fetal images using blind
sweep protocols. The system incorporates a classification model along with a
web-based platform for asynchronous specialist reviews. By identifying key
frames in blind sweep studies, the AI system allows specialists to concentrate
on interpretation rather than having to review entire videos. To evaluate its
performance, blind sweep videos captured by a small group of soft-trained
midwives using a low-cost Point-of-Care Ultrasound (POCUS) device were
analyzed. The system demonstrated promising results in identifying standard
fetal planes from sweeps made by non-experts. A field evaluation indicated good
usability and a low cognitive workload, suggesting that it has the potential to
expand access to prenatal imaging in underserved regions.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [80] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了一种新的视频帧插值质量评估指标PSNR_DIV，通过运动发散加权增强PSNR，在保持高效计算的同时显著提升评估准确性


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值质量评估指标（如PSNR、SSIM、LPIPS）忽略时间一致性，而专门指标如FloLPIPS计算效率低下，限制了实际应用

Method: 基于运动发散加权的PSNR改进方法，借鉴档案电影修复中的时间不一致性检测技术，突出运动场中的奇异点来加权图像误差

Result: 在BVI-VFI数据集上评估显示，PSNR_DIV相比FloLPIPS提升0.09皮尔逊相关系数，速度提升2.5倍，内存使用减少4倍，性能在所有内容类别中保持一致

Conclusion: PSNR_DIV的高效性和准确性使其能够快速评估质量并作为视频帧插值神经网络训练的损失函数，具有实际应用价值

Abstract: Video frame interpolation is a fundamental tool for temporal video
enhancement, but existing quality metrics struggle to evaluate the perceptual
impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and
LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored
towards video frame interpolation, like FloLPIPS, have been developed but
suffer from computational inefficiency that limits their practical application.
We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric
that enhances PSNR through motion divergence weighting, a technique adapted
from archival film restoration where it was developed to detect temporal
inconsistencies. Our approach highlights singularities in motion fields which
is then used to weight image errors. Evaluation on the BVI-VFI dataset (180
sequences across multiple frame rates, resolutions and interpolation methods)
shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant
improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while
being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains
consistent across all content categories and are robust to the motion estimator
used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast
quality evaluation and practical use as a loss function for training neural
networks for video frame interpolation tasks. An implementation of our metric
is available at www.github.com/conalld/psnr-div.

</details>


### [81] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出Median2Median (M2M)框架，针对结构化噪声进行零样本去噪，通过新颖采样策略生成伪独立子图像对，无需训练数据即可有效处理各向异性相关噪声。


<details>
  <summary>Details</summary>
Motivation: 现实图像常被强各向异性相关的结构化噪声破坏，现有方法对此处理效果有限。数据驱动方法需要大量高质量标签且泛化性差，而零样本方法仅适用于独立同分布噪声。需要开发能处理结构化噪声的零样本去噪方案。

Method: M2M引入方向性插值和广义中值滤波生成伪独立子图像对，采用随机分配策略扩大采样空间并消除系统偏差，使采样对适用于Noise2Noise训练。

Result: 在真实模拟研究中，M2M在独立同分布噪声下与最先进零样本方法相当，在相关噪声下始终优于现有方法。

Conclusion: M2M是结构化噪声抑制的高效、无数据解决方案，标志着在超越严格独立同分布假设下实现有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical
imaging. However, real-world images are often degraded by structured noise with
strong anisotropic correlations that existing methods struggle to remove. Most
data-driven approaches rely on large datasets with high-quality labels and
still suffer from limited generalizability, whereas existing zero-shot methods
avoid this limitation but remain effective only for independent and identically
distributed (i.i.d.) noise. To address this gap, we propose Median2Median
(M2M), a zero-shot denoising framework designed for structured noise. M2M
introduces a novel sampling strategy that generates pseudo-independent
sub-image pairs from a single noisy input. This strategy leverages directional
interpolation and generalized median filtering to adaptively exclude values
distorted by structured artifacts. To further enlarge the effective sampling
space and eliminate systematic bias, a randomized assignment strategy is
employed, ensuring that the sampled sub-image pairs are suitable for
Noise2Noise training. In our realistic simulation studies, M2M performs on par
with state-of-the-art zero-shot methods under i.i.d. noise, while consistently
outperforming them under correlated noise. These findings establish M2M as an
efficient, data-free solution for structured noise suppression and mark the
first step toward effective zero-shot denoising beyond the strict i.i.d.
assumption.

</details>


### [82] [GFSR-Net: Guided Focus via Segment-Wise Relevance Network for Interpretable Deep Learning in Medical Imaging](https://arxiv.org/abs/2510.01919)
*Jhonatan Contreras,Thomas Bocklitz*

Main category: eess.IV

TL;DR: GFSR-Net是一种通过少量人工标注来引导深度学习模型关注图像中与诊断相关区域的方法，旨在提高医学图像分析的可解释性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中取得了显著成功，但其在临床实践中的应用受到可解释性不足的限制。模型可能在不解释推理过程的情况下做出正确预测，或者依赖与疾病无关的图像区域或现实条件下不存在的视觉线索，这会降低信任度并增加误诊风险。

Method: GFSR-Net使用少量人工标注来近似人类在图像中直观关注的位置，不需要精确边界或详尽标记。在训练过程中，模型学习将其注意力与这些区域对齐，逐步强调具有诊断意义的特征。该方法适用于不同类型的自然和医学图像，包括胸部X光、视网膜扫描和皮肤图像。

Result: 实验表明，GFSR-Net在达到相当或更优准确率的同时，产生的显著性地图能更好地反映人类预期。这减少了对无关模式的依赖，并提高了对自动化诊断工具的信心。

Conclusion: GFSR-Net通过引导模型关注与诊断相关的图像区域，有效提高了医学图像分析的可解释性和可靠性，为临床实践中的深度学习应用提供了更可信的解决方案。

Abstract: Deep learning has achieved remarkable success in medical image analysis,
however its adoption in clinical practice is limited by a lack of
interpretability. These models often make correct predictions without
explaining their reasoning. They may also rely on image regions unrelated to
the disease or visual cues, such as annotations, that are not present in
real-world conditions. This can reduce trust and increase the risk of
misleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance
Network (GFSR-Net), an approach designed to improve interpretability and
reliability in medical imaging. GFSR-Net uses a small number of human
annotations to approximate where a person would focus within an image
intuitively, without requiring precise boundaries or exhaustive markings,
making the process fast and practical. During training, the model learns to
align its focus with these areas, progressively emphasizing features that carry
diagnostic meaning. This guidance works across different types of natural and
medical images, including chest X-rays, retinal scans, and dermatological
images. Our experiments demonstrate that GFSR achieves comparable or superior
accuracy while producing saliency maps that better reflect human expectations.
This reduces the reliance on irrelevant patterns and increases confidence in
automated diagnostic tools.

</details>


### [83] [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](https://arxiv.org/abs/2510.02109)
*Jong Bum Won,Wesley De Neve,Joris Vankerschaver,Utku Ozbulak*

Main category: eess.IV

TL;DR: SpurBreast是一个专门设计的乳腺MRI数据集，旨在研究深度神经网络在医学影像中学习虚假相关性的问题。该数据集包含故意引入的虚假相关性，特别是磁场强度和图像方向两个主要虚假信号，用于评估模型对这些非临床特征的依赖程度。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据集缺乏系统性研究虚假相关性的能力，深度神经网络可能学习非临床特征而非有意义的医学模式，这限制了模型在真实世界中的部署效果。

Method: 通过分析100多个涉及患者、设备和成像协议的特征，识别出两个主要虚假信号：磁场强度（全局特征）和图像方向（局部特征）。创建包含虚假相关性的数据集分割，并同时提供无虚假相关性的基准数据集。

Result: 研究表明深度神经网络确实会利用这些非临床信号，在验证集上获得高准确率，但在无偏测试数据上泛化能力差。

Conclusion: SpurBreast数据集为系统研究临床相关和不相关特征、不确定性估计、对抗鲁棒性和泛化策略提供了重要资源，有助于提升医学影像AI模型的可靠性。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in medical
imaging, yet their real-world deployment remains challenging due to spurious
correlations, where models can learn non-clinical features instead of
meaningful medical patterns. Existing medical imaging datasets are not designed
to systematically study this issue, largely due to restrictive licensing and
limited supplementary patient data. To address this gap, we introduce
SpurBreast, a curated breast MRI dataset that intentionally incorporates
spurious correlations to evaluate their impact on model performance. Analyzing
over 100 features involving patient, device, and imaging protocol, we identify
two dominant spurious signals: magnetic field strength (a global feature
influencing the entire image) and image orientation (a local feature affecting
spatial alignment). Through controlled dataset splits, we demonstrate that DNNs
can exploit these non-clinical signals, achieving high validation accuracy
while failing to generalize to unbiased test data. Alongside these two datasets
containing spurious correlations, we also provide benchmark datasets without
spurious correlations, allowing researchers to systematically investigate
clinically relevant and irrelevant features, uncertainty estimation,
adversarial robustness, and generalization strategies. Models and datasets are
available at https://github.com/utkuozbulak/spurbreast.

</details>


### [84] [Measurement-Guided Consistency Model Sampling for Inverse Problems](https://arxiv.org/abs/2510.02208)
*Amirreza Tanevardi,Pooria Abbas Rad Moghadam,Sajjad Amini*

Main category: eess.IV

TL;DR: 本文提出了一种改进的一致性采样方法，专门用于逆问题重建，通过测量一致性机制引导采样器的随机性，在保持高效性的同时确保与测量数据的一致性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在解决逆成像问题时存在多步采样速度慢的问题，一致性模型虽然能实现快速生成，但直接应用于逆问题的研究不足。

Method: 提出改进的一致性采样方法，将采样器的随机性与测量算子相关联的测量一致性机制相结合，在保证测量数据保真度的同时保持基于一致性生成的高效性。

Result: 在Fashion-MNIST和LSUN Bedroom数据集上的实验表明，该方法在感知指标和像素级指标上均有提升，包括FID、KID、PSNR和SSIM，仅需少量步骤即可获得竞争性或更优的重建结果。

Conclusion: 该方法成功地将一致性模型的高效性与逆问题重建的准确性相结合，为快速高质量的图像重建提供了有效解决方案。

Abstract: Diffusion models have become powerful generative priors for solving inverse
imaging problems, but their reliance on slow multi-step sampling limits
practical deployment. Consistency models address this bottleneck by enabling
high-quality generation in a single or only a few steps, yet their direct
adaptation to inverse problems is underexplored. In this paper, we present a
modified consistency sampling approach tailored for inverse problem
reconstruction: the sampler's stochasticity is guided by a
measurement-consistency mechanism tied to the measurement operator, which
enforces fidelity to the acquired measurements while retaining the efficiency
of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom
datasets demonstrate consistent improvements in perceptual and pixel-level
metrics, including Fr\'echet Inception Distance, Kernel Inception Distance,
peak signal-to-noise ratio, and structural similarity index measure, compared
to baseline consistency sampling, yielding competitive or superior
reconstructions with only a handful of steps.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [85] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 论文探讨了领域专家提供的精炼知识在创建有效辅导系统中的作用，提出了两种利用专家知识的方法：使用XAI技术自动生成课程，以及利用专家制定的课程表开发自适应辅导系统。


<details>
  <summary>Details</summary>
Motivation: 当前AI教育社区往往忽视了领域专家提供的精炼知识在创建有效辅导系统中的作用，本文旨在强调这一主题的重要性。

Method: 1. 使用可解释AI技术结合专家指定的解题规则自动生成课程；2. 利用专家制定的学习目标概念课程表开发自适应辅导系统。

Result: 通过传粉者识别辅导系统的案例研究，证明了这些方法的重要性和可行性。

Conclusion: 专家精炼知识在创建新型教育系统中具有重要作用，能够提升学习体验并提高算法效率。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [86] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的响应编辑生成高质量的负样本，解决了合成偏好标注中的风格和长度偏差问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声，影响了模型对齐效果。

Method: 开发了基于LLM引导响应编辑的硬负样本生成框架，创建VaPR数据集（30K样本），用于微调LLaVA、Qwen2VL等LVLM模型。

Result: 在10个基准测试中平均提升6.5%（LLaVA）、4.0%（Qwen2VL）和1.5%（Qwen2.5VL），显著改善推理能力，减少二元问题中的"是"倾向。

Conclusion: VaPR框架有效解决了合成偏好数据中的偏差问题，且可推广到开源LLM作为编辑器，为LVLM对齐提供了高质量数据解决方案。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [87] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法虽然旨在提升大语言模型的推理能力，但研究发现其反而会缩小推理边界，原因在于负干扰现象和赢家通吃现象。


<details>
  <summary>Details</summary>
Motivation: 探究RLVR方法为何会限制推理能力扩展，而非提升它。

Method: 通过理论分析和实证研究，揭示了RLVR学习动态中的负干扰和赢家通吃现象，并提出了针对低概率问题的数据筛选算法。

Result: 发现RLVR会强化高概率正确解的问题，抑制低概率问题，导致模型收敛到狭窄的解决策略。提出的数据筛选算法显著提升了Pass@k性能。

Conclusion: RLVR的失败源于其固有的策略内采样机制，通过优化数据筛选可以缓解这一问题，提升模型推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [88] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN)方法通过生成多个代理轨迹并使用行为叙事进行选择，显著提高了计算机使用代理在复杂任务中的成功率和鲁棒性，在OSWorld上达到69.9%的SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了在长期复杂任务中的应用。

Method: 提出bBoN方法，通过生成多个代理轨迹并使用描述代理轨迹的行为叙事进行选择，实现广泛探索和原则性轨迹选择。

Result: 在OSWorld上达到69.9%的SOTA性能，接近人类水平（72%），在WindowsAgentArena和AndroidWorld上表现出强泛化能力。

Conclusion: 当正确实施时，扩展计算机使用代理具有惊人的有效性，而有效扩展需要结构化的轨迹理解和选择，bBoN为此提供了实用框架。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [89] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本文提出了一种在低资源设置下开发语言模型和多模态模型的方法，通过模型融合技术将多模态模型与纯语言模型结合，以保持多模态模型在纯语言任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉-语言模型参数庞大且训练数据量远超儿童语言习得的数据量，这种差异促使研究者开发更符合发展规律的低资源模型。多模态模型在纯语言任务上表现不佳的问题也需要解决。

Method: 使用发展合理的数据集在低资源设置下训练语言模型和多模态模型，并采用模型融合技术，通过加权线性插值将多模态模型参数与纯语言模型参数融合。

Result: 多模态模型在BabyLM基准测试中表现优于之前的基线，但在专注于语法的纯语言基准测试中表现不佳。模型融合技术在一定程度上缓解了这个问题，同时保持了多模态性能。

Conclusion: 多模态模型在纯语言任务（特别是语法相关任务）上确实存在性能不足的问题，而模型融合技术是解决这一问题的有效方法，能够在保持多模态性能的同时提升纯语言任务表现。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [90] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个用于系统化基准测试、分析和解释视觉语言模型（VLMs）的工具包，支持从开源VLMs的任何层提取中间输出。


<details>
  <summary>Details</summary>
Motivation: 为了简化对视觉语言模型的系统化分析和理解，解决模型特定复杂性带来的挑战。

Method: 提供统一的YAML可配置接口，抽象模型特定复杂性，支持16种最先进的基座VLM及其30多种变体，可扩展新模型而不改变核心逻辑。

Result: 通过两个简单分析实验展示了工具包的使用，揭示了VLM隐藏表示在不同层和目标概念上的系统性差异。

Conclusion: VLM-Lens作为开源项目发布，旨在加速社区对VLM的理解和改进工作。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [91] [JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator](https://arxiv.org/abs/2510.01213)
*Tao Han,Ang Li,Qinyu Chen,Chang Gao*

Main category: eess.SP

TL;DR: JaneEye是一种基于事件相机的高效能眼动追踪硬件加速器，专为可穿戴XR设备设计，通过轻量化神经网络和硬件优化实现高精度、低延迟和低功耗的眼动追踪。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的眼动追踪系统在XR应用中存在精度不足、延迟高和能耗大的问题，而事件相机具有超高时间分辨率和低功耗特性，为XR眼动追踪提供了更好的解决方案。

Method: 提出超轻量级神经网络架构，采用新型ConvJANET层简化传统ConvLSTM，仅保留遗忘门以减半计算复杂度；使用自定义线性激活函数近似和定点量化；通过软硬件协同设计实现ASIC芯片优化。

Result: 在3ET+数据集上达到2.45像素误差，仅使用17.6K参数，支持1250Hz事件帧率；12nm ASIC芯片在400MHz频率下实现0.5ms端到端延迟（相当于2000FPS），能效为18.9μJ/帧。

Conclusion: JaneEye为下一代XR可穿戴设备树立了低功耗高性能眼动追踪的新标杆，证明了事件相机在眼动追踪领域的巨大潜力。

Abstract: Eye tracking has become a key technology for gaze-based interactions in
Extended Reality (XR). However, conventional frame-based eye-tracking systems
often fall short of XR's stringent requirements for high accuracy, low latency,
and energy efficiency. Event cameras present a compelling alternative, offering
ultra-high temporal resolution and low power consumption. In this paper, we
present JaneEye, an energy-efficient event-based eye-tracking hardware
accelerator designed specifically for wearable devices, leveraging sparse,
high-temporal-resolution event data. We introduce an ultra-lightweight neural
network architecture featuring a novel ConvJANET layer, which simplifies the
traditional ConvLSTM by retaining only the forget gate, thereby halving
computational complexity without sacrificing temporal modeling capability. Our
proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+
dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To
further enhance hardware efficiency, we employ custom linear approximations of
activation functions (hardsigmoid and hardtanh) and fixed-point quantization.
Through software-hardware co-design, our 12-nm ASIC implementation operates at
400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames
Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a
new benchmark in low-power, high-performance eye-tracking solutions suitable
for integration into next-generation XR wearables.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [92] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人体化身的框架，通过使用材料点方法模拟器实现高真实感、鲁棒的动画和照片级渲染，在动态建模精度、渲染质量和鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的3D人体化身方法在松散衣物动态建模方面存在精度有限或对新动画输入鲁棒性不足的问题，需要更准确和鲁棒的解决方案。

Method: 采用材料点方法模拟器，结合各向异性本构模型和新型碰撞处理算法来建模复杂衣物变形和身体接触，并与支持准阴影的3D高斯溅射渲染的规范化身相结合。

Result: MPMAvatar在动态建模精度、渲染精度、鲁棒性和效率方面显著优于现有最先进的基于物理的化身方法，并能以零样本方式泛化到未见过的交互场景。

Conclusion: 该框架成功解决了松散衣物物理动态建模的挑战，为创建高度真实且鲁棒的3D人体化身提供了有效解决方案，突破了传统学习方法的泛化限制。

Abstract: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [93] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一种针对感兴趣区域的高效3D场景重建方法，通过对象感知的相机选择、目标对象训练和高质量对象重建的无缝集成，在保持实时性能的同时显著提升局部细节质量并减小模型尺寸。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在场景中均匀分配资源，限制了感兴趣区域的精细细节重建，导致模型尺寸过大。需要一种能够优先处理重要对象细节的方法。

Method: 提出ROI-GS框架，包含对象引导的相机选择、针对性对象训练，以及将高质量对象重建无缝集成到全局场景中的技术。

Result: 实验显示ROI-GS显著提升局部质量（PSNR最高提升2.96 dB），模型尺寸减少约17%，单对象场景训练速度更快，优于现有方法。

Conclusion: ROI-GS通过对象感知的资源分配策略，有效解决了3D场景重建中细节与效率的平衡问题，为高质量实时3D重建提供了可行方案。

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

</details>


### [94] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种可重光照框架，将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，实现了更高质量的镜面物体重建和重光照效果。


<details>
  <summary>Details</summary>
Motivation: 准确重建和重光照镜面物体一直是个挑战，现有神经渲染方法依赖简化的BRDF模型或耦合漫反射和镜面反射的参数化，限制了材质恢复的准确性和重光照保真度。

Method: 将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，采用延迟着色；使用基于扩散的表面法线和漫反射颜色先验指导早期优化；采用由粗到精的环境贴图优化策略。

Result: 在复杂镜面场景上的大量实验表明，该方法实现了高质量的几何和材质重建，相比现有高斯泼溅方法，在新光照条件下提供了更真实和一致的重光照效果。

Conclusion: 该框架通过物理一致的材质分解和优化策略，显著提升了镜面物体的重建和重光照质量，为神经渲染领域提供了有效的解决方案。

Abstract: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [95] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地回顾了基于深度学习的3D MRI重建方法，分析了四种主要方法：点云、网格、形状感知和体积模型，并讨论了临床应用、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在医学影像3D重建中的重要性日益增加，需要为研究人员提供一个结构化的方法概览，以识别推进深度学习在医学影像中更稳健、可泛化和临床影响力的机会。

Method: 采用文献综述方法，系统分析四种主要3D重建方法：点云模型、网格模型、形状感知模型和体积模型，并对每种方法的技术基础、局限性和应用进行评估。

Result: 提供了从心脏到神经到肺部成像的广泛概述，重点关注模型在病变解剖中的临床应用，以及训练和测试数据的影响，同时考察了公开数据集、计算需求和评估指标。

Conclusion: 该综述旨在为研究人员提供当前3D重建方法的结构化概览，以识别推进深度学习在医学影像中更稳健、可泛化和临床影响力解决方案的机会，并强调了多模态集成和跨模态框架等新兴研究方向。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [96] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 提出了一种基于低秩表示的神经压缩-重建框架，通过向量量化自编码器解决解码器计算瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 当代神经压缩方法虽然压缩率高，但基于卷积的解码器计算复杂、成本高，阻碍了技术应用

Method: 在向量量化自编码器中引入低秩表示，通过对学习到的潜在表示执行一系列计算高效的低秩操作来重建图像

Result: 该方法显著降低了神经压缩/重建解码阶段的计算开销，基本消除了解码器计算瓶颈，同时保持高质量图像输出

Conclusion: 基于低秩表示的新框架能有效解决神经压缩中的解码器瓶颈问题，为实际应用提供了可行方案

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [97] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出了一种自适应门控融合网络（AGFN），通过双门融合机制根据信息熵和模态重要性自适应调整特征权重，有效处理多模态情感分析中的噪声、缺失或语义冲突问题。


<details>
  <summary>Details</summary>
Motivation: 传统多模态融合方法难以处理模态质量变化（如噪声、缺失或语义冲突），导致在识别细微情感时性能不佳。

Method: 采用双门融合机制，基于信息熵和模态重要性自适应调整特征权重，在单模态编码和跨模态交互后优先选择信息丰富的模态线索。

Result: 在CMU-MOSI和CMU-MOSEI数据集上，AGFN在准确率上显著优于强基线，能够有效识别细微情感并具有鲁棒性能。

Conclusion: AGFN通过学习更广泛的特征分布增强泛化能力，减少特征位置与预测误差的相关性，创建更鲁棒的多模态特征表示。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [98] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 本文提出了一种无监督动态特征选择（DFS）方法，用于增强视觉任务中的潜在表示，通过去除噪声和冗余特征来提高模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受到噪声和无关特征的影响，这会降低模型的性能和泛化能力。现有方法通常依赖标注数据，限制了其广泛应用。

Method: 提出无监督动态特征选择（DFS）方法，为每个实例识别并移除图像中的误导性或冗余信息，确保只有最相关特征贡献到潜在空间。该方法不依赖标注数据，具有广泛适用性。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在各种任务（包括聚类和图像生成）中实现了显著的泛化性能提升，同时计算成本增加极小。

Conclusion: 无监督DFS是一种有效的潜在表示增强方法，能够提高模型性能并保持较低的计算开销，适用于多种视觉任务。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [99] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了G²RPO框架，通过奇异随机采样策略和多粒度优势集成模块，解决了现有基于流的强化学习方法在偏好对齐中的稀疏和窄奖励信号问题，实现了更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索高价值样本时有效，但由于稀疏和窄奖励信号导致偏好对齐效果不佳。需要解决随机采样过程中奖励与注入噪声相关性不足以及固定粒度去噪偏差的问题。

Method: 1. 奇异随机采样策略：支持逐步随机探索，强制奖励与注入噪声之间的高相关性；2. 多粒度优势集成模块：聚合多个扩散尺度的优势计算，产生更全面稳健的采样方向评估。

Result: 在各种奖励模型上的实验表明，G²RPO在领域内和领域外评估中均显著优于现有的基于流的GRPO基线方法。

Conclusion: G²RPO框架通过改进的采样策略和评估机制，有效提升了基于流的强化学习在生成模型与人类偏好对齐方面的性能和鲁棒性。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [100] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 提出了Anchored Posterior Sampling (APS)方法，用于预训练离散扩散基础模型的后验采样，解决了现有方法在梯度引导、连续松弛和维度诅咒等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在分类数据建模方面具有统一框架、更快推理和无需训练贝叶斯推理等优势，但现有后验采样方法面临稀疏信号、适用性限制和维度诅咒等问题。

Method: 基于两个关键创新：离散嵌入空间中的量化期望梯度引导和自适应解码的锚定重掩码，构建了APS方法。

Result: 在线性和非线性逆问题的标准基准测试中，APS在离散扩散采样器中实现了最先进的性能，并在无需训练的样式化和文本引导编辑中展示了优势。

Conclusion: APS方法有效克服了离散扩散后验采样的主要挑战，为预训练扩散模型的后验采样提供了实用解决方案。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [101] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS是一种用于扩散模型增量更新的新方法，通过识别与目标概念相关的神经元来避免灾难性遗忘，同时保持零样本生成能力。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中增量更新扩散模型具有实用性，但计算上具有挑战性。现有方法在持续学习场景下容易产生灾难性遗忘问题。

Method: 提出概念神经元选择（CNS）方法，识别扩散模型中与目标概念密切相关的神经元，以增量方式微调这些神经元并联合保留先前概念的知识。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中都优于先前方法。

Conclusion: CNS实现了无融合操作，减少了持续个性化所需的内存存储和处理时间，是一种简单有效的持续学习策略。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [102] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: EqM是一种基于平衡动力学的生成建模框架，摒弃传统扩散/流模型中的非平衡时间条件动态，通过学习隐式能量景观的平衡梯度，在推理时采用基于优化的采样过程。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和流模型依赖于非平衡的时间条件动态，这限制了采样过程的灵活性和效率。EqM旨在通过平衡动力学视角构建更简单、更灵活的生成模型框架。

Method: EqM学习隐式能量景观的平衡梯度，在推理时使用可调节步长、自适应优化器和自适应计算的梯度下降进行采样，取代传统的时间条件速度函数。

Result: 在ImageNet 256×256上达到FID 1.90，超越了传统扩散/流模型的生成性能，并能自然处理部分噪声图像去噪、OOD检测和图像合成等任务。

Conclusion: EqM通过统一的平衡景观取代时间条件速度，在流模型和能量基模型之间建立了更紧密的桥梁，为优化驱动的推理提供了简单途径。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [103] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是一种基于ZK-SNARKs的水印系统，用于图像生成模型，通过选择性层电路转换和LSB隐写术实现安全、可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型的发展，合成媒体的真实性、所有权和滥用问题日益严重。传统水印方法存在质量下降、易移除或需要模型内部信息的问题，无法满足安全可扩展部署的需求。

Method: 提出ZK-WAGON系统，使用ZK-SNARKs技术实现水印。采用选择性层ZK电路创建（SL-ZKCC）方法将关键层转换为电路，显著减少证明生成时间。通过LSB隐写术将ZK-SNARK证明嵌入生成图像中。

Result: 该系统在GAN和扩散模型上进行了验证，提供了一个安全、模型无关的可信AI图像生成流程。

Conclusion: ZK-WAGON为图像生成模型提供了一种安全、可验证的水印解决方案，能够在不暴露模型权重或敏感信息的情况下证明图像来源。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [104] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 本文提出了一种基于扩散模型的光子晶体能带图生成方法，通过结合Transformer编码器和潜在扩散模型来高效生成光子能带图，解决传统计算方法的高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 光子晶体在光子技术和量子技术中发挥重要作用，但计算光子能带图需要求解大量麦克斯韦方程，计算成本高昂，特别是在逆向设计优化循环中。现有方法难以高效处理复杂的三维结构。

Method: 该方法将Transformer编码器与潜在扩散模型相结合：Transformer编码器从输入结构中提取上下文嵌入，潜在扩散模型基于这些嵌入生成对应的光子能带图。

Result: 该方法能够生成光子能带图，并展示了Transformer和扩散模型在捕捉光子学中复杂干涉和散射现象方面的适用性。

Conclusion: 这项工作为光子学领域开辟了新的替代建模策略，有望扩展到任意三维结构，为光子晶体设计提供高效的计算工具。

Abstract: Photonic crystals enable fine control over light propagation at the
nanoscale, and thus play a central role in the development of photonic and
quantum technologies. Photonic band diagrams (BDs) are a key tool to
investigate light propagation into such inhomogeneous structured materials.
However, computing BDs requires solving Maxwell's equations across many
configurations, making it numerically expensive, especially when embedded in
optimization loops for inverse design techniques, for example. To address this
challenge, we introduce the first approach for BD generation based on diffusion
models, with the capacity to later generalize and scale to arbitrary three
dimensional structures. Our method couples a transformer encoder, which
extracts contextual embeddings from the input structure, with a latent
diffusion model to generate the corresponding BD. In addition, we provide
insights into why transformers and diffusion models are well suited to capture
the complex interference and scattering phenomena inherent to photonics, paving
the way for new surrogate modeling strategies in this domain.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [105] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 该论文研究发现预训练视频模型在社交感知方面存在差距，语言模型比视频模型更符合人类相似性判断。通过人类行为数据微调视频模型可以显著提高与人类社交感知的对齐度。


<details>
  <summary>Details</summary>
Motivation: 研究现代视频和语言模型是否能捕捉人类在社交视频中感知的相似性结构，以及如何利用人类行为数据将这种结构注入到模型中。

Method: 引入包含49,000多个三元组相似性判断的新基准数据集，使用混合三元组-RSA目标通过LoRA微调TimeSformer视频模型，使其成对距离与人类相似性对齐。

Result: 微调后的视频模型在保留视频上显著提高了与人类感知的对齐度，增强了社交情感属性（亲密感、效价、支配性、沟通）的编码能力。

Conclusion: 预训练视频模型在社交识别方面存在差距，行为引导的微调可以将视频表征塑造成更符合人类社交感知的形式。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it
remains unclear whether state-of-the-art AI models encode the same similarity
structure. We study (Q1) whether modern video and language models capture
human-perceived similarity in social videos, and (Q2) how to instill this
structure into models using human behavioral data. To address this, we
introduce a new benchmark of over 49,000 odd-one-out similarity judgments on
250 three-second video clips of social interactions, and discover a modality
gap: despite the task being visual, caption-based language embeddings align
better with human similarity than any pretrained video model. We close this gap
by fine-tuning a TimeSformer video model on these human judgments with our
novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning
pairwise distances to human similarity. This fine-tuning protocol yields
significantly improved alignment with human perceptions on held-out videos in
terms of both explained variance and odd-one-out triplet accuracy. Variance
partitioning shows that the fine-tuned video model increases shared variance
with language embeddings and explains additional unique variance not captured
by the language model. Finally, we test transfer via linear probes and find
that human-similarity fine-tuning strengthens the encoding of social-affective
attributes (intimacy, valence, dominance, communication) relative to the
pretrained baseline. Overall, our findings highlight a gap in pretrained video
models' social recognition and demonstrate that behavior-guided fine-tuning
shapes video representations toward human social perception.

</details>


### [106] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出了MIG-Vis方法，利用扩散模型可视化并验证神经潜在子空间中编码的视觉语义属性，发现高级视觉皮层中存在结构化的语义表征


<details>
  <summary>Details</summary>
Motivation: 解决高级视觉区域中神经群体如何编码物体中心视觉信息的科学问题，特别是特征特异性视觉信息在神经群体中的分布和组织方式

Method: 使用变分自编码器推断神经群体的组间解缠神经潜在子空间，然后提出互信息引导的扩散合成过程来可视化每个潜在组编码的特定视觉语义特征

Result: 在两只猕猴的下颞叶皮层多会话神经放电数据集上验证，发现神经潜在组对多样视觉特征具有清晰的语义选择性，包括物体姿态、类别间变换和类内内容

Conclusion: 该方法为高级视觉皮层中结构化语义表征提供了直接、可解释的证据，推进了对编码原理的理解

Abstract: Understanding how neural populations in higher visual areas encode
object-centered visual information remains a central challenge in computational
neuroscience. Prior works have investigated representational alignment between
artificial neural networks and the visual cortex. Nevertheless, these findings
are indirect and offer limited insights to the structure of neural populations
themselves. Similarly, decoding-based methods have quantified semantic features
from neural populations but have not uncovered their underlying organizations.
This leaves open a scientific question: "how feature-specific visual
information is distributed across neural populations in higher visual areas,
and whether it is organized into structured, semantically meaningful
subspaces." To tackle this problem, we present MIG-Vis, a method that leverages
the generative power of diffusion models to visualize and validate the
visual-semantic attributes encoded in neural latent subspaces. Our method first
uses a variational autoencoder to infer a group-wise disentangled neural latent
subspace from neural populations. Subsequently, we propose a mutual information
(MI)-guided diffusion synthesis procedure to visualize the specific
visual-semantic features encoded by each latent group. We validate MIG-Vis on
multi-session neural spiking datasets from the inferior temporal (IT) cortex of
two macaques. The synthesized results demonstrate that our method identifies
neural latent groups with clear semantic selectivity to diverse visual
features, including object pose, inter-category transformations, and
intra-class content. These findings provide direct, interpretable evidence of
structured semantic representation in the higher visual cortex and advance our
understanding of its encoding principles.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [107] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一个视觉语言导航系统，通过微调互联网预训练的图像扩散模型进行路径规划，生成视觉路径掩码来指导机器人导航，在真实世界评估中显著优于现有基础模型。


<details>
  <summary>Details</summary>
Motivation: 解决机器人需要适应多样化人类指令并在非结构化开放世界中安全操作的问题，同时克服现有视觉语言模型在导航任务中由于动作空间差异和预训练目标不同导致的迁移困难。

Method: 引入VENTURA系统，微调图像扩散模型生成路径掩码（视觉规划），使用轻量级行为克隆策略将视觉规划转化为可执行轨迹，通过自监督跟踪模型和VLM增强的标注进行规模化训练。

Result: 在真实世界评估中，VENTURA在物体到达、障碍物避让和地形偏好任务上优于最先进的基础模型基线，成功率提高33%，碰撞减少54%，并能泛化到未见任务组合。

Conclusion: VENTURA展示了通过视觉规划方法实现机器人导航的有效性，具有涌现的组合能力，为机器人适应开放世界环境提供了有前景的解决方案。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [108] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个用于机器人双手机器人操作的数据收集框架，通过VR遥操作和主动感知技术将人类演示转化为机器人策略


<details>
  <summary>Details</summary>
Motivation: 解决复杂双手机器人操作的数据收集问题，通过便携式系统实现真实世界演示的高质量采集

Method: 结合便携式VR遥操作套件和传感器控制器，通过精确姿态对齐桥接人机运动学，引入沉浸式3D模型渲染、自包含可穿戴计算机和高效校准方法

Result: 在6个挑战性双手机器人任务上，仅使用ActiveUMI数据训练的策略在分布内任务上达到70%平均成功率，在新对象和新环境中保持56%的成功率

Conclusion: 便携式数据收集系统结合学习主动感知技术，为创建可泛化、高能力的真实世界机器人策略提供了有效且可扩展的途径

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [109] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout是一个用于3D室内布局合成的新框架，通过解耦物理和语义细化来解决传统方法泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法由于固定数据集而泛化性差，而基于LLM和VLM的方法虽然语义丰富但缺乏灵活细化能力，导致布局不理想。

Method: 开发了语义细化工具(SRT)修正抽象对象关系，物理细化工具(PRT)通过网格匹配算法解决空间问题，并采用多智能体框架协调这些工具。

Result: 实验表明DisCo-Layout在生成真实、连贯且可泛化的3D室内布局方面达到最先进性能。

Conclusion: DisCo-Layout通过解耦和协调物理与语义细化，有效解决了3D室内布局合成的关键挑战。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [110] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 论文研究通过显式地将策略与相机外参关联来实现视角不变模仿学习，发现这种方法能显著提升行为克隆策略在不同视角下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决传统模仿学习策略在视角变化时性能下降的问题，特别是当策略依赖静态背景中的视觉线索来推断相机姿态时，这种捷径方法在相机位置或工作空间几何变化时会失效。

Method: 使用Plucker嵌入的像素光线表示，将策略显式地条件化在相机外参上，并在RoboSuite和ManiSkill中设计了六个操作任务来评估策略在视角变化下的鲁棒性。

Result: 实验表明，未使用外参条件的策略在固定场景中依赖背景线索推断相机姿态，但在视角变化时性能崩溃；而使用外参条件的策略能恢复性能，实现仅使用RGB图像的鲁棒控制。

Conclusion: 显式地将策略条件化在相机外参上能有效提升视角不变模仿学习的鲁棒性，避免对静态背景线索的依赖，为RGB-only控制提供了可靠解决方案。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [111] [MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging](https://arxiv.org/abs/2510.01298)
*Berker Demirel,Marco Fumero,Theofanis Karaletsos,Francesco Locatello*

Main category: q-bio.QM

TL;DR: MorphGen是一个基于扩散模型的荧光显微镜图像生成模型，能够跨多种细胞类型和扰动进行可控生成，在药物发现和基因编辑领域具有应用价值。


<details>
  <summary>Details</summary>
Motivation: 加速基于高内涵图像的药物筛选和基因编辑分析，通过计算机模拟细胞对干预的反应来替代部分实验。

Method: 使用扩散生成模型，通过对齐损失训练使其表示与OpenPhenom的表型嵌入匹配，同时联合生成完整的荧光通道而非压缩为RGB图像。

Result: MorphGen的FID分数比之前的SOTA模型MorphoDiff低35%以上，通过CellProfiler验证了生物一致性，并保留了细胞器级别的细节。

Conclusion: MorphGen在生成生物一致的荧光显微镜图像方面优于现有方法，能够提供更精细的形态学分析能力。

Abstract: Simulating in silico cellular responses to interventions is a promising
direction to accelerate high-content image-based assays, critical for advancing
drug discovery and gene editing. To support this, we introduce MorphGen, a
state-of-the-art diffusion-based generative model for fluorescent microscopy
that enables controllable generation across multiple cell types and
perturbations. To capture biologically meaningful patterns consistent with
known cellular morphologies, MorphGen is trained with an alignment loss to
match its representations to the phenotypic embeddings of OpenPhenom, a
state-of-the-art biological foundation model. Unlike prior approaches that
compress multichannel stains into RGB images -- thus sacrificing
organelle-specific detail -- MorphGen generates the complete set of fluorescent
channels jointly, preserving per-organelle structures and enabling a
fine-grained morphological analysis that is essential for biological
interpretation. We demonstrate biological consistency with real images via
CellProfiler features, and MorphGen attains an FID score over $35\%$ lower than
the prior state-of-the-art MorphoDiff, which only generates RGB images for a
single cell type. Code is available at https://github.com/czi-ai/MorphGen.

</details>


### [112] [A Multicentric Dataset for Training and Benchmarking Breast Cancer Segmentation in H&E Slides](https://arxiv.org/abs/2510.02037)
*Carlijn Lems,Leslie Tessier,John-Melle Bokhorst,Mart van Rijthoven,Witali Aswolinskiy,Matteo Pozzi,Natalie Klubickova,Suzanne Dintzis,Michela Campora,Maschenka Balkenhol,Peter Bult,Joey Spronck,Thomas Detone,Mattia Barbareschi,Enrico Munari,Giuseppe Bogina,Jelle Wesseling,Esther H. Lips,Francesco Ciompi,Frédérique Meeuwsen,Jeroen van der Laak*

Main category: q-bio.QM

TL;DR: BEETLE数据集：用于乳腺癌H&E染色全切片图像多类语义分割的新数据集，包含587个活检和切除样本，涵盖所有分子亚型和组织学分级，特别关注现有数据集中代表性不足的形态学特征。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌分割数据集缺乏形态多样性，无法支持模型泛化性和跨异质患者队列的稳健生物标志物验证。

Method: 通过多样化标注策略收集来自三个临床中心和两个公共数据集的587个样本，使用七种扫描仪数字化，标注四个类别：浸润性上皮、非浸润性上皮、坏死和其他。

Result: 创建了BEETLE数据集，特别关注导管原位癌和分散性小叶肿瘤细胞等代表性不足的形态学特征，提供了标准化基准测试集。

Conclusion: 该数据集的多样性和与自动化生物标志物量化领域的相关性确保了其高重用潜力，为乳腺癌分割模型提供了标准化基准测试平台。

Abstract: Automated semantic segmentation of whole-slide images (WSIs) stained with
hematoxylin and eosin (H&E) is essential for large-scale artificial
intelligence-based biomarker analysis in breast cancer. However, existing
public datasets for breast cancer segmentation lack the morphological diversity
needed to support model generalizability and robust biomarker validation across
heterogeneous patient cohorts. We introduce BrEast cancEr hisTopathoLogy
sEgmentation (BEETLE), a dataset for multiclass semantic segmentation of
H&E-stained breast cancer WSIs. It consists of 587 biopsies and resections from
three collaborating clinical centers and two public datasets, digitized using
seven scanners, and covers all molecular subtypes and histological grades.
Using diverse annotation strategies, we collected annotations across four
classes - invasive epithelium, non-invasive epithelium, necrosis, and other -
with particular focus on morphologies underrepresented in existing datasets,
such as ductal carcinoma in situ and dispersed lobular tumor cells. The
dataset's diversity and relevance to the rapidly growing field of automated
biomarker quantification in breast cancer ensure its high potential for reuse.
Finally, we provide a well-curated, multicentric external evaluation set to
enable standardized benchmarking of breast cancer segmentation models.

</details>
