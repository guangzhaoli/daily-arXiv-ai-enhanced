<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 181]
- [cs.GR](#cs.GR) [Total: 2]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.CR](#cs.CR) [Total: 3]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.RO](#cs.RO) [Total: 5]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.CL](#cs.CL) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [ESCA: Contextualizing Embodied Agents via Scene-Graph Generation](https://arxiv.org/abs/2510.15963)
*Jiani Huang,Amish Sethi,Matthew Kuo,Mayank Keoliya,Neelay Velingker,JungHo Jung,Ser-Nam Lim,Ziyang Li,Mayur Naik*

Main category: cs.CV

TL;DR: ESCA框架通过SGClip模型实现多模态大语言模型在具身智能中的结构化时空理解，无需人工标注场景图，显著提升感知精度和任务性能


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型训练主要依赖高层视觉-声音-文本对，缺乏像素级视觉内容与文本语义的细粒度结构化对齐

Method: 提出ESCA框架，核心是SGClip模型——基于CLIP的可提示场景图生成模型，通过神经符号学习管道在8.7万+开放域视频上训练，利用视频-字幕对的模型驱动自监督和结构化推理

Result: SGClip在场景图生成和动作定位基准测试中表现优异，ESCA框架持续改进开源和商业MLLMs，在两个具身环境中实现最先进性能，显著减少感知错误并使开源模型超越专有基线

Conclusion: ESCA框架通过结构化时空理解有效提升了具身智能体的性能，证明了无需人工标注的场景图生成在增强多模态大语言模型感知能力方面的价值

Abstract: Multi-modal large language models (MLLMs) are making rapid progress toward
general-purpose embodied agents. However, current training pipelines primarily
rely on high-level vision-sound-text pairs and lack fine-grained, structured
alignment between pixel-level visual content and textual semantics. To overcome
this challenge, we propose ESCA, a new framework for contextualizing embodied
agents through structured spatial-temporal understanding. At its core is
SGClip, a novel CLIP-based, open-domain, and promptable model for generating
scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic
learning pipeline, which harnesses model-driven self-supervision from
video-caption pairs and structured reasoning, thereby eliminating the need for
human-labeled scene graph annotations. We demonstrate that SGClip supports both
prompt-based inference and task-specific fine-tuning, excelling in scene graph
generation and action localization benchmarks. ESCA with SGClip consistently
improves both open-source and commercial MLLMs, achieving state-of-the-art
performance across two embodied environments. Notably, it significantly reduces
agent perception errors and enables open-source models to surpass proprietary
baselines.

</details>


### [2] [CrossRay3D: Geometry and Distribution Guidance for Efficient Multimodal 3D Detection](https://arxiv.org/abs/2510.15991)
*Huiming Yang*

Main category: cs.CV

TL;DR: 提出了CrossRay3D稀疏跨模态检测器，通过Ray-Aware Supervision和Class-Balanced Supervision提升token表示质量，在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，且运行速度快1.84倍。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏检测器忽视token表示质量，导致前景质量次优和性能受限。本文发现几何结构保持和类别分布是提升稀疏检测器性能的关键。

Method: 提出Sparse Selector (SS)模块，包含Ray-Aware Supervision (RAS)保持几何信息，Class-Balanced Supervision自适应重加权类别语义，以及Ray Positional Encoding解决LiDAR和图像模态分布差异。

Result: 在nuScenes基准上达到72.4 mAP和74.7 NDS的SOTA性能，运行速度快1.84倍，且在LiDAR或相机数据部分或全部缺失时仍保持强鲁棒性。

Conclusion: CrossRay3D通过提升token表示质量，实现了高性能、高效率的稀疏跨模态3D检测，在复杂场景下表现出色。

Abstract: The sparse cross-modality detector offers more advantages than its
counterpart, the Bird's-Eye-View (BEV) detector, particularly in terms of
adaptability for downstream tasks and computational cost savings. However,
existing sparse detectors overlook the quality of token representation, leaving
it with a sub-optimal foreground quality and limited performance. In this
paper, we identify that the geometric structure preserved and the class
distribution are the key to improving the performance of the sparse detector,
and propose a Sparse Selector (SS). The core module of SS is Ray-Aware
Supervision (RAS), which preserves rich geometric information during the
training stage, and Class-Balanced Supervision, which adaptively reweights the
salience of class semantics, ensuring that tokens associated with small objects
are retained during token sampling. Thereby, outperforming other sparse
multi-modal detectors in the representation of tokens. Additionally, we design
Ray Positional Encoding (Ray PE) to address the distribution differences
between the LiDAR modality and the image. Finally, we integrate the
aforementioned module into an end-to-end sparse multi-modality detector, dubbed
CrossRay3D. Experiments show that, on the challenging nuScenes benchmark,
CrossRay3D achieves state-of-the-art performance with 72.4 mAP and 74.7 NDS,
while running 1.84 faster than other leading methods. Moreover, CrossRay3D
demonstrates strong robustness even in scenarios where LiDAR or camera data are
partially or entirely missing.

</details>


### [3] [InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects](https://arxiv.org/abs/2510.16017)
*Ibrahim Sheikh Mohamed,Abdullah Yahya Abdullah Omaisan*

Main category: cs.CV

TL;DR: 该论文提出了一种利用街头CCTV视频流进行多缺陷检测和分割的完整流程，结合YOLO目标检测器和视觉语言模型生成结构化维修计划


<details>
  <summary>Details</summary>
Motivation: 智慧城市基础设施监控需要自动化检测系统，人工检查成本高且危险，现有系统通常只能处理单一缺陷类型或输出非结构化结果，无法直接指导维修工作

Method: 使用YOLO系列目标检测器进行多缺陷检测和分割，然后将检测结果传递给视觉语言模型进行场景感知总结，生成包含事件描述、推荐工具、尺寸、维修计划和紧急警报的JSON格式结构化行动方案

Result: 在公共数据集和捕获的CCTV片段上的实验评估表明，系统能够准确识别多种缺陷并生成连贯的总结

Conclusion: 讨论了将系统扩展到城市范围部署的挑战和方向

Abstract: Infrastructure in smart cities is increasingly monitored by networks of
closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop
cracks, potholes, and fluid leaks that threaten public safety and require
timely repair. Manual inspection is costly and hazardous, and existing
automatic systems typically address individual defect types or provide
unstructured outputs that cannot directly guide maintenance crews. This paper
proposes a comprehensive pipeline that leverages street CCTV streams for multi
defect detection and segmentation using the YOLO family of object detectors and
passes the detections to a vision language model (VLM) for scene aware
summarization. The VLM generates a structured action plan in JSON format that
includes incident descriptions, recommended tools, dimensions, repair plans,
and urgent alerts. We review literature on pothole, crack and leak detection,
highlight recent advances in large vision language models such as QwenVL and
LLaVA, and describe the design of our early prototype. Experimental evaluation
on public datasets and captured CCTV clips demonstrates that the system
accurately identifies diverse defects and produces coherent summaries. We
conclude by discussing challenges and directions for scaling the system to city
wide deployments.

</details>


### [4] [IAD-GPT: Advancing Visual Knowledge in Multimodal Large Language Model for Industrial Anomaly Detection](https://arxiv.org/abs/2510.16036)
*Zewen Li,Zitong Yu,Qilang Ye,Weicheng Xie,Wei Zhuo,Linlin Shen*

Main category: cs.CV

TL;DR: IAD-GPT是一个基于多模态大语言模型的工业异常检测新范式，通过异常提示生成器、文本引导增强器和多掩码融合模块，实现了自监督和少样本的异常检测与分割，在MVTec-AD和VisA数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统工业异常检测方法缺乏多轮人机对话和详细描述能力，而基于大模型的方法尚未充分激发在异常检测任务中的潜力。本文旨在结合丰富的文本语义与图像信息，开发基于MLLM的工业异常检测新方法。

Method: 提出IAD-GPT框架：1）使用异常提示生成器生成详细异常提示；2）文本引导增强器通过图像特征与正常/异常文本提示的交互动态选择增强路径；3）多掩码融合模块将掩码作为专家知识增强LLM对像素级异常的感知。

Result: 在MVTec-AD和VisA数据集上的广泛实验表明，该方法在自监督和少样本异常检测与分割任务上达到了最先进的性能。

Conclusion: IAD-GPT成功地将多模态大语言模型应用于工业异常检测，通过文本语义与视觉信息的有效结合，显著提升了异常检测的准确性和解释能力。

Abstract: The robust causal capability of Multimodal Large Language Models (MLLMs) hold
the potential of detecting defective objects in Industrial Anomaly Detection
(IAD). However, most traditional IAD methods lack the ability to provide
multi-turn human-machine dialogues and detailed descriptions, such as the color
of objects, the shape of an anomaly, or specific types of anomalies. At the
same time, methods based on large pre-trained models have not fully stimulated
the ability of large models in anomaly detection tasks. In this paper, we
explore the combination of rich text semantics with both image-level and
pixel-level information from images and propose IAD-GPT, a novel paradigm based
on MLLMs for IAD. We employ Abnormal Prompt Generator (APG) to generate
detailed anomaly prompts for specific objects. These specific prompts from the
large language model (LLM) are used to activate the detection and segmentation
functions of the pre-trained visual-language model (i.e., CLIP). To enhance the
visual grounding ability of MLLMs, we propose Text-Guided Enhancer, wherein
image features interact with normal and abnormal text prompts to dynamically
select enhancement pathways, which enables language models to focus on specific
aspects of visual data, enhancing their ability to accurately interpret and
respond to anomalies within images. Moreover, we design a Multi-Mask Fusion
module to incorporate mask as expert knowledge, which enhances the LLM's
perception of pixel-level anomalies. Extensive experiments on MVTec-AD and VisA
datasets demonstrate our state-of-the-art performance on self-supervised and
few-shot anomaly detection and segmentation tasks, such as MVTec-AD and VisA
datasets. The codes are available at
\href{https://github.com/LiZeWen1225/IAD-GPT}{https://github.com/LiZeWen1225/IAD-GPT}.

</details>


### [5] [Effect of Reporting Mode and Clinical Experience on Radiologists' Gaze and Image Analysis Behavior in Chest Radiography](https://arxiv.org/abs/2510.16070)
*Mahta Khoobi,Marc Sebastian von der Stueck,Felix Barajas Ordonez,Anca-Maria Iancu,Eric Corban,Julia Nowak,Aleksandar Kargaliev,Valeria Perelygina,Anna-Sophie Schott,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung,Robert Siepmann*

Main category: cs.CV

TL;DR: 本研究评估了三种放射学报告模式（自由文本、结构化报告、AI辅助结构化报告）对图像分析行为、诊断准确性、效率和使用体验的影响。结果显示AI辅助结构化报告在诊断准确性和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 探索结构化报告和人工智能如何改变放射科医生与影像研究的交互方式，比较不同报告模式对放射学工作流程的影响。

Method: 前瞻性研究（2024年7-12月），8名读者（4名新手和4名非新手）使用定制化查看器和眼动追踪系统分析35张床边胸片。比较三种报告模式的诊断准确性、报告时间、眼动指标和用户体验问卷。

Result: AI辅助结构化报告的诊断准确性最高（κ=0.71），报告时间最短（25±9秒）。结构化报告和AI辅助结构化报告显著降低了放射片区域的扫视次数和报告区域的注视时间。AI辅助结构化报告是最受欢迎的模式。

Conclusion: 结构化报告通过引导视觉注意力向图像方向提高效率，而AI预填充的结构化报告进一步提升了诊断准确性和用户满意度。

Abstract: Structured reporting (SR) and artificial intelligence (AI) may transform how
radiologists interact with imaging studies. This prospective study (July to
December 2024) evaluated the impact of three reporting modes: free-text (FT),
structured reporting (SR), and AI-assisted structured reporting (AI-SR), on
image analysis behavior, diagnostic accuracy, efficiency, and user experience.
Four novice and four non-novice readers (radiologists and medical students)
each analyzed 35 bedside chest radiographs per session using a customized
viewer and an eye-tracking system. Outcomes included diagnostic accuracy
(compared with expert consensus using Cohen's $\kappa$), reporting time per
radiograph, eye-tracking metrics, and questionnaire-based user experience.
Statistical analysis used generalized linear mixed models with Bonferroni
post-hoc tests with a significance level of ($P \le .01$). Diagnostic accuracy
was similar in FT ($\kappa = 0.58$) and SR ($\kappa = 0.60$) but higher in
AI-SR ($\kappa = 0.71$, $P < .001$). Reporting times decreased from $88 \pm 38$
s (FT) to $37 \pm 18$ s (SR) and $25 \pm 9$ s (AI-SR) ($P < .001$). Saccade
counts for the radiograph field ($205 \pm 135$ (FT), $123 \pm 88$ (SR), $97 \pm
58$ (AI-SR)) and total fixation duration for the report field ($11 \pm 5$ s
(FT), $5 \pm 3$ s (SR), $4 \pm 1$ s (AI-SR)) were lower with SR and AI-SR ($P <
.001$ each). Novice readers shifted gaze towards the radiograph in SR, while
non-novice readers maintained their focus on the radiograph. AI-SR was the
preferred mode. In conclusion, SR improves efficiency by guiding visual
attention toward the image, and AI-prefilled SR further enhances diagnostic
accuracy and user satisfaction.

</details>


### [6] [Data-Driven Analysis of Intersectional Bias in Image Classification: A Framework with Bias-Weighted Augmentation](https://arxiv.org/abs/2510.16072)
*Farjana Yesmin*

Main category: cs.CV

TL;DR: 提出一个数据驱动的框架来分析和缓解图像分类中的交叉偏见，通过引入交叉公平性评估框架和偏见加权增强策略，显著提高代表性不足类别的准确性并减少公平性指标差异。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在训练数据不平衡时会出现交叉偏见，即由多个属性（如对象类别和环境条件）相互作用产生的系统性错误。

Method: 1. 引入交叉公平性评估框架（IFEF），结合定量公平性指标和可解释性工具识别偏见模式；2. 提出偏见加权增强（BWA）策略，基于子组分布统计自适应调整数据增强强度。

Result: 在Open Images V7数据集上的实验显示，BWA将代表性不足类别的准确性提高了最多24个百分点，同时将公平性指标差异减少了35%。统计分析证实改进的显著性（p < 0.05）。

Conclusion: 该方法为分析和解决图像分类系统中的交叉偏见提供了一个可复现的途径。

Abstract: Machine learning models trained on imbalanced datasets often exhibit
intersectional biases-systematic errors arising from the interaction of
multiple attributes such as object class and environmental conditions. This
paper presents a data-driven framework for analyzing and mitigating such biases
in image classification. We introduce the Intersectional Fairness Evaluation
Framework (IFEF), which combines quantitative fairness metrics with
interpretability tools to systematically identify bias patterns in model
predictions. Building on this analysis, we propose Bias-Weighted Augmentation
(BWA), a novel data augmentation strategy that adapts transformation
intensities based on subgroup distribution statistics. Experiments on the Open
Images V7 dataset with five object classes demonstrate that BWA improves
accuracy for underrepresented class-environment intersections by up to 24
percentage points while reducing fairness metric disparities by 35%.
Statistical analysis across multiple independent runs confirms the significance
of improvements (p < 0.05). Our methodology provides a replicable approach for
analyzing and addressing intersectional biases in image classification systems.

</details>


### [7] [Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch](https://arxiv.org/abs/2510.16088)
*Zia Badar*

Main category: cs.CV

TL;DR: 提出一种可微分的量化方法，支持多比特量化，在ImageNet数据集上使用ResNet18仅需15个训练周期即可达到接近全精度模型的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有量化方法中存在的两个问题：1）大多数方法使用不可微分的量化函数，在反向传播中需要手动设置导数；2）现有的移位/对数量化方法要么避免激活量化，要么准确率较低。

Method: 提出一种可微分的量化方法，支持n比特量化，特别适用于对数量化形式2^n。该方法提供了收敛性证明，确保能收敛到最优神经网络。

Result: 在ImageNet图像分类任务中，仅权重量化时准确率比全精度模型低不到1%；权重和激活同时量化时达到与SOTA方法相当的准确率，仅需15个训练周期。推理成本略高于1比特量化，但不需要高精度乘法。

Conclusion: 该方法提供了一种可微分的量化方案，支持多比特量化，在保持高准确率的同时显著减少训练周期，为神经网络量化提供了有效的解决方案。

Abstract: Quantization of neural networks provides benefits of inference in less
compute and memory requirements. Previous work in quantization lack two
important aspects which this work provides. First almost all previous work in
quantization used a non-differentiable approach and for learning; the
derivative is usually set manually in backpropogation which make the learning
ability of algorithm questionable, our approach is not just differentiable, we
also provide proof of convergence of our approach to the optimal neural
network. Second previous work in shift/logrithmic quantization either have
avoided activation quantization along with weight quantization or achieved less
accuracy. Learning logrithmic quantize values of form $2^n$ requires the
quantization function can scale to more than 1 bit quantization which is
another benifit of our quantization that it provides $n$ bits quantization as
well. Our approach when tested with image classification task using imagenet
dataset, resnet18 and weight quantization only achieves less than 1 percent
accuracy compared to full precision accuracy while taking only 15 epochs to
train using shift bit quantization and achieves comparable to SOTA approaches
accuracy in both weight and activation quantization using shift bit
quantization in 15 training epochs with slightly higher(only higher cpu
instructions) inference cost compared to 1 bit quantization(without logrithmic
quantization) and not requiring any higher precision multiplication.

</details>


### [8] [StripRFNet: A Strip Receptive Field and Shape-Aware Network for Road Damage Detection](https://arxiv.org/abs/2510.16115)
*Jianhan Lin,Yuchu Qin,Shuai Gao,Yikang Rui,Jie Liu,Yanjie Lv*

Main category: cs.CV

TL;DR: 提出StripRFNet网络，通过三个模块解决道路损伤检测中的形状多样性、细长裂缝捕捉和小尺度损伤识别难题，在RDD2022基准测试中达到最优性能


<details>
  <summary>Details</summary>
Motivation: 道路表面损伤威胁交通安全并阻碍可持续城市发展，但现有检测方法面临损伤形状多样、细长裂缝难以捕捉、小尺度损伤识别错误率高等挑战

Method: StripRFNet包含三个核心模块：形状感知模块（SPM）通过大尺度可分离核注意力增强形状判别；条带感受野模块（SRFM）使用大条带卷积和池化捕捉细长裂缝特征；小尺度增强模块（SSEM）利用高分辨率P2特征图和动态上采样提升小目标检测

Result: 在RDD2022基准测试中，中国子集的F1-score、mAP50和mAP50:95分别比基线提升4.4、2.9和3.4个百分点；完整数据集上达到80.33%的最高F1-score，优于CRDDC'2022参与者和ORDDC'2024 Phase 2结果，同时保持有竞争力的推理速度

Conclusion: StripRFNet在道路损伤检测中实现了最先进的准确性和实时效率，为智能道路维护和可持续基础设施管理提供了有前景的工具

Abstract: Well-maintained road networks are crucial for achieving Sustainable
Development Goal (SDG) 11. Road surface damage not only threatens traffic
safety but also hinders sustainable urban development. Accurate detection,
however, remains challenging due to the diverse shapes of damages, the
difficulty of capturing slender cracks with high aspect ratios, and the high
error rates in small-scale damage recognition. To address these issues, we
propose StripRFNet, a novel deep neural network comprising three modules: (1) a
Shape Perception Module (SPM) that enhances shape discrimination via large
separable kernel attention (LSKA) in multi-scale feature aggregation; (2) a
Strip Receptive Field Module (SRFM) that employs large strip convolutions and
pooling to capture features of slender cracks; and (3) a Small-Scale
Enhancement Module (SSEM) that leverages a high-resolution P2 feature map, a
dedicated detection head, and dynamic upsampling to improve small-object
detection. Experiments on the RDD2022 benchmark show that StripRFNet surpasses
existing methods. On the Chinese subset, it improves F1-score, mAP50, and
mAP50:95 by 4.4, 2.9, and 3.4 percentage points over the baseline,
respectively. On the full dataset, it achieves the highest F1-score of 80.33%
compared with CRDDC'2022 participants and ORDDC'2024 Phase 2 results, while
maintaining competitive inference speed. These results demonstrate that
StripRFNet achieves state-of-the-art accuracy and real-time efficiency,
offering a promising tool for intelligent road maintenance and sustainable
infrastructure management.

</details>


### [9] [ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles](https://arxiv.org/abs/2510.16118)
*Nishad Sahu,Shounak Sural,Aditya Satish Patil,Ragunathan,Rajkumar*

Main category: cs.CV

TL;DR: ObjectTransforms是一种通过在训练和推理时对物体进行特定变换来量化和减少基于视觉的目标检测中不确定性的技术。


<details>
  <summary>Details</summary>
Motivation: 基于视觉的目标检测神经网络在面对数据偏差和分布偏移等问题时存在不确定性，这对自动驾驶等安全关键决策构成威胁。

Method: 在训练时，ObjectTransforms对单个物体进行颜色空间扰动以提高对光照和颜色变化的鲁棒性，并使用扩散模型生成多样化的行人实例。在推理时，对检测到的物体应用扰动，利用检测得分的方差实时量化预测不确定性。

Result: 在NuImages 10K数据集上使用YOLOv8的实验表明，该方法在所有物体类别上均实现了显著的准确率提升和不确定性减少，并且在推理时对假阳性的不确定性预测值高于真阳性。

Conclusion: ObjectTransforms作为一种轻量级但有效的机制，在训练和推理时分别具有减少和量化基于视觉的感知中不确定性的潜力。

Abstract: Reliable perception is fundamental for safety critical decision making in
autonomous driving. Yet, vision based object detector neural networks remain
vulnerable to uncertainty arising from issues such as data bias and
distributional shifts. In this paper, we introduce ObjectTransforms, a
technique for quantifying and reducing uncertainty in vision based object
detection through object specific transformations at both training and
inference times. At training time, ObjectTransforms perform color space
perturbations on individual objects, improving robustness to lighting and color
variations. ObjectTransforms also uses diffusion models to generate realistic,
diverse pedestrian instances. At inference time, object perturbations are
applied to detected objects and the variance of detection scores are used to
quantify predictive uncertainty in real time. This uncertainty signal is then
used to filter out false positives and also recover false negatives, improving
the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K
dataset demonstrate that our method yields notable accuracy improvements and
uncertainty reduction across all object classes during training, while
predicting desirably higher uncertainty values for false positives as compared
to true positives during inference. Our results highlight the potential of
ObjectTransforms as a lightweight yet effective mechanism for reducing and
quantifying uncertainty in vision-based perception during training and
inference respectively.

</details>


### [10] [Aria Gen 2 Pilot Dataset](https://arxiv.org/abs/2510.16134)
*Chen Kong,James Fort,Aria Kang,Jonathan Wittmer,Simon Green,Tianwei Shen,Yipu Zhao,Cheng Peng,Gustavo Solaira,Andrew Berkovich,Nikhil Raina,Vijay Baiyya,Evgeniy Oleinik,Eric Huang,Fan Zhang,Julian Straub,Mark Schwesinger,Luis Pesqueira,Xiaqing Pan,Jakob Julian Engel,Carl Ren,Mingfei Yan,Richard Newcombe*

Main category: cs.CV

TL;DR: A2PD是一个使用Aria Gen 2眼镜采集的自我中心多模态开放数据集，包含日常活动场景的原始传感器数据和算法输出数据。


<details>
  <summary>Details</summary>
Motivation: 提供及时可访问的自我中心多模态数据集，展示设备在不同用户和条件下感知佩戴者、环境和交互的能力。

Method: 使用Aria Gen 2眼镜采集主要对象Dia'ane及其朋友的日常活动数据，涵盖清洁、烹饪、进食、玩耍和户外步行五个主要场景。

Result: 发布了包含原始传感器数据和多种机器感知算法输出的综合数据集，展示了设备在多样化条件下的稳健性能。

Conclusion: A2PD数据集通过projectaria.com公开提供，并配套开源工具和使用示例，支持相关研究发展。

Abstract: The Aria Gen 2 Pilot Dataset (A2PD) is an egocentric multimodal open dataset
captured using the state-of-the-art Aria Gen 2 glasses. To facilitate timely
access, A2PD is released incrementally with ongoing dataset enhancements. The
initial release features Dia'ane, our primary subject, who records her daily
activities alongside friends, each equipped with Aria Gen 2 glasses. It
encompasses five primary scenarios: cleaning, cooking, eating, playing, and
outdoor walking. In each of the scenarios, we provide comprehensive raw sensor
data and output data from various machine perception algorithms. These data
illustrate the device's ability to perceive the wearer, the surrounding
environment, and interactions between the wearer and the environment, while
maintaining robust performance across diverse users and conditions. The A2PD is
publicly available at projectaria.com, with open-source tools and usage
examples provided in Project Aria Tools.

</details>


### [11] [GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer](https://arxiv.org/abs/2510.16136)
*Sayan Deb Sarkar,Sinisa Stekovic,Vincent Lepetit,Iro Armeni*

Main category: cs.CV

TL;DR: 提出一种基于通用引导原理的无训练方法，通过周期性添加可微分损失函数引导，成功将外观（纹理和几何细节）从图像或文本转移到3D资产上，解决了现有方法在输入和外观对象几何差异较大时失败的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的方法在输入和外观对象的几何结构差异较大时效果不佳，而直接应用3D生成模型也无法产生令人满意的结果，因此需要一种更有效的外观迁移方法。

Method: 基于预训练的整流流模型（条件于图像或文本），在采样过程中周期性添加可微分损失函数作为引导，包括部件感知损失和自相似性损失。

Result: 该方法在定性和定量评估中均优于基线方法，成功将纹理和几何细节转移到3D资产上，并通过GPT-based系统和用户研究验证了评估效果。

Conclusion: 该方法具有通用性，可扩展到不同类型的扩散模型和引导函数，为3D外观迁移提供了有效的解决方案。

Abstract: Transferring appearance to 3D assets using different representations of the
appearance object - such as images or text - has garnered interest due to its
wide range of applications in industries like gaming, augmented reality, and
digital content creation. However, state-of-the-art methods still fail when the
geometry between the input and appearance objects is significantly different. A
straightforward approach is to directly apply a 3D generative model, but we
show that this ultimately fails to produce appealing results. Instead, we
propose a principled approach inspired by universal guidance. Given a
pretrained rectified flow model conditioned on image or text, our training-free
method interacts with the sampling process by periodically adding guidance.
This guidance can be modeled as a differentiable loss function, and we
experiment with two different types of guidance including part-aware losses for
appearance and self-similarity. Our experiments show that our approach
successfully transfers texture and geometric details to the input 3D asset,
outperforming baselines both qualitatively and quantitatively. We also show
that traditional metrics are not suitable for evaluating the task due to their
inability of focusing on local details and comparing dissimilar inputs, in
absence of ground truth data. We thus evaluate appearance transfer quality with
a GPT-based system objectively ranking outputs, ensuring robust and human-like
assessment, as further confirmed by our user study. Beyond showcased scenarios,
our method is general and could be extended to different types of diffusion
models and guidance functions.

</details>


### [12] [C-arm Guidance: A Self-supervised Approach To Automated Positioning During Stroke Thrombectomy](https://arxiv.org/abs/2510.16145)
*Ahmad Arrabi,Jay hwasung Jung,J Le,A Nguyen,J Reed,E Stahl,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出基于深度学习的自监督框架，用于自动化血栓切除术中的关键环节，通过回归任务分类骨骼标志点，提升手术效率和安全性


<details>
  <summary>Details</summary>
Motivation: 血栓切除术是治疗缺血性中风的有效方法，但需要大量资源和专业人员。希望通过深度学习自动化关键步骤来提高效率

Method: 自监督框架，使用基于回归的前置任务来分类各种骨骼标志点

Result: 模型在回归和分类任务上均优于现有方法，位置前置任务显著提升下游分类性能

Conclusion: 该框架为完全自主的C臂控制奠定了基础，未来将优化从骨盆到头部的轨迹规划，所有代码已开源

Abstract: Thrombectomy is one of the most effective treatments for ischemic stroke, but
it is resource and personnel-intensive. We propose employing deep learning to
automate critical aspects of thrombectomy, thereby enhancing efficiency and
safety. In this work, we introduce a self-supervised framework that classifies
various skeletal landmarks using a regression-based pretext task. Our
experiments demonstrate that our model outperforms existing methods in both
regression and classification tasks. Notably, our results indicate that the
positional pretext task significantly enhances downstream classification
performance. Future work will focus on extending this framework toward fully
autonomous C-arm control, aiming to optimize trajectories from the pelvis to
the head during stroke thrombectomy procedures. All code used is available at
https://github.com/AhmadArrabi/C_arm_guidance

</details>


### [13] [DuetMatch: Harmonizing Semi-Supervised Brain MRI Segmentation via Decoupled Branch Optimization](https://arxiv.org/abs/2510.16146)
*Thanh-Huy Nguyen,Hoang-Thien Nguyen,Vi Vu,Ba-Thinh Lam,Phat Huynh,Tianyang Wang,Xingjian Li,Ulas Bagci,Min Xu*

Main category: cs.CV

TL;DR: DuetMatch是一个用于医学图像分割的双分支半监督框架，通过异步优化编码器和解码器来解决联合优化问题，在噪声条件下提高一致性，并在多个脑MRI分割数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 医学图像标注数据有限，半监督学习具有吸引力，但教师-学生框架的联合优化会阻碍收敛和稳定性，特别是在具有挑战性的场景中。

Method: 提出DuetMatch框架，采用异步优化策略，每个分支分别优化编码器或解码器；引入解耦dropout扰动增强正则化；设计成对CutMix交叉引导增强模型多样性；提出一致性匹配机制减少噪声伪标签的确认偏差。

Result: 在ISLES2022和BraTS等脑MRI分割基准数据集上的广泛实验表明，DuetMatch在多种半监督分割场景下始终优于最先进方法。

Conclusion: DuetMatch框架在医学图像分割任务中表现出有效性和鲁棒性，为解决半监督学习中的优化和噪声问题提供了创新解决方案。

Abstract: The limited availability of annotated data in medical imaging makes
semi-supervised learning increasingly appealing for its ability to learn from
imperfect supervision. Recently, teacher-student frameworks have gained
popularity for their training benefits and robust performance. However, jointly
optimizing the entire network can hinder convergence and stability, especially
in challenging scenarios. To address this for medical image segmentation, we
propose DuetMatch, a novel dual-branch semi-supervised framework with
asynchronous optimization, where each branch optimizes either the encoder or
decoder while keeping the other frozen. To improve consistency under noisy
conditions, we introduce Decoupled Dropout Perturbation, enforcing
regularization across branches. We also design Pair-wise CutMix Cross-Guidance
to enhance model diversity by exchanging pseudo-labels through augmented input
pairs. To mitigate confirmation bias from noisy pseudo-labels, we propose
Consistency Matching, refining labels using stable predictions from frozen
teacher models. Extensive experiments on benchmark brain MRI segmentation
datasets, including ISLES2022 and BraTS, show that DuetMatch consistently
outperforms state-of-the-art methods, demonstrating its effectiveness and
robustness across diverse semi-supervised segmentation scenarios.

</details>


### [14] [Automated C-Arm Positioning via Conformal Landmark Localization](https://arxiv.org/abs/2510.16160)
*Ahmad Arrabi,Jay Hwasung Jung,Jax Luo,Nathan Franssen,Scott Raymond,Safwan Wshah*

Main category: cs.CV

TL;DR: 提出了一种自主导航C臂到预定义解剖标志的管道，利用X射线图像预测3D位移向量，结合不确定性量化和符合预测来提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前临床工作流程依赖手动对齐C臂，这会增加辐射暴露和手术延迟，需要自动化解决方案来提高效率和安全性。

Method: 使用X射线图像作为输入，模型预测3D位移向量；结合概率损失和骨骼姿态正则化来确保解剖合理性；使用符合预测校准不确定性。

Result: 在DeepDRR生成的合成X射线数据集上验证，显示出强大的定位精度和良好校准的预测边界。

Conclusion: 该管道具有作为安全可靠自主C臂系统组件的潜力，能够减少辐射暴露并提高手术效率。

Abstract: Accurate and reliable C-arm positioning is essential for fluoroscopy-guided
interventions. However, clinical workflows rely on manual alignment that
increases radiation exposure and procedural delays. In this work, we present a
pipeline that autonomously navigates the C-arm to predefined anatomical
landmarks utilizing X-ray images. Given an input X-ray image from an arbitrary
starting location on the operating table, the model predicts a 3D displacement
vector toward each target landmark along the body. To ensure reliable
deployment, we capture both aleatoric and epistemic uncertainties in the
model's predictions and further calibrate them using conformal prediction. The
derived prediction regions are interpreted as 3D confidence regions around the
predicted landmark locations. The training framework combines a probabilistic
loss with skeletal pose regularization to encourage anatomically plausible
outputs. We validate our approach on a synthetic X-ray dataset generated from
DeepDRR. Results show not only strong localization accuracy across multiple
architectures but also well-calibrated prediction bounds. These findings
highlight the pipeline's potential as a component in safe and reliable
autonomous C-arm systems. Code is available at
https://github.com/AhmadArrabi/C_arm_guidance_APAH

</details>


### [15] [Cost Savings from Automatic Quality Assessment of Generated Images](https://arxiv.org/abs/2510.16179)
*Xavier Giro-i-Nieto,Nefeli Andreou,Anqi Liang,Manel Baradad,Francesc Moreno-Noguer,Aleix Martinez*

Main category: cs.CV

TL;DR: 本文提出了一种成本节约公式，用于评估自动图像质量评估(IQA)引擎在生成图像生产流程中的经济效益，通过在背景修复用例中应用该公式展示了51.61%的成本节约。


<details>
  <summary>Details</summary>
Motivation: 当前深度生成模型生成的图像质量尚无法达到传统摄影方法的标准，生产流程中需要人工图像质量评估，这个过程成本高且效率低。引入自动预过滤阶段可以降低人工评估工作量。

Method: 提出一个通用公式来估计基于IQA引擎精度和通过率的成本节约，并在背景修复用例中应用简单的AutoML解决方案进行验证。

Result: 在背景修复用例中，使用简单的AutoML解决方案实现了51.61%的显著成本节约。

Conclusion: 自动IQA预过滤可以有效降低生成图像生产流程的成本，提出的成本节约公式为评估IQA引擎的经济效益提供了实用工具。

Abstract: Deep generative models have shown impressive progress in recent years, making
it possible to produce high quality images with a simple text prompt or a
reference image. However, state of the art technology does not yet meet the
quality standards offered by traditional photographic methods. For this reason,
production pipelines that use generated images often include a manual stage of
image quality assessment (IQA). This process is slow and expensive, especially
because of the low yield of automatically generated images that pass the
quality bar. The IQA workload can be reduced by introducing an automatic
pre-filtering stage, that will increase the overall quality of the images sent
to review and, therefore, reduce the average cost required to obtain a high
quality image. We present a formula that estimates the cost savings depending
on the precision and pass yield of a generic IQA engine. This formula is
applied in a use case of background inpainting, showcasing a significant cost
saving of 51.61% obtained with a simple AutoML solution.

</details>


### [16] [Seeing Through the Brain: New Insights from Decoding Visual Stimuli with fMRI](https://arxiv.org/abs/2510.16196)
*Zheng Huang,Enpei Zhang,Yinghao Cai,Weikang Qiu,Carl Yang,Elynn Chen,Xiang Zhang,Rex Ying,Dawei Zhou,Yujun Yan*

Main category: cs.CV

TL;DR: 本文提出PRISM模型，通过将fMRI信号投影到结构化文本空间作为中间表示来重建视觉刺激，在真实数据集上实现了高达8%的感知损失减少。


<details>
  <summary>Details</summary>
Motivation: 理解大脑如何编码视觉信息是神经科学和机器学习的核心挑战。目前尚不清楚哪种潜在空间最适合将fMRI信号转换为视觉刺激，以及如何有效组织该空间来表示视觉刺激。

Method: 提出PRISM模型，包括：1）将fMRI信号投影到结构化文本空间；2）对象中心扩散模块通过组合单个对象生成图像以减少对象检测错误；3）属性关系搜索模块自动识别与神经活动最匹配的关键属性和关系。

Result: 在真实世界数据集上的广泛实验表明，该框架优于现有方法，实现了高达8%的感知损失减少。

Conclusion: 研究结果强调了使用结构化文本作为中间空间来桥接fMRI信号和图像重建的重要性。

Abstract: Understanding how the brain encodes visual information is a central challenge
in neuroscience and machine learning. A promising approach is to reconstruct
visual stimuli, essentially images, from functional Magnetic Resonance Imaging
(fMRI) signals. This involves two stages: transforming fMRI signals into a
latent space and then using a pretrained generative model to reconstruct
images. The reconstruction quality depends on how similar the latent space is
to the structure of neural activity and how well the generative model produces
images from that space. Yet, it remains unclear which type of latent space best
supports this transformation and how it should be organized to represent visual
stimuli effectively. We present two key findings. First, fMRI signals are more
similar to the text space of a language model than to either a vision based
space or a joint text image space. Second, text representations and the
generative model should be adapted to capture the compositional nature of
visual stimuli, including objects, their detailed attributes, and
relationships. Building on these insights, we propose PRISM, a model that
Projects fMRI sIgnals into a Structured text space as an interMediate
representation for visual stimuli reconstruction. It includes an object centric
diffusion module that generates images by composing individual objects to
reduce object detection errors, and an attribute relationship search module
that automatically identifies key attributes and relationships that best align
with the neural activity. Extensive experiments on real world datasets
demonstrate that our framework outperforms existing methods, achieving up to an
8% reduction in perceptual loss. These results highlight the importance of
using structured text as the intermediate space to bridge fMRI signals and
image reconstruction.

</details>


### [17] [Data-Centric AI for Tropical Agricultural Mapping: Challenges, Strategies and Scalable Solutions](https://arxiv.org/abs/2510.16207)
*Mateus Pinto da Silva,Sabrina P. L. P. Correa,Hugo N. Oliveira,Ian M. Nunes,Jefersson A. dos Santos*

Main category: cs.CV

TL;DR: 本文提出以数据为中心的人工智能方法来解决热带农业遥感制图面临的挑战，强调数据质量对模型稳健性的重要性，并推荐了25种实用策略和9种最成熟的方法。


<details>
  <summary>Details</summary>
Motivation: 热带地区农业遥感制图面临缺乏高质量标注数据、标注成本高、数据变异性和区域泛化等独特挑战，传统以模型为中心的方法受到限制。

Method: 采用数据为中心的人工智能视角，重点关注数据质量和数据管理，包括置信学习、核心集选择、数据增强和主动学习等技术。

Result: 确定了25种适用于大规模农业制图管道的策略，并提出了使用9种最成熟方法的实用管道。

Conclusion: 数据为中心的方法更适合热带农业的动态现实，能够提高AI模型在热带农业遥感制图中的表现和可扩展性。

Abstract: Mapping agriculture in tropical areas through remote sensing presents unique
challenges, including the lack of high-quality annotated data, the elevated
costs of labeling, data variability, and regional generalisation. This paper
advocates a Data-Centric Artificial Intelligence (DCAI) perspective and
pipeline, emphasizing data quality and curation as key drivers for model
robustness and scalability. It reviews and prioritizes techniques such as
confident learning, core-set selection, data augmentation, and active learning.
The paper highlights the readiness and suitability of 25 distinct strategies in
large-scale agricultural mapping pipelines. The tropical context is of high
interest, since high cloudiness, diverse crop calendars, and limited datasets
limit traditional model-centric approaches. This tutorial outlines practical
solutions as a data-centric approach for curating and training AI models better
suited to the dynamic realities of tropical agriculture. Finally, we propose a
practical pipeline using the 9 most mature and straightforward methods that can
be applied to a large-scale tropical agricultural mapping project.

</details>


### [18] [StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales](https://arxiv.org/abs/2510.16209)
*Nyle Siddiqui,Rohit Gupta,Sirnam Swetha,Mubarak Shah*

Main category: cs.CV

TL;DR: 提出了一种名为StretchySnake的灵活训练方法，通过动态调整时空分辨率来解决视频SSMs的时空不灵活性问题，在多个动作识别基准上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频理解训练方法主要针对transformer设计，无法充分利用SSMs的线性复杂度和隐藏状态递归特性。固定分辨率的训练导致模型在处理未见时空分辨率的视频时性能下降，限制了模型在长短视频上的通用性。

Method: 提出灵活训练方法：在训练过程中采样不同时空分辨率的视频，动态插值模型权重以适应任意时空尺度。比较了五种灵活训练变体，确定了最适合视频SSMs的策略。

Result: 在短动作（UCF-101、HMDB-51）和长动作（COIN、Breakfast）基准上，StretchySnake比transformer和SSM基线性能提升高达28%，在细粒度动作（SSV2、Diving-48）上表现出强适应性。

Conclusion: 该方法提供了一个简单的即插即用训练方案，使视频SSMs在不同动作识别场景下更加鲁棒、分辨率无关且高效。

Abstract: State space models (SSMs) have emerged as a competitive alternative to
transformers in various tasks. Their linear complexity and hidden-state
recurrence make them particularly attractive for modeling long sequences,
whereas attention becomes quadratically expensive. However, current training
methods for video understanding are tailored towards transformers and fail to
fully leverage the unique attributes of SSMs. For example, video models are
often trained at a fixed resolution and video length to balance the quadratic
scaling of attention cost against performance. Consequently, these models
suffer from degraded performance when evaluated on videos with spatial and
temporal resolutions unseen during training; a property we call spatio-temporal
inflexibility. In the context of action recognition, this severely limits a
model's ability to retain performance across both short- and long-form videos.
Therefore, we propose a flexible training method that leverages and improves
the inherent adaptability of SSMs. Our method samples videos at varying
temporal and spatial resolutions during training and dynamically interpolates
model weights to accommodate any spatio-temporal scale. This instills our SSM,
which we call StretchySnake, with spatio-temporal flexibility and enables it to
seamlessly handle videos ranging from short, fine-grained clips to long,
complex activities. We introduce and compare five different variants of
flexible training, and identify the most effective strategy for video SSMs. On
short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks,
StretchySnake outperforms transformer and SSM baselines alike by up to 28%,
with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore,
our method provides a simple drop-in training recipe that makes video SSMs more
robust, resolution-agnostic, and efficient across diverse action recognition
scenarios.

</details>


### [19] [VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction](https://arxiv.org/abs/2510.16220)
*Djamel Eddine Boukhari*

Main category: cs.CV

TL;DR: 提出VM-BeautyNet，一种结合Vision Transformer和Mamba-based Vision模型的异构集成架构，用于面部美观预测，在SCUT-FBP5500数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有CNN模型难以捕捉全局面部特征，而ViT虽然能建模长距离空间关系但存在二次复杂度问题。需要一种能结合全局特征建模和高效长距离依赖捕获的方法。

Method: 使用异构集成架构，融合ViT和Mamba-based Vision模型的互补优势。ViT负责捕捉全局面部结构和对称性，Mamba模型以线性复杂度高效建模长距离依赖关系。

Result: 在SCUT-FBP5500数据集上取得PC=0.9212、MAE=0.2085、RMSE=0.2698的SOTA性能。Grad-CAM可视化证实了两个骨干网络的互补特征提取能力。

Conclusion: VM-BeautyNet为计算美学提供了强大的新架构范式，通过可解释性分析揭示了模型的决策过程。

Abstract: Facial Beauty Prediction (FBP) is a complex and challenging computer vision
task, aiming to model the subjective and intricate nature of human aesthetic
perception. While deep learning models, particularly Convolutional Neural
Networks (CNNs), have made significant strides, they often struggle to capture
the global, holistic facial features that are critical to human judgment.
Vision Transformers (ViT) address this by effectively modeling long-range
spatial relationships, but their quadratic complexity can be a bottleneck. This
paper introduces a novel, heterogeneous ensemble architecture,
\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths
of a Vision Transformer and a Mamba-based Vision model, a recent advancement in
State-Space Models (SSMs). The ViT backbone excels at capturing global facial
structure and symmetry, while the Mamba backbone efficiently models long-range
dependencies with linear complexity, focusing on sequential features and
textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our
proposed VM-BeautyNet achieves state-of-the-art performance, with a
\textbf{Pearson Correlation (PC) of 0.9212}, a \textbf{Mean Absolute Error
(MAE) of 0.2085}, and a \textbf{Root Mean Square Error (RMSE) of 0.2698}.
Furthermore, through Grad-CAM visualizations, we provide interpretability
analysis that confirms the complementary feature extraction of the two
backbones, offering new insights into the model's decision-making process and
presenting a powerful new architectural paradigm for computational aesthetics.

</details>


### [20] [Designing a Convolutional Neural Network for High-Accuracy Oral Cavity Squamous Cell Carcinoma (OCSCC) Detection](https://arxiv.org/abs/2510.16235)
*Vishal Manikanden,Aniketh Bandlamudi,Daniel Haehn*

Main category: cs.CV

TL;DR: 该研究开发了一个用于早期检测口腔鳞状细胞癌（OCSCC）的卷积神经网络系统，结合图像采集硬件，分析不同分辨率对检测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 由于OCSCC早期症状不明显、生长缓慢且发生在隐蔽区域，常常被漏诊导致可预防的死亡。CNN凭借其精确的图像分割和模式识别能力，有望成为早期检测的有效手段。

Method: 训练了一个CNN模型，使用4293张训练图像（包括良恶性肿瘤和阴性样本），评估其在OCSCC检测中的精确度、召回率和mAP。测试数据集包含不同分辨率的图像，并开发了图像采集硬件和应用程序。

Result: 图像分辨率越高，预测准确性越高，但呈对数增长趋势，表明像素数增加带来的收益递减。设计的增强硬件能够捕获详细图像并提高检测效果。

Conclusion: CNN结合专用硬件可以有效检测OCSCC，图像分辨率对检测准确性有重要影响，但高分辨率带来的改进存在收益递减现象。

Abstract: Oral Cavity Squamous Cell Carcinoma (OCSCC) is the most common type of head
and neck cancer. Due to the subtle nature of its early stages, deep and hidden
areas of development, and slow growth, OCSCC often goes undetected, leading to
preventable deaths. However, properly trained Convolutional Neural Networks
(CNNs), with their precise image segmentation techniques and ability to apply
kernel matrices to modify the RGB values of images for accurate image pattern
recognition, would be an effective means for early detection of OCSCC. Pairing
this neural network with image capturing and processing hardware would allow
increased efficacy in OCSCC detection. The aim of our project is to develop a
Convolutional Neural Network trained to recognize OCSCC, as well as to design a
physical hardware system to capture and process detailed images, in order to
determine the image quality required for accurate predictions. A CNN was
trained on 4293 training images consisting of benign and malignant tumors, as
well as negative samples, and was evaluated for its precision, recall, and Mean
Average Precision (mAP) in its predictions of OCSCC. A testing dataset of
randomly assorted images of cancerous, non-cancerous, and negative images was
chosen, and each image was altered to represent 5 common resolutions. This test
data set was thoroughly analyzed by the CNN and predictions were scored on the
basis of accuracy. The designed enhancement hardware was used to capture
detailed images, and its impact was scored. An application was developed to
facilitate the testing process and bring open access to the CNN. Images of
increasing resolution resulted in higher-accuracy predictions on a logarithmic
scale, demonstrating the diminishing returns of higher pixel counts.

</details>


### [21] [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://arxiv.org/abs/2510.16258)
*Claire McLean,Makenzie Meendering,Tristan Swartz,Orri Gabbay,Alexandra Olsen,Rachel Jacobs,Nicholas Rosen,Philippe de Bree,Tony Garcia,Gadsden Merrill,Jake Sandakly,Julia Buffalini,Neham Jain,Steven Krenn,Moneish Kumar,Dejan Markovic,Evonne Ng,Fabian Prada,Andrew Saba,Siwei Zhang,Vasu Agrawal,Tim Godisart,Alexander Richard,Michael Zollhoefer*

Main category: cs.CV

TL;DR: Embody 3D是一个包含500小时3D运动数据的多模态数据集，涵盖439名参与者的单人多人和交互行为数据


<details>
  <summary>Details</summary>
Motivation: 为研究人类行为、手势识别、情感交互和协作活动提供大规模的3D运动数据支持

Method: 在多摄像头采集环境中收集439名参与者的3D运动数据，包括身体追踪、手部追踪和身体形状数据，并提供文本标注和独立音频轨道

Result: 创建了包含54百万帧3D运动追踪数据的Embody 3D数据集，涵盖单人多人和交互行为场景

Conclusion: Embody 3D数据集为人类行为分析、社交交互研究和多模态学习提供了丰富的3D运动数据资源

Abstract: The Codec Avatars Lab at Meta introduces Embody 3D, a multimodal dataset of
500 individual hours of 3D motion data from 439 participants collected in a
multi-camera collection stage, amounting to over 54 million frames of tracked
3D motion. The dataset features a wide range of single-person motion data,
including prompted motions, hand gestures, and locomotion; as well as
multi-person behavioral and conversational data like discussions, conversations
in different emotional states, collaborative activities, and co-living
scenarios in an apartment-like space. We provide tracked human motion including
hand tracking and body shape, text annotations, and a separate audio track for
each participant.

</details>


### [22] [Proactive Scene Decomposition and Reconstruction](https://arxiv.org/abs/2510.16272)
*Baicheng Li,Zike Yan,Dong Wu,Hongbin Zha*

Main category: cs.CV

TL;DR: 提出了一种主动场景分解与重建的新任务，通过观察人类-物体交互来迭代解构和重建动态环境，解决静态物体级重建中的模糊性问题。


<details>
  <summary>Details</summary>
Motivation: 人类行为是场景动态的主要来源，蕴含着丰富的动态线索。传统的静态物体级重建方法存在固有模糊性，需要利用人类意图性交互来动态优化重建过程。

Method: 基于高斯泼溅技术，通过观察人类-物体交互来迭代解构和重建环境，集成相机和物体姿态估计、实例分解、在线地图更新等多个任务。

Result: 在多个真实场景中验证了有效性，实现了准确一致的动态场景建模，具有逼真高效的渲染效果。

Conclusion: 该方法为传统物体级重建方法提供了灵活、渐进的替代方案，能够有效利用第一人称视角直播中的人类-物体交互线索。

Abstract: Human behaviors are the major causes of scene dynamics and inherently contain
rich cues regarding the dynamics. This paper formalizes a new task of proactive
scene decomposition and reconstruction, an online approach that leverages
human-object interactions to iteratively disassemble and reconstruct the
environment. By observing these intentional interactions, we can dynamically
refine the decomposition and reconstruction process, addressing inherent
ambiguities in static object-level reconstruction. The proposed system
effectively integrates multiple tasks in dynamic environments such as accurate
camera and object pose estimation, instance decomposition, and online map
updating, capitalizing on cues from human-object interactions in egocentric
live streams for a flexible, progressive alternative to conventional
object-level reconstruction methods. Aided by the Gaussian splatting technique,
accurate and consistent dynamic scene modeling is achieved with photorealistic
and efficient rendering. The efficacy is validated in multiple real-world
scenarios with promising advantages.

</details>


### [23] [Cerberus: Real-Time Video Anomaly Detection via Cascaded Vision-Language Models](https://arxiv.org/abs/2510.16290)
*Yue Zheng,Xiufang Shi,Jiming Chen,Yuanchao Shu*

Main category: cs.CV

TL;DR: Cerberus是一个两阶段级联系统，通过运动掩码提示和基于规则的偏差检测，在保持97.2%准确率的同时实现151.79倍速度提升，为实时视频异常检测提供实用解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉语言模型的视频异常检测方法计算成本高、视觉定位性能不稳定，阻碍了实时部署。

Method: 采用两阶段级联系统：离线学习正常行为规则，在线推理时结合轻量级过滤和细粒度VLM推理。关键创新包括运动掩码提示和基于规则的偏差检测。

Result: 在四个数据集上的评估显示，Cerberus在NVIDIA L40S GPU上平均达到57.68 fps，速度提升151.79倍，准确率97.2%，与最先进的VLM-based VAD方法相当。

Conclusion: Cerberus通过高效的架构设计，成功解决了实时视频异常检测中的计算效率和性能稳定性问题，为实际应用提供了可行方案。

Abstract: Video anomaly detection (VAD) has rapidly advanced by recent development of
Vision-Language Models (VLMs). While these models offer superior zero-shot
detection capabilities, their immense computational cost and unstable visual
grounding performance hinder real-time deployment. To overcome these
challenges, we introduce Cerberus, a two-stage cascaded system designed for
efficient yet accurate real-time VAD. Cerberus learns normal behavioral rules
offline, and combines lightweight filtering with fine-grained VLM reasoning
during online inference. The performance gains of Cerberus come from two key
innovations: motion mask prompting and rule-based deviation detection. The
former directs the VLM's attention to regions relevant to motion, while the
latter identifies anomalies as deviations from learned norms rather than
enumerating possible anomalies. Extensive evaluations on four datasets show
that Cerberus on average achieves 57.68 fps on an NVIDIA L40S GPU, a
151.79$\times$ speedup, and 97.2\% accuracy comparable to the state-of-the-art
VLM-based VAD methods, establishing it as a practical solution for real-time
video analytics.

</details>


### [24] [OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models](https://arxiv.org/abs/2510.16295)
*Ryoto Miyamoto,Xin Fan,Fuyuko Kido,Tsuneo Matsumoto,Hayato Yamana*

Main category: cs.CV

TL;DR: OpenLVLM-MIA是一个新的基准测试，揭示了评估大型视觉语言模型成员推理攻击时的根本挑战。研究发现之前的高攻击成功率实际上来自数据集构建中的分布偏差，而非真实的成员状态识别。


<details>
  <summary>Details</summary>
Motivation: 针对现有MIA评估方法存在分布偏差问题，作者希望建立一个更公正的基准来准确评估大型视觉语言模型的隐私保护能力。

Method: 创建了包含6000张图像的受控基准数据集，其中成员和非成员样本的分布被仔细平衡，并提供了三个不同训练阶段的真实成员标签。

Result: 实验表明，在无偏条件下，最先进的MIA方法性能收敛到随机猜测水平，说明之前的评估结果存在误导性。

Conclusion: OpenLVLM-MIA为LVLM隐私保护技术开发提供了透明无偏的基准，明确了当前MIA研究的局限性。

Abstract: OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in
evaluating membership inference attacks (MIA) against large vision-language
models (LVLMs). While prior work has reported high attack success rates, our
analysis suggests that these results often arise from detecting distributional
bias introduced during dataset construction rather than from identifying true
membership status. To address this issue, we introduce a controlled benchmark
of 6{,}000 images where the distributions of member and non-member samples are
carefully balanced, and ground-truth membership labels are provided across
three distinct training stages. Experiments using OpenLVLM-MIA demonstrated
that the performance of state-of-the-art MIA methods converged to random chance
under unbiased conditions. By offering a transparent and unbiased benchmark,
OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and
provides a solid foundation for developing stronger privacy-preserving
techniques.

</details>


### [25] [Stroke2Sketch: Harnessing Stroke Attributes for Training-Free Sketch Generation](https://arxiv.org/abs/2510.16319)
*Rui Yang,Huining Li,Yiyi Long,Xiaojun Wu,Shengfeng He*

Main category: cs.CV

TL;DR: Stroke2Sketch是一个无需训练的新框架，通过跨图像笔画注意力机制实现参考风格到内容图像的精确笔画属性迁移，在笔画控制和语义一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 生成参考风格引导的草图需要精确迁移笔画属性（如线条粗细、变形和纹理稀疏度），同时保持语义结构和内容保真度。

Method: 提出Stroke2Sketch框架，引入嵌入自注意力层的跨图像笔画注意力机制，建立细粒度语义对应关系；开发自适应对比度增强和语义聚焦注意力来强化内容保存和前景强调。

Result: 该方法能有效合成风格忠实且接近手工绘制结果的草图，在表达性笔画控制和语义一致性方面优于现有方法。

Conclusion: Stroke2Sketch通过创新的注意力机制成功实现了精确的笔画属性迁移，为风格化草图生成提供了有效的解决方案。

Abstract: Generating sketches guided by reference styles requires precise transfer of
stroke attributes, such as line thickness, deformation, and texture sparsity,
while preserving semantic structure and content fidelity. To this end, we
propose Stroke2Sketch, a novel training-free framework that introduces
cross-image stroke attention, a mechanism embedded within self-attention layers
to establish fine-grained semantic correspondences and enable accurate stroke
attribute transfer. This allows our method to adaptively integrate reference
stroke characteristics into content images while maintaining structural
integrity. Additionally, we develop adaptive contrast enhancement and
semantic-focused attention to reinforce content preservation and foreground
emphasis. Stroke2Sketch effectively synthesizes stylistically faithful sketches
that closely resemble handcrafted results, outperforming existing methods in
expressive stroke control and semantic coherence. Codes are available at
https://github.com/rane7/Stroke2Sketch.

</details>


### [26] [Scaling Laws for Deepfake Detection](https://arxiv.org/abs/2510.16320)
*Wenhao Wang,Longqi Cai,Taihong Xiao,Yuxiao Wang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文系统研究了深度伪造检测任务的缩放定律，发现检测误差随着真实图像域数量和深度伪造方法数量的增加呈幂律衰减，类似于大语言模型的缩放规律。


<details>
  <summary>Details</summary>
Motivation: 由于现有数据集无法满足缩放定律研究的规模需求，作者构建了目前最大的深度伪造检测数据集ScaleDF，以系统分析模型性能与训练数据规模之间的关系。

Method: 构建包含580万张真实图像（来自51个不同域）和880万张伪造图像（由102种深度伪造方法生成）的ScaleDF数据集，通过系统实验分析模型性能随真实域数量、伪造方法数量和训练图像数量的变化规律。

Result: 观察到深度伪造检测的平均误差随真实域数量或深度伪造方法数量的增加呈可预测的幂律衰减，这一规律可用于预测达到目标性能所需的额外数据量。

Conclusion: 缩放定律为深度伪造检测提供了数据中心的应对策略，但同时也存在局限性，需要进一步研究预训练和数据增强在缩放中的作用。

Abstract: This paper presents a systematic study of scaling laws for the deepfake
detection task. Specifically, we analyze the model performance against the
number of real image domains, deepfake generation methods, and training images.
Since no existing dataset meets the scale requirements for this research, we
construct ScaleDF, the largest dataset to date in this field, which contains
over 5.8 million real images from 51 different datasets (domains) and more than
8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we
observe power-law scaling similar to that shown in large language models
(LLMs). Specifically, the average detection error follows a predictable
power-law decay as either the number of real domains or the number of deepfake
methods increases. This key observation not only allows us to forecast the
number of additional real domains or deepfake methods required to reach a
target performance, but also inspires us to counter the evolving deepfake
technology in a data-centric manner. Beyond this, we examine the role of
pre-training and data augmentations in deepfake detection under scaling, as
well as the limitations of scaling itself.

</details>


### [27] [Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention](https://arxiv.org/abs/2510.16325)
*Yuyao Zhang,Yu-Wing Tai*

Main category: cs.CV

TL;DR: Scale-DiT提出了一种新的扩散框架，通过分层局部注意力和低分辨率全局引导，实现了高效、可扩展的超高分辨率图像生成，在4K分辨率下比密集注意力基线快2倍以上且内存使用更低。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型受限于注意力机制的二次复杂性和缺乏原生4K训练数据，无法生成超高分辨率（超过1K×1K）图像，需要解决全局结构一致性和细粒度纹理合成的挑战。

Method: 采用分层局部注意力机制，将高分辨率潜在空间划分为固定大小的局部窗口以降低计算复杂度；使用低分辨率潜在空间和缩放位置锚点注入全局语义；通过轻量级LoRA适配器在去噪过程中连接全局和局部路径；采用Hilbert曲线重新排列token序列和融合内核优化推理效率。

Result: Scale-DiT在4K×4K分辨率下实现了2倍以上的推理速度提升和更低的内存使用，在FID、IS、CLIP Score等定量指标和定性比较中均优于或匹配依赖原生4K训练的SOTA方法，展现出更好的全局一致性和更清晰的局部细节。

Conclusion: 分层局部注意力结合引导性低分辨率锚点是一种有前景且有效的方法，可推动超高分辨率图像生成的进步，无需额外高分辨率训练数据即可可靠扩展到4K分辨率。

Abstract: Ultra-high-resolution text-to-image generation demands both fine-grained
texture synthesis and globally coherent structure, yet current diffusion models
remain constrained to sub-$1K \times 1K$ resolutions due to the prohibitive
quadratic complexity of attention and the scarcity of native $4K$ training
data. We present \textbf{Scale-DiT}, a new diffusion framework that introduces
hierarchical local attention with low-resolution global guidance, enabling
efficient, scalable, and semantically coherent image synthesis at ultra-high
resolutions. Specifically, high-resolution latents are divided into fixed-size
local windows to reduce attention complexity from quadratic to near-linear,
while a low-resolution latent equipped with scaled positional anchors injects
global semantics. A lightweight LoRA adaptation bridges global and local
pathways during denoising, ensuring consistency across structure and detail. To
maximize inference efficiency, we repermute token sequence in Hilbert curve
order and implement a fused-kernel for skipping masked operations, resulting in
a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT
achieves more than $2\times$ faster inference and lower memory usage compared
to dense attention baselines, while reliably scaling to $4K \times 4K$
resolution without requiring additional high-resolution training data. On both
quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons,
Scale-DiT delivers superior global coherence and sharper local detail, matching
or outperforming state-of-the-art methods that rely on native 4K training.
Taken together, these results highlight hierarchical local attention with
guided low-resolution anchors as a promising and effective approach for
advancing ultra-high-resolution image generation.

</details>


### [28] [DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution](https://arxiv.org/abs/2510.16326)
*Yi Wei,Shunpu Tang,Liang Zhao,Qiangian Yang*

Main category: cs.CV

TL;DR: DiffusionX是一个云边协同框架，通过轻量级设备端扩散模型快速生成预览图，云端大模型进行最终优化，实现高效的多轮提示生成，减少15.8%的平均生成时间。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成过程计算密集，用户需要多次迭代优化提示词，增加了延迟和云端资源负担。

Method: 提出云边协同框架，设备端使用轻量扩散模型快速生成预览，云端使用高容量模型进行最终优化；引入噪声水平预测器动态平衡计算负载。

Result: 相比Stable Diffusion v1.5减少15.8%平均生成时间，图像质量相当；比Tiny-SD仅慢0.9%但图像质量显著提升。

Conclusion: DiffusionX在保持图像质量的同时显著提升效率，展示了高效且可扩展的云边协同生成方案。

Abstract: Recent advances in diffusion models have driven remarkable progress in image
generation. However, the generation process remains computationally intensive,
and users often need to iteratively refine prompts to achieve the desired
results, further increasing latency and placing a heavy burden on cloud
resources. To address this challenge, we propose DiffusionX, a cloud-edge
collaborative framework for efficient multi-round, prompt-based generation. In
this system, a lightweight on-device diffusion model interacts with users by
rapidly producing preview images, while a high-capacity cloud model performs
final refinements after the prompt is finalized. We further introduce a noise
level predictor that dynamically balances the computation load, optimizing the
trade-off between latency and cloud workload. Experiments show that DiffusionX
reduces average generation time by 15.8% compared with Stable Diffusion v1.5,
while maintaining comparable image quality. Moreover, it is only 0.9% slower
than Tiny-SD with significantly improved image quality, thereby demonstrating
efficiency and scalability with minimal overhead.

</details>


### [29] [TokenAR: Multiple Subject Generation via Autoregressive Token-level enhancement](https://arxiv.org/abs/2510.16332)
*Haiyue Sun,Qingdong He,Jinlong Peng,Peng Tang,Jiangning Zhang,Junwei Zhu,Xiaobin Hu,Shuicheng Yan*

Main category: cs.CV

TL;DR: TokenAR框架通过token级增强机制解决多参考图像生成中的身份混淆问题，包含token索引嵌入、指令token注入和身份token解纠缠策略，在InstructAR数据集上验证了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在多参考图像生成中难以解耦不同参考身份，导致身份混淆问题。

Method: 提出TokenAR框架，包含三个token级增强组件：1）Token Index Embedding聚类token索引；2）Instruct Token Injection注入详细视觉特征；3）身份token解纠缠策略（ITD）引导token独立表示每个身份特征。

Result: 在包含28K训练对的InstructAR数据集上，该方法在保持高质量背景重建的同时实现了良好的身份一致性，超越了当前最先进模型。

Conclusion: TokenAR框架显著增强了基于AR的条件图像生成能力，为解决多参考图像生成中的身份混淆问题提供了有效方案。

Abstract: Autoregressive Model (AR) has shown remarkable success in conditional image
generation. However, these approaches for multiple reference generation
struggle with decoupling different reference identities. In this work, we
propose the TokenAR framework, specifically focused on a simple but effective
token-level enhancement mechanism to address reference identity confusion
problem. Such token-level enhancement consists of three parts, 1). Token Index
Embedding clusters the tokens index for better representing the same reference
images; 2). Instruct Token Injection plays as a role of extra visual feature
container to inject detailed and complementary priors for reference tokens; 3).
The identity-token disentanglement strategy (ITD) explicitly guides the token
representations toward independently representing the features of each
identity.This token-enhancement framework significantly augments the
capabilities of existing AR based methods in conditional image generation,
enabling good identity consistency while preserving high quality background
reconstruction. Driven by the goal of high-quality and high-diversity in
multi-subject generation, we introduce the InstructAR Dataset, the first
open-source, large-scale, multi-reference input, open domain image generation
dataset that includes 28K training pairs, each example has two reference
subjects, a relative prompt and a background with mask annotation, curated for
multiple reference image generation training and evaluating. Comprehensive
experiments validate that our approach surpasses current state-of-the-art
models in multiple reference image generation task. The implementation code and
datasets will be made publicly. Codes are available, see
https://github.com/lyrig/TokenAR

</details>


### [30] [RL makes MLLMs see better than SFT](https://arxiv.org/abs/2510.16333)
*Junha Song,Sangdoo Yun,Dongyoon Han,Jaegul Choo,Byeongho Heo*

Main category: cs.CV

TL;DR: 该论文挑战了多模态语言模型性能主要继承自LLM骨干的假设，重点分析了视觉编码器在MLLM中的作用。研究发现强化学习训练比监督微调能产生更强、更精确的视觉表示，并提出了PIVOT方法，仅用1%计算成本就能训练出更优的视觉编码器。


<details>
  <summary>Details</summary>
Motivation: 当前MLLM研究过于关注LLM骨干而忽视了视觉编码器的作用，特别是在训练范式从SFT转向RL后，缺乏对视觉编码器如何被重塑的分析。

Method: 通过多种实验分析训练策略对MLLMs的影响，包括ImageNet分类、分割和梯度可视化，并开发了PIVOT方法优化视觉编码器。

Result: RL训练比SFT产生更强、更精确定位的视觉表示，PIVOT训练的视觉编码器在MLLMs中表现优于更大规模训练的模型，且计算成本仅为标准视觉预训练的1%。

Conclusion: 视觉编码器在MLLMs中起着关键作用，RL训练能显著提升视觉表示质量，PIVOT方法为MLLMs视觉骨干的发展提供了高效路径。

Abstract: A dominant assumption in Multimodal Language Model (MLLM) research is that
its performance is largely inherited from the LLM backbone, given its immense
parameter scale and remarkable capabilities. This has created a void in the
understanding of the vision encoder, which determines how MLLMs perceive
images. The recent shift in MLLM training paradigms, from Supervised Finetuning
(SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the
significant lack of analysis on how such training reshapes the vision encoder
as well as the MLLM. To address this, we first investigate the impact of
training strategies on MLLMs, where RL shows a clear advantage over SFT in
strongly vision-related VQA benchmarks. Motivated by this, we conduct a
critical yet under-explored analysis of the vision encoder of MLLMs through
diverse and in-depth experiments, ranging from ImageNet classification and
segmentation to gradient visualization. Our results demonstrate that MLLM's
post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on
MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual
representations. Specifically, the key finding of our study is that RL produces
stronger and precisely localized visual representations compared to SFT,
boosting the ability of the vision encoder for MLLM. We then reframe our
findings into a simple recipe for building strong vision encoders for MLLMs,
Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs,
a PIVOT-trained vision encoder outperforms even larger and more heavily-trained
counterparts, despite requiring less than 1% of the computational cost of
standard vision pretraining. This result opens an effective and efficient path
for advancing the vision backbones of MLLMs. Project page available at
https://june-page.github.io/pivot/

</details>


### [31] [On the Provable Importance of Gradients for Language-Assisted Image Clustering](https://arxiv.org/abs/2510.16335)
*Bo Peng,Jie Lu,Guangquan Zhang,Zhen Fang*

Main category: cs.CV

TL;DR: 提出GradNorm框架，通过梯度范数筛选正名词来改进语言辅助图像聚类，理论上有严格误差界保证，实验证明达到SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有语言辅助图像聚类方法基于CLIP特征空间筛选正名词缺乏理论依据，需要更严谨的理论基础

Method: 基于梯度范数的框架，通过反向传播交叉熵损失的梯度大小来衡量名词与图像语义的接近程度

Result: 理论证明GradNorm包含现有方法为特例，实验在多个基准测试中达到最先进的聚类性能

Conclusion: GradNorm为语言辅助图像聚类提供了理论保证的解决方案，显著提升了聚类效果

Abstract: This paper investigates the recently emerged problem of Language-assisted
Image Clustering (LaIC), where textual semantics are leveraged to improve the
discriminability of visual representations to facilitate image clustering. Due
to the unavailability of true class names, one of core challenges of LaIC lies
in how to filter positive nouns, i.e., those semantically close to the images
of interest, from unlabeled wild corpus data. Existing filtering strategies are
predominantly based on the off-the-shelf feature space learned by CLIP;
however, despite being intuitive, these strategies lack a rigorous theoretical
foundation. To fill this gap, we propose a novel gradient-based framework,
termed as GradNorm, which is theoretically guaranteed and shows strong
empirical performance. In particular, we measure the positiveness of each noun
based on the magnitude of gradients back-propagated from the cross-entropy
between the predicted target distribution and the softmax output.
Theoretically, we provide a rigorous error bound to quantify the separability
of positive nouns by GradNorm and prove that GradNorm naturally subsumes
existing filtering strategies as extremely special cases of itself.
Empirically, extensive experiments show that GradNorm achieves the
state-of-the-art clustering performance on various benchmarks.

</details>


### [32] [MIRAD - A comprehensive real-world robust anomaly detection dataset for Mass Individualization](https://arxiv.org/abs/2510.16370)
*Pulin Li,Guocheng Wu,Li Yin,Yuxin Zheng,Wei Zhang,Yanjie Zhou*

Main category: cs.CV

TL;DR: 提出了首个针对社交制造场景的异常检测基准数据集MIRAD，该数据集捕捉了个性化产品、分布式制造节点和成像异质性三大挑战，评估显示现有SOTA方法在该数据集上性能显著下降。


<details>
  <summary>Details</summary>
Motivation: 社交制造模式在实现大规模个性化生产的同时，面临着质量控制的重大挑战，特别是在缺陷检测方面，主要困难包括产品高度定制化、生产碎片化小批量订单以及分布式站点成像环境差异大。

Method: 构建MIRAD数据集，包含三个关键维度：多样化的个性化产品（类内差异大）、来自六个地理分散制造节点的数据、以及显著的成像异质性（光照、背景、运动条件变化）。在此基础上对SOTA异常检测方法进行广泛评估。

Result: 所有模型在MIRAD数据集上的性能相比传统基准都出现了显著下降，凸显了现实世界个性化生产中缺陷检测的未解决复杂性。

Conclusion: MIRAD数据集通过连接工业需求和学术研究，为开发工业5.0所需的稳健质量控制解决方案提供了现实基础，数据集已公开可用。

Abstract: Social manufacturing leverages community collaboration and scattered
resources to realize mass individualization in modern industry. However, this
paradigm shift also introduces substantial challenges in quality control,
particularly in defect detection. The main difficulties stem from three
aspects. First, products often have highly customized configurations. Second,
production typically involves fragmented, small-batch orders. Third, imaging
environments vary considerably across distributed sites. To overcome the
scarcity of real-world datasets and tailored algorithms, we introduce the Mass
Individualization Robust Anomaly Detection (MIRAD) dataset. As the first
benchmark explicitly designed for anomaly detection in social manufacturing,
MIRAD captures three critical dimensions of this domain: (1) diverse
individualized products with large intra-class variation, (2) data collected
from six geographically dispersed manufacturing nodes, and (3) substantial
imaging heterogeneity, including variations in lighting, background, and motion
conditions. We then conduct extensive evaluations of state-of-the-art (SOTA)
anomaly detection methods on MIRAD, covering one-class, multi-class, and
zero-shot approaches. Results show a significant performance drop across all
models compared with conventional benchmarks, highlighting the unresolved
complexities of defect detection in real-world individualized production. By
bridging industrial requirements and academic research, MIRAD provides a
realistic foundation for developing robust quality control solutions essential
for Industry 5.0. The dataset is publicly available at
https://github.com/wu33learn/MIRAD.

</details>


### [33] [Cataract-LMM: Large-Scale, Multi-Source, Multi-Task Benchmark for Deep Learning in Surgical Video Analysis](https://arxiv.org/abs/2510.16371)
*Mohammad Javad Ahmadi,Iman Gandomi,Parisa Abdi,Seyed-Farzad Mohammadi,Amirhossein Taslimi,Mehdi Khodaparast,Hassan Hashemi,Mahdi Tavakoli,Hamid D. Taghirad*

Main category: cs.CV

TL;DR: 提出了一个包含3000个白内障手术视频的大规模数据集，带有四层标注，用于训练可泛化的深度学习模型。


<details>
  <summary>Details</summary>
Motivation: 当前白内障手术数据集缺乏多样性和标注深度，无法满足计算机辅助手术系统对高质量训练数据的需求。

Method: 收集来自两个手术中心的3000个超声乳化白内障手术视频，包含四种标注层：时间手术阶段、器械和解剖结构实例分割、器械-组织交互跟踪、基于ICO-OSCAR标准的技能评分。

Result: 通过基准实验验证了数据集在手术工作流识别、场景分割和自动技能评估等关键AI任务上的技术质量，并建立了领域适应基线。

Conclusion: 该数据集为白内障手术AI研究提供了重要的资源基础，支持更准确的模型训练和性能评估。

Abstract: The development of computer-assisted surgery systems depends on large-scale,
annotated datasets. Current resources for cataract surgery often lack the
diversity and annotation depth needed to train generalizable deep-learning
models. To address this gap, we present a dataset of 3,000 phacoemulsification
cataract surgery videos from two surgical centers, performed by surgeons with a
range of experience levels. This resource is enriched with four annotation
layers: temporal surgical phases, instance segmentation of instruments and
anatomical structures, instrument-tissue interaction tracking, and quantitative
skill scores based on the established competency rubrics like the ICO-OSCAR.
The technical quality of the dataset is supported by a series of benchmarking
experiments for key surgical AI tasks, including workflow recognition, scene
segmentation, and automated skill assessment. Furthermore, we establish a
domain adaptation baseline for the phase recognition task by training a model
on a subset of surgical centers and evaluating its performance on a held-out
center. The dataset and annotations are available in Google Form
(https://docs.google.com/forms/d/e/1FAIpQLSfmyMAPSTGrIy2sTnz0-TMw08ZagTimRulbAQcWdaPwDy187A/viewform?usp=dialog).

</details>


### [34] [iWatchRoadv2: Pothole Detection, Geospatial Mapping, and Intelligent Road Governance](https://arxiv.org/abs/2510.16375)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoadv2是一个用于印度道路坑洼实时检测、GPS地理标记和道路健康可视化的端到端自动化平台，基于YOLO模型和OpenStreetMap，支持智能治理和公共参与。


<details>
  <summary>Details</summary>
Motivation: 印度道路网络多样且维护不足，道路坑洼存在严重安全隐患和维护挑战，需要自动化解决方案来改善道路基础设施维护。

Method: 使用自标注的7000多张印度道路仪表盘摄像头图像数据集微调Ultralytics YOLO模型进行坑洼检测，结合OCR时间戳和GPS日志进行精确定位，通过优化后端数据库管理道路段和承包商信息。

Result: 开发了完整的pothole监测平台，具有实时检测、地理标记、智能警报、网络界面等功能，支持从检测到修复验证的完整生命周期自动化。

Conclusion: iWatchRoadv2通过自动化pothole监测生命周期，实现了数据驱动的智慧城市管理、透明治理和道路基础设施维护的可持续改进，提供了经济高效且可扩展的解决方案。

Abstract: Road potholes pose significant safety hazards and maintenance challenges,
particularly on India's diverse and under-maintained road networks. This paper
presents iWatchRoadv2, a fully automated end-to-end platform for real-time
pothole detection, GPS-based geotagging, and dynamic road health visualization
using OpenStreetMap (OSM). We curated a self-annotated dataset of over 7,000
dashcam frames capturing diverse Indian road conditions, weather patterns, and
lighting scenarios, which we used to fine-tune the Ultralytics YOLO model for
accurate pothole detection. The system synchronizes OCR-extracted video
timestamps with external GPS logs to precisely geolocate each detected pothole,
enriching detections with comprehensive metadata, including road segment
attribution and contractor information managed through an optimized backend
database. iWatchRoadv2 introduces intelligent governance features that enable
authorities to link road segments with contract metadata through a secure login
interface. The system automatically sends alerts to contractors and officials
when road health deteriorates, supporting automated accountability and warranty
enforcement. The intuitive web interface delivers actionable analytics to
stakeholders and the public, facilitating evidence-driven repair planning,
budget allocation, and quality assessment. Our cost-effective and scalable
solution streamlines frame processing and storage while supporting seamless
public engagement for urban and rural deployments. By automating the complete
pothole monitoring lifecycle, from detection to repair verification,
iWatchRoadv2 enables data-driven smart city management, transparent governance,
and sustainable improvements in road infrastructure maintenance. The platform
and live demonstration are accessible at
https://smlab.niser.ac.in/project/iwatchroad.

</details>


### [35] [Demeter: A Parametric Model of Crop Plant Morphology from the Real World](https://arxiv.org/abs/2510.16377)
*Tianhang Cheng,Albert J. Zhai,Evan Z. Chen,Rui Zhou,Yawen Deng,Zitong Li,Kejie Zhao,Janice Shiu,Qianyu Zhao,Yide Xu,Xinlei Wang,Yuan Shen,Sheng Wang,Lisa Ainsworth,Kaiyu Guan,Shenlong Wang*

Main category: cs.CV

TL;DR: Demeter是一个数据驱动的参数化植物形态模型，能够处理不同物种的拓扑结构变化，并建模三种形状变异来源：关节运动、子组件形状变化和非刚性变形。


<details>
  <summary>Details</summary>
Motivation: 当前存在强大的人类和动物3D参数化模型，但缺乏同等表达能力的植物建模方法。植物形态建模在农业、生态学和计算机图形学中具有重要应用价值。

Method: 开发了Demeter参数化模型，将植物形态的关键因素（拓扑、形状、关节运动和变形）编码为紧凑的学习表示。基于大规模大豆农场数据集进行训练和验证。

Result: 实验表明Demeter能够有效合成形状、重建结构并模拟生物物理过程。模型在作物植物建模方面表现出色。

Conclusion: Demeter为植物形态建模提供了强大的参数化框架，填补了现有建模方法的空白，在农业和生态学应用中具有广阔前景。

Abstract: Learning 3D parametric shape models of objects has gained popularity in
vision and graphics and has showed broad utility in 3D reconstruction,
generation, understanding, and simulation. While powerful models exist for
humans and animals, equally expressive approaches for modeling plants are
lacking. In this work, we present Demeter, a data-driven parametric model that
encodes key factors of a plant morphology, including topology, shape,
articulation, and deformation into a compact learned representation. Unlike
previous parametric models, Demeter handles varying shape topology across
various species and models three sources of shape variation: articulation,
subcomponent shape variation, and non-rigid deformation. To advance crop plant
modeling, we collected a large-scale, ground-truthed dataset from a soybean
farm as a testbed. Experiments show that Demeter effectively synthesizes
shapes, reconstructs structures, and simulates biophysical processes. Code and
data is available at https://tianhang-cheng.github.io/Demeter/.

</details>


### [36] [SPLite Hand: Sparsity-Aware Lightweight 3D Hand Pose Estimation](https://arxiv.org/abs/2510.16396)
*Yeh Keng Hao,Hsu Tzu Wei,Sun Min*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级框架，通过稀疏卷积、SPLite解码器和量化感知训练，在AR/VR边缘设备上实现了高效实时的手部姿态估计，在Raspberry Pi 5上达到2.98倍加速。


<details>
  <summary>Details</summary>
Motivation: 随着AR/VR设备的普及，边缘设备需要实时推理、低功耗和低延迟的深度学习模型，但现有方法难以平衡效率与性能。

Method: 采用编码器-解码器架构，在ResNet-18骨干网络上应用稀疏卷积利用手部姿态图像的稀疏性，提出SPLite解码器加速解码过程，并使用量化感知训练减少内存占用。

Result: 端到端效率提升42%，解码帧率在Raspberry Pi 5上提升3.1倍，量化后PA-MPJPE仅从9.0mm增至9.1mm，整体系统加速2.98倍。

Conclusion: 该方法在保持与先进方法相当精度的同时，显著提升了计算效率，适用于AR/VR边缘设备的实时手部姿态估计。

Abstract: With the increasing ubiquity of AR/VR devices, the deployment of deep
learning models on edge devices has become a critical challenge. These devices
require real-time inference, low power consumption, and minimal latency. Many
framework designers face the conundrum of balancing efficiency and performance.
We design a light framework that adopts an encoder-decoder architecture and
introduces several key contributions aimed at improving both efficiency and
accuracy. We apply sparse convolution on a ResNet-18 backbone to exploit the
inherent sparsity in hand pose images, achieving a 42% end-to-end efficiency
improvement. Moreover, we propose our SPLite decoder. This new architecture
significantly boosts the decoding process's frame rate by 3.1x on the Raspberry
Pi 5, while maintaining accuracy on par. To further optimize performance, we
apply quantization-aware training, reducing memory usage while preserving
accuracy (PA-MPJPE increases only marginally from 9.0 mm to 9.1 mm on
FreiHAND). Overall, our system achieves a 2.98x speed-up on a Raspberry Pi 5
CPU (BCM2712 quad-core Arm A76 processor). Our method is also evaluated on
compound benchmark datasets, demonstrating comparable accuracy to
state-of-the-art approaches while significantly enhancing computational
efficiency.

</details>


### [37] [REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting](https://arxiv.org/abs/2510.16410)
*Changyue Shi,Minghao Chen,Yiping Mao,Chuxiao Yang,Xinyuan Hu,Jiajun Ding,Zhou Yu*

Main category: cs.CV

TL;DR: REALM是一个创新的MLLM-agent框架，能够实现开放世界的基于推理的3D分割，无需大量3D特定后训练。通过3D高斯溅射表示和全局到局部空间定位策略，在多个基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决复杂人类指令与精确3D对象定位之间的差距，现有3D分割方法难以解释模糊的基于推理的指令，而2D视觉语言模型缺乏内在的3D空间理解。

Method: 在3D高斯溅射表示上直接进行分割，采用全局到局部空间定位策略：首先并行输入多个全局视图进行粗粒度定位，然后合成近距离新视图进行细粒度局部分割。

Result: 在LERF、3D-OVS和新提出的REALM3D基准测试中，REALM在解释显式和隐式指令方面取得了显著性能，并支持多种3D交互任务。

Conclusion: REALM框架在3D对象分割和交互任务中表现出实用性和多功能性，为复杂指令驱动的3D场景理解提供了有效解决方案。

Abstract: Bridging the gap between complex human instructions and precise 3D object
grounding remains a significant challenge in vision and robotics. Existing 3D
segmentation methods often struggle to interpret ambiguous, reasoning-based
instructions, while 2D vision-language models that excel at such reasoning lack
intrinsic 3D spatial understanding. In this paper, we introduce REALM, an
innovative MLLM-agent framework that enables open-world reasoning-based
segmentation without requiring extensive 3D-specific post-training. We perform
segmentation directly on 3D Gaussian Splatting representations, capitalizing on
their ability to render photorealistic novel views that are highly suitable for
MLLM comprehension. As directly feeding one or more rendered views to the MLLM
can lead to high sensitivity to viewpoint selection, we propose a novel
Global-to-Local Spatial Grounding strategy. Specifically, multiple global views
are first fed into the MLLM agent in parallel for coarse-level localization,
aggregating responses to robustly identify the target object. Then, several
close-up novel views of the object are synthesized to perform fine-grained
local segmentation, yielding accurate and consistent 3D masks. Extensive
experiments show that REALM achieves remarkable performance in interpreting
both explicit and implicit instructions across LERF, 3D-OVS, and our newly
introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly
supports a range of 3D interaction tasks, including object removal,
replacement, and style transfer, demonstrating its practical utility and
versatility. Project page: https://ChangyueShi.github.io/REALM.

</details>


### [38] [SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning](https://arxiv.org/abs/2510.16416)
*Xiaojun Guo,Runyu Zhou,Yifei Wang,Qi Zhang,Chenheng Zhang,Stefanie Jegelka,Xiaohan Wang,Jiajun Chai,Guojun Yin,Wei Lin,Yisen Wang*

Main category: cs.CV

TL;DR: SSL4RL是一个新颖的框架，利用自监督学习任务作为强化学习微调的可验证奖励来源，解决了视觉语言模型在视觉证据利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型往往无法充分利用视觉证据，要么依赖语言先验，要么使用文本捷径进行推理。强化学习可以调整模型行为，但缺乏可扩展可靠的奖励机制。

Method: 将自监督学习目标（如预测图像旋转或重建掩码补丁）转化为密集、自动的奖励信号，无需人工偏好数据或不可靠的AI评估器。

Result: SSL4RL显著提高了视觉中心和视觉语言推理基准的性能，并在图学习中也显示出显著增益。

Conclusion: SSL4RL建立了一个通用有效的范式，使用可验证的自监督目标来对齐多模态模型，为未来工作提供了新的设计原则。

Abstract: Vision-language models (VLMs) have shown remarkable abilities by integrating
large language models with visual inputs. However, they often fail to utilize
visual evidence adequately, either depending on linguistic priors in
vision-centric tasks or resorting to textual shortcuts during reasoning.
Although reinforcement learning (RL) can align models with desired behaviors,
its application to VLMs has been hindered by the lack of scalable and reliable
reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel
framework that leverages self-supervised learning (SSL) tasks as a source of
verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL
objectives-such as predicting image rotation or reconstructing masked
patches-into dense, automatic reward signals, eliminating the need for human
preference data or unreliable AI evaluators. Experiments show that SSL4RL
substantially improves performance on both vision-centric and vision-language
reasoning benchmarks. Furthermore, through systematic ablations, we identify
key factors-such as task difficulty, model scale, and semantic alignment with
the target domain-that influence the effectiveness of SSL4RL tasks, offering
new design principles for future work. We also demonstrate the framework's
generality by applying it to graph learning, where it yields significant gains.
SSL4RL establishes a versatile and effective paradigm for aligning multimodal
models using verifiable, self-supervised objectives.

</details>


### [39] [LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching](https://arxiv.org/abs/2510.16438)
*Aidyn Ubingazhibov,Rémi Pautrat,Iago Suárez,Shaohui Liu,Marc Pollefeys,Viktor Larsson*

Main category: cs.CV

TL;DR: LightGlueStick是一种轻量级的点和线段匹配器，通过注意力线消息传递(ALMP)机制显式暴露线段连通性，在保持高性能的同时实现实时应用。


<details>
  <summary>Details</summary>
Motivation: 传统点线匹配独立处理，GlueStick虽能联合匹配但架构较重无法实时应用。受点匹配进展启发，需要开发轻量级联合匹配器。

Method: 提出Attentional Line Message Passing(ALMP)组件，显式暴露线段连通性，实现节点间高效通信的轻量架构。

Result: 在多个基准测试中达到新的最先进水平，代码已开源。

Conclusion: LightGlueStick通过轻量设计成功解决了联合点线匹配的实时应用问题，在性能与效率间取得良好平衡。

Abstract: Lines and points are complementary local features, whose combination has
proven effective for applications such as SLAM and Structure-from-Motion. The
backbone of these pipelines are the local feature matchers, establishing
correspondences across images. Traditionally, point and line matching have been
treated as independent tasks. Recently, GlueStick proposed a GNN-based network
that simultaneously operates on points and lines to establish matches. While
running a single joint matching reduced the overall computational complexity,
the heavy architecture prevented real-time applications or deployment to edge
devices.
  Inspired by recent progress in point matching, we propose LightGlueStick, a
lightweight matcher for points and line segments. The key novel component in
our architecture is the Attentional Line Message Passing (ALMP), which
explicitly exposes the connectivity of the lines to the network, allowing for
efficient communication between nodes. In thorough experiments we show that
LightGlueStick establishes a new state-of-the-art across different benchmarks.
The code is available at https://github.com/aubingazhib/LightGlueStick.

</details>


### [40] [EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning](https://arxiv.org/abs/2510.16442)
*Haoran Sun,Chen Cai,Huiping Zhuang,Kong Aik Lee,Lap-Pui Chau,Yi Wang*

Main category: cs.CV

TL;DR: 本文提出了可解释深度伪造视频检测任务，并设计了EDVD-LLaMA多模态大语言模型推理框架，通过时空特征提取和细粒度思维链机制，在实现准确检测的同时提供可追溯的推理过程和可信解释。


<details>
  <summary>Details</summary>
Motivation: 传统深度伪造视频检测方法存在原理不透明、泛化能力不足的问题，无法应对不断演变的伪造技术，迫切需要能够识别伪造内容并提供可验证推理解释的检测器。

Method: 1. 时空细微信息标记化提取并融合全局和局部跨帧深度伪造特征；2. 构建细粒度多模态思维链机制，引入面部特征数据作为硬约束；3. 建立可解释推理FF++基准数据集，支持推理和检测的双重监督。

Result: 实验表明EDVD-LLaMA在检测精度、可解释性以及处理跨伪造方法和跨数据集场景方面表现出卓越性能和鲁棒性，相比之前的方法提供了更可解释且更优的解决方案。

Conclusion: 该方法为深度伪造视频检测提供了一个可解释且性能优越的解决方案，能够有效应对不断演变的伪造技术挑战。

Abstract: The rapid development of deepfake video technology has not only facilitated
artistic creation but also made it easier to spread misinformation. Traditional
deepfake video detection (DVD) methods face issues such as a lack of
transparency in their principles and insufficient generalization capabilities
to cope with evolving forgery techniques. This highlights an urgent need for
detectors that can identify forged content and provide verifiable reasoning
explanations. This paper proposes the explainable deepfake video detection
(EDVD) task and designs the EDVD-LLaMA multimodal, a large language model
(MLLM) reasoning framework, which provides traceable reasoning processes
alongside accurate detection results and trustworthy explanations. Our approach
first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT)
to extract and fuse global and local cross-frame deepfake features, providing
rich spatio-temporal semantic information input for MLLM reasoning. Second, we
construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which
introduces facial feature data as hard constraints during the reasoning process
to achieve pixel-level spatio-temporal video localization, suppress
hallucinated outputs, and enhance the reliability of the chain of thought. In
addition, we build an Explainable Reasoning FF++ benchmark dataset
(ER-FF++set), leveraging structured data to annotate videos and ensure quality
control, thereby supporting dual supervision for reasoning and detection.
Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding
performance and robustness in terms of detection accuracy, explainability, and
its ability to handle cross-forgery methods and cross-dataset scenarios.
Compared to previous DVD methods, it provides a more explainable and superior
solution. The source code and dataset will be publicly available.

</details>


### [41] [RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba](https://arxiv.org/abs/2510.16444)
*Kunyu Peng,Di Wen,Jia Fu,Jiamin Wu,Kailun Yang,Junwei Zheng,Ruiping Liu,Yufan Chen,Yuqian Fu,Danda Pani Paudel,Luc Van Gool,Rainer Stiefelhagen*

Main category: cs.CV

TL;DR: RAVAR任务旨在通过自然语言描述识别特定人物的细粒度原子级动作。作者扩展了RefAVA数据集为RefAVA++，并提出RefAtomNet++框架，通过多层级语义对齐交叉注意力和多轨迹Mamba建模，在部分关键词、场景属性和整体句子级别实现更好的跨模态对齐，取得了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 传统动作识别和检测任务无法满足复杂多人场景中基于语言指导的精确动作理解需求。现有的RefAtomNet模型在跨模态信息对齐和检索方面存在局限，导致目标人物定位和细粒度动作预测性能不佳。

Method: 提出RefAtomNet++框架，采用多层级语义对齐交叉注意力机制和多轨迹Mamba建模。在部分关键词和场景属性级别，通过动态选择最近视觉空间标记构建扫描轨迹；设计多层级语义对齐交叉注意力策略，有效聚合不同语义层级的时空标记。

Result: RefAtomNet++在RefAVA++数据集上建立了新的state-of-the-art结果，显著提升了语言引导的原子级视频动作识别性能。

Conclusion: RefAtomNet++通过改进的跨模态标记聚合机制，有效解决了复杂多人场景中基于语言描述的精确动作识别问题，为交互式人类动作分析提供了更强大的解决方案。

Abstract: Referring Atomic Video Action Recognition (RAVAR) aims to recognize
fine-grained, atomic-level actions of a specific person of interest conditioned
on natural language descriptions. Distinct from conventional action recognition
and detection tasks, RAVAR emphasizes precise language-guided action
understanding, which is particularly critical for interactive human action
analysis in complex multi-person scenarios. In this work, we extend our
previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million
frames and >75.1k annotated persons in total. We benchmark this dataset using
baselines from multiple related domains, including atomic action localization,
video question answering, and text-video retrieval, as well as our earlier
model, RefAtomNet. Although RefAtomNet surpasses other baselines by
incorporating agent attention to highlight salient features, its ability to
align and retrieve cross-modal information remains limited, leading to
suboptimal performance in localizing the target person and predicting
fine-grained actions. To overcome the aforementioned limitations, we introduce
RefAtomNet++, a novel framework that advances cross-modal token aggregation
through a multi-hierarchical semantic-aligned cross-attention mechanism
combined with multi-trajectory Mamba modeling at the partial-keyword,
scene-attribute, and holistic-sentence levels. In particular, scanning
trajectories are constructed by dynamically selecting the nearest visual
spatial tokens at each timestep for both partial-keyword and scene-attribute
levels. Moreover, we design a multi-hierarchical semantic-aligned
cross-attention strategy, enabling more effective aggregation of spatial and
temporal tokens across different semantic hierarchies. Experiments show that
RefAtomNet++ establishes new state-of-the-art results. The dataset and code are
released at https://github.com/KPeng9510/refAVA2.

</details>


### [42] [Enhancing Rotated Object Detection via Anisotropic Gaussian Bounding Box and Bhattacharyya Distance](https://arxiv.org/abs/2510.16445)
*Chien Thai,Mai Xuan Trang,Huong Ninh,Hoang Hiep Ly,Anh Son Le*

Main category: cs.CV

TL;DR: 提出一种改进的损失函数，通过高斯边界框表示和Bhattacharyya距离提升旋转物体检测的准确性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 传统目标检测框架在旋转物体检测中表现不佳，无法有效捕捉方向变化，特别是在航空影像、遥感等应用中

Method: 使用各向异性高斯表示解决类方形物体的各向同性方差问题，结合旋转不变损失函数来捕捉旋转物体的几何特性

Result: 在多个旋转物体检测器上集成该损失函数后，平均精度指标相比现有方法有显著提升

Conclusion: 该方法为旋转物体检测建立了新的基准，对需要精确物体定位的应用具有重要价值

Abstract: Detecting rotated objects accurately and efficiently is a significant
challenge in computer vision, particularly in applications such as aerial
imagery, remote sensing, and autonomous driving. Although traditional object
detection frameworks are effective for axis-aligned objects, they often
underperform in scenarios involving rotated objects due to their limitations in
capturing orientation variations. This paper introduces an improved loss
function aimed at enhancing detection accuracy and robustness by leveraging the
Gaussian bounding box representation and Bhattacharyya distance. In addition,
we advocate for the use of an anisotropic Gaussian representation to address
the issues associated with isotropic variance in square-like objects. Our
proposed method addresses these challenges by incorporating a
rotation-invariant loss function that effectively captures the geometric
properties of rotated objects. We integrate this proposed loss function into
state-of-the-art deep learning-based rotated object detection detectors, and
extensive experiments demonstrated significant improvements in mean Average
Precision metrics compared to existing methods. The results highlight the
potential of our approach to establish new benchmark in rotated object
detection, with implications for a wide range of applications requiring precise
and reliable object localization irrespective of orientation.

</details>


### [43] [VIPAMIN: Visual Prompt Initialization via Embedding Selection and Subspace Expansion](https://arxiv.org/abs/2510.16446)
*Jaekyun Park,Hye Won Chung*

Main category: cs.CV

TL;DR: VIPAMIN是一种视觉提示初始化策略，通过将提示与嵌入空间中的语义信息区域对齐，并注入超出预训练子空间的新表示方向，来增强自监督模型的适应能力。


<details>
  <summary>Details</summary>
Motivation: 在大规模基础模型时代，完全微调预训练网络对每个下游任务往往资源消耗过大。现有的视觉提示调优方法在应用于自监督骨干网络时，往往无法专门化提示或丰富表示空间，特别是在具有挑战性的任务和数据稀缺设置中。

Method: VIPAMIN通过两个关键步骤：(1) 将提示与嵌入空间中的语义信息区域对齐；(2) 注入超出预训练子空间的新表示方向。该方法仅需单次前向传播和轻量级操作。

Result: VIPAMIN在不同任务和数据集规模上一致提高了性能，在视觉提示调优方面达到了新的最先进水平。

Conclusion: VIPAMIN是一种简单而有效的视觉提示初始化策略，能够显著提升自监督模型在下游任务中的适应能力，特别是在数据稀缺和挑战性任务场景中表现优异。

Abstract: In the era of large-scale foundation models, fully fine-tuning pretrained
networks for each downstream task is often prohibitively resource-intensive.
Prompt tuning offers a lightweight alternative by introducing tunable prompts
while keeping the backbone frozen. However, existing visual prompt tuning
methods often fail to specialize the prompts or enrich the representation
space--especially when applied to self-supervised backbones. We show that these
limitations become especially pronounced in challenging tasks and data-scarce
settings, where effective adaptation is most critical. In this work, we
introduce VIPAMIN, a visual prompt initialization strategy that enhances
adaptation of self-supervised models by (1) aligning prompts with semantically
informative regions in the embedding space, and (2) injecting novel
representational directions beyond the pretrained subspace. Despite its
simplicity--requiring only a single forward pass and lightweight
operations--VIPAMIN consistently improves performance across diverse tasks and
dataset sizes, setting a new state of the art in visual prompt tuning. Our code
is available at https://github.com/iamjaekyun/vipamin.

</details>


### [44] [Instance-Aware Pseudo-Labeling and Class-Focused Contrastive Learning for Weakly Supervised Domain Adaptive Segmentation of Electron Microscopy](https://arxiv.org/abs/2510.16450)
*Shan Xiong,Jiabao Chen,Ye Wang,Jialin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种弱监督域自适应方法，用于电子显微镜图像中线粒体的高效分割，通过多任务学习和交叉教学机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决电子显微镜图像中线粒体分割中标注成本高的问题，传统无监督域自适应方法在实际应用中性能较低，需要更有效的弱监督方法。

Method: 采用多任务学习框架，结合分割和中心检测任务，引入交叉教学机制和类聚焦跨域对比学习，并提出实例感知伪标签选择策略。

Result: 在挑战性数据集上的验证表明，该方法优于现有的无监督和弱监督域自适应方法，显著缩小了与监督上限的性能差距。

Conclusion: 该方法在弱监督和无监督设置下均表现出色，为生物医学图像分割提供了一种高效的解决方案。

Abstract: Annotation-efficient segmentation of the numerous mitochondria instances from
various electron microscopy (EM) images is highly valuable for biological and
neuroscience research. Although unsupervised domain adaptation (UDA) methods
can help mitigate domain shifts and reduce the high costs of annotating each
domain, they typically have relatively low performance in practical
applications. Thus, we investigate weakly supervised domain adaptation (WDA)
that utilizes additional sparse point labels on the target domain, which
require minimal annotation effort and minimal expert knowledge. To take full
use of the incomplete and imprecise point annotations, we introduce a multitask
learning framework that jointly conducts segmentation and center detection with
a novel cross-teaching mechanism and class-focused cross-domain contrastive
learning. While leveraging unlabeled image regions is essential, we introduce
segmentation self-training with a novel instance-aware pseudo-label (IPL)
selection strategy. Unlike existing methods that typically rely on pixel-wise
pseudo-label filtering, the IPL semantically selects reliable and diverse
pseudo-labels with the help of the detection task. Comprehensive validations
and comparisons on challenging datasets demonstrate that our method outperforms
existing UDA and WDA methods, significantly narrowing the performance gap with
the supervised upper bound. Furthermore, under the UDA setting, our method also
achieves substantial improvements over other UDA techniques.

</details>


### [45] [NavQ: Learning a Q-Model for Foresighted Vision-and-Language Navigation](https://arxiv.org/abs/2510.16457)
*Peiran Xu,Xicheng Gong,Yadong MU*

Main category: cs.CV

TL;DR: 提出了一种前瞻性的视觉语言导航方法，通过Q-learning学习场景布局和物体关系知识，结合未来信息进行A*搜索优化导航决策。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖历史信息做决策，忽略了行动的未来影响和长期结果，需要开发具有前瞻性的导航智能体。

Method: 使用Q-learning在大规模无标签轨迹数据上训练Q模型，生成描述行动潜在未来信息的Q特征，通过跨模态未来编码器将Q特征与导航指令结合，采用A*搜索策略探索更可能到达目的地的区域。

Result: 在广泛使用的目标导向VLN数据集上的大量实验验证了该方法的有效性。

Conclusion: 该方法通过引入未来信息建模和前瞻性搜索策略，显著提升了目标导向视觉语言导航的性能。

Abstract: In this work we concentrate on the task of goal-oriented Vision-and-Language
Navigation (VLN). Existing methods often make decisions based on historical
information, overlooking the future implications and long-term outcomes of the
actions. In contrast, we aim to develop a foresighted agent. Specifically, we
draw upon Q-learning to train a Q-model using large-scale unlabeled trajectory
data, in order to learn the general knowledge regarding the layout and object
relations within indoor scenes. This model can generate a Q-feature, analogous
to the Q-value in traditional Q-network, for each candidate action, which
describes the potential future information that may be observed after taking
the specific action. Subsequently, a cross-modal future encoder integrates the
task-agnostic Q-feature with navigation instructions to produce a set of action
scores reflecting future prospects. These scores, when combined with the
original scores based on history, facilitate an A*-style searching strategy to
effectively explore the regions that are more likely to lead to the
destination. Extensive experiments conducted on widely used goal-oriented VLN
datasets validate the effectiveness of the proposed method.

</details>


### [46] [HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars](https://arxiv.org/abs/2510.16463)
*Haocheng Tang,Ruoke Yan,Xinhui Yin,Qi Zhang,Xinfeng Zhang,Siwei Ma,Wen Gao,Chuanmin Jia*

Main category: cs.CV

TL;DR: HGC-Avatar是一个用于动态3D虚拟人高效传输和高质量渲染的分层高斯压缩框架，通过结构层和运动层的解耦设计，结合人脸注意力机制，在低码率下显著提升视觉质量和压缩效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于3D高斯泼溅的压缩方法缺乏人体先验知识，导致在数字人编码传输中码率效率和重建质量不理想，难以应用于可流式传输的3D虚拟人系统。

Method: 将高斯表示解耦为结构层（通过StyleUNet生成器将姿态映射到高斯）和运动层（利用SMPL-X模型紧凑表示时序姿态变化），支持分层压缩、渐进解码和可控渲染，并加入人脸注意力机制保护身份和表情细节。

Result: 实验结果表明HGC-Avatar为快速3D虚拟人渲染提供了可流式传输解决方案，在视觉质量和压缩效率上均显著优于现有方法。

Conclusion: 该分层设计结合人体先验和人脸注意力机制，有效解决了动态3D虚拟人传输中的码率效率和质量问题，具有实际应用价值。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast,
photorealistic rendering of dynamic 3D scenes, showing strong potential in
immersive communication. However, in digital human encoding and transmission,
the compression methods based on general 3DGS representations are limited by
the lack of human priors, resulting in suboptimal bitrate efficiency and
reconstruction quality at the decoder side, which hinders their application in
streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical
Gaussian Compression framework designed for efficient transmission and
high-quality rendering of dynamic avatars. Our method disentangles the Gaussian
representation into a structural layer, which maps poses to Gaussians via a
StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model
to represent temporal pose variations compactly and semantically. This
hierarchical design supports layer-wise compression, progressive decoding, and
controllable rendering from diverse pose inputs such as video sequences or
text. Since people are most concerned with facial realism, we incorporate a
facial attention mechanism during StyleUNet training to preserve identity and
expression details under low-bitrate constraints. Experimental results
demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar
rendering, while significantly outperforming prior methods in both visual
quality and compression efficiency.

</details>


### [47] [PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies](https://arxiv.org/abs/2510.16505)
*Lukas Selch,Yufang Hou,M. Jehanzeb Mirza,Sivan Doveh,James Glass,Rogerio Feris,Wei Lin*

Main category: cs.CV

TL;DR: PRISMM-Bench是首个基于真实科学论文中审稿人标记的不一致性的基准测试，用于评估大型多模态模型在检测和解决跨文本、图表、方程等多模态不一致性方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试要么孤立单一模态，要么依赖合成错误，无法捕捉真实科学论文中的多模态复杂性。模型需要能够可靠地理解和推理论文中的多模态内容，以确保科学研究的清晰性、可重复性和可信度。

Method: 通过多阶段流程（包括审阅挖掘、LLM辅助过滤和人工验证）从242篇论文中收集262个不一致性。设计了三个任务：不一致性识别、修正和配对匹配，并引入结构化JSON答案表示以减少语言偏见。

Result: 对21个领先LMM的评估显示性能极低（26.1-54.2%），包括大型开源模型（GLM-4.5V 106B, InternVL3 78B）和专有模型（Gemini 2.5 Pro, GPT-5）。

Conclusion: 结果突显了多模态科学推理的挑战性，强调了开发可信科学助手的必要性。

Abstract: Large Multimodal Models (LMMs) are increasingly applied to scientific
research, yet it remains unclear whether they can reliably understand and
reason over the multimodal complexity of papers. A central challenge lies in
detecting and resolving inconsistencies across text, figures, tables, and
equations, issues that are often subtle, domain-specific, and ultimately
undermine clarity, reproducibility, and trust. Existing benchmarks overlook
this issue, either isolating single modalities or relying on synthetic errors
that fail to capture real-world complexity. We introduce PRISMM-Bench
(Peer-Review-sourced Inconsistency Set for Multimodal Models), the first
benchmark grounded in real reviewer-flagged inconsistencies in scientific
papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering
and human verification, we curate 262 inconsistencies from 242 papers. Based on
this set, we design three tasks, namely inconsistency identification, remedy
and pair matching, which assess a model's capacity to detect, correct, and
reason over inconsistencies across different modalities. Furthermore, to
address the notorious problem of choice-only shortcuts in multiple-choice
evaluation, where models exploit answer patterns without truly understanding
the question, we further introduce structured JSON-based answer representations
that minimize linguistic biases by reducing reliance on superficial stylistic
cues. We benchmark 21 leading LMMs, including large open-weight models
(GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5
with high reasoning). Results reveal strikingly low performance (26.1-54.2%),
underscoring the challenge of multimodal scientific reasoning and motivating
progress towards trustworthy scientific assistants.

</details>


### [48] [OOS-DSD: Improving Out-of-stock Detection in Retail Images using Auxiliary Tasks](https://arxiv.org/abs/2510.16508)
*Franko Šikić,Sven Lončarić*

Main category: cs.CV

TL;DR: OOS-DSD是一种基于深度学习的缺货检测方法，通过辅助学习（同时检测缺货、分割产品和估计场景深度）提升性能，在mAP指标上比现有最优方法提高1.8%。


<details>
  <summary>Details</summary>
Motivation: 缺货检测是零售验证中的重要过程，需要准确推断货架上产品的可用性。现有方法存在性能瓶颈，需要更有效的解决方案。

Method: 基于YOLOv8目标检测架构，增加卷积分支进行多任务学习：缺货检测、产品分割和深度估计。深度估计分支使用Depth Anything V2模型生成的伪标签进行训练，并提出深度归一化方法稳定训练过程。

Result: 实验结果表明，该方法在mAP指标上比现有最优方法提高1.8%。消融研究证实辅助学习使mAP提高3.7%，深度归一化方法使mAP提高4.2%。

Conclusion: OOS-DSD通过多任务辅助学习和深度归一化技术有效提升了缺货检测性能，证明了辅助学习策略在零售场景中的有效性。

Abstract: Out-of-stock (OOS) detection is a very important retail verification process
that aims to infer the unavailability of products in their designated areas on
the shelf. In this paper, we introduce OOS-DSD, a novel deep learning-based
method that advances OOS detection through auxiliary learning. In particular,
we extend a well-established YOLOv8 object detection architecture with
additional convolutional branches to simultaneously detect OOS, segment
products, and estimate scene depth. While OOS detection and product
segmentation branches are trained using ground truth data, the depth estimation
branch is trained using pseudo-labeled annotations produced by the
state-of-the-art (SOTA) depth estimation model Depth Anything V2. Furthermore,
since the aforementioned pseudo-labeled depth estimates display relative depth,
we propose an appropriate depth normalization procedure that stabilizes the
training process. The experimental results show that the proposed method
surpassed the performance of the SOTA OOS detection methods by 1.8% of the mean
average precision (mAP). In addition, ablation studies confirm the
effectiveness of auxiliary learning and the proposed depth normalization
procedure, with the former increasing mAP by 3.7% and the latter by 4.2%.

</details>


### [49] [Image Categorization and Search via a GAT Autoencoder and Representative Models](https://arxiv.org/abs/2510.16514)
*Duygu Sap,Martin Lotz,Connor Mattinson*

Main category: cs.CV

TL;DR: 提出了一种基于图注意力网络（GAT）自编码器的图像分类和检索方法，通过构建图像和类别的代表模型来实现分类和检索。


<details>
  <summary>Details</summary>
Motivation: 传统的图像分类和检索方法可能无法充分捕捉图像之间的复杂关系，需要一种能够利用图像间相似性关系并构建上下文感知表示的方法。

Method: 使用图结构表示图像（节点）和相似性关系（边），通过GAT自编码器构建上下文感知的潜在表示，然后从这些嵌入中获取类别代表，通过比较查询图像代表与类别代表进行分类，并在识别出的类别中检索最相似的图像。

Result: 通过实验证明了这种基于代表的方法的有效性，包括GAT自编码器和标准基于特征的技术。

Conclusion: 提出的代表中心方法能够有效利用图结构和注意力机制来提升图像分类和检索的性能。

Abstract: We propose a method for image categorization and retrieval that leverages
graphs and a graph attention network (GAT)-based autoencoder. Our approach is
representative-centric, that is, we execute the categorization and retrieval
process via the representative models we construct for the images and image
categories. We utilize a graph where nodes represent images (or their
representatives) and edges capture similarity relationships. GAT highlights
important features and relationships between images, enabling the autoencoder
to construct context-aware latent representations that capture the key features
of each image relative to its neighbors. We obtain category representatives
from these embeddings and categorize a query image by comparing its
representative to the category representatives. We then retrieve the most
similar image to the query image within its identified category. We demonstrate
the effectiveness of our representative-centric approach through experiments
with both the GAT autoencoders and standard feature-based techniques.

</details>


### [50] [Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions](https://arxiv.org/abs/2510.16540)
*Jihoon Kwon,Kyle Min,Jy-yong Sohn*

Main category: cs.CV

TL;DR: READ是一种针对视觉语言模型的微调方法，通过添加token级重建和句子级对齐两个辅助目标来增强组合推理能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在组合推理方面表现不佳，主要原因是文本编码器倾向于关注单个单词而非词间关系，对比训练主要将单词与视觉对象对齐，限制了模型理解结构化关系的能力。

Method: READ方法在对比学习基础上添加两个辅助目标：(1) token级重建目标：使用冻结的预训练解码器基于原始caption的嵌入重建替代caption；(2) 句子级对齐目标：在嵌入空间中显式对齐改写句子。

Result: READ-CLIP在五个主要组合推理基准测试中达到最先进性能，比最强传统微调基线提升高达4.1%。应用到现有CLIP变体（如NegCLIP和FSC-CLIP）也能提升性能。

Conclusion: 重建和对齐目标具有互补优势：重建目标鼓励编码器捕捉caption内词间关系，对齐目标确保不同表达的改写句子具有一致表示。

Abstract: Despite recent advances, vision-language models trained with standard
contrastive objectives still struggle with compositional reasoning -- the
ability to understand structured relationships between visual and linguistic
elements. This shortcoming is largely due to the tendency of the text encoder
to focus on individual words rather than their relations, a limitation
reinforced by contrastive training that primarily aligns words with visual
objects. In this paper, we introduce REconstruction and Alignment of text
Descriptions (READ), a fine-tuning method designed to enhance compositional
reasoning by adding two auxiliary objectives to the contrastive learning: (1) a
token-level reconstruction objective, where a frozen pre-trained decoder
reconstructs alternative captions based on the embedding of the original
caption; and (2) a sentence-level alignment objective, which explicitly aligns
paraphrased sentences in the embedding space. We show that READ-CLIP, a model
derived by applying the READ method to the pre-trained CLIP model, achieves the
state-of-the-art performance across five major compositional reasoning
benchmarks, outperforming the strongest conventional fine-tuning baseline by up
to 4.1%. Furthermore, applying the READ to existing CLIP variants (including
NegCLIP and FSC-CLIP) also improves performance on these benchmarks.
Quantitative and qualitative analyses reveal that our proposed objectives --
reconstruction and alignment -- offer complementary benefits: the former
encourages the encoder to capture relationships between words within a caption,
while the latter ensures consistent representations for paraphrases expressed
with different wording.

</details>


### [51] [Watch Where You Move: Region-aware Dynamic Aggregation and Excitation for Gait Recognition](https://arxiv.org/abs/2510.16541)
*Binyuan Huang,Yongdong Luo,Xianda Guo,Xiawu Zheng,Zheng Zhu,Jiahui Pan,Chengju Zhou*

Main category: cs.CV

TL;DR: 提出了GaitRDAE框架，通过动态搜索运动区域和自适应时间尺度分配来提升步态识别性能


<details>
  <summary>Details</summary>
Motivation: 现有方法使用预定义区域进行时序建模，难以适应动态变化的运动区域和其特定模式

Method: 包含两个核心模块：RDA模块动态搜索每个区域的最佳时序感受野，RDE模块强调稳定行为模式运动区域的学习

Result: 在多个基准数据集上达到了最先进的性能

Conclusion: GaitRDAE框架能有效处理动态运动区域并适应其特定模式，显著提升步态识别准确率

Abstract: Deep learning-based gait recognition has achieved great success in various
applications. The key to accurate gait recognition lies in considering the
unique and diverse behavior patterns in different motion regions, especially
when covariates affect visual appearance. However, existing methods typically
use predefined regions for temporal modeling, with fixed or equivalent temporal
scales assigned to different types of regions, which makes it difficult to
model motion regions that change dynamically over time and adapt to their
specific patterns. To tackle this problem, we introduce a Region-aware Dynamic
Aggregation and Excitation framework (GaitRDAE) that automatically searches for
motion regions, assigns adaptive temporal scales and applies corresponding
attention. Specifically, the framework includes two core modules: the
Region-aware Dynamic Aggregation (RDA) module, which dynamically searches the
optimal temporal receptive field for each region, and the Region-aware Dynamic
Excitation (RDE) module, which emphasizes the learning of motion regions
containing more stable behavior patterns while suppressing attention to static
regions that are more susceptible to covariates. Experimental results show that
GaitRDAE achieves state-of-the-art performance on several benchmark datasets.

</details>


### [52] [Fit for Purpose? Deepfake Detection in the Real World](https://arxiv.org/abs/2510.16556)
*Guangyu Lin,Li Lin,Christina P. Walker,Daniel S. Schiff,Shu Hu*

Main category: cs.CV

TL;DR: 本文提出了首个基于真实世界政治深度伪造事件的基准测试，发现现有检测器在真实政治深度伪造内容上表现不佳，呼吁开发更具政治背景的检测框架。


<details>
  <summary>Details</summary>
Motivation: AI生成内容的快速扩散加剧了政治深度伪造的风险，但现有检测模型大多基于实验室合成数据训练，缺乏对真实世界政治深度伪造的泛化能力。

Method: 基于政治深度伪造事件数据库构建系统基准，对学术界、政府和工业界的最先进深度伪造检测器进行系统性评估。

Result: 学术界和政府开发的检测器表现相对较差，付费工具性能稍好但所有检测器都难以有效泛化到真实政治深度伪造，且对简单操作（尤其是视频领域）易受攻击。

Conclusion: 需要开发更具政治背景的深度伪造检测框架，以在真实世界环境中更好地保护公众。

Abstract: The rapid proliferation of AI-generated content, driven by advances in
generative adversarial networks, diffusion models, and multimodal large
language models, has made the creation and dissemination of synthetic media
effortless, heightening the risks of misinformation, particularly political
deepfakes that distort truth and undermine trust in political institutions. In
turn, governments, research institutions, and industry have strongly promoted
deepfake detection initiatives as solutions. Yet, most existing models are
trained and validated on synthetic, laboratory-controlled datasets, limiting
their generalizability to the kinds of real-world political deepfakes
circulating on social platforms that affect the public. In this work, we
introduce the first systematic benchmark based on the Political Deepfakes
Incident Database, a curated collection of real-world political deepfakes
shared on social media since 2018. Our study includes a systematic evaluation
of state-of-the-art deepfake detectors across academia, government, and
industry. We find that the detectors from academia and government perform
relatively poorly. While paid detection tools achieve relatively higher
performance than free-access models, all evaluated detectors struggle to
generalize effectively to authentic political deepfakes, and are vulnerable to
simple manipulations, especially in the video domain. Results urge the need for
politically contextualized deepfake detection frameworks to better safeguard
the public in real-world settings.

</details>


### [53] [SHIELD: Suppressing Hallucinations In LVLM Encoders via Bias and Vulnerability Defense](https://arxiv.org/abs/2510.16596)
*Yiyang Huang,Liang Shi,Yitian Zhang,Yi Xu,Yun Fu*

Main category: cs.CV

TL;DR: 本文提出了SHIELD框架，首次将LVLM中的物体幻觉问题追溯到视觉编码器，并提出三种无训练策略来缓解幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在跨模态任务中表现出色，但物体幻觉问题（模型生成看似合理但不准确的物体描述）仍然是一个重大挑战。与以往关注LLM组件的研究不同，本文首次将LVLM幻觉问题追溯到视觉编码器。

Method: 提出SHIELD框架，包含三种无训练策略：1）重新加权视觉token以减少统计偏差；2）引入噪声衍生token来对抗固有偏差；3）应用对抗攻击和对比解码来解决脆弱性问题。

Result: 实验表明SHIELD在多种基准测试和LVLM家族中有效缓解了物体幻觉问题，同时在通用LVLM基准测试上表现出色，展示了其广泛适用性。

Conclusion: SHIELD框架成功识别并解决了视觉编码器导致的物体幻觉问题，为LVLM的可靠性提升提供了有效解决方案。

Abstract: Large Vision-Language Models (LVLMs) excel in diverse cross-modal tasks.
However, object hallucination, where models produce plausible but inaccurate
object descriptions, remains a significant challenge. In contrast to previous
work focusing on LLM components, this paper is the first to trace LVLM
hallucinations to visual encoders and identifies three key issues: statistical
bias, inherent bias, and vulnerability. To address these challenges, we propose
SHIELD, a training-free framework that mitigates hallucinations through three
strategies: re-weighting visual tokens to reduce statistical bias, introducing
noise-derived tokens to counter inherent bias, and applying adversarial attacks
with contrastive decoding to address vulnerability. Experiments demonstrate
that SHIELD effectively mitigates object hallucinations across diverse
benchmarks and LVLM families. Moreover, SHIELD achieves strong performance on
the general LVLM benchmark, highlighting its broad applicability. Code will be
released.

</details>


### [54] [VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.16598)
*Jiaying Zhu,Yurui Zhu,Xin Lu,Wenrui Yan,Dong Li,Kunlin Liu,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: VisionSelector是一种轻量级即插即用的视觉token压缩框架，通过可微分Top-K机制和课程退火策略实现自适应token选择，在各种压缩率下保持高性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型中高分辨率图像或多图像输入产生的大量视觉token带来的计算和内存瓶颈问题，避免传统压缩方法因启发式规则而丢弃关键信息或产生注意力偏差。

Method: 提出VisionSelector评分器模块，与MLLM主干解耦，包含可微分Top-K机制和课程退火策略，实现端到端可学习的token压缩决策过程。

Result: 仅需12.85M可训练参数，在30%保留预算下保持MME基准100%准确率，在10%保留预算下比先前方法提升12.14%，预填充速度提升一倍。

Conclusion: VisionSelector在各种压缩预算下均表现出优越性能，能够自适应识别关键token，实现高效且通用的视觉token压缩。

Abstract: Multimodal Large Language Models (MLLMs) encounter significant computational
and memory bottlenecks from the massive number of visual tokens generated by
high-resolution images or multi-image inputs. Previous token compression
techniques are often constrained by heuristic rules that risk discarding
critical information. They may suffer from biases, such as attention sinks,
that lead to sharp performance drops under aggressive compression ratios. To
address these limitations, we reformulate token compression as a lightweight
plug-and-play framework that reformulates token compression into an end-to-end
learnable decision process. To be specific, we propose VisionSelector, a scorer
module decoupled from the MLLM backbone that incorporates a differentiable
Top-K mechanism and a curriculum annealing strategy to bridge the
training-inference gap, enabling efficient and adaptive token selection various
arbitrary compression rates. Remarkably lightweight with only 12.85M trainable
parameters, VisionSelector demonstrates generalization across various
compression rates and adaptively identifying critical tokens. This leads to
superior performance across all compression budgets, evidenced by preserving
100% accuracy on MME with 30% retention budget, outperforming prior methods by
12.14% at 10% retention budget, and doubling prefill speed. Our code is
available at https://github.com/JulietChoo/VisionSelector .

</details>


### [55] [A Deep Learning Framework for Real-Time Image Processing in Medical Diagnostics: Enhancing Accuracy and Speed in Clinical Applications](https://arxiv.org/abs/2510.16611)
*Melika Filvantorkaman,Maral Filvan Torkaman*

Main category: cs.CV

TL;DR: 提出一个集成多种先进神经网络架构的深度学习框架，用于实时医学图像分析，在多个成像模态上实现高精度和快速推理。


<details>
  <summary>Details</summary>
Motivation: 传统医学图像处理技术缺乏实时临床使用所需的精度、鲁棒性和速度，需要克服这些限制来提升诊断效率和准确性。

Method: 整合U-Net、EfficientNet和Transformer等神经网络架构，结合模型剪枝、量化和GPU加速等实时优化策略，支持边缘设备、本地服务器和云基础设施的灵活部署。

Result: 在公共基准数据集上达到分类准确率92%以上、分割Dice分数超过91%、推理时间低于80毫秒的先进性能。

Conclusion: 该框架能显著加速诊断工作流程，减轻临床医生工作负担，支持在时间敏感的医疗环境中可信赖的AI集成。

Abstract: Medical imaging plays a vital role in modern diagnostics; however,
interpreting high-resolution radiological data remains time-consuming and
susceptible to variability among clinicians. Traditional image processing
techniques often lack the precision, robustness, and speed required for
real-time clinical use. To overcome these limitations, this paper introduces a
deep learning framework for real-time medical image analysis designed to
enhance diagnostic accuracy and computational efficiency across multiple
imaging modalities, including X-ray, CT, and MRI. The proposed system
integrates advanced neural network architectures such as U-Net, EfficientNet,
and Transformer-based models with real-time optimization strategies including
model pruning, quantization, and GPU acceleration. The framework enables
flexible deployment on edge devices, local servers, and cloud infrastructures,
ensuring seamless interoperability with clinical systems such as PACS and EHR.
Experimental evaluations on public benchmark datasets demonstrate
state-of-the-art performance, achieving classification accuracies above 92%,
segmentation Dice scores exceeding 91%, and inference times below 80
milliseconds. Furthermore, visual explanation tools such as Grad-CAM and
segmentation overlays enhance transparency and clinical interpretability. These
results indicate that the proposed framework can substantially accelerate
diagnostic workflows, reduce clinician workload, and support trustworthy AI
integration in time-critical healthcare environments.

</details>


### [56] [Self-Supervised Learning to Fly using Efficient Semantic Segmentation and Metric Depth Estimation for Low-Cost Autonomous UAVs](https://arxiv.org/abs/2510.16624)
*Sebastian Mocanu,Emil Slusanschi,Marius Leordeanu*

Main category: cs.CV

TL;DR: 本文提出了一种仅使用视觉的小型无人机自主飞行系统，结合语义分割和单目深度估计，在无GPS或昂贵传感器的情况下实现室内环境下的障碍物规避、场景探索和自主安全降落。


<details>
  <summary>Details</summary>
Motivation: 解决小型无人机在受控室内环境中自主飞行的挑战，特别是在缺乏GPS信号和昂贵传感器（如LiDAR）的情况下，需要开发轻量级且高效的视觉导航方案。

Method: 采用知识蒸馏框架，使用基于颜色的SVM教师网络生成训练数据，训练轻量级U-Net学生网络进行实时语义分割；创新性地提出自适应尺度因子算法，通过语义地面平面检测和相机内参将非度量单目深度预测转换为精确度量距离测量。

Result: 在5x4米实验室环境中测试，平均距离误差为14.4厘米；30次真实环境飞行测试和100次数字孪生环境测试显示，该方法增加了监视距离，减少了任务时间，并保持100%成功率；端到端学习实现87.5%的自主任务成功率。

Conclusion: 该工作推进了结构化环境中基于视觉的无人机导航实践，解决了度量深度估计和计算效率挑战，使系统能够在资源受限平台上部署。

Abstract: This paper presents a vision-only autonomous flight system for small UAVs
operating in controlled indoor environments. The system combines semantic
segmentation with monocular depth estimation to enable obstacle avoidance,
scene exploration, and autonomous safe landing operations without requiring GPS
or expensive sensors such as LiDAR. A key innovation is an adaptive scale
factor algorithm that converts non-metric monocular depth predictions into
accurate metric distance measurements by leveraging semantic ground plane
detection and camera intrinsic parameters, achieving a mean distance error of
14.4 cm. The approach uses a knowledge distillation framework where a
color-based Support Vector Machine (SVM) teacher generates training data for a
lightweight U-Net student network (1.6M parameters) capable of real-time
semantic segmentation. For more complex environments, the SVM teacher can be
replaced with a state-of-the-art segmentation model. Testing was conducted in a
controlled 5x4 meter laboratory environment with eight cardboard obstacles
simulating urban structures. Extensive validation across 30 flight tests in a
real-world environment and 100 flight tests in a digital-twin environment
demonstrates that the combined segmentation and depth approach increases the
distance traveled during surveillance and reduces mission time while
maintaining 100% success rates. The system is further optimized through
end-to-end learning, where a compact student neural network learns complete
flight policies from demonstration data generated by our best-performing
method, achieving an 87.5% autonomous mission success rate. This work advances
practical vision-based drone navigation in structured environments,
demonstrating solutions for metric depth estimation and computational
efficiency challenges that enable deployment on resource-constrained platforms.

</details>


### [57] [MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models](https://arxiv.org/abs/2510.16641)
*Young-Jun Lee,Byung-Kwan Lee,Jianshu Zhang,Yechan Hwang,Byungsoo Ko,Han-Gyu Kim,Dongyu Yao,Xuankun Rong,Eojin Joo,Seung-Ho Han,Bowon Ko,Ho-Jin Choi*

Main category: cs.CV

TL;DR: MultiVerse是一个新颖的多轮对话基准，包含647个对话，覆盖12个流行VLM评估基准的484个任务，使用基于检查表的评估方法评估18个VLM模型。


<details>
  <summary>Details</summary>
Motivation: 现有多轮对话数据集无法完全捕捉真实应用中复杂对话场景的广度和深度，需要更全面的评估基准。

Method: 从12个VLM评估基准中提取647个对话，每个对话平均4轮，提出基于检查表的评估方法，使用GPT-4o作为自动评估器，测量37个关键方面。

Result: 即使最强模型（如GPT-4o）在复杂多轮对话中仅达到50%成功率，提供完整对话上下文能显著提升较小或较弱模型的性能。

Conclusion: MultiVerse为评估VLM多轮交互能力提供了全面的基准，强调了上下文学习的重要性，揭示了当前模型在多轮对话中的局限性。

Abstract: Vision-and-Language Models (VLMs) have shown impressive capabilities on
single-turn benchmarks, yet real-world applications often demand more intricate
multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only
partially capture the breadth and depth of conversational scenarios encountered
by users. In this work, we introduce MultiVerse, a novel multi-turn
conversation benchmark featuring 647 dialogues - each averaging four turns -
derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484
tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from
factual knowledge and perception to advanced reasoning tasks such as
mathematics and coding. To facilitate robust assessment, we propose a
checklist-based evaluation method that leverages GPT-4o as the automated
evaluator, measuring performance across 37 key aspects, including perceptual
accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on
MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve
only a 50% success rate in complex multi-turn conversations, highlighting the
dataset's challenging nature. Notably, we find that providing full dialogue
context significantly enhances performance for smaller or weaker models,
emphasizing the importance of in-context learning. We believe MultiVerse is a
landscape of evaluating multi-turn interaction abilities for VLMs.

</details>


### [58] [Structured Interfaces for Automated Reasoning with 3D Scene Graphs](https://arxiv.org/abs/2510.16643)
*Aaron Ray,Jacob Arkin,Harel Biggie,Chuchu Fan,Luca Carlone,Nicholas Roy*

Main category: cs.CV

TL;DR: 提出使用检索增强生成（RAG）和Cypher查询语言接口，解决大型语言模型（LLMs）与3D场景图（3DSGs）结合时规模扩展问题，提高自然语言接地任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D场景图序列化为文本放入LLM上下文窗口，但这种方法无法扩展到大型或丰富的3D场景图。需要一种更有效的接口来连接自然语言和机器人的世界表示。

Method: 使用图数据库编码3D场景图，为LLM提供Cypher查询语言作为工具接口，通过检索增强生成选择与任务相关的3D场景图子集。

Result: 在指令跟随和场景问答任务上的评估显示，Cypher接口方法在大型丰富图上显著优于基线方法，性能大幅提升同时大幅减少场景图内容的token数量。

Conclusion: Cypher作为3D场景图的接口可显著扩展到大型丰富图，在本地和云端模型上都表现优异，为接地语言任务带来巨大性能改进。

Abstract: In order to provide a robot with the ability to understand and react to a
user's natural language inputs, the natural language must be connected to the
robot's underlying representations of the world. Recently, large language
models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for
grounding natural language and representing the world. In this work, we address
the challenge of using LLMs with 3DSGs to ground natural language. Existing
methods encode the scene graph as serialized text within the LLM's context
window, but this encoding does not scale to large or rich 3DSGs. Instead, we
propose to use a form of Retrieval Augmented Generation to select a subset of
the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide
a query language interface (Cypher) as a tool to the LLM with which it can
retrieve relevant data for language grounding. We evaluate our approach on
instruction following and scene question-answering tasks and compare against
baseline context window and code generation methods. Our results show that
using Cypher as an interface to 3D scene graphs scales significantly better to
large, rich graphs on both local and cloud-based models. This leads to large
performance improvements in grounded language tasks while also substantially
reducing the token count of the scene graph content. A video supplement is
available at https://www.youtube.com/watch?v=zY_YI9giZSA.

</details>


### [59] [Universal and Transferable Attacks on Pathology Foundation Models](https://arxiv.org/abs/2510.16660)
*Yuntian Wang,Xilin Yang,Che-Yung Shen,Nir Pillar,Aydogan Ozcan*

Main category: cs.CV

TL;DR: UTAP是一种针对病理学基础模型的通用可转移对抗性扰动，通过固定弱噪声模式系统性地破坏多个病理学基础模型的特征表示能力，导致下游任务性能下降。


<details>
  <summary>Details</summary>
Motivation: 揭示病理学基础模型的关键脆弱性，为模型鲁棒性评估建立高标准基准，推动防御机制发展，确保AI在病理学中的安全可靠部署。

Method: 使用深度学习优化生成固定且弱的噪声模式，将该噪声添加到病理图像中，系统性地破坏多个病理学基础模型的特征表示能力。

Result: UTAP在多个最先进的病理学基础模型上进行了系统评估，通过视觉不可察觉的输入图像修改，导致模型性能显著下降，且具有跨数据集和跨模型的通用性和可转移性。

Conclusion: UTAP构成了对各种新兴病理学基础模型及其应用的广泛威胁，强调了推进防御机制和对抗训练的必要性，为模型鲁棒性评估提供了关键基准。

Abstract: We introduce Universal and Transferable Adversarial Perturbations (UTAP) for
pathology foundation models that reveal critical vulnerabilities in their
capabilities. Optimized using deep learning, UTAP comprises a fixed and weak
noise pattern that, when added to a pathology image, systematically disrupts
the feature representation capabilities of multiple pathology foundation
models. Therefore, UTAP induces performance drops in downstream tasks that
utilize foundation models, including misclassification across a wide range of
unseen data distributions. In addition to compromising the model performance,
we demonstrate two key features of UTAP: (1) universality: its perturbation can
be applied across diverse field-of-views independent of the dataset that UTAP
was developed on, and (2) transferability: its perturbation can successfully
degrade the performance of various external, black-box pathology foundation
models - never seen before. These two features indicate that UTAP is not a
dedicated attack associated with a specific foundation model or image dataset,
but rather constitutes a broad threat to various emerging pathology foundation
models and their applications. We systematically evaluated UTAP across various
state-of-the-art pathology foundation models on multiple datasets, causing a
significant drop in their performance with visually imperceptible modifications
to the input images using a fixed noise pattern. The development of these
potent attacks establishes a critical, high-standard benchmark for model
robustness evaluation, highlighting a need for advancing defense mechanisms and
potentially providing the necessary assets for adversarial training to ensure
the safe and reliable deployment of AI in pathology.

</details>


### [60] [HYDRA: HYbrid knowledge Distillation and spectral Reconstruction Algorithm for high channel hyperspectral camera applications](https://arxiv.org/abs/2510.16664)
*Christopher Thirgood,Oscar Mendez,Erin Ling,Jon Storey,Simon Hadfield*

Main category: cs.CV

TL;DR: 该论文提出了一种名为HYDRA的新方法，通过混合知识蒸馏和光谱重建架构，从三通道自然图像中重建包含数百个通道的高光谱图像，解决了现有方法在密集光谱重建中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多尺度注意力方法在稀疏光谱重建中表现良好，但无法有效处理现代高光谱传感器包含的数百个通道。需要开发能够处理密集光谱且具有良好泛化能力的新方法。

Method: 采用教师-学生模型架构，教师模型封装潜在高光谱图像数据，学生模型学习从自然图像到教师编码域的映射。结合新颖的训练方法实现高质量光谱重建。

Result: 在各项指标上均达到最先进性能，准确率提升18%，在不同通道深度下推理速度均快于当前最先进模型。

Conclusion: HYDRA方法成功解决了现有光谱重建模型的关键限制，为高光谱图像在计算机视觉中的广泛应用提供了有效支持。

Abstract: Hyperspectral images (HSI) promise to support a range of new applications in
computer vision. Recent research has explored the feasibility of generalizable
Spectral Reconstruction (SR), the problem of recovering a HSI from a natural
three-channel color image in unseen scenarios.
  However, previous Multi-Scale Attention (MSA) works have only demonstrated
sufficient generalizable results for very sparse spectra, while modern HSI
sensors contain hundreds of channels.
  This paper introduces a novel approach to spectral reconstruction via our
HYbrid knowledge Distillation and spectral Reconstruction Architecture (HYDRA).
  Using a Teacher model that encapsulates latent hyperspectral image data and a
Student model that learns mappings from natural images to the Teacher's encoded
domain, alongside a novel training method, we achieve high-quality spectral
reconstruction.
  This addresses key limitations of prior SR models, providing SOTA performance
across all metrics, including an 18\% boost in accuracy, and faster inference
times than current SOTA models at various channel depths.

</details>


### [61] [Pursuing Minimal Sufficiency in Spatial Reasoning](https://arxiv.org/abs/2510.16688)
*Yejie Guo,Yunzhong Hou,Wufei Ma,Meng Tang,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 本文提出MSSR（最小充分空间推理器），通过构建最小充分信息集来解决VLMs在3D空间推理中的瓶颈问题，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在3D空间推理方面存在两个根本瓶颈：基于2D预训练导致的3D理解能力不足，以及冗余3D信息引发的推理失败。

Method: 采用双智能体框架：感知智能体使用专家模型工具包提取充分信息（包括新颖的SOG模块），推理智能体迭代优化信息以实现最小化，形成闭环推理路径。

Result: 在两个具有挑战性的基准测试中显著提高了准确性，达到了最先进的性能，并产生可解释的推理路径。

Conclusion: 通过明确追求充分性和最小性，MSSR框架为解决空间推理问题提供了有前景的方法，并为未来模型生成高质量训练数据。

Abstract: Spatial reasoning, the ability to ground language in 3D understanding,
remains a persistent challenge for Vision-Language Models (VLMs). We identify
two fundamental bottlenecks: inadequate 3D understanding capabilities stemming
from 2D-centric pre-training, and reasoning failures induced by redundant 3D
information. To address these, we first construct a Minimal Sufficient Set
(MSS) of information before answering a given question: a compact selection of
3D perception results from \textit{expert models}. We introduce MSSR (Minimal
Sufficient Spatial Reasoner), a dual-agent framework that implements this
principle. A Perception Agent programmatically queries 3D scenes using a
versatile perception toolbox to extract sufficient information, including a
novel SOG (Situated Orientation Grounding) module that robustly extracts
language-grounded directions. A Reasoning Agent then iteratively refines this
information to pursue minimality, pruning redundant details and requesting
missing ones in a closed loop until the MSS is curated. Extensive experiments
demonstrate that our method, by explicitly pursuing both sufficiency and
minimality, significantly improves accuracy and achieves state-of-the-art
performance across two challenging benchmarks. Furthermore, our framework
produces interpretable reasoning paths, offering a promising source of
high-quality training data for future models. Source code is available at
https://github.com/gyj155/mssr.

</details>


### [62] [SDPA++: A General Framework for Self-Supervised Denoising with Patch Aggregation](https://arxiv.org/abs/2510.16702)
*Huy Minh Nhat Nguyen,Triet Hoang Minh Dao,Chau Vinh Hoang Truong,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 提出SDPA++框架，一种仅使用噪声OCT图像的自监督去噪方法，通过自融合生成伪真值图像来训练去噪模型


<details>
  <summary>Details</summary>
Motivation: OCT图像分析对眼科疾病诊断至关重要，但获取成对的干净和噪声OCT图像数据集存在挑战，传统监督方法受限

Method: 自监督去噪框架，首先通过自融合和自监督去噪生成伪真值图像，然后使用基于块的策略训练去噪模型集成

Result: 在IEEE SPS VIP Cup真实数据集上验证，通过CNR、MSR、TP、EP等指标显示性能提升，该数据集仅包含真实噪声图像无干净参考

Conclusion: 该方法在仅有噪声图像的情况下有效提升OCT图像质量，具有临床应用的潜力

Abstract: Optical Coherence Tomography (OCT) is a widely used non-invasive imaging
technique that provides detailed three-dimensional views of the retina, which
are essential for the early and accurate diagnosis of ocular diseases.
Consequently, OCT image analysis and processing have emerged as key research
areas in biomedical imaging. However, acquiring paired datasets of clean and
real-world noisy OCT images for supervised denoising models remains a
formidable challenge due to intrinsic speckle noise and practical constraints
in clinical imaging environments. To address these issues, we propose SDPA++: A
General Framework for Self-Supervised Denoising with Patch Aggregation. Our
novel approach leverages only noisy OCT images by first generating
pseudo-ground-truth images through self-fusion and self-supervised denoising.
These refined images then serve as targets to train an ensemble of denoising
models using a patch-based strategy that effectively enhances image clarity.
Performance improvements are validated via metrics such as Contrast-to-Noise
Ratio (CNR), Mean Square Ratio (MSR), Texture Preservation (TP), and Edge
Preservation (EP) on the real-world dataset from the IEEE SPS Video and Image
Processing Cup. Notably, the VIP Cup dataset contains only real-world noisy OCT
images without clean references, highlighting our method's potential for
improving image quality and diagnostic outcomes in clinical practice.

</details>


### [63] [Connecting Domains and Contrasting Samples: A Ladder for Domain Generalization](https://arxiv.org/abs/2510.16704)
*Tianxin Wei,Yifan Chen,Xinrui He,Wenxuan Bao,Jingrui He*

Main category: cs.CV

TL;DR: 该论文提出了一种新的领域对比学习范式DCCL，通过增强跨领域的概念连通性来解决领域泛化问题，在五个标准基准测试中超越了现有最优方法。


<details>
  <summary>Details</summary>
Motivation: 解决训练和测试样本之间的分布偏移问题，当前直接应用对比学习会降低领域泛化性能，因为缺乏领域内的类内连通性。

Method: 提出领域连接对比学习(DCCL)：在数据侧使用更激进的数据增强和跨领域正样本；在模型侧使用模型锚定和生成变换损失来利用预训练表示中的类内连通性。

Result: 在五个标准领域泛化基准测试中，DCCL即使在没有领域监督的情况下也优于现有最优基线方法。

Conclusion: DCCL通过增强跨领域的概念连通性，能够获得更具泛化能力的表示，有效解决了领域泛化问题。

Abstract: Distribution shifts between training and testing samples frequently occur in
practice and impede model generalization performance. This crucial challenge
thereby motivates studies on domain generalization (DG), which aim to predict
the label on unseen target domain data by solely using data from source
domains. It is intuitive to conceive the class-separated representations
learned in contrastive learning (CL) are able to improve DG, while the reality
is quite the opposite: users observe directly applying CL deteriorates the
performance. We analyze the phenomenon with the insights from CL theory and
discover lack of intra-class connectivity in the DG setting causes the
deficiency. We thus propose a new paradigm, domain-connecting contrastive
learning (DCCL), to enhance the conceptual connectivity across domains and
obtain generalizable representations for DG. On the data side, more aggressive
data augmentation and cross-domain positive samples are introduced to improve
intra-class connectivity. On the model side, to better embed the unseen test
domains, we propose model anchoring to exploit the intra-class connectivity in
pre-trained representations and complement the anchoring with generative
transformation loss. Extensive experiments on five standard DG benchmarks are
performed. The results verify that DCCL outperforms state-of-the-art baselines
even without domain supervision. The detailed model implementation and the code
are provided through https://github.com/weitianxin/DCCL

</details>


### [64] [HumanCM: One Step Human Motion Prediction](https://arxiv.org/abs/2510.16709)
*Liu Haojie,Gao Suixiang*

Main category: cs.CV

TL;DR: HumanCM是一个基于一致性模型的一步人体运动预测框架，通过单步生成实现高效推理，在保持运动连贯性的同时大幅减少推理步骤。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的方法需要多步去噪，推理效率较低。作者希望开发一个能够实现高效单步生成的人体运动预测框架。

Method: 采用基于Transformer的时空架构，学习噪声和干净运动状态之间的自一致映射，使用时间嵌入来建模长距离依赖关系。

Result: 在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM在达到与最先进扩散模型相当或更好的精度的同时，将推理步骤减少了两个数量级。

Conclusion: HumanCM证明了通过一致性模型实现高效单步人体运动预测的可行性，为实时应用提供了有前景的解决方案。

Abstract: We present HumanCM, a one-step human motion prediction framework built upon
consistency models. Instead of relying on multi-step denoising as in
diffusion-based methods, HumanCM performs efficient single-step generation by
learning a self-consistent mapping between noisy and clean motion states. The
framework adopts a Transformer-based spatiotemporal architecture with temporal
embeddings to model long-range dependencies and preserve motion coherence.
Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves
comparable or superior accuracy to state-of-the-art diffusion models while
reducing inference steps by up to two orders of magnitude.

</details>


### [65] [Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes](https://arxiv.org/abs/2510.16714)
*Xiongkun Linghu,Jiangyong Huang,Ziyu Zhu,Baoxiong Jia,Siyuan Huang*

Main category: cs.CV

TL;DR: 提出SCENECOT框架，通过将复杂3D场景推理任务分解为简单子问题并构建视觉线索，实现基于链式思维的人类式3D场景推理。


<details>
  <summary>Details</summary>
Motivation: 现有3D大语言模型在基于场景的问答任务上表现不佳，主要缺乏对人类式场景-对象推理机制的研究。

Method: 提出基于多模态专家模块的SCENECOT方法，开发包含18.5万高质量实例的SCENECOT-185K数据集。

Result: 在多个复杂3D场景推理基准测试中取得优异性能，具有高水平的问答一致性。

Conclusion: 这是首次成功将链式思维推理应用于3D场景理解，展示了扩展到更广泛3D场景理解场景的潜力。

Abstract: Existing research on 3D Large Language Models (LLMs) still struggles to
achieve grounded question-answering, primarily due to the under-exploration of
the mech- anism of human-like scene-object grounded reasoning. This paper
bridges the gap by presenting a novel framework. We first introduce a grounded
Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a
complex reasoning task into simpler and manageable problems, and building
corresponding visual clues based on multimodal expert modules. To enable such a
method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning
dataset, consisting of 185K high-quality instances. Extensive experiments
across various complex 3D scene reasoning benchmarks demonstrate that our new
framework achieves strong performance with high grounding-QA coherence. To the
best of our knowledge, this is the first successful application of CoT
reasoning to 3D scene understanding, enabling step-by-step human-like reasoning
and showing potential for extension to broader 3D scene understanding
scenarios.

</details>


### [66] [Vision-Centric 4D Occupancy Forecasting and Planning via Implicit Residual World Models](https://arxiv.org/abs/2510.16729)
*Jianbiao Mei,Yu Yang,Xuemeng Yang,Licheng Wen,Jiajun Lv,Botian Shi,Yong Liu*

Main category: cs.CV

TL;DR: IR-WM是一个隐式残差世界模型，专注于建模当前状态和世界演变，通过预测残差变化来避免冗余的背景重建，在4D占用预测和轨迹规划中达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的端到端自动驾驶系统在视觉中心世界模型中存在效率问题，它们会完全重建未来场景，这导致大量计算能力被浪费在建模静态背景上。

Method: IR-WM首先从视觉观察建立鲁棒的鸟瞰图表示，然后利用前一时刻的BEV特征作为时间先验，仅预测基于自车动作和场景上下文的残差变化。通过对齐模块来校准语义和动态错位，并研究不同的预测-规划耦合方案。

Result: 在nuScenes基准测试中，IR-WM在4D占用预测和轨迹规划方面都达到了顶级性能。

Conclusion: 隐式残差世界模型通过专注于建模世界演变而非完全重建，显著提高了规划准确性，证明了该方法的有效性。

Abstract: End-to-end autonomous driving systems increasingly rely on vision-centric
world models to understand and predict their environment. However, a common
ineffectiveness in these models is the full reconstruction of future scenes,
which expends significant capacity on redundantly modeling static backgrounds.
To address this, we propose IR-WM, an Implicit Residual World Model that
focuses on modeling the current state and evolution of the world. IR-WM first
establishes a robust bird's-eye-view representation of the current state from
the visual observation. It then leverages the BEV features from the previous
timestep as a strong temporal prior and predicts only the "residual", i.e., the
changes conditioned on the ego-vehicle's actions and scene context. To
alleviate error accumulation over time, we further apply an alignment module to
calibrate semantic and dynamic misalignments. Moreover, we investigate
different forecasting-planning coupling schemes and demonstrate that the
implicit future state generated by world models substantially improves planning
accuracy. On the nuScenes benchmark, IR-WM achieves top performance in both 4D
occupancy forecasting and trajectory planning.

</details>


### [67] [UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid](https://arxiv.org/abs/2510.16730)
*Tianyang Dou,Ming Li,Jiangying Qin,Xuan Liao,Jiageng Zhong,Armin Gruen,Mengyi Deng*

Main category: cs.CV

TL;DR: UKANFormer是一种新型语义分割模型，用于在Allen Coral Atlas噪声监督下实现高精度珊瑚礁映射，通过全局-局部Transformer架构提取语义结构和边界细节，在噪声标签设置下优于传统基线。


<details>
  <summary>Details</summary>
Motivation: 全球珊瑚礁产品如Allen Coral Atlas虽然覆盖范围广，但在空间精度和语义一致性方面存在局限，特别是在需要精细边界划分的区域。需要开发能够在噪声监督下实现高精度映射的模型。

Method: 基于UKAN架构，在解码器中加入全局-局部Transformer模块，同时提取全局语义结构和局部边界细节，专门设计用于处理来自Allen Coral Atlas的噪声监督数据。

Result: 在实验中，UKANFormer达到珊瑚类IoU 67.00%和像素精度83.98%，在相同噪声标签设置下优于传统基线，产生的预测在视觉和结构上比训练使用的噪声标签更准确。

Conclusion: 该研究挑战了数据质量直接限制模型性能的观念，表明架构设计可以减轻标签噪声，在不可靠标签稀缺的生态监测场景下支持可扩展映射。

Abstract: Coral reefs are vital yet fragile ecosystems that require accurate
large-scale mapping for effective conservation. Although global products such
as the Allen Coral Atlas provide unprecedented coverage of global coral reef
distri-bution, their predictions are frequently limited in spatial precision
and semantic consistency, especially in regions requiring fine-grained boundary
delineation. To address these challenges, we propose UKANFormer, a novel
se-mantic segmentation model designed to achieve high-precision mapping under
noisy supervision derived from Allen Coral Atlas. Building upon the UKAN
architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans)
block in the decoder, enabling the extraction of both global semantic
structures and local boundary details. In experiments, UKANFormer achieved a
coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming
conventional baselines under the same noisy labels setting. Remarkably, the
model produces predictions that are visually and structurally more accurate
than the noisy labels used for training. These results challenge the notion
that data quality directly limits model performance, showing that architectural
design can mitigate label noise and sup-port scalable mapping under imperfect
supervision. UKANFormer provides a foundation for ecological monitoring where
reliable labels are scarce.

</details>


### [68] [A Comprehensive Survey on World Models for Embodied AI](https://arxiv.org/abs/2510.16732)
*Xinqing Li,Xin He,Le Zhang,Yun Liu*

Main category: cs.CV

TL;DR: 这篇论文提出了一个关于具身AI中世界模型的统一框架，包括问题设定、学习目标和三轴分类法，系统化了数据资源和评估指标，并指出了当前的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 具身AI需要能够感知、行动并预测行动如何重塑未来世界状态的智能体。世界模型作为内部模拟器，能够捕捉环境动态，支持感知、预测和决策。

Method: 提出了三轴分类法：(1)功能性：决策耦合vs通用目的；(2)时间建模：序列模拟和推理vs全局差异预测；(3)空间表示：全局潜在向量、标记特征序列、空间潜在网格和分解渲染表示。

Result: 系统化了机器人、自动驾驶和通用视频设置下的数据资源和指标，包括像素预测质量、状态级理解和任务性能。对最先进模型进行了定量比较。

Conclusion: 指出了关键开放挑战：统一数据集的稀缺性、需要评估物理一致性而非像素保真度的指标、模型性能与实时控制所需计算效率的权衡，以及实现长期时间一致性同时减轻误差积累的核心建模困难。

Abstract: Embodied AI requires agents that perceive, act, and anticipate how actions
reshape future world states. World models serve as internal simulators that
capture environment dynamics, enabling forward and counterfactual rollouts to
support perception, prediction, and decision making. This survey presents a
unified framework for world models in embodied AI. Specifically, we formalize
the problem setting and learning objectives, and propose a three-axis taxonomy
encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2)
Temporal Modeling, Sequential Simulation and Inference vs. Global Difference
Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature
Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We
systematize data resources and metrics across robotics, autonomous driving, and
general video settings, covering pixel prediction quality, state-level
understanding, and task performance. Furthermore, we offer a quantitative
comparison of state-of-the-art models and distill key open challenges,
including the scarcity of unified datasets and the need for evaluation metrics
that assess physical consistency over pixel fidelity, the trade-off between
model performance and the computational efficiency required for real-time
control, and the core modeling difficulty of achieving long-horizon temporal
consistency while mitigating error accumulation. Finally, we maintain a curated
bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.

</details>


### [69] [Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling](https://arxiv.org/abs/2510.16751)
*Erik Riise,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: 研究表明，离散的自回归模型通过beam搜索在图像生成中比连续扩散模型更有效，2B参数的自回归模型能超越12B参数的扩散模型，强调了模型架构对推理时优化的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管推理时搜索策略在大型语言模型中取得了革命性进展，但在图像生成领域应用类似策略却面临困难，特别是连续扩散模型中搜索策略效果有限。

Method: 利用离散、序列化的视觉自回归模型进行beam搜索，通过早期剪枝和计算重用来优化图像生成过程。

Result: beam搜索显著提升了文本到图像生成质量，2B参数的自回归模型在多个基准测试中超越了12B参数的扩散模型。

Conclusion: 模型架构（而非仅仅规模）对于视觉生成中的推理时优化至关重要，离散标记空间为实现有效搜索提供了关键优势。

Abstract: While inference-time scaling through search has revolutionized Large Language
Models, translating these gains to image generation has proven difficult.
Recent attempts to apply search strategies to continuous diffusion models show
limited benefits, with simple random sampling often performing best. We
demonstrate that the discrete, sequential nature of visual autoregressive
models enables effective search for image generation. We show that beam search
substantially improves text-to-image generation, enabling a 2B parameter
autoregressive model to outperform a 12B parameter diffusion model across
benchmarks. Systematic ablations show that this advantage comes from the
discrete token space, which allows early pruning and computational reuse, and
our verifier analysis highlights trade-offs between speed and reasoning
capability. These findings suggest that model architecture, not just scale, is
critical for inference-time optimization in visual generation.

</details>


### [70] [Prominence-Aware Artifact Detection and Dataset for Image Super-Resolution](https://arxiv.org/abs/2510.16752)
*Ivan Molodetskikh,Kirill Malyshev,Mark Mirgaleev,Nikita Zagainov,Evgeney Bogatyrev,Dmitriy Vatolin*

Main category: cs.CV

TL;DR: 该论文提出了一种基于人类感知的SR伪影显著性评估方法，通过构建包含1302个伪影样本的数据集并训练回归器来生成空间显著性热图，以更准确地评估和减轻超分辨率图像中的伪影问题。


<details>
  <summary>Details</summary>
Motivation: 随着生成式超分辨率模型能力的增强，其产生的伪影问题日益突出。现有的方法将伪影视为统一的二元缺陷，但不同伪影对人类观察者的视觉影响程度存在显著差异，需要根据其显著性而非简单存在与否来进行评估。

Method: 1. 构建包含1302个来自11种当代图像超分辨率方法的伪影样本数据集，每个伪影都配有众包显著性评分；2. 基于该数据集训练轻量级回归器，生成空间显著性热图来评估伪影的视觉显著性。

Result: 训练出的回归器在检测显著伪影方面优于现有方法，能够生成准确的空间显著性热图来量化伪影对人类观察者的视觉影响程度。

Conclusion: 该研究提出的显著性感知评估方法能够更准确地反映超分辨率图像中伪影的实际视觉影响，为SR模型的优化提供了新的评估维度，并发布了数据集和代码以促进相关研究。

Abstract: Generative image super-resolution (SR) is rapidly advancing in visual quality
and detail restoration. As the capacity of SR models expands, however, so does
their tendency to produce artifacts: incorrect, visually disturbing details
that reduce perceived quality. Crucially, their perceptual impact varies: some
artifacts are barely noticeable while others strongly degrade the image. We
argue that artifacts should be characterized by their prominence to human
observers rather than treated as uniform binary defects. Motivated by this, we
present a novel dataset of 1302 artifact examples from 11 contemporary image-SR
methods, where each artifact is paired with a crowdsourced prominence score.
Building on this dataset, we train a lightweight regressor that produces
spatial prominence heatmaps and outperforms existing methods at detecting
prominent artifacts. We release the dataset and code to facilitate
prominence-aware evaluation and mitigation of SR artifacts.

</details>


### [71] [WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement](https://arxiv.org/abs/2510.16765)
*Shengyu Zhu,Fan,Fuxuan Zhang*

Main category: cs.CV

TL;DR: WaMaIR是一个新颖的图像修复框架，通过全局多尺度小波变换卷积扩大感受野来提取图像特征，并使用基于Mamba的通道感知模块捕获特征通道内的长程依赖关系，同时提出多尺度纹理增强损失来有效保留纹理细节。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN方法在恢复精细纹理细节方面存在挑战，主要受限于CNN结构的小感受野和缺乏通道特征建模能力。

Method: 提出WaMaIR框架，包含三个核心组件：全局多尺度小波变换卷积(GMWTConvs)用于扩大感受野；基于Mamba的通道感知模块(MCAM)用于捕获通道间长程依赖；多尺度纹理增强损失(MTELoss)用于指导模型保留纹理结构。

Result: 大量实验证实WaMaIR在图像修复效果和计算效率方面均优于现有最先进方法。

Conclusion: WaMaIR通过创新的感受野扩展和通道建模机制，有效解决了图像修复中的纹理细节恢复问题，实现了更好的修复效果和计算性能。

Abstract: Image restoration is a fundamental and challenging task in computer vision,
where CNN-based frameworks demonstrate significant computational efficiency.
However, previous CNN-based methods often face challenges in adequately
restoring fine texture details, which are limited by the small receptive field
of CNN structures and the lack of channel feature modeling. In this paper, we
propose WaMaIR, which is a novel framework with a large receptive field for
image perception and improves the reconstruction of texture details in restored
images. Specifically, we introduce the Global Multiscale Wavelet Transform
Convolutions (GMWTConvs) for expandding the receptive field to extract image
features, preserving and enriching texture features in model inputs. Meanwhile,
we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to
capture long-range dependencies within feature channels, which enhancing the
model sensitivity to color, edges, and texture information. Additionally, we
propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to
guide the model in preserving detailed texture structures effectively.
Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods,
achieving better image restoration and efficient computational performance of
the model.

</details>


### [72] [Region in Context: Text-condition Image editing with Human-like semantic reasoning](https://arxiv.org/abs/2510.16772)
*Thuy Phuong Vu,Dinh-Cuong Hoang,Minhhuy Le,Phan Xuan Tan*

Main category: cs.CV

TL;DR: 提出Region in Context框架，通过多级语义对齐实现文本条件图像编辑，解决现有方法区域孤立导致的编辑不一致问题


<details>
  <summary>Details</summary>
Motivation: 现有文本条件图像编辑方法将区域孤立处理，仅依赖局部线索，导致编辑不一致、过渡不自然或整体图像连贯性丧失

Method: 引入双级引导机制：区域在完整图像上下文中表示并与详细区域级描述对齐，同时整个图像与视觉语言模型生成的场景级描述匹配

Result: 实验表明该方法能产生更连贯且与指令对齐的编辑结果

Conclusion: 该框架通过多级语义对齐实现了更精确和协调的图像编辑

Abstract: Recent research has made significant progress in localizing and editing image
regions based on text. However, most approaches treat these regions in
isolation, relying solely on local cues without accounting for how each part
contributes to the overall visual and semantic composition. This often results
in inconsistent edits, unnatural transitions, or loss of coherence across the
image. In this work, we propose Region in Context, a novel framework for
text-conditioned image editing that performs multilevel semantic alignment
between vision and language, inspired by the human ability to reason about
edits in relation to the whole scene. Our method encourages each region to
understand its role within the global image context, enabling precise and
harmonized changes. At its core, the framework introduces a dual-level guidance
mechanism: regions are represented with full-image context and aligned with
detailed region-level descriptions, while the entire image is simultaneously
matched to a comprehensive scene-level description generated by a large
vision-language model. These descriptions serve as explicit verbal references
of the intended content, guiding both local modifications and global structure.
Experiments show that it produces more coherent and instruction-aligned
results. Code is available at:
https://github.com/thuyvuphuong/Region-in-Context.git

</details>


### [73] [EMRRG: Efficient Fine-Tuning Pre-trained X-ray Mamba Networks for Radiology Report Generation](https://arxiv.org/abs/2510.16776)
*Mingzheng Zhang,Jinfeng Gao,Dan Xu,Jiangrui Yu,Yuhan Qiao,Lan Chen,Jin Tang,Xiao Wang*

Main category: cs.CV

TL;DR: 本文提出EMRRG框架，使用参数高效方法微调预训练的Mamba网络进行X光报告生成，在三个基准数据集上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有医疗报告生成模型主要依赖LLM，对预训练视觉基础模型和高级微调技术探索有限，主流框架要么避免微调要么使用简单方法如LoRA，忽略了增强跨注意力机制的潜力，同时非Transformer架构如Mamba网络在医疗报告生成中研究不足。

Method: 将X光图像分割为补丁并标记化，通过SSM-based视觉骨干网络进行特征提取，使用Partial LoRA获得最优性能，LLM与混合解码器生成医疗报告，实现端到端训练。

Result: 在三个广泛使用的基准数据集上的广泛实验完全验证了所提出策略对X光MRG的有效性。

Conclusion: EMRRG框架在X光医疗报告生成任务中表现出色，证明了参数高效微调预训练Mamba网络的有效性。

Abstract: X-ray image-based medical report generation (MRG) is a pivotal area in
artificial intelligence that can significantly reduce diagnostic burdens for
clinicians and patient wait times. Existing MRG models predominantly rely on
Large Language Models (LLMs) to improve report generation, with limited
exploration of pre-trained vision foundation models or advanced fine-tuning
techniques. Mainstream frameworks either avoid fine-tuning or utilize
simplistic methods like LoRA, often neglecting the potential of enhancing
cross-attention mechanisms. Additionally, while Transformer-based models
dominate vision-language tasks, non-Transformer architectures, such as the
Mamba network, remain underexplored for medical report generation, presenting a
promising avenue for future research. In this paper, we propose EMRRG, a novel
X-ray report generation framework that fine-tunes pre-trained Mamba networks
using parameter-efficient methods. Specifically, X-ray images are divided into
patches, tokenized, and processed by an SSM-based vision backbone for feature
extraction, with Partial LoRA yielding optimal performance. An LLM with a
hybrid decoder generates the medical report, enabling end-to-end training and
achieving strong results on benchmark datasets. Extensive experiments on three
widely used benchmark datasets fully validated the effectiveness of our
proposed strategies for the X-ray MRG. The source code of this paper will be
released on https://github.com/Event-AHU/Medical_Image_Analysis.

</details>


### [74] [GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation](https://arxiv.org/abs/2510.16777)
*Junbo Li,Weimin Yuan,Yinuo Wang,Yue Zeng,Shihao Shu,Cai Meng,Xiangzhi Bai*

Main category: cs.CV

TL;DR: GS2POSE提出了一种基于Bundle Adjustment原理的6D物体姿态估计算法，通过李代数和3D高斯散射技术构建姿态可微渲染管道，在纹理缺失和光照变化场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有6D姿态估计方法依赖2D图像特征与3D模型特征的对应关系，在纹理缺失物体和光照变化条件下表现不佳。

Method: 利用李代数扩展3DGS能力，构建姿态可微渲染管道，通过迭代优化输入图像与渲染图像的差异来优化姿态，同时更新3DGS模型中的颜色参数以适应光照变化。

Result: 在T-LESS、LineMod-Occlusion和LineMod数据集上分别实现了1.4%、2.8%和2.5%的精度提升。

Conclusion: GS2POSE通过结合Bundle Adjustment原理和3D高斯散射技术，有效解决了纹理缺失和光照变化下的6D姿态估计问题，显著提升了估计精度。

Abstract: Accurate 6D pose estimation of 3D objects is a fundamental task in computer
vision, and current research typically predicts the 6D pose by establishing
correspondences between 2D image features and 3D model features. However, these
methods often face difficulties with textureless objects and varying
illumination conditions. To overcome these limitations, we propose GS2POSE, a
novel approach for 6D object pose estimation. GS2POSE formulates a pose
regression algorithm inspired by the principles of Bundle Adjustment (BA). By
leveraging Lie algebra, we extend the capabilities of 3DGS to develop a
pose-differentiable rendering pipeline, which iteratively optimizes the pose by
comparing the input image to the rendered image. Additionally, GS2POSE updates
color parameters within the 3DGS model, enhancing its adaptability to changes
in illumination. Compared to previous models, GS2POSE demonstrates accuracy
improvements of 1.4\%, 2.8\% and 2.5\% on the T-LESS, LineMod-Occlusion and
LineMod datasets, respectively.

</details>


### [75] [Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features](https://arxiv.org/abs/2510.16781)
*Shihao Ji,Zihui Song*

Main category: cs.CV

TL;DR: 提出一种无需训练的视频理解框架，通过结合预训练视觉语言模型和传统机器学习算法，将视频理解重构为自监督的时空聚类问题。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型依赖大量标注数据进行特定任务训练，成本高且可扩展性差。需要将大型视觉语言模型的零样本推理能力有效迁移到视频领域。

Method: 使用预训练VLM的冻结视觉编码器将视频转换为语义特征轨迹，通过Kernel Temporal Segmentation划分事件片段，然后进行无监督密度聚类识别重复场景，最后选择代表性关键帧生成多模态摘要。

Result: 该框架实现了零样本的视频内容自动结构化分析，提供了一种有效、可解释且模型无关的解决方案。

Conclusion: 提出的训练免费框架成功地将VLM的语义先验与经典模式发现算法结合，为视频理解提供了新的有效途径。

Abstract: The remarkable zero-shot reasoning capabilities of large-scale Visual
Language Models (VLMs) on static images have yet to be fully translated to the
video domain. Conventional video understanding models often rely on extensive,
task-specific training on annotated datasets, a process that is both costly and
limited in scalability. This paper introduces a novel, training-free framework
for video understanding that circumvents end-to-end training by synergistically
combining the rich semantic priors of pre-trained VLMs with classic machine
learning algorithms for pattern discovery. Our core idea is to reframe video
understanding as a self-supervised spatio-temporal clustering problem within a
high-dimensional semantic feature space. The proposed pipeline first transforms
a video stream into a semantic feature trajectory using the frozen visual
encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal
Segmentation (KTS), a robust machine learning technique, to partition the
continuous feature stream into discrete, semantically coherent event segments.
These segments are then subjected to unsupervised density-based clustering to
identify recurring macroscopic scenes and themes throughout the video. By
selecting representative keyframes from each discovered cluster and leveraging
the VLM's generative capabilities for textual description, our framework
automatically produces a structured, multi-modal summary of the video content.
This approach provides an effective, interpretable, and model-agnostic pathway
for zero-shot, automated structural analysis of video content.

</details>


### [76] [Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs](https://arxiv.org/abs/2510.16785)
*Jiazhen Liu,Long Chen*

Main category: cs.CV

TL;DR: LENS是一种新颖的即插即用解决方案，通过在冻结的MLLM上附加轻量级可训练头，提取关键点特征来实现分割，既保持模型泛化能力又达到竞争性分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法需要微调MLLM以产生与掩码解码器兼容的特定输出，这会改变模型的输出空间并损害其内在泛化能力，违背构建统一模型的目标。

Method: LENS在完全冻结的MLLM上附加轻量级可训练头，通过精炼注意力图中的空间线索来提取关键点，并将其描述为与掩码解码器直接兼容的点状特征。

Result: LENS实现了与基于重训练方法竞争或更优的分割性能，同时完全保留了MLLM的泛化能力，而微调方法会显著降低这种能力。

Conclusion: LENS的可附加设计为扩展MLLM建立了一个高效强大的范式，为真正多才多艺的统一模型铺平了道路。

Abstract: Integrating diverse visual capabilities into a unified model is a significant
trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion
of segmentation poses a distinct set of challenges. To equip MLLMs with
pixel-level segmentation abilities, prevailing methods require finetuning the
model to produce specific outputs compatible with a mask decoder. This process
typically alters the model's output space and compromises its intrinsic
generalization, which undermines the goal of building a unified model. We
introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel
plug-and-play solution. LENS attaches a lightweight, trainable head to a
completely frozen MLLM. By refining the spatial cues embedded in attention
maps, LENS extracts keypoints and describes them into point-wise features
directly compatible with the mask decoder. Extensive experiments validate our
approach: LENS achieves segmentation performance competitive with or superior
to that of retraining-based methods. Crucially, it does so while fully
preserving the MLLM's generalization capabilities, which are significantly
degraded by finetuning approaches. As such, the attachable design of LENS
establishes an efficient and powerful paradigm for extending MLLMs, paving the
way for truly multi-talented, unified models.

</details>


### [77] [Unsupervised Monocular Road Segmentation for Autonomous Driving via Scene Geometry](https://arxiv.org/abs/2510.16790)
*Sara Hatami Rostami,Behrooz Nasihatkon*

Main category: cs.CV

TL;DR: 提出了一种完全无监督的道路分割方法，利用几何先验和时序一致性，无需人工标注数据，在Cityscapes数据集上达到0.82 IoU。


<details>
  <summary>Details</summary>
Motivation: 消除对昂贵人工标注数据的依赖，实现可扩展的无监督道路分割。

Method: 首先基于几何先验生成弱标签（地平线以上为非道路，车辆前方四边形为道路），然后通过跨帧跟踪局部特征点并利用互信息最大化来增强时序一致性。

Result: 在Cityscapes数据集上实现了0.82的IoU，表现出高精度和良好的时序稳定性。

Conclusion: 几何约束和时序一致性结合的方法在无监督道路分割中具有巨大潜力，为自动驾驶提供了简单有效的解决方案。

Abstract: This paper presents a fully unsupervised approach for binary road
segmentation (road vs. non-road), eliminating the reliance on costly manually
labeled datasets. The method leverages scene geometry and temporal cues to
distinguish road from non-road regions. Weak labels are first generated from
geometric priors, marking pixels above the horizon as non-road and a predefined
quadrilateral in front of the vehicle as road. In a refinement stage, temporal
consistency is enforced by tracking local feature points across frames and
penalizing inconsistent label assignments using mutual information
maximization. This enhances both precision and temporal stability. On the
Cityscapes dataset, the model achieves an Intersection-over-Union (IoU) of
0.82, demonstrating high accuracy with a simple design. These findings
demonstrate the potential of combining geometric constraints and temporal
consistency for scalable unsupervised road segmentation in autonomous driving.

</details>


### [78] [Personalized Image Filter: Mastering Your Photographic Style](https://arxiv.org/abs/2510.16791)
*Chengxuan Zhu,Shuchen Weng,Jiacong Fang,Peixuan Zhang,Si Li,Chao Xu,Boxin Shi*

Main category: cs.CV

TL;DR: 提出了个性化图像滤镜（PIF），基于预训练的文本到图像扩散模型，通过文本反演技术学习参考图像的摄影风格，实现摄影风格的提取和迁移。


<details>
  <summary>Details</summary>
Motivation: 摄影风格作为特定摄影概念的组合，是著名摄影师魅力的体现。但学习和迁移摄影风格需要深刻理解照片从未知原始外观如何被编辑。现有方法要么无法从参考图像中学习有意义的摄影概念，要么无法保留内容图像的内容。

Method: 基于预训练的文本到图像扩散模型，利用生成先验学习摄影概念的平均外观以及如何根据文本提示调整它们。通过文本反演技术优化摄影概念的提示词来学习参考图像的摄影风格。

Result: PIF在提取和迁移各种摄影风格方面表现出色。

Conclusion: PIF方法能够有效解决摄影风格学习和迁移中的关键问题，实现了高质量的摄影风格提取和转换。

Abstract: Photographic style, as a composition of certain photographic concepts, is the
charm behind renowned photographers. But learning and transferring photographic
style need a profound understanding of how the photo is edited from the unknown
original appearance. Previous works either fail to learn meaningful
photographic concepts from reference images, or cannot preserve the content of
the content image. To tackle these issues, we proposed a Personalized Image
Filter (PIF). Based on a pretrained text-to-image diffusion model, the
generative prior enables PIF to learn the average appearance of photographic
concepts, as well as how to adjust them according to text prompts. PIF then
learns the photographic style of reference images with the textual inversion
technique, by optimizing the prompts for the photographic concepts. PIF shows
outstanding performance in extracting and transferring various kinds of
photographic style. Project page: https://pif.pages.dev/

</details>


### [79] [An RGB-D Image Dataset for Lychee Detection and Maturity Classification for Robotic Harvesting](https://arxiv.org/abs/2510.16800)
*Zhenpeng Zhang,Yi Wang,Shanglei Chai,Yingying Liu,Zekai Xie,Wenhao Huang,Pengyu Li,Zipei Luo,Dajiang Lu,Yibin Tian*

Main category: cs.CV

TL;DR: 该研究构建了一个用于荔枝检测和成熟度分类的开源数据集，包含11,414张图像，涵盖不同品种、天气条件和成熟阶段，并通过深度学习模型验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏在自然生长环境中具有一致性和全面注释的荔枝开源数据集，而高质量数据对于开发基于视觉的荔枝采摘机器人至关重要。

Method: 收集了不同荔枝品种在不同天气条件和时间段的彩色（RGB）图像，包含三个成熟阶段，通过数据增强扩展样本量，采用三人独立标注、第四人验证的方式确保标注一致性，并进行详细统计分析。

Result: 构建了包含11,414张图像的数据集（878张原始RGB图像、8,780张增强RGB图像和1,756张深度图像），标注了9,658对荔枝检测和成熟度分类标签，并通过三种代表性深度学习模型验证了数据集的有效性。

Conclusion: 该数据集填补了荔枝检测和成熟度分类领域的空白，为开发荔枝采摘机器人提供了重要的数据支持，并已公开供学术研究使用。

Abstract: Lychee is a high-value subtropical fruit. The adoption of vision-based
harvesting robots can significantly improve productivity while reduce reliance
on labor. High-quality data are essential for developing such harvesting
robots. However, there are currently no consistently and comprehensively
annotated open-source lychee datasets featuring fruits in natural growing
environments. To address this, we constructed a dataset to facilitate lychee
detection and maturity classification. Color (RGB) images were acquired under
diverse weather conditions, and at different times of the day, across multiple
lychee varieties, such as Nuomici, Feizixiao, Heiye, and Huaizhi. The dataset
encompasses three different ripeness stages and contains 11,414 images,
consisting of 878 raw RGB images, 8,780 augmented RGB images, and 1,756 depth
images. The images are annotated with 9,658 pairs of lables for lychee
detection and maturity classification. To improve annotation consistency, three
individuals independently labeled the data, and their results were then
aggregated and verified by a fourth reviewer. Detailed statistical analyses
were done to examine the dataset. Finally, we performed experiments using three
representative deep learning models to evaluate the dataset. It is publicly
available for academic

</details>


### [80] [ReefNet: A Large scale, Taxonomically Enriched Dataset and Benchmark for Hard Coral Classification](https://arxiv.org/abs/2510.16822)
*Yahia Battach,Abdulwahab Felemban,Faizan Farooq Khan,Yousef A. Radwan,Xiang Li,Fabio Marchese,Sara Beery,Burton H. Jones,Francesca Benzoni,Mohamed Elhoseiny*

Main category: cs.CV

TL;DR: ReefNet是一个大型公共珊瑚礁图像数据集，包含约92.5万个属级硬珊瑚注释，旨在推动珊瑚礁监测的自动化发展。


<details>
  <summary>Details</summary>
Motivation: 由于气候变化等人为压力导致珊瑚礁快速衰退，急需可扩展的自动化监测方法。现有数据集存在规模小、地理范围有限、标签粗糙等问题。

Method: 整合了76个CoralNet来源和红海Al Wajh站点的图像数据，提供细粒度、分类学映射的标签。提出了两种评估设置：源内基准测试和跨源基准测试。

Result: 监督学习在源内表现良好，但在跨域时性能显著下降；零样本学习整体表现较差，特别是对稀有和视觉相似属的分类效果不佳。

Conclusion: ReefNet为领域泛化和细粒度珊瑚分类提供了具有挑战性的基准，将促进鲁棒、领域自适应的全球珊瑚礁监测和保护技术的发展。

Abstract: Coral reefs are rapidly declining due to anthropogenic pressures such as
climate change, underscoring the urgent need for scalable, automated
monitoring. We introduce ReefNet, a large public coral reef image dataset with
point-label annotations mapped to the World Register of Marine Species (WoRMS).
ReefNet aggregates imagery from 76 curated CoralNet sources and an additional
site from Al Wajh in the Red Sea, totaling approximately 925000 genus-level
hard coral annotations with expert-verified labels. Unlike prior datasets,
which are often limited by size, geography, or coarse labels and are not
ML-ready, ReefNet offers fine-grained, taxonomically mapped labels at a global
scale to WoRMS. We propose two evaluation settings: (i) a within-source
benchmark that partitions each source's images for localized evaluation, and
(ii) a cross-source benchmark that withholds entire sources to test domain
generalization. We analyze both supervised and zero-shot classification
performance on ReefNet and find that while supervised within-source performance
is promising, supervised performance drops sharply across domains, and
performance is low across the board for zero-shot models, especially for rare
and visually similar genera. This provides a challenging benchmark intended to
catalyze advances in domain generalization and fine-grained coral
classification. We will release our dataset, benchmarking code, and pretrained
models to advance robust, domain-adaptive, global coral reef monitoring and
conservation.

</details>


### [81] [Robust Cross-Domain Adaptation in Texture Features Transferring for Wood Chip Moisture Content Prediction](https://arxiv.org/abs/2510.16832)
*Abdur Rahman,Mohammad Marufuzzaman,Jason Street,Haifeng Wang,Veera G. Gude,Randy Buchanan*

Main category: cs.CV

TL;DR: 提出了一种名为AdaptMoist的领域自适应方法，通过结合五种纹理特征来预测不同来源木屑的水分含量，解决了现有方法在材料来源变化时准确性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 木屑水分含量的准确快速预测对生物燃料生产优化至关重要。现有直接方法（烘箱干燥）处理时间长且破坏样本，而间接方法（近红外光谱、电容、图像）在木屑来源多样时准确性不足。材料来源的变异性会改变数据分布，影响数据驱动模型的性能。

Method: 1. 从木屑图像中提取五种不同的纹理特征进行分析；2. 提出AdaptMoist领域自适应方法，利用纹理特征将知识从一个木屑数据源迁移到另一个；3. 提出基于调整互信息的模型保存标准。

Result: 1. 结合所有五种纹理特征的方法达到95%的准确率；2. AdaptMoist方法将跨领域预测准确率提高23%，平均准确率达到80%（非自适应模型为57%）。

Conclusion: AdaptMoist作为一种鲁棒的解决方案，能够有效应对不同领域木屑水分含量估计的挑战，为依赖木屑的行业提供了潜在解决方案。

Abstract: Accurate and quick prediction of wood chip moisture content is critical for
optimizing biofuel production and ensuring energy efficiency. The current
widely used direct method (oven drying) is limited by its longer processing
time and sample destructiveness. On the other hand, existing indirect methods,
including near-infrared spectroscopy-based, electrical capacitance-based, and
image-based approaches, are quick but not accurate when wood chips come from
various sources. Variability in the source material can alter data
distributions, undermining the performance of data-driven models. Therefore,
there is a need for a robust approach that effectively mitigates the impact of
source variability. Previous studies show that manually extracted texture
features have the potential to predict wood chip moisture class. Building on
this, in this study, we conduct a comprehensive analysis of five distinct
texture feature types extracted from wood chip images to predict moisture
content. Our findings reveal that a combined feature set incorporating all five
texture features achieves an accuracy of 95% and consistently outperforms
individual texture features in predicting moisture content. To ensure robust
moisture prediction, we propose a domain adaptation method named AdaptMoist
that utilizes the texture features to transfer knowledge from one source of
wood chip data to another, addressing variability across different domains. We
also proposed a criterion for model saving based on adjusted mutual
information. The AdaptMoist method improves prediction accuracy across domains
by 23%, achieving an average accuracy of 80%, compared to 57% for non-adapted
models. These results highlight the effectiveness of AdaptMoist as a robust
solution for wood chip moisture content estimation across domains, making it a
potential solution for wood chip-reliant industries.

</details>


### [82] [From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display](https://arxiv.org/abs/2510.16833)
*Xiangyu Mu,Dongliang Zhou,Jie Hou,Haijun Zhang,Weili Guan*

Main category: cs.CV

TL;DR: 提出M2HVideo框架，将服装模特视频转换为身份可控的逼真人类视频，解决头部与身体运动不协调和身份漂移问题


<details>
  <summary>Details</summary>
Motivation: 服装模特展示是线上时尚展示的经济替代方案，但缺乏真实感和表现细节，需要提升视频生成质量

Method: 采用动态姿态感知头部编码器融合面部语义和身体姿态，使用镜像损失和DDIM单步去噪保持面部细节，设计分布感知适配器增强时序一致性

Result: 在UBC时尚数据集、自建ASOS数据集和现场采集的MannequinVideos数据集上实验表明，M2HVideo在服装一致性、身份保持和视频保真度方面优于现有方法

Conclusion: M2HVideo框架有效解决了模特到人类视频生成的关键挑战，实现了高质量的身份可控视频生成

Abstract: Mannequin-based clothing displays offer a cost-effective alternative to
real-model showcases for online fashion presentation, but lack realism and
expressive detail. To overcome this limitation, we introduce a new task called
mannequin-to-human (M2H) video generation, which aims to synthesize
identity-controllable, photorealistic human videos from footage of mannequins.
We propose M2HVideo, a pose-aware and identity-preserving video generation
framework that addresses two key challenges: the misalignment between head and
body motion, and identity drift caused by temporal modeling. In particular,
M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial
semantics with body pose to produce consistent identity embeddings across
frames. To address the loss of fine facial details due to latent space
compression, we introduce a mirror loss applied in pixel space through a
denoising diffusion implicit model (DDIM)-based one-step denoising.
Additionally, we design a distribution-aware adapter that aligns statistical
distributions of identity and clothing features to enhance temporal coherence.
Extensive experiments on the UBC fashion dataset, our self-constructed ASOS
dataset, and the newly collected MannequinVideos dataset captured on-site
demonstrate that M2HVideo achieves superior performance in terms of clothing
consistency, identity preservation, and video fidelity in comparison to
state-of-the-art methods.

</details>


### [83] [2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting](https://arxiv.org/abs/2510.16837)
*Haofan Ren,Qingsong Yan,Ming Lu,Rongfeng Lu,Zunjie Zhu*

Main category: cs.CV

TL;DR: 2DGS-R提出了一种分层训练方法，通过引入法向一致性正则化和原地克隆操作，在保持几何精度的同时显著提升了2D高斯溅射的渲染质量，仅增加1%存储和少量训练时间开销。


<details>
  <summary>Details</summary>
Motivation: 当前3D高斯溅射(3DGS)难以准确表示表面，而2D高斯溅射(2DGS)虽然几何保真度有所提升但渲染质量受损，表明在单阶段训练中同时优化几何和渲染质量不可行。

Method: 采用分层训练策略：首先使用法向一致性正则化训练原始2D高斯；然后选择渲染质量不足的2D高斯进行原地克隆操作；最后冻结不透明度进行微调。

Result: 相比原始2DGS，仅增加1%存储和最小额外训练时间，但实现了高质量渲染结果并保持了精细几何结构，在视觉保真度和几何重建精度上均有提升。

Conclusion: 2DGS-R方法有效平衡了效率与性能，证明了分层训练策略在同时优化渲染质量和几何精度方面的有效性。

Abstract: Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced
neural fields, as it enables high-fidelity rendering with impressive visual
quality. However, 3DGS has difficulty accurately representing surfaces. In
contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian
disks. Despite advancements in geometric fidelity, rendering quality remains
compromised, highlighting the challenge of achieving both high-quality
rendering and precise geometric structures. This indicates that optimizing both
geometric and rendering quality in a single training stage is currently
unfeasible. To overcome this limitation, we present 2DGS-R, a new method that
uses a hierarchical training approach to improve rendering quality while
maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians
with the normal consistency regularization. Then 2DGS-R selects the 2D
Gaussians with inadequate rendering quality and applies a novel in-place
cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R
model with opacity frozen. Experimental results show that compared to the
original 2DGS, our method requires only 1\% more storage and minimal additional
training time. Despite this negligible overhead, it achieves high-quality
rendering results while preserving fine geometric structures. These findings
indicate that our approach effectively balances efficiency with performance,
leading to improvements in both visual fidelity and geometric reconstruction
accuracy.

</details>


### [84] [ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification](https://arxiv.org/abs/2510.16854)
*Akhila Kambhatla,Taminul Islam,Khaled R Ahmed*

Main category: cs.CV

TL;DR: ArmFormer是一种轻量级基于Transformer的语义分割框架，结合CBAM注意力机制和MixVisionTransformer架构，在保持计算效率的同时实现武器像素级精确分割，适用于边缘设备部署。


<details>
  <summary>Details</summary>
Motivation: 传统武器检测方法只能提供粗粒度的边界框定位，缺乏细粒度分割能力；现有语义分割模型要么牺牲精度追求效率，要么计算资源需求过高无法在边缘设备部署。

Method: 提出ArmFormer框架，将CBAM注意力模块与MixVisionTransformer架构结合，使用CBAM增强的编码器主干和注意力集成的解码器，实现手枪、步枪、刀具、左轮手枪和人类五类目标的语义分割。

Result: 在保持实时推理速度82.26 FPS的情况下，达到80.64% mIoU和89.13% mFscore的SOTA性能，仅需4.886G FLOPs和3.66M参数，比重量级模型节省48倍计算量。

Conclusion: ArmFormer是部署在便携安防摄像头、监控无人机和嵌入式AI加速器上的最优解决方案，为分布式安防基础设施提供高效的武器检测能力。

Abstract: The escalating threat of weapon-related violence necessitates automated
detection systems capable of pixel-level precision for accurate threat
assessment in real-time security applications. Traditional weapon detection
approaches rely on object detection frameworks that provide only coarse
bounding box localizations, lacking the fine-grained segmentation required for
comprehensive threat analysis. Furthermore, existing semantic segmentation
models either sacrifice accuracy for computational efficiency or require
excessive computational resources incompatible with edge deployment scenarios.
This paper presents ArmFormer, a lightweight transformer-based semantic
segmentation framework that strategically integrates Convolutional Block
Attention Module (CBAM) with MixVisionTransformer architecture to achieve
superior accuracy while maintaining computational efficiency suitable for
resource-constrained edge devices. Our approach combines CBAM-enhanced encoder
backbone with attention-integrated hamburger decoder to enable multi-class
weapon segmentation across five categories: handgun, rifle, knife, revolver,
and human. Comprehensive experiments demonstrate that ArmFormer achieves
state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while
maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M
parameters, ArmFormer outperforms heavyweight models requiring up to 48x more
computation, establishing it as the optimal solution for deployment on portable
security cameras, surveillance drones, and embedded AI accelerators in
distributed security infrastructure.

</details>


### [85] [BARL: Bilateral Alignment in Representation and Label Spaces for Semi-Supervised Volumetric Medical Image Segmentation](https://arxiv.org/abs/2510.16863)
*Shujian Gao,Yuan Wang,Zekuan Yu*

Main category: cs.CV

TL;DR: BARL提出了一种半监督医学图像分割框架，通过在表示空间和标签空间进行双边对齐，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督医学图像分割方法主要依赖标签空间一致性，但忽略了表示空间对齐的重要性，导致模型难以学习到既有判别性又具有空间一致性的特征表示。

Method: BARL框架包含两个协作分支，通过双路径正则化(DPR)和渐进认知偏差校正(PCBC)实现标签空间对齐，通过区域级和病灶实例匹配实现表示空间对齐。

Result: 在四个公共基准数据集和一个私有CBCT数据集上的实验表明，BARL一致超越了最先进的半监督医学图像分割方法。

Conclusion: BARL通过双边对齐策略有效解决了半监督医学图像分割中的表示空间对齐问题，为复杂病理模式的分割提供了有效解决方案。

Abstract: Semi-supervised medical image segmentation (SSMIS) seeks to match fully
supervised performance while sharply reducing annotation cost. Mainstream SSMIS
methods rely on \emph{label-space consistency}, yet they overlook the equally
critical \emph{representation-space alignment}. Without harmonizing latent
features, models struggle to learn representations that are both discriminative
and spatially coherent. To this end, we introduce \textbf{Bilateral Alignment
in Representation and Label spaces (BARL)}, a unified framework that couples
two collaborative branches and enforces alignment in both spaces. For
label-space alignment, inspired by co-training and multi-scale decoding, we
devise \textbf{Dual-Path Regularization (DPR)} and \textbf{Progressively
Cognitive Bias Correction (PCBC)} to impose fine-grained cross-branch
consistency while mitigating error accumulation from coarse to fine scales. For
representation-space alignment, we conduct region-level and lesion-instance
matching between branches, explicitly capturing the fragmented, complex
pathological patterns common in medical imagery. Extensive experiments on four
public benchmarks and a proprietary CBCT dataset demonstrate that BARL
consistently surpasses state-of-the-art SSMIS methods. Ablative studies further
validate the contribution of each component. Code will be released soon.

</details>


### [86] [Registration is a Powerful Rotation-Invariance Learner for 3D Anomaly Detection](https://arxiv.org/abs/2510.16865)
*Yuyang Yu,Zhengwei Chen,Xuemiao Xu,Lei Zhang,Haoxin Yang,Yongwei Nie,Shengfeng He*

Main category: cs.CV

TL;DR: 提出了一种基于配准引导的旋转不变特征提取框架，将点云配准与基于记忆库的异常检测目标相结合，解决现有方法在特征变换不一致和局部几何细节捕捉能力有限的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于记忆库的3D点云异常检测方法存在特征变换不一致和判别能力有限的问题，特别是在捕捉局部几何细节和实现旋转不变性方面。当配准失败时，这些问题会导致不可靠的检测结果。

Method: 提出配准引导的旋转不变特征提取框架，将点云配准与异常检测任务联合优化。通过将特征提取嵌入到配准学习过程中，使网络能够学习到对旋转鲁棒且对异常检测有效的特征表示。

Result: 在Anomaly-ShapeNet和Real3D-AD数据集上的大量实验表明，该方法在有效性和泛化性方面均优于现有方法。

Conclusion: 点云配准在指导特征提取向旋转不变和局部判别表示方面起着关键作用，联合优化配准和表示学习能够显著提升3D异常检测性能。

Abstract: 3D anomaly detection in point-cloud data is critical for industrial quality
control, aiming to identify structural defects with high reliability. However,
current memory bank-based methods often suffer from inconsistent feature
transformations and limited discriminative capacity, particularly in capturing
local geometric details and achieving rotation invariance. These limitations
become more pronounced when registration fails, leading to unreliable detection
results. We argue that point-cloud registration plays an essential role not
only in aligning geometric structures but also in guiding feature extraction
toward rotation-invariant and locally discriminative representations. To this
end, we propose a registration-induced, rotation-invariant feature extraction
framework that integrates the objectives of point-cloud registration and
memory-based anomaly detection. Our key insight is that both tasks rely on
modeling local geometric structures and leveraging feature similarity across
samples. By embedding feature extraction into the registration learning
process, our framework jointly optimizes alignment and representation learning.
This integration enables the network to acquire features that are both robust
to rotations and highly effective for anomaly detection. Extensive experiments
on the Anomaly-ShapeNet and Real3D-AD datasets demonstrate that our method
consistently outperforms existing approaches in effectiveness and
generalizability.

</details>


### [87] [Uncovering Brain-Like Hierarchical Patterns in Vision-Language Models through fMRI-Based Neural Encoding](https://arxiv.org/abs/2510.16870)
*Yudan Ren,Xinlong Wang,Kexin Wang,Tian Xia,Zihan Ma,Zhaowei Li,Xiangrong Bi,Xiao Li,Xiaowei He*

Main category: cs.CV

TL;DR: 提出了一种神经元级别的分析框架，通过结合人工神经元分析和fMRI体素编码，研究视觉语言模型与人脑处理机制之间的相似性，发现了四种关键的共享表征机制。


<details>
  <summary>Details</summary>
Motivation: 当前对人工神经网络与人脑处理机制之间相似性的理解有限：单模态研究无法捕捉大脑的多模态处理能力，多模态研究主要关注高层输出而忽略了神经元的关键作用。

Method: 结合细粒度人工神经元分析和基于fMRI的体素编码，对CLIP和METER两种架构不同的视觉语言模型进行神经元级别的分析。

Result: 发现四种关键发现：人工神经元能成功预测生物神经元活动；两者都表现出功能冗余；人工神经元表现出与生物神经元相似的极性模式；不同架构驱动不同的生物神经元激活模式。

Conclusion: 这些结果为视觉语言模型在神经元级别上存在类脑分层处理提供了有力证据。

Abstract: While brain-inspired artificial intelligence(AI) has demonstrated promising
results, current understanding of the parallels between artificial neural
networks (ANNs) and human brain processing remains limited: (1) unimodal ANN
studies fail to capture the brain's inherent multimodal processing
capabilities, and (2) multimodal ANN research primarily focuses on high-level
model outputs, neglecting the crucial role of individual neurons. To address
these limitations, we propose a novel neuron-level analysis framework that
investigates the multimodal information processing mechanisms in
vision-language models (VLMs) through the lens of human brain activity. Our
approach uniquely combines fine-grained artificial neuron (AN) analysis with
fMRI-based voxel encoding to examine two architecturally distinct VLMs: CLIP
and METER. Our analysis reveals four key findings: (1) ANs successfully predict
biological neurons (BNs) activities across multiple functional networks
(including language, vision, attention, and default mode), demonstrating shared
representational mechanisms; (2) Both ANs and BNs demonstrate functional
redundancy through overlapping neural representations, mirroring the brain's
fault-tolerant and collaborative information processing mechanisms; (3) ANs
exhibit polarity patterns that parallel the BNs, with oppositely activated BNs
showing mirrored activation trends across VLM layers, reflecting the complexity
and bidirectional nature of neural information processing; (4) The
architectures of CLIP and METER drive distinct BNs: CLIP's independent branches
show modality-specific specialization, whereas METER's cross-modal design
yields unified cross-modal activation, highlighting the architecture's
influence on ANN brain-like properties. These results provide compelling
evidence for brain-like hierarchical processing in VLMs at the neuronal level.

</details>


### [88] [Class-N-Diff: Classification-Induced Diffusion Model Can Make Fair Skin Cancer Diagnosis](https://arxiv.org/abs/2510.16887)
*Nusrat Munia,Abdullah Imran*

Main category: cs.CV

TL;DR: 提出Class-N-Diff模型，将分类器集成到扩散模型中，同时生成和分类皮肤镜图像，提高合成图像的真实性和多样性。


<details>
  <summary>Details</summary>
Motivation: 传统类别条件生成模型难以准确生成特定医学类别的图像，限制了其在皮肤癌诊断等应用中的实用性。

Method: 在扩散模型中集成分类器，基于类别条件引导图像生成，实现更好的类别控制。

Result: 模型能够生成更真实、多样的图像，分类器性能也得到提升，适用于下游诊断任务。

Conclusion: Class-N-Diff是一种增强扩散模型合成皮肤镜图像质量和实用性的强大工具。

Abstract: Generative models, especially Diffusion Models, have demonstrated remarkable
capability in generating high-quality synthetic data, including medical images.
However, traditional class-conditioned generative models often struggle to
generate images that accurately represent specific medical categories, limiting
their usefulness for applications such as skin cancer diagnosis. To address
this problem, we propose a classification-induced diffusion model, namely,
Class-N-Diff, to simultaneously generate and classify dermoscopic images. Our
Class-N-Diff model integrates a classifier within a diffusion model to guide
image generation based on its class conditions. Thus, the model has better
control over class-conditioned image synthesis, resulting in more realistic and
diverse images. Additionally, the classifier demonstrates improved performance,
highlighting its effectiveness for downstream diagnostic tasks. This unique
integration in our Class-N-Diff makes it a robust tool for enhancing the
quality and utility of diffusion model-based synthetic dermoscopic image
generation. Our code is available at https://github.com/Munia03/Class-N-Diff.

</details>


### [89] [Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback](https://arxiv.org/abs/2510.16888)
*Zongjian Li,Zheyuan Liu,Qihui Zhang,Bin Lin,Shenghai Yuan,Zhiyuan Yan,Yang Ye,Wangbo Yu,Yuwei Niu,Li Yuan*

Main category: cs.CV

TL;DR: Edit-R1是一个基于策略优化的后训练框架，通过DiffusionNFT方法和MLLM奖励模型提升指令图像编辑的泛化能力，在多个基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 监督微调训练的模型容易过拟合到标注模式，限制了其在训练分布之外的探索和泛化能力。

Method: 使用Diffusion Negative-aware Finetuning（DiffusionNFT）策略优化方法，结合多模态大语言模型作为统一奖励模型，并设计了低方差分组过滤机制。

Result: UniWorld-V2在ImgEdit和GEdit-Bench基准测试中分别获得4.49和7.83分，达到state-of-the-art水平，且框架具有模型无关性。

Conclusion: Edit-R1框架有效解决了指令图像编辑模型的过拟合问题，显著提升了泛化性能，具有广泛的适用性。

Abstract: Instruction-based image editing has achieved remarkable progress; however,
models solely trained via supervised fine-tuning often overfit to annotated
patterns, hindering their ability to explore and generalize beyond training
distributions. To this end, we introduce Edit-R1, a novel post-training
framework for instruction-based image editing based on policy optimization.
Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a
likelihood-free policy optimization method consistent with the flow matching
forward process, thereby enabling the use of higher-order samplers and more
efficient training. Another key challenge here is the absence of a universal
reward model, resulting from the diverse nature of editing instructions and
tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM)
as a unified, training-free reward model, leveraging its output logits to
provide fine-grained feedback. Furthermore, we carefully design a low-variance
group filtering mechanism to reduce MLLM scoring noise and stabilize
optimization. UniWorld-V2, trained with this framework, achieves
\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks,
scoring 4.49 and 7.83, respectively. Crucially, our framework is
model-agnostic, delivering substantial performance gains when applied to
diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its
wide applicability. Code and models are publicly available at
https://github.com/PKU-YuanGroup/UniWorld-V2.

</details>


### [90] [Contrail-to-Flight Attribution Using Ground Visible Cameras and Flight Surveillance Data](https://arxiv.org/abs/2510.16891)
*Ramon Dalmau,Gabriel Jarry,Philippe Very*

Main category: cs.CV

TL;DR: 本文提出了一种基于地面摄像机的凝结尾迹-航班归因框架，用于将观测到的凝结尾迹与生成它们的航班进行关联，解决了卫星观测分辨率不足的问题。


<details>
  <summary>Details</summary>
Motivation: 航空业的非CO2效应（特别是凝结尾迹）对气候影响显著，但验证和校准凝结尾迹物理模型需要将观测到的凝结尾迹与生成航班进行准确关联，而卫星观测存在时空分辨率限制。

Method: 利用地面可见摄像机凝结尾迹序列数据集，开发模块化框架，通过几何表示和距离度量、时间平滑处理以及基于概率的分配策略，将地面摄像机观测的凝结尾迹与基于飞机监视和气象数据推导的理论凝结尾迹进行关联。

Result: 建立了一个强大的基线系统，为未来凝结尾迹与源航班关联研究提供了模块化框架。

Conclusion: 地面摄像机方法能够捕捉凝结尾迹形成初期的高分辨率数据，为凝结尾迹-航班归因提供了有效的替代方案。

Abstract: Aviation's non-CO2 effects, particularly contrails, are a significant
contributor to its climate impact. Persistent contrails can evolve into
cirrus-like clouds that trap outgoing infrared radiation, with radiative
forcing potentially comparable to or exceeding that of aviation's CO2
emissions. While physical models simulate contrail formation, evolution and
dissipation, validating and calibrating these models requires linking observed
contrails to the flights that generated them, a process known as
contrail-to-flight attribution. Satellite-based attribution is challenging due
to limited spatial and temporal resolution, as contrails often drift and deform
before detection. In this paper, we evaluate an alternative approach using
ground-based cameras, which capture contrails shortly after formation at high
spatial and temporal resolution, when they remain thin, linear, and visually
distinct. Leveraging the ground visible camera contrail sequences (GVCCS)
dataset, we introduce a modular framework for attributing contrails observed
using ground-based cameras to theoretical contrails derived from aircraft
surveillance and meteorological data. The framework accommodates multiple
geometric representations and distance metrics, incorporates temporal
smoothing, and enables flexible probability-based assignment strategies. This
work establishes a strong baseline and provides a modular framework for future
research in linking contrails to their source flight.

</details>


### [91] [Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation](https://arxiv.org/abs/2510.16913)
*Akhila Kambhatla,Ahmed R Khaled*

Main category: cs.CV

TL;DR: 该论文评估了四种基于Transformer的架构在热成像武器分割任务中的性能，发现SegFormer-b5在分割精度上表现最佳，而SegFormer-b0在推理速度上最优。


<details>
  <summary>Details</summary>
Motivation: 热成像武器分割在低光照和视觉遮挡条件下对监控和安全应用至关重要。虽然CNN在热成像分割文献中占主导地位，但其捕捉长距离依赖和精细结构细节的能力有限。ViT在RGB分割任务中表现出色，但在热成像武器分割中的潜力尚未充分探索。

Method: 该研究在自定义热成像数据集上评估了四种基于Transformer的架构：SegFormer、DeepLabV3+、SegNeXt和Swin Transformer。数据集包含9,711张图像，使用SAM2自动标注。在MMSegmentation框架中采用标准增强策略进行模型训练和公平比较。

Result: 实验结果显示分割性能显著提升：SegFormer-b5达到最高mIoU（94.15%）和像素精度（97.04%），SegFormer-b0提供最快推理速度（98.32 FPS）和竞争性mIoU（90.84%）。SegNeXt-mscans提供平衡性能（85.12 FPS和92.24% mIoU），DeepLabV3+ R101-D8达到92.76% mIoU（29.86 FPS）。

Conclusion: Transformer架构在低光照和遮挡热成像环境中的武器检测表现出强大的泛化能力，具有灵活的精度-速度权衡，适用于各种实时安全应用。

Abstract: Thermal weapon segmentation is crucial for surveillance and security
applications, enabling robust detection under lowlight and visually obscured
conditions where RGB-based systems fail. While convolutional neural networks
(CNNs) dominate thermal segmentation literature, their ability to capture
long-range dependencies and fine structural details is limited. Vision
Transformers (ViTs), with their global context modeling capabilities, have
achieved state-of-the-art results in RGB segmentation tasks, yet their
potential in thermal weapon segmentation remains underexplored. This work
adapts and evaluates four transformer-based architectures SegFormer,
DeepLabV3\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a
custom thermal dataset comprising 9,711 images collected from real world
surveillance videos and automatically annotated using SAM2. We employ standard
augmentation strategies within the MMSegmentation framework to ensure robust
model training and fair architectural comparison. Experimental results
demonstrate significant improvements in segmentation performance: SegFormer-b5
achieves the highest mIoU (94.15\%) and Pixel Accuracy (97.04\%), while
SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive
mIoU (90.84\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and
92.24\% mIoU, and DeepLabV3\+ R101-D8 reaches 92.76\% mIoU at 29.86 FPS. The
transformer architectures demonstrate robust generalization capabilities for
weapon detection in low-light and occluded thermal environments, with flexible
accuracy-speed trade-offs suitable for diverse real-time security applications.

</details>


### [92] [Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input](https://arxiv.org/abs/2510.16926)
*Chenxu Li,Zhicai Wang,Yuan Sheng,Xingyu Zhu,Yanbin Hao,Xiang Wang*

Main category: cs.CV

TL;DR: 提出了Res-Bench基准，用于评估多模态大语言模型在不同分辨率下的性能稳定性，引入新的鲁棒性指标来量化模型的分辨率鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型评估主要关注语义性能，忽视了分辨率鲁棒性这一关键问题，即模型在不同输入分辨率下性能是否稳定。

Method: 构建包含14,400个样本的Res-Bench基准，涵盖12个分辨率级别和6个核心能力维度，提出Spearman相关性、绝对/相对连续误差等鲁棒性指标。

Result: 对领先MLLMs进行了大规模评估，包括模型中心化与任务中心化鲁棒性分析、预处理策略（填充和超分辨率）研究以及微调稳定性增强探索。

Conclusion: Res-Bench为评估多模态大语言模型的分辨率鲁棒性提供了系统框架，揭示了当前模型在分辨率变化下的性能稳定性问题。

Abstract: Multimodal Large Language Models (MLLMs) increasingly support dynamic image
resolutions. However, current evaluation paradigms primarily assess semantic
performance, overlooking the critical question of resolution robustness -
whether performance remains stable across varying input resolutions. To address
this gap, we introduce \textbf{Res-Bench}, a comprehensive benchmark comprising
14,400 samples across 12 resolution levels and six core capability dimensions.
We designed a novel evaluation framework that goes beyond traditional accuracy
metrics to capture performance stability. This framework introduces multiple
robustness metrics: Spearman's correlation for assessing resolution-performance
trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring
performance volatility. Using these metrics, we conducted a large-scale
evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and
task-centric robustness examination, (2) investigation of preprocessing
strategies including padding and super-resolution, and (3) exploration of
fine-tuning for stability enhancement.

</details>


### [93] [Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis](https://arxiv.org/abs/2510.16973)
*Praveenbalaji Rajendran,Mojtaba Safari,Wenfeng He,Mingzhe Hu,Shansong Wang,Jun Zhou,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 这篇综述文章对医学图像分析中的基础模型进行了全面系统的分析，涵盖了视觉专用和视觉语言基础模型的分类、训练策略、临床应用，并讨论了领域适应、高效微调等挑战及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 尽管基础模型在医学图像分析领域快速发展，但该领域仍缺乏对架构演进、训练范式和临床应用的系统性综述，因此需要统一的综合分析来弥补这一空白。

Method: 通过系统分类将研究分为视觉专用和视觉语言基础模型，分析其架构基础、训练策略和下游临床任务，并进行定量元分析来表征数据集利用和应用领域的时间趋势。

Result: 文章提供了基础模型在医学图像分析中的结构化分析，识别了领域适应、高效微调、计算约束和可解释性等关键挑战，并提出了联邦学习、知识蒸馏等新兴解决方案。

Conclusion: 确定了增强基础模型鲁棒性、可解释性和临床整合的关键未来研究方向，以加速其在真实医疗实践中的转化应用。

Abstract: Recent advancements in artificial intelligence (AI), particularly foundation
models (FMs), have revolutionized medical image analysis, demonstrating strong
zero- and few-shot performance across diverse medical imaging tasks, from
segmentation to report generation. Unlike traditional task-specific AI models,
FMs leverage large corpora of labeled and unlabeled multimodal datasets to
learn generalized representations that can be adapted to various downstream
clinical applications with minimal fine-tuning. However, despite the rapid
proliferation of FM research in medical imaging, the field remains fragmented,
lacking a unified synthesis that systematically maps the evolution of
architectures, training paradigms, and clinical applications across modalities.
To address this gap, this review article provides a comprehensive and
structured analysis of FMs in medical image analysis. We systematically
categorize studies into vision-only and vision-language FMs based on their
architectural foundations, training strategies, and downstream clinical tasks.
Additionally, a quantitative meta-analysis of the studies was conducted to
characterize temporal trends in dataset utilization and application domains. We
also critically discuss persistent challenges, including domain adaptation,
efficient fine-tuning, computational constraints, and interpretability along
with emerging solutions such as federated learning, knowledge distillation, and
advanced prompting. Finally, we identify key future research directions aimed
at enhancing the robustness, explainability, and clinical integration of FMs,
thereby accelerating their translation into real-world medical practice.

</details>


### [94] [One-step Diffusion Models with Bregman Density Ratio Matching](https://arxiv.org/abs/2510.16983)
*Yuanzhi Zhu,Eleftherios Tsonis,Lucas Degeorge,Vicky Kalogeiton*

Main category: cs.CV

TL;DR: 提出了Di-Bregman框架，通过Bregman散度密度比匹配来统一扩散蒸馏方法，实现了高效的一步扩散生成。


<details>
  <summary>Details</summary>
Motivation: 扩散和流模型生成质量高但计算成本大，现有蒸馏方法缺乏统一的理论基础。

Method: 将扩散蒸馏建模为Bregman散度密度比匹配问题，提供了凸分析视角的统一框架。

Result: 在CIFAR-10和文本到图像生成任务上，相比反向KL蒸馏获得更好的一步FID，同时保持高视觉保真度。

Conclusion: Bregman密度比匹配是通向高效一步扩散生成的实用且理论严谨的路径。

Abstract: Diffusion and flow models achieve high generative quality but remain
computationally expensive due to slow multi-step sampling. Distillation methods
accelerate them by training fast student generators, yet most existing
objectives lack a unified theoretical foundation. In this work, we propose
Di-Bregman, a compact framework that formulates diffusion distillation as
Bregman divergence-based density-ratio matching. This convex-analytic view
connects several existing objectives through a common lens. Experiments on
CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves
improved one-step FID over reverse-KL distillation and maintains high visual
fidelity compared to the teacher model. Our results highlight Bregman
density-ratio matching as a practical and theoretically-grounded route toward
efficient one-step diffusion generation.

</details>


### [95] [CARE: Contrastive Alignment for ADL Recognition from Event-Triggered Sensor Streams](https://arxiv.org/abs/2510.16988)
*Junhao Zhao,Zishuai Liu,Ruili Fang,Jin Lu,Linghan Zhang,Fei Dou*

Main category: cs.CV

TL;DR: 提出了CARE框架，通过序列-图像对比对齐方法解决ADL识别中序列和图像表示方法的局限性，实现跨表示对齐和任务特定可区分性。


<details>
  <summary>Details</summary>
Motivation: 现有ADL识别方法存在表示层面的局限性：序列方法保持时间顺序但对噪声敏感且缺乏空间感知，图像方法捕获全局模式但压缩时间动态和扭曲传感器布局。简单的融合方法无法有效对齐两种表示视图。

Method: CARE框架包含：(1)时间感知、抗噪声的序列编码；(2)空间感知和频率敏感的图像表示；(3)联合对比-分类目标进行端到端学习，通过序列-图像对比对齐(SICA)优化表示学习。

Result: 在三个CASAS数据集上达到最先进性能：米兰89.8%、开罗88.9%、京都7号73.3%，并展示了对传感器故障和布局变化的鲁棒性。

Conclusion: CARE框架通过对比对齐有效结合了序列和图像表示的互补优势，为智能家居中的可靠ADL识别提供了有前景的解决方案。

Abstract: The recognition of Activities of Daily Living (ADLs) from event-triggered
ambient sensors is an essential task in Ambient Assisted Living, yet existing
methods remain constrained by representation-level limitations. Sequence-based
approaches preserve temporal order of sensor activations but are sensitive to
noise and lack spatial awareness, while image-based approaches capture global
patterns and implicit spatial correlations but compress fine-grained temporal
dynamics and distort sensor layouts. Naive fusion (e.g., feature concatenation)
fail to enforce alignment between sequence- and image-based representation
views, underutilizing their complementary strengths. We propose Contrastive
Alignment for ADL Recognition from Event-Triggered Sensor Streams (CARE), an
end-to-end framework that jointly optimizes representation learning via
Sequence-Image Contrastive Alignment (SICA) and classification via
cross-entropy, ensuring both cross-representation alignment and task-specific
discriminability. CARE integrates (i) time-aware, noise-resilient sequence
encoding with (ii) spatially-informed and frequency-sensitive image
representations, and employs (iii) a joint contrastive-classification objective
for end-to-end learning of aligned and discriminative embeddings. Evaluated on
three CASAS datasets, CARE achieves state-of-the-art performance (89.8% on
Milan, 88.9% on Cairo, and 73.3% on Kyoto7) and demonstrates robustness to
sensor malfunctions and layout variability, highlighting its potential for
reliable ADL recognition in smart homes.

</details>


### [96] [Training-free Online Video Step Grounding](https://arxiv.org/abs/2510.16989)
*Luca Zanella,Massimiliano Mancini,Yiming Wang,Alessio Tonioni,Elisa Ricci*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练、在线运行的视频步骤定位方法BaGLM，利用大型多模态模型的零样本能力，通过贝叶斯滤波整合历史帧信息，在三个数据集上超越了需要训练的传统离线方法。


<details>
  <summary>Details</summary>
Motivation: 传统视频步骤定位方法需要带标注的训练数据且只能离线处理，成本高且无法满足在线决策需求。本文旨在开发无需训练、在线运行的VSG方法。

Method: 利用大型多模态模型的零样本能力预测步骤，提出BaGLM方法：通过贝叶斯滤波整合历史帧信息，使用大型语言模型提取步骤依赖矩阵和步骤进度估计。

Result: 在三个数据集上的实验表明，BaGLM的性能优于当前最先进的基于训练的离线方法，证明了无需训练在线方法的有效性。

Conclusion: BaGLM展示了利用大型多模态模型进行零样本在线视频步骤定位的可行性，为实时视频理解任务提供了新的解决方案。

Abstract: Given a task and a set of steps composing it, Video Step Grounding (VSG) aims
to detect which steps are performed in a video. Standard approaches for this
task require a labeled training set (e.g., with step-level annotations or
narrations), which may be costly to collect. Moreover, they process the full
video offline, limiting their applications for scenarios requiring online
decisions. Thus, in this work, we explore how to perform VSG online and without
training. We achieve this by exploiting the zero-shot capabilities of recent
Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step
associated with a restricted set of frames, without access to the whole video.
We show that this online strategy without task-specific tuning outperforms
offline and training-based models. Motivated by this finding, we develop
Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting
knowledge of past frames into the LMM-based predictions. BaGLM exploits
Bayesian filtering principles, modeling step transitions via (i) a dependency
matrix extracted through large language models and (ii) an estimation of step
progress. Experiments on three datasets show superior performance of BaGLM over
state-of-the-art training-based offline methods.

</details>


### [97] [An empirical study of the effect of video encoders on Temporal Video Grounding](https://arxiv.org/abs/2510.17007)
*Ignacio M. De la Jara,Cristian Rodriguez-Opazo,Edison Marrese-Taylor,Felipe Bravo-Marquez*

Main category: cs.CV

TL;DR: 本文通过实证研究探讨了不同视频特征对时序视频定位任务的影响，发现仅仅改变视频编码器就能显著影响模型性能，并揭示了不同特征之间的互补潜力。


<details>
  <summary>Details</summary>
Motivation: 时序视频定位是计算机视觉中的基础任务，但现有研究主要集中在少数几种视频表示方法上，这可能导致长期来看出现架构过拟合问题。

Method: 在三个知名基准数据集（Charades-STA、ActivityNet-Captions和YouCookII）上，使用基于CNN、时序推理和Transformer的不同视频编码器提取特征，并在经典架构上进行对比研究。

Result: 研究发现，仅仅改变视频编码器就会导致模型性能出现显著差异，同时揭示了使用特定特征时产生的明显模式和错误。

Conclusion: 不同视频特征之间存在互补潜力，这为未来时序视频定位任务的特征选择提供了重要指导。

Abstract: Temporal video grounding is a fundamental task in computer vision, aiming to
localize a natural language query in a long, untrimmed video. It has a key role
in the scientific community, in part due to the large amount of video generated
every day. Although we find extensive work in this task, we note that research
remains focused on a small selection of video representations, which may lead
to architectural overfitting in the long run. To address this issue, we propose
an empirical study to investigate the impact of different video features on a
classical architecture. We extract features for three well-known benchmarks,
Charades-STA, ActivityNet-Captions and YouCookII, using video encoders based on
CNNs, temporal reasoning and transformers. Our results show significant
differences in the performance of our model by simply changing the video
encoder, while also revealing clear patterns and errors derived from the use of
certain features, ultimately indicating potential feature complementarity.

</details>


### [98] [Do Satellite Tasks Need Special Pretraining?](https://arxiv.org/abs/2510.17014)
*Ani Vanyan,Alvard Barseghyan,Hakob Tamazyan,Tigran Galstyan,Vahan Huroyan,Naira Hovakimyan,Hrant Khachatrian*

Main category: cs.CV

TL;DR: 本文质疑专门为遥感应用定制的基础模型是否比通用视觉基础模型更有用，通过实验证明在ViT-B规模下，专门模型并未带来一致的改进。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证专门针对遥感应用的基础模型是否比通用视觉基础模型更有效，特别是在小规模场景下。

Method: 设计了一个简单基准测试，测量遥感模型在低分辨率图像上的泛化能力；在MillionAID数据集上训练iBOT自监督视觉编码器，并针对遥感特性进行修改。

Result: 实验结果显示，在ViT-B规模下，这些预训练模型都没有比通用基线带来一致的改进。

Conclusion: 结论是至少在ViT-B规模下，专门的基础模型并不比通用视觉基础模型更有用。

Abstract: Foundation models have advanced machine learning across various modalities,
including images. Recently multiple teams trained foundation models specialized
for remote sensing applications. This line of research is motivated by the
distinct characteristics of remote sensing imagery, specific applications and
types of robustness useful for satellite image analysis. In this work we
systematically challenge the idea that specific foundation models are more
useful than general-purpose vision foundation models, at least in the small
scale. First, we design a simple benchmark that measures generalization of
remote sensing models towards images with lower resolution for two downstream
tasks. Second, we train iBOT, a self-supervised vision encoder, on MillionAID,
an ImageNet-scale satellite imagery dataset, with several modifications
specific to remote sensing. We show that none of those pretrained models bring
consistent improvements upon general-purpose baselines at the ViT-B scale.

</details>


### [99] [Enrich and Detect: Video Temporal Grounding with Multimodal LLMs](https://arxiv.org/abs/2510.17023)
*Shraman Pramanick,Effrosyni Mavroudi,Yale Song,Rama Chellappa,Lorenzo Torresani,Triantafyllos Afouras*

Main category: cs.CV

TL;DR: ED-VTG是一种基于多模态大语言模型的细粒度视频时序定位方法，通过两阶段处理流程实现自然语言查询在视频中的精确定位。


<details>
  <summary>Details</summary>
Motivation: 利用多模态大语言模型的能力来联合处理文本和视频，有效解决视频时序定位问题，特别是在零样本评估场景下超越专门模型。

Method: 采用两阶段处理：首先将语言查询转换为包含缺失细节和线索的丰富句子，然后使用轻量级解码器基于上下文化表示预测准确边界。训练时采用多实例学习目标动态选择最优查询版本。

Result: 在多个视频时序定位和段落定位基准测试中取得最先进结果，显著优于所有先前基于LLM的时序定位方法，在零样本评估场景中具有明显优势。

Conclusion: ED-VTG方法展示了多模态大语言模型在视频时序定位任务中的强大潜力，能够实现与专门模型相当甚至更优的性能，特别是在零样本场景下表现出色。

Abstract: We introduce ED-VTG, a method for fine-grained video temporal grounding
utilizing multi-modal large language models. Our approach harnesses the
capabilities of multimodal LLMs to jointly process text and video, in order to
effectively localize natural language queries in videos through a two-stage
process. Rather than being directly grounded, language queries are initially
transformed into enriched sentences that incorporate missing details and cues
to aid in grounding. In the second stage, these enriched queries are grounded,
using a lightweight decoder, which specializes at predicting accurate
boundaries conditioned on contextualized representations of the enriched
queries. To mitigate noise and reduce the impact of hallucinations, our model
is trained with a multiple-instance-learning objective that dynamically selects
the optimal version of the query for each training sample. We demonstrate
state-of-the-art results across various benchmarks in temporal video grounding
and paragraph grounding settings. Experiments reveal that our method
significantly outperforms all previously proposed LLM-based temporal grounding
approaches and is either superior or comparable to specialized models, while
maintaining a clear advantage against them in zero-shot evaluation scenarios.

</details>


### [100] [Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding](https://arxiv.org/abs/2510.17034)
*Yutong Zhong*

Main category: cs.CV

TL;DR: 提出W2R2训练框架解决3D多模态定位中的2D语义偏差问题，通过解耦表征学习和针对性捷径抑制，显著提升定位精度


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在3D定位中存在严重的2D语义偏差，过度依赖2D图像特征而忽视3D几何输入，导致融合性能不佳

Method: W2R2框架：将2D特征作为语义标识（What），3D特征作为空间锚点（Where），使用双目标损失函数（对齐损失和伪标签损失）进行解耦表征学习

Result: 在ScanRefer和ScanQA数据集上验证，W2R2在定位精度和鲁棒性方面取得显著提升，尤其在复杂室外场景中表现突出

Conclusion: W2R2通过重新构建模型内部表征空间，在不改变推理架构的情况下实现了精确的3D定位，为解决2D语义偏差问题提供了有效方案

Abstract: Multimodal 3D grounding has garnered considerable interest in Vision-Language
Models (VLMs) \cite{yin2025spatial} for advancing spatial reasoning in complex
environments. However, these models suffer from a severe "2D semantic bias"
that arises from over-reliance on 2D image features for coarse localization,
largely disregarding 3D geometric inputs and resulting in suboptimal fusion
performance. In this paper, we propose a novel training framework called
What-Where Representation Re-Forming (W2R2) to tackle this issue via
disentangled representation learning and targeted shortcut suppression. Our
approach fundamentally reshapes the model's internal space by designating 2D
features as semantic beacons for "What" identification and 3D features as
spatial anchors for "Where" localization, enabling precise 3D grounding without
modifying inference architecture. Key components include a dual-objective loss
function with an Alignment Loss that supervises fused predictions using adapted
cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes
overly effective 2D-dominant pseudo-outputs via a margin-based mechanism.
Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of
W2R2, with significant gains in localization accuracy and robustness,
particularly in cluttered outdoor scenes.

</details>


### [101] [Conditional Synthetic Live and Spoof Fingerprint Generation](https://arxiv.org/abs/2510.17035)
*Syed Konain Abbas,Sandip Purnapatra,M. G. Sarwar Murshed,Conor Miller-Lynch,Lambert Igene,Soumyabrata Dey,Stephanie Schuckers,Faraz Hussain*

Main category: cs.CV

TL;DR: 本文提出了一种使用条件StyleGAN2-ADA和StyleGAN3生成高分辨率合成活体指纹，并通过CycleGANs转换为逼真欺骗指纹的新方法，解决了指纹数据收集中的隐私、成本和可访问性问题。


<details>
  <summary>Details</summary>
Motivation: 大型指纹数据集收集耗时昂贵且需要严格隐私保护，研究者探索使用合成指纹数据来解决这些问题。

Method: 采用条件StyleGAN2-ADA和StyleGAN3架构生成高分辨率合成活体指纹，并利用CycleGANs将其转换为模拟不同攻击材料的欺骗指纹，创建了两个合成数据集。

Result: StyleGAN3模型FID低至5，生成指纹在0.01% FAR下达到99.47% TAR；StyleGAN2-ADA达到98.67% TAR。质量评估和匹配实验证实了强大的隐私保护性能。

Conclusion: 该方法成功生成了高质量的合成指纹数据集，具有强大的隐私保护特性，为生物识别系统开发提供了可行的替代方案。

Abstract: Large fingerprint datasets, while important for training and evaluation, are
time-consuming and expensive to collect and require strict privacy measures.
Researchers are exploring the use of synthetic fingerprint data to address
these issues. This paper presents a novel approach for generating synthetic
fingerprint images (both spoof and live), addressing concerns related to
privacy, cost, and accessibility in biometric data collection. Our approach
utilizes conditional StyleGAN2-ADA and StyleGAN3 architectures to produce
high-resolution synthetic live fingerprints, conditioned on specific finger
identities (thumb through little finger). Additionally, we employ CycleGANs to
translate these into realistic spoof fingerprints, simulating a variety of
presentation attack materials (e.g., EcoFlex, Play-Doh). These synthetic spoof
fingerprints are crucial for developing robust spoof detection systems. Through
these generative models, we created two synthetic datasets (DB2 and DB3), each
containing 1,500 fingerprint images of all ten fingers with multiple
impressions per finger, and including corresponding spoofs in eight material
types. The results indicate robust performance: our StyleGAN3 model achieves a
Fr\'echet Inception Distance (FID) as low as 5, and the generated fingerprints
achieve a True Accept Rate of 99.47% at a 0.01% False Accept Rate. The
StyleGAN2-ADA model achieved a TAR of 98.67% at the same 0.01% FAR. We assess
fingerprint quality using standard metrics (NFIQ2, MINDTCT), and notably,
matching experiments confirm strong privacy preservation, with no significant
evidence of identity leakage, confirming the strong privacy-preserving
properties of our synthetic datasets.

</details>


### [102] [Click, Predict, Trust: Clinician-in-the-Loop AI Segmentation for Lung Cancer CT-Based Prognosis within the Knowledge-to-Action Framework](https://arxiv.org/abs/2510.17039)
*Mohammad R. Salmanpour,Sonya Falahati,Amir Hossein Pouria,Amin Mousavi,Somayeh Sadat Mehrnia,Morteza Alizadeh,Arman Gorji,Zeinab Farsangi,Alireza Safarian,Mehdi Maghsudi,Carlos Uribe,Arman Rahmim,Ren Yuan*

Main category: cs.CV

TL;DR: 本研究开发了一个临床医生参与的深度学习管道，用于肺癌CT图像分割和预后预测，VNet模型在SSL下表现最佳，实现了准确、可重复且临床可信的预后分析。


<details>
  <summary>Details</summary>
Motivation: 肺癌是主要癌症死因，CT成像在筛查和预后中至关重要。手动分割存在变异性和耗时问题，而深度学习面临临床采用障碍，需要提高可重复性、预后准确性和临床信任度。

Method: 基于知识到行动框架，使用多中心999名患者的CT数据，比较5种DL模型（3D Attention U-Net、ResUNet、VNet、ReconNet、SAM-Med3D），评估分割可重复性和预后建模（监督学习vs半监督学习），6名医生进行定性评估。

Result: VNet表现最佳（Dice=0.83，IoU=0.71），具有最佳放射组学稳定性（平均相关性=0.76，ICC=0.65）和SSL预测准确性（准确率=0.88，F1=0.83）。SSL始终优于SL，放射科医生偏好VNet的瘤周表征和平滑边界。

Conclusion: VNet与SSL结合可实现准确、可重复且临床可信的CT肺癌预后分析，为以医生为中心的AI转化提供了可行路径。

Abstract: Lung cancer remains the leading cause of cancer mortality, with CT imaging
central to screening, prognosis, and treatment. Manual segmentation is variable
and time-intensive, while deep learning (DL) offers automation but faces
barriers to clinical adoption. Guided by the Knowledge-to-Action framework,
this study develops a clinician-in-the-loop DL pipeline to enhance
reproducibility, prognostic accuracy, and clinical trust. Multi-center CT data
from 999 patients across 12 public datasets were analyzed using five DL models
(3D Attention U-Net, ResUNet, VNet, ReconNet, SAM-Med3D), benchmarked against
expert contours on whole and click-point cropped images. Segmentation
reproducibility was assessed using 497 PySERA-extracted radiomic features via
Spearman correlation, ICC, Wilcoxon tests, and MANOVA, while prognostic
modeling compared supervised (SL) and semi-supervised learning (SSL) across 38
dimensionality reduction strategies and 24 classifiers. Six physicians
qualitatively evaluated masks across seven domains, including clinical
meaningfulness, boundary quality, prognostic value, trust, and workflow
integration. VNet achieved the best performance (Dice = 0.83, IoU = 0.71),
radiomic stability (mean correlation = 0.76, ICC = 0.65), and predictive
accuracy under SSL (accuracy = 0.88, F1 = 0.83). SSL consistently outperformed
SL across models. Radiologists favored VNet for peritumoral representation and
smoother boundaries, preferring AI-generated initial masks for refinement
rather than replacement. These results demonstrate that integrating VNet with
SSL yields accurate, reproducible, and clinically trusted CT-based lung cancer
prognosis, highlighting a feasible path toward physician-centered AI
translation.

</details>


### [103] [Person Re-Identification via Generalized Class Prototypes](https://arxiv.org/abs/2510.17043)
*Md Ahmed Al Muzaddid,William J. Beksi*

Main category: cs.CV

TL;DR: 本文提出了一种广义选择方法，用于改进行人重识别中的类别表示选择，通过平衡准确率和平均精度均值，在多个嵌入方法上取得了超越现有技术的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有行人重识别研究主要集中在特征提取和目标函数改进上，而类别表示选择这一重要方向尚未得到充分探索。虽然过去有研究在训练阶段使用类别质心，但在检索阶段对替代表示的研究较少，且现有技术无法达到最优的重识别指标。

Method: 提出了一种广义选择方法，不局限于类别质心，而是选择更优的类别表示。该方法允许根据具体应用需求调整每个类别的表示数量，在多个重识别嵌入方法上进行应用。

Result: 该方法在准确率和平均精度均值之间取得了良好平衡，在所有测试的嵌入方法上都显著提升了性能，超越了当代最佳结果。

Conclusion: 广义表示选择方法是行人重识别中一个有效的研究方向，通过优化类别表示选择可以带来显著的性能提升，且具有灵活的应用适应性。

Abstract: Advanced feature extraction methods have significantly contributed to
enhancing the task of person re-identification. In addition, modifications to
objective functions have been developed to further improve performance.
Nonetheless, selecting better class representatives is an underexplored area of
research that can also lead to advancements in re-identification performance.
Although past works have experimented with using the centroid of a gallery
image class during training, only a few have investigated alternative
representations during the retrieval stage. In this paper, we demonstrate that
these prior techniques yield suboptimal results in terms of re-identification
metrics. To address the re-identification problem, we propose a generalized
selection method that involves choosing representations that are not limited to
class centroids. Our approach strikes a balance between accuracy and mean
average precision, leading to improvements beyond the state of the art. For
example, the actual number of representations per class can be adjusted to meet
specific application requirements. We apply our methodology on top of multiple
re-identification embeddings, and in all cases it substantially improves upon
contemporary results

</details>


### [104] [Video Reasoning without Training](https://arxiv.org/abs/2510.17045)
*Deepak Sridhar,Kartikeya Bhardwaj,Jeya Pradha Jeyaraj,Nuno Vasconcelos,Ankita Nayak,Harris Teague*

Main category: cs.CV

TL;DR: 提出V-Reason方法，通过熵值信号优化推理过程，无需RL训练即可显著提升视频推理性能，同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 当前视频推理模型依赖昂贵的强化学习和冗长的思维链，导致训练和推理计算成本高，且推理过程控制机制有限。

Method: 利用模型输出的熵值作为信号，通过小型可训练控制器对LMM的值缓存进行优化，调整推理过程中的微探索和微利用行为。

Result: 在多个视频推理数据集上显著超越基础指令调优模型，与RL训练模型的差距缩小到0.6%以内，输出token减少58.6%。

Conclusion: 基于熵的推理过程优化方法能够有效提升模型性能，同时大幅提高效率，为视频推理提供了新的优化方向。

Abstract: Video reasoning using Large Multimodal Models (LMMs) relies on costly
reinforcement learning (RL) and verbose chain-of-thought, resulting in
substantial computational overhead during both training and inference.
Moreover, the mechanisms that control the thinking process in these reasoning
models are very limited. In this paper, using entropy of the model's output as
a signal, we discover that the high-quality models go through a series of
micro-explorations and micro-exploitations which keep the reasoning process
grounded (i.e., avoid excessive randomness while the model is exploring or
thinking through an answer). We further observe that once this "thinking"
process is over, more accurate models demonstrate a better convergence by
reducing the entropy significantly via a final exploitation phase (i.e., a more
certain convergence towards a solution trajectory). We then use these novel,
theoretically-grounded insights to tune the model's behavior directly at
inference, without using any RL or supervised fine-tuning. Specifically, during
inference, our proposed approach called V-Reason (Video-Reason) adapts the
value cache of the LMM via a few optimization steps on a small, trainable
controller using an entropy-based objective, i.e., no supervision from any
dataset or RL is necessary. This tuning improves the model's micro-exploration
and exploitation behavior during inference. Our experiments show that our
proposed method achieves significant improvements over the base
instruction-tuned models across several video reasoning datasets, narrowing the
gap with RL-trained models to within 0.6% average accuracy without any
training, while offering massive efficiency benefits: output tokens are reduced
by 58.6% compared to the RL model.

</details>


### [105] [How Universal Are SAM2 Features?](https://arxiv.org/abs/2510.17051)
*Masoud Khairi Atani,Alon Harell,Hyomin Choi,Runyu Yang,Fabien Racape,Ivan V. Bajic*

Main category: cs.CV

TL;DR: 本文研究了通用视觉基础模型与专用模型之间的权衡，通过比较Hiera通用编码器和SAM2专用分割模型的特性适应性，量化了专业化带来的信息理论成本。


<details>
  <summary>Details</summary>
Motivation: 理解通用基础视觉模型与其专用对应模型之间的权衡对于高效特征编码设计至关重要，但目前尚未完全理解这一权衡关系。

Method: 使用轻量级可训练neck来探测冻结特征的适应性，比较Hiera通用编码器和SAM2专用分割模型的特性适应性，并进行跨neck分析。

Result: SAM2在空间相关任务（如深度估计）上表现优异，但在概念较远的任务（如姿态估计和图像描述）上表现不如通用模型Hiera，显示出语义信息的损失。跨neck分析表明每个适应级别都会产生进一步的表示瓶颈。

Conclusion: 专业化虽然在某些任务上有效，但会损失更广泛的语义信息，为设计高效特征编码和适应策略提供了定量基础。

Abstract: The trade-off between general-purpose foundation vision models and their
specialized counterparts is critical for efficient feature coding design and is
not yet fully understood. We investigate this trade-off by comparing the
feature versatility of the general-purpose Hiera encoder against the
segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight,
trainable neck to probe the adaptability of their frozen features, we quantify
the information-theoretic cost of specialization. Our results reveal that while
SAM2's specialization is highly effective for spatially-related tasks like
depth estimation, it comes at a cost. The specialized SAM2 encoder
underperforms its generalist predecessor, Hiera, on conceptually distant tasks
such as pose estimation and image captioning, demonstrating a measurable loss
of broader semantic information. A novel cross-neck analysis on SAM2 reveals
that each level of adaptation creates a further representational bottleneck.
Our analysis illuminates these trade-offs in feature universality, providing a
quantitative foundation for designing efficient feature coding and adaptation
strategies for diverse downstream applications.

</details>


### [106] [ProDAT: Progressive Density-Aware Tail-Drop for Point Cloud Coding](https://arxiv.org/abs/2510.17068)
*Zhe Luo,Wenjing Jia,Stuart Perry*

Main category: cs.CV

TL;DR: ProDAT是一种新颖的密度感知尾丢弃机制，用于渐进式点云编码，通过单一模型实现多比特率下的渐进解码，并在编码效率上优于现有学习型编码方法。


<details>
  <summary>Details</summary>
Motivation: 三维点云在自动驾驶、增强现实等应用中需要实时处理和低延迟，但大数据量和带宽限制阻碍了在资源受限环境中的高质量服务部署。现有学习型点云几何编码方法的固定潜在表示不支持渐进解码。

Method: 提出ProDAT，利用密度信息作为指导信号，根据重要性自适应解码潜在特征和坐标，从而实现使用单一模型在多个比特率下的渐进解码。

Result: 在基准数据集上的实验结果显示，ProDAT不仅实现了渐进编码，而且在编码效率上优于最先进的学习型编码技术，在SemanticKITTI上PSNR-D2的BD-rate提升超过28.6%，在ShapeNet上超过18.15%。

Conclusion: ProDAT通过密度感知机制成功解决了点云渐进编码的问题，在保持高编码效率的同时实现了灵活的渐进解码能力。

Abstract: Three-dimensional (3D) point clouds are becoming increasingly vital in
applications such as autonomous driving, augmented reality, and immersive
communication, demanding real-time processing and low latency. However, their
large data volumes and bandwidth constraints hinder the deployment of
high-quality services in resource-limited environments. Progres- sive coding,
which allows for decoding at varying levels of detail, provides an alternative
by allowing initial partial decoding with subsequent refinement. Although
recent learning-based point cloud geometry coding methods have achieved notable
success, their fixed latent representation does not support progressive
decoding. To bridge this gap, we propose ProDAT, a novel density-aware
tail-drop mechanism for progressive point cloud coding. By leveraging density
information as a guidance signal, latent features and coordinates are decoded
adaptively based on their significance, therefore achieving progressive
decoding at multiple bitrates using one single model. Experimental results on
benchmark datasets show that the proposed ProDAT not only enables progressive
coding but also achieves superior coding efficiency compared to
state-of-the-art learning-based coding techniques, with over 28.6% BD-rate
improvement for PSNR- D2 on SemanticKITTI and over 18.15% for ShapeNet

</details>


### [107] [Towards a Generalizable Fusion Architecture for Multimodal Object Detection](https://arxiv.org/abs/2510.17078)
*Jad Berjawi,Yoann Dupas,Christophe C'erin*

Main category: cs.CV

TL;DR: FMCAF是一种用于RGB和红外图像融合的预处理架构，通过频域滤波和跨注意力融合模块提升多模态目标检测性能，在不同数据集上表现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 多模态目标检测在挑战性条件下通过多传感器互补信息提高鲁棒性，但现有方法往往针对特定数据集设计，缺乏泛化性。

Method: 提出FMCAF架构，包含频域滤波模块（Freq-Filter）抑制冗余频谱特征，以及跨注意力融合模块（MCAF）增强模态间特征共享。

Result: 在LLVIP（低光行人检测）和VEDAI（航空车辆检测）数据集上，FMCAF优于传统融合方法，VEDAI上mAP@50提升13.9%，LLVIP上提升1.1%。

Conclusion: FMCAF作为一种灵活的预处理架构，为未来多模态检测管道提供了稳健的融合基础，具有良好泛化潜力。

Abstract: Multimodal object detection improves robustness in chal- lenging conditions
by leveraging complementary cues from multiple sensor modalities. We introduce
Filtered Multi- Modal Cross Attention Fusion (FMCAF), a preprocess- ing
architecture designed to enhance the fusion of RGB and infrared (IR) inputs.
FMCAF combines a frequency- domain filtering block (Freq-Filter) to suppress
redun- dant spectral features with a cross-attention-based fusion module (MCAF)
to improve intermodal feature sharing. Unlike approaches tailored to specific
datasets, FMCAF aims for generalizability, improving performance across
different multimodal challenges without requiring dataset- specific tuning. On
LLVIP (low-light pedestrian detec- tion) and VEDAI (aerial vehicle detection),
FMCAF outper- forms traditional fusion (concatenation), achieving +13.9% mAP@50
on VEDAI and +1.1% on LLVIP. These results support the potential of FMCAF as a
flexible foundation for robust multimodal fusion in future detection pipelines.

</details>


### [108] [GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation](https://arxiv.org/abs/2510.17095)
*Ruitong Gan,Junran Peng,Yang Liu,Chuanchen Luo,Qing Li,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: GSPlane通过引入平面先验和动态高斯重分类器，显著提升了高斯溅射方法在平面区域重建的几何精度和网格质量，同时保持渲染质量。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的高斯溅射表示在重建平面区域时往往难以达到足够的平滑度和精度，而平面是3D场景特别是人造环境中的基本元素，需要结构化表示以支持下游应用。

Method: 利用现成的分割和法线预测模型提取平面先验，建立结构化平面高斯坐标表示；引入动态高斯重分类器自适应地将持续高梯度的平面高斯重新分类为非平面；利用优化后的平面先验优化网格布局。

Result: 在各种基准测试中，平面先验的引入显著提高了提取网格的几何精度，同时减少了顶点和面的数量，改善了拓扑结构，且没有牺牲渲染质量。

Conclusion: GSPlane方法有效解决了高斯溅射在平面重建中的精度问题，为场景编辑和物理模拟等下游应用提供了更好的结构化平面表示。

Abstract: Planes are fundamental primitives of 3D sences, especially in man-made
environments such as indoor spaces and urban streets. Representing these planes
in a structured and parameterized format facilitates scene editing and physical
simulations in downstream applications. Recently, Gaussian Splatting (GS) has
demonstrated remarkable effectiveness in the Novel View Synthesis task, with
extensions showing great potential in accurate surface reconstruction. However,
even state-of-the-art GS representations often struggle to reconstruct planar
regions with sufficient smoothness and precision. To address this issue, we
propose GSPlane, which recovers accurate geometry and produces clean and
well-structured mesh connectivity for plane regions in the reconstructed scene.
By leveraging off-the-shelf segmentation and normal prediction models, GSPlane
extracts robust planar priors to establish structured representations for
planar Gaussian coordinates, which help guide the training process by enforcing
geometric consistency. To further enhance training robustness, a Dynamic
Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians
with persistently high gradients as non-planar, ensuring more reliable
optimization. Furthermore, we utilize the optimized planar priors to refine the
mesh layouts, significantly improving topological structure while reducing the
number of vertices and faces. We also explore applications of the structured
planar representation, which enable decoupling and flexible manipulation of
objects on supportive planes. Extensive experiments demonstrate that, with no
sacrifice in rendering quality, the introduction of planar priors significantly
improves the geometric accuracy of the extracted meshes across various
baselines.

</details>


### [109] [Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement](https://arxiv.org/abs/2510.17105)
*Xiaogang Xu,Jian Wang,Yunfan Lu,Ruihang Chu,Ruixing Wang,Jiafei Wu,Bei Yu,Liang Lin*

Main category: cs.CV

TL;DR: 该论文提出了一种新的优化策略，用于改进预训练扩散模型在低光场景下的条件控制，通过潜在空间精炼和双向交互机制来提升内容保真度同时保持感知真实感。


<details>
  <summary>Details</summary>
Motivation: 预训练扩散模型在低光视觉任务中往往牺牲内容保真度来获得更高的感知真实感，这主要是由于缺乏合适的条件潜在建模以及条件潜在与噪声潜在之间缺乏双向交互。

Method: 提出了一种潜在精炼管道，通过引入生成先验来恢复VAE编码过程中丢失的空间细节；同时设计了一种动态交互机制，让精炼后的条件潜在与噪声潜在进行双向交互。

Result: 大量实验表明，该方法显著提升了预训练扩散模型的保真度性能，同时保持了良好的真实感和美学效果。

Conclusion: 该优化策略是即插即用的，可以无缝集成到现有的扩散网络中，为低光视觉任务提供更有效的控制方法。

Abstract: Diffusion-based methods, leveraging pre-trained large models like Stable
Diffusion via ControlNet, have achieved remarkable performance in several
low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods
often sacrifice content fidelity to attain higher perceptual realism. This
issue is exacerbated in low-light scenarios, where severely degraded
information caused by the darkness limits effective control. We identify two
primary causes of fidelity loss: the absence of suitable conditional latent
modeling and the lack of bidirectional interaction between the conditional
latent and noisy latent in the diffusion process. To address this, we propose a
novel optimization strategy for conditioning in pre-trained diffusion models,
enhancing fidelity while preserving realism and aesthetics. Our method
introduces a mechanism to recover spatial details lost during VAE encoding,
i.e., a latent refinement pipeline incorporating generative priors.
Additionally, the refined latent condition interacts dynamically with the noisy
latent, leading to improved restoration performance. Our approach is
plug-and-play, seamlessly integrating into existing diffusion networks to
provide more effective control. Extensive experiments demonstrate significant
fidelity improvements in PTDB methods.

</details>


### [110] [Towards Imperceptible Watermarking Via Environment Illumination for Consumer Cameras](https://arxiv.org/abs/2510.17114)
*Hodaka Kawachi,Tomoya Nakamura,Hiroaki Santo,SaiKiran Kumar Tedla,Trevor Dalton Canham,Yasushi Yagi,Michael S. Brown*

Main category: cs.CV

TL;DR: 提出一种使用LED环境照明生成对人眼不可见但能被消费级相机检测的水印方法，通过光谱调制而非强度调制实现不可感知性


<details>
  <summary>Details</summary>
Motivation: 解决传统可见光通信中水印可见性的问题，为隐私保护和内容验证提供一种不可感知的元数据嵌入方案

Method: 优化LED光源的光谱特性，使其在人眼视觉系统下最小可见，同时保持对消费级相机传感器的高检测性，结合人眼对可见光谱的敏感性、相机传感器光谱敏感性和LED生成宽带白光的能力

Result: 在标准低帧率（30-60 fps）下能够实现水印提取，虽然信息传输速率较低（10秒视频嵌入128位），但足以支持隐私保护和内容验证的关键元数据需求

Conclusion: 该方法为环境照明水印提供了一种实用的解决方案，在保持不可感知性的同时实现了有效的元数据嵌入

Abstract: This paper introduces a method for using LED-based environmental lighting to
produce visually imperceptible watermarks for consumer cameras. Our approach
optimizes an LED light source's spectral profile to be minimally visible to the
human eye while remaining highly detectable by typical consumer cameras. The
method jointly considers the human visual system's sensitivity to visible
spectra, modern consumer camera sensors' spectral sensitivity, and narrowband
LEDs' ability to generate broadband spectra perceived as "white light"
(specifically, D65 illumination). To ensure imperceptibility, we employ
spectral modulation rather than intensity modulation. Unlike conventional
visible light communication, our approach enables watermark extraction at
standard low frame rates (30-60 fps). While the information transfer rate is
modest-embedding 128 bits within a 10-second video clip-this capacity is
sufficient for essential metadata supporting privacy protection and content
verification.

</details>


### [111] [GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection](https://arxiv.org/abs/2510.17131)
*Xin Gao,Jiyao Liu,Guanghao Li,Yueming Lyu,Jianxiong Gao,Weichen Yu,Ningsheng Xu,Liang Wang,Caifeng Shan,Ziwei Liu,Chenyang Si*

Main category: cs.CV

TL;DR: GOOD是一个基于扩散模型的OOD样本生成框架，通过双层级引导机制（图像级和特征级）生成更可控和多样化的OOD样本，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过扰动文本条件嵌入生成OOD样本，存在语义不稳定性和多样性不足的问题，限制了在真实OOD场景下的泛化能力。

Method: 提出双层级引导：1）图像级引导基于对数分割梯度降低输入似然；2）特征级引导基于分类器潜在空间中的k-NN距离促进特征稀疏区域采样。引入统一的自适应OOD评分机制。

Result: 定量和定性分析表明，使用GOOD生成的样本训练可以显著提升OOD检测性能。

Conclusion: GOOD框架通过直接引导扩散采样轨迹，解决了现有方法的局限性，实现了更可控和多样化的OOD样本生成，为OOD检测提供了有效解决方案。

Abstract: Recent advancements have explored text-to-image diffusion models for
synthesizing out-of-distribution (OOD) samples, substantially enhancing the
performance of OOD detection. However, existing approaches typically rely on
perturbing text-conditioned embeddings, resulting in semantic instability and
insufficient shift diversity, which limit generalization to realistic OOD. To
address these challenges, we propose GOOD, a novel and flexible framework that
directly guides diffusion sampling trajectories towards OOD regions using
off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level
guidance: (1) Image-level guidance based on the gradient of log partition to
reduce input likelihood, drives samples toward low-density regions in pixel
space. (2) Feature-level guidance, derived from k-NN distance in the
classifier's latent space, promotes sampling in feature-sparse regions. Hence,
this dual-guidance design enables more controllable and diverse OOD sample
generation. Additionally, we introduce a unified OOD score that adaptively
combines image and feature discrepancies, enhancing detection robustness. We
perform thorough quantitative and qualitative analyses to evaluate the
effectiveness of GOOD, demonstrating that training with samples generated by
GOOD can notably enhance OOD detection performance.

</details>


### [112] [KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation](https://arxiv.org/abs/2510.17137)
*WenBo Xu,Liu Liu,Li Zhang,Ran Zhang,Hao Wu,Dan Guo,Meng Wang*

Main category: cs.CV

TL;DR: KineDiff3D是一个统一的框架，通过运动学感知扩散模型从单视图输入重建多样化铰接物体实例并进行姿态估计。


<details>
  <summary>Details</summary>
Motivation: 铰接物体（如笔记本电脑和抽屉）由于其多部件几何结构和可变的关节配置，在3D重建和姿态估计方面存在显著挑战，这些结构多样性在不同状态下引入了复杂性。

Method: 首先通过新型运动学感知VAE（KA-VAE）将完整几何（SDFs）、关节角度和部件分割编码到结构化潜在空间；然后使用两个条件扩散模型：一个用于回归全局姿态（SE(3)）和关节参数，另一个用于从部分观测生成运动学感知潜在代码；最后通过迭代优化模块双向优化重建精度和运动学参数。

Result: 在合成、半合成和真实世界数据集上的实验结果表明，该方法在准确重建铰接物体和估计其运动学特性方面具有有效性。

Conclusion: KineDiff3D框架成功解决了铰接物体的3D重建和运动学参数估计问题，通过运动学感知的扩散模型实现了准确的形状重建和姿态估计。

Abstract: Articulated objects, such as laptops and drawers, exhibit significant
challenges for 3D reconstruction and pose estimation due to their multi-part
geometries and variable joint configurations, which introduce structural
diversity across different states. To address these challenges, we propose
KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object
Shape Reconstruction and Generation, a unified framework for reconstructing
diverse articulated instances and pose estimation from single view input.
Specifically, we first encode complete geometry (SDFs), joint angles, and part
segmentation into a structured latent space via a novel Kinematic-Aware VAE
(KA-VAE). In addition, we employ two conditional diffusion models: one for
regressing global pose (SE(3)) and joint parameters, and another for generating
the kinematic-aware latent code from partial observations. Finally, we produce
an iterative optimization module that bidirectionally refines reconstruction
accuracy and kinematic parameters via Chamfer-distance minimization while
preserving articulation constraints. Experimental results on synthetic,
semi-synthetic, and real-world datasets demonstrate the effectiveness of our
approach in accurately reconstructing articulated objects and estimating their
kinematic properties.

</details>


### [113] [GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image](https://arxiv.org/abs/2510.17157)
*Yinghui Wang,Xinyu Zhang,Peng Du*

Main category: cs.CV

TL;DR: GACO-CAD是一个两阶段后训练框架，通过深度和法线图作为几何先验，结合强化学习的组长度奖励，从单张图像生成可编辑的CAD模型，在几何精度和建模简洁性上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在从2D图像推断3D几何时空间推理能力有限的问题，降低工业概念设计的门槛。

Method: 两阶段框架：1）监督微调阶段使用深度和表面法线图作为密集几何先验，与RGB图像形成多通道输入；2）强化学习阶段引入组长度奖励，促进生成更紧凑的建模序列。

Result: 在DeepCAD和Fusion360数据集上达到SOTA性能，在代码有效性、几何精度和建模简洁性方面均优于现有方法。

Conclusion: GACO-CAD通过几何先验和简洁建模奖励机制，有效提升了从单图像生成CAD模型的几何准确性和建模效率。

Abstract: Generating editable, parametric CAD models from a single image holds great
potential to lower the barriers of industrial concept design. However, current
multi-modal large language models (MLLMs) still struggle with accurately
inferring 3D geometry from 2D images due to limited spatial reasoning
capabilities. We address this limitation by introducing GACO-CAD, a novel
two-stage post-training framework. It is designed to achieve a joint objective:
simultaneously improving the geometric accuracy of the generated CAD models and
encouraging the use of more concise modeling procedures. First, during
supervised fine-tuning, we leverage depth and surface normal maps as dense
geometric priors, combining them with the RGB image to form a multi-channel
input. In the context of single-view reconstruction, these priors provide
complementary spatial cues that help the MLLM more reliably recover 3D geometry
from 2D observations. Second, during reinforcement learning, we introduce a
group length reward that, while preserving high geometric fidelity, promotes
the generation of more compact and less redundant parametric modeling
sequences. A simple dynamic weighting strategy is adopted to stabilize
training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD
achieves state-of-the-art performance under the same MLLM backbone,
consistently outperforming existing methods in terms of code validity,
geometric accuracy, and modeling conciseness.

</details>


### [114] [Investigating Adversarial Robustness against Preprocessing used in Blackbox Face Recognition](https://arxiv.org/abs/2510.17169)
*Roland Croft,Brian Du,Darcy Joseph,Sharath Kumar*

Main category: cs.CV

TL;DR: 该论文研究了人脸识别系统中预处理步骤对对抗攻击迁移性的影响，发现人脸检测模型的选择会显著降低攻击成功率，并提出了一种预处理不变的方法来提高攻击迁移性。


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到对抗样本攻击，但现有的黑盒攻击往往忽略了预处理步骤的重要性。本文旨在研究不同预处理技术对对抗攻击迁移性的影响。

Method: 研究了多种最先进的对抗攻击方法在不同预处理技术下的迁移性，分析了人脸检测模型和降采样插值方法的影响，并提出了一种基于输入变换的预处理不变方法。

Result: 人脸检测模型的选择可使攻击成功率降低高达78%，而插值方法影响较小。提出的预处理不变方法可将攻击迁移性提高高达27%。

Conclusion: 预处理在人脸识别系统中具有重要作用，考虑预处理因素可以显著提高人脸对抗样本的对抗泛化能力。

Abstract: Face Recognition (FR) models have been shown to be vulnerable to adversarial
examples that subtly alter benign facial images, exposing blind spots in these
systems, as well as protecting user privacy. End-to-end FR systems first obtain
preprocessed faces from diverse facial imagery prior to computing the
similarity of the deep feature embeddings. Whilst face preprocessing is a
critical component of FR systems, and hence adversarial attacks against them,
we observe that this preprocessing is often overlooked in blackbox settings.
Our study seeks to investigate the transferability of several out-of-the-box
state-of-the-art adversarial attacks against FR when applied against different
preprocessing techniques used in a blackbox setting. We observe that the choice
of face detection model can degrade the attack success rate by up to 78%,
whereas choice of interpolation method during downsampling has relatively
minimal impacts. Furthermore, we find that the requirement for facial
preprocessing even degrades attack strength in a whitebox setting, due to the
unintended interaction of produced noise vectors against face detection models.
Based on these findings, we propose a preprocessing-invariant method using
input transformations that improves the transferability of the studied attacks
by up to 27%. Our findings highlight the importance of preprocessing in FR
systems, and the need for its consideration towards improving the adversarial
generalisation of facial adversarial examples.

</details>


### [115] [Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling](https://arxiv.org/abs/2510.17171)
*Feihong Yan,Peiru Wang,Yao Zhu,Kaiyu Pang,Qingyan Wei,Huiqi Li,Linfeng Zhang*

Main category: cs.CV

TL;DR: GtR是一种无需训练的分层采样策略，通过将图像生成分解为结构生成和细节重建两个阶段，在保持生成质量的同时实现3.72倍加速


<details>
  <summary>Details</summary>
Motivation: 解决MAR模型在并行生成视觉内容时，由于空间相关视觉标记的建模复杂性而导致的加速潜力受限问题

Method: 提出Generation then Reconstruction (GtR)两阶段策略：第一阶段缓慢生成全局语义结构，第二阶段快速重建剩余细节标记；并引入Frequency-Weighted Token Selection (FTS)为图像细节区域分配更多计算资源

Result: 在ImageNet类条件生成和文本到图像生成任务中实现3.72倍加速，同时保持可比质量（FID: 1.59, IS: 304.4 vs 原始1.59, 299.1）

Conclusion: GtR方法显著优于现有加速方法，在不同模型规模和生成任务上均表现出色，证明了分层采样策略在MAR模型加速中的有效性

Abstract: Masked Autoregressive (MAR) models promise better efficiency in visual
generation than autoregressive (AR) models for the ability of parallel
generation, yet their acceleration potential remains constrained by the
modeling complexity of spatially correlated visual tokens in a single step. To
address this limitation, we introduce Generation then Reconstruction (GtR), a
training-free hierarchical sampling strategy that decomposes generation into
two stages: structure generation establishing global semantic scaffolding,
followed by detail reconstruction efficiently completing remaining tokens.
Assuming that it is more difficult to create an image from scratch than to
complement images based on a basic image framework, GtR is designed to achieve
acceleration by computing the reconstruction stage quickly while maintaining
the generation quality by computing the generation stage slowly. Moreover,
observing that tokens on the details of an image often carry more semantic
information than tokens in the salient regions, we further propose
Frequency-Weighted Token Selection (FTS) to offer more computation budget to
tokens on image details, which are localized based on the energy of high
frequency information. Extensive experiments on ImageNet class-conditional and
text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining
comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1),
substantially outperforming existing acceleration methods across various model
scales and generation tasks. Our codes will be released in
https://github.com/feihongyan1/GtR.

</details>


### [116] [Benchmarking Out-of-Distribution Detection for Plankton Recognition: A Systematic Evaluation of Advanced Methods in Marine Ecological Monitoring](https://arxiv.org/abs/2510.17179)
*Yingzi Han,Jiakai He,Chuanlong Xie,Jianping Li*

Main category: cs.CV

TL;DR: 该论文针对浮游生物识别中存在的分布偏移问题，首次构建了大规模OoD检测基准，并系统评估了22种方法，发现ViM方法在远OoD场景下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 浮游生物识别模型在真实部署中面临分布偏移挑战，由于浮游生物形态复杂、物种多样性高且不断有新物种发现，导致推理时出现不可预测错误。当前缺乏系统整合最新计算机视觉发展和统一评估基准。

Method: 基于DYB-PlanktonNet数据集精心设计了一系列模拟不同分布偏移场景的OoD基准，系统评估了22种OoD检测方法。

Result: 大量实验结果表明，ViM方法在构建的基准中显著优于其他方法，特别是在远OoD场景下关键指标有显著提升。

Conclusion: 该研究为浮游生物自动识别中的算法选择提供了可靠参考，为未来浮游生物OoD检测研究奠定了坚实基础，是浮游生物识别领域首次大规模系统性的OoD检测方法评估分析。

Abstract: Automated plankton recognition models face significant challenges during
real-world deployment due to distribution shifts (Out-of-Distribution, OoD)
between training and test data. This stems from plankton's complex
morphologies, vast species diversity, and the continuous discovery of novel
species, which leads to unpredictable errors during inference. Despite rapid
advancements in OoD detection methods in recent years, the field of plankton
recognition still lacks a systematic integration of the latest computer vision
developments and a unified benchmark for large-scale evaluation. To address
this, this paper meticulously designed a series of OoD benchmarks simulating
various distribution shift scenarios based on the DYB-PlanktonNet dataset
\cite{875n-f104-21}, and systematically evaluated twenty-two OoD detection
methods. Extensive experimental results demonstrate that the ViM
\cite{wang2022vim} method significantly outperforms other approaches in our
constructed benchmarks, particularly excelling in Far-OoD scenarios with
substantial improvements in key metrics. This comprehensive evaluation not only
provides a reliable reference for algorithm selection in automated plankton
recognition but also lays a solid foundation for future research in plankton
OoD detection. To our knowledge, this study marks the first large-scale,
systematic evaluation and analysis of Out-of-Distribution data detection
methods in plankton recognition. Code is available at
https://github.com/BlackJack0083/PlanktonOoD.

</details>


### [117] [Capturing Head Avatar with Hand Contacts from a Monocular Video](https://arxiv.org/abs/2510.17181)
*Haonan He,Yufeng Zheng,Jie Song*

Main category: cs.CV

TL;DR: 提出了一种联合学习详细头部虚拟形象和手脸交互引起的非刚性变形的新框架，解决了手脸交互中相对姿态捕捉和手引起面部变形学习的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部区域，忽略了手脸自然交互（如手托下巴、手指轻触脸颊等传达认知状态的重要交互），这些交互对于创建逼真的3D头部虚拟形象至关重要。

Method: 1）在姿态跟踪中结合深度顺序损失和接触正则化确保正确空间关系；2）从手脸交互数据集学习手引起面部变形的PCA基；3）引入基于物理模拟的接触损失减少穿插伪影。

Result: 在iPhone拍摄的RGB(D)视频和合成数据集上评估，相比最先进的表面重建方法，能捕捉更好的外观和更准确的面部变形几何。

Conclusion: 该方法能够有效处理手脸交互场景，生成更逼真和物理合理的3D头部虚拟形象，在手脸交互建模方面优于现有方法。

Abstract: Photorealistic 3D head avatars are vital for telepresence, gaming, and VR.
However, most methods focus solely on facial regions, ignoring natural
hand-face interactions, such as a hand resting on the chin or fingers gently
touching the cheek, which convey cognitive states like pondering. In this work,
we present a novel framework that jointly learns detailed head avatars and the
non-rigid deformations induced by hand-face interactions.
  There are two principal challenges in this task. First, naively tracking hand
and face separately fails to capture their relative poses. To overcome this, we
propose to combine depth order loss with contact regularization during pose
tracking, ensuring correct spatial relationships between the face and hand.
Second, no publicly available priors exist for hand-induced deformations,
making them non-trivial to learn from monocular videos. To address this, we
learn a PCA basis specific to hand-induced facial deformations from a face-hand
interaction dataset. This reduces the problem to estimating a compact set of
PCA parameters rather than a full spatial deformation field. Furthermore,
inspired by physics-based simulation, we incorporate a contact loss that
provides additional supervision, significantly reducing interpenetration
artifacts and enhancing the physical plausibility of the results.
  We evaluate our approach on RGB(D) videos captured by an iPhone.
Additionally, to better evaluate the reconstructed geometry, we construct a
synthetic dataset of avatars with various types of hand interactions. We show
that our method can capture better appearance and more accurate deforming
geometry of the face than SOTA surface reconstruction methods.

</details>


### [118] [HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery](https://arxiv.org/abs/2510.17188)
*Vaibhav Rathore,Divyam Gupta,Biplab Banerjee*

Main category: cs.CV

TL;DR: HIDISC是一个基于双曲表示学习的领域泛化广义类别发现框架，通过GPT引导的扩散增强和切线CutMix插值，在无需目标域数据的情况下实现对新领域和新类别的泛化。


<details>
  <summary>Details</summary>
Motivation: 现有GCD方法假设训练和测试数据来自同一领域，无法处理开放世界中的分布偏移问题。DG-GCD旨在解决这一限制，但现有方法计算成本高且存在误差累积。

Method: 使用GPT引导的扩散增强生成多样化领域变化；提出切线CutMix进行曲率感知插值；结合惩罚Busemann对齐、混合双曲对比正则化和自适应异常值排斥的统一损失函数；可学习曲率参数适应数据集复杂性。

Result: 在PACS、Office-Home和DomainNet数据集上达到最先进水平，一致优于现有的欧几里得和双曲(DG)-GCD基线方法。

Conclusion: HIDISC通过双曲表示学习框架有效解决了DG-GCD问题，在保持效率的同时实现了领域和类别级别的泛化。

Abstract: Generalized Category Discovery (GCD) aims to classify test-time samples into
either seen categories** -- available during training -- or novel ones, without
relying on label supervision. Most existing GCD methods assume simultaneous
access to labeled and unlabeled data during training and arising from the same
domain, limiting applicability in open-world scenarios involving distribution
shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by
requiring models to generalize to unseen domains containing novel categories,
without accessing targetdomain data during training. The only prior DG-GCD
method, DG2CD-Net, relies on episodic training with multiple synthetic domains
and task vector aggregation, incurring high computational cost and error
accumulation. We propose HIDISC, a hyperbolic representation learning framework
that achieves domain and category-level generalization without episodic
simulation. To expose the model to minimal but diverse domain variations, we
augment the source domain using GPT-guided diffusion, avoiding overfitting
while maintaining efficiency. To structure the representation space, we
introduce Tangent CutMix, a curvature-aware interpolation that synthesizes
pseudo-novel samples in tangent space, preserving manifold consistency. A
unified loss -- combining penalized Busemann alignment, hybrid hyperbolic
contrastive regularization, and adaptive outlier repulsion -- **facilitates
compact, semantically structured embeddings. A learnable curvature parameter
further adapts the geometry to dataset complexity. HIDISC achieves
state-of-the-art results on PACS , Office-Home , and DomainNet, consistently
outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.

</details>


### [119] [ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models](https://arxiv.org/abs/2510.17197)
*Pu Zhang,Yuwei Li,Xingyuan Xian,Guoming Tang*

Main category: cs.CV

TL;DR: 提出一种零样本的视觉令牌剪枝方法，通过平衡任务相关性和信息多样性来减少视觉语言模型的推理成本，在剪枝90%令牌的情况下仍能保持高性能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型处理能力的增强，视觉令牌冗余导致推理成本急剧上升。现有剪枝方法忽视文本提示的指导，无法优先考虑任务相关性。

Method: 采用分层方法：首先选择核心任务相关视觉令牌，然后补充多样性令牌以保留更广泛上下文。引入提示感知视角，明确建模视觉令牌剪枝的任务相关性与信息多样性平衡。

Result: 在多个模型和基准测试中，该方法在剪枝高达90%令牌的情况下，性能达到或超越最先进方法，仅造成最小精度损失，同时显著降低GPU内存占用和推理延迟。

Conclusion: 提出的提示感知视觉令牌剪枝方法有效解决了视觉令牌冗余问题，在保持模型性能的同时大幅降低了推理成本，为视觉语言模型的高效部署提供了可行方案。

Abstract: As the capabilities of Vision-Language Models (VLMs) advance, they can
process increasingly large inputs, which, unlike in LLMs, generates significant
visual token redundancy and leads to prohibitive inference costs. While many
methods aim to reduce these costs by pruning visual tokens, existing
approaches, whether based on attention or diversity, typically neglect the
guidance of the text prompt and thus fail to prioritize task relevance. In this
work, we propose a novel, zero-shot method that reframes the problem by
introducing a prompt-aware perspective, explicitly modeling visual token
pruning as a balance between task relevance and information diversity. Our
hierarchical approach first selects a core set of task-relevant visual tokens
and then supplements them with diversity tokens to preserve broader context.
Experiments across multiple models and benchmarks show that our method achieves
performance that matches or surpasses the state-of-the-art with only minimal
accuracy loss, even when pruning up to 90\% of the tokens. Furthermore, these
gains are accompanied by significant reductions in GPU memory footprint and
inference latency.

</details>


### [120] [From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh](https://arxiv.org/abs/2510.17198)
*M Saifuzzaman Rafat,Mohd Ruhul Ameen,Akif Islam,Abu Saleh Musa Miah,Jungpil Shin*

Main category: cs.CV

TL;DR: 使用Segment Anything Model (SAM)进行微调，以高精度监测孟加拉国河流侵蚀，并创建了首个包含消失定居点注释的数据集。


<details>
  <summary>Details</summary>
Motivation: 孟加拉国河流每年侵蚀大量土地和村庄，传统监测方法困难，需要更精确的自动化工具来跟踪这一缓慢的灾难。

Method: 利用历史Google Earth图像构建新数据集，先通过简单颜色通道分析进行粗略分割，然后微调SAM的掩码解码器以识别河岸侵蚀的细微特征。

Result: 模型在河岸侵蚀监测中表现出色，平均IoU达到86.30%，Dice分数达到92.60%，显著优于传统方法和现成的深度学习模型。

Conclusion: 该研究提供了监测河流侵蚀的强大新工具，有助于政策制定者和灾害管理机构预测侵蚀轨迹并保护脆弱社区。

Abstract: The great rivers of Bangladesh, arteries of commerce and sustenance, are also
agents of relentless destruction. Each year, they swallow whole villages and
vast tracts of farmland, erasing communities from the map and displacing
thousands of families. To track this slow-motion catastrophe has, until now,
been a Herculean task for human analysts. Here we show how a powerful
general-purpose vision model, the Segment Anything Model (SAM), can be adapted
to this task with remarkable precision. To do this, we assembled a new dataset
- a digital chronicle of loss compiled from historical Google Earth imagery of
Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur
Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially,
this dataset is the first to include manually annotated data on the settlements
that have vanished beneath the water. Our method first uses a simple
color-channel analysis to provide a rough segmentation of land and water, and
then fine-tunes SAM's mask decoder to recognize the subtle signatures of
riverbank erosion. The resulting model demonstrates a keen eye for this
destructive process, achieving a mean Intersection over Union of 86.30% and a
Dice score of 92.60% - a performance that significantly surpasses traditional
methods and off-the-shelf deep learning models. This work delivers three key
contributions: the first annotated dataset of disappeared settlements in
Bangladesh due to river erosion; a specialized AI model fine-tuned for this
critical task; and a method for quantifying land loss with compelling visual
evidence. Together, these tools provide a powerful new lens through which
policymakers and disaster management agencies can monitor erosion, anticipate
its trajectory, and ultimately protect the vulnerable communities in its path.

</details>


### [121] [Round Outcome Prediction in VALORANT Using Tactical Features from Video Analysis](https://arxiv.org/abs/2510.17199)
*Nirai Hayakawa,Kazumasa Shimari,Kazuma Yamasaki,Hirotatsu Hoshikawa,Rikuto Tsuchida,Kenichi Matsumoto*

Main category: cs.CV

TL;DR: 基于TimeSformer视频识别模型，通过分析VALORANT游戏小地图中的战术特征（如角色位置和游戏内事件）来预测回合结果，在数据集增强后达到约81%的预测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有电竞比赛结果预测研究多基于比赛日志数据和统计信息，但VALORANT作为需要复杂策略的FPS游戏，需要更深入的战术分析。本研究旨在通过分析比赛录像中的小地图信息来构建更准确的回合结果预测模型。

Method: 基于TimeSformer视频识别模型，提取小地图信息中的详细战术特征，包括角色位置信息和游戏内事件，并通过数据集增强来提升模型性能。

Result: 在增强数据集上训练的模型达到了约81%的预测准确率，特别是在回合的中后期阶段表现显著优于仅使用小地图信息本身的模型。

Conclusion: 利用比赛录像中的战术特征对于VALORANT回合结果预测非常有效，证明了基于视觉信息的战术分析在电竞预测中的潜力。

Abstract: Recently, research on predicting match outcomes in esports has been actively
conducted, but much of it is based on match log data and statistical
information. This research targets the FPS game VALORANT, which requires
complex strategies, and aims to build a round outcome prediction model by
analyzing minimap information in match footage. Specifically, based on the
video recognition model TimeSformer, we attempt to improve prediction accuracy
by incorporating detailed tactical features extracted from minimap information,
such as character position information and other in-game events. This paper
reports preliminary results showing that a model trained on a dataset augmented
with such tactical event labels achieved approximately 81% prediction accuracy,
especially from the middle phases of a round onward, significantly
outperforming a model trained on a dataset with the minimap information itself.
This suggests that leveraging tactical features from match footage is highly
effective for predicting round outcomes in VALORANT.

</details>


### [122] [EndoCIL: A Class-Incremental Learning Framework for Endoscopic Image Classification](https://arxiv.org/abs/2510.17200)
*Bingrong Liu,Jun Shi,Yushan Zheng*

Main category: cs.CV

TL;DR: EndoCIL是一个专门针对内窥镜图像诊断的类增量学习框架，通过三个关键组件解决现有方法在领域差异和类别不平衡方面的不足，在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 内窥镜图像分析需要模型能够持续适应新的临床数据，同时保持对已学习类别的性能。现有基于回放的类增量学习方法由于内窥镜图像固有的严重领域差异和类别不平衡问题，无法有效缓解灾难性遗忘。

Method: EndoCIL包含三个核心组件：1）基于最大均值差异的回放（MDBR），使用分布对齐的贪婪策略选择多样且有代表性的样本；2）先验正则化类别平衡损失（PRCBL），通过整合先验类别分布和平衡权重来缓解阶段间和阶段内的类别不平衡；3）全连接层梯度校准（CFG），调整分类器梯度以减少对新类别的偏置。

Result: 在四个公共内窥镜数据集上的广泛实验表明，EndoCIL在不同缓冲区大小和评估指标下普遍优于最先进的类增量学习方法。

Conclusion: 该框架有效平衡了终身内窥镜诊断中的稳定性和可塑性，在临床可扩展性和部署方面显示出良好潜力。

Abstract: Class-incremental learning (CIL) for endoscopic image analysis is crucial for
real-world clinical applications, where diagnostic models should continuously
adapt to evolving clinical data while retaining performance on previously
learned ones. However, existing replay-based CIL methods fail to effectively
mitigate catastrophic forgetting due to severe domain discrepancies and class
imbalance inherent in endoscopic imaging. To tackle these challenges, we
propose EndoCIL, a novel and unified CIL framework specifically tailored for
endoscopic image diagnosis. EndoCIL incorporates three key components: Maximum
Mean Discrepancy Based Replay (MDBR), employing a distribution-aligned greedy
strategy to select diverse and representative exemplars, Prior Regularized
Class Balanced Loss (PRCBL), designed to alleviate both inter-phase and
intra-phase class imbalance by integrating prior class distributions and
balance weights into the loss function, and Calibration of Fully-Connected
Gradients (CFG), which adjusts the classifier gradients to mitigate bias toward
new classes. Extensive experiments conducted on four public endoscopic datasets
demonstrate that EndoCIL generally outperforms state-of-the-art CIL methods
across varying buffer sizes and evaluation metrics. The proposed framework
effectively balances stability and plasticity in lifelong endoscopic diagnosis,
showing promising potential for clinical scalability and deployment.

</details>


### [123] [Optimizing DINOv2 with Registers for Face Anti-Spoofing](https://arxiv.org/abs/2510.17201)
*Mika Feng,Pierre Gallin-Martel,Koichi Ito,Takafumi Aoki*

Main category: cs.CV

TL;DR: 提出基于DINOv2的活体检测方法，通过注意力机制抑制扰动来识别真假人脸图像的细微差异


<details>
  <summary>Details</summary>
Motivation: 人脸识别系统容易受到照片欺骗攻击，需要在识别前检测活体性以防止恶意绕过认证

Method: 使用带registers的DINOv2模型提取通用特征，抑制注意力机制中的扰动，聚焦于关键细微特征

Result: 在ICCV2025反欺骗研讨会数据集和SiW数据集上验证了方法的有效性

Conclusion: 该方法能够有效检测物理-数字混合攻击，提高人脸识别系统的安全性

Abstract: Face recognition systems are designed to be robust against variations in head
pose, illumination, and image blur during capture. However, malicious actors
can exploit these systems by presenting a face photo of a registered user,
potentially bypassing the authentication process. Such spoofing attacks must be
detected prior to face recognition. In this paper, we propose a DINOv2-based
spoofing attack detection method to discern minute differences between live and
spoofed face images. Specifically, we employ DINOv2 with registers to extract
generalizable features and to suppress perturbations in the attention
mechanism, which enables focused attention on essential and minute features. We
demonstrate the effectiveness of the proposed method through experiments
conducted on the dataset provided by ``The 6th Face Anti-Spoofing Workshop:
Unified Physical-Digital Attacks Detection@ICCV2025'' and SiW dataset.

</details>


### [124] [$\mathcal{V}isi\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs](https://arxiv.org/abs/2510.17205)
*Yingqi Fan,Anhao Zhao,Jinlan Fu,Junlong Tong,Hui Su,Yijie Pan,Wei Zhang,Xiaoyu Shen*

Main category: cs.CV

TL;DR: 本文揭示了MLLMs存在三阶段跨模态交互过程，并基于此提出了VisiPruner训练无关剪枝框架，显著减少计算开销。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中表现出色，但存在计算开销大的问题，现有剪枝方法缺乏对MLLMs处理多模态信息机制的根本理解。

Method: 通过系统分析发现MLLMs的三阶段跨模态交互过程，并基于此设计VisiPruner框架，该框架无需训练即可剪枝。

Result: VisiPruner在LLaVA-v1.5 7B上减少99%视觉相关注意力计算和53.9% FLOPs，性能优于现有剪枝方法，且具有良好泛化性。

Conclusion: 研究不仅提供了有效的剪枝方案，还为训练高效MLLMs提供了可操作的指导原则，即模型架构应与内在分层处理动态对齐。

Abstract: Multimodal Large Language Models (MLLMs) have achieved strong performance
across vision-language tasks, but suffer from significant computational
overhead due to the quadratic growth of attention computations with the number
of multimodal tokens. Though efforts have been made to prune tokens in MLLMs,
\textit{they lack a fundamental understanding of how MLLMs process and fuse
multimodal information.} Through systematic analysis, we uncover a
\textbf{three-stage} cross-modal interaction process: (1) Shallow layers
recognize task intent, with visual tokens acting as passive attention sinks;
(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few
critical visual tokens; (3) Deep layers discard vision tokens, focusing solely
on linguistic refinement. Based on these findings, we propose
\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\% of
vision-related attention computations and 53.9\% of FLOPs on LLaVA-v1.5 7B. It
significantly outperforms existing token pruning methods and generalizes across
diverse MLLMs. Beyond pruning, our insights further provide actionable
guidelines for training efficient MLLMs by aligning model architecture with its
intrinsic layer-wise processing dynamics. Our code is available at:
https://github.com/EIT-NLP/VisiPruner.

</details>


### [125] [When One Moment Isn't Enough: Multi-Moment Retrieval with Cross-Moment Interactions](https://arxiv.org/abs/2510.17218)
*Zhuo Cao,Heming Du,Bingqing Zhang,Xin Yu,Xue Li,Sen Wang*

Main category: cs.CV

TL;DR: 该论文提出了QV-M²多时刻检索数据集和FlashMMR框架，解决了现有单时刻检索方法在真实应用中的不足，通过多时刻后验证模块提升了视频时序定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有时刻检索方法主要关注单时刻检索，但真实应用中一个查询可能对应多个相关时刻，现有数据集和方法无法满足视频时序定位需求。

Method: 提出了FlashMMR框架，包含多时刻后验证模块，通过约束时序调整和验证模块重新评估候选片段，修剪低置信度提议，实现鲁棒的多时刻对齐。

Result: 在QV-M²数据集上，FlashMMR相比之前SOTA方法在G-mAP上提升3.00%，在mAP@3+tgt上提升2.70%，在mR@3上提升2.56%。

Conclusion: QV-M²数据集和FlashMMR方法为推进更现实和具有挑战性的视频时序定位场景研究奠定了基础。

Abstract: Existing Moment retrieval (MR) methods focus on Single-Moment Retrieval
(SMR). However, one query can correspond to multiple relevant moments in
real-world applications. This makes the existing datasets and methods
insufficient for video temporal grounding. By revisiting the gap between
current MR tasks and real-world applications, we introduce a high-quality
datasets called QVHighlights Multi-Moment Dataset (QV-M$^2$), along with new
evaluation metrics tailored for multi-moment retrieval (MMR). QV-M$^2$ consists
of 2,212 annotations covering 6,384 video segments. Building on existing
efforts in MMR, we propose a framework called FlashMMR. Specifically, we
propose a Multi-moment Post-verification module to refine the moment
boundaries. We introduce constrained temporal adjustment and subsequently
leverage a verification module to re-evaluate the candidate segments. Through
this sophisticated filtering pipeline, low-confidence proposals are pruned, and
robust multi-moment alignment is achieved. We retrain and evaluate 6 existing
MR methods on QV-M$^2$ and QVHighlights under both SMR and MMR settings.
Results show that QV-M$^2$ serves as an effective benchmark for training and
evaluating MMR models, while FlashMMR provides a strong baseline. Specifically,
on QV-M$^2$, it achieves improvements over prior SOTA method by 3.00% on G-mAP,
2.70% on mAP@3+tgt, and 2.56% on mR@3. The proposed benchmark and method
establish a foundation for advancing research in more realistic and challenging
video temporal grounding scenarios. Code is released at
https://github.com/Zhuo-Cao/QV-M2.

</details>


### [126] [Fair and Interpretable Deepfake Detection in Videos](https://arxiv.org/abs/2510.17264)
*Akihito Yoshii,Ryosuke Sonoda,Ramya Srinivasan*

Main category: cs.CV

TL;DR: 提出一个公平感知的深度伪造检测框架，整合时序特征学习和人口统计感知数据增强，以提升公平性和可解释性


<details>
  <summary>Details</summary>
Motivation: 现有深度伪造检测方法存在偏见、缺乏透明度、无法捕捉时序信息，导致不同人口统计群体的决策偏见和不可靠结果

Method: 利用基于序列的聚类进行深度伪造视频的时序建模，概念提取提升检测可靠性，引入人口统计感知数据增强方法平衡欠代表群体并应用频域变换保留伪造伪影

Result: 在FaceForensics++、DFD、Celeb-DF和DFDC数据集上的广泛实验表明，该方法在公平性和准确性之间取得了最佳平衡

Conclusion: 所提出的公平感知深度伪造检测框架在公平性和准确性方面优于现有最先进方法

Abstract: Existing deepfake detection methods often exhibit bias, lack transparency,
and fail to capture temporal information, leading to biased decisions and
unreliable results across different demographic groups. In this paper, we
propose a fairness-aware deepfake detection framework that integrates temporal
feature learning and demographic-aware data augmentation to enhance fairness
and interpretability. Our method leverages sequence-based clustering for
temporal modeling of deepfake videos and concept extraction to improve
detection reliability while also facilitating interpretable decisions for
non-expert users. Additionally, we introduce a demography-aware data
augmentation method that balances underrepresented groups and applies
frequency-domain transformations to preserve deepfake artifacts, thereby
mitigating bias and improving generalization. Extensive experiments on
FaceForensics++, DFD, Celeb-DF, and DFDC datasets using state-of-the-art (SoTA)
architectures (Xception, ResNet) demonstrate the efficacy of the proposed
method in obtaining the best tradeoff between fairness and accuracy when
compared to SoTA.

</details>


### [127] [FineVision: Open Data Is All You Need](https://arxiv.org/abs/2510.17269)
*Luis Wiedmann,Orr Zohar,Amir Mahla,Xiaohan Wang,Rui Li,Thibaud Frere,Leandro von Werra,Aritra Roy Gosthipaty,Andrés Marafioti*

Main category: cs.CV

TL;DR: FineVision是一个精心收集、整理和统一的大规模视觉语言数据集，包含2400万个样本，是同类中最大的开放资源。通过半自动化流程整合200多个数据源，并进行严格去重和去污染处理。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型的发展受到数据集碎片化、不一致性和污染问题的阻碍，需要高质量、统一的大规模数据集来推动研究。

Method: 采用半自动化、人机协同的流程：自动化处理批量数据摄入和模式映射，人工审核员审查映射结果并抽样检查，确保标注准确性和数据质量。同时进行严格的去重和去污染处理。

Result: 在FineVision上训练的模型在广泛评估中持续优于现有开放数据集训练的模型，证明了数据规模、数据质量和人机协同监督的优势。

Conclusion: FineVision数据集和整理工具的开源将加速以数据为中心的视觉语言模型研究，强调了高质量数据对模型性能的重要性。

Abstract: The advancement of vision-language models (VLMs) is hampered by a fragmented
landscape of inconsistent and contaminated public datasets. We introduce
FineVision, a meticulously collected, curated, and unified corpus of 24 million
samples - the largest open resource of its kind. We unify more than 200 sources
into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation
performs bulk ingestion and schema mapping, while reviewers audit mappings and
spot-check outputs to verify faithful consumption of annotations, appropriate
formatting and diversity, and safety; issues trigger targeted fixes and
re-runs. The workflow further applies rigorous de-duplication within and across
sources and decontamination against 66 public benchmarks. FineVision also
encompasses agentic/GUI tasks with a unified action space; reviewers validate
schemas and inspect a sample of trajectories to confirm executable fidelity.
Models trained on FineVision consistently outperform those trained on existing
open mixtures across a broad evaluation suite, underscoring the benefits of
scale, data hygiene, and balanced automation with human oversight. We release
the corpus and curation tools to accelerate data-centric VLM research.

</details>


### [128] [Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models](https://arxiv.org/abs/2510.17274)
*Katie Luo,Jingwei Ji,Tong He,Runsheng Xu,Yichen Xie,Dragomir Anguelov,Mingxing Tan*

Main category: cs.CV

TL;DR: 提出了Plug-and-Forecast（PnF）方法，通过多模态大语言模型增强现有运动预测模型，利用自然语言描述复杂场景，实现零样本推理能力提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统依赖专门模型进行感知和运动预测，虽然在标准条件下表现可靠，但在多样化真实场景中的成本效益泛化仍面临挑战。

Method: 设计提示词从MLLMs提取结构化场景理解，并将这些信息蒸馏为可学习嵌入来增强现有行为预测模型，无需微调即可实现快速适配。

Result: 在Waymo Open Motion Dataset和nuScenes Dataset上验证，两个最先进的运动预测模型都实现了持续的性能提升。

Conclusion: PnF方法证明了利用MLLMs的自然语言能力可以有效增强运动预测模型的泛化能力，为自动驾驶系统提供实用的增强方案。

Abstract: Current autonomous driving systems rely on specialized models for perceiving
and predicting motion, which demonstrate reliable performance in standard
conditions. However, generalizing cost-effectively to diverse real-world
scenarios remains a significant challenge. To address this, we propose
Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion
forecasting models with multimodal large language models (MLLMs). PnF builds on
the insight that natural language provides a more effective way to describe and
handle complex scenarios, enabling quick adaptation to targeted behaviors. We
design prompts to extract structured scene understanding from MLLMs and distill
this information into learnable embeddings to augment existing behavior
prediction models. Our method leverages the zero-shot reasoning capabilities of
MLLMs to achieve significant improvements in motion prediction performance,
while requiring no fine-tuning -- making it practical to adopt. We validate our
approach on two state-of-the-art motion forecasting models using the Waymo Open
Motion Dataset and the nuScenes Dataset, demonstrating consistent performance
improvements across both benchmarks.

</details>


### [129] [SG-CLDFF: A Novel Framework for Automated White Blood Cell Classification and Segmentation](https://arxiv.org/abs/2510.17278)
*Mehdi Zekriyapanah Gashti,Mostafa Mohammadpour,Ghasem Farjamnia*

Main category: cs.CV

TL;DR: 提出了一种新颖的显著性引导跨层深度特征融合框架（SG-CLDFF），通过结合显著性预处理和多尺度深度特征聚合，提高了白细胞分析的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 白细胞在显微图像中的准确分割和分类对于血液疾病诊断至关重要，但由于染色变异性、复杂背景和类别不平衡等问题，仍然具有挑战性。

Method: SG-CLDFF框架首先计算显著性先验来突出候选白细胞区域，然后使用轻量级混合骨干网络（EfficientSwin风格）生成多分辨率表示，通过ResNeXt-CC启发的跨层融合模块融合浅层和深层特征。采用多任务学习，同时进行分割和细胞类型分类，使用类别感知加权损失和显著性对齐正则化来缓解不平衡问题。

Result: 在标准公共基准（BCCD、LISC、ALL-IDB）上验证，相比强CNN和Transformer基线，在IoU、F1和分类准确率方面取得了一致性提升。消融研究证明了显著性预处理和跨层融合的各自贡献。

Conclusion: SG-CLDFF为临床工作流程中更可靠的自动化白细胞分析提供了一条实用且可解释的路径。

Abstract: Accurate segmentation and classification of white blood cells (WBCs) in
microscopic images are essential for diagnosis and monitoring of many
hematological disorders, yet remain challenging due to staining variability,
complex backgrounds, and class imbalance. In this paper, we introduce a novel
Saliency-Guided Cross-Layer Deep Feature Fusion framework (SG-CLDFF) that
tightly integrates saliency-driven preprocessing with multi-scale deep feature
aggregation to improve both robustness and interpretability for WBC analysis.
SG-CLDFF first computes saliency priors to highlight candidate WBC regions and
guide subsequent feature extraction. A lightweight hybrid backbone
(EfficientSwin-style) produces multi-resolution representations, which are
fused by a ResNeXt-CC-inspired cross-layer fusion module to preserve
complementary information from shallow and deep layers. The network is trained
in a multi-task setup with concurrent segmentation and cell-type classification
heads, using class-aware weighted losses and saliency-alignment regularization
to mitigate imbalance and suppress background activation. Interpretability is
enforced through Grad-CAM visualizations and saliency consistency checks,
allowing model decisions to be inspected at the regional level. We validate the
framework on standard public benchmarks (BCCD, LISC, ALL-IDB), reporting
consistent gains in IoU, F1, and classification accuracy compared to strong CNN
and transformer baselines. An ablation study also demonstrates the individual
contributions of saliency preprocessing and cross-layer fusion. SG-CLDFF offers
a practical and explainable path toward more reliable automated WBC analysis in
clinical workflows.

</details>


### [130] [Machine Vision-Based Surgical Lighting System:Design and Implementation](https://arxiv.org/abs/2510.17287)
*Amir Gharghabi,Mahdi Hakiminezhad,Maryam Shafaei,Shaghayegh Gharghabi*

Main category: cs.CV

TL;DR: 提出了一种基于YOLOv11目标检测算法的新型手术照明系统，通过自动识别蓝色标记来自动调整LED光源位置，减少外科医生疲劳并提高照明一致性。


<details>
  <summary>Details</summary>
Motivation: 传统手术照明系统依赖手动调整，导致外科医生疲劳、颈部劳损以及因漂移和阴影造成的不一致照明问题。

Method: 使用YOLOv11算法识别手术区域上方的蓝色标记，通过两个带有倾斜-平移支架的伺服电机将高功率LED光源引导至识别位置。

Result: YOLO模型在验证集上达到96.7%的mAP@50，验证集包含带有蓝色球形标记的模拟手术场景标注图像。

Conclusion: 这种基于机器视觉的自动化照明解决方案减少了外科医生的身体负担，提高了照明一致性，有助于改善手术效果。

Abstract: Effortless and ergonomically designed surgical lighting is critical for
precision and safety during procedures. However, traditional systems often rely
on manual adjustments, leading to surgeon fatigue, neck strain, and
inconsistent illumination due to drift and shadowing. To address these
challenges, we propose a novel surgical lighting system that leverages the
YOLOv11 object detection algorithm to identify a blue marker placed above the
target surgical site. A high-power LED light source is then directed to the
identified location using two servomotors equipped with tilt-pan brackets. The
YOLO model achieves 96.7% mAP@50 on the validation set consisting of annotated
images simulating surgical scenes with the blue spherical marker. By automating
the lighting process, this machine vision-based solution reduces physical
strain on surgeons, improves consistency in illumination, and supports improved
surgical outcomes.

</details>


### [131] [Exploring Structural Degradation in Dense Representations for Self-supervised Learning](https://arxiv.org/abs/2510.17299)
*Siran Dai,Qianqian Xu,Peisong Wen,Yang Liu,Qingming Huang*

Main category: cs.CV

TL;DR: 本文发现自监督学习中的反直觉现象：长时间训练会损害密集预测任务性能，称为自监督密集退化(SDD)。作者提出了密集表示结构估计器(DSE)来评估无标注的密集性能，并基于此开发了模型选择策略和正则化方法。


<details>
  <summary>Details</summary>
Motivation: 观察到自监督学习训练时间越长，密集预测任务性能反而下降的现象，这在16种SOTA SSL方法中普遍存在。需要解决无标注情况下评估密集性能的挑战。

Method: 提出密集表示结构估计器(DSE)，包含类相关性度量和有效维度度量。基于DSE开发模型选择策略和正则化方法。

Result: 在16种SSL方法和4个基准测试上的实验表明，模型选择平均提升mIoU 3.0%，DSE正则化能有效缓解密集退化问题。

Conclusion: 自监督密集退化是普遍现象，提出的DSE指标和相应方法能有效解决该问题，提升密集预测任务性能。

Abstract: In this work, we observe a counterintuitive phenomenon in self-supervised
learning (SSL): longer training may impair the performance of dense prediction
tasks (e.g., semantic segmentation). We refer to this phenomenon as
Self-supervised Dense Degradation (SDD) and demonstrate its consistent presence
across sixteen state-of-the-art SSL methods with various losses, architectures,
and datasets. When the model performs suboptimally on dense tasks at the end of
training, measuring the performance during training becomes essential. However,
evaluating dense performance effectively without annotations remains an open
challenge. To tackle this issue, we introduce a Dense representation Structure
Estimator (DSE), composed of a class-relevance measure and an effective
dimensionality measure. The proposed DSE is both theoretically grounded and
empirically validated to be closely correlated with the downstream performance.
Based on this metric, we introduce a straightforward yet effective model
selection strategy and a DSE-based regularization method. Experiments on
sixteen SSL methods across four benchmarks confirm that model selection
improves mIoU by $3.0\%$ on average with negligible computational cost.
Additionally, DSE regularization consistently mitigates the effects of dense
degradation. Code is available at
https://github.com/EldercatSAM/SSL-Degradation.

</details>


### [132] [LongInsightBench: A Comprehensive Benchmark for Evaluating Omni-Modal Models on Human-Centric Long-Video Understanding](https://arxiv.org/abs/2510.17305)
*ZhaoYang Han,Qihan Lin,Hao Liang,Bowen Chen,Zhou Liu,Wentao Zhang*

Main category: cs.CV

TL;DR: LongInsightBench是首个专注于评估模型对长视频理解能力的基准测试，涵盖视觉、音频和文本多模态，包含约1000个信息密集的长视频和6个挑战性任务场景。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解基准主要关注短视频片段，缺乏对长视频中复杂上下文信息（如人类语言、视角、动作等）的综合评估能力。

Method: 从FineVideo数据集中精选约1000个长视频，开发三步半自动化数据质量保证流程，设计6个任务场景（包括事件内和事件间任务），进行多模态融合实验。

Result: 实验表明全模态模型在精确时间定位和长程因果推理任务上仍面临挑战，扩展实验揭示了多模态融合中的信息丢失和处理偏差问题。

Conclusion: LongInsightBench为长视频理解提供了全面评估框架，揭示了当前全模态模型的局限性，特别是在时序推理和因果推断方面的不足。

Abstract: We introduce \textbf{LongInsightBench}, the first benchmark designed to
assess models' ability to understand long videos, with a focus on human
language, viewpoints, actions, and other contextual elements, while integrating
\textbf{visual, audio, and text} modalities. Our benchmark excels in three key
areas: \textbf{a) Long-Duration, Information-Dense Videos:} We carefully select
approximately 1,000 videos from open-source datasets FineVideo based on
duration limit and the information density of both visual and audio modalities,
focusing on content like lectures, interviews, and vlogs, which contain rich
language elements. \textbf{b) Diverse and Challenging Task Scenarios:} We have
designed six challenging task scenarios, including both Intra-Event and
Inter-Event Tasks. \textbf{c) Rigorous and Comprehensive Quality Assurance
Pipelines:} We have developed a three-step, semi-automated data quality
assurance pipeline to ensure the difficulty and validity of the synthesized
questions and answer options. Based on LongInsightBench, we designed a series
of experiments. Experimental results shows that Omni-modal models(OLMs) still
face challenge in tasks requiring precise temporal localization (T-Loc) and
long-range causal inference (CE-Caus). Extended experiments reveal the
information loss and processing bias in multi-modal fusion of OLMs. Our dataset
and code is available at
https://anonymous.4open.science/r/LongInsightBench-910F/.

</details>


### [133] [CausalMamba: Scalable Conditional State Space Models for Neural Causal Inference](https://arxiv.org/abs/2510.17318)
*Sangyoon Bae,Jiook Cha*

Main category: cs.CV

TL;DR: CausalMamba是一个可扩展的框架，解决了fMRI因果推断的两个关键问题：从BOLD信号推断神经因果关系的病态性质，以及现有方法（如DCM）的计算不可行性。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI因果推断中的根本限制：从血流动力学扭曲的BOLD信号推断神经因果关系的病态性质，以及现有方法（如动态因果建模DCM）的计算不可行性。

Method: 将复杂的逆问题分解为两个可处理的阶段：BOLD反卷积以恢复潜在神经活动，然后使用新颖的条件Mamba架构进行因果图推断。

Result: 在模拟数据上，CausalMamba比DCM准确率高37%；在真实任务fMRI数据中，恢复已知神经通路的准确率达88%，而传统方法在99%以上的受试者中无法识别这些典型回路。工作记忆数据的网络分析显示大脑会根据刺激策略性地转移其主要因果枢纽。

Conclusion: 这项工作为神经科学家提供了一个实用的工具，用于大规模因果推断，能够捕捉认知功能的基本回路模式和灵活网络动态。

Abstract: We introduce CausalMamba, a scalable framework that addresses fundamental
limitations in fMRI-based causal inference: the ill-posed nature of inferring
neural causality from hemodynamically distorted BOLD signals and the
computational intractability of existing methods like Dynamic Causal Modeling
(DCM). Our approach decomposes this complex inverse problem into two tractable
stages: BOLD deconvolution to recover latent neural activity, followed by
causal graph inference using a novel Conditional Mamba architecture. On
simulated data, CausalMamba achieves 37% higher accuracy than DCM. Critically,
when applied to real task fMRI data, our method recovers well-established
neural pathways with 88% fidelity, whereas conventional approaches fail to
identify these canonical circuits in over 99% of subjects. Furthermore, our
network analysis of working memory data reveals that the brain strategically
shifts its primary causal hub-recruiting executive or salience networks
depending on the stimulus-a sophisticated reconfiguration that remains
undetected by traditional methods. This work provides neuroscientists with a
practical tool for large-scale causal inference that captures both fundamental
circuit motifs and flexible network dynamics underlying cognitive function.

</details>


### [134] [A Single Set of Adversarial Clothes Breaks Multiple Defense Methods in the Physical World](https://arxiv.org/abs/2510.17322)
*Wei Zhang,Zhanhao Hu,Xiao Li,Xiaopei Zhu,Xiaolin Hu*

Main category: cs.CV

TL;DR: 本文评估了现有对抗性防御方法在面对大尺寸对抗衣物攻击时的表现，发现这些防御方法在数字和物理世界中均表现不佳，揭示了现有防御方法在应对大尺寸自然对抗攻击时的共同脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究发现，简单增大对抗补丁的尺寸就能使现有防御方法失效，因此需要评估这些防御方法在面对覆盖人体大部分区域的对抗衣物时的有效性。对抗衣物不仅尺寸大，而且比大补丁在人体上看起来更自然，为评估对抗性防御提供了良好的测试案例。

Method: 通过实验评估多种对抗性防御方法对抗对抗衣物的效果，包括在数字世界和物理世界中的测试。特别地，针对Faster R-CNN检测器，制作了一组能够突破多种防御方法的对抗衣物。

Result: 所有防御方法在对抗衣物攻击下表现均不佳。在物理世界中，针对Faster R-CNN的一组对抗衣物对未防御检测器的攻击成功率为96.06%，对九个防御模型的攻击成功率均超过64.84%。

Conclusion: 现有对抗性防御方法在面对大尺寸、自然的对抗衣物攻击时存在共同的脆弱性，表明当前防御技术在实际应用中仍面临严峻挑战。

Abstract: In recent years, adversarial attacks against deep learning-based object
detectors in the physical world have attracted much attention. To defend
against these attacks, researchers have proposed various defense methods
against adversarial patches, a typical form of physically-realizable attack.
However, our experiments showed that simply enlarging the patch size could make
these defense methods fail. Motivated by this, we evaluated various defense
methods against adversarial clothes which have large coverage over the human
body. Adversarial clothes provide a good test case for adversarial defense
against patch-based attacks because they not only have large sizes but also
look more natural than a large patch on humans. Experiments show that all the
defense methods had poor performance against adversarial clothes in both the
digital world and the physical world. In addition, we crafted a single set of
clothes that broke multiple defense methods on Faster R-CNN. The set achieved
an Attack Success Rate (ASR) of 96.06% against the undefended detector and over
64.84% ASRs against nine defended models in the physical world, unveiling the
common vulnerability of existing adversarial defense methods against
adversarial clothes. Code is available at:
https://github.com/weiz0823/adv-clothes-break-multiple-defenses.

</details>


### [135] [CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration](https://arxiv.org/abs/2510.17330)
*Gyuhwan Park,Kihyun Na,Injung Kim*

Main category: cs.CV

TL;DR: CharDiff是一个基于扩散模型的框架，通过字符级引导有效恢复和识别严重退化的车牌图像。


<details>
  <summary>Details</summary>
Motivation: 车牌图像恢复不仅对LPR系统预处理很重要，还能提高证据价值、增强视觉界面清晰度，促进车牌图像的进一步利用。

Method: 利用外部分割和OCR模块提取细粒度字符级先验，引入CHARM模块确保每个字符的引导仅限于其自身区域，避免区域间干扰。

Result: 在Roboflow-LP数据集上，CharDiff显著优于基线恢复模型，CER相对降低了28%。

Conclusion: 结构化字符引导条件化有效增强了基于扩散的车牌恢复和识别在实际部署场景中的鲁棒性。

Abstract: The significance of license plate image restoration goes beyond the
preprocessing stage of License Plate Recognition (LPR) systems, as it also
serves various purposes, including increasing evidential value, enhancing the
clarity of visual interface, and facilitating further utilization of license
plate images. We propose a novel diffusion-based framework with character-level
guidance, CharDiff, which effectively restores and recognizes severely degraded
license plate images captured under realistic conditions. CharDiff leverages
fine-grained character-level priors extracted through external segmentation and
Optical Character Recognition (OCR) modules tailored for low-quality license
plate images. For precise and focused guidance, CharDiff incorporates a novel
Character-guided Attention through Region-wise Masking (CHARM) module, which
ensures that each character's guidance is restricted to its own region, thereby
avoiding interference with other regions. In experiments, CharDiff
significantly outperformed the baseline restoration models in both restoration
quality and recognition accuracy, achieving a 28% relative reduction in CER on
the Roboflow-LP dataset, compared to the best-performing baseline model. These
results indicate that the structured character-guided conditioning effectively
enhances the robustness of diffusion-based license plate restoration and
recognition in practical deployment scenarios.

</details>


### [136] [iDETEX: Empowering MLLMs for Intelligent DETailed EXplainable IQA](https://arxiv.org/abs/2510.17332)
*Zhaoran Zhao,Xinli Yue,Jianhui Sun,Yuhao Xie,Tao Shao,Liangchao Yao,Fan Xia,Yuetang Deng*

Main category: cs.CV

TL;DR: iDETEX是一个统一的多模态大语言模型，能够同时执行质量定位、感知和描述三个关键任务，在ViDA-UGC基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决图像质量评估从标量质量预测向更可解释、人类对齐评估范式发展的挑战，实现详细和可解释的IQA。

Method: 设计任务特定的离线增强模块和数据混合策略，配合在线增强策略充分利用多源监督，构建统一的MLLM模型。

Result: 在ViDA-UGC基准测试中达到最先进性能，在ICCV MIPI 2025详细图像质量评估挑战赛中排名第一。

Conclusion: iDETEX展示了在提供准确和可解释质量评估方面的有效性和鲁棒性。

Abstract: Image Quality Assessment (IQA) has progressed from scalar quality prediction
to more interpretable, human-aligned evaluation paradigms. In this work, we
address the emerging challenge of detailed and explainable IQA by proposing
iDETEX-a unified multimodal large language model (MLLM) capable of
simultaneously performing three key tasks: quality grounding, perception, and
description. To facilitate efficient and generalizable training across these
heterogeneous subtasks, we design a suite of task-specific offline augmentation
modules and a data mixing strategy. These are further complemented by online
enhancement strategies to fully exploit multi-sourced supervision. We validate
our approach on the large-scale ViDA-UGC benchmark, where iDETEX achieves
state-of-the-art performance across all subtasks. Our model ranks first in the
ICCV MIPI 2025 Detailed Image Quality Assessment Challenge, demonstrating its
effectiveness and robustness in delivering accurate and interpretable quality
assessments.

</details>


### [137] [Nearest-Class Mean and Logits Agreement for Wildlife Open-Set Recognition](https://arxiv.org/abs/2510.17338)
*Jiahao Huo,Mufhumudzi Muthivhi,Terence L. van Zyl,Fredrik Gustafsson*

Main category: cs.CV

TL;DR: 提出一种后处理的开放集识别方法，通过比较特征空间和logit空间的概率分布来识别未知类别，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 当前野生动物分类模型在封闭世界设定下训练，面对未知类别时会产生过度自信的预测。现有OSR方法需要重新训练模型，存在效率问题。

Method: 基于输入到最近类均值(NCM)的距离构建概率分布，然后与softmax概率进行比较，衡量NCM和分类头之间的一致性。

Result: 在两个数据集上排名前三，AUROC分别达到93.41和95.35，表现优于现有方法。

Conclusion: 该方法在开放集识别任务中表现出色且稳定，无需重新训练模型，具有良好的实用性。

Abstract: Current state-of-the-art Wildlife classification models are trained under the
closed world setting. When exposed to unknown classes, they remain
overconfident in their predictions. Open-set Recognition (OSR) aims to classify
known classes while rejecting unknown samples. Several OSR methods have been
proposed to model the closed-set distribution by observing the feature, logit,
or softmax probability space. A significant drawback of many existing
approaches is the requirement to retrain the pre-trained classification model
with the OSR-specific strategy. This study contributes a post-processing OSR
method that measures the agreement between the models' features and predicted
logits. We propose a probability distribution based on an input's distance to
its Nearest Class Mean (NCM). The NCM-based distribution is then compared with
the softmax probabilities from the logit space to measure agreement between the
NCM and the classification head. Our proposed strategy ranks within the top
three on two evaluated datasets, showing consistent performance across the two
datasets. In contrast, current state-of-the-art methods excel on a single
dataset. We achieve an AUROC of 93.41 and 95.35 for African and Swedish
animals. The code can be found
https://github.com/Applied-Representation-Learning-Lab/OSR.

</details>


### [138] [Exploring The Missing Semantics In Event Modality](https://arxiv.org/abs/2510.17347)
*Jingqian Wu,Shengpeng Xu,Yunbo Jia,Edmund Y. Lam*

Main category: cs.CV

TL;DR: 提出了Semantic-E2VID框架，通过跨模态特征对齐和语义感知特征融合，将视觉语义知识从SAM模型迁移到事件相机，显著提升了事件到视频重建的质量。


<details>
  <summary>Details</summary>
Motivation: 事件相机仅捕获强度变化，忽略了静态物体和背景，导致捕获的事件模态缺乏语义信息。现有E2V方法往往忽视语义信息在视频重建中的重要作用。

Method: 引入跨模态特征对齐模块将SAM的视觉语义迁移到事件编码器；提出语义感知特征融合块整合学习到的语义；设计语义感知E2V监督机制利用SAM生成的类别标签。

Result: 在多个基准测试中显著提升帧质量，优于现有最先进的E2V方法。

Conclusion: Semantic-E2VID成功解决了事件模态中语义信息缺失的问题，为事件到视频重建提供了有效的语义增强解决方案。

Abstract: Event cameras offer distinct advantages such as low latency, high dynamic
range, and efficient motion capture. However, event-to-video reconstruction
(E2V), a fundamental event-based vision task, remains challenging, particularly
for reconstructing and recovering semantic information. This is primarily due
to the nature of the event camera, as it only captures intensity changes,
ignoring static objects and backgrounds, resulting in a lack of semantic
information in captured event modality. Further, semantic information plays a
crucial role in video and frame reconstruction, yet is often overlooked by
existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V
framework that explores the missing visual semantic knowledge in event modality
and leverages it to enhance event-to-video reconstruction. Specifically,
Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to
transfer the robust visual semantics from a frame-based vision foundation
model, the Segment Anything Model (SAM), to the event encoder, while aligning
the high-level features from distinct modalities. To better utilize the learned
semantic feature, we further propose a semantic-aware feature fusion (SFF)
block to integrate learned semantics in frame modality to form event
representations with rich semantics that can be decoded by the event decoder.
Further, to facilitate the reconstruction of semantic information, we propose a
novel Semantic Perceptual E2V Supervision that helps the model to reconstruct
semantic details by leveraging SAM-generated categorical labels. Extensive
experiments demonstrate that Semantic-E2VID significantly enhances frame
quality, outperforming state-of-the-art E2V methods across multiple benchmarks.
The sample code is included in the supplementary material.

</details>


### [139] [M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception](https://arxiv.org/abs/2510.17363)
*U. V. B. L Udugama,George Vosselman,Francesco Nex*

Main category: cs.CV

TL;DR: M2H是一个新颖的多任务学习框架，用于从单目图像进行语义分割、深度、边缘和表面法线估计，通过窗口化跨任务注意力模块实现高效的特征交换，在保持计算效率的同时提升多任务性能。


<details>
  <summary>Details</summary>
Motivation: 在边缘设备上部署实时空间感知需要高效的多任务模型，能够利用互补任务信息同时最小化计算开销。传统方法要么使用独立的单任务模型，要么采用共享编码器-解码器架构，存在性能或效率限制。

Method: 提出了Multi-Mono-Hydra (M2H)框架，基于轻量级ViT-based DINOv2骨干网络，引入窗口化跨任务注意力模块，实现结构化特征交换同时保留任务特定细节。

Result: 在NYUDv2数据集上超越最先进的多任务模型，在Hypersim上超过单任务深度和语义基线，在Cityscapes数据集上表现优异，且在笔记本电脑硬件上保持计算效率。

Conclusion: M2H作为单目空间感知系统的基础，支持动态环境中的3D场景图构建，在真实世界数据上验证了其在实际空间感知任务中的实用性。

Abstract: Deploying real-time spatial perception on edge devices requires efficient
multi-task models that leverage complementary task information while minimizing
computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel
multi-task learning framework designed for semantic segmentation and depth,
edge, and surface normal estimation from a single monocular image. Unlike
conventional approaches that rely on independent single-task models or shared
encoder-decoder architectures, M2H introduces a Window-Based Cross-Task
Attention Module that enables structured feature exchange while preserving
task-specific details, improving prediction consistency across tasks. Built on
a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time
deployment and serves as the foundation for monocular spatial perception
systems supporting 3D scene graph construction in dynamic environments.
Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task
models on NYUDv2, surpasses single-task depth and semantic baselines on
Hypersim, and achieves superior performance on the Cityscapes dataset, all
while maintaining computational efficiency on laptop hardware. Beyond
benchmarks, M2H is validated on real-world data, demonstrating its practicality
in spatial perception tasks.

</details>


### [140] [Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs](https://arxiv.org/abs/2510.17364)
*Vaggelis Dorovatas,Soroush Seifi,Gunshi Gupta,Rahaf Aljundi*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，通过LLM引导的视觉令牌选择、递归处理和基于字幕的问答，使Video-LLMs能够在流式视频场景中高效处理长视频并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决Video-LLMs在流式视频处理中的挑战，即需要实时处理小时级视频并及时响应查询，而传统方法需要完整视频访问权限。

Method: 1) 基于LLM注意力选择重要视觉令牌（可丢弃95%不重要的令牌）；2) 递归处理历史选定令牌以保持时序连贯性；3) 使用基于字幕的轻量级问答系统。

Result: 在流式视频基准测试中达到最先进性能，在效率和效果之间取得良好平衡，性能损失最小。

Conclusion: 该方法为Video-LLMs在流式视频应用中的实际部署提供了可行的解决方案，无需额外训练即可显著提升处理效率。

Abstract: Video Large Language Models (Video-LLMs) excel at understanding videos
in-context, provided they have full access to the video when answering queries.
However, these models face challenges in streaming scenarios where hour-long
videos must be processed online, and questions need timely responses. In this
work, we propose a training-free approach compatible with standard Video-LLMs,
leveraging three key concepts: 1) LLM-informed selection of visual tokens to
identify those that the LLM has attended to and contributed to its
understanding of each short clip. Our attention-based selection allows us to
discard up to ~95% of unimportant visual tokens with minimal performance loss;
2) Recurrent processing of past selected tokens to generate temporally coherent
understanding of each processed clip; 3) Caption-based question answering for
lightweight and accurate responses. Our method achieves state-of-the-art
performance on streaming video benchmarks, striking a balance between
efficiency and effectiveness.

</details>


### [141] [Beyond Real Faces: Synthetic Datasets Can Achieve Reliable Recognition Performance without Privacy Compromise](https://arxiv.org/abs/2510.17372)
*Paweł Borsukiewicz,Fadi Boutros,Iyiola E. Olatunji,Charles Beumier,Wendkûuni C. Ouedraogo,Jacques Klein,Tegawendé F. Bissyandé*

Main category: cs.CV

TL;DR: 合成面部数据可以替代真实数据集，在保护隐私的同时达到甚至超越真实数据的识别准确率（最高95.67%），并提供了前所未有的偏差控制能力。


<details>
  <summary>Details</summary>
Motivation: 面部识别系统需要大量真实面部数据，但未经同意收集数据存在伦理和法律风险（如GDPR）。合成数据作为隐私保护替代方案缺乏实证研究。

Method: 通过系统文献回顾识别25个合成面部数据集（2018-2025），结合实验验证，评估隐私保护合成数据的7个关键要求：身份泄露预防、类内变异性、身份可分离性、数据集规模、伦理数据来源、偏差缓解和基准可靠性。

Result: 最佳合成数据集（VariFace, VIGFace）识别准确率分别达95.67%和94.91%，超过CASIA-WebFace（94.70%）。合成数据确保适当的类内变异性同时保持身份可分离性，虽然继承有限偏差，但通过生成参数提供了前所未有的偏差控制能力。

Conclusion: 合成面部数据是面部识别研究中科学可行且伦理上必要的替代方案。

Abstract: The deployment of facial recognition systems has created an ethical dilemma:
achieving high accuracy requires massive datasets of real faces collected
without consent, leading to dataset retractions and potential legal liabilities
under regulations like GDPR. While synthetic facial data presents a promising
privacy-preserving alternative, the field lacks comprehensive empirical
evidence of its viability. This study addresses this critical gap through
extensive evaluation of synthetic facial recognition datasets. We present a
systematic literature review identifying 25 synthetic facial recognition
datasets (2018-2025), combined with rigorous experimental validation. Our
methodology examines seven key requirements for privacy-preserving synthetic
data: identity leakage prevention, intra-class variability, identity
separability, dataset scale, ethical data sourcing, bias mitigation, and
benchmark reliability. Through experiments involving over 10 million synthetic
samples, extended by a comparison of results reported on five standard
benchmarks, we provide the first comprehensive empirical assessment of
synthetic data's capability to replace real datasets. Best-performing synthetic
datasets (VariFace, VIGFace) achieve recognition accuracies of 95.67% and
94.91% respectively, surpassing established real datasets including
CASIA-WebFace (94.70%). While those images remain private, publicly available
alternatives Vec2Face (93.52%) and CemiFace (93.22%) come close behind. Our
findings reveal that they ensure proper intra-class variability while
maintaining identity separability. Demographic bias analysis shows that, even
though synthetic data inherits limited biases, it offers unprecedented control
for bias mitigation through generation parameters. These results establish
synthetic facial data as a scientifically viable and ethically imperative
alternative for facial recognition research.

</details>


### [142] [Facial Expression-based Parkinson's Disease Severity Diagnosis via Feature Fusion and Adaptive Class Balancing](https://arxiv.org/abs/2510.17373)
*Yintao Zhou,Wei Huang,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于多表情特征融合和自适应类别平衡策略的帕金森病严重程度诊断方法，解决了现有方法依赖单一表情、忽略类别不平衡和只能进行二分类的问题。


<details>
  <summary>Details</summary>
Motivation: 当前基于面部表情的PD诊断方法存在三个主要问题：依赖单一表情可能导致误诊、忽略不同PD阶段的类别不平衡问题、大多数方法只能进行二分类而无法诊断严重程度。

Method: 通过注意力机制融合多种面部表情特征，并采用自适应类别平衡策略动态调整训练样本的贡献度，基于类别分布和分类难度来缓解类别不平衡问题。

Result: 实验结果表明该方法在PD严重程度诊断方面具有良好性能，注意力特征融合和自适应类别平衡策略都显示出有效性。

Conclusion: 该方法为PD严重程度诊断提供了一种有效的解决方案，通过多表情特征融合和类别平衡策略提高了诊断准确性和实用性。

Abstract: Parkinson's disease (PD) severity diagnosis is crucial for early detecting
potential patients and adopting tailored interventions. Diagnosing PD based on
facial expression is grounded in PD patients' "masked face" symptom and gains
growing interest recently for its convenience and affordability. However,
current facial expression-based approaches often rely on single type of
expression which can lead to misdiagnosis, and ignore the class imbalance
across different PD stages which degrades the prediction performance. Moreover,
most existing methods focus on binary classification (i.e., PD / non-PD) rather
than diagnosing the severity of PD. To address these issues, we propose a new
facial expression-based method for PD severity diagnosis which integrates
multiple facial expression features through attention-based feature fusion.
Moreover, we mitigate the class imbalance problem via an adaptive class
balancing strategy which dynamically adjusts the contribution of training
samples based on their class distribution and classification difficulty.
Experimental results demonstrate the promising performance of the proposed
method for PD severity diagnosis, as well as the efficacy of attention-based
feature fusion and adaptive class balancing.

</details>


### [143] [Closed-Loop Transfer for Weakly-supervised Affordance Grounding](https://arxiv.org/abs/2510.17384)
*Jiajin Tang,Zhengxuan Wei,Ge Zheng,Sibei Yang*

Main category: cs.CV

TL;DR: LoopTrans是一个闭环框架，通过双向知识转移（从外中心到自我中心，再返回）来增强交互场景中的功能感知能力，解决了传统单向转移方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 人类能够通过观察他人与物体的互动来学习新物体的功能。现有弱监督功能感知方法仅从外中心图像单向转移到自我中心图像，限制了在复杂交互场景中的适用性。

Method: 提出LoopTrans闭环框架，包含统一的跨模态定位和去噪知识蒸馏机制，实现外中心与自我中心图像之间的双向知识转移。

Result: 实验表明LoopTrans在所有图像和视频基准测试指标上均取得一致提升，即使在人体完全遮挡物体交互区域的挑战性场景下也能有效处理。

Conclusion: LoopTrans通过闭环双向知识转移机制成功克服了传统方法的局限性，显著提升了功能感知在复杂交互场景中的性能。

Abstract: Humans can perform previously unexperienced interactions with novel objects
simply by observing others engage with them. Weakly-supervised affordance
grounding mimics this process by learning to locate object regions that enable
actions on egocentric images, using exocentric interaction images with
image-level annotations. However, extracting affordance knowledge solely from
exocentric images and transferring it one-way to egocentric images limits the
applicability of previous works in complex interaction scenarios. Instead, this
study introduces LoopTrans, a novel closed-loop framework that not only
transfers knowledge from exocentric to egocentric but also transfers back to
enhance exocentric knowledge extraction. Within LoopTrans, several innovative
mechanisms are introduced, including unified cross-modal localization and
denoising knowledge distillation, to bridge domain gaps between object-centered
egocentric and interaction-centered exocentric images while enhancing knowledge
transfer. Experiments show that LoopTrans achieves consistent improvements
across all metrics on image and video benchmarks, even handling challenging
scenarios where object interaction regions are fully occluded by the human
body.

</details>


### [144] [Monitoring Horses in Stalls: From Object to Event Detection](https://arxiv.org/abs/2510.17409)
*Dmitrii Galimzianov,Viacheslav Vyshegorodtsev,Ivan Nezhivykh*

Main category: cs.CV

TL;DR: 提出基于视觉的自动化监控系统，使用YOLOv11和BoT-SORT技术检测和追踪马厩内的马匹和人员，通过物体轨迹和空间关系推断事件状态，实现马匹行为实时监测。


<details>
  <summary>Details</summary>
Motivation: 传统马匹行为监测方法劳动密集且耗时，需要自动化解决方案来早期发现健康福利问题。

Method: 使用YOLOv11进行目标检测，BoT-SORT进行多目标追踪，结合CLIP和GroundingDINO构建标注数据集，基于物体轨迹和空间关系推断五种事件类型。

Result: 定性评估显示系统在马相关事件检测上表现可靠，但人员检测因数据不足存在局限性，系统能处理摄像头盲区问题。

Conclusion: 该研究为马场实时行为监测奠定了基础，对动物福利和厩舍管理具有重要意义，未来需改进人员检测的数据不足问题。

Abstract: Monitoring the behavior of stalled horses is essential for early detection of
health and welfare issues but remains labor-intensive and time-consuming. In
this study, we present a prototype vision-based monitoring system that
automates the detection and tracking of horses and people inside stables using
object detection and multi-object tracking techniques. The system leverages
YOLOv11 and BoT-SORT for detection and tracking, while event states are
inferred based on object trajectories and spatial relations within the stall.
To support development, we constructed a custom dataset annotated with
assistance from foundation models CLIP and GroundingDINO. The system
distinguishes between five event types and accounts for the camera's blind
spots. Qualitative evaluation demonstrated reliable performance for
horse-related events, while highlighting limitations in detecting people due to
data scarcity. This work provides a foundation for real-time behavioral
monitoring in equine facilities, with implications for animal welfare and
stable management.

</details>


### [145] [DeepDetect: Learning All-in-One Dense Keypoints](https://arxiv.org/abs/2510.17422)
*Shaharyar Ahmed Khan Tareen,Filza Khan Tareen*

Main category: cs.CV

TL;DR: DeepDetect是一个智能、一体化的密集关键点检测器，通过融合传统检测器输出生成真值掩码，并训练轻量级ESPNet模型，在关键点密度、可重复性和正确匹配数方面超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统和基于学习的关键点检测方法存在对光度变化敏感、关键点密度和可重复性低、对挑战性场景适应能力有限、缺乏语义理解等问题，无法优先处理视觉重要区域。

Method: 首先融合7个关键点检测器和2个边缘检测器的输出创建真值掩码，提取多样视觉线索；然后使用这些掩码作为标签训练轻量高效的ESPNet模型，使DeepDetect能够语义聚焦图像并产生高密度关键点。

Result: 在Oxford Affine Covariant Regions数据集上的评估显示，DeepDetect在关键点密度（0.5143）、可重复性（0.9582）和正确匹配数（59,003）方面均达到最大值，超越其他检测器。

Conclusion: DeepDetect成功统一了传统检测器的优势，通过深度学习实现了对多样化视觉退化条件的高度适应性，在关键点检测性能上取得了显著提升。

Abstract: Keypoint detection is the foundation of many computer vision tasks, including
image registration, structure-from motion, 3D reconstruction, visual odometry,
and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning
based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong
performance yet suffer from key limitations: sensitivity to photometric
changes, low keypoint density and repeatability, limited adaptability to
challenging scenes, and lack of semantic understanding, often failing to
prioritize visually important regions. We present DeepDetect, an intelligent,
all-in-one, dense keypoint detector that unifies the strengths of classical
detectors using deep learning. Firstly, we create ground-truth masks by fusing
outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from
corners and blobs to prominent edges and textures in the images. Afterwards, a
lightweight and efficient model: ESPNet, is trained using these masks as
labels, enabling DeepDetect to focus semantically on images while producing
highly dense keypoints, that are adaptable to diverse and visually degraded
conditions. Evaluations on the Oxford Affine Covariant Regions dataset
demonstrate that DeepDetect surpasses other detectors in keypoint density,
repeatability, and the number of correct matches, achieving maximum values of
0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003
(correct matches).

</details>


### [146] [Leveraging AV1 motion vectors for Fast and Dense Feature Matching](https://arxiv.org/abs/2510.17434)
*Julien Zouein,Hossein Javidnia,François Pitié,Anil Kokaram*

Main category: cs.CV

TL;DR: 利用AV1运动向量生成密集亚像素对应点和短轨迹，通过余弦一致性过滤，在短视频上性能与SIFT相当但CPU使用率更低，匹配密度更高且几何精度有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索压缩域对应点作为资源高效的前端处理方案，为完整流程的可扩展性提供清晰路径。

Method: 重新利用AV1运动向量生成密集亚像素对应点和短轨迹，使用余弦一致性进行过滤。

Result: 在117帧视频片段上，运动向量匹配成功注册所有图像并重建46-62万个点，重投影误差为0.51-0.53像素；束调整时间随匹配密度增加而增长。

Conclusion: 压缩域对应点是一个实用且资源高效的前端处理方案，具有在完整流程中扩展的明确路径。

Abstract: We repurpose AV1 motion vectors to produce dense sub-pixel correspondences
and short tracks filtered by cosine consistency. On short videos, this
compressed-domain front end runs comparably to sequential SIFT while using far
less CPU, and yields denser matches with competitive pairwise geometry. As a
small SfM demo on a 117-frame clip, MV matches register all images and
reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows
with match density. These results show compressed-domain correspondences are a
practical, resource-efficient front end with clear paths to scaling in full
pipelines.

</details>


### [147] [Rethinking Nighttime Image Deraining via Learnable Color Space Transformation](https://arxiv.org/abs/2510.17440)
*Qiyuan Guan,Xiang Chen,Guiyue Jin,Jiyu Jin,Shumin Fan,Tianyu Song,Jinshan Pan*

Main category: cs.CV

TL;DR: 提出了一种新的高质量夜间图像去雨基准数据集HQ-NightRain，并开发了颜色空间转换网络CST-Net，通过可学习的颜色空间转换器和隐式光照引导来提升夜间去雨效果。


<details>
  <summary>Details</summary>
Motivation: 夜间图像去雨面临比白天更大的挑战，主要由于夜间场景的固有复杂性和缺乏高质量的数据集来准确表示雨和光照之间的耦合效应。

Method: 开发了CST-Net网络，包含可学习的颜色空间转换器(CSC)用于在Y通道更好地去除雨迹，以及隐式光照引导机制来捕获光照信息指导去雨过程。

Result: 广泛的实验证明了所提出数据集的价值和方法的有效性，HQ-NightRain数据集相比现有数据集具有更高的协调性和真实性。

Conclusion: 该方法为夜间图像去雨任务提供了新的高质量基准和有效的解决方案，源代码和数据集已公开。

Abstract: Compared to daytime image deraining, nighttime image deraining poses
significant challenges due to inherent complexities of nighttime scenarios and
the lack of high-quality datasets that accurately represent the coupling effect
between rain and illumination. In this paper, we rethink the task of nighttime
image deraining and contribute a new high-quality benchmark, HQ-NightRain,
which offers higher harmony and realism compared to existing datasets. In
addition, we develop an effective Color Space Transformation Network (CST-Net)
for better removing complex rain from nighttime scenes. Specifically, we
propose a learnable color space converter (CSC) to better facilitate rain
removal in the Y channel, as nighttime rain is more pronounced in the Y channel
compared to the RGB color space. To capture illumination information for
guiding nighttime deraining, implicit illumination guidance is introduced
enabling the learned features to improve the model's robustness in complex
scenarios. Extensive experiments show the value of our dataset and the
effectiveness of our method. The source code and datasets are available at
https://github.com/guanqiyuan/CST-Net.

</details>


### [148] [Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS](https://arxiv.org/abs/2510.17479)
*Feng Zhou,Wenkai Guo,Pu Cao,Zhicheng Zhang,Jianqin Yin*

Main category: cs.CV

TL;DR: 该论文提出了一种针对稀疏视图3D高斯泼溅(3DGS)的改进初始化方法，通过增强SfM点云覆盖和3DGS自初始化来解决过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图3DGS容易过拟合训练视图，导致新视角渲染出现模糊等伪影。研究发现初始化是决定性因素，而训练时约束只能带来有限的改进。

Method: 设计了三个组件：(1)频率感知SfM通过低频视图增强改善低纹理区域覆盖；(2)3DGS自初始化将光度监督转换为额外点云；(3)点云正则化通过几何/可见性先验确保多视图一致性和均匀空间覆盖。

Result: 在LLFF和Mip-NeRF360数据集上的实验表明，该方法在稀疏视图设置下取得了持续的性能提升。

Conclusion: 该方法作为一种更强的初始化策略，有效解决了稀疏视图3DGS的过拟合问题，代码已开源。

Abstract: Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training
views, leading to artifacts like blurring in novel view rendering. Prior work
addresses it either by enhancing the initialization (\emph{i.e.}, the point
cloud from Structure-from-Motion (SfM)) or by adding training-time constraints
(regularization) to the 3DGS optimization. Yet our controlled ablations reveal
that initialization is the decisive factor: it determines the attainable
performance band in sparse-view 3DGS, while training-time constraints yield
only modest within-band improvements at extra cost. Given initialization's
primacy, we focus our design there. Although SfM performs poorly under sparse
views due to its reliance on feature matching, it still provides reliable seed
points. Thus, building on SfM, our effort aims to supplement the regions it
fails to cover as comprehensively as possible. Specifically, we design: (i)
frequency-aware SfM that improves low-texture coverage via low-frequency view
augmentation and relaxed multi-view correspondences; (ii) 3DGS
self-initialization that lifts photometric supervision into additional points,
compensating SfM-sparse regions with learned Gaussian centers; and (iii)
point-cloud regularization that enforces multi-view consistency and uniform
spatial coverage through simple geometric/visibility priors, yielding a clean
and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate
consistent gains in sparse-view settings, establishing our approach as a
stronger initialization strategy. Code is available at
https://github.com/zss171999645/ItG-GS.

</details>


### [149] [SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries](https://arxiv.org/abs/2510.17482)
*Chenxu Dang,Haiyan Liu,Guangjun Bao,Pei An,Xinyue Tang,Jie Ma,Bingchuan Sun,Yan Wang*

Main category: cs.CV

TL;DR: SparseWorld是一种新型的4D占用世界模型，通过稀疏动态查询实现灵活、自适应和高效的语义占用感知与预测。


<details>
  <summary>Details</summary>
Motivation: 现有占用世界模型依赖静态固定嵌入或网格，限制了感知灵活性，且基于网格的"原地分类"与真实场景的动态连续性存在潜在不匹配。

Method: 提出Range-Adaptive Perception模块（可学习查询通过ego车辆状态调制并增强时空关联）和State-Conditioned Forecasting模块（用回归引导的预测替代分类预测），以及Temporal-Aware Self-Scheduling训练策略。

Result: 在感知、预测和规划任务上达到最先进性能，可视化实验验证了模型在灵活性、适应性和效率方面的优势。

Conclusion: SparseWorld通过稀疏动态查询有效解决了现有占用模型的局限性，实现了更好的4D环境建模。

Abstract: Semantic occupancy has emerged as a powerful representation in world models
for its ability to capture rich spatial semantics. However, most existing
occupancy world models rely on static and fixed embeddings or grids, which
inherently limit the flexibility of perception. Moreover, their ``in-place
classification" over grids exhibits a potential misalignment with the dynamic
and continuous nature of real scenarios.In this paper, we propose SparseWorld,
a novel 4D occupancy world model that is flexible, adaptive, and efficient,
powered by sparse and dynamic queries. We propose a Range-Adaptive Perception
module, in which learnable queries are modulated by the ego vehicle states and
enriched with temporal-spatial associations to enable extended-range
perception. To effectively capture the dynamics of the scene, we design a
State-Conditioned Forecasting module, which replaces classification-based
forecasting with regression-guided formulation, precisely aligning the dynamic
queries with the continuity of the 4D environment. In addition, We specifically
devise a Temporal-Aware Self-Scheduling training strategy to enable smooth and
efficient training. Extensive experiments demonstrate that SparseWorld achieves
state-of-the-art performance across perception, forecasting, and planning
tasks. Comprehensive visualizations and ablation studies further validate the
advantages of SparseWorld in terms of flexibility, adaptability, and
efficiency. The code is available at https://github.com/MSunDYY/SparseWorld.

</details>


### [150] [Split-Fuse-Transport: Annotation-Free Saliency via Dual Clustering and Optimal Transport Alignment](https://arxiv.org/abs/2510.17484)
*Muhammad Umer Ramzan,Ali Zia,Abdelwahed Khamis,Noman Ali,Usman Ali,Wei Xiang*

Main category: cs.CV

TL;DR: POTNet通过引入熵引导的双聚类头和最优传输对齐，在无监督显著目标检测中实现了接近监督方法的性能，无需像素级标签。


<details>
  <summary>Details</summary>
Motivation: 现有无监督SOD方法依赖不可靠的伪掩码，且原型质量差导致最优传输的全局一致性利用不足。作者发现边界像素和内部像素具有不同几何特性，需要针对性处理。

Method: 提出POTNet，采用熵引导的双聚类头：高熵像素用谱聚类，低熵像素用k-means，然后通过最优传输对齐两个原型集。这种分割-融合-传输设计能生成更清晰的伪掩码。

Result: 在五个基准测试上，AutoSOD（基于POTNet的端到端无监督SOD管道）比无监督方法F-measure提升26%，比弱监督方法提升36%，显著缩小了与全监督模型的差距。

Conclusion: 通过改进原型生成和最优传输对齐，无监督SOD可以达到接近监督方法的精度，证明了无需像素级标签也能获得高质量分割结果的可行性。

Abstract: Salient object detection (SOD) aims to segment visually prominent regions in
images and serves as a foundational task for various computer vision
applications. We posit that SOD can now reach near-supervised accuracy without
a single pixel-level label, but only when reliable pseudo-masks are available.
We revisit the prototype-based line of work and make two key observations.
First, boundary pixels and interior pixels obey markedly different geometry;
second, the global consistency enforced by optimal transport (OT) is
underutilized if prototype quality is weak. To address this, we introduce
POTNet, an adaptation of Prototypical Optimal Transport that replaces POT's
single k-means step with an entropy-guided dual-clustering head: high-entropy
pixels are organized by spectral clustering, low-entropy pixels by k-means, and
the two prototype sets are subsequently aligned by OT. This
split-fuse-transport design yields sharper, part-aware pseudo-masks in a single
forward pass, without handcrafted priors. Those masks supervise a standard
MaskFormer-style encoder-decoder, giving rise to AutoSOD, an end-to-end
unsupervised SOD pipeline that eliminates SelfMask's offline voting yet
improves both accuracy and training efficiency. Extensive experiments on five
benchmarks show that AutoSOD outperforms unsupervised methods by up to 26% and
weakly supervised methods by up to 36% in F-measure, further narrowing the gap
to fully supervised models.

</details>


### [151] [Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization](https://arxiv.org/abs/2510.17501)
*Yuanli Wu,Long Zhang,Yue Du,Bin Li*

Main category: cs.CV

TL;DR: 提出了一种基于评分标准和伪标签的零样本视频摘要方法，通过将少量真实标注转化为伪标签来指导LLM进行可解释的场景评估，在保持零样本优势的同时提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督方法标注成本高且跨数据集泛化能力有限，无监督方法难以捕捉高层次语义，零样本方法对提示模板敏感且依赖数据集特定的分数归一化。

Method: 使用评分标准引导的伪标签提示框架，将少量真实标注转化为高置信度伪标签，构建结构化评分标准。推理时对首尾片段基于描述评分，中间片段结合相邻场景的上下文摘要来评估叙事进展和冗余。

Result: 在SumMe和TVSum数据集上分别达到57.58和63.05的F1分数，超越无监督和先前零样本基线，接近监督方法性能。

Conclusion: 评分标准引导的伪标签有效稳定了基于LLM的评分，为视频摘要建立了一个通用、可解释的零样本范式。

Abstract: With the rapid proliferation of video content across social media,
surveillance, and education platforms, efficiently summarizing long videos into
concise yet semantically faithful surrogates has become increasingly vital.
Existing supervised methods achieve strong in-domain accuracy by learning from
dense annotations but suffer from high labeling costs and limited cross-dataset
generalization, while unsupervised approaches, though label-free, often fail to
capture high-level human semantics and fine-grained narrative cues. More
recently, zero-shot prompting pipelines have leveraged large language models
(LLMs) for training-free video summarization, yet remain highly sensitive to
handcrafted prompt templates and dataset-specific score normalization. To
overcome these limitations, we introduce a rubric-guided, pseudo-labeled
prompting framework that transforms a small subset of ground-truth annotations
into high-confidence pseudo labels, which are aggregated into structured,
dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During
inference, first and last segments are scored based solely on their
descriptions, whereas intermediate ones incorporate brief contextual summaries
of adjacent scenes to assess narrative progression and redundancy. This
contextual prompting enables the LLM to balance local salience and global
coherence without parameter tuning. On SumMe and TVSum, our method achieves F1
scores of \textbf{57.58} and \textbf{63.05}, surpassing unsupervised and prior
zero-shot baselines while approaching supervised performance. The results
demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based
scoring and establishes a general, interpretable zero-shot paradigm for video
summarization.

</details>


### [152] [MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models](https://arxiv.org/abs/2510.17519)
*Yongshun Zhang,Zhongyi Fan,Yonghang Zhang,Zhangzikang Li,Weifeng Chen,Zhongwei Feng,Chaoyue Wang,Peng Hou,Anxiang Zeng*

Main category: cs.CV

TL;DR: 提出一个针对大规模视频生成模型的训练框架，通过优化数据预处理、模型架构、训练策略和基础设施四个支柱，显著提升了训练效率和性能，并开源了完整的模型权重和训练代码。


<details>
  <summary>Details</summary>
Motivation: 大规模视频生成模型训练面临跨模态文本-视频对齐、长序列处理和复杂时空依赖等挑战，导致训练资源密集且困难。

Method: 优化四个支柱：数据预处理、模型架构、训练策略和基础设施，包括数据预处理、视频压缩、参数缩放、课程式预训练和对齐导向的后训练。

Result: 开发的MUG-V 10B模型在整体性能上匹配当前最先进的视频生成器，在电商导向的视频生成任务中超越领先的开源基线。

Conclusion: 成功实现了高效的大规模视频生成模型训练，并开源了完整的训练代码和模型，为社区提供了重要的资源和工具。

Abstract: In recent years, large-scale generative models for visual content
(\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable
progress. However, training large-scale video generation models remains
particularly challenging and resource-intensive due to cross-modal text-video
alignment, the long sequences involved, and the complex spatiotemporal
dependencies. To address these challenges, we present a training framework that
optimizes four pillars: (i) data processing, (ii) model architecture, (iii)
training strategy, and (iv) infrastructure for large-scale video generation
models. These optimizations delivered significant efficiency gains and
performance improvements across all stages of data preprocessing, video
compression, parameter scaling, curriculum-based pretraining, and
alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent
state-of-the-art video generators overall and, on e-commerce-oriented video
generation tasks, surpasses leading open-source baselines in human evaluations.
More importantly, we open-source the complete stack, including model weights,
Megatron-Core-based large-scale training code, and inference pipelines for
video generation and enhancement. To our knowledge, this is the first public
release of large-scale video generation training code that exploits
Megatron-Core to achieve high training efficiency and near-linear multi-node
scaling, details are available in
\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.

</details>


### [153] [MambaX-Net: Dual-Input Mamba-Enhanced Cross-Attention Network for Longitudinal MRI Segmentation](https://arxiv.org/abs/2510.17529)
*Yovin Yahathugoda,Davide Prezzi,Piyalitt Ittichaiwong,Vicky Goh,Sebastien Ourselin,Michela Antonelli*

Main category: cs.CV

TL;DR: 提出了MambaX-Net，一种用于前列腺癌主动监测的半监督双扫描3D分割架构，能够利用时间序列MRI数据进行准确的前列腺分割，在有限标注数据下优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决主动监测中多时间点MRI数据分割的挑战，现有深度学习模型在纵向分析和专家标注稀缺场景下表现不佳。

Method: 提出MambaX-Net架构，包含Mamba增强的交叉注意力模块和形状提取器模块，结合半监督自训练策略利用伪标签进行学习。

Result: 在纵向主动监测数据集上评估，MambaX-Net显著优于最先进的U-Net和Transformer模型，在有限和噪声数据下实现优越的前列腺区域分割。

Conclusion: MambaX-Net为前列腺癌主动监测提供了一种有效的纵向分割解决方案，能够处理时间演化特征和长程空间依赖关系。

Abstract: Active Surveillance (AS) is a treatment option for managing low and
intermediate-risk prostate cancer (PCa), aiming to avoid overtreatment while
monitoring disease progression through serial MRI and clinical follow-up.
Accurate prostate segmentation is an important preliminary step for automating
this process, enabling automated detection and diagnosis of PCa. However,
existing deep-learning segmentation models are often trained on
single-time-point and expertly annotated datasets, making them unsuitable for
longitudinal AS analysis, where multiple time points and a scarcity of expert
labels hinder their effective fine-tuning. To address these challenges, we
propose MambaX-Net, a novel semi-supervised, dual-scan 3D segmentation
architecture that computes the segmentation for time point t by leveraging the
MRI and the corresponding segmentation mask from the previous time point. We
introduce two new components: (i) a Mamba-enhanced Cross-Attention Module,
which integrates the Mamba block into cross attention to efficiently capture
temporal evolution and long-range spatial dependencies, and (ii) a Shape
Extractor Module that encodes the previous segmentation mask into a latent
anatomical representation for refined zone delination. Moreover, we introduce a
semi-supervised self-training strategy that leverages pseudo-labels generated
from a pre-trained nnU-Net, enabling effective learning without expert
annotations. MambaX-Net was evaluated on a longitudinal AS dataset, and results
showed that it significantly outperforms state-of-the-art U-Net and
Transformer-based models, achieving superior prostate zone segmentation even
when trained on limited and noisy data.

</details>


### [154] [WP-CrackNet: A Collaborative Adversarial Learning Framework for End-to-End Weakly-Supervised Road Crack Detection](https://arxiv.org/abs/2510.17566)
*Nachuan Ma,Zhengfei Song,Qiang Hu,Xiaoyu Tang,Chengxi Zhang,Rui Fan,Lihua Xie*

Main category: cs.CV

TL;DR: WP-CrackNet是一种弱监督的道路裂缝检测方法，仅使用图像级标签进行像素级检测，通过分类器、重建器和检测器的对抗学习实现高精度检测。


<details>
  <summary>Details</summary>
Motivation: 减少对昂贵像素级标注的依赖，实现智能基础设施维护的可扩展道路检测。

Method: 集成分类器生成CAM、重建器测量特征可推理性、检测器生成像素级结果，通过对抗学习和路径感知注意力模块提升检测性能。

Result: 在三个图像级数据集上的实验表明，WP-CrackNet达到与监督方法相当的结果，优于现有弱监督方法。

Conclusion: 该方法显著推进了可扩展道路检测，源代码和数据集已公开。

Abstract: Road crack detection is essential for intelligent infrastructure maintenance
in smart cities. To reduce reliance on costly pixel-level annotations, we
propose WP-CrackNet, an end-to-end weakly-supervised method that trains with
only image-level labels for pixel-wise crack detection. WP-CrackNet integrates
three components: a classifier generating class activation maps (CAMs), a
reconstructor measuring feature inferability, and a detector producing
pixel-wise road crack detection results. During training, the classifier and
reconstructor alternate in adversarial learning to encourage crack CAMs to
cover complete crack regions, while the detector learns from pseudo labels
derived from post-processed crack CAMs. This mutual feedback among the three
components improves learning stability and detection accuracy. To further boost
detection performance, we design a path-aware attention module (PAAM) that
fuses high-level semantics from the classifier with low-level structural cues
from the reconstructor by modeling spatial and channel-wise dependencies.
Additionally, a center-enhanced CAM consistency module (CECCM) is proposed to
refine crack CAMs using center Gaussian weighting and consistency constraints,
enabling better pseudo-label generation. We create three image-level datasets
and extensive experiments show that WP-CrackNet achieves comparable results to
supervised methods and outperforms existing weakly-supervised methods,
significantly advancing scalable road inspection. The source code package and
datasets are available at https://mias.group/WP-CrackNet/.

</details>


### [155] [PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception](https://arxiv.org/abs/2510.17568)
*Kaichen Zhou,Yuhan Wang,Grace Chen,Xinhai Chang,Gaspard Beaudouin,Fangneng Zhan,Paul Pu Liang,Mengyu Wang*

Main category: cs.CV

TL;DR: PAGE-4D是一个扩展VGGT到动态场景的前馈模型，解决了静态3D模型在动态场景中的局限性，通过动态感知聚合器分离静态和动态信息，实现相机姿态估计、深度预测和点云重建。


<details>
  <summary>Details</summary>
Motivation: 现有的3D前馈模型（如VGGT）在静态场景中表现良好，但在涉及复杂动态元素（如移动的人或可变形物体）的真实世界场景中表现不佳。

Method: 提出PAGE-4D模型，引入动态感知聚合器，通过预测动态感知掩码来分离静态和动态信息，抑制动态区域用于姿态估计，增强动态区域用于几何重建。

Result: PAGE-4D在动态场景中 consistently优于原始VGGT，在相机姿态估计、单目和视频深度估计以及稠密点图重建方面取得更优结果。

Conclusion: PAGE-4D成功解决了动态场景中多任务4D重建的内在冲突，为动态场景的3D理解提供了有效解决方案。

Abstract: Recent 3D feed-forward models, such as the Visual Geometry Grounded
Transformer (VGGT), have shown strong capability in inferring 3D attributes of
static scenes. However, since they are typically trained on static datasets,
these models often struggle in real-world scenarios involving complex dynamic
elements, such as moving humans or deformable objects like umbrellas. To
address this limitation, we introduce PAGE-4D, a feedforward model that extends
VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and
point cloud reconstruction -- all without post-processing. A central challenge
in multi-task 4D reconstruction is the inherent conflict between tasks:
accurate camera pose estimation requires suppressing dynamic regions, while
geometry reconstruction requires modeling them. To resolve this tension, we
propose a dynamics-aware aggregator that disentangles static and dynamic
information by predicting a dynamics-aware mask -- suppressing motion cues for
pose estimation while amplifying them for geometry reconstruction. Extensive
experiments show that PAGE-4D consistently outperforms the original VGGT in
dynamic scenarios, achieving superior results in camera pose estimation,
monocular and video depth estimation, and dense point map reconstruction.

</details>


### [156] [Expose Camouflage in the Water: Underwater Camouflaged Instance Segmentation and Dataset](https://arxiv.org/abs/2510.17585)
*Chuhong Wang,Hua Li,Chongyi Li,Huazhong Liu,Xiongxin Tang,Sam Kwong*

Main category: cs.CV

TL;DR: 本文提出了首个水下伪装实例分割数据集UCIS4K和基于Segment Anything Model的水下伪装实例分割网络UCIS-SAM，通过三个关键模块解决水下环境下的伪装物体分割挑战。


<details>
  <summary>Details</summary>
Motivation: 水下环境存在颜色失真、低对比度和模糊等问题，传统基于陆地数据集训练的伪装实例分割方法在水下场景表现不佳，需要专门针对水下环境的解决方案。

Method: 提出UCIS-SAM网络，包含三个核心模块：通道平衡优化模块(CBOM)增强水下特征学习，频域真值整合模块(FDTIM)强调物体本质特征，多尺度特征频率聚合模块(MFFAM)强化低对比度伪装实例边界。

Result: 在提出的UCIS4K数据集和公共基准测试上的广泛实验表明，UCIS-SAM优于当前最先进的方法。

Conclusion: 该研究为水下伪装实例分割提供了首个专门数据集和有效的网络架构，显著提升了水下环境中伪装物体的分割精度。

Abstract: With the development of underwater exploration and marine protection,
underwater vision tasks are widespread. Due to the degraded underwater
environment, characterized by color distortion, low contrast, and blurring,
camouflaged instance segmentation (CIS) faces greater challenges in accurately
segmenting objects that blend closely with their surroundings. Traditional
camouflaged instance segmentation methods, trained on terrestrial-dominated
datasets with limited underwater samples, may exhibit inadequate performance in
underwater scenes. To address these issues, we introduce the first underwater
camouflaged instance segmentation (UCIS) dataset, abbreviated as UCIS4K, which
comprises 3,953 images of camouflaged marine organisms with instance-level
annotations. In addition, we propose an Underwater Camouflaged Instance
Segmentation network based on Segment Anything Model (UCIS-SAM). Our UCIS-SAM
includes three key modules. First, the Channel Balance Optimization Module
(CBOM) enhances channel characteristics to improve underwater feature learning,
effectively addressing the model's limited understanding of underwater
environments. Second, the Frequency Domain True Integration Module (FDTIM) is
proposed to emphasize intrinsic object features and reduce interference from
camouflage patterns, enhancing the segmentation performance of camouflaged
objects blending with their surroundings. Finally, the Multi-scale Feature
Frequency Aggregation Module (MFFAM) is designed to strengthen the boundaries
of low-contrast camouflaged instances across multiple frequency bands,
improving the model's ability to achieve more precise segmentation of
camouflaged objects. Extensive experiments on the proposed UCIS4K and public
benchmarks show that our UCIS-SAM outperforms state-of-the-art approaches.

</details>


### [157] [ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling](https://arxiv.org/abs/2510.17603)
*Shuyuan Zhang,Chenhan Jiang,Zuoou Li,Jiankang Deng*

Main category: cs.CV

TL;DR: ShapeCraft是一个新颖的多代理框架，用于从自然语言生成3D资源，通过图结构表示和分层解析实现几何准确和语义丰富的3D生成。


<details>
  <summary>Details</summary>
Motivation: 解决现有文本到3D生成方法产生的非结构化网格和交互性差的问题，使其更适合艺术工作流程。

Method: 提出基于图的程序形状（GPS）表示，将复杂自然语言分解为子任务图，使用LLM代理分层解析用户输入并迭代优化程序建模和绘画。

Result: 定性和定量实验显示ShapeCraft在生成几何准确和语义丰富的3D资源方面优于现有基于LLM的代理。

Conclusion: ShapeCraft展示了通过动画和用户定制编辑等交互应用的广泛潜力。

Abstract: 3D generation from natural language offers significant potential to reduce
expert manual modeling efforts and enhance accessibility to 3D assets. However,
existing methods often yield unstructured meshes and exhibit poor
interactivity, making them impractical for artistic workflows. To address these
limitations, we represent 3D assets as shape programs and introduce ShapeCraft,
a novel multi-agent framework for text-to-3D generation. At its core, we
propose a Graph-based Procedural Shape (GPS) representation that decomposes
complex natural language into a structured graph of sub-tasks, thereby
facilitating accurate LLM comprehension and interpretation of spatial
relationships and semantic shape details. Specifically, LLM agents
hierarchically parse user input to initialize GPS, then iteratively refine
procedural modeling and painting to produce structured, textured, and
interactive 3D assets. Qualitative and quantitative experiments demonstrate
ShapeCraft's superior performance in generating geometrically accurate and
semantically rich 3D assets compared to existing LLM-based agents. We further
show the versatility of ShapeCraft through examples of animated and
user-customized editing, highlighting its potential for broader interactive
applications.

</details>


### [158] [Integrating BIM and UAV-based photogrammetry for Automated 3D Structure Model Segmentation](https://arxiv.org/abs/2510.17609)
*Siqi Chen,Shanyue Guan*

Main category: cs.CV

TL;DR: 提出了一种基于机器学习的3D点云自动分割框架，结合无人机扫描的真实点云和BIM生成的合成数据，解决了基础设施模型分割中手动标注耗时且易出错的问题。


<details>
  <summary>Details</summary>
Motivation: 无人机技术结合摄影测量能够高效获取基础设施的高分辨率3D模型，但传统的手动分割方法耗时且易出错，需要自动化解决方案。

Method: 使用真实无人机扫描点云和BIM合成数据的互补优势，通过机器学习框架实现3D点云的自动分割，利用小规模数据集和BIM数据补充来减少训练时间。

Result: 在铁路轨道数据集上的验证显示，该方法能够高精度识别和分割主要组件（如轨道和枕木），同时保持合理的分割精度。

Conclusion: 该自动化方法提高了3D基础设施模型分割的精度和效率，推动了无人机与BIM技术在结构健康监测和基础设施管理中的集成。

Abstract: The advancement of UAV technology has enabled efficient, non-contact
structural health monitoring. Combined with photogrammetry, UAVs can capture
high-resolution scans and reconstruct detailed 3D models of infrastructure.
However, a key challenge remains in segmenting specific structural components
from these models-a process traditionally reliant on time-consuming and
error-prone manual labeling. To address this issue, we propose a machine
learning-based framework for automated segmentation of 3D point clouds. Our
approach uses the complementary strengths of real-world UAV-scanned point
clouds and synthetic data generated from Building Information Modeling (BIM) to
overcome the limitations associated with manual labeling. Validation on a
railroad track dataset demonstrated high accuracy in identifying and segmenting
major components such as rails and crossties. Moreover, by using smaller-scale
datasets supplemented with BIM data, the framework significantly reduced
training time while maintaining reasonable segmentation accuracy. This
automated approach improves the precision and efficiency of 3D infrastructure
model segmentation and advances the integration of UAV and BIM technologies in
structural health monitoring and infrastructure management.

</details>


### [159] [One Dinomaly2 Detect Them All: A Unified Framework for Full-Spectrum Unsupervised Anomaly Detection](https://arxiv.org/abs/2510.17611)
*Jia Guo,Shuai Lu,Lei Fan,Zelin Li,Donglin Di,Yang Song,Weihang Zhang,Wenbing Zhu,Hong Yan,Fang Chen,Huiqi Li,Hongen Liao*

Main category: cs.CV

TL;DR: Dinomaly2是一个统一的无监督异常检测框架，通过五个简单元素的组合在标准重建框架中实现卓越性能，在多模态、多任务设置下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的多类异常检测模型性能明显落后于先进的单类模型，且该领域已分裂为针对特定场景的专门方法，存在部署障碍，需要统一解决方案。

Method: 基于"少即是多"理念，在标准重建框架中协调五个简单元素，实现方法简约化，无需修改即可自然扩展到不同任务。

Result: 在12个异常检测基准测试中，Dinomaly2在多种模态、任务设置和应用领域均表现出全谱系优势，多类模型在MVTec-AD和VisA上分别达到99.9%和99.3%的I-AUROC。

Conclusion: 简约设计、计算可扩展性和通用适用性的结合使Dinomaly2成为现实世界异常检测应用的统一解决方案。

Abstract: Unsupervised anomaly detection (UAD) has evolved from building specialized
single-class models to unified multi-class models, yet existing multi-class
models significantly underperform the most advanced one-for-one counterparts.
Moreover, the field has fragmented into specialized methods tailored to
specific scenarios (multi-class, 3D, few-shot, etc.), creating deployment
barriers and highlighting the need for a unified solution. In this paper, we
present Dinomaly2, the first unified framework for full-spectrum image UAD,
which bridges the performance gap in multi-class models while seamlessly
extending across diverse data modalities and task settings. Guided by the "less
is more" philosophy, we demonstrate that the orchestration of five simple
element achieves superior performance in a standard reconstruction-based
framework. This methodological minimalism enables natural extension across
diverse tasks without modification, establishing that simplicity is the
foundation of true universality. Extensive experiments on 12 UAD benchmarks
demonstrate Dinomaly2's full-spectrum superiority across multiple modalities
(2D, multi-view, RGB-3D, RGB-IR), task settings (single-class, multi-class,
inference-unified multi-class, few-shot) and application domains (industrial,
biological, outdoor). For example, our multi-class model achieves unprecedented
99.9% and 99.3% image-level (I-) AUROC on MVTec-AD and VisA respectively. For
multi-view and multi-modal inspection, Dinomaly2 demonstrates state-of-the-art
performance with minimum adaptations. Moreover, using only 8 normal examples
per class, our method surpasses previous full-shot models, achieving 98.7% and
97.4% I-AUROC on MVTec-AD and VisA. The combination of minimalistic design,
computational scalability, and universal applicability positions Dinomaly2 as a
unified solution for the full spectrum of real-world anomaly detection
applications.

</details>


### [160] [CaMiT: A Time-Aware Car Model Dataset for Classification and Generation](https://arxiv.org/abs/2510.17626)
*Frédéric LIN,Biruk Abere Ambaw,Adrian Popescu,Hejer Ammar,Romaric Audigier,Hervé Le Borgne*

Main category: cs.CV

TL;DR: CaMiT数据集研究汽车模型随时间演变的视觉识别问题，提出了时间增量学习策略来提高AI系统对视觉环境变化的适应能力。


<details>
  <summary>Details</summary>
Motivation: AI系统需要适应不断变化的视觉环境，特别是在物体外观随时间变化的领域中。汽车模型作为技术制品的代表性类别，其外观演变具有研究价值。

Method: 构建CaMiT数据集（787K标注样本+5.1M未标注样本），提出时间增量分类设置，评估两种策略：时间增量预训练（更新主干网络）和时间增量分类器学习（仅更新最后一层）。

Result: 静态预训练在领域内数据上表现与大规模通用模型相当但更高效，但跨年份测试时准确率下降。时间增量学习策略显著提高了时间鲁棒性。

Conclusion: CaMiT为研究细粒度视觉识别和生成中的时间适应问题提供了丰富的基准，时间增量学习是解决视觉环境演变的有效方法。

Abstract: AI systems must adapt to evolving visual environments, especially in domains
where object appearances change over time. We introduce Car Models in Time
(CaMiT), a fine-grained dataset capturing the temporal evolution of car models,
a representative class of technological artifacts. CaMiT includes 787K labeled
samples of 190 car models (2007-2023) and 5.1M unlabeled samples (2005-2023),
supporting both supervised and self-supervised learning. Static pretraining on
in-domain data achieves competitive performance with large-scale generalist
models while being more resource-efficient, yet accuracy declines when models
are tested across years. To address this, we propose a time-incremental
classification setting, a realistic continual learning scenario with emerging,
evolving, and disappearing classes. We evaluate two strategies:
time-incremental pretraining, which updates the backbone, and time-incremental
classifier learning, which updates only the final layer, both improving
temporal robustness. Finally, we explore time-aware image generation that
leverages temporal metadata during training, yielding more realistic outputs.
CaMiT offers a rich benchmark for studying temporal adaptation in fine-grained
visual recognition and generation.

</details>


### [161] [Self-supervised Pre-training for Mapping of Archaeological Stone Wall in Historic Landscapes Using High-Resolution DEM Derivatives](https://arxiv.org/abs/2510.17644)
*Zexian Huang,Mashnoon Islam,Brian Armstrong,Kourosh Khoshelham,Martin Tomko*

Main category: cs.CV

TL;DR: DINO-CV是一个用于自动测绘低矮干石墙的分割框架，利用高分辨率机载LiDAR衍生的数字高程模型（DEMs）克服植被遮挡问题，并通过自监督跨视图预训练策略解决标注数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 干石墙具有重要的遗产和环境价值，但许多墙体因难以接近和手动测绘成本高而未被识别。深度学习分割方法面临视觉遮挡和标注数据有限两大挑战。

Method: 提出DINO-CV分割框架，使用DEMs捕捉植被下的地形结构，分析结构而非光谱线索。采用基于知识蒸馏的自监督跨视图预训练策略，学习多个DEM衍生物的不变视觉和几何表示，支持多种视觉骨干网络。

Result: 在澳大利亚Budj Bim UNESCO世界遗产文化景观中应用，识别出澳大利亚最密集的殖民时期干石墙集合。测试区域平均交并比（mIoU）达到68.6%，仅使用10%标注数据微调后仍保持63.8% mIoU。

Conclusion: 自监督学习在高分辨率DEM衍生物上的应用，为植被覆盖和遗产丰富环境中标注稀缺的干石墙自动测绘展示了潜力。

Abstract: Dry-stone walls hold significant heritage and environmental value. Mapping
these structures is essential for ecosystem preservation and wildfire
management in Australia. Yet, many walls remain unidentified due to their
inaccessibility and the high cost of manual mapping. Deep learning-based
segmentation offers a scalable solution, but two major challenges persist: (1)
visual occlusion of low-lying walls by dense vegetation, and (2) limited
labeled data for supervised training. We propose DINO-CV, a segmentation
framework for automatic mapping of low-lying dry-stone walls using
high-resolution Airborne LiDAR-derived digital elevation models (DEMs). DEMs
overcome visual occlusion by capturing terrain structures hidden beneath
vegetation, enabling analysis of structural rather than spectral cues. DINO-CV
introduces a self-supervised cross-view pre-training strategy based on
knowledge distillation to mitigate data scarcity. It learns invariant visual
and geometric representations across multiple DEM derivatives, supporting
various vision backbones including ResNet, Wide ResNet, and Vision
Transformers. Applied to the UNESCO World Heritage cultural landscape of Budj
Bim, Victoria, the method identifies one of Australia's densest collections of
colonial dry-stone walls beyond Indigenous heritage contexts. DINO-CV achieves
a mean Intersection over Union (mIoU) of 68.6% on test areas and maintains
63.8% mIoU when fine-tuned with only 10% labeled data. These results
demonstrate the potential of self-supervised learning on high-resolution DEM
derivatives for automated dry-stone wall mapping in vegetated and heritage-rich
environments with scarce annotations.

</details>


### [162] [Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs](https://arxiv.org/abs/2510.17651)
*Sébastien Thuau,Siba Haidar,Ayush Bajracharya,Rachid Chelouah*

Main category: cs.CV

TL;DR: 该论文比较了两种节俭联邦学习方法用于暴力检测：基于视觉语言模型(VLM)的零样本/联邦微调，以及基于紧凑3D卷积神经网络(CNN3D)的个性化训练。在非独立同分布设置下，两种方法都达到90%以上准确率，CNN3D在ROC AUC和能量效率方面略优，而VLM在上下文推理方面表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发资源高效的联邦学习方法来检测暴力行为，重点关注能源效率和环境可持续性，为视频监控提供负责任的人工智能解决方案。

Method: 使用LLaVA-7B视觉语言模型和65.8M参数CNN3D作为代表案例，比较零样本/联邦微调VLM与个性化训练CNN3D两种策略，评估准确性、校准性和能源使用情况。

Result: 两种方法都超过90%准确率，CNN3D在ROC AUC和对数损失方面略优于LoRA调优的VLM，且能耗更低。VLM在上下文推理和多模态推理方面保持优势。

Conclusion: 提出混合模型：轻量级CNN用于常规分类，选择性激活VLM处理复杂场景。该框架为视频监控提供了可复现的、资源感知的AI基线，支持实时多模态和生命周期感知系统。

Abstract: We examine frugal federated learning approaches to violence detection by
comparing two complementary strategies: (i) zero-shot and federated fine-tuning
of vision-language models (VLMs), and (ii) personalized training of a compact
3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter
CNN3D as representative cases, we evaluate accuracy, calibration, and energy
usage under realistic non-IID settings. Both approaches exceed 90% accuracy.
CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and
log loss, while using less energy. VLMs remain favorable for contextual
reasoning and multimodal inference. We quantify energy and CO$_2$ emissions
across training and inference, and analyze sustainability trade-offs for
deployment. To our knowledge, this is the first comparative study of LoRA-tuned
vision-language models and personalized CNNs for federated violence detection,
with an emphasis on energy efficiency and environmental metrics. These findings
support a hybrid model: lightweight CNNs for routine classification, with
selective VLM activation for complex or descriptive scenarios. The resulting
framework offers a reproducible baseline for responsible, resource-aware AI in
video surveillance, with extensions toward real-time, multimodal, and
lifecycle-aware systems.

</details>


### [163] [4DSegStreamer: Streaming 4D Panoptic Segmentation via Dual Threads](https://arxiv.org/abs/2510.17664)
*Ling Liu,Jun Tian,Li Yi*

Main category: cs.CV

TL;DR: 4DSegStreamer是一个用于4D全景分割的流式处理框架，采用双线程系统在有限时间预算内实现实时细粒度感知。


<details>
  <summary>Details</summary>
Motivation: 解决高度动态环境（如密集人群疏散和复杂自动驾驶场景）中实时4D全景分割的需求，现有方法在高FPS条件下鲁棒性不足。

Method: 采用预测线程和推理线程的双线程系统：预测线程利用历史运动几何信息预测未来动态；推理线程通过对齐最新记忆并补偿自运动和动态物体运动来确保及时预测。

Result: 在室内HOI4D数据集和室外SemanticKITTI、nuScenes数据集上的实验表明，该方法在复杂场景中能准确预测动态物体，特别是在高FPS条件下表现出优越鲁棒性。

Conclusion: 4DSegStreamer是一个通用框架，可无缝集成到现有3D和4D分割方法中，实现实时处理能力，在动态物体预测方面效果显著。

Abstract: 4D panoptic segmentation in a streaming setting is critical for highly
dynamic environments, such as evacuating dense crowds and autonomous driving in
complex scenarios, where real-time, fine-grained perception within a
constrained time budget is essential. In this paper, we introduce
4DSegStreamer, a novel framework that employs a Dual-Thread System to
efficiently process streaming frames. The framework is general and can be
seamlessly integrated into existing 3D and 4D segmentation methods to enable
real-time capability. It also demonstrates superior robustness compared to
existing streaming perception approaches, particularly under high FPS
conditions. The system consists of a predictive thread and an inference thread.
The predictive thread leverages historical motion and geometric information to
extract features and forecast future dynamics. The inference thread ensures
timely prediction for incoming frames by aligning with the latest memory and
compensating for ego-motion and dynamic object movements. We evaluate
4DSegStreamer on the indoor HOI4D dataset and the outdoor SemanticKITTI and
nuScenes datasets. Comprehensive experiments demonstrate the effectiveness of
our approach, particularly in accurately predicting dynamic objects in complex
scenes.

</details>


### [164] [PICABench: How Far Are We from Physically Realistic Image Editing?](https://arxiv.org/abs/2510.17681)
*Yuandong Pu,Le Zhuo,Songhao Han,Jinbo Xing,Kaiwen Zhu,Shuo Cao,Bin Fu,Si Liu,Hongsheng Li,Yu Qiao,Wenlong Zhang,Xi Chen,Yihao Liu*

Main category: cs.CV

TL;DR: 论文提出了PICABench基准和PICAEval评估协议，用于系统评估图像编辑的物理真实性，涵盖光学、力学等8个子维度，发现现有模型在物理一致性方面仍有很大提升空间。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型主要关注指令完成度，但忽视了编辑过程中的物理效应（如阴影、反射等），这影响了生成图像的真实性。

Method: 提出PICABench基准系统评估物理真实性，使用PICAEval协议（基于VLM-as-a-judge和人工标注）进行可靠评估，并构建PICA-100K数据集从视频中学习物理知识。

Result: 评估主流模型后发现，物理真实性仍是一个具有挑战性的问题，现有模型在这方面有较大改进空间。

Conclusion: 该研究为从简单内容编辑转向物理一致性真实感提供了基准和解决方案基础，推动了图像编辑向更高真实感发展。

Abstract: Image editing has achieved remarkable progress recently. Modern editing
models could already follow complex instructions to manipulate the original
content. However, beyond completing the editing instructions, the accompanying
physical effects are the key to the generation realism. For example, removing
an object should also remove its shadow, reflections, and interactions with
nearby objects. Unfortunately, existing models and benchmarks mainly focus on
instruction completion but overlook these physical effects. So, at this moment,
how far are we from physically realistic image editing? To answer this, we
introduce PICABench, which systematically evaluates physical realism across
eight sub-dimension (spanning optics, mechanics, and state transitions) for
most of the common editing operations (add, remove, attribute change, etc). We
further propose the PICAEval, a reliable evaluation protocol that uses
VLM-as-a-judge with per-case, region-level human annotations and questions.
Beyond benchmarking, we also explore effective solutions by learning physics
from videos and construct a training dataset PICA-100K. After evaluating most
of the mainstream models, we observe that physical realism remains a
challenging problem with large rooms to explore. We hope that our benchmark and
proposed solutions can serve as a foundation for future work moving from naive
content editing toward physically consistent realism.

</details>


### [165] [Intelligent Communication Mixture-of-Experts Boosted-Medical Image Segmentation Foundation Model](https://arxiv.org/abs/2510.17684)
*Xinwei Zhang,Hu Chen,Zhe Yuan,Sukun Tian,Peng Feng*

Main category: cs.CV

TL;DR: 提出IC-MoE模型，通过混合专家和语义引导对比学习解决医学图像分割中高层特征表示不足和预训练权重结构完整性被破坏的问题


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割基础模型在微调过程中存在高层特征表示不足和预训练权重结构完整性被破坏两个关键问题

Method: 1. 构建基础专家、语义专家和自适应专家，采用像素概率自适应投票策略进行专家选择和融合；2. 提出语义引导对比学习方法增强高层特征表示能力

Result: 在三个公开医学图像分割数据集上的实验表明IC-MoE优于其他SOTA模型，具有优秀的泛化能力

Conclusion: IC-MoE有效补充了基础医学图像分割模型的高层特征和预训练结构完整性

Abstract: Foundation models for medical image segmentation have achieved remarkable
performance. Adaptive fine-tuning of natural image segmentation foundation
models is crucial for medical image segmentation tasks. However, some
limitations exist in existing fine-tuning methods: 1) insufficient
representation of high-level features and 2) the fine-tuning process disrupts
the structural integrity of pretrained weights. Inspired by these critical
problems, we propose an intelligent communication mixture-of-experts
boosted-medical image segmentation foundation model, named IC-MoE, with twofold
ideas: 1) We construct basic experts, semantic experts, and adaptive experts.
Moreover, we implement a pixel probability adaptive voting strategy, which
enables expert selection and fusion through label consistency and load
balancing. This approach preliminarily enhances the representation capability
of high-level features while preserving the structural integrity of pretrained
weights. 2) We propose a semantic-guided contrastive learning method to address
the issue of weak supervision in contrastive learning. This method further
enhances the representation capability of high-level features while preserving
the structural integrity of pretrained weights. Extensive experiments across
three public medical image segmentation datasets demonstrate that the IC-MoE
outperforms other SOTA models. Consequently, the proposed IC-MoE effectively
supplements foundational medical image segmentation models with high-level
features and pretrained structural integrity. We also validate the superior
generalizability of the IC-MoE across diverse medical image segmentation
scenarios.

</details>


### [166] [Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning](https://arxiv.org/abs/2510.17685)
*Min Cao,Xinyu Zhou,Ding Jiang,Bo Du,Mang Ye,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了Bi-IRRA框架，通过双向隐式关系推理和多维全局对齐来解决多语言文本到图像人物检索中的模态异质性问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像人物检索方法存在两个主要问题：全局方法忽略细粒度跨模态差异，局部方法需要先验信息进行显式部分对齐；同时当前方法主要针对英语，限制了在多语言环境中的应用。

Method: 提出Bi-IRRA框架，包含双向隐式关系推理模块（通过掩码图像和文本的双向预测来隐式增强跨语言和跨模态的局部关系建模）和多维全局对齐模块（桥接模态异质性）。

Result: 该方法在所有多语言文本到图像人物检索数据集上取得了新的最先进结果。

Conclusion: Bi-IRRA框架有效解决了多语言文本到图像人物检索中的模态异质性问题，为多语言环境下的跨模态检索提供了新的解决方案。

Abstract: Text-to-image person retrieval (TIPR) aims to identify the target person
using textual descriptions, facing challenge in modality heterogeneity. Prior
works have attempted to address it by developing cross-modal global or local
alignment strategies. However, global methods typically overlook fine-grained
cross-modal differences, whereas local methods require prior information to
explore explicit part alignments. Additionally, current methods are
English-centric, restricting their application in multilingual contexts. To
alleviate these issues, we pioneer a multilingual TIPR task by developing a
multilingual TIPR benchmark, for which we leverage large language models for
initial translations and refine them by integrating domain-specific knowledge.
Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation
Reasoning and Aligning framework to learn alignment across languages and
modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module
enables bidirectional prediction of masked image and text, implicitly enhancing
the modeling of local relations across languages and modalities, a
multi-dimensional global alignment module is integrated to bridge the modality
heterogeneity. The proposed method achieves new state-of-the-art results on all
multilingual TIPR datasets. Data and code are presented in
https://github.com/Flame-Chasers/Bi-IRRA.

</details>


### [167] [Towards 3D Objectness Learning in an Open World](https://arxiv.org/abs/2510.17686)
*Taichi Liu,Zhenyu Wang,Ruofeng Liu,Guang Wang,Desheng Zhang*

Main category: cs.CV

TL;DR: 提出OP3Det，一种无需文本提示的开放世界3D检测器，利用2D基础模型的泛化能力结合3D几何先验，通过跨模态专家混合实现广义3D物体检测。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集3D检测器难以泛化到开放世界场景，现有3D开放词汇模型在词汇扩展和语义重叠方面存在困难，需要解决广义3D物体发现的问题。

Method: 引入2D基础模型的强泛化和零样本能力，结合2D语义先验和3D几何先验生成类别无关的候选区域；通过点云和RGB图像的跨模态专家混合，动态路由单模态和多模态特征学习广义3D物体性。

Result: 在广泛实验中，OP3Det显著超越现有开放世界3D检测器达16.0%的AR提升，相比封闭世界3D检测器实现13.5%的改进。

Conclusion: OP3Det在开放世界3D物体检测方面表现出色，证明了结合2D基础模型和跨模态融合策略在广义3D物体发现中的有效性。

Abstract: Recent advancements in 3D object detection and novel category detection have
made significant progress, yet research on learning generalized 3D objectness
remains insufficient. In this paper, we delve into learning open-world 3D
objectness, which focuses on detecting all objects in a 3D scene, including
novel objects unseen during training. Traditional closed-set 3D detectors
struggle to generalize to open-world scenarios, while directly incorporating 3D
open-vocabulary models for open-world ability struggles with vocabulary
expansion and semantic overlap. To achieve generalized 3D object discovery, We
propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect
any objects within 3D scenes without relying on hand-crafted text prompts. We
introduce the strong generalization and zero-shot capabilities of 2D foundation
models, utilizing both 2D semantic priors and 3D geometric priors for
class-agnostic proposals to broaden 3D object discovery. Then, by integrating
complementary information from point cloud and RGB image in the cross-modal
mixture of experts, OP3Det dynamically routes uni-modal and multi-modal
features to learn generalized 3D objectness. Extensive experiments demonstrate
the extraordinary performance of OP3Det, which significantly surpasses existing
open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement
compared to closed-world 3D detectors.

</details>


### [168] [GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver](https://arxiv.org/abs/2510.17699)
*Aleksandr Oganov,Ilya Bykov,Eva Neudachina,Mishan Aliev,Alexander Tolmachev,Alexander Sidorov,Aleksandr Zuev,Andrey Okhotin,Denis Rakitin,Aibek Alanov*

Main category: cs.CV

TL;DR: 提出了一种名为广义对抗求解器的新方法，通过简单的ODE求解器参数化和对抗训练，在保持少量采样步数的同时提高生成质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型虽然生成质量优秀，但采样过程计算昂贵。现有方法虽然通过蒸馏减少了采样步数，但训练复杂且细节保留不足

Method: 结合了简单的ODE求解器参数化（无需复杂训练技巧）和对抗训练，将原始蒸馏损失与对抗训练相结合以减少伪影并增强细节保真度

Result: 在相似资源约束下，相比现有求解器训练方法表现出更优越的性能

Conclusion: 广义对抗求解器提供了一种简单有效的方法来平衡扩散模型的采样效率和生成质量

Abstract: While diffusion models achieve state-of-the-art generation quality, they
still suffer from computationally expensive sampling. Recent works address this
issue with gradient-based optimization methods that distill a few-step ODE
diffusion solver from the full sampling process, reducing the number of
function evaluations from dozens to just a few. However, these approaches often
rely on intricate training techniques and do not explicitly focus on preserving
fine-grained details. In this paper, we introduce the Generalized Solver: a
simple parameterization of the ODE sampler that does not require additional
training tricks and improves quality over existing approaches. We further
combine the original distillation loss with adversarial training, which
mitigates artifacts and enhances detail fidelity. We call the resulting method
the Generalized Adversarial Solver and demonstrate its superior performance
compared to existing solver training methods under similar resource
constraints. Code is available at https://github.com/3145tttt/GAS.

</details>


### [169] [Elastic ViTs from Pretrained Models without Retraining](https://arxiv.org/abs/2510.17700)
*Walter Simoncini,Michael Dorkenwald,Tijmen Blankevoort,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CV

TL;DR: SnapViT是一种后预训练结构化剪枝方法，能够在不同计算预算下实现弹性推理，无需标签数据或重新训练，在5分钟内生成可调整的弹性模型。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型只有有限的预定义尺寸，无法满足实际部署中的多样化计算约束需求。

Method: 结合梯度信息与跨网络结构相关性（通过进化算法近似），使用自监督重要性评分机制，无需分类头即可泛化到各种模型。

Result: 在DINO、SigLIPv2、DeIT和AugReg模型上表现出优于现有方法的性能，支持多种稀疏度设置。

Conclusion: SnapViT提供了一种高效的视觉Transformer剪枝策略，通过新颖的Hessian非对角结构近似和自监督评分机制，实现了无需重新训练或标签的强性能保持。

Abstract: Vision foundation models achieve remarkable performance but are only
available in a limited set of pre-determined sizes, forcing sub-optimal
deployment choices under real-world constraints. We introduce SnapViT:
Single-shot network approximation for pruned Vision Transformers, a new
post-pretraining structured pruning method that enables elastic inference
across a continuum of compute budgets. Our approach efficiently combines
gradient information with cross-network structure correlations, approximated
via an evolutionary algorithm, does not require labeled data, generalizes to
models without a classification head, and is retraining-free. Experiments on
DINO, SigLIPv2, DeIT, and AugReg models demonstrate superior performance over
state-of-the-art methods across various sparsities, requiring less than five
minutes on a single A100 GPU to generate elastic models that can be adjusted to
any computational budget. Our key contributions include an efficient pruning
strategy for pretrained Vision Transformers, a novel evolutionary approximation
of Hessian off-diagonal structures, and a self-supervised importance scoring
mechanism that maintains strong performance without requiring retraining or
labels. Code and pruned models are available at: https://elastic.ashita.nl/

</details>


### [170] [Improving Cross-Patient Generalization in Parkinson's Disease Detection through Chunk-Based Analysis of Hand-Drawn Patterns](https://arxiv.org/abs/2510.17703)
*Mhd Adnan Albani,Riad Sonbol*

Main category: cs.CV

TL;DR: 提出了一种基于图像分块策略的两阶段帕金森病检测方法，通过将手绘图像分为2x2块分别处理，使用集成方法合并决策，显著提高了对未见患者数据的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有帕金森病检测方法存在两个主要问题：数据集不足以及对未见患者数据的鲁棒性差。需要开发一种能够更好处理未见患者数据的检测方法。

Method: 两阶段方法：第一阶段按绘图类型（圆形、波形、螺旋）分类；第二阶段将图像分为2x2块，分别提取特征并检测帕金森病指标，最后使用集成方法合并各块的决策。

Result: 在NewHandPD数据集上，对已见患者准确率达到97.08%，对未见患者达到94.91%，性能差距仅为2.17个百分点，相比之前工作的4.76个百分点下降有明显改善。

Conclusion: 提出的分块策略和两阶段方法有效解决了帕金森病检测中的数据集不足和鲁棒性问题，特别是在处理未见患者数据方面表现出色，为早期帕金森病检测提供了更可靠的解决方案。

Abstract: Parkinson's disease (PD) is a neurodegenerative disease affecting about 1% of
people over the age of 60, causing motor impairments that impede hand
coordination activities such as writing and drawing. Many approaches have tried
to support early detection of Parkinson's disease based on hand-drawn images;
however, we identified two major limitations in the related works: (1) the lack
of sufficient datasets, (2) the robustness when dealing with unseen patient
data. In this paper, we propose a new approach to detect Parkinson's disease
that consists of two stages: The first stage classifies based on their drawing
type(circle, meander, spiral), and the second stage extracts the required
features from the images and detects Parkinson's disease. We overcame the
previous two limitations by applying a chunking strategy where we divide each
image into 2x2 chunks. Each chunk is processed separately when extracting
features and recognizing Parkinson's disease indicators. To make the final
classification, an ensemble method is used to merge the decisions made from
each chunk. Our evaluation shows that our proposed approach outperforms the top
performing state-of-the-art approaches, in particular on unseen patients. On
the NewHandPD dataset our approach, it achieved 97.08% accuracy for seen
patients and 94.91% for unseen patients, our proposed approach maintained a gap
of only 2.17 percentage points, compared to the 4.76-point drop observed in
prior work.

</details>


### [171] [Automatic Classification of Circulating Blood Cell Clusters based on Multi-channel Flow Cytometry Imaging](https://arxiv.org/abs/2510.17716)
*Suqiang Ma,Subhadeep Sengupta,Yao Lee,Beikang Gu,Xianyan Chen,Xianqiao Wang,Yang Liu,Mengjia Xu,Galit H. Frydman,He Li*

Main category: cs.CV

TL;DR: 本文提出了一种用于分析循环血细胞簇（CCCs）图像的计算框架，采用两步分析策略：首先使用YOLOv11模型分类细胞簇图像，然后通过多通道荧光染色识别细胞类型，准确率超过95%。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏自动分析包含细胞簇图像的工具，而细胞簇具有不规则形状、大小和异质性细胞类型，需要多通道染色来识别特定细胞类型。

Method: 两步分析策略：1）通过微调YOLOv11模型将图像分类为细胞簇和非簇组；2）通过叠加簇轮廓与多通道荧光染色区域来识别细胞类型。

Result: 该方法在簇分类和表型识别方面均达到超过95%的准确率，优于传统CNN和ViT模型。

Conclusion: 该自动化框架有效分析流式细胞术中的CCC图像，利用明场和荧光数据，具有扩展到免疫和肿瘤细胞簇分析的潜力。

Abstract: Circulating blood cell clusters (CCCs) containing red blood cells (RBCs),
white blood cells(WBCs), and platelets are significant biomarkers linked to
conditions like thrombosis, infection, and inflammation. Flow cytometry, paired
with fluorescence staining, is commonly used to analyze these cell clusters,
revealing cell morphology and protein profiles. While computational approaches
based on machine learning have advanced the automatic analysis of single-cell
flow cytometry images, there is a lack of effort to build tools to
automatically analyze images containing CCCs. Unlike single cells, cell
clusters often exhibit irregular shapes and sizes. In addition, these cell
clusters often consist of heterogeneous cell types, which require multi-channel
staining to identify the specific cell types within the clusters. This study
introduces a new computational framework for analyzing CCC images and
identifying cell types within clusters. Our framework uses a two-step analysis
strategy. First, it categorizes images into cell cluster and non-cluster groups
by fine-tuning the You Only Look Once(YOLOv11) model, which outperforms
traditional convolutional neural networks (CNNs), Vision Transformers (ViT).
Then, it identifies cell types by overlaying cluster contours with regions from
multi-channel fluorescence stains, enhancing accuracy despite cell debris and
staining artifacts. This approach achieved over 95% accuracy in both cluster
classification and phenotype identification. In summary, our automated
framework effectively analyzes CCC images from flow cytometry, leveraging both
bright-field and fluorescence data. Initially tested on blood cells, it holds
potential for broader applications, such as analyzing immune and tumor cell
clusters, supporting cellular research across various diseases.

</details>


### [172] [Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions](https://arxiv.org/abs/2510.17719)
*Zhiqiang Teng,Beibei Lin,Tingting Chen,Zifeng Yuan,Xuanyi Li,Xuanyu Zhang,Shunli Zhang*

Main category: cs.CV

TL;DR: RaindropGS是一个针对3D高斯溅射（3DGS）在雨滴条件下的综合基准测试，通过真实世界雨滴重建数据集评估从无约束雨滴污染图像到清晰3DGS重建的完整流程。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常使用已知相机姿态的合成雨滴图像评估3DGS，但真实场景中雨滴会干扰相机姿态估计和点云初始化，且合成与真实雨滴存在显著领域差距。

Method: 构建包含三个对齐图像集（雨滴聚焦、背景聚焦和无雨地面真实）的真实世界雨滴重建数据集，设计完整基准测试流程：数据准备、数据处理和雨滴感知3DGS评估。

Result: 揭示了现有3DGS方法在无约束雨滴图像上的性能限制，以及不同流程组件（相机焦点位置、不准确姿态和点云初始化）对重建的干扰影响。

Conclusion: 为开发更鲁棒的雨滴条件下3DGS方法提供了明确方向，建立了全面的评估基准。

Abstract: 3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe
occlusions and optical distortions caused by raindrop contamination on the
camera lens, substantially degrading reconstruction quality. Existing
benchmarks typically evaluate 3DGS using synthetic raindrop images with known
camera poses (constrained images), assuming ideal conditions. However, in
real-world scenarios, raindrops often interfere with accurate camera pose
estimation and point cloud initialization. Moreover, a significant domain gap
between synthetic and real raindrops further impairs generalization. To tackle
these issues, we introduce RaindropGS, a comprehensive benchmark designed to
evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images
to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline
consists of three parts: data preparation, data processing, and raindrop-aware
3DGS evaluation, including types of raindrop interference, camera pose
estimation and point cloud initialization, single image rain removal
comparison, and 3D Gaussian training comparison. First, we collect a real-world
raindrop reconstruction dataset, in which each scene contains three aligned
image sets: raindrop-focused, background-focused, and rain-free ground truth,
enabling a comprehensive evaluation of reconstruction quality under different
focus conditions. Through comprehensive experiments and analyses, we reveal
critical insights into the performance limitations of existing 3DGS methods on
unconstrained raindrop images and the varying impact of different pipeline
components: the impact of camera focus position on 3DGS reconstruction
performance, and the interference caused by inaccurate pose and point cloud
initialization on reconstruction. These insights establish clear directions for
developing more robust 3DGS methods under raindrop conditions.

</details>


### [173] [MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues](https://arxiv.org/abs/2510.17722)
*Yaning Pan,Zekun Wang,Qianqian Xie,Yongqian Wen,Yuanxing Zhang,Guohui Zhang,Haoxuan Hu,Zhiyu Pan,Yibing Huang,Zhidong Gan,Yonghong Lin,An Ping,Tianhao Peng,Jiaheng Liu*

Main category: cs.CV

TL;DR: MT-Video-Bench是一个用于评估多模态大语言模型在多轮视频对话中表现的综合基准测试，包含987个精心策划的多轮对话，涵盖6个核心能力。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准仅限于单轮问答，无法反映真实场景中多轮对话的复杂性，需要开发专门针对多轮视频对话的评估工具。

Method: 构建MT-Video-Bench基准测试，主要评估感知性和交互性相关的6个核心能力，涵盖多样化领域的987个多轮对话，并与真实应用场景（如交互式体育分析和智能视频辅导）严格对齐。

Result: 对各种开源和闭源MLLMs进行了广泛评估，揭示了这些模型在处理多轮视频对话时存在显著的性能差异和局限性。

Conclusion: MT-Video-Bench基准测试将公开提供，以促进未来多模态大语言模型在多轮视频对话理解方面的研究发展。

Abstract: The recent development of Multimodal Large Language Models (MLLMs) has
significantly advanced AI's ability to understand visual modalities. However,
existing evaluation benchmarks remain limited to single-turn question
answering, overlooking the complexity of multi-turn dialogues in real-world
scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video
understanding benchmark for evaluating MLLMs in multi-turn dialogues.
Specifically, our MT-Video-Bench mainly assesses six core competencies that
focus on perceptivity and interactivity, encompassing 987 meticulously curated
multi-turn dialogues from diverse domains. These capabilities are rigorously
aligned with real-world applications, such as interactive sports analysis and
multi-turn video-based intelligent tutoring. With MT-Video-Bench, we
extensively evaluate various state-of-the-art open-source and closed-source
MLLMs, revealing their significant performance discrepancies and limitations in
handling multi-turn video dialogues. The benchmark will be publicly available
to foster future research.

</details>


### [174] [Signature Forgery Detection: Improving Cross-Dataset Generalization](https://arxiv.org/abs/2510.17724)
*Matheus Ramos Parracho*

Main category: cs.CV

TL;DR: 该研究比较了基于原始签名图像和壳预处理两种方法在跨数据集签名验证中的性能，发现原始图像模型整体表现更好，但壳预处理方法显示出改进潜力。


<details>
  <summary>Details</summary>
Motivation: 解决离线签名验证中深度学习模型难以跨数据集泛化的问题，因为笔迹风格和采集协议的差异会降低性能。

Method: 使用CEDAR、ICDAR和GPDS Synthetic三个公开基准数据集，开发了两种实验流程：基于原始签名图像的方法和基于壳预处理的方法。

Result: 原始图像模型在跨基准测试中取得了更高的性能，而基于壳预处理的方法显示出未来改进的潜力。

Conclusion: 虽然原始图像方法当前表现更好，但壳预处理方法为开发鲁棒的跨域签名验证系统提供了有前景的方向。

Abstract: Automated signature verification is a critical biometric technique used in
banking, identity authentication, and legal documentation. Despite the notable
progress achieved by deep learning methods, most approaches in offline
signature verification still struggle to generalize across datasets, as
variations in handwriting styles and acquisition protocols often degrade
performance. This study investigates feature learning strategies for signature
forgery detection, focusing on improving cross-dataset generalization -- that
is, model robustness when trained on one dataset and tested on another. Using
three public benchmarks -- CEDAR, ICDAR, and GPDS Synthetic -- two experimental
pipelines were developed: one based on raw signature images and another
employing a preprocessing method referred to as shell preprocessing. Several
behavioral patterns were identified and analyzed; however, no definitive
superiority between the two approaches was established. The results show that
the raw-image model achieved higher performance across benchmarks, while the
shell-based model demonstrated promising potential for future refinement toward
robust, cross-domain signature verification.

</details>


### [175] [Can Image-To-Video Models Simulate Pedestrian Dynamics?](https://arxiv.org/abs/2510.17731)
*Aaron Appelle,Jerome P. Lynch*

Main category: cs.CV

TL;DR: 基于扩散变换器（DiT）的图像到视频（I2V）模型通过在大型视频数据集上训练，展现出强大的世界建模能力。本研究探索这些模型能否在拥挤公共场景中生成逼真的行人运动模式。


<details>
  <summary>Details</summary>
Motivation: 利用高性能I2V模型的内在世界建模能力，研究其在复杂公共场景中生成真实行人轨迹的潜力。

Method: 从行人轨迹基准中提取关键帧作为条件输入I2V模型，通过行人动力学的定量指标评估轨迹预测性能。

Result: 通过定量评估验证I2V模型在生成逼真行人运动模式方面的表现。

Conclusion: 基于DiT的I2V模型具备生成真实拥挤场景行人运动的能力，为行人轨迹预测提供了新思路。

Abstract: Recent high-performing image-to-video (I2V) models based on variants of the
diffusion transformer (DiT) have displayed remarkable inherent world-modeling
capabilities by virtue of training on large scale video datasets. We
investigate whether these models can generate realistic pedestrian movement
patterns in crowded public scenes. Our framework conditions I2V models on
keyframes extracted from pedestrian trajectory benchmarks, then evaluates their
trajectory prediction performance using quantitative measures of pedestrian
dynamics.

</details>


### [176] [Joint Multi-Condition Representation Modelling via Matrix Factorisation for Visual Place Recognition](https://arxiv.org/abs/2510.17739)
*Timur Ismagilov,Shakaiba Majeed,Michael Milford,Tan Viet Tuyen Nguyen,Sarvapali D. Ramchurn,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 提出一种无需训练、描述符无关的多参考视觉位置识别方法，通过矩阵分解和残差匹配提升定位性能，在多变外观和视角条件下显著优于单参考和现有多参考基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决多参考视觉位置识别中，随着数据多样性和模型复杂度增加带来的计算成本问题，避免训练过程，同时提升在多变外观和视角条件下的定位鲁棒性。

Method: 提出训练自由的描述符无关方法，通过矩阵分解将多个参考描述符联合建模为基表示，实现基于投影的残差匹配，并引入SotonMV多视角基准数据集。

Result: 在多外观数据上，Recall@1提升高达18%，在非结构化数据上获得约5%的性能增益，表现出强大的泛化能力且保持轻量化。

Conclusion: 该方法在多变外观和视角条件下显著优于现有方法，证明了训练自由的多参考描述符融合策略的有效性和实用性。

Abstract: We address multi-reference visual place recognition (VPR), where reference
sets captured under varying conditions are used to improve localisation
performance. While deep learning with large-scale training improves robustness,
increasing data diversity and model complexity incur extensive computational
cost during training and deployment. Descriptor-level fusion via voting or
aggregation avoids training, but often targets multi-sensor setups or relies on
heuristics with limited gains under appearance and viewpoint change. We propose
a training-free, descriptor-agnostic approach that jointly models places using
multiple reference descriptors via matrix decomposition into basis
representations, enabling projection-based residual matching. We also introduce
SotonMV, a structured benchmark for multi-viewpoint VPR. On multi-appearance
data, our method improves Recall@1 by up to ~18% over single-reference and
outperforms multi-reference baselines across appearance and viewpoint changes,
with gains of ~5% on unstructured data, demonstrating strong generalisation
while remaining lightweight.

</details>


### [177] [Towards Explainable Skin Cancer Classification: A Dual-Network Attention Model with Lesion Segmentation and Clinical Metadata Fusion](https://arxiv.org/abs/2510.17773)
*Md. Enamul Atiq,Shaikh Anowarul Fattah*

Main category: cs.CV

TL;DR: 提出了一种基于双编码器注意力的框架，结合分割病灶和临床元数据来提升皮肤病变分类的准确性和可解释性，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 皮肤癌早期检测至关重要，但现有深度学习模型存在"黑箱"问题，且面临类内差异大、类间差异小的挑战，限制了临床信任。

Method: 使用带有双注意力门和空洞空间金字塔池化的Deep-UNet进行病灶分割，分类阶段采用两个DenseNet201编码器（原始图像和分割病灶），通过多头交叉注意力融合特征，并加入基于Transformer的患者元数据模块。

Result: 在HAM10000、ISIC 2018和2019数据集上实现了最先进的分割性能，显著提高了分类准确率和平均AUC，Grad-CAM热图验证了模型基于病灶区域进行预测的可靠性。

Conclusion: 精确的病灶分割、临床数据和注意力融合的结合能够构建更准确、可解释的皮肤癌分类模型。

Abstract: Skin cancer is a life-threatening disease where early detection significantly
improves patient outcomes. Automated diagnosis from dermoscopic images is
challenging due to high intra-class variability and subtle inter-class
differences. Many deep learning models operate as "black boxes," limiting
clinical trust. In this work, we propose a dual-encoder attention-based
framework that leverages both segmented lesions and clinical metadata to
enhance skin lesion classification in terms of both accuracy and
interpretability. A novel Deep-UNet architecture with Dual Attention Gates
(DAG) and Atrous Spatial Pyramid Pooling (ASPP) is first employed to segment
lesions. The classification stage uses two DenseNet201 encoders-one on the
original image and another on the segmented lesion whose features are fused via
multi-head cross-attention. This dual-input design guides the model to focus on
salient pathological regions. In addition, a transformer-based module
incorporates patient metadata (age, sex, lesion site) into the prediction. We
evaluate our approach on the HAM10000 dataset and the ISIC 2018 and 2019
challenges. The proposed method achieves state-of-the-art segmentation
performance and significantly improves classification accuracy and average AUC
compared to baseline models. To validate our model's reliability, we use
Gradient-weighted Class Activation Mapping (Grad-CAM) to generate heatmaps.
These visualizations confirm that our model's predictions are based on the
lesion area, unlike models that rely on spurious background features. These
results demonstrate that integrating precise lesion segmentation and clinical
data with attention-based fusion leads to a more accurate and interpretable
skin cancer classification model.

</details>


### [178] [SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference](https://arxiv.org/abs/2510.17777)
*Samir Khaki,Junxian Guo,Jiaming Tang,Shang Yang,Yukang Chen,Konstantinos N. Plataniotis,Yao Lu,Song Han,Zhijian Liu*

Main category: cs.CV

TL;DR: SparseVILA是一种高效的视觉语言模型推理新范式，通过解耦视觉稀疏性来加速推理，在保持多轮对话能力的同时实现显著速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理高分辨率图像和长视频时，视觉token数量激增导致推理延迟严重，限制了模型的可扩展性。

Method: 采用解耦的视觉稀疏化策略：在预填充阶段剪枝冗余视觉token，在解码阶段仅检索与查询相关的token。基于AWQ优化的推理流水线实现。

Result: 在长上下文视频任务上实现4.0倍预填充加速、2.5倍解码加速和2.6倍端到端加速，同时在文档理解和推理任务上提升准确性。

Conclusion: SparseVILA通过解耦查询无关剪枝和查询感知检索，为高效多模态推理开辟了新方向，提供无需训练、架构无关的加速框架。

Abstract: Vision Language Models (VLMs) have rapidly advanced in integrating visual and
textual reasoning, powering applications across high-resolution image
understanding, long-video analysis, and multi-turn conversation. However, their
scalability remains limited by the growing number of visual tokens that
dominate inference latency. We present SparseVILA, a new paradigm for efficient
VLM inference that decouples visual sparsity across the prefilling and decoding
stages. SparseVILA distributes sparsity across stages by pruning redundant
visual tokens during prefill and retrieving only query-relevant tokens during
decoding. This decoupled design matches leading prefill pruning methods while
preserving multi-turn fidelity by retaining most of the visual cache so that
query-aware tokens can be retrieved at each conversation round. Built on an
AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster
prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end
speedup on long-context video tasks -- while improving accuracy on
document-understanding and reasoning tasks. By decoupling query-agnostic
pruning and query-aware retrieval, SparseVILA establishes a new direction for
efficient multimodal inference, offering a training-free, architecture-agnostic
framework for accelerating large VLMs without sacrificing capability.

</details>


### [179] [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://arxiv.org/abs/2510.17790)
*Yuhao Yang,Zhen Yang,Zi-Yi Dou,Anh Nguyen,Keen You,Omar Attia,Andrew Szot,Michael Feng,Ram Ramrakhya,Alexander Toshev,Chao Huang,Yinfei Yang,Zhe Gan*

Main category: cs.CV

TL;DR: UltraCUA提出了一种混合动作模型，将GUI原始操作与高级程序化工具调用相结合，显著提升了计算机使用代理的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前计算机使用代理仅依赖原始操作（点击、输入、滚动），存在视觉定位精度要求高、执行链长导致级联失败和性能瓶颈的问题，且无法利用丰富的程序化接口。

Method: 采用四步方法：1）自动化工具扩展管道；2）合成数据引擎生成17,000+可验证任务；3）大规模高质量混合动作轨迹收集；4）两阶段训练（监督微调+在线强化学习）。

Result: 在OSWorld上相对基础模型提升22%，步骤减少11%；在WindowsAgentArena上达到21.7%成功率，优于基于Windows数据训练的基线模型。

Conclusion: 混合动作机制是关键创新，能减少错误传播并保持执行效率，为计算机使用代理提供了新的发展方向。

Abstract: Multimodal agents for computer use rely exclusively on primitive actions
(click, type, scroll) that require accurate visual grounding and lengthy
execution chains, leading to cascading failures and performance bottlenecks.
While other agents leverage rich programmatic interfaces (APIs, MCP servers,
tools), computer-use agents (CUAs) remain isolated from these capabilities. We
present UltraCUA, a foundation model that bridges this gap through hybrid
action -- seamlessly integrating GUI primitives with high-level programmatic
tool calls. To achieve this, our approach comprises four key components: (1) an
automated pipeline that scales programmatic tools from software documentation,
open-source repositories, and code generation; (2) a synthetic data engine
producing over 17,000 verifiable tasks spanning real-world computer-use
scenarios; (3) a large-scale high-quality hybrid action trajectory collection
with both low-level GUI actions and high-level programmatic tool calls; and (4)
a two-stage training pipeline combining supervised fine-tuning with online
reinforcement learning, enabling strategic alternation between low-level and
high-level actions. Experiments with our 7B and 32B models demonstrate
substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA
models achieve an average 22% relative improvement over base models, while
being 11% faster in terms of steps. Out-of-domain evaluation on
WindowsAgentArena shows our model reaches 21.7% success rate, outperforming
baselines trained on Windows data. The hybrid action mechanism proves critical,
reducing error propagation while maintaining execution efficiency.

</details>


### [180] [Glyph: Scaling Context Windows via Visual-Text Compression](https://arxiv.org/abs/2510.17800)
*Jiale Cheng,Yusen Liu,Xinyu Zhang,Yulin Fei,Wenyi Hong,Ruiliang Lyu,Weihan Wang,Zhe Su,Xiaotao Gu,Xiao Liu,Yushi Bai,Jie Tang,Hongning Wang,Minlie Huang*

Main category: cs.CV

TL;DR: Glyph是一个通过视觉上下文扩展来解决长文本处理挑战的框架，它将长文本渲染成图像，利用视觉语言模型进行处理，实现3-4倍的token压缩，同时保持与主流LLM相当的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型对长上下文建模需求的增加，扩展到百万token级别会带来极高的计算和内存成本，限制了长上下文LLM的实际应用。

Method: 提出Glyph框架，将长文本渲染为图像，使用视觉语言模型处理，并设计了基于LLM的遗传搜索来优化视觉渲染配置，平衡准确性和压缩率。

Result: 在多个长上下文基准测试中，Glyph实现了3-4倍的token压缩，预填充和解码速度提升约4倍，SFT训练速度提升约2倍。128K上下文的VLM可扩展到处理1M token级别的文本任务。

Conclusion: 视觉上下文扩展是解决长文本处理挑战的有效方法，Glyph框架在保持准确性的同时显著提升了处理效率，并为现实世界多模态任务带来益处。

Abstract: Large language models (LLMs) increasingly rely on long-context modeling for
tasks such as document understanding, code analysis, and multi-step reasoning.
However, scaling context windows to the million-token level brings prohibitive
computational and memory costs, limiting the practicality of long-context LLMs.
In this work, we take a different perspective-visual context scaling-to tackle
this challenge. Instead of extending token-based sequences, we propose Glyph, a
framework that renders long texts into images and processes them with
vision-language models (VLMs). This approach substantially compresses textual
input while preserving semantic information, and we further design an
LLM-driven genetic search to identify optimal visual rendering configurations
for balancing accuracy and compression. Through extensive experiments, we
demonstrate that our method achieves 3-4x token compression while maintaining
accuracy comparable to leading LLMs such as Qwen3-8B on various long-context
benchmarks. This compression also leads to around 4x faster prefilling and
decoding, and approximately 2x faster SFT training. Furthermore, under extreme
compression, a 128K-context VLM could scale to handle 1M-token-level text
tasks. In addition, the rendered text data benefits real-world multimodal
tasks, such as document understanding. Our code and model are released at
https://github.com/thu-coai/Glyph.

</details>


### [181] [ConsistEdit: Highly Consistent and Precise Training-free Visual Editing](https://arxiv.org/abs/2510.17803)
*Zixin Yin,Ling-Hao Chen,Lionel Ni,Xili Dai*

Main category: cs.CV

TL;DR: ConsistEdit是一种针对MM-DiT架构的新型注意力控制方法，通过视觉专用注意力控制、掩码引导预注意力融合和差异化token操作，在图像和视频编辑任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有训练自由注意力控制方法在编辑强度和源一致性之间存在平衡问题，特别是在多轮和视频编辑中视觉错误会累积。大多数方法强制执行全局一致性，限制了细粒度编辑能力。

Method: 基于对MM-DiT注意力机制的深入分析，提出ConsistEdit方法，包含三个关键技术：视觉专用注意力控制、掩码引导预注意力融合、差异化操作query、key和value tokens。

Result: 实验证明ConsistEdit在广泛图像和视频编辑任务中达到最先进性能，支持结构一致和不一致场景，是首个无需手工调整即可在所有推理步骤和注意力层进行编辑的方法。

Conclusion: ConsistEdit显著提升了可靠性和一致性，支持稳健的多轮和多区域编辑，并支持渐进式结构一致性调整，实现了更精细的控制。

Abstract: Recent advances in training-free attention control methods have enabled
flexible and efficient text-guided editing capabilities for existing generation
models. However, current approaches struggle to simultaneously deliver strong
editing strength while preserving consistency with the source. This limitation
becomes particularly critical in multi-round and video editing, where visual
errors can accumulate over time. Moreover, most existing methods enforce global
consistency, which limits their ability to modify individual attributes such as
texture while preserving others, thereby hindering fine-grained editing.
Recently, the architectural shift from U-Net to MM-DiT has brought significant
improvements in generative performance and introduced a novel mechanism for
integrating text and vision modalities. These advancements pave the way for
overcoming challenges that previous methods failed to resolve. Through an
in-depth analysis of MM-DiT, we identify three key insights into its attention
mechanisms. Building on these, we propose ConsistEdit, a novel attention
control method specifically tailored for MM-DiT. ConsistEdit incorporates
vision-only attention control, mask-guided pre-attention fusion, and
differentiated manipulation of the query, key, and value tokens to produce
consistent, prompt-aligned edits. Extensive experiments demonstrate that
ConsistEdit achieves state-of-the-art performance across a wide range of image
and video editing tasks, including both structure-consistent and
structure-inconsistent scenarios. Unlike prior methods, it is the first
approach to perform editing across all inference steps and attention layers
without handcraft, significantly enhancing reliability and consistency, which
enables robust multi-round and multi-region editing. Furthermore, it supports
progressive adjustment of structural consistency, enabling finer control.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [182] [Filtering of Small Components for Isosurface Generation](https://arxiv.org/abs/2510.16684)
*Devin Zhao,Rephael Wenger*

Main category: cs.GR

TL;DR: 该论文研究了如何通过简单的预过滤方法去除等值面中的微小干扰成分，保留主要可视化结构。


<details>
  <summary>Details</summary>
Motivation: 从CT扫描或MRI等扫描数据构建的等值面通常包含大量微小成分，这些成分会干扰可视化效果，且不属于任何几何模型的一部分。

Method: 采用简单的数据预过滤技术来移除等值面中的微小组件，同时不影响构成可视化主体的大型组件。

Result: 通过实验验证了这种过滤方法的有效性，能够有效去除干扰性的微小成分。

Conclusion: 简单的预过滤处理可以有效改善等值面可视化质量，去除不必要的微小干扰成分，同时保留主要结构。

Abstract: Let $f: \mathbb{R}^3 \rightarrow \mathbb{R}$ be a scalar field. An isosurface
is a piecewise linear approximation of a level set $f^{-1}(\sigma)$ for some
$\sigma \in \mathbb{R}$ built from some regular grid sampling of $f$.
Isosurfaces constructed from scanned data such as CT scans or MRIs often
contain extremely small components that distract from the visualization and do
not form part of any geometric model produced from the data. Simple
prefiltering of the data can remove such small components while having no
effect on the large components that form the body of the visualization. We
present experimental results on such filtering.

</details>


### [183] [Shape-aware Inertial Poser: Motion Tracking for Humans with Diverse Shapes Using Sparse Inertial Sensors](https://arxiv.org/abs/2510.17101)
*Lu Yin,Ziying Shi,Yinghao Wu,Xinyu Yi,Feng Xu,Shihui Guo*

Main category: cs.GR

TL;DR: SAIP是首个考虑不同体型的稀疏惯性传感器运动捕捉方法，通过分解传感器测量值中的体型和姿态相关性，解决了现有方法在非标准体型（如儿童）上泛化性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有惯性运动捕捉方法主要基于标准成人体型模板，无法有效处理体型差异较大的个体（如儿童），主要原因是体型变化会导致IMU测量的加速度发生变化。

Method: 1. 训练回归模型将真实身体的IMU加速度转换到模板成人体型；2. 使用现有方法估计模板体型下的全身运动；3. 通过第二个回归模型将关节速度映射回真实身体，并结合体型感知的物理优化策略计算全局运动；4. 引入首个惯性体型估计方案，使用MLP网络建模体型条件下的IMU-姿态相关性。

Result: 方法在包含10名儿童和10名成人（身高110-190cm）的400分钟IMU-运动数据集上验证有效，SAIP能够有效处理不同体型的运动捕捉任务。

Conclusion: SAIP通过考虑体型差异和分解传感器测量值，显著提升了稀疏惯性运动捕捉在不同体型个体上的泛化能力，并提供了首个包含不同体型的数据集。

Abstract: Human motion capture with sparse inertial sensors has gained significant
attention recently. However, existing methods almost exclusively rely on a
template adult body shape to model the training data, which poses challenges
when generalizing to individuals with largely different body shapes (such as a
child). This is primarily due to the variation in IMU-measured acceleration
caused by changes in body shape. To fill this gap, we propose Shape-aware
Inertial Poser (SAIP), the first solution considering body shape differences in
sparse inertial-based motion capture. Specifically, we decompose the sensor
measurements related to shape and pose in order to effectively model their
joint correlations. Firstly, we train a regression model to transfer the
IMU-measured accelerations of a real body to match the template adult body
model, compensating for the shape-related sensor measurements. Then, we can
easily follow the state-of-the-art methods to estimate the full body motions of
the template-shaped body. Finally, we utilize a second regression model to map
the joint velocities back to the real body, combined with a shape-aware
physical optimization strategy to calculate global motions on the subject.
Furthermore, our method relies on body shape awareness, introducing the first
inertial shape estimation scheme. This is accomplished by modeling the
shape-conditioned IMU-pose correlation using an MLP-based network. To validate
the effectiveness of SAIP, we also present the first IMU motion capture dataset
containing individuals of different body sizes. This dataset features 10
children and 10 adults, with heights ranging from 110 cm to 190 cm, and a total
of 400 minutes of paired IMU-Motion samples. Extensive experimental results
demonstrate that SAIP can effectively handle motion capture tasks for diverse
body shapes. The code and dataset are available at
https://github.com/yinlu5942/SAIP.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [184] [Detecting streaks in smart telescopes images with Deep Learning](https://arxiv.org/abs/2510.17540)
*Olivier Parisot,Mahmoud Jaziri*

Main category: astro-ph.IM

TL;DR: 使用深度学习方法来检测天文图像中的卫星条纹，以减轻卫星对天文观测的负面影响。


<details>
  <summary>Details</summary>
Motivation: 卫星在夜空的可见性日益增加，对天文学和天文摄影产生了负面影响，导致图像中出现条纹，需要额外的后处理来减轻影响。

Method: 测试和调整各种深度学习方法，用于检测2022年3月至2023年2月期间使用智能望远镜捕获的原始天文数据中的条纹。

Result: 论文展示了如何应用深度学习技术来检测天文图像中的卫星条纹。

Conclusion: 深度学习方法是有效检测和减轻卫星条纹对天文观测影响的有前景的解决方案。

Abstract: The growing negative impact of the visibility of satellites in the night sky
is influencing the practice of astronomy and astrophotograph, both at the
amateur and professional levels. The presence of these satellites has the
effect of introducing streaks into the images captured during astronomical
observation, requiring the application of additional post processing to
mitigate the undesirable impact, whether for data loss or cosmetic reasons. In
this paper, we show how we test and adapt various Deep Learning approaches to
detect streaks in raw astronomical data captured between March 2022 and
February 2023 with smart telescopes.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [185] [FedPURIN: Programmed Update and Reduced INformation for Sparse Personalized Federated Learning](https://arxiv.org/abs/2510.16065)
*Lunchen Xie,Zehua He,Qingjiang Shi*

Main category: cs.LG

TL;DR: FedPURIN是一个通信高效的个性化联邦学习框架，通过整数编程识别关键参数进行稀疏聚合，显著减少通信负担同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决个性化联邦学习中通信效率低下的问题，现有方法在数据异构性下通信负担过重，阻碍实际部署。

Method: 提出基于整数编程的关键参数识别策略，集成到稀疏聚合方案中，仅传输重要参数来减少通信量。

Result: 在标准图像分类基准测试中，在不同非IID条件下表现出与最先进方法相竞争的性能，同时实现可量化的通信减少。

Conclusion: FedPURIN为通信高效的个性化联邦学习建立了新范式，特别适用于具有异构数据源的边缘智能系统。

Abstract: Personalized Federated Learning (PFL) has emerged as a critical research
frontier addressing data heterogeneity issue across distributed clients. Novel
model architectures and collaboration mechanisms are engineered to accommodate
statistical disparities while producing client-specific models. Parameter
decoupling represents a promising paradigm for maintaining model performance in
PFL frameworks. However, the communication efficiency of many existing methods
remains suboptimal, sustaining substantial communication burdens that impede
practical deployment. To bridge this gap, we propose Federated Learning with
Programmed Update and Reduced INformation (FedPURIN), a novel framework that
strategically identifies critical parameters for transmission through an
integer programming formulation. This mathematically grounded strategy is
seamlessly integrated into a sparse aggregation scheme, achieving a significant
communication reduction while preserving the efficacy. Comprehensive
evaluations on standard image classification benchmarks under varied non-IID
conditions demonstrate competitive performance relative to state-of-the-art
methods, coupled with quantifiable communication reduction through sparse
aggregation. The framework establishes a new paradigm for
communication-efficient PFL, particularly advantageous for edge intelligence
systems operating with heterogeneous data sources.

</details>


### [186] [Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity](https://arxiv.org/abs/2510.16814)
*Simon Jaxy,Anton Theys,Patrick Willett,W. Chris Carleton,Ralf Vandam,Pieter Libin*

Main category: cs.LG

TL;DR: 本文提出了一种基于深度学习的半监督正-无标记学习策略，用于考古预测建模，通过动态伪标记和条件随机场方法解决考古数据中标签稀缺和类别不平衡的问题。


<details>
  <summary>Details</summary>
Motivation: 考古预测建模需要结合已知位置与环境、文化和地理空间变量来预测未发现遗址的位置，但面临正样本稀缺和大多数位置无标签的结构性标签稀缺问题。

Method: 采用半监督正-无标记学习策略，实现为语义分割模型，结合动态伪标记和通过RNN实现的条件随机场来增强标签置信度，在数字高程模型和原始卫星图像两个数据集上进行评估。

Result: 在数字高程模型数据集上，模型性能与最先进的LAMAP方法相当，但获得了更高的Dice分数；在原始卫星图像上，通过分层k折交叉验证评估，模型保持性能并产生更具可解释性的预测表面。

Conclusion: 半监督学习为在大型、稀疏标注的景观中识别未发现遗址提供了一种有前景的方法。

Abstract: Archaeological predictive modelling estimates where undiscovered sites are
likely to occur by combining known locations with environmental, cultural, and
geospatial variables. We address this challenge using a deep learning approach
but must contend with structural label scarcity inherent to archaeology:
positives are rare, and most locations are unlabeled. To address this, we adopt
a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a
semantic segmentation model and evaluated on two datasets covering a
representative range of archaeological periods. Our approach employs dynamic
pseudolabeling, refined with a Conditional Random Field (CRF) implemented via
an RNN, increasing label confidence under severe class imbalance. On a
geospatial dataset derived from a digital elevation model (DEM), our model
performs on par with the state-of-the-art, LAMAP, while achieving higher Dice
scores. On raw satellite imagery, assessed end-to-end with stratified k-fold
cross-validation, it maintains performance and yields predictive surfaces with
improved interpretability. Overall, our results indicate that semi-supervised
learning offers a promising approach to identifying undiscovered sites across
large, sparsely annotated landscapes.

</details>


### [187] [Fly-CL: A Fly-Inspired Framework for Enhancing Efficient Decorrelation and Reduced Training Time in Pre-trained Model-based Continual Representation Learning](https://arxiv.org/abs/2510.16877)
*Heming Zou,Yunliang Zang,Wutong Xu,Xiangyang Ji*

Main category: cs.LG

TL;DR: Fly-CL是一个受果蝇嗅觉电路启发的持续表示学习框架，通过解决相似性匹配中的多重共线性问题，显著减少训练时间，同时达到或超越现有最先进方法的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的持续表示学习方法直接利用预训练特征进行下游任务时，在相似性匹配阶段容易受到多重共线性的影响，而更先进的方法在实时低延迟应用中计算成本过高。

Method: 受果蝇嗅觉电路启发，Fly-CL框架与多种预训练骨干网络兼容，通过渐进式解决多重共线性问题，实现低时间复杂度的有效相似性匹配。

Result: 在不同网络架构和数据场景下的广泛模拟实验验证了Fly-CL的有效性，显著减少训练时间的同时性能达到或超越当前最先进方法。

Conclusion: Fly-CL通过生物启发设计成功解决了持续表示学习中的多重共线性挑战，为实时低延迟应用提供了高效解决方案。

Abstract: Using a nearly-frozen pretrained model, the continual representation learning
paradigm reframes parameter updates as a similarity-matching problem to
mitigate catastrophic forgetting. However, directly leveraging pretrained
features for downstream tasks often suffers from multicollinearity in the
similarity-matching stage, and more advanced methods can be computationally
prohibitive for real-time, low-latency applications. Inspired by the fly
olfactory circuit, we propose Fly-CL, a bio-inspired framework compatible with
a wide range of pretrained backbones. Fly-CL substantially reduces training
time while achieving performance comparable to or exceeding that of current
state-of-the-art methods. We theoretically show how Fly-CL progressively
resolves multicollinearity, enabling more effective similarity matching with
low time complexity. Extensive simulation experiments across diverse network
architectures and data regimes validate Fly-CL's effectiveness in addressing
this challenge through a biologically inspired design. Code is available at
https://github.com/gfyddha/Fly-CL.

</details>


### [188] [Domain Generalizable Continual Learning](https://arxiv.org/abs/2510.16914)
*Hongwei Yan,Guanglong Sun,Zhiqi Kang,Yi Zhong,Liyuan Wang*

Main category: cs.LG

TL;DR: 本文提出了领域泛化持续学习（DGCL）新范式，针对模型在顺序学习多个单领域任务时需同时保持任务性能和跨领域泛化能力的问题，开发了自适应领域变换（DoT）方法。


<details>
  <summary>Details</summary>
Motivation: 现实世界中智能系统需要持续学习新技能并泛化到未见场景，但现有持续学习方法假设训练和测试领域相同，无法应对领域变化的挑战。

Method: 提出DoT方法，受人类大脑分布式+枢纽理论启发，在表示学习中解耦语义和领域相关信息，自适应地跨领域变换任务表示以实现输出对齐。

Result: DoT作为插件策略显著提升了现有持续学习方法在DGCL设置下的性能，验证了其积累领域泛化知识的能力和资源效率。

Conclusion: DoT方法有效解决了DGCL的核心挑战，为动态环境下的智能系统提供了实用的解决方案。

Abstract: To adapt effectively to dynamic real-world environments, intelligent systems
must continually acquire new skills while generalizing them to diverse, unseen
scenarios. Here, we introduce a novel and realistic setting named domain
generalizable continual learning (DGCL): a model learns sequential tasks with
each involving a single domain, aiming to perform well across all encountered
tasks and domains. This setting poses unique challenges in acquiring,
retaining, and leveraging both semantic- and domain-relevant information for
robust generalization. Although state-of-the-art continual learning (CL)
methods have employed pre-trained models (PTMs) to enhance task-specific
generalization, they typically assume identical training and testing domains
for each task and therefore perform poorly in DGCL. To this end, we propose
adaptive Domain Transformation (DoT), an innovative PTMs-based approach
tailored to DGCL. Inspired by the distributed-plus-hub theory of the human
brain, DoT disentangles semantic- and domain-relevant information in
representation learning, and adaptively transforms task representations across
various domains for output alignment, ensuring balanced and generalized
predictions. DoT serves as a plug-in strategy that greatly facilitates
state-of-the-art CL baselines under both full parameter tuning and
parameter-efficient tuning paradigms in DGCL, validated by extensive
experiments. Also, DoT is shown to accumulate domain-generalizable knowledge
from DGCL, and ensure resource efficiency with a lightweight implementation.

</details>


### [189] [Matricial Free Energy as a Gaussianizing Regularizer: Enhancing Autoencoders for Gaussian Code Generation](https://arxiv.org/abs/2510.17120)
*Rishi Sonthalia,Raj Rao Nadakuditi*

Main category: cs.LG

TL;DR: 提出了一种基于矩阵自由能的自动编码器正则化方法，通过优化编码矩阵的奇异值分布来获得高斯化编码


<details>
  <summary>Details</summary>
Motivation: 为了解决自动编码器编码分布的非高斯性问题，提高编码的泛化能力，特别是在欠定逆问题中的应用

Method: 基于矩阵自由能理论定义可微损失函数，通过优化编码矩阵的奇异值分布使其接近高斯随机矩阵的分布，使用标准随机梯度下降进行训练

Result: 实验证明该方法能够产生高斯化编码，在训练集和测试集上都具有良好的泛化性能，并成功应用于欠定逆问题

Conclusion: 矩阵自由能正则化是自动编码器的一种有效正则化方法，能够可靠地产生高斯编码，为欠定逆问题提供了新的解决方案

Abstract: We introduce a novel regularization scheme for autoencoders based on
matricial free energy. Our approach defines a differentiable loss function in
terms of the singular values of the code matrix (code dimension x batch size).
From the standpoint of free probability an d random matrix theory, this loss
achieves its minimum when the singular value distribution of the code matrix
coincides with that of an appropriately sculpted random metric with i.i.d.
Gaussian entries. Empirical simulations demonstrate that minimizing the
negative matricial free energy through standard stochastic gradient-based
training yields Gaussian-like codes that generalize across training and test
sets. Building on this foundation, we propose a matricidal free energy
maximizing autoencoder that reliably produces Gaussian codes and show its
application to underdetermined inverse problems.

</details>


### [190] [Latent Spaces Beyond Synthesis: From GANs to Diffusion Models](https://arxiv.org/abs/2510.17383)
*Ludovica Schaerf*

Main category: cs.LG

TL;DR: 本文分析了生成视觉模型中内部表征的演变，从GANs和VAEs到扩散模型的转变，提出了"严格意义上的合成"与"广义合成"的区分，探讨扩散模型如何通过分层表征挑战统一内部空间的假设。


<details>
  <summary>Details</summary>
Motivation: 研究生成视觉模型中内部表征的演变，特别是扩散模型如何改变我们对合成过程的理解，挑战传统的统一潜在空间假设。

Method: 通过模型架构的详细分析和有针对性的实验设置，干预分层表征，研究扩散模型中的表征分布。

Result: 发现扩散模型将表征负担分散到不同层次，从而质疑了统一内部空间的假设。

Conclusion: 生成式AI应被理解为专业过程的涌现配置，而非内容的直接合成，需要重新定位对生成AI的理解框架。

Abstract: This paper examines the evolving nature of internal representations in
generative visual models, focusing on the conceptual and technical shift from
GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's
account of synthesis as the amalgamation of distributed representations, we
propose a distinction between "synthesis in a strict sense", where a compact
latent space wholly determines the generative process, and "synthesis in a
broad sense," which characterizes models whose representational labor is
distributed across layers. Through close readings of model architectures and a
targeted experimental setup that intervenes in layerwise representations, we
show how diffusion models fragment the burden of representation and thereby
challenge assumptions of unified internal space. By situating these findings
within media theoretical frameworks and critically engaging with metaphors such
as the latent space and the Platonic Representation Hypothesis, we argue for a
reorientation of how generative AI is understood: not as a direct synthesis of
content, but as an emergent configuration of specialized processes.

</details>


### [191] [MILES: Modality-Informed Learning Rate Scheduler for Balancing Multimodal Learning](https://arxiv.org/abs/2510.17394)
*Alejandro Guerra-Manzanares,Farah E. Shamout*

Main category: cs.LG

TL;DR: 提出了MILES方法，通过动态调整学习率来平衡多模态学习，解决模态过拟合问题，提高多模态和单模态预测性能


<details>
  <summary>Details</summary>
Motivation: 多模态神经网络训练中存在模态过拟合问题，导致网络过度依赖某个模态，限制了多模态学习的潜力，性能提升有限

Method: MILES（Modality-Informed Learning ratE Scheduler）利用训练过程中模态条件利用率的差异动态调整学习率，平衡各模态的学习速度

Result: 在四个多模态联合融合任务上评估，MILES优于7个最先进基线方法，有效平衡模态使用，提高多模态性能并增强模态编码器

Conclusion: 平衡多模态学习对提升模型性能具有重要影响，MILES方法能够有效解决模态过拟合问题

Abstract: The aim of multimodal neural networks is to combine diverse data sources,
referred to as modalities, to achieve enhanced performance compared to relying
on a single modality. However, training of multimodal networks is typically
hindered by modality overfitting, where the network relies excessively on one
of the available modalities. This often yields sub-optimal performance,
hindering the potential of multimodal learning and resulting in marginal
improvements relative to unimodal models. In this work, we present the
Modality-Informed Learning ratE Scheduler (MILES) for training multimodal joint
fusion models in a balanced manner. MILES leverages the differences in
modality-wise conditional utilization rates during training to effectively
balance multimodal learning. The learning rate is dynamically adjusted during
training to balance the speed of learning from each modality by the multimodal
model, aiming for enhanced performance in both multimodal and unimodal
predictions. We extensively evaluate MILES on four multimodal joint fusion
tasks and compare its performance to seven state-of-the-art baselines. Our
results show that MILES outperforms all baselines across all tasks and fusion
methods considered in our study, effectively balancing modality usage during
training. This results in improved multimodal performance and stronger modality
encoders, which can be leveraged when dealing with unimodal samples or absent
modalities. Overall, our work highlights the impact of balancing multimodal
learning on improving model performance.

</details>


### [192] [ZACH-ViT: A Zero-Token Vision Transformer with ShuffleStrides Data Augmentation for Robust Lung Ultrasound Classification](https://arxiv.org/abs/2510.17650)
*Athanasios Angelakis,Amne Mousa,Micah L. A. Heldeweg,Laurens A. Biesheuvel,Mark A. Haaksma,Jasper M. Smit,Pieter R. Tuinman,Paul W. G. Elbers*

Main category: cs.LG

TL;DR: ZACH-ViT是一种0.25M参数的视觉Transformer变体，用于区分心源性肺水肿和非心源性肺部疾病，在LUS视频分类中表现出色，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于非心源性炎症模式、间质性肺病和健康肺部的高视觉变异性，区分心源性肺水肿和非心源性肺部在肺部超声视频中具有挑战性，现有自动化分类方法难以处理这种异质性。

Method: 提出ZACH-ViT模型，移除位置嵌入和[CLS]标记，使其完全置换不变；引入ShuffleStrides数据增强技术，置换探头视图序列和帧顺序；在380个LUS视频上评估。

Result: ZACH-ViT在验证和测试集上获得最高ROC-AUC（0.80和0.79），平衡灵敏度（0.60）和特异性（0.91），而竞争模型均失效；训练速度比Minimal ViT快1.35倍，参数减少2.5倍。

Conclusion: 将架构设计与数据结构对齐可以在小数据医学成像中超越规模效应，支持实时临床部署。

Abstract: Differentiating cardiogenic pulmonary oedema (CPE) from non-cardiogenic and
structurally normal lungs in lung ultrasound (LUS) videos remains challenging
due to the high visual variability of non-cardiogenic inflammatory patterns
(NCIP/ARDS-like), interstitial lung disease, and healthy lungs. This
heterogeneity complicates automated classification as overlapping B-lines and
pleural artefacts are common. We introduce ZACH-ViT (Zero-token Adaptive
Compact Hierarchical Vision Transformer), a 0.25 M-parameter Vision Transformer
variant that removes both positional embeddings and the [CLS] token, making it
fully permutation-invariant and suitable for unordered medical image data. To
enhance generalization, we propose ShuffleStrides Data Augmentation (SSDA),
which permutes probe-view sequences and frame orders while preserving
anatomical validity. ZACH-ViT was evaluated on 380 LUS videos from 95
critically ill patients against nine state-of-the-art baselines. Despite the
heterogeneity of the non-cardiogenic group, ZACH-ViT achieved the highest
validation and test ROC-AUC (0.80 and 0.79) with balanced sensitivity (0.60)
and specificity (0.91), while all competing models collapsed to trivial
classification. It trains 1.35x faster than Minimal ViT (0.62M parameters) with
2.5x fewer parameters, supporting real-time clinical deployment. These results
show that aligning architectural design with data structure can outperform
scale in small-data medical imaging.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [193] [ISO/IEC-Compliant Match-on-Card Face Verification with Short Binary Templates](https://arxiv.org/abs/2510.16078)
*Abdelilah Ganmati,Karim Afdel,Lahcen Koutti*

Main category: cs.CR

TL;DR: 提出了一种实用的卡上匹配人脸验证设计，使用64/128位紧凑模板，通过PCA-ITQ离线生成模板，在卡上通过恒定时间汉明距离进行比对，满足ISO/IEC传输约束和隐私目标。


<details>
  <summary>Details</summary>
Motivation: 为了解决人脸验证系统中模板安全性和隐私保护问题，同时满足ISO/IEC标准约束，设计一种能够在智能卡上安全高效进行人脸验证的方案。

Method: 使用PCA-ITQ方法生成64/128位紧凑二进制模板，在卡上通过恒定时间汉明距离进行比对，采用ISO/IEC 7816-4和14443-4命令APDU，固定长度载荷和仅决策状态字。

Result: 在CelebA数据集上，64位和128位模板在FAR=1%时达到TPR=0.836，验证时间在9.6kbps下分别为43.9ms和52.3ms，在38.4kbps下均小于14ms。128位模板相比64位有更低的EER。

Conclusion: 短二进制模板、固定载荷仅决策APDU和恒定时间匹配能够满足ISO/IEC传输约束，具有宽裕的时间余量，并符合ISO/IEC 24745隐私目标。需要进一步在多数据集和硬件层面验证。

Abstract: We present a practical match-on-card design for face verification in which
compact 64/128-bit templates are produced off-card by PCA-ITQ and compared
on-card via constant-time Hamming distance. We specify ISO/IEC 7816-4 and
14443-4 command APDUs with fixed-length payloads and decision-only status words
(no score leakage), together with a minimal per-identity EEPROM map. Using real
binary codes from a CelebA working set (55 identities, 412 images), we (i)
derive operating thresholds from ROC/DET, (ii) replay enroll->verify
transactions at those thresholds, and (iii) bound end-to-end time by pure link
latency plus a small constant on-card budget. Even at the slowest contact rate
(9.6 kbps), total verification time is 43.9 ms (64 b) and 52.3 ms (128 b); at
38.4 kbps both are <14 ms. At FAR = 1%, both code lengths reach TPR = 0.836,
while 128 b lowers EER relative to 64 b. An optional +6 B helper (targeted
symbol-level parity over empirically unstable bits) is latency-negligible.
Overall, short binary templates, fixed-payload decision-only APDUs, and
constant-time matching satisfy ISO/IEC transport constraints with wide timing
margin and align with ISO/IEC 24745 privacy goals. Limitations: single-dataset
evaluation and design-level (pre-hardware) timing; we outline AgeDB/CFP-FP and
on-card microbenchmarks as next steps.

</details>


### [194] [Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries](https://arxiv.org/abs/2510.16581)
*Xinfeng Li,Shengyuan Pang,Jialin Wu,Jiangyi Deng,Huanlong Zhong,Yanjiao Chen,Jie Zhang,Wenyuan Xu*

Main category: cs.CR

TL;DR: Patronus是一个防御框架，保护文本到图像模型免受白盒攻击者的恶意微调攻击，通过内部调节器和非可微调学习机制确保模型安全。


<details>
  <summary>Details</summary>
Motivation: 现有的安全措施（如内容审核或模型对齐）在白盒攻击者面前失效，这些攻击者知道并可以调整模型参数（如通过微调）来生成不安全图像。

Method: 设计了一个内部调节器，将不安全输入特征解码为零向量，同时保持良性输入特征的解码性能；并采用非可微调学习机制加强模型对齐，防止恶意微调。

Result: 实验验证了Patronus在安全内容生成上的性能完整性，以及拒绝不安全内容生成的有效性，同时确认了其对各种白盒攻击者微调攻击的韧性。

Conclusion: Patronus框架为文本到图像模型提供了全面的保护，有效抵御白盒攻击者的恶意微调，确保模型安全性和性能。

Abstract: Text-to-image (T2I) models, though exhibiting remarkable creativity in image
generation, can be exploited to produce unsafe images. Existing safety
measures, e.g., content moderation or model alignment, fail in the presence of
white-box adversaries who know and can adjust model parameters, e.g., by
fine-tuning. This paper presents a novel defensive framework, named Patronus,
which equips T2I models with holistic protection to defend against white-box
adversaries. Specifically, we design an internal moderator that decodes unsafe
input features into zero vectors while ensuring the decoding performance of
benign input features. Furthermore, we strengthen the model alignment with a
carefully designed non-fine-tunable learning mechanism, ensuring the T2I model
will not be compromised by malicious fine-tuning. We conduct extensive
experiments to validate the intactness of the performance on safe content
generation and the effectiveness of rejecting unsafe content generation.
Results also confirm the resilience of Patronus against various fine-tuning
attacks by white-box adversaries.

</details>


### [195] [VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models](https://arxiv.org/abs/2510.17759)
*Qilin Liao,Anamika Lochab,Ruqi Zhang*

Main category: cs.CR

TL;DR: VERA-V是一个基于变分推理的多模态越狱发现框架，通过联合学习文本-图像提示的后验分布，生成隐蔽的对抗性输入来绕过视觉语言模型的安全防护。


<details>
  <summary>Details</summary>
Motivation: 现有多模态红队方法依赖脆弱模板、关注单攻击场景，且只能暴露有限的漏洞，需要更全面的越狱发现方法。

Method: 使用变分推理框架学习文本-图像提示的联合后验分布，集成三种策略：基于排版的文本提示、基于扩散的图像合成和结构化干扰物来分散VLM注意力。

Result: 在HarmBench和HADES基准测试中，VERA-V在开源和前沿VLM上均优于现有最佳基线，在GPT-4o上攻击成功率比最佳基线高出53.75%。

Conclusion: VERA-V提供了一种概率化的多模态越狱发现方法，能够高效生成多样化越狱样本，并为模型漏洞提供分布性洞察。

Abstract: Vision-Language Models (VLMs) extend large language models with visual
reasoning, but their multimodal design also introduces new, underexplored
vulnerabilities. Existing multimodal red-teaming methods largely rely on
brittle templates, focus on single-attack settings, and expose only a narrow
subset of vulnerabilities. To address these limitations, we introduce VERA-V, a
variational inference framework that recasts multimodal jailbreak discovery as
learning a joint posterior distribution over paired text-image prompts. This
probabilistic view enables the generation of stealthy, coupled adversarial
inputs that bypass model guardrails. We train a lightweight attacker to
approximate the posterior, allowing efficient sampling of diverse jailbreaks
and providing distributional insights into vulnerabilities. VERA-V further
integrates three complementary strategies: (i) typography-based text prompts
that embed harmful cues, (ii) diffusion-based image synthesis that introduces
adversarial signals, and (iii) structured distractors to fragment VLM
attention. Experiments on HarmBench and HADES benchmarks show that VERA-V
consistently outperforms state-of-the-art baselines on both open-source and
frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the
best baseline on GPT-4o.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [196] [Lung Cancer Classification from CT Images Using ResNet](https://arxiv.org/abs/2510.16310)
*Olajumoke O. Adekunle,Joseph D. Akinyemi,Khadijat T. Ladoja,Olufade F. W. Onifade*

Main category: eess.IV

TL;DR: 本研究提出了一种基于ResNet50的深度学习新方法，用于CT图像中肺癌亚型的多分类，在LC25000数据集上取得了98.8%的测试准确率。


<details>
  <summary>Details</summary>
Motivation: 现有肺癌分类研究主要关注良恶性二分类，但临床需要更精细的亚型分类。当前自动化系统的预测效果尚未达到临床采用标准。

Method: 使用预训练的ResNet50模型，在LC25000数据集（15,000张肺部CT图像）上进行训练。添加自定义层并进行超参数微调，将图像分为三类（两类恶性，一类良性）。

Result: 在测试集（2,250张图像）上达到98.8%的准确率，显著优于先前模型在同一数据集上的表现。

Conclusion: 该方法在肺癌多分类任务中表现出色，为临床肺癌诊断提供了更准确的自动化工具。

Abstract: Lung cancer, a malignancy originating in lung tissues, is commonly diagnosed
and classified using medical imaging techniques, particularly computed
tomography (CT). Despite the integration of machine learning and deep learning
methods, the predictive efficacy of automated systems for lung cancer
classification from CT images remains below the desired threshold for clinical
adoption. Existing research predominantly focuses on binary classification,
distinguishing between malignant and benign lung nodules. In this study, a
novel deep learning-based approach is introduced, aimed at an improved
multi-class classification, discerning various subtypes of lung cancer from CT
images. Leveraging a pre-trained ResNet model, lung tissue images were
classified into three distinct classes, two of which denote malignancy and one
benign. Employing a dataset comprising 15,000 lung CT images sourced from the
LC25000 histopathological images, the ResNet50 model was trained on 10,200
images, validated on 2,550 images, and tested on the remaining 2,250 images.
Through the incorporation of custom layers atop the ResNet architecture and
meticulous hyperparameter fine-tuning, a remarkable test accuracy of 98.8% was
recorded. This represents a notable enhancement over the performance of prior
models on the same dataset.

</details>


### [197] [Time-Embedded Algorithm Unrolling for Computational MRI](https://arxiv.org/abs/2510.16321)
*Junno Yun,Yaşar Utku Alçalar,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 本文提出了一种时间嵌入算法展开方案，通过引入时间相关的可学习参数来改进MRI重建中的逆问题求解，在保持参数效率的同时提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统的算法展开方法在MRI重建中共享跨迭代的近端算子网络，这可能导致伪影或模糊。虽然使用不同的网络可能有益，但会显著增加参数数量并导致过拟合。

Method: 受近似消息传递中变化阈值近端算子和扩散模型中时间嵌入的启发，将迭代相关的近端操作和Onsager校正构建为时间嵌入神经网络，数据保真度操作中的标量权重也设为时间相关可学习参数。

Result: 在fastMRI数据集上的广泛实验表明，该方法有效减少了混叠伪影和噪声放大，达到了最先进的性能，且时间嵌入策略可扩展到现有算法展开方法。

Conclusion: 时间嵌入算法展开方案在不显著增加计算复杂度的前提下提升了重建质量，为逆问题求解提供了有效解决方案。

Abstract: Algorithm unrolling methods have proven powerful for solving the regularized
least squares problem in computational magnetic resonance imaging (MRI). These
approaches unfold an iterative algorithm with a fixed number of iterations,
typically alternating between a neural network-based proximal operator for
regularization, a data fidelity operation and auxiliary updates with learnable
parameters. While the connection to optimization methods dictate that the
proximal operator network should be shared across unrolls, this can introduce
artifacts or blurring. Heuristically, practitioners have shown that using
distinct networks may be beneficial, but this significantly increases the
number of learnable parameters, making it challenging to prevent overfitting.
To address these shortcomings, by taking inspirations from proximal operators
with varying thresholds in approximate message passing (AMP) and the success of
time-embedding in diffusion models, we propose a time-embedded algorithm
unrolling scheme for inverse problems. Specifically, we introduce a novel
perspective on the iteration-dependent proximal operation in vector AMP (VAMP)
and the subsequent Onsager correction in the context of algorithm unrolling,
framing them as a time-embedded neural network. Similarly, the scalar weights
in the data fidelity operation and its associated Onsager correction are cast
as time-dependent learnable parameters. Our extensive experiments on the
fastMRI dataset, spanning various acceleration rates and datasets, demonstrate
that our method effectively reduces aliasing artifacts and mitigates noise
amplification, achieving state-of-the-art performance. Furthermore, we show
that our time-embedding strategy extends to existing algorithm unrolling
approaches, enhancing reconstruction quality without increasing the
computational complexity significantly.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [198] [NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?](https://arxiv.org/abs/2510.16263)
*Jierui Peng,Yanyan Zhang,Yicheng Duan,Tuo Liang,Vipin Chaudhary,Yu Yin*

Main category: cs.RO

TL;DR: NEBULA是一个用于单臂操作的统一评估生态系统，通过细粒度能力测试和系统性压力测试来解决VLA智能体评估中指标粗糙、缺乏技能诊断和鲁棒性测量的问题。


<details>
  <summary>Details</summary>
Motivation: 当前VLA智能体的评估主要依赖粗糙的端任务成功率指标，无法提供精确的技能诊断或测量对现实世界扰动的鲁棒性，且碎片化的数据环境阻碍了可复现研究和通用模型的发展。

Method: 引入NEBULA生态系统，采用双轴评估协议：细粒度能力测试用于精确技能诊断，系统性压力测试用于测量鲁棒性；提供标准化API和大规模聚合数据集以减少碎片化。

Result: 使用NEBULA评估发现，顶级VLA智能体在空间推理和动态适应等关键能力上表现不佳，这些缺陷被传统端任务成功率指标所掩盖。

Conclusion: NEBULA通过同时测量智能体能做什么以及在什么情况下可靠执行，为开发鲁棒、通用的具身智能体提供了实用基础。

Abstract: The evaluation of Vision-Language-Action (VLA) agents is hindered by the
coarse, end-task success metric that fails to provide precise skill diagnosis
or measure robustness to real-world perturbations. This challenge is
exacerbated by a fragmented data landscape that impedes reproducible research
and the development of generalist models. To address these limitations, we
introduce \textbf{NEBULA}, a unified ecosystem for single-arm manipulation that
enables diagnostic and reproducible evaluation. NEBULA features a novel
dual-axis evaluation protocol that combines fine-grained \textit{capability
tests} for precise skill diagnosis with systematic \textit{stress tests} that
measure robustness. A standardized API and a large-scale, aggregated dataset
are provided to reduce fragmentation and support cross-dataset training and
fair comparison. Using NEBULA, we demonstrate that top-performing VLAs struggle
with key capabilities such as spatial reasoning and dynamic adaptation, which
are consistently obscured by conventional end-task success metrics. By
measuring both what an agent can do and when it does so reliably, NEBULA
provides a practical foundation for robust, general-purpose embodied agents.

</details>


### [199] [DINO-CVA: A Multimodal Goal-Conditioned Vision-to-Action Model for Autonomous Catheter Navigation](https://arxiv.org/abs/2510.17038)
*Pedram Fekri,Majid Roshanfar,Samuel Barbeau,Seyedfarzad Famouri,Thomas Looi,Dale Podolsky,Mehrdad Zadeh,Javad Dargahi*

Main category: cs.RO

TL;DR: DINO-CVA是一个多模态目标条件行为克隆框架，用于实现自主导管导航，融合视觉观察和操纵杆运动学，减少对操作者的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有导管介入系统主要依赖手动操作，导致操作者疲劳、辐射暴露增加和结果变异性。需要智能自主系统来改善这些问题。

Method: 提出DINO-CVA框架，将视觉观察和操纵杆运动学融合到联合嵌入空间，通过自回归方式从专家演示中预测动作，并使用目标条件引导导航。

Result: 实验显示DINO-CVA在预测动作方面达到高准确率，与仅基于运动学的基线性能相当，同时将预测基于解剖环境。

Conclusion: 该研究证明了多模态目标条件架构在导管导航中的可行性，是减少操作者依赖、提高导管治疗可靠性的重要一步。

Abstract: Cardiac catheterization remains a cornerstone of minimally invasive
interventions, yet it continues to rely heavily on manual operation. Despite
advances in robotic platforms, existing systems are predominantly follow-leader
in nature, requiring continuous physician input and lacking intelligent
autonomy. This dependency contributes to operator fatigue, more radiation
exposure, and variability in procedural outcomes. This work moves towards
autonomous catheter navigation by introducing DINO-CVA, a multimodal
goal-conditioned behavior cloning framework. The proposed model fuses visual
observations and joystick kinematics into a joint embedding space, enabling
policies that are both vision-aware and kinematic-aware. Actions are predicted
autoregressively from expert demonstrations, with goal conditioning guiding
navigation toward specified destinations. A robotic experimental setup with a
synthetic vascular phantom was designed to collect multimodal datasets and
evaluate performance. Results show that DINO-CVA achieves high accuracy in
predicting actions, matching the performance of a kinematics-only baseline
while additionally grounding predictions in the anatomical environment. These
findings establish the feasibility of multimodal, goal-conditioned
architectures for catheter navigation, representing an important step toward
reducing operator dependency and improving the reliability of catheterbased
therapies.

</details>


### [200] [DiffVLA++: Bridging Cognitive Reasoning and End-to-End Driving through Metric-Guided Alignment](https://arxiv.org/abs/2510.17148)
*Yu Gao,Yiru Wang,Anqing Jiang,Heng Yuwen,Wang Shuo,Sun Hao,Wang Jijun*

Main category: cs.RO

TL;DR: DiffVLA++是一个增强的自动驾驶框架，通过度量引导对齐显式桥接认知推理和端到端规划，在ICCV 2025挑战赛中取得49.12的EPDMS分数。


<details>
  <summary>Details</summary>
Motivation: 传统端到端驾驶模型缺乏世界知识来处理长尾场景，而视觉语言动作模型缺乏3D推理能力导致物理不可行动作。需要结合两者的优势。

Method: 1. 构建直接生成语义基础驾驶轨迹的VLA模块；2. 设计具有密集轨迹词汇表的E2E模块确保物理可行性；3. 引入度量引导的轨迹评分器来对齐VLA和E2E模块的输出。

Result: 在ICCV 2025自动驾驶挑战赛中，DiffVLA++取得了49.12的EPDMS分数。

Conclusion: DiffVLA++成功整合了认知推理和端到端规划的优势，通过度量引导对齐实现了更好的自动驾驶性能。

Abstract: Conventional end-to-end (E2E) driving models are effective at generating
physically plausible trajectories, but often fail to generalize to long-tail
scenarios due to the lack of essential world knowledge to understand and reason
about surrounding environments. In contrast, Vision-Language-Action (VLA)
models leverage world knowledge to handle challenging cases, but their limited
3D reasoning capability can lead to physically infeasible actions. In this work
we introduce DiffVLA++, an enhanced autonomous driving framework that
explicitly bridges cognitive reasoning and E2E planning through metric-guided
alignment. First, we build a VLA module directly generating semantically
grounded driving trajectories. Second, we design an E2E module with a dense
trajectory vocabulary that ensures physical feasibility. Third, and most
critically, we introduce a metric-guided trajectory scorer that guides and
aligns the outputs of the VLA and E2E modules, thereby integrating their
complementary strengths. The experiment on the ICCV 2025 Autonomous Grand
Challenge leaderboard shows that DiffVLA++ achieves EPDMS of 49.12.

</details>


### [201] [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://arxiv.org/abs/2510.17439)
*Zhengshen Zhang,Hao Li,Yalun Dai,Zhengbang Zhu,Lei Zhou,Chenchen Liu,Dong Wang,Francis E. H. Tay,Sijin Chen,Ziwei Liu,Yuxiao Liu,Xinghang Li,Pan Zhou*

Main category: cs.RO

TL;DR: FALCON提出了一种新的范式，通过将丰富的3D空间标记注入动作头来解决现有VLA模型在3D空间推理上的局限性，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于2D编码器的视觉-语言-动作模型存在空间推理差距，限制了泛化能力和适应性。现有的3D集成技术要么需要专用传感器且跨模态迁移能力差，要么缺乏几何信息并损害视觉-语言对齐。

Method: FALCON利用空间基础模型从RGB图像中提取强几何先验，通过空间增强动作头消费空间标记而非将其拼接到视觉-语言主干中，可选地融合深度或姿态信息而无需重新训练。

Result: 在三个仿真基准和十一个真实世界任务中，FALCON实现了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件以及物体尺度和高度变化下保持鲁棒性。

Conclusion: FALCON成功解决了空间表示、模态可迁移性和对齐方面的限制，为3D空间推理提供了一种有效的解决方案。

Abstract: Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.

</details>


### [202] [Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats](https://arxiv.org/abs/2510.17783)
*Simeon Adebola,Chung Min Kim,Justin Kerr,Shuangyu Xie,Prithvi Akella,Jose Luis Susa Rincon,Eugen Solowjow,Ken Goldberg*

Main category: cs.RO

TL;DR: Botany-Bot系统使用立体相机、数字转盘、工业机器人臂和3D分割高斯飞溅模型，为活体植物构建详细的"注释数字孪生"，并通过机器人算法操纵叶片拍摄被遮挡细节的高分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 商业植物表型系统因叶片遮挡无法感知许多植物细节，需要开发能获取植物遮挡部分细节的系统。

Method: 使用两个立体相机、数字转盘、工业机器人臂和3D分割高斯飞溅模型构建系统，开发机器人算法操纵叶片拍摄被遮挡细节的高分辨率图像。

Result: 叶片分割准确率90.8%，叶片检测准确率86.2%，叶片抬起/推动准确率77.9%，叶片正反面细节图像拍摄准确率77.3%。

Conclusion: Botany-Bot系统能够有效构建植物的详细数字孪生，并成功获取被遮挡植物细节的高质量图像，代码和数据集已开源。

Abstract: Commercial plant phenotyping systems using fixed cameras cannot perceive many
plant details due to leaf occlusion. In this paper, we present Botany-Bot, a
system for building detailed "annotated digital twins" of living plants using
two stereo cameras, a digital turntable inside a lightbox, an industrial robot
arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms
for manipulating leaves to take high-resolution indexable images of occluded
details such as stem buds and the underside/topside of leaves. Results from
experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy,
detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and
take detailed overside/underside images with 77.3% accuracy. Code, videos, and
datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [203] [Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts](https://arxiv.org/abs/2510.16342)
*Tong Zhang,Ru Zhang,Jianyi Liu,Zhen Yang,Gongshen Liu*

Main category: cs.AI

TL;DR: SELECT是一种动态锚点选择框架，通过两阶段评估机制自动发现最优锚点进行精确擦除，同时保留相关概念，解决了固定锚点策略导致的概念重新出现和侵蚀问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型的概念擦除方法依赖固定锚点策略，存在概念重新出现和侵蚀等关键问题。通过因果追踪发现擦除对锚点选择具有固有敏感性。

Method: 提出SELECT框架，定义Sibling Exclusive Concepts作为更优锚点类别，采用新颖的两阶段评估机制自动发现最优锚点并进行精确擦除。

Result: 广泛评估表明SELECT作为通用锚点解决方案，不仅高效适应多种擦除框架，而且在关键性能指标上持续优于现有基线，单个概念的锚点挖掘平均仅需4秒。

Conclusion: SELECT框架通过动态锚点选择有效解决了固定锚点策略的局限性，为概念擦除提供了更精确和高效的解决方案。

Abstract: Existing concept erasure methods for text-to-image diffusion models commonly
rely on fixed anchor strategies, which often lead to critical issues such as
concept re-emergence and erosion. To address this, we conduct causal tracing to
reveal the inherent sensitivity of erasure to anchor selection and define
Sibling Exclusive Concepts as a superior class of anchors. Based on this
insight, we propose \textbf{SELECT} (Sibling-Exclusive Evaluation for
Contextual Targeting), a dynamic anchor selection framework designed to
overcome the limitations of fixed anchors. Our framework introduces a novel
two-stage evaluation mechanism that automatically discovers optimal anchors for
precise erasure while identifying critical boundary anchors to preserve related
concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor
solution, not only efficiently adapts to multiple erasure frameworks but also
consistently outperforms existing baselines across key performance metrics,
averaging only 4 seconds for anchor mining of a single concept.

</details>


### [204] [End-to-end Listen, Look, Speak and Act](https://arxiv.org/abs/2510.16756)
*Siyin Wang,Wenyi Yu,Xianzhao Chen,Xiaohai Tian,Jun Zhang,Lu Lu,Chao Zhang*

Main category: cs.AI

TL;DR: ELLSA是首个全双工、端到端的多模态模型，能够同时感知和生成视觉、文本、语音和动作，实现更自然的人机交互。


<details>
  <summary>Details</summary>
Motivation: 人类交互本质上是多模态和全双工的，需要同时感知和生成多种模态信息。现有模型缺乏这种能力，限制了交互的自然性。

Method: 提出SA-MoE架构（自注意力专家混合），将各模态路由到专门专家，通过统一注意力主干进行融合，实现联合多模态感知和并发生成。

Result: 在语音交互和机器人操作基准测试中，ELLSA达到单模态基线水平，同时支持对话动作轮转、缺陷指令拒绝、边说边做等高级交互行为。

Conclusion: ELLSA代表了向更自然通用交互智能迈进的步骤，有助于实现人工通用智能。

Abstract: Human interaction is inherently multimodal and full-duplex: we listen while
watching, speak while acting, and fluidly adapt to turn-taking and
interruptions. Realizing these capabilities is essential for building models
simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act),
which, to our knowledge, is the first full-duplex, end-to-end model that
simultaneously perceives and generates across vision, text, speech, and action
within a single architecture, enabling interaction patterns previously out of
reach, yielding more natural, human-like behaviors. At its core is a novel
SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each
modality to specialized experts and fuses them through a unified attention
backbone. This provides a generalizable solution for joint multimodal
perception and concurrent generation, leveraging strong pre-trained components
while enabling efficient modality integration and mitigating modality
interference. On speech-interaction and robot-manipulation benchmarks, ELLSA
matches modality-specific baselines, while uniquely supporting advanced
multimodal and full-duplex behaviors such as dialogue and action turn-taking,
defective instruction rejection, speaking-while-acting, context-grounded visual
question answering, and action barge-ins. We contend that ELLSA represents a
step toward more natural and general interactive intelligence, contributing to
the broader pursuit of artificial general intelligence. All data, code and
model checkpoints will be released upon acceptance.

</details>


### [205] [MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning](https://arxiv.org/abs/2510.17590)
*Mir Nafis Sharear Shopnil,Sharad Duwal,Abhishek Tyagi,Adiba Mahbub Proma*

Main category: cs.AI

TL;DR: MIRAGE是一个用于多模态信息验证的智能代理框架，通过分解验证流程并集成视觉语言模型与网络检索，在无领域特定训练的情况下实现高效错误信息检测。


<details>
  <summary>Details</summary>
Motivation: 网络平台上的多模态错误信息传播迅速，传统监督检测模型需要领域特定训练数据且难以泛化到不同的操纵策略，手动事实核查能力不足。

Method: MIRAGE框架将多模态验证分解为四个顺序模块：视觉真实性评估、跨模态一致性分析、检索增强的事实核查和校准判断模块，通过智能代理协调视觉语言模型推理与网络检索。

Result: 在MMFakeBench验证集上，MIRAGE达到81.65% F1分数和75.1%准确率，显著优于最强的零样本基线（GPT-4V + MMD-Agent），同时保持较低的假阳性率。测试集结果确认了泛化能力。

Conclusion: 分解的智能代理推理与网络检索相结合，可以在无需领域特定训练的情况下达到监督检测器的性能，为标记数据稀缺的多模态错误信息检测提供了可行方案。

Abstract: Misinformation spreads across web platforms through billions of daily
multimodal posts that combine text and images, overwhelming manual
fact-checking capacity. Supervised detection models require domain-specific
training data and fail to generalize across diverse manipulation tactics. We
present MIRAGE, an inference-time, model-pluggable agentic framework that
decomposes multimodal verification into four sequential modules: visual
veracity assessment detects AI-generated images, cross-modal consistency
analysis identifies out-of-context repurposing, retrieval-augmented factual
checking grounds claims in web evidence through iterative question generation,
and a calibrated judgment module integrates all signals. MIRAGE orchestrates
vision-language model reasoning with targeted web retrieval, outputs structured
and citation-linked rationales. On MMFakeBench validation set (1,000 samples),
MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming
the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65
points while maintaining 34.3% false positive rate versus 97.3% for a
judge-only baseline. Test set results (5,000 samples) confirm generalization
with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification
contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97
points. Our results demonstrate that decomposed agentic reasoning with web
retrieval can match supervised detector performance without domain-specific
training, enabling misinformation detection across modalities where labeled
data remains scarce.

</details>


### [206] [Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs](https://arxiv.org/abs/2510.17771)
*Zhining Liu,Ziyi Chen,Hui Liu,Chen Luo,Xianfeng Tang,Suhang Wang,Joy Zeng,Zhenwei Dai,Zhan Shi,Tianxin Wei,Benoit Dumoulin,Hanghang Tong*

Main category: cs.AI

TL;DR: 论文发现视觉语言模型存在"看到但不相信"的现象，即模型在输出错误答案时实际上已经感知到了正确的视觉证据，但未能有效利用这些证据。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在多模态任务中表现良好，但即使存在正确的视觉证据时仍会失败。本研究旨在系统性地探究这些失败是由于未能感知证据还是未能有效利用证据。

Method: 通过分析层级注意力动态，发现浅层主要关注文本，深层稀疏但可靠地关注局部证据区域。提出一种无需训练的推理时干预方法，通过选择性注意力掩码突出深层证据区域。

Result: 该方法在LLaVA、Qwen、Gemma和InternVL等多个主流VLM家族中一致提高了准确率，证明了模型内部编码了可靠证据但未充分利用。

Conclusion: 研究表明VLMs内部编码了可靠证据但利用不足，通过使这些信号显式化可以弥合感知与推理之间的差距，有助于提升VLM的诊断理解和可靠性。

Abstract: Vision-Language Models (VLMs) achieve strong results on multimodal tasks such
as visual question answering, yet they can still fail even when the correct
visual evidence is present. In this work, we systematically investigate whether
these failures arise from not perceiving the evidence or from not leveraging it
effectively. By examining layer-wise attention dynamics, we find that shallow
layers focus primarily on text, while deeper layers sparsely but reliably
attend to localized evidence regions. Surprisingly, VLMs often perceive the
visual evidence when outputting incorrect answers, a phenomenon we term
``seeing but not believing'' that widely exists in major VLM families. Building
on this, we introduce an inference-time intervention that highlights deep-layer
evidence regions through selective attention-based masking. It requires no
training and consistently improves accuracy across multiple families, including
LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable
evidence internally but under-utilize it, making such signals explicit can
bridge the gap between perception and reasoning, advancing the diagnostic
understanding and reliability of VLMs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [207] [Unlocking Off-the-Grid Sparse Recovery with Unlimited Sensing: Simultaneous Super-Resolution in Time and Amplitude](https://arxiv.org/abs/2510.16948)
*Ruiming Guo,Ayush Bhandari*

Main category: cs.IT

TL;DR: 本文提出了一种基于Unlimited Sensing Framework (USF)的超分辨率方法，通过模数编码增强测量精度，克服传统数字采集中的幅值-时间分辨率限制，在低比特量化下实现幅值和时间的超分辨率恢复。


<details>
  <summary>Details</summary>
Motivation: 传统超分辨率方法忽略了模拟-数字接口的关键挑战，当脉冲信号存在强弱幅值差异时，常规数字采集会导致强分量截断或弱分量被量化噪声淹没。

Method: 利用Unlimited Sensing Framework中的模数编码技术，开发适用于非带限核的新理论结果，并提出鲁棒的离网格稀疏恢复算法。

Result: 数值模拟和硬件实验验证了该方法在低比特量化下的有效性，成功实现了幅值和时间的超分辨率。

Conclusion: 基于USF的模数编码方法能够突破传统超分辨率的基本限制，为实际应用如飞行时间成像提供了有效的解决方案。

Abstract: The recovery of Dirac impulses, or spikes, from filtered measurements is a
classical problem in signal processing. As the spikes lie in the continuous
domain while measurements are discrete, this task is known as super-resolution
or off-the-grid sparse recovery. Despite significant theoretical and
algorithmic advances over the past decade, these developments often overlook
critical challenges at the analog-digital interface. In particular, when spikes
exhibit strong-weak amplitude disparity, conventional digital acquisition may
result in clipping of strong components or loss of weak ones beneath the
quantization noise floor. This motivates a broader perspective:
super-resolution must simultaneously resolve both amplitude and temporal
structure. Under a fixed bit budget, such information loss is unavoidable. In
contrast, the emerging theory and practice of the Unlimited Sensing Framework
(USF) demonstrate that these fundamental limitations can be overcome. Building
on this foundation, we demonstrate that modulo encoding within USF enables
digital super-resolution by enhancing measurement precision, thereby unlocking
temporal super-resolution beyond conventional limits. We develop new
theoretical results that extend to non-bandlimited kernels commonly encountered
in practice and introduce a robust algorithm for off-the-grid sparse recovery.
To demonstrate practical impact, we instantiate our framework in the context of
time-of-flight imaging. Both numerical simulations and hardware experiments
validate the effectiveness of our approach under low-bit quantization, enabling
super-resolution in amplitude and time.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [208] [Taming Modality Entanglement in Continual Audio-Visual Segmentation](https://arxiv.org/abs/2510.17234)
*Yuyang Hong,Qi Yang,Tao Zhang,Zili Wang,Zhaojin Fu,Kun Ding,Bin Fan,Shiming Xiang*

Main category: cs.MM

TL;DR: 该论文提出了一种新的持续音频-视觉分割任务，设计了碰撞驱动的多模态重放框架来解决模态语义漂移和共现混淆问题，在多模态持续学习场景中显著优于单模态方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注粗粒度任务，在细粒度持续学习场景中处理模态纠缠存在局限性。为了填补这一空白，需要解决多模态语义漂移和共现混淆这两个关键挑战。

Method: 提出了碰撞驱动的多模态重放框架，包括多模态样本选择策略来选择模态一致性高的样本进行重放，以及碰撞驱动的样本重放机制来增加易混淆类的重放频率。

Result: 构建了三个音频-视觉增量场景进行验证，综合实验表明该方法在多模态持续学习场景中显著优于单模态持续学习方法。

Conclusion: 该方法成功解决了细粒度多模态持续学习中的关键挑战，为音频-视觉分割任务提供了有效的解决方案，在多模态场景中表现出优越性能。

Abstract: Recently, significant progress has been made in multi-modal continual
learning, aiming to learn new tasks sequentially in multi-modal settings while
preserving performance on previously learned ones. However, existing methods
mainly focus on coarse-grained tasks, with limitations in addressing modality
entanglement in fine-grained continual learning settings. To bridge this gap,
we introduce a novel Continual Audio-Visual Segmentation (CAVS) task, aiming to
continuously segment new classes guided by audio. Through comprehensive
analysis, two critical challenges are identified: 1) multi-modal semantic
drift, where a sounding objects is labeled as background in sequential tasks;
2) co-occurrence confusion, where frequent co-occurring classes tend to be
confused. In this work, a Collision-based Multi-modal Rehearsal (CMR) framework
is designed to address these challenges. Specifically, for multi-modal semantic
drift, a Multi-modal Sample Selection (MSS) strategy is proposed to select
samples with high modal consistency for rehearsal. Meanwhile, for co-occurence
confusion, a Collision-based Sample Rehearsal (CSR) mechanism is designed,
allowing for the increase of rehearsal sample frequency of those confusable
classes during training process. Moreover, we construct three audio-visual
incremental scenarios to verify effectiveness of our method. Comprehensive
experiments demonstrate that our method significantly outperforms single-modal
continual learning methods.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [209] [Conveying Meaning through Gestures: An Investigation into Semantic Co-Speech Gesture Generation](https://arxiv.org/abs/2510.17599)
*Hendric Voss,Lisa Michelle Bohnenkamp,Stefan Kopp*

Main category: cs.HC

TL;DR: 本研究比较了两种语音手势生成框架AQ-GT和AQ-GT-a，发现无语义输入的原始框架在训练领域内概念传达更有效，而语义增强版本在泛化能力上表现更好，但并未提升人类相似度。


<details>
  <summary>Details</summary>
Motivation: 评估语义增强是否能够提升语音手势生成的质量和人类感知效果，探索语义标注与手势生成性能之间的复杂关系。

Method: 使用SAGA空间通信语料库中的句子，通过用户中心评估方法，测试概念识别和人类相似度感知，比较AQ-GT和AQ-GT-a两种框架的表现。

Result: AQ-GT在训练领域内概念传达更有效，AQ-GT-a在形状和大小等新情境中泛化能力更强，参与者认为AQ-GT-a手势更富有表现力但人类相似度无显著提升。

Conclusion: 显式语义增强并不能保证手势生成质量的提升，其效果高度依赖上下文，存在专业化与泛化能力之间的权衡。

Abstract: This study explores two frameworks for co-speech gesture generation, AQ-GT
and its semantically-augmented variant AQ-GT-a, to evaluate their ability to
convey meaning through gestures and how humans perceive the resulting
movements. Using sentences from the SAGA spatial communication corpus,
contextually similar sentences, and novel movement-focused sentences, we
conducted a user-centered evaluation of concept recognition and human-likeness.
Results revealed a nuanced relationship between semantic annotations and
performance. The original AQ-GT framework, lacking explicit semantic input, was
surprisingly more effective at conveying concepts within its training domain.
Conversely, the AQ-GT-a framework demonstrated better generalization,
particularly for representing shape and size in novel contexts. While
participants rated gestures from AQ-GT-a as more expressive and helpful, they
did not perceive them as more human-like. These findings suggest that explicit
semantic enrichment does not guarantee improved gesture generation and that its
effectiveness is highly dependent on the context, indicating a potential
trade-off between specialization and generalization.

</details>


### [210] [ImaGGen: Zero-Shot Generation of Co-Speech Semantic Gestures Grounded in Language and Image Input](https://arxiv.org/abs/2510.17617)
*Hendric Voss,Stefan Kopp*

Main category: cs.HC

TL;DR: 本文提出了一种零样本系统，通过结合语言输入和图像输入来生成具有语义一致性的标志性手势和指示性手势，解决了当前手势生成方法只能产生简单节拍手势的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前的手势生成方法仅限于生成伴随语音节奏的简单节拍手势，无法产生具有语义意义的标志性或指示性手势。这些语义手势不能仅从语言输入中推导，因为语言本身缺乏手势所携带的视觉意义。

Method: 开发了一个零样本系统，整合了图像分析管道（提取物体形状、对称性等关键属性）、语义匹配模块（将视觉细节与口语文本关联）和逆运动学引擎，能够生成标志性、指示性手势并与自然节拍手势结合。

Result: 用户研究表明，在语音表达模糊的场景中，系统生成的手势显著提高了参与者识别物体属性的能力，证实了手势的可解释性和交流价值。

Conclusion: 虽然复杂形状表示仍存在挑战，但研究结果强调了上下文感知语义手势对于创建表达性和协作性虚拟代理的重要性，朝着高效、鲁棒的具身人机交互迈出了重要一步。

Abstract: Human communication combines speech with expressive nonverbal cues such as
hand gestures that serve manifold communicative functions. Yet, current
generative gesture generation approaches are restricted to simple, repetitive
beat gestures that accompany the rhythm of speaking but do not contribute to
communicating semantic meaning. This paper tackles a core challenge in
co-speech gesture synthesis: generating iconic or deictic gestures that are
semantically coherent with a verbal utterance. Such gestures cannot be derived
from language input alone, which inherently lacks the visual meaning that is
often carried autonomously by gestures. We therefore introduce a zero-shot
system that generates gestures from a given language input and additionally is
informed by imagistic input, without manual annotation or human intervention.
Our method integrates an image analysis pipeline that extracts key object
properties such as shape, symmetry, and alignment, together with a semantic
matching module that links these visual details to spoken text. An inverse
kinematics engine then synthesizes iconic and deictic gestures and combines
them with co-generated natural beat gestures for coherent multimodal
communication. A comprehensive user study demonstrates the effectiveness of our
approach. In scenarios where speech alone was ambiguous, gestures generated by
our system significantly improved participants' ability to identify object
properties, confirming their interpretability and communicative value. While
challenges remain in representing complex shapes, our results highlight the
importance of context-aware semantic gestures for creating expressive and
collaborative virtual agents or avatars, marking a substantial step forward
towards efficient and robust, embodied human-agent interaction. More
information and example videos are available here:
https://review-anon-io.github.io/ImaGGen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [211] [From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models](https://arxiv.org/abs/2510.17247)
*Zefan Cai,Haoyi Qiu,Haozhe Zhao,Ke Wan,Jiachen Li,Jiuxiang Gu,Wen Xiao,Nanyun Peng,Junjie Hu*

Main category: cs.CL

TL;DR: VideoBiasEval是一个用于评估视频生成中社会偏见的诊断框架，通过分析偏好数据集、奖励模型和对齐调优模型中的偏见传播，发现对齐调优会强化并稳定社会偏见。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型通过基于人类偏好的对齐调优提高了视觉质量，但可能无意中编码并放大了社会偏见。需要系统追踪这些偏见在整条对齐流水线中的演变。

Method: 提出了VideoBiasEval框架，基于社会偏见分类学，采用基于事件的提示策略分离语义内容和演员属性，并引入多粒度指标评估偏见。

Result: 对齐调优不仅加强了代表性偏见，还使其在时间上稳定，产生更平滑但更刻板的描绘。偏见在人类偏好数据集、奖励模型和对齐调优模型中传播。

Conclusion: 研究结果强调需要在整个对齐过程中进行偏见感知的评估和缓解，以确保公平和社会负责任的视频生成。

Abstract: Recent advances in video diffusion models have significantly enhanced
text-to-video generation, particularly through alignment tuning using reward
models trained on human preferences. While these methods improve visual
quality, they can unintentionally encode and amplify social biases. To
systematically trace how such biases evolve throughout the alignment pipeline,
we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating
social representation in video generation. Grounded in established social bias
taxonomies, VideoBiasEval employs an event-based prompting strategy to
disentangle semantic content (actions and contexts) from actor attributes
(gender and ethnicity). It further introduces multi-granular metrics to
evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,
(3) distributional shifts in social attributes across model variants, and (4)
the temporal persistence of bias within videos. Using this framework, we
conduct the first end-to-end analysis connecting biases in human preference
datasets, their amplification in reward models, and their propagation through
alignment-tuned video diffusion models. Our results reveal that alignment
tuning not only strengthens representational biases but also makes them
temporally stable, producing smoother yet more stereotyped portrayals. These
findings highlight the need for bias-aware evaluation and mitigation throughout
the alignment process to ensure fair and socially responsible video generation.

</details>
