<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 94]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.GR](#cs.GR) [Total: 2]
- [cs.SD](#cs.SD) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [eess.IV](#eess.IV) [Total: 9]
- [eess.SP](#eess.SP) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Privacy Enhancement for Gaze Data Using a Noise-Infused Autoencoder](https://arxiv.org/abs/2508.10918)
*Samantha Aziz,Oleg Komogortsev*

Main category: cs.CV

TL;DR: 提出一种隐私增强机制，通过潜在噪声自编码器保护用户视线数据，防止未经同意的跨会话重识别，同时保持数据的可用性。


<details>
  <summary>Details</summary>
Motivation: 解决视线数据在跨会话使用中的隐私问题，防止用户被重识别，同时确保数据在良性任务中的可用性。

Method: 使用潜在噪声自编码器对视线信号进行处理，平衡隐私与实用性。

Result: 显著降低生物识别的可识别性，同时实用性损失最小，且保留生理上合理的视线模式。

Conclusion: 该机制为视线数据隐私保护提供了实用且有效的解决方案，推动了视线系统隐私保护的发展。

Abstract: We present a privacy-enhancing mechanism for gaze signals using a
latent-noise autoencoder that prevents users from being re-identified across
play sessions without their consent, while retaining the usability of the data
for benign tasks. We evaluate privacy-utility trade-offs across biometric
identification and gaze prediction tasks, showing that our approach
significantly reduces biometric identifiability with minimal utility
degradation. Unlike prior methods in this direction, our framework retains
physiologically plausible gaze patterns suitable for downstream use, which
produces favorable privacy-utility trade-off. This work advances privacy in
gaze-based systems by providing a usable and effective mechanism for protecting
sensitive gaze data.

</details>


### [2] [A Survey on Video Temporal Grounding with Multimodal Large Language Model](https://arxiv.org/abs/2508.10922)
*Jianlong Wu,Wei Liu,Ye Liu,Meng Liu,Liqiang Nie,Zhouchen Lin,Chang Wen Chen*

Main category: cs.CV

TL;DR: 综述探讨了基于多模态大语言模型（MLLMs）的视频时序定位（VTG）研究，通过三维分类法系统分析了当前进展，并提出了未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补VTG-MLLMs领域缺乏全面综述的空白，推动视频时序定位技术的发展。

Method: 采用三维分类法：1) MLLMs的功能角色；2) 训练范式；3) 视频特征处理技术。

Result: 总结了VTG-MLLMs的架构、训练策略和特征处理技术，并分析了其在零样本、多任务和多域中的表现。

Conclusion: VTG-MLLMs在性能与泛化能力上超越传统方法，但仍存在局限性，需进一步研究。

Abstract: The recent advancement in video temporal grounding (VTG) has significantly
enhanced fine-grained video understanding, primarily driven by multimodal large
language models (MLLMs). With superior multimodal comprehension and reasoning
abilities, VTG approaches based on MLLMs (VTG-MLLMs) are gradually surpassing
traditional fine-tuned methods. They not only achieve competitive performance
but also excel in generalization across zero-shot, multi-task, and multi-domain
settings. Despite extensive surveys on general video-language understanding,
comprehensive reviews specifically addressing VTG-MLLMs remain scarce. To fill
this gap, this survey systematically examines current research on VTG-MLLMs
through a three-dimensional taxonomy: 1) the functional roles of MLLMs,
highlighting their architectural significance; 2) training paradigms, analyzing
strategies for temporal reasoning and task adaptation; and 3) video feature
processing techniques, which determine spatiotemporal representation
effectiveness. We further discuss benchmark datasets, evaluation protocols, and
summarize empirical findings. Finally, we identify existing limitations and
propose promising research directions. For additional resources and details,
readers are encouraged to visit our repository at
https://github.com/ki-lw/Awesome-MLLMs-for-Video-Temporal-Grounding.

</details>


### [3] [VSF: Simple, Efficient, and Effective Negative Guidance in Few-Step Image Generation Models By \underline{V}alue \underline{S}ign \underline{F}lip](https://arxiv.org/abs/2508.10931)
*Wenqi Guo,Shan Du*

Main category: cs.CV

TL;DR: VSF是一种通过翻转负提示的注意力值符号来动态抑制不想要内容的高效方法，适用于少步扩散和流匹配图像生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG、NASA、NAG）在负提示引导方面存在不足，VSF旨在通过简单高效的方式提升负提示的遵循能力。

Method: VSF通过翻转负提示的注意力值符号动态抑制不想要的内容，计算开销小，兼容多种架构（如MMDiT、跨注意力模型）。

Result: 实验表明，VSF在少步模型中显著优于现有方法，在非少步模型中甚至优于CFG，同时保持图像质量。

Conclusion: VSF是一种高效且通用的负提示引导方法，适用于多种生成任务。

Abstract: We introduce Value Sign Flip (VSF), a simple and efficient method for
incorporating negative prompt guidance in few-step diffusion and flow-matching
image generation models. Unlike existing approaches such as classifier-free
guidance (CFG), NASA, and NAG, VSF dynamically suppresses undesired content by
flipping the sign of attention values from negative prompts. Our method
requires only small computational overhead and integrates effectively with
MMDiT-style architectures such as Stable Diffusion 3.5 Turbo, as well as
cross-attention-based models like Wan. We validate VSF on challenging datasets
with complex prompt pairs and demonstrate superior performance in both static
image and video generation tasks. Experimental results show that VSF
significantly improves negative prompt adherence compared to prior methods in
few-step models, and even CFG in non-few-step models, while maintaining
competitive image quality. Code and ComfyUI node are available in
https://github.com/weathon/VSF/tree/main.

</details>


### [4] [Relative Pose Regression with Pose Auto-Encoders: Enhancing Accuracy and Data Efficiency for Retail Applications](https://arxiv.org/abs/2508.10933)
*Yoli Shavit,Yosi Keller*

Main category: cs.CV

TL;DR: 提出了一种基于相机姿态自动编码器（PAE）的相对姿态回归（RPR）方法，用于提升单图像绝对姿态回归（APR）的定位精度，减少数据收集负担。


<details>
  <summary>Details</summary>
Motivation: 现代零售环境中，精确的相机定位对提升客户体验和库存管理至关重要。现有方法虽有效，但需要大量数据支持。

Method: 扩展PAE至RPR任务，提出一种无需额外存储图像或姿态数据的重定位方案，通过PAE-based RPR优化APR预测。

Result: 在室内基准测试中，该方法显著提升了APR定位精度，且仅需30%数据即可达到竞争性能。

Conclusion: PAE-based RPR是一种高效且数据需求低的相机定位优化方法，适用于零售场景。

Abstract: Accurate camera localization is crucial for modern retail environments,
enabling enhanced customer experiences, streamlined inventory management, and
autonomous operations. While Absolute Pose Regression (APR) from a single image
offers a promising solution, approaches that incorporate visual and spatial
scene priors tend to achieve higher accuracy. Camera Pose Auto-Encoders (PAEs)
have recently been introduced to embed such priors into APR. In this work, we
extend PAEs to the task of Relative Pose Regression (RPR) and propose a novel
re-localization scheme that refines APR predictions using PAE-based RPR,
without requiring additional storage of images or pose data. We first introduce
PAE-based RPR and establish its effectiveness by comparing it with image-based
RPR models of equivalent architectures. We then demonstrate that our refinement
strategy, driven by a PAE-based RPR, enhances APR localization accuracy on
indoor benchmarks. Notably, our method is shown to achieve competitive
performance even when trained with only 30% of the data, substantially reducing
the data collection burden for retail deployment. Our code and pre-trained
models are available at: https://github.com/yolish/camera-pose-auto-encoders

</details>


### [5] [ViPE: Video Pose Engine for 3D Geometric Perception](https://arxiv.org/abs/2508.10934)
*Jiahui Huang,Qunjie Zhou,Hesam Rabeti,Aleksandr Korovko,Huan Ling,Xuanchi Ren,Tianchang Shen,Jun Gao,Dmitry Slepichev,Chen-Hsuan Lin,Jiawei Ren,Kevin Xie,Joydeep Biswas,Laura Leal-Taixe,Sanja Fidler*

Main category: cs.CV

TL;DR: ViPE是一种高效视频处理引擎，用于从无约束视频中估计相机参数和深度图，性能优于现有基线，并开源了大规模标注数据集。


<details>
  <summary>Details</summary>
Motivation: 解决从野外视频中获取一致且精确的3D注释的挑战，以支持空间AI系统的发展。

Method: ViPE通过估计相机内参、相机运动和密集近度量深度图，适用于多种场景和相机模型。

Result: 在TUM/KITTI序列上表现优于现有基线18%/50%，运行速度为3-5FPS，并标注了包含96M帧的大规模数据集。

Conclusion: ViPE及其开源数据集有望加速空间AI系统的开发。

Abstract: Accurate 3D geometric perception is an important prerequisite for a wide
range of spatial AI systems. While state-of-the-art methods depend on
large-scale training data, acquiring consistent and precise 3D annotations from
in-the-wild videos remains a key challenge. In this work, we introduce ViPE, a
handy and versatile video processing engine designed to bridge this gap. ViPE
efficiently estimates camera intrinsics, camera motion, and dense, near-metric
depth maps from unconstrained raw videos. It is robust to diverse scenarios,
including dynamic selfie videos, cinematic shots, or dashcams, and supports
various camera models such as pinhole, wide-angle, and 360{\deg} panoramas. We
have benchmarked ViPE on multiple benchmarks. Notably, it outperforms existing
uncalibrated pose estimation baselines by 18%/50% on TUM/KITTI sequences, and
runs at 3-5FPS on a single GPU for standard input resolutions. We use ViPE to
annotate a large-scale collection of videos. This collection includes around
100K real-world internet videos, 1M high-quality AI-generated videos, and 2K
panoramic videos, totaling approximately 96M frames -- all annotated with
accurate camera poses and dense depth maps. We open-source ViPE and the
annotated dataset with the hope of accelerating the development of spatial AI
systems.

</details>


### [6] [HQ-OV3D: A High Box Quality Open-World 3D Detection Framework based on Diffision Model](https://arxiv.org/abs/2508.10935)
*Qi Liu,Yabei Li,Hongsong Wang,Lei He*

Main category: cs.CV

TL;DR: HQ-OV3D框架通过跨模态几何一致性和基于DDIM的去噪机制，提升了开放词汇3D检测中伪标签的几何质量，显著提高了新类别的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭式3D检测框架无法满足开放世界应用（如自动驾驶）的需求，现有开放词汇3D检测方法在几何质量（尤其是边界框精度）上存在不足。

Method: 提出HQ-OV3D框架，包含两个关键组件：1) 利用跨模态几何一致性的IMCV提案生成器；2) 通过DDIM去噪机制利用标注类别几何先验的ACA去噪器。

Result: 与现有最优方法相比，使用HQ-OV3D生成的伪标签训练，新类别的mAP提高了7.37%。

Conclusion: HQ-OV3D不仅是一个强大的开放词汇3D检测器，还可作为高质量伪标签生成器，适用于现有开放词汇检测或标注流程。

Abstract: Traditional closed-set 3D detection frameworks fail to meet the demands of
open-world applications like autonomous driving. Existing open-vocabulary 3D
detection methods typically adopt a two-stage pipeline consisting of
pseudo-label generation followed by semantic alignment. While vision-language
models (VLMs) recently have dramatically improved the semantic accuracy of
pseudo-labels, their geometric quality, particularly bounding box precision,
remains commonly neglected.To address this issue, we propose a High Box Quality
Open-Vocabulary 3D Detection (HQ-OV3D) framework, dedicated to generate and
refine high-quality pseudo-labels for open-vocabulary classes. The framework
comprises two key components: an Intra-Modality Cross-Validated (IMCV) Proposal
Generator that utilizes cross-modality geometric consistency to generate
high-quality initial 3D proposals, and an Annotated-Class Assisted (ACA)
Denoiser that progressively refines 3D proposals by leveraging geometric priors
from annotated categories through a DDIM-based denoising mechanism.Compared to
the state-of-the-art method, training with pseudo-labels generated by our
approach achieves a 7.37% improvement in mAP on novel classes, demonstrating
the superior quality of the pseudo-labels produced by our framework. HQ-OV3D
can serve not only as a strong standalone open-vocabulary 3D detector but also
as a plug-in high-quality pseudo-label generator for existing open-vocabulary
detection or annotation pipelines.

</details>


### [7] [Vision-Only Gaussian Splatting for Collaborative Semantic Occupancy Prediction](https://arxiv.org/abs/2508.10936)
*Cheng Chen,Hao Huang,Saurabh Bagchi*

Main category: cs.CV

TL;DR: 提出了一种基于稀疏3D语义高斯泼溅的协作3D语义占用预测方法，解决了现有方法的高通信成本或依赖深度估计的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉方法在3D语义占用预测中依赖密集3D体素或2D平面特征，导致高通信成本或需要额外监督，限制了协作场景的适用性。

Method: 通过共享和融合中间高斯基元，实现跨代理融合、几何与语义联合编码，以及稀疏对象中心消息传递。

Result: 在mIoU和IoU指标上分别优于单代理感知和基线协作方法，且在低通信量下仍保持高性能。

Conclusion: 该方法在协作感知中显著提升了性能，同时降低了通信成本，适用于资源受限的场景。

Abstract: Collaborative perception enables connected vehicles to share information,
overcoming occlusions and extending the limited sensing range inherent in
single-agent (non-collaborative) systems. Existing vision-only methods for 3D
semantic occupancy prediction commonly rely on dense 3D voxels, which incur
high communication costs, or 2D planar features, which require accurate depth
estimation or additional supervision, limiting their applicability to
collaborative scenarios. To address these challenges, we propose the first
approach leveraging sparse 3D semantic Gaussian splatting for collaborative 3D
semantic occupancy prediction. By sharing and fusing intermediate Gaussian
primitives, our method provides three benefits: a neighborhood-based
cross-agent fusion that removes duplicates and suppresses noisy or inconsistent
Gaussians; a joint encoding of geometry and semantics in each primitive, which
reduces reliance on depth supervision and allows simple rigid alignment; and
sparse, object-centric messages that preserve structural information while
reducing communication volume. Extensive experiments demonstrate that our
approach outperforms single-agent perception and baseline collaborative methods
by +8.42 and +3.28 points in mIoU, and +5.11 and +22.41 points in IoU,
respectively. When further reducing the number of transmitted Gaussians, our
method still achieves a +1.9 improvement in mIoU, using only 34.6%
communication volume, highlighting robust performance under limited
communication budgets.

</details>


### [8] [Personalized Face Super-Resolution with Identity Decoupling and Fitting](https://arxiv.org/abs/2508.10937)
*Jiarui Yang,Hang Guo,Wen Huang,Tao Dai,Shutao Xia*

Main category: cs.CV

TL;DR: 提出了一种新的面部超分辨率方法IDFSR，通过身份解耦和拟合，在极端退化场景下提升身份恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端退化（如8倍以上缩放）时，难以恢复真实且身份一致的面部图像，常产生幻觉效果。

Method: 采用掩码、参考图像对齐和身份嵌入三个关键设计，预训练扩散模型解耦风格与身份，并微调身份嵌入。

Result: IDFSR在极端退化条件下显著优于现有方法，尤其在身份一致性上表现优异。

Conclusion: IDFSR通过身份解耦和拟合，有效解决了极端退化下的面部超分辨率问题，提升了身份一致性和感知质量。

Abstract: In recent years, face super-resolution (FSR) methods have achieved remarkable
progress, generally maintaining high image fidelity and identity (ID)
consistency under standard settings. However, in extreme degradation scenarios
(e.g., scale $> 8\times$), critical attributes and ID information are often
severely lost in the input image, making it difficult for conventional models
to reconstruct realistic and ID-consistent faces. Existing methods tend to
generate hallucinated faces under such conditions, producing restored images
lacking authentic ID constraints. To address this challenge, we propose a novel
FSR method with Identity Decoupling and Fitting (IDFSR), designed to enhance ID
restoration under large scaling factors while mitigating hallucination effects.
Our approach involves three key designs: 1) \textbf{Masking} the facial region
in the low-resolution (LR) image to eliminate unreliable ID cues; 2)
\textbf{Warping} a reference image to align with the LR input, providing style
guidance; 3) Leveraging \textbf{ID embeddings} extracted from ground truth (GT)
images for fine-grained ID modeling and personalized adaptation. We first
pretrain a diffusion-based model to explicitly decouple style and ID by forcing
it to reconstruct masked LR face regions using both style and identity
embeddings. Subsequently, we freeze most network parameters and perform
lightweight fine-tuning of the ID embedding using a small set of target ID
images. This embedding encodes fine-grained facial attributes and precise ID
information, significantly improving both ID consistency and perceptual
quality. Extensive quantitative evaluations and visual comparisons demonstrate
that the proposed IDFSR substantially outperforms existing approaches under
extreme degradation, particularly achieving superior performance on ID
consistency.

</details>


### [9] [Deep Learning for Automated Identification of Vietnamese Timber Species: A Tool for Ecological Monitoring and Conservation](https://arxiv.org/abs/2508.10938)
*Tianyu Song,Van-Doan Duong,Thi-Phuong Le,Ton Viet Ta*

Main category: cs.CV

TL;DR: 利用深度学习模型（如ShuffleNetV2）实现越南常见木材物种的高精度自动分类，平衡了性能与计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统木材分类方法依赖人工和专家知识，效率低且耗时，需自动化解决方案。

Method: 构建自定义图像数据集，评估五种卷积神经网络架构（ResNet50、EfficientNet等）。

Result: ShuffleNetV2表现最佳，平均准确率99.29%，F1分数99.35%。

Conclusion: 轻量级深度学习模型适用于资源受限环境，为生态信息学提供可扩展的自动化分类方案。

Abstract: Accurate identification of wood species plays a critical role in ecological
monitoring, biodiversity conservation, and sustainable forest management.
Traditional classification approaches relying on macroscopic and microscopic
inspection are labor-intensive and require expert knowledge. In this study, we
explore the application of deep learning to automate the classification of ten
wood species commonly found in Vietnam. A custom image dataset was constructed
from field-collected wood samples, and five state-of-the-art convolutional
neural network architectures--ResNet50, EfficientNet, MobileViT, MobileNetV3,
and ShuffleNetV2--were evaluated. Among these, ShuffleNetV2 achieved the best
balance between classification performance and computational efficiency, with
an average accuracy of 99.29\% and F1-score of 99.35\% over 20 independent
runs. These results demonstrate the potential of lightweight deep learning
models for real-time, high-accuracy species identification in
resource-constrained environments. Our work contributes to the growing field of
ecological informatics by providing scalable, image-based solutions for
automated wood classification and forest biodiversity assessment.

</details>


### [10] [NIRMAL Pooling: An Adaptive Max Pooling Approach with Non-linear Activation for Enhanced Image Classification](https://arxiv.org/abs/2508.10940)
*Nirmal Gaud,Krishna Kumar Jha,Jhimli Adhikari,Adhini Nasarin P S,Joydeep Das,Samarth S Deshpande,Nitasha Barara,Vaduguru Venkata Ramya,Santu Saha,Mehmet Tarik Baran,Sarangi Venkateshwarlu,Anusha M D,Surej Mouli,Preeti Katiyar,Vipin Kumar Chaudhary*

Main category: cs.CV

TL;DR: NIRMAL Pooling是一种新型的CNN池化层，结合自适应最大池化和非线性激活函数，在图像分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统池化方法在复杂数据集上表现有限，NIRMAL Pooling旨在通过动态调整参数和引入非线性激活提升性能。

Method: NIRMAL Pooling结合自适应最大池化和ReLU激活，动态调整参数以适应不同输出维度。

Result: 在MNIST Digits、MNIST Fashion和CIFAR-10数据集上，NIRMAL Pooling的准确率均优于标准Max Pooling。

Conclusion: NIRMAL Pooling为图像识别任务提供了更灵活可靠的池化方法，显著提升CNN性能。

Abstract: This paper presents NIRMAL Pooling, a novel pooling layer for Convolutional
Neural Networks (CNNs) that integrates adaptive max pooling with non-linear
activation function for image classification tasks. The acronym NIRMAL stands
for Non-linear Activation, Intermediate Aggregation, Reduction, Maximum,
Adaptive, and Localized. By dynamically adjusting pooling parameters based on
desired output dimensions and applying a Rectified Linear Unit (ReLU)
activation post-pooling, NIRMAL Pooling improves robustness and feature
expressiveness. We evaluated its performance against standard Max Pooling on
three benchmark datasets: MNIST Digits, MNIST Fashion, and CIFAR-10. NIRMAL
Pooling achieves test accuracies of 99.25% (vs. 99.12% for Max Pooling) on
MNIST Digits, 91.59% (vs. 91.44%) on MNIST Fashion, and 70.49% (vs. 68.87%) on
CIFAR-10, demonstrating consistent improvements, particularly on complex
datasets. This work highlights the potential of NIRMAL Pooling to enhance CNN
performance in diverse image recognition tasks, offering a flexible and
reliable alternative to traditional pooling methods.

</details>


### [11] [Topological Structure Description for Artcode Detection Using the Shape of Orientation Histogram](https://arxiv.org/abs/2508.10942)
*Liming Xu,Dave Towey,Andrew P. French,Steve Benford*

Main category: cs.CV

TL;DR: 论文研究了Artcodes的检测问题，提出了一种新的特征描述符（形状方向直方图）来识别拓扑结构相似的物体，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着智能手机和VR/AR技术的普及，环境中可能出现更多与虚拟元素连接的物体。识别这些物体（如Artcodes）是触发后续交互的第一步。

Method: 提出了一种新的特征描述符（形状方向直方图），用于描述Artcodes的拓扑结构，并构建了检测系统进行实验验证。

Result: 实验结果表明，所提出的特征描述符能有效表示拓扑结构，检测系统在识别Artcodes方面表现良好。

Conclusion: 该研究为拓扑物体检测提供了初步解决方案，有望推动新的交互方式和应用场景。

Abstract: The increasing ubiquity of smartphones and resurgence of VR/AR techniques, it
is expected that our everyday environment may soon be decorating with objects
connecting with virtual elements. Alerting to the presence of these objects is
therefore the first step for motivating follow-up further inspection and
triggering digital material attached to the objects. This work studies a
special kind of these objects -- Artcodes -- a human-meaningful and
machine-readable decorative markers that camouflage themselves with freeform
appearance by encoding information into their topology. We formulate this
problem of recongising the presence of Artcodes as Artcode proposal detection,
a distinct computer vision task that classifies topologically similar but
geometrically and semantically different objects as a same class. To deal with
this problem, we propose a new feature descriptor, called the shape of
orientation histogram, to describe the generic topological structure of an
Artcode. We collect datasets and conduct comprehensive experiments to evaluate
the performance of the Artcode detection proposer built upon this new feature
vector. Our experimental results show the feasibility of the proposed feature
vector for representing topological structures and the effectiveness of the
system for detecting Artcode proposals. Although this work is an initial
attempt to develop a feature-based system for detecting topological objects
like Artcodes, it would open up new interaction opportunities and spark
potential applications of topological object detection.

</details>


### [12] [Analysis of the Compaction Behavior of Textile Reinforcements in Low-Resolution In-Situ CT Scans via Machine-Learning and Descriptor-Based Methods](https://arxiv.org/abs/2508.10943)
*Christian Düreth,Jan Condé-Wolter,Marek Danczak,Karsten Tittmann,Jörn Jaschinski,Andreas Hornig,Maik Gude*

Main category: cs.CV

TL;DR: 该研究提出了一种通过低分辨率CT量化干纺织增强材料中嵌套行为的方法，并利用3D-UNet进行语义分割，分析了空间结构，结果与显微图像验证一致。


<details>
  <summary>Details</summary>
Motivation: 理解纺织增强复合材料的多尺度结构对预测建模至关重要，嵌套行为直接影响其力学性能。

Method: 采用低分辨率CT进行原位压实实验，利用3D-UNet进行语义分割，并通过两点相关函数分析空间结构。

Result: 模型在分割任务中表现优异（IoU为0.822，F1分数为0.902），结果与显微图像验证一致。

Conclusion: 该方法为从工业CT数据中提取关键几何特征提供了可靠途径，并为逆向建模和基于描述符的结构分析奠定了基础。

Abstract: A detailed understanding of material structure across multiple scales is
essential for predictive modeling of textile-reinforced composites. Nesting --
characterized by the interlocking of adjacent fabric layers through local
interpenetration and misalignment of yarns -- plays a critical role in defining
mechanical properties such as stiffness, permeability, and damage tolerance.
This study presents a framework to quantify nesting behavior in dry textile
reinforcements under compaction using low-resolution computed tomography (CT).
In-situ compaction experiments were conducted on various stacking
configurations, with CT scans acquired at 20.22 $\mu$m per voxel resolution. A
tailored 3D{-}UNet enabled semantic segmentation of matrix, weft, and fill
phases across compaction stages corresponding to fiber volume contents of
50--60 %. The model achieved a minimum mean Intersection-over-Union of 0.822
and an $F1$ score of 0.902. Spatial structure was subsequently analyzed using
the two-point correlation function $S_2$, allowing for probabilistic extraction
of average layer thickness and nesting degree. The results show strong
agreement with micrograph-based validation. This methodology provides a robust
approach for extracting key geometrical features from industrially relevant CT
data and establishes a foundation for reverse modeling and descriptor-based
structural analysis of composite preforms.

</details>


### [13] [iWatchRoad: Scalable Detection and Geospatial Visualization of Potholes for Smart Cities](https://arxiv.org/abs/2508.10945)
*Rishi Raj Sahoo,Surbhi Saswati Mohanty,Subhankar Mishra*

Main category: cs.CV

TL;DR: iWatchRoad是一个端到端的自动化系统，用于实时检测、GPS标记和映射道路坑洞，特别适用于印度多样化的道路环境。


<details>
  <summary>Details</summary>
Motivation: 道路坑洞对安全和车辆寿命构成威胁，尤其在印度等道路维护不足的地区。

Method: 利用自标注数据集和YOLO模型进行实时检测，结合OCR模块同步GPS数据，通过OSM可视化。

Result: 系统在复杂条件下提高了检测精度，并生成政府可用的道路评估数据。

Conclusion: iWatchRoad是成本低、硬件高效且可扩展的解决方案，适用于发展中国家的道路管理。

Abstract: Potholes on the roads are a serious hazard and maintenance burden. This poses
a significant threat to road safety and vehicle longevity, especially on the
diverse and under-maintained roads of India. In this paper, we present a
complete end-to-end system called iWatchRoad for automated pothole detection,
Global Positioning System (GPS) tagging, and real time mapping using
OpenStreetMap (OSM). We curated a large, self-annotated dataset of over 7,000
frames captured across various road types, lighting conditions, and weather
scenarios unique to Indian environments, leveraging dashcam footage. This
dataset is used to fine-tune, Ultralytics You Only Look Once (YOLO) model to
perform real time pothole detection, while a custom Optical Character
Recognition (OCR) module was employed to extract timestamps directly from video
frames. The timestamps are synchronized with GPS logs to geotag each detected
potholes accurately. The processed data includes the potholes' details and
frames as metadata is stored in a database and visualized via a user friendly
web interface using OSM. iWatchRoad not only improves detection accuracy under
challenging conditions but also provides government compatible outputs for road
assessment and maintenance planning through the metadata visible on the
website. Our solution is cost effective, hardware efficient, and scalable,
offering a practical tool for urban and rural road management in developing
regions, making the system automated. iWatchRoad is available at
https://smlab.niser.ac.in/project/iwatchroad

</details>


### [14] [IPG: Incremental Patch Generation for Generalized Adversarial Patch Training](https://arxiv.org/abs/2508.10946)
*Wonho Lee,Hyunsik Na,Jisu Lee,Daeseon Choi*

Main category: cs.CV

TL;DR: 本文提出了一种名为增量补丁生成（IPG）的方法，能够高效生成对抗性补丁，比现有方法快11.1倍，同时保持攻击性能。


<details>
  <summary>Details</summary>
Motivation: 对抗性补丁对AI模型的鲁棒性构成挑战，尤其是在计算机视觉任务中。本文旨在解决传统对抗性示例的局限性，提出更高效的补丁生成方法。

Method: 采用增量补丁生成（IPG）方法，通过实验和消融研究（如YOLO特征分布可视化和对抗训练结果）验证其有效性。

Result: IPG生成的补丁能够覆盖更广泛的模型漏洞，且生成的数据集可作为构建鲁棒模型的知识基础。

Conclusion: IPG在对抗性补丁防御和实际应用（如自动驾驶、安全系统和医学影像）中具有潜力，能够提升AI模型在动态高风险环境中的鲁棒性。

Abstract: The advent of adversarial patches poses a significant challenge to the
robustness of AI models, particularly in the domain of computer vision tasks
such as object detection. In contradistinction to traditional adversarial
examples, these patches target specific regions of an image, resulting in the
malfunction of AI models. This paper proposes Incremental Patch Generation
(IPG), a method that generates adversarial patches up to 11.1 times more
efficiently than existing approaches while maintaining comparable attack
performance. The efficacy of IPG is demonstrated by experiments and ablation
studies including YOLO's feature distribution visualization and adversarial
training results, which show that it produces well-generalized patches that
effectively cover a broader range of model vulnerabilities. Furthermore,
IPG-generated datasets can serve as a robust knowledge foundation for
constructing a robust model, enabling structured representation, advanced
reasoning, and proactive defenses in AI security ecosystems. The findings of
this study suggest that IPG has considerable potential for future utilization
not only in adversarial patch defense but also in real-world applications such
as autonomous vehicles, security systems, and medical imaging, where AI models
must remain resilient to adversarial attacks in dynamic and high-stakes
environments.

</details>


### [15] [MedAtlas: Evaluating LLMs for Multi-Round, Multi-Task Medical Reasoning Across Diverse Imaging Modalities and Clinical Text](https://arxiv.org/abs/2508.10947)
*Ronghao Xu,Zhen Huang,Yangbo Wei,Xiaoqian Zhou,Zikang Xu,Ting Liu,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: MedAtlas是一个新的医疗多模态基准框架，旨在评估大语言模型在真实医疗推理任务中的表现，支持多轮对话、多模态图像交互和多任务集成。


<details>
  <summary>Details</summary>
Motivation: 现有医疗多模态基准通常局限于单图像、单轮任务，无法反映临床实践中的纵向和多模态交互特性。

Method: MedAtlas框架包含多轮对话、多模态图像交互、多任务集成和高临床保真度，支持四种核心任务。

Result: 基准测试显示现有多模态模型在多阶段临床推理中存在显著性能差距。

Conclusion: MedAtlas为开发稳健可信的医疗AI提供了一个具有挑战性的评估平台。

Abstract: Artificial intelligence has demonstrated significant potential in clinical
decision-making; however, developing models capable of adapting to diverse
real-world scenarios and performing complex diagnostic reasoning remains a
major challenge. Existing medical multi-modal benchmarks are typically limited
to single-image, single-turn tasks, lacking multi-modal medical image
integration and failing to capture the longitudinal and multi-modal interactive
nature inherent to clinical practice. To address this gap, we introduce
MedAtlas, a novel benchmark framework designed to evaluate large language
models on realistic medical reasoning tasks. MedAtlas is characterized by four
key features: multi-turn dialogue, multi-modal medical image interaction,
multi-task integration, and high clinical fidelity. It supports four core
tasks: open-ended multi-turn question answering, closed-ended multi-turn
question answering, multi-image joint reasoning, and comprehensive disease
diagnosis. Each case is derived from real diagnostic workflows and incorporates
temporal interactions between textual medical histories and multiple imaging
modalities, including CT, MRI, PET, ultrasound, and X-ray, requiring models to
perform deep integrative reasoning across images and clinical texts. MedAtlas
provides expert-annotated gold standards for all tasks. Furthermore, we propose
two novel evaluation metrics: Round Chain Accuracy and Error Propagation
Resistance. Benchmark results with existing multi-modal models reveal
substantial performance gaps in multi-stage clinical reasoning. MedAtlas
establishes a challenging evaluation platform to advance the development of
robust and trustworthy medical AI.

</details>


### [16] [From Promise to Practical Reality: Transforming Diffusion MRI Analysis with Fast Deep Learning Enhancement](https://arxiv.org/abs/2508.10950)
*Xinyi Wang,Michael Barnett,Frederique Boonstra,Yael Barnett,Mariano Cabezas,Arkiev D'Souza,Matthew C. Kiernan,Kain Kyle,Meng Law,Lynette Masters,Zihao Tang,Stephen Tisch,Sicong Tu,Anneke Van Der Walt,Dongang Wang,Fernando Calamante,Weidong Cai,Chenyu Wang*

Main category: cs.CV

TL;DR: FastFOD-Net是一种基于深度学习的FOD增强框架，显著提升了临床扩散MRI数据的分析效率和准确性，适用于多种神经系统疾病研究。


<details>
  <summary>Details</summary>
Motivation: 解决临床扩散MRI数据（单壳低角度分辨率）中FOD估计的可靠性问题，并推动深度学习方法在临床中的广泛应用。

Method: 采用加速端到端深度学习框架FastFOD-Net，优化FOD增强性能，提升训练和推理效率。

Result: FastFOD-Net在健康对照和六种神经系统疾病中表现优异，速度比前代快60倍，降低了样本量需求。

Conclusion: FastFOD-Net有望加速临床神经科学研究，增强扩散MRI分析的可靠性和临床信任。

Abstract: Fiber orientation distribution (FOD) is an advanced diffusion MRI modeling
technique that represents complex white matter fiber configurations, and a key
step for subsequent brain tractography and connectome analysis. Its reliability
and accuracy, however, heavily rely on the quality of the MRI acquisition and
the subsequent estimation of the FODs at each voxel. Generating reliable FODs
from widely available clinical protocols with single-shell and
low-angular-resolution acquisitions remains challenging but could potentially
be addressed with recent advances in deep learning-based enhancement
techniques. Despite advancements, existing methods have predominantly been
assessed on healthy subjects, which have proved to be a major hurdle for their
clinical adoption. In this work, we validate a newly optimized enhancement
framework, FastFOD-Net, across healthy controls and six neurological disorders.
This accelerated end-to-end deep learning framework enhancing FODs with
superior performance and delivering training/inference efficiency for clinical
use ($60\times$ faster comparing to its predecessor). With the most
comprehensive clinical evaluation to date, our work demonstrates the potential
of FastFOD-Net in accelerating clinical neuroscience research, empowering
diffusion MRI analysis for disease differentiation, improving interpretability
in connectome applications, and reducing measurement errors to lower sample
size requirements. Critically, this work will facilitate the more widespread
adoption of, and build clinical trust in, deep learning based methods for
diffusion MRI enhancement. Specifically, FastFOD-Net enables robust analysis of
real-world, clinical diffusion MRI data, comparable to that achievable with
high-quality research acquisitions.

</details>


### [17] [Empowering Multimodal LLMs with External Tools: A Comprehensive Survey](https://arxiv.org/abs/2508.10955)
*Wenbin An,Jiahao Nie,Yaqiang Wu,Feng Tian,Shijian Lu,Qinghua Zheng*

Main category: cs.CV

TL;DR: 综述探讨了如何通过外部工具增强多模态大语言模型（MLLMs）的性能，涵盖数据获取、任务提升、评估改进及未来方向。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在数据质量、复杂任务表现和评估协议上的局限性，借鉴人类利用外部工具的能力。

Method: 通过四个维度分析外部工具的作用：数据获取与标注、任务性能提升、模型评估改进、及未来发展方向。

Result: 外部工具能显著提升MLLMs的性能和可靠性，尤其在数据质量和任务表现上。

Conclusion: 外部工具对MLLMs的发展具有变革潜力，未来需进一步探索其应用和优化。

Abstract: By integrating the perception capabilities of multimodal encoders with the
generative power of Large Language Models (LLMs), Multimodal Large Language
Models (MLLMs), exemplified by GPT-4V, have achieved great success in various
multimodal tasks, pointing toward a promising pathway to artificial general
intelligence. Despite this progress, the limited quality of multimodal data,
poor performance on many complex downstream tasks, and inadequate evaluation
protocols continue to hinder the reliability and broader applicability of MLLMs
across diverse domains. Inspired by the human ability to leverage external
tools for enhanced reasoning and problem-solving, augmenting MLLMs with
external tools (e.g., APIs, expert models, and knowledge bases) offers a
promising strategy to overcome these challenges. In this paper, we present a
comprehensive survey on leveraging external tools to enhance MLLM performance.
Our discussion is structured along four key dimensions about external tools:
(1) how they can facilitate the acquisition and annotation of high-quality
multimodal data; (2) how they can assist in improving MLLM performance on
challenging downstream tasks; (3) how they enable comprehensive and accurate
evaluation of MLLMs; (4) the current limitations and future directions of
tool-augmented MLLMs. Through this survey, we aim to underscore the
transformative potential of external tools in advancing MLLM capabilities,
offering a forward-looking perspective on their development and applications.
The project page of this paper is publicly available
athttps://github.com/Lackel/Awesome-Tools-for-MLLMs.

</details>


### [18] [ORBIT: An Object Property Reasoning Benchmark for Visual Inference Tasks](https://arxiv.org/abs/2508.10956)
*Abhishek Kolari,Mohammadhossein Khojasteh,Yifan Jiang,Floris den Hengst,Filip Ilievski*

Main category: cs.CV

TL;DR: ORBIT是一个多级推理的VQA基准测试，用于评估视觉语言模型在对象属性推理上的表现，结果显示现有模型在复杂推理和真实图像上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在VQA任务中表现优异，但缺乏对对象属性的抽象和推理能力的系统评估。

Method: 提出了一个系统评估框架，包含三种图像类型、三个推理级别和四个对象属性维度，并构建了ORBIT基准测试。

Result: 在零样本设置下，12个最先进的视觉语言模型表现显著低于人类，最佳模型准确率仅为40%。

Conclusion: ORBIT揭示了现有模型在复杂推理和真实图像上的局限性，呼吁开发更强大的推理方法和基准测试。

Abstract: While vision-language models (VLMs) have made remarkable progress on many
popular visual question answering (VQA) benchmarks, it remains unclear whether
they abstract and reason over depicted objects. Inspired by human object
categorisation, object property reasoning involves identifying and recognising
low-level details and higher-level abstractions. While current VQA benchmarks
consider a limited set of object property attributes like size, they typically
blend perception and reasoning, and lack representativeness in terms of
reasoning and image categories. To this end, we introduce a systematic
evaluation framework with images of three representative types, three reasoning
levels of increasing complexity, and four object property dimensions driven by
prior work on commonsense reasoning. We develop a procedure to instantiate this
benchmark into ORBIT, a multi-level reasoning VQA benchmark for object
properties comprising 360 images paired with a total of 1,080 count-based
questions. Experiments with 12 state-of-the-art VLMs in zero-shot settings
reveal significant limitations compared to humans, with the best-performing
model only reaching 40\% accuracy. VLMs struggle particularly with realistic
(photographic) images, counterfactual reasoning about physical and functional
properties, and higher counts. ORBIT points to the need to develop methods for
scalable benchmarking, generalize annotation guidelines, and explore additional
reasoning VLMs. We make the ORBIT benchmark and the experimental code available
to support such endeavors.

</details>


### [19] [CSNR and JMIM Based Spectral Band Selection for Reducing Metamerism in Urban Driving](https://arxiv.org/abs/2508.10962)
*Jiarong Li,Imad Ali Shah,Diarmaid Geever,Fiachra Collins,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 该论文提出了一种基于高光谱成像（HSI）的方法，通过选择信息量最大的波段来增强对易受伤害道路使用者（VRU）的感知能力，显著减少了视觉模糊问题。


<details>
  <summary>Details</summary>
Motivation: 解决RGB图像中因同色异谱现象导致的视觉模糊问题，提升自动驾驶系统对VRU的识别能力。

Method: 结合信息论技术和图像质量指标（对比信噪比）选择最具信息量的HSI波段，并在H-City数据集上验证。

Result: 选定的HSI波段在多种指标上显著优于RGB图像，提升了VRU与背景的区分度。

Conclusion: 该方法为ADAS和自动驾驶系统提供了更鲁棒的感知输入，有助于提高道路安全性。

Abstract: Protecting Vulnerable Road Users (VRU) is a critical safety challenge for
automotive perception systems, particularly under visual ambiguity caused by
metamerism, a phenomenon where distinct materials appear similar in RGB
imagery. This work investigates hyperspectral imaging (HSI) to overcome this
limitation by capturing unique material signatures beyond the visible spectrum,
especially in the Near-Infrared (NIR). To manage the inherent
high-dimensionality of HSI data, we propose a band selection strategy that
integrates information theory techniques (joint mutual information
maximization, correlation analysis) with a novel application of an image
quality metric (contrast signal-to-noise ratio) to identify the most spectrally
informative bands. Using the Hyperspectral City V2 (H-City) dataset, we
identify three informative bands (497 nm, 607 nm, and 895 nm, $\pm$27 nm) and
reconstruct pseudo-color images for comparison with co-registered RGB.
Quantitative results demonstrate increased dissimilarity and perceptual
separability of VRU from the background. The selected HSI bands yield
improvements of 70.24%, 528.46%, 1206.83%, and 246.62% for dissimilarity
(Euclidean, SAM, $T^2$) and perception (CIE $\Delta E$) metrics, consistently
outperforming RGB and confirming a marked reduction in metameric confusion. By
providing a spectrally optimized input, our method enhances VRU separability,
establishing a robust foundation for downstream perception tasks in Advanced
Driver Assistance Systems (ADAS) and Autonomous Driving (AD), ultimately
contributing to improved road safety.

</details>


### [20] [EVCtrl: Efficient Control Adapter for Visual Generation](https://arxiv.org/abs/2508.10963)
*Zixiang Yang,Yue Ma,Yinhan Zhang,Shanhui Mo,Dongrui Liu,Linfeng Zhang*

Main category: cs.CV

TL;DR: EVCtrl是一种轻量级、即插即用的控制适配器，通过时空双缓存策略减少冗余计算，显著提升图像和视频生成效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法如ControlNet在实现精确时空控制时，引入了大量冗余计算和延迟，尤其在视频生成中问题突出。

Method: 提出时空双缓存策略：空间上分区网络为全局和局部功能区域，局部感知缓存减少冗余；时间上选择性跳过不必要的去噪步骤。

Result: 在CogVideo-Controlnet和Wan2.1-Controlnet上分别实现2.16和2.05倍加速，生成质量几乎无损。

Conclusion: EVCtrl无需重新训练模型即可高效实现图像和视频的精确控制生成。

Abstract: Visual generation includes both image and video generation, training
probabilistic models to create coherent, diverse, and semantically faithful
content from scratch. While early research focused on unconditional sampling,
practitioners now demand controllable generation that allows precise
specification of layout, pose, motion, or style. While ControlNet grants
precise spatial-temporal control, its auxiliary branch markedly increases
latency and introduces redundant computation in both uncontrolled regions and
denoising steps, especially for video. To address this problem, we introduce
EVCtrl, a lightweight, plug-and-play control adapter that slashes overhead
without retraining the model. Specifically, we propose a spatio-temporal dual
caching strategy for sparse control information. For spatial redundancy, we
first profile how each layer of DiT-ControlNet responds to fine-grained
control, then partition the network into global and local functional zones. A
locality-aware cache focuses computation on the local zones that truly need the
control signal, skipping the bulk of redundant computation in global regions.
For temporal redundancy, we selectively omit unnecessary denoising steps to
improve efficiency. Extensive experiments on CogVideo-Controlnet,
Wan2.1-Controlnet, and Flux demonstrate that our method is effective in image
and video control generation without the need for training. For example, it
achieves 2.16 and 2.05 times speedups on CogVideo-Controlnet and
Wan2.1-Controlnet, respectively, with almost no degradation in generation
quality.Codes are available in the supplementary materials.

</details>


### [21] [Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision](https://arxiv.org/abs/2508.10972)
*Rosiana Natalie,Wenqian Xu,Ruei-Che Chang,Rada Mihalcea,Anhong Guo*

Main category: cs.CV

TL;DR: 该论文研究了视觉语言模型（VLMs）在模拟低视力人群视觉感知方面的能力，发现结合视觉信息和示例图像响应能显著提高模拟准确性。


<details>
  <summary>Details</summary>
Motivation: 探索VLMs在无障碍领域的模拟能力，填补了此前研究的空白。

Method: 通过调查40名低视力参与者收集数据，构建基准数据集，并用GPT-4o生成模拟代理，评估其与参与者原始回答的一致性。

Result: VLMs在仅提供部分信息时模拟效果较差（一致性0.59），但结合视觉信息和示例图像响应后一致性显著提高（0.70）。

Conclusion: VLMs在模拟低视力人群视觉感知时，需要结合多源信息以提高准确性，单一样例已足够，额外样例效果有限。

Abstract: Advances in vision language models (VLMs) have enabled the simulation of
general human behavior through their reasoning and problem solving
capabilities. However, prior research has not investigated such simulation
capabilities in the accessibility domain. In this paper, we evaluate the extent
to which VLMs can simulate the vision perception of low vision individuals when
interpreting images. We first compile a benchmark dataset through a survey
study with 40 low vision participants, collecting their brief and detailed
vision information and both open-ended and multiple-choice image perception and
recognition responses to up to 25 images. Using these responses, we construct
prompts for VLMs (GPT-4o) to create simulated agents of each participant,
varying the included information on vision information and example image
responses. We evaluate the agreement between VLM-generated responses and
participants' original answers. Our results indicate that VLMs tend to infer
beyond the specified vision ability when given minimal prompts, resulting in
low agreement (0.59). The agreement between the agent' and participants'
responses remains low when only either the vision information (0.59) or example
image responses (0.59) are provided, whereas a combination of both
significantly increase the agreement (0.70, p < 0.0001). Notably, a single
example combining both open-ended and multiple-choice responses, offers
significant performance improvements over either alone (p < 0.0001), while
additional examples provided minimal benefits (p > 0.05).

</details>


### [22] [Are Large Pre-trained Vision Language Models Effective Construction Safety Inspectors?](https://arxiv.org/abs/2508.11011)
*Xuezheng Chen,Zhengbo Zou*

Main category: cs.CV

TL;DR: 提出了ConstructionSite 10k数据集，包含10,000张建筑工地图像，用于评估和微调视觉语言模型（VLMs）在建筑安全检查中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开放的、全面的数据集来评估和微调VLMs在建筑安全检查中的表现，限制了其在实际任务中的适用性。

Method: 构建了一个包含10,000张建筑工地图像的数据集，标注了图像描述、安全规则违反视觉问答（VQA）和建筑元素视觉定位三个任务。

Result: 评估显示现有VLMs在零样本和少样本设置下具有显著的泛化能力，但仍需额外训练以适应实际建筑工地场景。

Conclusion: ConstructionSite 10k为研究人员提供了训练和评估VLMs的基准，推动了建筑安全检查领域的发展。

Abstract: Construction safety inspections typically involve a human inspector
identifying safety concerns on-site. With the rise of powerful Vision Language
Models (VLMs), researchers are exploring their use for tasks such as detecting
safety rule violations from on-site images. However, there is a lack of open
datasets to comprehensively evaluate and further fine-tune VLMs in construction
safety inspection. Current applications of VLMs use small, supervised datasets,
limiting their applicability in tasks they are not directly trained for. In
this paper, we propose the ConstructionSite 10k, featuring 10,000 construction
site images with annotations for three inter-connected tasks, including image
captioning, safety rule violation visual question answering (VQA), and
construction element visual grounding. Our subsequent evaluation of current
state-of-the-art large pre-trained VLMs shows notable generalization abilities
in zero-shot and few-shot settings, while additional training is needed to make
them applicable to actual construction sites. This dataset allows researchers
to train and evaluate their own VLMs with new architectures and techniques,
providing a valuable benchmark for construction safety inspection.

</details>


### [23] [Can Multi-modal (reasoning) LLMs detect document manipulation?](https://arxiv.org/abs/2508.11021)
*Zisheng Liang,Kidus Zewde,Rudra Pratap Singh,Disha Patil,Zexi Chen,Jiayu Xue,Yao Yao,Yifei Chen,Qinzhe Liu,Simiao Ren*

Main category: cs.CV

TL;DR: 研究评估了多模态大语言模型在检测文档欺诈中的表现，发现部分模型在零样本泛化上优于传统方法，但模型大小与准确性相关性有限。


<details>
  <summary>Details</summary>
Motivation: 文档欺诈对依赖安全文档的行业构成威胁，需要有效的检测机制。

Method: 通过提示优化和模型推理过程分析，评估多模态LLMs在识别欺诈指标（如篡改文本、格式不一致等）上的能力。

Result: 表现最佳的多模态LLMs在零样本泛化上优于传统方法，但部分视觉LLMs表现不佳。模型大小与准确性相关性有限。

Conclusion: 多模态LLMs在文档欺诈检测中具有潜力，但需任务特定微调，为未来研究提供了基础。

Abstract: Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

</details>


### [24] [MedSAMix: A Training-Free Model Merging Approach for Medical Image Segmentation](https://arxiv.org/abs/2508.11032)
*Yanwu Yang,Guinan Su,Jiesi Hu,Francesco Sammarco,Jonas Geiping,Thomas Wolfers*

Main category: cs.CV

TL;DR: MedSAMix是一种无需训练的模型融合方法，结合通用模型（如SAM）和专用模型（如MedSAM）的优势，用于医学图像分割。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像分割模型（如MedSAM）因数据有限、异质性和分布偏移等问题，泛化能力受限。

Method: 提出零阶优化方法自动发现最优层融合方案，并开发两种优化策略以满足不同临床需求。

Result: 在25个医学分割任务中，MedSAMix显著提升性能，专用任务和多任务评估分别提升6.67%和4.37%。

Conclusion: MedSAMix有效减少模型偏差，提升领域专用性和泛化能力。

Abstract: Universal medical image segmentation models have emerged as a promising
paradigm due to their strong generalizability across diverse tasks, showing
great potential for a wide range of clinical applications. This potential has
been partly driven by the success of general-purpose vision models such as the
Segment Anything Model (SAM), which has inspired the development of various
fine-tuned variants for medical segmentation tasks. However, fine-tuned
variants like MedSAM are trained on comparatively limited medical imaging data
that often suffers from heterogeneity, scarce annotations, and distributional
shifts. These challenges limit their ability to generalize across a wide range
of medical segmentation tasks. In this regard, we propose MedSAMix, a
training-free model merging method that integrates the strengths of both
generalist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical
image segmentation. In contrast to traditional model merging approaches that
rely on manual configuration and often result in suboptimal outcomes, we
propose a zero-order optimization method to automatically discover optimal
layer-wise merging solutions. Furthermore, for clinical applications, we
develop two regimes to meet the demand of domain-specificity and
generalizability in different scenarios by single-task optimization and
multi-objective optimization respectively. Extensive evaluations on 25 medical
segmentation tasks demonstrate that MedSAMix effectively mitigates model bias
and consistently improves performance in both domain-specific accuracy and
generalization, achieving improvements of 6.67% on specialized tasks and 4.37%
on multi-task evaluations.

</details>


### [25] [Advancing 3D Scene Understanding with MV-ScanQA Multi-View Reasoning Evaluation and TripAlign Pre-training Dataset](https://arxiv.org/abs/2508.11058)
*Wentao Mo,Qingchao Chen,Yuxin Peng,Siyuan Huang,Yang Liu*

Main category: cs.CV

TL;DR: MV-ScanQA和TripAlign数据集解决了现有3D视觉语言学习数据集的局限性，通过多视角推理和丰富的多对象对齐信号，提升了3D场景理解能力。LEGO方法在预训练后表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有3D VL数据集缺乏多视角推理和丰富的上下文对齐，限制了模型对远距离对象的深度理解。

Method: 提出MV-ScanQA数据集和TripAlign预训练语料库，开发LEGO方法将2D LVLMs知识迁移到3D领域。

Result: LEGO在MV-ScanQA和现有3D密集描述和问答基准上达到最先进性能。

Conclusion: MV-ScanQA和TripAlign推动了3D VL学习的发展，LEGO方法展示了多视角推理的潜力。

Abstract: The advancement of 3D vision-language (3D VL) learning is hindered by several
limitations in existing 3D VL datasets: they rarely necessitate reasoning
beyond a close range of objects in single viewpoint, and annotations often link
instructions to single objects, missing richer contextual alignments between
multiple objects. This significantly curtails the development of models capable
of deep, multi-view 3D scene understanding over distant objects. To address
these challenges, we introduce MV-ScanQA, a novel 3D question answering dataset
where 68% of questions explicitly require integrating information from multiple
views (compared to less than 7% in existing datasets), thereby rigorously
testing multi-view compositional reasoning. To facilitate the training of
models for such demanding scenarios, we present TripAlign dataset, a
large-scale and low-cost 2D-3D-language pre-training corpus containing 1M <2D
view, set of 3D objects, text> triplets that explicitly aligns groups of
contextually related objects with text, providing richer, view-grounded
multi-object multimodal alignment signals than previous single-object
annotations. We further develop LEGO, a baseline method for the multi-view
reasoning challenge in MV-ScanQA, transferring knowledge from pre-trained 2D
LVLMs to 3D domain with TripAlign. Empirically, LEGO pre-trained on TripAlign
achieves state-of-the-art performance not only on the proposed MV-ScanQA, but
also on existing benchmarks for 3D dense captioning and question answering.
Datasets and code are available at
https://matthewdm0816.github.io/tripalign-mvscanqa.

</details>


### [26] [Data-Driven Abdominal Phenotypes of Type 2 Diabetes in Lean, Overweight, and Obese Cohorts](https://arxiv.org/abs/2508.11063)
*Lucas W. Remedios,Chloe Choe,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 该研究利用AI分析腹部3D影像，发现不同体重人群中2型糖尿病的腹部特征相似，包括肌肉脂肪化、内脏脂肪增多等。


<details>
  <summary>Details</summary>
Motivation: 尽管BMI是2型糖尿病的已知风险因素，但瘦人和肥胖者的患病差异表明，详细的身体组成可能揭示腹部表型与糖尿病的关系。

Method: 研究通过分割腹部CT扫描，使用随机森林分类和SHAP分析，识别与糖尿病风险相关的特征，并在不同体重亚组中验证。

Result: 随机森林模型的AUC为0.72-0.74，发现肌肉脂肪化、内脏脂肪增多等是跨体重类别的共同糖尿病特征。

Conclusion: 腹部特征驱动的2型糖尿病风险在不同体重类别中可能一致。

Abstract: Purpose: Although elevated BMI is a well-known risk factor for type 2
diabetes, the disease's presence in some lean adults and absence in others with
obesity suggests that detailed body composition may uncover abdominal
phenotypes of type 2 diabetes. With AI, we can now extract detailed
measurements of size, shape, and fat content from abdominal structures in 3D
clinical imaging at scale. This creates an opportunity to empirically define
body composition signatures linked to type 2 diabetes risk and protection using
large-scale clinical data. Approach: To uncover BMI-specific diabetic abdominal
patterns from clinical CT, we applied our design four times: once on the full
cohort (n = 1,728) and once on lean (n = 497), overweight (n = 611), and obese
(n = 620) subgroups separately. Briefly, our experimental design transforms
abdominal scans into collections of explainable measurements through
segmentation, classifies type 2 diabetes through a cross-validated random
forest, measures how features contribute to model-estimated risk or protection
through SHAP analysis, groups scans by shared model decision patterns
(clustering from SHAP) and links back to anatomical differences
(classification). Results: The random-forests achieved mean AUCs of 0.72-0.74.
There were shared type 2 diabetes signatures in each group; fatty skeletal
muscle, older age, greater visceral and subcutaneous fat, and a smaller or
fat-laden pancreas. Univariate logistic regression confirmed the direction of
14-18 of the top 20 predictors within each subgroup (p < 0.05). Conclusions:
Our findings suggest that abdominal drivers of type 2 diabetes may be
consistent across weight classes.

</details>


### [27] [HierOctFusion: Multi-scale Octree-based 3D Shape Generation via Part-Whole-Hierarchy Message Passing](https://arxiv.org/abs/2508.11106)
*Xinjie Gao,Bi'an Du,Wei Hu*

Main category: cs.CV

TL;DR: HierOctFusion提出了一种基于多尺度八叉树的扩散模型，通过分层特征交互和语义部分信息注入，提升了3D内容生成的精细度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法将3D对象视为整体，忽略了语义部分层次结构，且高分辨率建模计算成本高。HierOctFusion利用对象的稀疏性和层次性，提出分层生成方法。

Method: 提出HierOctFusion模型，结合多尺度八叉树扩散和跨注意力机制，注入部分级信息，并构建带部分标注的3D数据集。

Result: 实验表明，HierOctFusion在形状质量和效率上优于现有方法。

Conclusion: HierOctFusion通过分层生成和语义部分信息传播，显著提升了3D内容生成的效果。

Abstract: 3D content generation remains a fundamental yet challenging task due to the
inherent structural complexity of 3D data. While recent octree-based diffusion
models offer a promising balance between efficiency and quality through
hierarchical generation, they often overlook two key insights: 1) existing
methods typically model 3D objects as holistic entities, ignoring their
semantic part hierarchies and limiting generalization; and 2) holistic
high-resolution modeling is computationally expensive, whereas real-world
objects are inherently sparse and hierarchical, making them well-suited for
layered generation. Motivated by these observations, we propose HierOctFusion,
a part-aware multi-scale octree diffusion model that enhances hierarchical
feature interaction for generating fine-grained and sparse object structures.
Furthermore, we introduce a cross-attention conditioning mechanism that injects
part-level information into the generation process, enabling semantic features
to propagate effectively across hierarchical levels from parts to the whole.
Additionally, we construct a 3D dataset with part category annotations using a
pre-trained segmentation model to facilitate training and evaluation.
Experiments demonstrate that HierOctFusion achieves superior shape quality and
efficiency compared to prior methods.

</details>


### [28] [UWB-PostureGuard: A Privacy-Preserving RF Sensing System for Continuous Ergonomic Sitting Posture Monitoring](https://arxiv.org/abs/2508.11115)
*Haotang Li,Zhenyu Qi,Sen He,Kebin Peng,Sheng Tan,Yili Ren,Tomas Cerny,Jiyue Zhao,Zi Wang*

Main category: cs.CV

TL;DR: UWB-PostureGuard是一种基于超宽带（UWB）技术的隐私保护坐姿监测系统，通过非接触式监测改善健康管理。


<details>
  <summary>Details</summary>
Motivation: 长时间使用电脑时的不良坐姿已成为公共健康问题，传统监测方法存在隐私和舒适性问题。

Method: 利用商用UWB设备提取坐姿特征，开发PoseGBDT模型捕捉时间依赖性。

Result: 在10名参与者和19种姿势的测试中，系统达到99.11%的准确率，且对环境变量具有鲁棒性。

Conclusion: 该系统为低成本、可扩展的隐私保护移动健康解决方案，有助于主动改善坐姿。

Abstract: Improper sitting posture during prolonged computer use has become a
significant public health concern. Traditional posture monitoring solutions
face substantial barriers, including privacy concerns with camera-based systems
and user discomfort with wearable sensors. This paper presents
UWB-PostureGuard, a privacy-preserving ultra-wideband (UWB) sensing system that
advances mobile technologies for preventive health management through
continuous, contactless monitoring of ergonomic sitting posture. Our system
leverages commercial UWB devices, utilizing comprehensive feature engineering
to extract multiple ergonomic sitting posture features. We develop PoseGBDT to
effectively capture temporal dependencies in posture patterns, addressing
limitations of traditional frame-wise classification approaches. Extensive
real-world evaluation across 10 participants and 19 distinct postures
demonstrates exceptional performance, achieving 99.11% accuracy while
maintaining robustness against environmental variables such as clothing
thickness, additional devices, and furniture configurations. Our system
provides a scalable, privacy-preserving mobile health solution on existing
platforms for proactive ergonomic management, improving quality of life at low
costs.

</details>


### [29] [Residual-based Efficient Bidirectional Diffusion Model for Image Dehazing and Haze Generation](https://arxiv.org/abs/2508.11134)
*Bing Liu,Le Wang,Hao Liu,Mingming Liu*

Main category: cs.CV

TL;DR: 提出了一种基于残差的高效双向扩散模型（RBDM），实现雾图和去雾图之间的双向转换。


<details>
  <summary>Details</summary>
Motivation: 现有深度去雾方法仅关注去雾，缺乏雾图和去雾图之间的双向转换能力。

Method: 设计了双马尔可夫链以平滑转换残差，通过扰动图像并预测噪声学习条件分布，使用统一评分函数降低计算成本。

Result: RBDM仅需15步采样即可实现雾图和去雾图的无尺寸限制双向转换，性能优于或媲美现有方法。

Conclusion: RBDM在合成和真实数据集上均表现出色，验证了其高效性和双向转换能力。

Abstract: Current deep dehazing methods only focus on removing haze from hazy images,
lacking the capability to translate between hazy and haze-free images. To
address this issue, we propose a residual-based efficient bidirectional
diffusion model (RBDM) that can model the conditional distributions for both
dehazing and haze generation. Firstly, we devise dual Markov chains that can
effectively shift the residuals and facilitate bidirectional smooth transitions
between them. Secondly, the RBDM perturbs the hazy and haze-free images at
individual timesteps and predicts the noise in the perturbed data to
simultaneously learn the conditional distributions. Finally, to enhance
performance on relatively small datasets and reduce computational costs, our
method introduces a unified score function learned on image patches instead of
entire images. Our RBDM successfully implements size-agnostic bidirectional
transitions between haze-free and hazy images with only 15 sampling steps.
Extensive experiments demonstrate that the proposed method achieves superior or
at least comparable performance to state-of-the-art methods on both synthetic
and real-world datasets.

</details>


### [30] [A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations](https://arxiv.org/abs/2508.11141)
*Bin Ma,Yifei Zhang,Yongjin Xian,Qi Li,Linna Zhou,Gongxun Miao*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习的多尺度图像与上下文关联探索算法（MICC），通过跨模态多尺度对齐和尺度感知融合网络，显著提升了谣言检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有谣言检测方法常忽略图像内容及多尺度上下文与图像的关系，导致关键信息丢失。

Method: 设计了SCLIP编码器生成统一语义嵌入，引入跨模态多尺度对齐模块和尺度感知融合网络。

Result: 在两个真实数据集上验证，性能显著优于现有方法。

Conclusion: MICC算法有效且具有实际应用潜力。

Abstract: Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

</details>


### [31] [LEARN: A Story-Driven Layout-to-Image Generation Framework for STEM Instruction](https://arxiv.org/abs/2508.11153)
*Maoquan Zhang,Bisser Raytchev,Xiujuan Sun*

Main category: cs.CV

TL;DR: LEARN是一个布局感知的扩散框架，用于生成STEM教育中教学对齐的插图。


<details>
  <summary>Details</summary>
Motivation: 解决STEM教育中抽象科学概念的视觉表达问题，减少认知负荷，提升学习效果。

Method: 利用BookCover数据集，通过布局条件生成、对比视觉语义训练和提示调制。

Result: 生成连贯的视觉序列，支持中高级推理，减少认知负荷。

Conclusion: LEARN为教育生成AI提供了新方向，未来可集成多模态系统和知识图谱。

Abstract: LEARN is a layout-aware diffusion framework designed to generate
pedagogically aligned illustrations for STEM education. It leverages a curated
BookCover dataset that provides narrative layouts and structured visual cues,
enabling the model to depict abstract and sequential scientific concepts with
strong semantic alignment. Through layout-conditioned generation, contrastive
visual-semantic training, and prompt modulation, LEARN produces coherent visual
sequences that support mid-to-high-level reasoning in line with Bloom's
taxonomy while reducing extraneous cognitive load as emphasized by Cognitive
Load Theory. By fostering spatially organized and story-driven narratives, the
framework counters fragmented attention often induced by short-form media and
promotes sustained conceptual focus. Beyond static diagrams, LEARN demonstrates
potential for integration with multimodal systems and curriculum-linked
knowledge graphs to create adaptive, exploratory educational content. As the
first generative approach to unify layout-based storytelling, semantic
structure learning, and cognitive scaffolding, LEARN represents a novel
direction for generative AI in education. The code and dataset will be released
to facilitate future research and practical deployment.

</details>


### [32] [Semi-supervised Image Dehazing via Expectation-Maximization and Bidirectional Brownian Bridge Diffusion Models](https://arxiv.org/abs/2508.11165)
*Bing Liu,Le Wang,Mingming Liu,Hao Liu,Rui Yao,Yong Zhou,Peng Liu,Tongqiang Xia*

Main category: cs.CV

TL;DR: 提出了一种基于期望最大化和双向布朗桥扩散模型（EM-B3DM）的半监督图像去雾方法，通过两阶段学习方案有效解决了真实世界雾霾图像的去雾问题。


<details>
  <summary>Details</summary>
Motivation: 现有去雾方法在处理真实世界雾霾图像时效果不佳，尤其是浓雾场景，主要原因是缺乏真实世界的配对数据和鲁棒先验。

Method: 采用两阶段学习方案：第一阶段使用EM算法解耦配对图像联合分布，并用布朗桥扩散模型建模；第二阶段利用预训练模型和大规模未配对数据提升性能，并引入细节增强的RDC模块。

Result: 在合成和真实数据集上，EM-B3DM表现优于或至少与现有最先进方法相当。

Conclusion: EM-B3DM通过半监督学习和两阶段方案，显著提升了真实世界雾霾图像的去雾效果。

Abstract: Existing dehazing methods deal with real-world haze images with difficulty,
especially scenes with thick haze. One of the main reasons is the lack of
real-world paired data and robust priors. To avoid the costly collection of
paired hazy and clear images, we propose an efficient semi-supervised image
dehazing method via Expectation-Maximization and Bidirectional Brownian Bridge
Diffusion Models (EM-B3DM) with a two-stage learning scheme. In the first
stage, we employ the EM algorithm to decouple the joint distribution of paired
hazy and clear images into two conditional distributions, which are then
modeled using a unified Brownian Bridge diffusion model to directly capture the
structural and content-related correlations between hazy and clear images. In
the second stage, we leverage the pre-trained model and large-scale unpaired
hazy and clear images to further improve the performance of image dehazing.
Additionally, we introduce a detail-enhanced Residual Difference Convolution
block (RDC) to capture gradient-level information, significantly enhancing the
model's representation capability. Extensive experiments demonstrate that our
EM-B3DM achieves superior or at least comparable performance to
state-of-the-art methods on both synthetic and real-world datasets.

</details>


### [33] [VFM-Guided Semi-Supervised Detection Transformer for Source-Free Object Detection in Remote Sensing Images](https://arxiv.org/abs/2508.11167)
*Jianhong Han,Yupei Wang,Liang Chen*

Main category: cs.CV

TL;DR: VG-DETR是一种基于视觉基础模型的半监督框架，用于解决源自由目标检测中的伪标签噪声问题，通过双级对齐和对比学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 在遥感场景中，源数据不可用导致传统域适应方法受限，SFOD因伪标签噪声易崩溃，VG-DETR旨在利用少量目标标注和视觉基础模型解决这一问题。

Method: 结合视觉基础模型的语义先验，提出伪标签挖掘策略和双级对齐方法，通过对比学习和特征匹配增强特征鲁棒性。

Result: VG-DETR在源自由遥感检测任务中表现优异。

Conclusion: VG-DETR通过整合视觉基础模型和半监督学习，有效提升了源自由目标检测的性能和鲁棒性。

Abstract: Unsupervised domain adaptation methods have been widely explored to bridge
domain gaps. However, in real-world remote-sensing scenarios, privacy and
transmission constraints often preclude access to source domain data, which
limits their practical applicability. Recently, Source-Free Object Detection
(SFOD) has emerged as a promising alternative, aiming at cross-domain
adaptation without relying on source data, primarily through a self-training
paradigm. Despite its potential, SFOD frequently suffers from training collapse
caused by noisy pseudo-labels, especially in remote sensing imagery with dense
objects and complex backgrounds. Considering that limited target domain
annotations are often feasible in practice, we propose a Vision
foundation-Guided DEtection TRansformer (VG-DETR), built upon a semi-supervised
framework for SFOD in remote sensing images. VG-DETR integrates a Vision
Foundation Model (VFM) into the training pipeline in a "free lunch" manner,
leveraging a small amount of labeled target data to mitigate pseudo-label noise
while improving the detector's feature-extraction capability. Specifically, we
introduce a VFM-guided pseudo-label mining strategy that leverages the VFM's
semantic priors to further assess the reliability of the generated
pseudo-labels. By recovering potentially correct predictions from
low-confidence outputs, our strategy improves pseudo-label quality and
quantity. In addition, a dual-level VFM-guided alignment method is proposed,
which aligns detector features with VFM embeddings at both the instance and
image levels. Through contrastive learning among fine-grained prototypes and
similarity matching between feature maps, this dual-level alignment further
enhances the robustness of feature representations against domain gaps.
Extensive experiments demonstrate that VG-DETR achieves superior performance in
source-free remote sensing detection tasks.

</details>


### [34] [Better Supervised Fine-tuning for VQA: Integer-Only Loss](https://arxiv.org/abs/2508.11170)
*Baihong Qian,Haotian Fan,Wenjie Liao,Yunqiu Wang,Tao Li,Junhui Cui*

Main category: cs.CV

TL;DR: 提出了一种名为IOVQA的微调方法，通过整数标签和针对性损失计算机制提升视觉语言模型在视频质量评估任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉内容评估中存在结果不精确和损失计算效率低的问题，限制了模型对关键评估指标的关注。

Method: 采用整数标签（范围[10,50]）和目标掩码策略，仅计算标签前两位整数的损失，以增强模型对关键数值评估的学习。

Result: 实验表明，该方法显著提升了模型在VQA任务中的准确性和一致性，在VQualA 2025挑战赛中排名第三。

Conclusion: 仅使用整数标签进行微调是优化视觉语言模型在定量评估场景中的有效方法。

Abstract: With the rapid advancement of vision language models(VLM), their ability to
assess visual content based on specific criteria and dimensions has become
increasingly critical for applications such as video-theme consistency
assessment and visual quality scoring. However, existing methods often suffer
from imprecise results and inefficient loss calculation, which limit the focus
of the model on key evaluation indicators. To address this, we propose
IOVQA(Integer-only VQA), a novel fine-tuning approach tailored for VLMs to
enhance their performance in video quality assessment tasks. The key innovation
of IOVQA lies in its label construction and its targeted loss calculation
mechanism. Specifically, during dataset curation, we constrain the model's
output to integers within the range of [10,50], ensuring numerical stability,
and convert decimal Overall_MOS to integer before using them as labels. We also
introduce a target-mask strategy: when computing the loss, only the first
two-digit-integer of the label is unmasked, forcing the model to learn the
critical components of the numerical evaluation. After fine-tuning the
Qwen2.5-VL model using the constructed dataset, experimental results
demonstrate that the proposed method significantly improves the model's
accuracy and consistency in the VQA task, ranking 3rd in VQualA 2025
GenAI-Bench AIGC Video Quality Assessment Challenge -- Track I. Our work
highlights the effectiveness of merely leaving integer labels during
fine-tuning, providing an effective idea for optimizing VLMs in quantitative
evaluation scenarios.

</details>


### [35] [Exploring the Tradeoff Between Diversity and Discrimination for Continuous Category Discovery](https://arxiv.org/abs/2508.11173)
*Ruobing Jiang,Yang Liu,Haobing Liu,Yanwei Yu,Chunyang Wang*

Main category: cs.CV

TL;DR: 提出了一种名为IDOD的方法，用于连续类别发现（CCD），通过独立多样性模块、联合发现模块和正交性模块，解决了现有方法在错误积累和存储空间占用上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有CCD方法在发现新类别和分类之间存在矛盾，且容易积累错误，同时需要大量存储空间防止遗忘。

Method: IDOD方法包括独立多样性模块、联合发现模块和正交性模块，分别用于避免分类特征单一化、减少错误积累和降低存储开销。

Result: 在细粒度数据集上，IDOD优于现有方法。

Conclusion: IDOD通过模块化设计有效解决了CCD中的关键问题，性能优于现有方法。

Abstract: Continuous category discovery (CCD) aims to automatically discover novel
categories in continuously arriving unlabeled data. This is a challenging
problem considering that there is no number of categories and labels in the
newly arrived data, while also needing to mitigate catastrophic forgetting.
Most CCD methods cannot handle the contradiction between novel class discovery
and classification well. They are also prone to accumulate errors in the
process of gradually discovering novel classes. Moreover, most of them use
knowledge distillation and data replay to prevent forgetting, occupying more
storage space. To address these limitations, we propose Independence-based
Diversity and Orthogonality-based Discrimination (IDOD). IDOD mainly includes
independent enrichment of diversity module, joint discovery of novelty module,
and continuous increment by orthogonality module. In independent enrichment,
the backbone is trained separately using contrastive loss to avoid it focusing
only on features for classification. Joint discovery transforms multi-stage
novel class discovery into single-stage, reducing error accumulation impact.
Continuous increment by orthogonality module generates mutually orthogonal
prototypes for classification and prevents forgetting with lower space overhead
via representative representation replay. Experimental results show that on
challenging fine-grained datasets, our method outperforms the state-of-the-art
methods.

</details>


### [36] [Fine-Grained VLM Fine-tuning via Latent Hierarchical Adapter Learning](https://arxiv.org/abs/2508.11176)
*Yumiao Zhao,Bo Jiang,Yuhe Ding,Xiao Wang,Jin Tang,Bin Luo*

Main category: cs.CV

TL;DR: LatHAdapter是一种新型适配器，通过利用下游数据的潜在语义层次结构，在双曲空间中优化视觉-语言模型的少样本分类任务。


<details>
  <summary>Details</summary>
Motivation: 现有适配器方法在视觉和文本表示对齐时未能捕捉类别与图像样本之间的一对多关联，且难以处理未知类别。

Method: 引入可学习的属性提示作为桥梁，在双曲空间中投影类别、属性和图像，并通过层次正则化学习潜在语义层次。

Result: 在四个少样本任务中，LatHAdapter显著优于其他微调方法，尤其在已知类别适应和未知类别泛化方面。

Conclusion: LatHAdapter通过潜在语义层次建模，有效解决了现有适配器方法的局限性，提升了少样本分类性能。

Abstract: Adapter-based approaches have garnered attention for fine-tuning pre-trained
Vision-Language Models (VLMs) on few-shot classification tasks. These methods
strive to develop a lightweight module that better aligns visual and (category)
textual representations, thereby enhancing performance on downstream few-shot
learning tasks. However, existing adapters generally learn/align (category)
textual-visual modalities via explicit spatial proximity in the underlying
embedding space, which i) fails to capture the inherent one-to-many
associations between categories and image samples and ii) struggles to
establish accurate associations between the unknown categories and images. To
address these issues, inspired by recent works on hyperbolic learning, we
develop a novel Latent Hierarchical Adapter (LatHAdapter) for fine-tuning VLMs
on downstream few-shot classification tasks. The core of LatHAdapter is to
exploit the latent semantic hierarchy of downstream training data and employ it
to provide richer, fine-grained guidance for the adapter learning process.
Specifically, LatHAdapter first introduces some learnable `attribute' prompts
as the bridge to align categories and images. Then, it projects the categories,
attribute prompts, and images within each batch in a hyperbolic space, and
employs hierarchical regularization to learn the latent semantic hierarchy of
them, thereby fully modeling the inherent one-to-many associations among
categories, learnable attributes, and image samples. Extensive experiments on
four challenging few-shot tasks show that the proposed LatHAdapter consistently
outperforms many other fine-tuning approaches, particularly in adapting known
classes and generalizing to unknown classes.

</details>


### [37] [Versatile Video Tokenization with Generative 2D Gaussian Splatting](https://arxiv.org/abs/2508.11183)
*Zhenghao Chen,Zicong Chen,Lei Liu,Yiming Wu,Dong Xu*

Main category: cs.CV

TL;DR: GVT是一种基于2D高斯生成策略的视频标记化方法，通过时空高斯嵌入和静态动态分离策略，显著提升了视频重建质量和动作识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频标记化方法在空间和时间维度上缺乏灵活性，导致冗余编码和难以区分静态与动态内容。

Method: 提出GVT，结合2D高斯生成策略和时空高斯嵌入机制，并通过高斯集划分策略分离静态与动态内容。

Result: GVT在视频重建、动作识别和压缩任务中表现优异，优于基线方法。

Conclusion: GVT通过高斯生成和动态分离策略，实现了高效且灵活的视频标记化。

Abstract: Video tokenization procedure is critical for a wide range of video processing
tasks. Most existing approaches directly transform video into fixed-grid and
patch-wise tokens, which exhibit limited versatility. Spatially, uniformly
allocating a fixed number of tokens often leads to over-encoding in
low-information regions. Temporally, reducing redundancy remains challenging
without explicitly distinguishing between static and dynamic content. In this
work, we propose the Gaussian Video Transformer (GVT), a versatile video
tokenizer built upon a generative 2D Gaussian Splatting (2DGS) strategy. We
first extract latent rigid features from a video clip and represent them with a
set of 2D Gaussians generated by our proposed Spatio-Temporal Gaussian
Embedding (STGE) mechanism in a feed-forward manner. Such generative 2D
Gaussians not only enhance spatial adaptability by assigning higher (resp.,
lower) rendering weights to regions with higher (resp., lower) information
content during rasterization, but also improve generalization by avoiding
per-video optimization.To enhance the temporal versatility, we introduce a
Gaussian Set Partitioning (GSP) strategy that separates the 2D Gaussians into
static and dynamic sets, which explicitly model static content shared across
different time-steps and dynamic content specific to each time-step, enabling a
compact representation.We primarily evaluate GVT on the video reconstruction,
while also assessing its performance on action recognition and compression
using the UCF101, Kinetics, and DAVIS datasets. Extensive experiments
demonstrate that GVT achieves a state-of-the-art video reconstruction quality,
outperforms the baseline MAGVIT-v2 in action recognition, and delivers
comparable compression performance.

</details>


### [38] [CHARM3R: Towards Unseen Camera Height Robust Monocular 3D Detector](https://arxiv.org/abs/2508.11185)
*Abhinav Kumar,Yuliang Guo,Zhihao Zhang,Xinyu Huang,Liu Ren,Xiaoming Liu*

Main category: cs.CV

TL;DR: 论文提出了一种针对单目3D检测器在相机高度变化时性能下降的解决方案CHARM3R，通过结合两种深度估计方法，显著提升了模型在未见相机高度上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D检测器在相机高度变化时性能下降，尤其是深度估计问题突出，需要一种更鲁棒的方法。

Method: 通过系统分析相机高度变化对深度估计的影响，提出CHARM3R模型，结合回归和基于地面的深度估计方法。

Result: CHARM3R在CARLA数据集上对未见相机高度的泛化能力提升超过45%，达到SOTA性能。

Conclusion: CHARM3R通过结合两种深度估计方法，有效解决了单目3D检测器在相机高度变化时的性能问题。

Abstract: Monocular 3D object detectors, while effective on data from one ego camera
height, struggle with unseen or out-of-distribution camera heights. Existing
methods often rely on Plucker embeddings, image transformations or data
augmentation. This paper takes a step towards this understudied problem by
first investigating the impact of camera height variations on state-of-the-art
(SoTA) Mono3D models. With a systematic analysis on the extended CARLA dataset
with multiple camera heights, we observe that depth estimation is a primary
factor influencing performance under height variations. We mathematically prove
and also empirically observe consistent negative and positive trends in mean
depth error of regressed and ground-based depth models, respectively, under
camera height changes. To mitigate this, we propose Camera Height Robust
Monocular 3D Detector (CHARM3R), which averages both depth estimates within the
model. CHARM3R improves generalization to unseen camera heights by more than
$45\%$, achieving SoTA performance on the CARLA dataset. Codes and Models at
https://github.com/abhi1kumar/CHARM3R

</details>


### [39] [Generating Dialogues from Egocentric Instructional Videos for Task Assistance: Dataset, Method and Benchmark](https://arxiv.org/abs/2508.11192)
*Lavisha Aggarwal,Vikas Bahirwani,Lin Li,Andrea Colaco*

Main category: cs.CV

TL;DR: 论文提出了一种自动将单人教学视频转化为任务指导对话的方法，并构建了大规模数据集HowToDIV。


<details>
  <summary>Details</summary>
Motivation: 现实世界中复杂任务需要专家指导，但缺乏对话-视频数据集支持AI代理研究。

Method: 利用大语言模型自动将单人教学视频转化为两人对话，并与视频片段对齐。

Result: 构建了包含507个对话、6636个问答对和24小时视频的HowToDIV数据集。

Conclusion: 该方法为任务指导对话研究提供了高效的数据生成方案，并建立了基准性能。

Abstract: Many everyday tasks ranging from fixing appliances, cooking recipes to car
maintenance require expert knowledge, especially when tasks are complex and
multi-step. Despite growing interest in AI agents, there is a scarcity of
dialogue-video datasets grounded for real world task assistance. In this paper,
we propose a simple yet effective approach that transforms single-person
instructional videos into task-guidance two-person dialogues, aligned with fine
grained steps and video-clips. Our fully automatic approach, powered by large
language models, offers an efficient alternative to the substantial cost and
effort required for human-assisted data collection. Using this technique, we
build HowToDIV, a large-scale dataset containing 507 conversations, 6636
question-answer pairs and 24 hours of videoclips across diverse tasks in
cooking, mechanics, and planting. Each session includes multi-turn conversation
where an expert teaches a novice user how to perform a task step by step, while
observing user's surrounding through a camera and microphone equipped wearable
device. We establish the baseline benchmark performance on HowToDIV dataset
through Gemma-3 model for future research on this new task of dialogues for
procedural-task assistance.

</details>


### [40] [UAV-VL-R1: Generalizing Vision-Language Models via Supervised Fine-Tuning and Multi-Stage GRPO for UAV Visual Reasoning](https://arxiv.org/abs/2508.11196)
*Jiajin Guan,Haibo Mei,Bonan Zhang,Dan Liu,Yuanshuang Fu,Yue Zhang*

Main category: cs.CV

TL;DR: 提出了一种轻量级视觉语言模型UAV-VL-R1，专门用于无人机航拍图像的结构化推理任务，通过混合训练方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 通用视觉语言模型在无人机航拍图像上性能下降，因其高分辨率、复杂空间语义和实时性要求，限制了其在结构化推理任务中的应用。

Method: 采用监督微调（SFT）和多阶段强化学习（RL）的混合训练方法，利用GRPO算法促进结构化推理。

Result: UAV-VL-R1在零样本准确率上比基线模型高出48.17%，并在多个任务上优于更大的72B模型。

Conclusion: UAV-VL-R1在性能和效率上均表现出色，适合资源受限的无人机平台实时部署。

Abstract: Recent advances in vision-language models (VLMs) have demonstrated strong
generalization in natural image tasks. However, their performance often
degrades on unmanned aerial vehicle (UAV)-based aerial imagery, which features
high resolution, complex spatial semantics, and strict real-time constraints.
These challenges limit the applicability of general-purpose VLMs to structured
aerial reasoning tasks. To address these challenges, we propose UAV-VL-R1, a
lightweight VLM explicitly designed for aerial visual reasoning. It is trained
using a hybrid method that combines supervised fine-tuning (SFT) and
multi-stage reinforcement learning (RL). We leverage the group relative policy
optimization (GRPO) algorithm to promote structured and interpretable reasoning
through rule-guided rewards and intra-group policy alignment. To support model
training and evaluation, we introduce a high-resolution visual question
answering dataset named HRVQA-VL, which consists of 50,019 annotated samples
covering eight UAV-relevant reasoning tasks, including object counting,
transportation recognition, and spatial scene inference. Experimental results
show that UAV-VL-R1 achieves a 48.17% higher zero-shot accuracy than the
Qwen2-VL-2B-Instruct baseline and even outperforms its 72B-scale variant, which
is 36x larger, on multiple tasks. Ablation studies reveal that while SFT
improves semantic alignment, it may reduce reasoning diversity in mathematical
tasks. GRPO-based RL compensates for this limitation by enhancing logical
flexibility and the robustness of inference. Additionally, UAV-VL-R1 requires
only 3.9GB of memory under FP16 inference and can be quantized to 2.5GB with
INT8, supporting real-time deployment on resource-constrained UAV platforms.

</details>


### [41] [A Coarse-to-Fine Human Pose Estimation Method based on Two-stage Distillation and Progressive Graph Neural Network](https://arxiv.org/abs/2508.11212)
*Zhangjian Ji,Wenjin Zhang,Shaotong Qiao,Kai Feng,Yuhua Qian*

Main category: cs.CV

TL;DR: 提出了一种新颖的从粗到细的两阶段知识蒸馏框架，用于轻量级人体姿态估计，通过挖掘关节结构信息和渐进式图卷积网络提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有的人体姿态估计方法计算资源消耗大，传统知识蒸馏框架未能充分利用关节间的上下文信息。

Method: 两阶段知识蒸馏：第一阶段引入关节结构损失挖掘语义信息；第二阶段使用渐进式图卷积网络（IGP-GCN）细化姿态。

Result: 在COCO和CrowdPose数据集上表现优异，尤其在复杂场景的CrowdPose上改进显著。

Conclusion: 该方法通过结构感知和渐进式优化，实现了轻量且高精度的人体姿态估计。

Abstract: Human pose estimation has been widely applied in the human-centric
understanding and generation, but most existing state-of-the-art human pose
estimation methods require heavy computational resources for accurate
predictions. In order to obtain an accurate, robust yet lightweight human pose
estimator, one feasible way is to transfer pose knowledge from a powerful
teacher model to a less-parameterized student model by knowledge distillation.
However, the traditional knowledge distillation framework does not fully
explore the contextual information among human joints. Thus, in this paper, we
propose a novel coarse-to-fine two-stage knowledge distillation framework for
human pose estimation. In the first-stage distillation, we introduce the human
joints structure loss to mine the structural information among human joints so
as to transfer high-level semantic knowledge from the teacher model to the
student model. In the second-stage distillation, we utilize an Image-Guided
Progressive Graph Convolutional Network (IGP-GCN) to refine the initial human
pose obtained from the first-stage distillation and supervise the training of
the IGP-GCN in the progressive way by the final output pose of teacher model.
The extensive experiments on the benchmark dataset: COCO keypoint and CrowdPose
datasets, show that our proposed method performs favorably against lots of the
existing state-of-the-art human pose estimation methods, especially for the
more complex CrowdPose dataset, the performance improvement of our model is
more significant.

</details>


### [42] [A CLIP-based Uncertainty Modal Modeling (UMM) Framework for Pedestrian Re-Identification in Autonomous Driving](https://arxiv.org/abs/2508.11218)
*Jialin Li,Shuqi Wu,Ning Wang*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级的不确定性模态建模（UMM）框架，用于解决自动驾驶中行人重识别（ReID）在多模态输入不确定或缺失时的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中，行人重识别需要处理多模态输入（如RGB、红外、草图或文本描述）的不确定性或缺失问题，而现有方法计算开销大，难以在资源受限环境中部署。

Method: UMM框架包含多模态令牌映射器、合成模态增强策略和跨模态线索交互学习器，结合CLIP的视觉-语言对齐能力，高效融合多模态输入。

Result: 实验表明，UMM在模态不确定条件下表现出强大的鲁棒性、泛化能力和计算效率。

Conclusion: UMM为自动驾驶中的行人重识别提供了一种可扩展且实用的解决方案。

Abstract: Re-Identification (ReID) is a critical technology in intelligent perception
systems, especially within autonomous driving, where onboard cameras must
identify pedestrians across views and time in real-time to support safe
navigation and trajectory prediction. However, the presence of uncertain or
missing input modalities--such as RGB, infrared, sketches, or textual
descriptions--poses significant challenges to conventional ReID approaches.
While large-scale pre-trained models offer strong multimodal semantic modeling
capabilities, their computational overhead limits practical deployment in
resource-constrained environments. To address these challenges, we propose a
lightweight Uncertainty Modal Modeling (UMM) framework, which integrates a
multimodal token mapper, synthetic modality augmentation strategy, and
cross-modal cue interactive learner. Together, these components enable unified
feature representation, mitigate the impact of missing modalities, and extract
complementary information across different data types. Additionally, UMM
leverages CLIP's vision-language alignment ability to fuse multimodal inputs
efficiently without extensive finetuning. Experimental results demonstrate that
UMM achieves strong robustness, generalization, and computational efficiency
under uncertain modality conditions, offering a scalable and practical solution
for pedestrian re-identification in autonomous driving scenarios.

</details>


### [43] [FantasyTalking2: Timestep-Layer Adaptive Preference Optimization for Audio-Driven Portrait Animation](https://arxiv.org/abs/2508.11255)
*MengChao Wang,Qiang Wang,Fan Jiang,Mu Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为Talking-Critic的多模态奖励模型和TLPO框架，用于优化音频驱动肖像动画的多维偏好对齐问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多维偏好（如运动自然性、唇同步准确性和视觉质量）上难以优化，且缺乏大规模高质量数据集。

Method: 引入Talking-Critic学习人类对齐的奖励函数，并构建Talking-NSQ数据集；提出TLPO框架，通过专家模块解耦和融合偏好。

Result: Talking-Critic在人类偏好评分上显著优于现有方法；TLPO在唇同步、运动自然性和视觉质量上均有显著提升。

Conclusion: 论文通过奖励模型和优化框架有效解决了多维偏好对齐问题，提升了音频驱动肖像动画的质量。

Abstract: Recent advances in audio-driven portrait animation have demonstrated
impressive capabilities. However, existing methods struggle to align with
fine-grained human preferences across multiple dimensions, such as motion
naturalness, lip-sync accuracy, and visual quality. This is due to the
difficulty of optimizing among competing preference objectives, which often
conflict with one another, and the scarcity of large-scale, high-quality
datasets with multidimensional preference annotations. To address these, we
first introduce Talking-Critic, a multimodal reward model that learns
human-aligned reward functions to quantify how well generated videos satisfy
multidimensional expectations. Leveraging this model, we curate Talking-NSQ, a
large-scale multidimensional human preference dataset containing 410K
preference pairs. Finally, we propose Timestep-Layer adaptive multi-expert
Preference Optimization (TLPO), a novel framework for aligning diffusion-based
portrait animation models with fine-grained, multidimensional preferences. TLPO
decouples preferences into specialized expert modules, which are then fused
across timesteps and network layers, enabling comprehensive, fine-grained
enhancement across all dimensions without mutual interference. Experiments
demonstrate that Talking-Critic significantly outperforms existing methods in
aligning with human preference ratings. Meanwhile, TLPO achieves substantial
improvements over baseline models in lip-sync accuracy, motion naturalness, and
visual quality, exhibiting superior performance in both qualitative and
quantitative evaluations. Ours project page:
https://fantasy-amap.github.io/fantasy-talking2/

</details>


### [44] [Generalized Decoupled Learning for Enhancing Open-Vocabulary Dense Perception](https://arxiv.org/abs/2508.11256)
*Junjie Wang,Keyu Chen,Yulin Li,Bin Chen,Hengshuang Zhao,Xiaojuan Qi,Zhuotao Tian*

Main category: cs.CV

TL;DR: DeCLIP通过解耦CLIP的自注意力模块，分别提取内容和上下文特征，提升密集视觉感知任务的性能。


<details>
  <summary>Details</summary>
Motivation: 解决CLIP在密集感知任务中局部特征表示不足的问题。

Method: 提出DeCLIP框架，通过解耦自注意力模块并分别增强内容和上下文特征。

Result: 在多种任务中实现最先进性能。

Conclusion: DeCLIP为开放词汇密集感知任务提供了坚实基础。

Abstract: Dense visual perception tasks have been constrained by their reliance on
predefined categories, limiting their applicability in real-world scenarios
where visual concepts are unbounded. While Vision-Language Models (VLMs) like
CLIP have shown promise in open-vocabulary tasks, their direct application to
dense perception often leads to suboptimal performance due to limitations in
local feature representation. In this work, we present our observation that
CLIP's image tokens struggle to effectively aggregate information from
spatially or semantically related regions, resulting in features that lack
local discriminability and spatial consistency. To address this issue, we
propose DeCLIP, a novel framework that enhances CLIP by decoupling the
self-attention module to obtain ``content'' and ``context'' features
respectively. \revise{The context features are enhanced by jointly distilling
semantic correlations from Vision Foundation Models (VFMs) and object integrity
cues from diffusion models, thereby enhancing spatial consistency. In parallel,
the content features are aligned with image crop representations and
constrained by region correlations from VFMs to improve local discriminability.
Extensive experiments demonstrate that DeCLIP establishes a solid foundation
for open-vocabulary dense perception, consistently achieving state-of-the-art
performance across a broad spectrum of tasks, including 2D detection and
segmentation, 3D instance segmentation, video instance segmentation, and 6D
object pose estimation.} Code is available at
https://github.com/xiaomoguhz/DeCLIP

</details>


### [45] [Vision-Language Models display a strong gender bias](https://arxiv.org/abs/2508.11262)
*Aiswarya Konavoor,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.CV

TL;DR: 研究分析了视觉语言模型（VLM）中性别关联的潜在偏见，提出了一种评估框架。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在共享表示空间中对齐图像和文本，但可能隐含地编码社会刻板印象，尤其是性别偏见。

Method: 通过计算人脸图像与描述职业和活动的短语之间的余弦相似度差异，量化性别关联。

Result: 建立了语句和类别级别的性别关联图谱，并提供了不确定性分析和稳健的评估框架。

Conclusion: 研究揭示了VLM中的性别偏见，并提出了一种可扩展的评估方法。

Abstract: Vision-language models (VLM) align images and text in a shared representation
space that is useful for retrieval and zero-shot transfer. Yet, this alignment
can encode and amplify social stereotypes in subtle ways that are not obvious
from standard accuracy metrics. In this study, we test whether the contrastive
vision-language encoder exhibits gender-linked associations when it places
embeddings of face images near embeddings of short phrases that describe
occupations and activities. We assemble a dataset of 220 face photographs split
by perceived binary gender and a set of 150 unique statements distributed
across six categories covering emotional labor, cognitive labor, domestic
labor, technical labor, professional roles, and physical labor. We compute
unit-norm image embeddings for every face and unit-norm text embeddings for
every statement, then define a statement-level association score as the
difference between the mean cosine similarity to the male set and the mean
cosine similarity to the female set, where positive values indicate stronger
association with the male set and negative values indicate stronger association
with the female set. We attach bootstrap confidence intervals by resampling
images within each gender group, aggregate by category with a separate
bootstrap over statements, and run a label-swap null model that estimates the
level of mean absolute association we would expect if no gender structure were
present. The outcome is a statement-wise and category-wise map of gender
associations in a contrastive vision-language space, accompanied by
uncertainty, simple sanity checks, and a robust gender bias evaluation
framework.

</details>


### [46] [Domain-aware Category-level Geometry Learning Segmentation for 3D Point Clouds](https://arxiv.org/abs/2508.11265)
*Pei He,Lingling Li,Licheng Jiao,Ronghua Shang,Fang Liu,Shuang Wang,Xu Liu,Wenping Ma*

Main category: cs.CV

TL;DR: 提出了一种类别级几何学习框架，用于解决3D语义分割中的领域泛化问题，通过几何嵌入和一致性学习提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前方法通过增强点云数据分布来缓解领域偏移，但忽略了类别级分布和对齐，导致模型泛化能力不足。

Method: 提出类别级几何嵌入（CGE）感知点云特征的细粒度几何属性，并结合几何一致性学习（GCL）模拟潜在3D分布和对齐几何嵌入。

Result: 实验结果表明，该方法在领域泛化的点云分割任务中具有竞争力，分割精度优于现有方法。

Conclusion: 通过类别级几何学习和一致性对齐，有效提升了3D语义分割模型的领域泛化能力。

Abstract: Domain generalization in 3D segmentation is a critical challenge in deploying
models to unseen environments. Current methods mitigate the domain shift by
augmenting the data distribution of point clouds. However, the model learns
global geometric patterns in point clouds while ignoring the category-level
distribution and alignment. In this paper, a category-level geometry learning
framework is proposed to explore the domain-invariant geometric features for
domain generalized 3D semantic segmentation. Specifically, Category-level
Geometry Embedding (CGE) is proposed to perceive the fine-grained geometric
properties of point cloud features, which constructs the geometric properties
of each class and couples geometric embedding to semantic learning. Secondly,
Geometric Consistent Learning (GCL) is proposed to simulate the latent 3D
distribution and align the category-level geometric embeddings, allowing the
model to focus on the geometric invariant information to improve
generalization. Experimental results verify the effectiveness of the proposed
method, which has very competitive segmentation accuracy compared with the
state-of-the-art domain generalized point cloud methods.

</details>


### [47] [Enhancing Supervised Composed Image Retrieval via Reasoning-Augmented Representation Engineering](https://arxiv.org/abs/2508.11272)
*Jun Li,Kai Li,Shaoguo Liu,Tingting Gao*

Main category: cs.CV

TL;DR: PMTFR框架通过Pyramid Patcher模块和多粒度视觉理解，结合训练免费的精炼方法，在监督CIR任务中超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在CIR任务中需要额外训练排序模型，且CoT技术的应用受限，难以在监督CIR中取得满意结果。

Method: 提出PMTFR框架，包括Pyramid Matching Model和Training-Free Refinement，利用Pyramid Patcher增强多粒度视觉理解，并从CoT数据中提取表示注入LVLMs。

Result: 在CIR基准测试中，PMTFR优于现有方法。

Conclusion: PMTFR通过简单有效的模块和训练免费精炼，显著提升了监督CIR任务的性能。

Abstract: Composed Image Retrieval (CIR) presents a significant challenge as it
requires jointly understanding a reference image and a modified textual
instruction to find relevant target images. Some existing methods attempt to
use a two-stage approach to further refine retrieval results. However, this
often requires additional training of a ranking model. Despite the success of
Chain-of-Thought (CoT) techniques in reducing training costs for language
models, their application in CIR tasks remains limited -- compressing visual
information into text or relying on elaborate prompt designs. Besides, existing
works only utilize it for zero-shot CIR, as it is challenging to achieve
satisfactory results in supervised CIR with a well-trained model. In this work,
we proposed a framework that includes the Pyramid Matching Model with
Training-Free Refinement (PMTFR) to address these challenges. Through a simple
but effective module called Pyramid Patcher, we enhanced the Pyramid Matching
Model's understanding of visual information at different granularities.
Inspired by representation engineering, we extracted representations from COT
data and injected them into the LVLMs. This approach allowed us to obtain
refined retrieval scores in the Training-Free Refinement paradigm without
relying on explicit textual reasoning, further enhancing performance. Extensive
experiments on CIR benchmarks demonstrate that PMTFR surpasses state-of-the-art
methods in supervised CIR tasks. The code will be made public.

</details>


### [48] [Probing the Representational Power of Sparse Autoencoders in Vision Models](https://arxiv.org/abs/2508.11277)
*Matthew Lyle Olson,Musashi Hinck,Neale Ratzlaff,Changbai Li,Phillip Howard,Vasudev Lal,Shao-Yen Tseng*

Main category: cs.CV

TL;DR: 稀疏自编码器（SAEs）在视觉模型中的应用潜力被广泛评估，展示了其在语义理解、泛化能力和可控生成方面的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管SAEs在语言模型中广泛应用，但在视觉领域的研究较少，本文旨在填补这一空白。

Method: 通过多种图像任务评估SAEs在视觉模型中的表现，包括视觉嵌入模型、多模态LMMs和扩散模型。

Result: SAE特征具有语义意义，能提升分布外泛化能力，并支持可控生成。

Conclusion: SAEs在视觉模型中具有提升可解释性、泛化能力和可控性的潜力。

Abstract: Sparse Autoencoders (SAEs) have emerged as a popular tool for interpreting
the hidden states of large language models (LLMs). By learning to reconstruct
activations from a sparse bottleneck layer, SAEs discover interpretable
features from the high-dimensional internal representations of LLMs. Despite
their popularity with language models, SAEs remain understudied in the visual
domain. In this work, we provide an extensive evaluation the representational
power of SAEs for vision models using a broad range of image-based tasks. Our
experimental results demonstrate that SAE features are semantically meaningful,
improve out-of-distribution generalization, and enable controllable generation
across three vision model architectures: vision embedding models, multi-modal
LMMs and diffusion models. In vision embedding models, we find that learned SAE
features can be used for OOD detection and provide evidence that they recover
the ontological structure of the underlying model. For diffusion models, we
demonstrate that SAEs enable semantic steering through text encoder
manipulation and develop an automated pipeline for discovering
human-interpretable attributes. Finally, we conduct exploratory experiments on
multi-modal LLMs, finding evidence that SAE features reveal shared
representations across vision and language modalities. Our study provides a
foundation for SAE evaluation in vision models, highlighting their strong
potential improving interpretability, generalization, and steerability in the
visual domain.

</details>


### [49] [Unifying Scale-Aware Depth Prediction and Perceptual Priors for Monocular Endoscope Pose Estimation and Tissue Reconstruction](https://arxiv.org/abs/2508.11282)
*Muzammil Khan,Enzo Kerkhof,Matteo Fusaglia,Koert Kuhlmann,Theo Ruers,Françoise J. Siepel*

Main category: cs.CV

TL;DR: 提出了一种用于单目内窥镜组织重建的统一框架，结合了尺度感知深度预测和时间约束的感知细化，解决了深度模糊和组织变形等问题。


<details>
  <summary>Details</summary>
Motivation: 单目内窥镜的位姿估计和组织重建面临深度模糊、组织变形、运动不一致等挑战，需要更鲁棒的解决方案。

Method: 框架包含MAPIS-Depth模块（结合Depth Pro和Depth Anything）和WEMA-RTDL模块，通过RAFT计算像素对应关系，并基于LPIPS感知相似性进行时间细化。

Result: 在HEVD和SCARED数据集上的评估表明，该框架优于现有方法。

Conclusion: 该框架通过伪度量深度估计和时间细化，显著提升了单目内窥镜的3D重建效果。

Abstract: Accurate endoscope pose estimation and 3D tissue surface reconstruction
significantly enhances monocular minimally invasive surgical procedures by
enabling accurate navigation and improved spatial awareness. However, monocular
endoscope pose estimation and tissue reconstruction face persistent challenges,
including depth ambiguity, physiological tissue deformation, inconsistent
endoscope motion, limited texture fidelity, and a restricted field of view. To
overcome these limitations, a unified framework for monocular endoscopic tissue
reconstruction that integrates scale-aware depth prediction with
temporally-constrained perceptual refinement is presented. This framework
incorporates a novel MAPIS-Depth module, which leverages Depth Pro for robust
initialisation and Depth Anything for efficient per-frame depth prediction, in
conjunction with L-BFGS-B optimisation, to generate pseudo-metric depth
estimates. These estimates are temporally refined by computing pixel
correspondences using RAFT and adaptively blending flow-warped frames based on
LPIPS perceptual similarity, thereby reducing artefacts arising from
physiological tissue deformation and motion. To ensure accurate registration of
the synthesised pseudo-RGBD frames from MAPIS-Depth, a novel WEMA-RTDL module
is integrated, optimising both rotation and translation. Finally, truncated
signed distance function-based volumetric fusion and marching cubes are applied
to extract a comprehensive 3D surface mesh. Evaluations on HEVD and SCARED,
with ablation and comparative analyses, demonstrate the framework's robustness
and superiority over state-of-the-art methods.

</details>


### [50] [TimeMachine: Fine-Grained Facial Age Editing with Identity Preservation](https://arxiv.org/abs/2508.11284)
*Yilin Mi,Qixin Yan,Zheng-Peng Duan,Chunle Guo,Hubery Yin,Hao Liu,Chen Li,Chongyi Li*

Main category: cs.CV

TL;DR: TimeMachine是一种基于扩散模型的框架，用于精确编辑面部年龄同时保持身份特征不变。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度年龄编辑和身份保持方面存在挑战。

Method: 通过多交叉注意力模块注入高精度年龄信息，并设计Age Classifier Guidance模块。

Result: 实验表明TimeMachine在细粒度年龄编辑和身份一致性保持上达到最优性能。

Conclusion: TimeMachine通过新框架和数据集解决了年龄编辑中的关键问题。

Abstract: With the advancement of generative models, facial image editing has made
significant progress. However, achieving fine-grained age editing while
preserving personal identity remains a challenging task.In this paper, we
propose TimeMachine, a novel diffusion-based framework that achieves accurate
age editing while keeping identity features unchanged. To enable fine-grained
age editing, we inject high-precision age information into the multi-cross
attention module, which explicitly separates age-related and identity-related
features. This design facilitates more accurate disentanglement of age
attributes, thereby allowing precise and controllable manipulation of facial
aging.Furthermore, we propose an Age Classifier Guidance (ACG) module that
predicts age directly in the latent space, instead of performing denoising
image reconstruction during training. By employing a lightweight module to
incorporate age constraints, this design enhances age editing accuracy by
modest increasing training cost. Additionally, to address the lack of
large-scale, high-quality facial age datasets, we construct a HFFA dataset
(High-quality Fine-grained Facial-Age dataset) which contains one million
high-resolution images labeled with identity and facial attributes.
Experimental results demonstrate that TimeMachine achieves state-of-the-art
performance in fine-grained age editing while preserving identity consistency.

</details>


### [51] [Hyperspectral vs. RGB for Pedestrian Segmentation in Urban Driving Scenes: A Comparative Study](https://arxiv.org/abs/2508.11301)
*Jiarong Li,Imad Ali Shah,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 研究探讨了高光谱成像（HSI）在行人分割中的潜力，通过最优波段选择显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: RGB成像中的同色异谱现象导致行人与背景难以区分，影响自动驾驶系统的安全性。

Method: 比较了RGB与两种降维方法（PCA和CSNR-JMIM），并评估了三种语义分割模型（U-Net、DeepLabV3+、SegFormer）。

Result: CSNR-JMIM方法在行人和骑手分割中均表现优于RGB，IoU和F1分数分别提升1.44%和2.18%。

Conclusion: 最优HSI波段选择能有效提升行人分割性能，对安全关键型自动驾驶应用具有重要潜力。

Abstract: Pedestrian segmentation in automotive perception systems faces critical
safety challenges due to metamerism in RGB imaging, where pedestrians and
backgrounds appear visually indistinguishable.. This study investigates the
potential of hyperspectral imaging (HSI) for enhanced pedestrian segmentation
in urban driving scenarios using the Hyperspectral City v2 (H-City) dataset. We
compared standard RGB against two dimensionality-reduction approaches by
converting 128-channel HSI data into three-channel representations: Principal
Component Analysis (PCA) and optimal band selection using Contrast
Signal-to-Noise Ratio with Joint Mutual Information Maximization (CSNR-JMIM).
Three semantic segmentation models were evaluated: U-Net, DeepLabV3+, and
SegFormer. CSNR-JMIM consistently outperformed RGB with an average improvements
of 1.44% in Intersection over Union (IoU) and 2.18% in F1-score for pedestrian
segmentation. Rider segmentation showed similar gains with 1.43% IoU and 2.25%
F1-score improvements. These improved performance results from enhanced
spectral discrimination of optimally selected HSI bands effectively reducing
false positives. This study demonstrates robust pedestrian segmentation through
optimal HSI band selection, showing significant potential for safety-critical
automotive applications.

</details>


### [52] [Denoise-then-Retrieve: Text-Conditioned Video Denoising for Video Moment Retrieval](https://arxiv.org/abs/2508.11313)
*Weijia Liu,Jiuxin Cao,Bo Miao,Zhiheng Fu,Xuelin Zhu,Jiawei Ge,Bo Liu,Mehwish Nasim,Ajmal Mian*

Main category: cs.CV

TL;DR: 提出了一种去噪后检索的范式（DRNet），通过过滤文本无关的视频片段提升多模态对齐和检索准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法编码所有视频片段（包括无关片段）会破坏多模态对齐并阻碍优化。

Method: 设计了文本条件去噪（TCD）和文本重构反馈（TRF）模块，动态识别噪声片段并纯化多模态表示。

Result: 在Charades-STA和QVHighlights数据集上超越现有方法。

Conclusion: 去噪后检索范式可提升视频时刻检索性能，并能无缝集成到其他先进模型中。

Abstract: Current text-driven Video Moment Retrieval (VMR) methods encode all video
clips, including irrelevant ones, disrupting multimodal alignment and hindering
optimization. To this end, we propose a denoise-then-retrieve paradigm that
explicitly filters text-irrelevant clips from videos and then retrieves the
target moment using purified multimodal representations. Following this
paradigm, we introduce the Denoise-then-Retrieve Network (DRNet), comprising
Text-Conditioned Denoising (TCD) and Text-Reconstruction Feedback (TRF)
modules. TCD integrates cross-attention and structured state space blocks to
dynamically identify noisy clips and produce a noise mask to purify multimodal
video representations. TRF further distills a single query embedding from
purified video representations and aligns it with the text embedding, serving
as auxiliary supervision for denoising during training. Finally, we perform
conditional retrieval using text embeddings on purified video representations
for accurate VMR. Experiments on Charades-STA and QVHighlights demonstrate that
our approach surpasses state-of-the-art methods on all metrics. Furthermore,
our denoise-then-retrieve paradigm is adaptable and can be seamlessly
integrated into advanced VMR models to boost performance.

</details>


### [53] [Logic Unseen: Revealing the Logical Blindspots of Vision-Language Models](https://arxiv.org/abs/2508.11317)
*Yuchen Zhou,Jiayu Tang,Shuo Yang,Xiaoyan Xiao,Yuqin Dai,Wenhao Yang,Chao Gou,Xiaobo Xia,Tat-Seng Chua*

Main category: cs.CV

TL;DR: LogicBench是一个诊断视觉语言模型（VLMs）逻辑理解能力的基准测试，LogicCLIP是提升VLMs逻辑敏感性的训练框架。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在逻辑理解方面存在明显不足，限制了其在实际应用中的可靠性。

Method: 提出LogicBench基准测试和LogicCLIP训练框架，后者通过逻辑感知数据生成和多目标对比学习提升逻辑理解。

Result: LogicCLIP在逻辑理解任务上显著优于基线模型，同时在通用视觉语言任务上保持竞争力。

Conclusion: LogicBench和LogicCLIP为提升VLMs的逻辑能力提供了重要资源。

Abstract: Vision-Language Models (VLMs), exemplified by CLIP, have emerged as
foundational for multimodal intelligence. However, their capacity for logical
understanding remains significantly underexplored, resulting in critical
''logical blindspots'' that limit their reliability in practical applications.
To systematically diagnose this, we introduce LogicBench, a comprehensive
benchmark with over 50,000 vision-language pairs across 9 logical categories
and 4 diverse scenarios: images, videos, anomaly detection, and medical
diagnostics. Our evaluation reveals that existing VLMs, even the
state-of-the-art ones, fall at over 40 accuracy points below human performance,
particularly in challenging tasks like Causality and Conditionality,
highlighting their reliance on surface semantics over critical logical
structures. To bridge this gap, we propose LogicCLIP, a novel training
framework designed to boost VLMs' logical sensitivity through advancements in
both data generation and optimization objectives. LogicCLIP utilizes
logic-aware data generation and a contrastive learning strategy that combines
coarse-grained alignment, a fine-grained multiple-choice objective, and a novel
logical structure-aware objective. Extensive experiments demonstrate
LogicCLIP's substantial improvements in logical comprehension across all
LogicBench domains, significantly outperforming baselines. Moreover, LogicCLIP
retains, and often surpasses, competitive performance on general
vision-language benchmarks, demonstrating that the enhanced logical
understanding does not come at the expense of general alignment. We believe
that LogicBench and LogicCLIP will be important resources for advancing VLM
logical capabilities.

</details>


### [54] [Delving into Dynamic Scene Cue-Consistency for Robust 3D Multi-Object Tracking](https://arxiv.org/abs/2508.11323)
*Haonan Zhang,Xinyao Wang,Boxi Wu,Tu Zheng,Wang Yunhua,Zheng Yang*

Main category: cs.CV

TL;DR: 提出了一种基于空间线索一致性的3D多目标跟踪方法DSC-Track，通过动态场景线索一致性提升跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖单个物体运动建模，在拥挤场景或检测不准确时表现不佳，忽略了物体间的几何关系。

Method: 设计了统一的时空编码器（PPF）和线索一致性变换模块，动态更新机制保留关键时空信息。

Result: 在nuScenes和Waymo数据集上验证了方法的有效性，nuScenes验证集和测试集的AMOTA分别达到73.2%和70.3%。

Conclusion: DSC-Track通过关注稳定的空间模式，显著提升了复杂场景下的3D多目标跟踪性能。

Abstract: 3D multi-object tracking is a critical and challenging task in the field of
autonomous driving. A common paradigm relies on modeling individual object
motion, e.g., Kalman filters, to predict trajectories. While effective in
simple scenarios, this approach often struggles in crowded environments or with
inaccurate detections, as it overlooks the rich geometric relationships between
objects. This highlights the need to leverage spatial cues. However, existing
geometry-aware methods can be susceptible to interference from irrelevant
objects, leading to ambiguous features and incorrect associations. To address
this, we propose focusing on cue-consistency: identifying and matching stable
spatial patterns over time. We introduce the Dynamic Scene Cue-Consistency
Tracker (DSC-Track) to implement this principle. Firstly, we design a unified
spatiotemporal encoder using Point Pair Features (PPF) to learn discriminative
trajectory embeddings while suppressing interference. Secondly, our
cue-consistency transformer module explicitly aligns consistent feature
representations between historical tracks and current detections. Finally, a
dynamic update mechanism preserves salient spatiotemporal information for
stable online tracking. Extensive experiments on the nuScenes and Waymo Open
Datasets validate the effectiveness and robustness of our approach. On the
nuScenes benchmark, for instance, our method achieves state-of-the-art
performance, reaching 73.2% and 70.3% AMOTA on the validation and test sets,
respectively.

</details>


### [55] [Noise Matters: Optimizing Matching Noise for Diffusion Classifiers](https://arxiv.org/abs/2508.11330)
*Yanghao Wang,Long Chen*

Main category: cs.CV

TL;DR: 提出了一种名为NoOp的噪声优化方法，通过学习匹配噪声来提升扩散分类器的稳定性和速度。


<details>
  <summary>Details</summary>
Motivation: 解决扩散分类器中噪声不稳定性问题，减少对大量噪声采样的依赖，提高分类速度。

Method: 通过频率匹配和空间匹配原则优化噪声，包括数据集特定噪声优化和图像特定噪声偏移训练。

Result: 在各种数据集上的实验验证了NoOp的有效性。

Conclusion: NoOp通过学习匹配噪声显著提升了扩散分类器的性能和效率。

Abstract: Although today's pretrained discriminative vision-language models (e.g.,
CLIP) have demonstrated strong perception abilities, such as zero-shot image
classification, they also suffer from the bag-of-words problem and spurious
bias. To mitigate these problems, some pioneering studies leverage powerful
generative models (e.g., pretrained diffusion models) to realize generalizable
image classification, dubbed Diffusion Classifier (DC). Specifically, by
randomly sampling a Gaussian noise, DC utilizes the differences of denoising
effects with different category conditions to classify categories.
Unfortunately, an inherent and notorious weakness of existing DCs is noise
instability: different random sampled noises lead to significant performance
changes. To achieve stable classification performance, existing DCs always
ensemble the results of hundreds of sampled noises, which significantly reduces
the classification speed. To this end, we firstly explore the role of noise in
DC, and conclude that: there are some ``good noises'' that can relieve the
instability. Meanwhile, we argue that these good noises should meet two
principles: Frequency Matching and Spatial Matching. Regarding both principles,
we propose a novel Noise Optimization method to learn matching (i.e., good)
noise for DCs: NoOp. For frequency matching, NoOp first optimizes a
dataset-specific noise: Given a dataset and a timestep t, optimize one randomly
initialized parameterized noise. For Spatial Matching, NoOp trains a
Meta-Network that adopts an image as input and outputs image-specific noise
offset. The sum of optimized noise and noise offset will be used in DC to
replace random noise. Extensive ablations on various datasets demonstrated the
effectiveness of NoOp.

</details>


### [56] [GANDiff FR: Hybrid GAN Diffusion Synthesis for Causal Bias Attribution in Face Recognition](https://arxiv.org/abs/2508.11334)
*Md Asgor Hossain Reaj,Rajan Das Gupta,Md Yeasin Rahat,Nafiz Fahad,Md Jawadul Hasan,Tze Hui Liew*

Main category: cs.CV

TL;DR: GANDiff FR 是一个合成框架，通过精确控制人口统计和环境因素来测量、解释和减少偏见，结合了 StyleGAN3 和扩散模型，实现了对姿态、光照和表情的细粒度控制。


<details>
  <summary>Details</summary>
Motivation: 为了解决人脸识别系统中的偏见问题，并提供一个可重复、可量化的公平性评估标准。

Method: 结合 StyleGAN3 的身份保持生成和扩散模型的属性控制，生成 10,000 张人口统计平衡的面孔，并通过自动检测和人工审核验证真实性。

Result: AdaFace 减少了 60% 的组间 TPR 差异，光照占剩余偏见的 42%。合成数据在跨数据集评估中表现出强迁移性（r 0.85）。

Conclusion: GANDiff FR 提供了一个可重复、符合法规的公平性审计标准，支持透明和可扩展的偏见评估。

Abstract: We introduce GANDiff FR, the first synthetic framework that precisely
controls demographic and environmental factors to measure, explain, and reduce
bias with reproducible rigor. GANDiff FR unifies StyleGAN3-based
identity-preserving generation with diffusion-based attribute control, enabling
fine-grained manipulation of pose around 30 degrees, illumination (four
directions), and expression (five levels) under ceteris paribus conditions. We
synthesize 10,000 demographically balanced faces across five cohorts validated
for realism via automated detection (98.2%) and human review (89%) to isolate
and quantify bias drivers. Benchmarking ArcFace, CosFace, and AdaFace under
matched operating points shows AdaFace reduces inter-group TPR disparity by 60%
(2.5% vs. 6.3%), with illumination accounting for 42% of residual bias.
Cross-dataset evaluation on RFW, BUPT, and CASIA WebFace confirms strong
synthetic-to-real transfer (r 0.85). Despite around 20% computational overhead
relative to pure GANs, GANDiff FR yields three times more attribute-conditioned
variants, establishing a reproducible, regulation-aligned (EU AI Act) standard
for fairness auditing. Code and data are released to support transparent,
scalable bias evaluation.

</details>


### [57] [Index-Aligned Query Distillation for Transformer-based Incremental Object Detection](https://arxiv.org/abs/2508.11339)
*Mingxiao Ma,Shunyao Zhu,Guoliang Kang*

Main category: cs.CV

TL;DR: 论文提出了一种名为IAQD的新蒸馏方法，用于解决基于Transformer的增量目标检测中的知识遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 在增量目标检测任务中，传统的匈牙利匹配方法会导致查询在不同迭代中匹配不同的查询，从而引发知识遗忘。

Method: 提出Index-Aligned Query Distillation (IAQD)，通过索引对齐的查询蒸馏，仅对关键查询进行蒸馏，保留旧类别的语义和空间编码能力。

Result: 在代表性基准测试中，IAQD有效缓解了知识遗忘，达到了新的最优性能。

Conclusion: IAQD是一种有效的解决方案，能够在增量目标检测中显著减少知识遗忘，同时不影响新类别的学习。

Abstract: Incremental object detection (IOD) aims to continuously expand the capability
of a model to detect novel categories while preserving its performance on
previously learned ones. When adopting a transformer-based detection model to
perform IOD, catastrophic knowledge forgetting may inevitably occur, meaning
the detection performance on previously learned categories may severely
degenerate. Previous typical methods mainly rely on knowledge distillation (KD)
to mitigate the catastrophic knowledge forgetting of transformer-based
detection models. Specifically, they utilize Hungarian Matching to build a
correspondence between the queries of the last-phase and current-phase
detection models and align the classifier and regressor outputs between matched
queries to avoid knowledge forgetting. However, we observe that in IOD task,
Hungarian Matching is not a good choice. With Hungarian Matching, the query of
the current-phase model may match different queries of the last-phase model at
different iterations during KD. As a result, the knowledge encoded in each
query may be reshaped towards new categories, leading to the forgetting of
previously encoded knowledge of old categories. Based on our observations, we
propose a new distillation approach named Index-Aligned Query Distillation
(IAQD) for transformer-based IOD. Beyond using Hungarian Matching, IAQD
establishes a correspondence between queries of the previous and current phase
models that have the same index. Moreover, we perform index-aligned
distillation only on partial queries which are critical for the detection of
previous categories. In this way, IAQD largely preserves the previous semantic
and spatial encoding capabilities without interfering with the learning of new
categories. Extensive experiments on representative benchmarks demonstrate that
IAQD effectively mitigates knowledge forgetting, achieving new state-of-the-art
performance.

</details>


### [58] [Cost-Effective Active Labeling for Data-Efficient Cervical Cell Classification](https://arxiv.org/abs/2508.11340)
*Yuanlin Liu,Zhihan Zhou,Mingqiang Wei,Youyi Song*

Main category: cs.CV

TL;DR: 提出了一种主动标注方法，用于高效构建具有代表性的宫颈细胞分类训练数据集，显著降低人工成本。


<details>
  <summary>Details</summary>
Motivation: 现有自动分类方法需要大量代表性训练数据，人工成本高昂。

Method: 利用分类器对未标注图像的不确定性，选择最有价值的图像进行标注。

Result: 新算法有效提升了训练数据集的代表性，并显著减少了人工成本。

Conclusion: 该方法为高效宫颈细胞分类开辟了新途径。

Abstract: Information on the number and category of cervical cells is crucial for the
diagnosis of cervical cancer. However, existing classification methods capable
of automatically measuring this information require the training dataset to be
representative, which consumes an expensive or even unaffordable human cost. We
herein propose active labeling that enables us to construct a representative
training dataset using a much smaller human cost for data-efficient cervical
cell classification. This cost-effective method efficiently leverages the
classifier's uncertainty on the unlabeled cervical cell images to accurately
select images that are most beneficial to label. With a fast estimation of the
uncertainty, this new algorithm exhibits its validity and effectiveness in
enhancing the representative ability of the constructed training dataset. The
extensive empirical results confirm its efficacy again in navigating the usage
of human cost, opening the avenue for data-efficient cervical cell
classification.

</details>


### [59] [Semantically Guided Adversarial Testing of Vision Models Using Language Models](https://arxiv.org/abs/2508.11341)
*Katarzyna Filus,Jorge M. Cruz-Duarte*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义引导的对抗目标选择框架，利用跨模态知识转移来选择最相关和最不相关的标签，优于传统静态方法。


<details>
  <summary>Details</summary>
Motivation: 现有对抗攻击的目标标签选择策略通常依赖随机性、模型预测或静态语义资源，缺乏可解释性、可重复性或灵活性。

Method: 使用预训练的语言和视觉-语言模型（如BERT、TinyLLAMA和CLIP）作为相似性来源，选择与真实标签最相关和最不相关的标签。

Result: 实验表明，这些模型能提供实用的对抗目标，并优于静态词汇数据库（如WordNet），尤其是在远距离类别关系上。

Conclusion: 预训练模型适合构建可解释、标准化和可扩展的对抗基准，适用于不同架构和数据集。

Abstract: In targeted adversarial attacks on vision models, the selection of the target
label is a critical yet often overlooked determinant of attack success. This
target label corresponds to the class that the attacker aims to force the model
to predict. Now, existing strategies typically rely on randomness, model
predictions, or static semantic resources, limiting interpretability,
reproducibility, or flexibility. This paper then proposes a semantics-guided
framework for adversarial target selection using the cross-modal knowledge
transfer from pretrained language and vision-language models. We evaluate
several state-of-the-art models (BERT, TinyLLAMA, and CLIP) as similarity
sources to select the most and least semantically related labels with respect
to the ground truth, forming best- and worst-case adversarial scenarios. Our
experiments on three vision models and five attack methods reveal that these
models consistently render practical adversarial targets and surpass static
lexical databases, such as WordNet, particularly for distant class
relationships. We also observe that static testing of target labels offers a
preliminary assessment of the effectiveness of similarity sources, \textit{a
priori} testing. Our results corroborate the suitability of pretrained models
for constructing interpretable, standardized, and scalable adversarial
benchmarks across architectures and datasets.

</details>


### [60] [HOID-R1: Reinforcement Learning for Open-World Human-Object Interaction Detection Reasoning with Multimodal Large Language Model](https://arxiv.org/abs/2508.11350)
*Zhenhao Zhang,Hanqing Wang,Xiangyu Zeng,Ziyu Cheng,Jiaxin Liu,Haoyu Yan,Zhirui Liu,Kaiyang Ji,Tianxiang Gui,Ke Hu,Kangyi Chen,Yahao Fan,Mokai Pan*

Main category: cs.CV

TL;DR: HOID-R1是一个结合了链式思维引导的监督微调和组相对策略优化的HOI检测框架，通过强化学习提升性能，并在开放世界泛化中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有开放词汇HOI检测方法过度依赖大语言模型，忽视了其3D空间理解能力，HOID-R1旨在解决这一问题。

Method: 结合链式思维引导的监督微调（SFT）和组相对策略优化（GRPO），并引入MLLM-as-a-judge机制监督推理输出。

Result: HOID-R1在HOI检测基准上达到最先进性能，并在开放世界泛化中优于现有方法。

Conclusion: HOID-R1通过多模态对齐和推理监督，显著提升了HOI检测的性能和泛化能力。

Abstract: Understanding and recognizing human-object interaction (HOI) is a pivotal
application in AR/VR and robotics. Recent open-vocabulary HOI detection
approaches depend exclusively on large language models for richer textual
prompts, neglecting their inherent 3D spatial understanding capabilities. To
address this shortcoming, we introduce HOID-R1, the first HOI detection
framework that integrates chain-of-thought (CoT) guided supervised fine-tuning
(SFT) with group relative policy optimization (GRPO) within a reinforcement
learning (RL) paradigm. Specifically, we initially apply SFT to imbue the model
with essential reasoning capabilities, forcing the model to articulate its
thought process in the output. Subsequently, we integrate GRPO to leverage
multi-reward signals for policy optimization, thereby enhancing alignment
across diverse modalities. To mitigate hallucinations in the CoT reasoning, we
introduce an "MLLM-as-a-judge" mechanism that supervises the CoT outputs,
further improving generalization. Extensive experiments show that HOID-R1
achieves state-of-the-art performance on HOI detection benchmarks and
outperforms existing methods in open-world generalization to novel scenarios.

</details>


### [61] [Leveraging the RETFound foundation model for optic disc segmentation in retinal images](https://arxiv.org/abs/2508.11354)
*Zhenyi Zhao,Muthu Rama Krishnan Mookiah,Emanuele Trucco*

Main category: cs.CV

TL;DR: RETFound首次被用于视盘分割任务，表现优异，超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 探索RETFound在视盘分割任务中的应用，验证其作为通用模型的潜力。

Method: 通过少量任务特定样本训练头部网络，适配RETFound进行视盘分割。

Result: 在多个公开和私有数据集上达到约96% Dice分数，性能优越。

Conclusion: RETFound在视盘分割任务中表现卓越，支持通用模型替代任务特定架构的潜力。

Abstract: RETFound is a well-known foundation model (FM) developed for fundus camera
and optical coherence tomography images. It has shown promising performance
across multiple datasets in diagnosing diseases, both eye-specific and
systemic, from retinal images. However, to our best knowledge, it has not been
used for other tasks. We present the first adaptation of RETFound for optic
disc segmentation, a ubiquitous and foundational task in retinal image
analysis. The resulting segmentation system outperforms state-of-the-art,
segmentation-specific baseline networks after training a head with only a very
modest number of task-specific examples. We report and discuss results with
four public datasets, IDRID, Drishti-GS, RIM-ONE-r3, and REFUGE, and a private
dataset, GoDARTS, achieving about 96% Dice consistently across all datasets.
Overall, our method obtains excellent performance in internal verification,
domain generalization and domain adaptation, and exceeds most of the
state-of-the-art baseline results. We discuss the results in the framework of
the debate about FMs as alternatives to task-specific architectures. The code
is available at: [link to be added after the paper is accepted]

</details>


### [62] [Does the Skeleton-Recall Loss Really Work?](https://arxiv.org/abs/2508.11374)
*Devansh Arora,Nitin Kumar,Sukrit Gupta*

Main category: cs.CV

TL;DR: 论文分析了Skeleton Recall Loss（SRL）的理论梯度，并发现其在管状数据集上的表现并未超越传统基线模型。


<details>
  <summary>Details</summary>
Motivation: 探讨拓扑保持损失函数（如SRL）在图像分割中的实际效果，验证其是否如声称的那样优于传统方法。

Method: 通过理论分析SRL的梯度，并在多个管状数据集上比较SRL与传统模型的性能。

Result: SRL在管状数据集上的表现未超越传统基线模型。

Conclusion: 拓扑保持损失函数在实际应用中存在局限性，为开发更有效的分割模型提供了重要见解。

Abstract: Image segmentation is an important and widely performed task in computer
vision. Accomplishing effective image segmentation in diverse settings often
requires custom model architectures and loss functions. A set of models that
specialize in segmenting thin tubular structures are topology
preservation-based loss functions. These models often utilize a pixel
skeletonization process claimed to generate more precise segmentation masks of
thin tubes and better capture the structures that other models often miss. One
such model, Skeleton Recall Loss (SRL) proposed by Kirchhoff et al.~\cite
{kirchhoff2024srl}, was stated to produce state-of-the-art results on benchmark
tubular datasets. In this work, we performed a theoretical analysis of the
gradients for the SRL loss. Upon comparing the performance of the proposed
method on some of the tubular datasets (used in the original work, along with
some additional datasets), we found that the performance of SRL-based
segmentation models did not exceed traditional baseline models. By providing
both a theoretical explanation and empirical evidence, this work critically
evaluates the limitations of topology-based loss functions, offering valuable
insights for researchers aiming to develop more effective segmentation models
for complex tubular structures.

</details>


### [63] [Unified Knowledge Distillation Framework: Fine-Grained Alignment and Geometric Relationship Preservation for Deep Face Recognition](https://arxiv.org/abs/2508.11376)
*Durgesh Mishra,Rishabh Uikey*

Main category: cs.CV

TL;DR: 提出了一种结合实例级嵌入蒸馏和关系相似性蒸馏的统一知识蒸馏框架，显著提升了人脸识别模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在捕捉细粒度实例细节和复杂关系结构方面表现不佳，导致性能不理想。

Method: 结合实例级嵌入蒸馏（动态硬挖掘策略）和关系相似性蒸馏（记忆库机制和样本挖掘策略）。

Result: 在多个基准数据集上优于现有方法，甚至在某些情况下学生模型能超越教师模型。

Conclusion: 统一框架通过同时优化实例级对齐和关系保持，实现了更全面的知识蒸馏。

Abstract: Knowledge Distillation is crucial for optimizing face recognition models for
deployment in computationally limited settings, such as edge devices.
Traditional KD methods, such as Raw L2 Feature Distillation or Feature
Consistency loss, often fail to capture both fine-grained instance-level
details and complex relational structures, leading to suboptimal performance.
We propose a unified approach that integrates two novel loss functions,
Instance-Level Embedding Distillation and Relation-Based Pairwise Similarity
Distillation. Instance-Level Embedding Distillation focuses on aligning
individual feature embeddings by leveraging a dynamic hard mining strategy,
thereby enhancing learning from challenging examples. Relation-Based Pairwise
Similarity Distillation captures relational information through pairwise
similarity relationships, employing a memory bank mechanism and a sample mining
strategy. This unified framework ensures both effective instance-level
alignment and preservation of geometric relationships between samples, leading
to a more comprehensive distillation process. Our unified framework outperforms
state-of-the-art distillation methods across multiple benchmark face
recognition datasets, as demonstrated by extensive experimental evaluations.
Interestingly, when using strong teacher networks compared to the student, our
unified KD enables the student to even surpass the teacher's accuracy.

</details>


### [64] [G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior Integration](https://arxiv.org/abs/2508.11379)
*Ramil Khafizov,Artem Komarichev,Ruslan Rakhimov,Peter Wonka,Evgeny Burnaev*

Main category: cs.CV

TL;DR: G-CUT3R是一种改进的3D场景重建方法，通过整合先验信息提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖输入图像，而实际场景中常有多模态数据可用。

Method: 在CUT3R基础上轻量化改进，为每种模态设计专用编码器，通过零卷积融合特征。

Result: 在多基准测试中性能显著提升，兼容不同输入模态。

Conclusion: G-CUT3R能有效利用先验信息，灵活适应多种输入组合。

Abstract: We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene
reconstruction that enhances the CUT3R model by integrating prior information.
Unlike existing feed-forward methods that rely solely on input images, our
method leverages auxiliary data, such as depth, camera calibrations, or camera
positions, commonly available in real-world scenarios. We propose a lightweight
modification to CUT3R, incorporating a dedicated encoder for each modality to
extract features, which are fused with RGB image tokens via zero convolution.
This flexible design enables seamless integration of any combination of prior
information during inference. Evaluated across multiple benchmarks, including
3D reconstruction and other multi-view tasks, our approach demonstrates
significant performance improvements, showing its ability to effectively
utilize available priors while maintaining compatibility with varying input
modalities.

</details>


### [65] [RMFAT: Recurrent Multi-scale Feature Atmospheric Turbulence Mitigator](https://arxiv.org/abs/2508.11409)
*Zhiming Liu,Nantheera Anantrasirichai*

Main category: cs.CV

TL;DR: RMFAT是一种轻量级循环框架，用于高效恢复受大气湍流影响的视频，显著减少计算负担并提升时空一致性。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致视频质量严重下降，现有方法计算成本高，难以实时部署。

Method: 采用轻量级循环框架，仅需两帧输入，结合多尺度特征编码和解码以及时空扭曲模块。

Result: 在清晰度恢复（SSIM提升9%）和推理速度（运行时间减少四倍）上优于现有方法。

Conclusion: RMFAT适用于实时大气湍流抑制任务，兼具高效性和性能优势。

Abstract: Atmospheric turbulence severely degrades video quality by introducing
distortions such as geometric warping, blur, and temporal flickering, posing
significant challenges to both visual clarity and temporal consistency. Current
state-of-the-art methods are based on transformer and 3D architectures and
require multi-frame input, but their large computational cost and memory usage
limit real-time deployment, especially in resource-constrained scenarios. In
this work, we propose RMFAT: Recurrent Multi-scale Feature Atmospheric
Turbulence Mitigator, designed for efficient and temporally consistent video
restoration under AT conditions. RMFAT adopts a lightweight recurrent framework
that restores each frame using only two inputs at a time, significantly
reducing temporal window size and computational burden. It further integrates
multi-scale feature encoding and decoding with temporal warping modules at both
encoder and decoder stages to enhance spatial detail and temporal coherence.
Extensive experiments on synthetic and real-world atmospheric turbulence
datasets demonstrate that RMFAT not only outperforms existing methods in terms
of clarity restoration (with nearly a 9\% improvement in SSIM) but also
achieves significantly improved inference speed (more than a fourfold reduction
in runtime), making it particularly suitable for real-time atmospheric
turbulence suppression tasks.

</details>


### [66] [SelfAdapt: Unsupervised Domain Adaptation of Cell Segmentation Models](https://arxiv.org/abs/2508.11411)
*Fabian H. Reith,Jannik Franzen,Dinesh R. Palli,J. Lorenz Rumberger,Dagmar Kainmueller*

Main category: cs.CV

TL;DR: SelfAdapt是一种无需标注数据即可调整预训练细胞分割模型的方法，通过学生-教师增强一致性训练、L2-SP正则化和无标签停止标准，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 尽管通用模型（如Cellpose）在多样细胞数据上表现优异，但在与训练数据差异较大的领域性能下降，而监督微调需要标注数据，可能难以获取。

Method: 采用学生-教师增强一致性训练，结合L2-SP正则化和无标签停止标准，实现无监督模型适应。

Result: 在LiveCell和TissueNet数据集上，AP0.5相对基线Cellpose提升高达29.64%，且无监督适应还能进一步提升监督微调后的模型。

Conclusion: SelfAdapt是一种高效的无监督适应方法，已集成到Cellpose框架中，代码开源。

Abstract: Deep neural networks have become the go-to method for biomedical instance
segmentation. Generalist models like Cellpose demonstrate state-of-the-art
performance across diverse cellular data, though their effectiveness often
degrades on domains that differ from their training data. While supervised
fine-tuning can address this limitation, it requires annotated data that may
not be readily available. We propose SelfAdapt, a method that enables the
adaptation of pre-trained cell segmentation models without the need for labels.
Our approach builds upon student-teacher augmentation consistency training,
introducing L2-SP regularization and label-free stopping criteria. We evaluate
our method on the LiveCell and TissueNet datasets, demonstrating relative
improvements in AP0.5 of up to 29.64% over baseline Cellpose. Additionally, we
show that our unsupervised adaptation can further improve models that were
previously fine-tuned with supervision. We release SelfAdapt as an easy-to-use
extension of the Cellpose framework. The code for our method is publicly
available at https: //github.com/Kainmueller-Lab/self_adapt.

</details>


### [67] [Training-free Dimensionality Reduction via Feature Truncation: Enhancing Efficiency in Privacy-preserving Multi-Biometric Systems](https://arxiv.org/abs/2508.11419)
*Florian Bayer,Maximilian Russo,Christian Rathgeb*

Main category: cs.CV

TL;DR: 论文研究了多模态生物特征模板尺寸缩减对性能的影响，通过融合不同模态的特征向量，在保持生物识别准确性的同时显著减少了模板尺寸。


<details>
  <summary>Details</summary>
Motivation: 生物识别技术广泛应用，但模板隐私和安全性是关键问题。多模态融合可提升安全性，但现有方法计算量大，尤其是在同态加密下。

Method: 利用深度神经网络提取特征，通过降维减少同态加密下的计算量，实验基于虚拟多模态生物特征数据库。

Result: 多模态融合可将模板尺寸减少67%，且等错误率（EER）不劣于最佳单模态性能。

Conclusion: 降维和特征融合在保持安全性和准确性的同时，显著提升了同态加密下的计算效率。

Abstract: Biometric recognition is widely used, making the privacy and security of
extracted templates a critical concern. Biometric Template Protection schemes,
especially those utilizing Homomorphic Encryption, introduce significant
computational challenges due to increased workload. Recent advances in deep
neural networks have enabled state-of-the-art feature extraction for face,
fingerprint, and iris modalities. The ubiquity and affordability of biometric
sensors further facilitate multi-modal fusion, which can enhance security by
combining features from different modalities. This work investigates the
biometric performance of reduced multi-biometric template sizes. Experiments
are conducted on an in-house virtual multi-biometric database, derived from
DNN-extracted features for face, fingerprint, and iris, using the FRGC, MCYT,
and CASIA databases. The evaluated approaches are (i) explainable and
straightforward to implement under encryption, (ii) training-free, and (iii)
capable of generalization. Dimensionality reduction of feature vectors leads to
fewer operations in the Homomorphic Encryption (HE) domain, enabling more
efficient encrypted processing while maintaining biometric accuracy and
security at a level equivalent to or exceeding single-biometric recognition.
Our results demonstrate that, by fusing feature vectors from multiple
modalities, template size can be reduced by 67 % with no loss in Equal Error
Rate (EER) compared to the best-performing single modality.

</details>


### [68] [ImagiDrive: A Unified Imagination-and-Planning Framework for Autonomous Driving](https://arxiv.org/abs/2508.11428)
*Jingyu Li,Bozhou Zhang,Xin Jin,Jiankang Deng,Xiatian Zhu,Li Zhang*

Main category: cs.CV

TL;DR: ImagiDrive是一个新的端到端自动驾驶框架，结合了视觉语言模型（VLM）和驾驶世界模型（DWM），通过想象-规划循环提升驾驶决策。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶需要丰富的上下文理解和精确的预测推理，而VLM和DWM分别擅长行为预测和场景生成，但两者的结合尚未充分研究。

Method: 提出ImagiDrive框架，将VLM驱动的驾驶代理与DWM驱动的场景想象器结合，通过迭代优化驾驶决策。

Result: 在nuScenes和NAVSIM数据集上验证了ImagiDrive的鲁棒性和优越性。

Conclusion: ImagiDrive通过整合VLM和DWM的优势，显著提升了自动驾驶的性能。

Abstract: Autonomous driving requires rich contextual comprehension and precise
predictive reasoning to navigate dynamic and complex environments safely.
Vision-Language Models (VLMs) and Driving World Models (DWMs) have
independently emerged as powerful recipes addressing different aspects of this
challenge. VLMs provide interpretability and robust action prediction through
their ability to understand multi-modal context, while DWMs excel in generating
detailed and plausible future driving scenarios essential for proactive
planning. Integrating VLMs with DWMs is an intuitive, promising, yet
understudied strategy to exploit the complementary strengths of accurate
behavioral prediction and realistic scene generation. Nevertheless, this
integration presents notable challenges, particularly in effectively connecting
action-level decisions with high-fidelity pixel-level predictions and
maintaining computational efficiency. In this paper, we propose ImagiDrive, a
novel end-to-end autonomous driving framework that integrates a VLM-based
driving agent with a DWM-based scene imaginer to form a unified
imagination-and-planning loop. The driving agent predicts initial driving
trajectories based on multi-modal inputs, guiding the scene imaginer to
generate corresponding future scenarios. These imagined scenarios are
subsequently utilized to iteratively refine the driving agent's planning
decisions. To address efficiency and predictive accuracy challenges inherent in
this integration, we introduce an early stopping mechanism and a trajectory
selection strategy. Extensive experimental validation on the nuScenes and
NAVSIM datasets demonstrates the robustness and superiority of ImagiDrive over
previous alternatives under both open-loop and closed-loop conditions.

</details>


### [69] [Remove360: Benchmarking Residuals After Object Removal in 3D Gaussian Splatting](https://arxiv.org/abs/2508.11431)
*Simona Kocour,Assia Benbihi,Torsten Sattler*

Main category: cs.CV

TL;DR: 论文提出了一个评估框架和数据集Remove360，用于衡量3D高斯泼溅中物体移除后的语义残留问题，揭示了当前技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究物体移除后语义信息的残留问题对于隐私保护和可编辑场景表示至关重要。

Method: 通过创建Remove360数据集和评估框架，实验分析了多种室内外场景中物体移除后的语义残留。

Result: 实验表明，当前方法在移除物体后仍可能保留语义信息，且下游模型能推断出被移除的内容。

Conclusion: 当前3D物体移除技术存在局限性，需要更鲁棒的解决方案来处理真实世界的复杂性。

Abstract: Understanding what semantic information persists after object removal is
critical for privacy-preserving 3D reconstruction and editable scene
representations. In this work, we introduce a novel benchmark and evaluation
framework to measure semantic residuals, the unintended semantic traces left
behind, after object removal in 3D Gaussian Splatting. We conduct experiments
across a diverse set of indoor and outdoor scenes, showing that current methods
can preserve semantic information despite the absence of visual geometry. We
also release Remove360, a dataset of pre/post-removal RGB images and
object-level masks captured in real-world environments. While prior datasets
have focused on isolated object instances, Remove360 covers a broader and more
complex range of indoor and outdoor scenes, enabling evaluation of object
removal in the context of full-scene representations. Given ground truth images
of a scene before and after object removal, we assess whether we can truly
eliminate semantic presence, and if downstream models can still infer what was
removed. Our findings reveal critical limitations in current 3D object removal
techniques and underscore the need for more robust solutions capable of
handling real-world complexity. The evaluation framework is available at
github.com/spatial-intelligence-ai/Remove360.git. Data are available at
huggingface.co/datasets/simkoc/Remove360.

</details>


### [70] [MM-R1: Unleashing the Power of Unified Multimodal Large Language Models for Personalized Image Generation](https://arxiv.org/abs/2508.11433)
*Qian Liang,Yujia Wu,Kuncheng Li,Jiwei Wei,Shiyuan He,Jinyu Guo,Ning Xie*

Main category: cs.CV

TL;DR: MM-R1框架通过跨模态思维链推理策略，解锁统一MLLMs在个性化图像生成中的潜力，无需针对每个新主题进行数据密集型微调。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs方法通常针对特定主题，需要大量数据微调，限制了可扩展性。

Method: 采用跨模态思维链推理（X-CoT）和分组奖励近端策略优化（GRPO），将个性化建模为视觉推理与生成过程。

Result: 实验表明，MM-R1以零样本方式生成高主题保真度和强文本对齐的图像。

Conclusion: MM-R1显著提升了统一MLLMs在个性化图像生成中的能力，解决了现有方法的可扩展性问题。

Abstract: Multimodal Large Language Models (MLLMs) with unified architectures excel
across a wide range of vision-language tasks, yet aligning them with
personalized image generation remains a significant challenge. Existing methods
for MLLMs are frequently subject-specific, demanding a data-intensive
fine-tuning process for every new subject, which limits their scalability. In
this paper, we introduce MM-R1, a framework that integrates a cross-modal
Chain-of-Thought (X-CoT) reasoning strategy to unlock the inherent potential of
unified MLLMs for personalized image generation. Specifically, we structure
personalization as an integrated visual reasoning and generation process: (1)
grounding subject concepts by interpreting and understanding user-provided
images and contextual cues, and (2) generating personalized images conditioned
on both the extracted subject representations and user prompts. To further
enhance the reasoning capability, we adopt Grouped Reward Proximal Policy
Optimization (GRPO) to explicitly align the generation. Experiments demonstrate
that MM-R1 unleashes the personalization capability of unified MLLMs to
generate images with high subject fidelity and strong text alignment in a
zero-shot manner.

</details>


### [71] [Inside Knowledge: Graph-based Path Generation with Explainable Data Augmentation and Curriculum Learning for Visual Indoor Navigation](https://arxiv.org/abs/2508.11446)
*Daniel Airinei,Elena Burceanu,Marius Leordeanu*

Main category: cs.CV

TL;DR: 提出了一种仅依赖视觉输入的高效实时室内导航方法，基于深度学习，无需特殊传感器或地图知识。


<details>
  <summary>Details</summary>
Motivation: 解决室内导航中GPS信号弱、现有解决方案复杂且难以部署的问题。

Method: 采用基于图的路径生成方法，结合可解释的数据增强和课程学习，实现自动化的数据收集、标注和训练。

Result: 创建了一个大型购物中心视频数据集，并开发了易于使用的Android应用。

Conclusion: 该方法仅依赖视觉输入，无需额外硬件或网络，具有高效和易部署的优势。

Abstract: Indoor navigation is a difficult task, as it generally comes with poor GPS
access, forcing solutions to rely on other sources of information. While
significant progress continues to be made in this area, deployment to
production applications is still lacking, given the complexity and additional
requirements of current solutions. Here, we introduce an efficient, real-time
and easily deployable deep learning approach, based on visual input only, that
can predict the direction towards a target from images captured by a mobile
device. Our technical approach, based on a novel graph-based path generation
method, combined with explainable data augmentation and curriculum learning,
includes contributions that make the process of data collection, annotation and
training, as automatic as possible, efficient and robust. On the practical
side, we introduce a novel largescale dataset, with video footage inside a
relatively large shopping mall, in which each frame is annotated with the
correct next direction towards different specific target destinations.
Different from current methods, ours relies solely on vision, avoiding the need
of special sensors, additional markers placed along the path, knowledge of the
scene map or internet access. We also created an easy to use application for
Android, which we plan to make publicly available. We make all our data and
code available along with visual demos on our project site

</details>


### [72] [Data-Driven Deepfake Image Detection Method -- The 2024 Global Deepfake Image Detection Challenge](https://arxiv.org/abs/2508.11464)
*Xiaoya Zhu,Yibing Nan,Shiguo Lian*

Main category: cs.CV

TL;DR: 本文提出了一种基于Swin Transformer V2-B分类网络的方法，结合在线数据增强和离线样本生成技术，用于检测Deepfake图像，并在竞赛中取得了优异成绩。


<details>
  <summary>Details</summary>
Motivation: 随着AI技术的快速发展，Deepfake技术带来了大量AI生成内容，同时也对数字安全提出了前所未有的挑战。因此，开发有效的Deepfake检测方法变得至关重要。

Method: 采用Swin Transformer V2-B分类网络，结合在线数据增强和离线样本生成技术，以丰富训练样本的多样性并提升模型的泛化能力。

Result: 该方法在Deepfake图像检测竞赛中获得了卓越奖。

Conclusion: 通过结合先进的分类网络和数据增强技术，本文提出的方法在Deepfake检测任务中表现优异，为数字安全提供了有效解决方案。

Abstract: With the rapid development of technology in the field of AI, deepfake
technology has emerged as a double-edged sword. It has not only created a large
amount of AI-generated content but also posed unprecedented challenges to
digital security. The task of the competition is to determine whether a face
image is a Deepfake image and output its probability score of being a Deepfake
image. In the image track competition, our approach is based on the Swin
Transformer V2-B classification network. And online data augmentation and
offline sample generation methods are employed to enrich the diversity of
training samples and increase the generalization ability of the model. Finally,
we got the award of excellence in Deepfake image detection.

</details>


### [73] [CoFi: A Fast Coarse-to-Fine Few-Shot Pipeline for Glomerular Basement Membrane Segmentation](https://arxiv.org/abs/2508.11469)
*Hongjin Fang,Daniel Reisenbüchler,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: CoFi是一种快速高效的粗到细少样本分割方法，用于电子显微镜图像中的肾小球基底膜分割，显著减少标注负担并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 传统监督深度学习方法依赖大量像素级标注，不适用于临床工作流程；少样本学习难以捕捉肾小球基底膜的精细结构。

Method: CoFi首先用三张标注图像训练轻量级网络生成粗分割掩码，再通过形态学修剪生成高质量点提示，引导SAM细化分割。

Result: Dice系数达74.54%，推理速度1.9 FPS，显著减轻标注和计算负担。

Conclusion: CoFi在肾病理学研究和临床应用中具有潜力，代码已开源。

Abstract: Accurate segmentation of the glomerular basement membrane (GBM) in electron
microscopy (EM) images is fundamental for quantifying membrane thickness and
supporting the diagnosis of various kidney diseases. While supervised deep
learning approaches achieve high segmentation accuracy, their reliance on
extensive pixel-level annotation renders them impractical for clinical
workflows. Few-shot learning can reduce this annotation burden but often
struggles to capture the fine structural details necessary for GBM analysis. In
this study, we introduce CoFi, a fast and efficient coarse-to-fine few-shot
segmentation pipeline designed for GBM delineation in EM images. CoFi first
trains a lightweight neural network using only three annotated images to
produce an initial coarse segmentation mask. This mask is then automatically
processed to generate high-quality point prompts with morphology-aware pruning,
which are subsequently used to guide SAM in refining the segmentation. The
proposed method achieved exceptional GBM segmentation performance, with a Dice
coefficient of 74.54% and an inference speed of 1.9 FPS. We demonstrate that
CoFi not only alleviates the annotation and computational burdens associated
with conventional methods, but also achieves accurate and reliable segmentation
results. The pipeline's speed and annotation efficiency make it well-suited for
research and hold strong potential for clinical applications in renal
pathology. The pipeline is publicly available at:
https://github.com/ddrrnn123/CoFi.

</details>


### [74] [TACR-YOLO: A Real-time Detection Framework for Abnormal Human Behaviors Enhanced with Coordinate and Task-Aware Representations](https://arxiv.org/abs/2508.11478)
*Xinyi Yin,Wenbo Yuan,Xuecheng Wu,Liangyu Fu,Danlei Huang*

Main category: cs.CV

TL;DR: TACR-YOLO是一种新型实时框架，用于特殊场景下的异常行为检测，通过改进小物体检测、任务冲突和多尺度融合，实现了91.92%的mAP。


<details>
  <summary>Details</summary>
Motivation: 特殊场景下的异常行为检测需求日益增长，但现有YOLO方法在小物体检测、任务冲突和多尺度融合方面存在不足。

Method: 提出TACR-YOLO框架，包括坐标注意力模块、任务感知注意力模块和强化颈部网络，并优化锚框大小和损失函数。

Result: 在PABD数据集上达到91.92%的mAP，速度和鲁棒性表现优异。

Conclusion: TACR-YOLO为特殊场景下的异常行为检测提供了新思路，推动了该领域的进展。

Abstract: Abnormal Human Behavior Detection (AHBD) under special scenarios is becoming
increasingly crucial. While YOLO-based detection methods excel in real-time
tasks, they remain hindered by challenges including small objects, task
conflicts, and multi-scale fusion in AHBD. To tackle them, we propose
TACR-YOLO, a new real-time framework for AHBD. We introduce a Coordinate
Attention Module to enhance small object detection, a Task-Aware Attention
Module to deal with classification-regression conflicts, and a Strengthen Neck
Network for refined multi-scale fusion, respectively. In addition, we optimize
Anchor Box sizes using K-means clustering and deploy DIoU-Loss to improve
bounding box regression. The Personnel Anomalous Behavior Detection (PABD)
dataset, which includes 8,529 samples across four behavior categories, is also
presented. Extensive experimental results indicate that TACR-YOLO achieves
91.92% mAP on PABD, with competitive speed and robustness. Ablation studies
highlight the contribution of each improvement. This work provides new insights
for abnormal behavior detection under special scenarios, advancing its
progress.

</details>


### [75] [OpenConstruction: A Systematic Synthesis of Open Visual Datasets for Data-Centric Artificial Intelligence in Construction Monitoring](https://arxiv.org/abs/2508.11482)
*Ruoxin Xiong,Yanyu Wang,Jiannan Cai,Kaijian Liu,Yuansheng Zhu,Pingbo Tang,Nora El-Gohary*

Main category: cs.CV

TL;DR: 该研究系统回顾了建筑行业中用于AI和ML应用的视觉数据集，提出了一个分类框架，并创建了开源目录OpenConstruction，同时指出了现有数据集的局限性并提出了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: 建筑行业越来越依赖视觉数据支持AI和ML应用，但现有数据集在规模、模态、标注质量和代表性上差异很大，缺乏系统性分类，限制了AI应用的进一步发展。

Method: 通过系统检索学术数据库和开放数据平台，收集了51个公开视觉数据集，并采用结构化数据模式进行分类。

Result: 研究提出了OpenConstruction开源目录，总结了数据集的分类和局限性，并基于FAIR原则提出了未来数据基础设施的路线图。

Conclusion: 该研究为建筑行业数据驱动的解决方案提供了支持，并指出了未来发展的战略重点。

Abstract: The construction industry increasingly relies on visual data to support
Artificial Intelligence (AI) and Machine Learning (ML) applications for site
monitoring. High-quality, domain-specific datasets, comprising images, videos,
and point clouds, capture site geometry and spatiotemporal dynamics, including
the location and interaction of objects, workers, and materials. However,
despite growing interest in leveraging visual datasets, existing resources vary
widely in sizes, data modalities, annotation quality, and representativeness of
real-world construction conditions. A systematic review to categorize their
data characteristics and application contexts is still lacking, limiting the
community's ability to fully understand the dataset landscape, identify
critical gaps, and guide future directions toward more effective, reliable, and
scalable AI applications in construction. To address this gap, this study
conducts an extensive search of academic databases and open-data platforms,
yielding 51 publicly available visual datasets that span the 2005-2024 period.
These datasets are categorized using a structured data schema covering (i) data
fundamentals (e.g., size and license), (ii) data modalities (e.g., RGB and
point cloud), (iii) annotation frameworks (e.g., bounding boxes), and (iv)
downstream application domains (e.g., progress tracking). This study
synthesizes these findings into an open-source catalog, OpenConstruction,
supporting data-driven method development. Furthermore, the study discusses
several critical limitations in the existing construction dataset landscape and
presents a roadmap for future data infrastructure anchored in the Findability,
Accessibility, Interoperability, and Reusability (FAIR) principles. By
reviewing the current landscape and outlining strategic priorities, this study
supports the advancement of data-centric solutions in the construction sector.

</details>


### [76] [CineTrans: Learning to Generate Videos with Cinematic Transitions via Masked Diffusion Models](https://arxiv.org/abs/2508.11484)
*Xiaoxue Wu,Bingjie Gao,Yu Qiao,Yaohui Wang,Xinyuan Chen*

Main category: cs.CV

TL;DR: CineTrans是一个新颖的框架，用于生成具有电影风格过渡的多镜头视频，通过注意力映射和掩码控制机制实现稳定的过渡。


<details>
  <summary>Details</summary>
Motivation: 尽管视频合成技术取得了显著进展，但多镜头视频生成的研究仍处于初级阶段，现有模型的过渡能力有限且不稳定。

Method: 构建了Cine250K数据集，分析了视频扩散模型的注意力映射与镜头边界的对应关系，设计了掩码控制机制，并在训练中微调。

Result: CineTrans生成的视频在过渡控制、时间一致性和整体质量上显著优于现有基线。

Conclusion: CineTrans通过创新的控制机制和数据集，成功实现了高质量的多镜头视频生成。

Abstract: Despite significant advances in video synthesis, research into multi-shot
video generation remains in its infancy. Even with scaled-up models and massive
datasets, the shot transition capabilities remain rudimentary and unstable,
largely confining generated videos to single-shot sequences. In this work, we
introduce CineTrans, a novel framework for generating coherent multi-shot
videos with cinematic, film-style transitions. To facilitate insights into the
film editing style, we construct a multi-shot video-text dataset Cine250K with
detailed shot annotations. Furthermore, our analysis of existing video
diffusion models uncovers a correspondence between attention maps in the
diffusion model and shot boundaries, which we leverage to design a mask-based
control mechanism that enables transitions at arbitrary positions and transfers
effectively in a training-free setting. After fine-tuning on our dataset with
the mask mechanism, CineTrans produces cinematic multi-shot sequences while
adhering to the film editing style, avoiding unstable transitions or naive
concatenations. Finally, we propose specialized evaluation metrics for
transition control, temporal consistency and overall quality, and demonstrate
through extensive experiments that CineTrans significantly outperforms existing
baselines across all criteria.

</details>


### [77] [Automated Building Heritage Assessment Using Street-Level Imagery](https://arxiv.org/abs/2508.11486)
*Kristina Dabrock,Tim Johansson,Anna Donarelli,Mikael Mangold,Noah Pflugradt,Jann Michael Weinand,Jochen Linßen*

Main category: cs.CV

TL;DR: 利用GPT和机器学习模型结合建筑登记数据，识别建筑的文化遗产价值，支持节能改造。


<details>
  <summary>Details</summary>
Motivation: 传统建筑文化遗产价值识别方法成本高且耗时，AI工具可提高效率。

Method: 使用GPT分析建筑立面图像的文化遗产价值，结合建筑登记数据训练机器学习模型。

Result: 模型在专家验证的库存上表现良好，结合数据时F1得分为0.71，仅用GPT数据时为0.60。

Conclusion: 该方法可提升数据库质量，支持节能改造中文化遗产价值的综合考虑。

Abstract: Detailed data is required to quantify energy conservation measures in
buildings, such as envelop retrofits, without compromising cultural heritage.
Novel artificial intelligence tools may improve efficiency in identifying
heritage values in buildings compared to costly and time-consuming traditional
inventories. In this study, the large language model GPT was used to detect
various aspects of cultural heritage value in fa\c{c}ade images. Using this
data and building register data as features, machine learning models were
trained to classify multi-family and non-residential buildings in Stockholm,
Sweden. Validation against an expert-created inventory shows a macro F1-score
of 0.71 using a combination of register data and features retrieved from GPT,
and a score of 0.60 using only GPT-derived data. The presented methodology can
contribute to a higher-quality database and thus support careful energy
efficiency measures and integrated consideration of heritage value in
large-scale energetic refurbishment scenarios.

</details>


### [78] [Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving](https://arxiv.org/abs/2508.11488)
*Bozhou Zhang,Jingyu Li,Nan Song,Li Zhang*

Main category: cs.CV

TL;DR: VeteranAD是一种端到端自动驾驶框架，通过将感知整合到规划过程中，实现了规划导向的优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法采用感知-规划顺序执行的方式，但感知与规划的分离可能导致优化不足。VeteranAD通过感知-规划一体化设计，实现更精准的规划导向感知。

Method: 提出VeteranAD框架，结合多模式锚定轨迹作为规划先验，感知模块专注于沿轨迹的交通元素，采用自回归策略逐步预测未来轨迹。

Result: 在NAVSIM和Bench2Drive数据集上，VeteranAD实现了最先进的性能。

Conclusion: VeteranAD通过感知-规划一体化设计，显著提升了端到端自动驾驶的准确性和可靠性。

Abstract: End-to-end autonomous driving has achieved remarkable advancements in recent
years. Existing methods primarily follow a perception-planning paradigm, where
perception and planning are executed sequentially within a fully differentiable
framework for planning-oriented optimization. We further advance this paradigm
through a perception-in-plan framework design, which integrates perception into
the planning process. This design facilitates targeted perception guided by
evolving planning objectives over time, ultimately enhancing planning
performance. Building on this insight, we introduce VeteranAD, a coupled
perception and planning framework for end-to-end autonomous driving. By
incorporating multi-mode anchored trajectories as planning priors, the
perception module is specifically designed to gather traffic elements along
these trajectories, enabling comprehensive and targeted perception. Planning
trajectories are then generated based on both the perception results and the
planning priors. To make perception fully serve planning, we adopt an
autoregressive strategy that progressively predicts future trajectories while
focusing on relevant regions for targeted perception at each step. With this
simple yet effective design, VeteranAD fully unleashes the potential of
planning-oriented end-to-end methods, leading to more accurate and reliable
driving behavior. Extensive experiments on the NAVSIM and Bench2Drive datasets
demonstrate that our VeteranAD achieves state-of-the-art performance.

</details>


### [79] [Hierarchical Graph Feature Enhancement with Adaptive Frequency Modulation for Visual Recognition](https://arxiv.org/abs/2508.11497)
*Feiyue Zhao,Zhichao Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为HGFE的新框架，通过将图推理集成到CNN中，增强结构感知和特征表示能力。


<details>
  <summary>Details</summary>
Motivation: 传统CNN依赖规则网格结构，限制了其对复杂拓扑关系和非局部语义的建模能力。

Method: HGFE构建了两个互补的图结构层次：局部空间依赖的窗口内图卷积和全局语义关系的窗口间超节点交互，并引入了自适应频率调制模块。

Result: 在多个数据集上的实验验证了HGFE在提升结构表示和整体识别性能方面的有效性。

Conclusion: HGFE是一种轻量级、端到端可训练的模块，可无缝集成到标准CNN骨干网络中，显著提升了视觉任务的性能。

Abstract: Convolutional neural networks (CNNs) have
  demonstrated strong performance in visual recognition tasks,
  but their inherent reliance on regular grid structures limits
  their capacity to model complex topological relationships and
  non-local semantics within images. To address this limita tion, we propose
the hierarchical graph feature enhancement
  (HGFE), a novel framework that integrates graph-based rea soning into CNNs to
enhance both structural awareness and
  feature representation. HGFE builds two complementary levels
  of graph structures: intra-window graph convolution to cap ture local spatial
dependencies and inter-window supernode
  interactions to model global semantic relationships. Moreover,
  we introduce an adaptive frequency modulation module that
  dynamically balances low-frequency and high-frequency signal
  propagation, preserving critical edge and texture information
  while mitigating over-smoothing. The proposed HGFE module
  is lightweight, end-to-end trainable, and can be seamlessly
  integrated into standard CNN backbone networks. Extensive
  experiments on CIFAR-100 (classification), PASCAL VOC,
  and VisDrone (detection), as well as CrackSeg and CarParts
  (segmentation), validated the effectiveness of the HGFE in
  improving structural representation and enhancing overall
  recognition performance.

</details>


### [80] [Handwritten Text Recognition of Historical Manuscripts Using Transformer-Based Models](https://arxiv.org/abs/2508.11499)
*Erez Meoded*

Main category: cs.CV

TL;DR: 该研究应用TrOCR模型处理16世纪拉丁手稿，通过针对性图像预处理和数据增强技术，显著提升了手写文本识别的性能。


<details>
  <summary>Details</summary>
Motivation: 历史手写文本识别（HTR）对档案文献的文化和学术价值至关重要，但数字化常因转录稀缺、语言变异和多样化的书写风格而受阻。

Method: 研究采用TrOCR模型，结合图像预处理和四种新颖的数据增强方法，并评估集成学习策略。

Result: 最佳单模型增强（Elastic）的字符错误率（CER）为1.86，而前5投票集成模型的CER为1.60，相对改进显著。

Conclusion: 领域特定的数据增强和集成策略显著提升了历史手稿的HTR性能。

Abstract: Historical handwritten text recognition (HTR) is essential for unlocking the
cultural and scholarly value of archival documents, yet digitization is often
hindered by scarce transcriptions, linguistic variation, and highly diverse
handwriting styles. In this study, we apply TrOCR, a state-of-the-art
transformer-based HTR model, to 16th-century Latin manuscripts authored by
Rudolf Gwalther. We investigate targeted image preprocessing and a broad suite
of data augmentation techniques, introducing four novel augmentation methods
designed specifically for historical handwriting characteristics. We also
evaluate ensemble learning approaches to leverage the complementary strengths
of augmentation-trained models. On the Gwalther dataset, our best single-model
augmentation (Elastic) achieves a Character Error Rate (CER) of 1.86, while a
top-5 voting ensemble achieves a CER of 1.60 - representing a 50% relative
improvement over the best reported TrOCR_BASE result and a 42% improvement over
the previous state of the art. These results highlight the impact of
domain-specific augmentations and ensemble strategies in advancing HTR
performance for historical manuscripts.

</details>


### [81] [AIM: Amending Inherent Interpretability via Self-Supervised Masking](https://arxiv.org/abs/2508.11502)
*Eyad Alshami,Shashank Agnihotri,Bernt Schiele,Margret Keuper*

Main category: cs.CV

TL;DR: AIM方法通过自监督掩码促进DNN使用真实特征而非虚假特征，提升模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决DNN常同时使用真实和虚假特征的问题，无需额外标注即可提升模型的可解释性和性能。

Method: 利用多阶段编码特征引导自监督、样本特定的特征掩码过程。

Result: 在多个数据集上验证，AIM显著提升可解释性（EPG评分）和准确性。

Conclusion: AIM能有效促进模型使用真实特征，提升泛化能力和人类对齐的可解释性。

Abstract: It has been observed that deep neural networks (DNNs) often use both genuine
as well as spurious features. In this work, we propose "Amending Inherent
Interpretability via Self-Supervised Masking" (AIM), a simple yet interestingly
effective method that promotes the network's utilization of genuine features
over spurious alternatives without requiring additional annotations. In
particular, AIM uses features at multiple encoding stages to guide a
self-supervised, sample-specific feature-masking process. As a result, AIM
enables the training of well-performing and inherently interpretable models
that faithfully summarize the decision process. We validate AIM across a
diverse range of challenging datasets that test both out-of-distribution
generalization and fine-grained visual understanding. These include
general-purpose classification benchmarks such as ImageNet100, HardImageNet,
and ImageWoof, as well as fine-grained classification datasets such as
Waterbirds, TravelingBirds, and CUB-200. AIM demonstrates significant dual
benefits: interpretability improvements, as measured by the Energy Pointing
Game (EPG) score, and accuracy gains over strong baselines. These consistent
gains across domains and architectures provide compelling evidence that AIM
promotes the use of genuine and meaningful features that directly contribute to
improved generalization and human-aligned interpretability.

</details>


### [82] [A Real-time Concrete Crack Detection and Segmentation Model Based on YOLOv11](https://arxiv.org/abs/2508.11517)
*Shaoze Huang,Qi Liu,Chao Chen,Yuhang Chen*

Main category: cs.CV

TL;DR: 提出YOLOv11-KW-TA-FP模型，用于混凝土裂缝检测与分割，通过动态卷积、三重注意力机制和FP-IoU损失函数优化性能。


<details>
  <summary>Details</summary>
Motivation: 长江三角洲地区基础设施老化加速，传统检测方法效率低，现有深度学习模型对小目标裂缝检测效果不佳。

Method: 基于YOLOv11n架构，集成动态KernelWarehouse卷积、三重注意力机制和FP-IoU损失函数的三阶段优化框架。

Result: 模型精度达91.3%，召回率76.6%，mAP@50为86.4%，在数据稀缺和噪声干扰下表现稳定。

Conclusion: 该研究为自动化基础设施检测提供了高效解决方案，具有显著工程实用价值。

Abstract: Accelerated aging of transportation infrastructure in the rapidly developing
Yangtze River Delta region necessitates efficient concrete crack detection, as
crack deterioration critically compromises structural integrity and regional
economic growth. To overcome the limitations of inefficient manual inspection
and the suboptimal performance of existing deep learning models, particularly
for small-target crack detection within complex backgrounds, this paper
proposes YOLOv11-KW-TA-FP, a multi-task concrete crack detection and
segmentation model based on the YOLOv11n architecture. The proposed model
integrates a three-stage optimization framework: (1) Embedding dynamic
KernelWarehouse convolution (KWConv) within the backbone network to enhance
feature representation through a dynamic kernel sharing mechanism; (2)
Incorporating a triple attention mechanism (TA) into the feature pyramid to
strengthen channel-spatial interaction modeling; and (3) Designing an FP-IoU
loss function to facilitate adaptive bounding box regression penalization.
Experimental validation demonstrates that the enhanced model achieves
significant performance improvements over the baseline, attaining 91.3%
precision, 76.6% recall, and 86.4% mAP@50. Ablation studies confirm the
synergistic efficacy of the proposed modules. Furthermore, robustness tests
indicate stable performance under conditions of data scarcity and noise
interference. This research delivers an efficient computer vision solution for
automated infrastructure inspection, exhibiting substantial practical
engineering value.

</details>


### [83] [Multi-State Tracker: Enhancing Efficient Object Tracking via Multi-State Specialization and Interaction](https://arxiv.org/abs/2508.11531)
*Shilei Wang,Gong Cheng,Pujian Lai,Dong Gao,Junwei Han*

Main category: cs.CV

TL;DR: 提出了一种高效的多状态跟踪器（MST），通过轻量级的状态增强和交互模块，显著提升了跟踪的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有高效跟踪器因计算复杂度和参数减少导致特征表示能力不足，限制了其在复杂环境中的跟踪性能。

Method: MST采用多状态生成（MSG）、状态特定增强（SSE）和跨状态交互（CSI）模块，通过轻量级设计增强特征表示。

Result: MST在多个数据集上表现优异，计算开销仅0.1 GFLOPs和0.66 M参数，在GOT-10K数据集上AO得分提升4.5%。

Conclusion: MST通过高效的多状态特征增强和交互，显著提升了跟踪性能，同时保持了低计算开销。

Abstract: Efficient trackers achieve faster runtime by reducing computational
complexity and model parameters. However, this efficiency often compromises the
expense of weakened feature representation capacity, thus limiting their
ability to accurately capture target states using single-layer features. To
overcome this limitation, we propose Multi-State Tracker (MST), which utilizes
highly lightweight state-specific enhancement (SSE) to perform specialized
enhancement on multi-state features produced by multi-state generation (MSG)
and aggregates them in an interactive and adaptive manner using cross-state
interaction (CSI). This design greatly enhances feature representation while
incurring minimal computational overhead, leading to improved tracking
robustness in complex environments. Specifically, the MSG generates multiple
state representations at multiple stages during feature extraction, while SSE
refines them to highlight target-specific features. The CSI module facilitates
information exchange between these states and ensures the integration of
complementary features. Notably, the introduced SSE and CSI modules adopt a
highly lightweight hidden state adaptation-based state space duality (HSA-SSD)
design, incurring only 0.1 GFLOPs in computation and 0.66 M in parameters.
Experimental results demonstrate that MST outperforms all previous efficient
trackers across multiple datasets, significantly improving tracking accuracy
and robustness. In particular, it shows excellent runtime performance, with an
AO score improvement of 4.5\% over the previous SOTA efficient tracker HCAT on
the GOT-10K dataset. The code is available at https://github.com/wsumel/MST.

</details>


### [84] [An Efficient Medical Image Classification Method Based on a Lightweight Improved ConvNeXt-Tiny Architecture](https://arxiv.org/abs/2508.11532)
*Jingsong Xia,Yue Yin,Xiuhan Li*

Main category: cs.CV

TL;DR: 提出了一种基于改进ConvNeXt-Tiny架构的医学图像分类方法，通过结构优化和损失函数设计提升性能并降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的计算环境中实现高效高精度医学图像分类的挑战。

Method: 引入双全局池化特征融合策略和轻量级通道注意力模块SEVector，并加入特征平滑损失函数。

Result: 在CPU条件下，10个训练周期内测试集分类准确率达89.10%，损失值稳定收敛。

Conclusion: 该方法在资源有限环境中有效提升医学图像分类性能，为模型部署提供了可行方案。

Abstract: Intelligent analysis of medical imaging plays a crucial role in assisting
clinical diagnosis. However, achieving efficient and high-accuracy image
classification in resource-constrained computational environments remains
challenging. This study proposes a medical image classification method based on
an improved ConvNeXt-Tiny architecture. Through structural optimization and
loss function design, the proposed method enhances feature extraction
capability and classification performance while reducing computational
complexity. Specifically, the method introduces a dual global pooling (Global
Average Pooling and Global Max Pooling) feature fusion strategy into the
ConvNeXt-Tiny backbone to simultaneously preserve global statistical features
and salient response information. A lightweight channel attention module,
termed Squeeze-and-Excitation Vector (SEVector), is designed to improve the
adaptive allocation of channel weights while minimizing parameter overhead.
Additionally, a Feature Smoothing Loss is incorporated into the loss function
to enhance intra-class feature consistency and suppress intra-class variance.
Under CPU-only conditions (8 threads), the method achieves a maximum
classification accuracy of 89.10% on the test set within 10 training epochs,
exhibiting a stable convergence trend in loss values. Experimental results
demonstrate that the proposed method effectively improves medical image
classification performance in resource-limited settings, providing a feasible
and efficient solution for the deployment and promotion of medical imaging
analysis models.

</details>


### [85] [Reinforcing Video Reasoning Segmentation to Think Before It Segments](https://arxiv.org/abs/2508.11538)
*Sitong Gong,Lu Zhang,Yunzhi Zhuge,Xu Jia,Pingping Zhang,Huchuan Lu*

Main category: cs.CV

TL;DR: Veason-R1是一种专用于视频推理分割（VRS）的大型视觉语言模型，通过结构化推理和强化学习优化，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在推理过程中缺乏可解释性，且由于时空推理不足导致性能不佳。

Method: 采用Group Relative Policy Optimization（GRPO）和Chain-of-Thought（CoT）初始化训练模型，结合高质量CoT数据优化推理链。

Result: 在多个基准测试中达到最先进性能，显著超越现有方法（如ReVOS和ReasonVOS），并表现出对幻觉的鲁棒性。

Conclusion: Veason-R1通过结构化推理和强化学习优化，显著提升了视频推理分割的性能和鲁棒性。

Abstract: Video reasoning segmentation (VRS) endeavors to delineate referred objects in
videos guided by implicit instructions that encapsulate human intent and
temporal logic. Previous approaches leverage large vision language models
(LVLMs) to encode object semantics into <SEG> tokens for mask prediction.
However, this paradigm suffers from limited interpretability during inference
and suboptimal performance due to inadequate spatiotemporal reasoning. Drawing
inspiration from seminal breakthroughs in reinforcement learning, we introduce
Veason-R1, a specialized LVLM for VRS that emphasizes structured reasoning in
segmentation. Veason-R1 is trained through Group Relative Policy Optimization
(GRPO) augmented with Chain-of-Thought (CoT) initialization. To begin with, we
curate high-quality CoT training data to instill structured reasoning
trajectories, bridging video-level semantics and frame-level spatial grounding,
yielding the supervised fine-tuned model Veason-SFT. Subsequently, GRPO
fine-tuning encourages efficient exploration of the reasoning space by
optimizing reasoning chains. To this end, we incorporate a holistic reward
mechanism that synergistically enhances spatial alignment and temporal
consistency, bolstering keyframe localization and fine-grained grounding.
Comprehensive empirical evaluations demonstrate that Veason-R1 achieves
state-of-the-art performance on multiple benchmarks, surpassing prior art by
significant margins (e.g., +1.3 J &F in ReVOS and +10.0 J &F in ReasonVOS),
while exhibiting robustness to hallucinations (+8.8 R). Our code and model
weights will be available at Veason-R1.

</details>


### [86] [Training-Free Anomaly Generation via Dual-Attention Enhancement in Diffusion Model](https://arxiv.org/abs/2508.11550)
*Zuo Zuo,Jiahao Dong,Yanyun Qu,Zongze Wu*

Main category: cs.CV

TL;DR: 提出了一种基于Stable Diffusion的无训练异常生成框架AAG，通过Cross-Attention Enhancement和Self-Attention Enhancement提升异常生成的逼真度和一致性。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测中数据稀缺问题突出，现有异常生成方法缺乏逼真度或需额外训练数据。

Method: 利用Stable Diffusion，结合文本提示和掩码，通过CAE和SAE机制生成逼真且与原始图像一致的异常。

Result: 在MVTec AD和VisA数据集上验证了AAG的有效性，生成的异常图像能提升下游检测任务性能。

Conclusion: AAG为工业异常检测提供了一种高效、无需训练的异常生成解决方案。

Abstract: Industrial anomaly detection (AD) plays a significant role in manufacturing
where a long-standing challenge is data scarcity. A growing body of works have
emerged to address insufficient anomaly data via anomaly generation. However,
these anomaly generation methods suffer from lack of fidelity or need to be
trained with extra data. To this end, we propose a training-free anomaly
generation framework dubbed AAG, which is based on Stable Diffusion (SD)'s
strong generation ability for effective anomaly image generation. Given a
normal image, mask and a simple text prompt, AAG can generate realistic and
natural anomalies in the specific regions and simultaneously keep contents in
other regions unchanged. In particular, we propose Cross-Attention Enhancement
(CAE) to re-engineer the cross-attention mechanism within Stable Diffusion
based on the given mask. CAE increases the similarity between visual tokens in
specific regions and text embeddings, which guides these generated visual
tokens in accordance with the text description. Besides, generated anomalies
need to be more natural and plausible with object in given image. We propose
Self-Attention Enhancement (SAE) which improves similarity between each normal
visual token and anomaly visual tokens. SAE ensures that generated anomalies
are coherent with original pattern. Extensive experiments on MVTec AD and VisA
datasets demonstrate effectiveness of AAG in anomaly generation and its
utility. Furthermore, anomaly images generated by AAG can bolster performance
of various downstream anomaly inspection tasks.

</details>


### [87] [TrajSV: A Trajectory-based Model for Sports Video Representations and Applications](https://arxiv.org/abs/2508.11569)
*Zheng Wang,Shihao Xu,Wei Shi*

Main category: cs.CV

TL;DR: TrajSV是一个基于轨迹的运动分析框架，通过数据预处理、CRNet和VRNet三个组件解决现有研究中的数据不足、轨迹框架不完善和标签需求问题，并在运动视频检索、动作识别和视频字幕生成中表现出色。


<details>
  <summary>Details</summary>
Motivation: 解决运动分析中数据不足、缺乏有效轨迹框架和标签需求的问题。

Method: 提出TrajSV框架，包括数据预处理、CRNet（轨迹增强Transformer）和VRNet（编码器-解码器架构），并使用三重对比损失进行无监督优化。

Result: 在三种运动（足球、篮球、排球）和三个下游任务（视频检索、动作识别、字幕生成）中表现优异，视频检索提升近70%，动作识别在9/17类别中领先，字幕生成提升近20%。

Conclusion: TrajSV在运动视频分析中实现了先进性能，并通过部署系统展示了实际应用潜力。

Abstract: Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

</details>


### [88] [Causality Matters: How Temporal Information Emerges in Video Language Models](https://arxiv.org/abs/2508.11576)
*Yumeng Shi,Quanyu Long,Yin Wu,Wenya Wang*

Main category: cs.CV

TL;DR: 研究发现，视频语言模型中的时间理解主要依赖于帧间注意力机制，而非位置编码。提出了两种效率优化策略。


<details>
  <summary>Details</summary>
Motivation: 探索视频语言模型中时间理解的机制，发现位置编码对性能影响较小，而帧顺序是关键。

Method: 通过分析实验揭示时间信息的整合路径，提出跨模态注意力分阶段和早期令牌截断的退出机制。

Result: 实验验证了两种策略的有效性，揭示了时间推理的因果信息路径。

Conclusion: 时间理解主要依赖帧间注意力，为未来模型改进提供了新思路。

Abstract: Video language models (VideoLMs) have made significant progress in multimodal
understanding. However, temporal understanding, which involves identifying
event order, duration, and relationships across time, still remains a core
challenge. Prior works emphasize positional encodings (PEs) as a key mechanism
for encoding temporal structure. Surprisingly, we find that removing or
modifying PEs in video inputs yields minimal degradation in the performance of
temporal understanding. In contrast, reversing the frame sequence while
preserving the original PEs causes a substantial drop. To explain this
behavior, we conduct substantial analysis experiments to trace how temporal
information is integrated within the model. We uncover a causal information
pathway: temporal cues are progressively synthesized through inter-frame
attention, aggregated in the final frame, and subsequently integrated into the
query tokens. This emergent mechanism shows that temporal reasoning emerges
from inter-visual token interactions under the constraints of causal attention,
which implicitly encodes temporal structure. Based on these insights, we
propose two efficiency-oriented strategies: staged cross-modal attention and a
temporal exit mechanism for early token truncation. Experiments on two
benchmarks validate the effectiveness of both approaches. To the best of our
knowledge, this is the first work to systematically investigate video temporal
understanding in VideoLMs, offering insights for future model improvement.

</details>


### [89] [DashCam Video: A complementary low-cost data stream for on-demand forest-infrastructure system monitoring](https://arxiv.org/abs/2508.11591)
*Durga Joshi,Chandi Witharana,Robert Fahey,Thomas Worthley,Zhe Zhu,Diego Cerrai*

Main category: cs.CV

TL;DR: 提出了一种低成本、可重复的框架，利用车载摄像头视频数据实时评估和定位路边植被与基础设施。


<details>
  <summary>Details</summary>
Motivation: 传统遥感方法（如LiDAR）成本高且不适用于实时监测，本研究旨在提供一种快速、低成本且可扩展的解决方案。

Method: 结合单目深度估计、深度误差校正和几何三角测量，从车载摄像头视频中生成准确的空间和结构数据。

Result: 深度校正模型表现优异（R2 = 0.92），低速车内摄像头配置下定位误差为2.83米，高度估计误差为2.09米（树木）和0.88米（杆状物）。

Conclusion: 该框架为城市植被和基础设施监测提供了实时、低成本且高效的解决方案，适用于公用事业公司和城市规划者。

Abstract: Our study introduces a novel, low-cost, and reproducible framework for
real-time, object-level structural assessment and geolocation of roadside
vegetation and infrastructure with commonly available but underutilized
dashboard camera (dashcam) video data. We developed an end-to-end pipeline that
combines monocular depth estimation, depth error correction, and geometric
triangulation to generate accurate spatial and structural data from
street-level video streams from vehicle-mounted dashcams. Depth maps were first
estimated using a state-of-the-art monocular depth model, then refined via a
gradient-boosted regression framework to correct underestimations, particularly
for distant objects. The depth correction model achieved strong predictive
performance (R2 = 0.92, MAE = 0.31 on transformed scale), significantly
reducing bias beyond 15 m. Further, object locations were estimated using
GPS-based triangulation, while object heights were calculated using pin hole
camera geometry. Our method was evaluated under varying conditions of camera
placement and vehicle speed. Low-speed vehicle with inside camera gave the
highest accuracy, with mean geolocation error of 2.83 m, and mean absolute
error (MAE) in height estimation of 2.09 m for trees and 0.88 m for poles. To
the best of our knowledge, it is the first framework to combine monocular depth
modeling, triangulated GPS-based geolocation, and real-time structural
assessment for urban vegetation and infrastructure using consumer-grade video
data. Our approach complements conventional RS methods, such as LiDAR and image
by offering a fast, real-time, and cost-effective solution for object-level
monitoring of vegetation risks and infrastructure exposure, making it
especially valuable for utility companies, and urban planners aiming for
scalable and frequent assessments in dynamic urban environments.

</details>


### [90] [CoreEditor: Consistent 3D Editing via Correspondence-constrained Diffusion](https://arxiv.org/abs/2508.11603)
*Zhe Zhu,Honghua Chen,Peng Li,Mingqiang Wei*

Main category: cs.CV

TL;DR: CoreEditor提出了一种新颖的文本驱动3D编辑框架，通过对应约束注意力机制和多视图语义相似性建模，显著提升了编辑的一致性和细节质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本驱动的3D编辑中难以保持多视图一致性，导致编辑不足和细节模糊。

Method: CoreEditor采用对应约束注意力机制，结合几何对齐和语义相似性建模，并设计了选择性编辑流程。

Result: 实验表明，CoreEditor能够生成高质量、3D一致的编辑结果，细节更清晰，优于现有方法。

Conclusion: CoreEditor通过创新的注意力机制和语义建模，解决了多视图一致性问题，提供了更灵活的用户控制。

Abstract: Text-driven 3D editing seeks to modify 3D scenes according to textual
descriptions, and most existing approaches tackle this by adapting pre-trained
2D image editors to multi-view inputs. However, without explicit control over
multi-view information exchange, they often fail to maintain cross-view
consistency, leading to insufficient edits and blurry details. We introduce
CoreEditor, a novel framework for consistent text-to-3D editing. The key
innovation is a correspondence-constrained attention mechanism that enforces
precise interactions between pixels expected to remain consistent throughout
the diffusion denoising process. Beyond relying solely on geometric alignment,
we further incorporate semantic similarity estimated during denoising, enabling
more reliable correspondence modeling and robust multi-view editing. In
addition, we design a selective editing pipeline that allows users to choose
preferred results from multiple candidates, offering greater flexibility and
user control. Extensive experiments show that CoreEditor produces high-quality,
3D-consistent edits with sharper details, significantly outperforming prior
methods.

</details>


### [91] [Controlling Multimodal LLMs via Reward-guided Decoding](https://arxiv.org/abs/2508.11616)
*Oscar Mañas,Pierluca D'Oro,Koustuv Sinha,Adriana Romero-Soriano,Michal Drozdzal,Aishwarya Agrawal*

Main category: cs.CV

TL;DR: 提出了一种通过奖励引导解码的方法，用于多模态大语言模型（MLLMs）的视觉定位控制。


<details>
  <summary>Details</summary>
Motivation: 适应多样化的用户需求，提升MLLMs的视觉定位能力。

Method: 构建两个独立的奖励模型，分别控制对象精度和召回率，动态调整解码过程。

Result: 在标准对象幻觉基准测试中表现优异，显著优于现有方法。

Conclusion: 该方法实现了对MLLM推理过程的动态控制，同时提升了性能。

Abstract: As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

</details>


### [92] [LoRAtorio: An intrinsic approach to LoRA Skill Composition](https://arxiv.org/abs/2508.11624)
*Niki Foteinopoulou,Ignas Budvytis,Stephan Liwicki*

Main category: cs.CV

TL;DR: LoRAtorio是一个无需训练的多LoRA组合框架，通过利用模型内在行为实现高效组合，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多LoRA组合时表现不佳，尤其是在开放环境中无法有效处理未知技能的数量和性质。

Method: 通过空间感知权重矩阵和加权聚合LoRA输出，结合分类器自由引导的改进方法。

Result: 在ClipScore上提升1.3%，GPT-4V评估中胜率72.43%，适用于多种潜在扩散模型。

Conclusion: LoRAtorio在多LoRA组合中表现卓越，具有广泛适用性和高效性。

Abstract: Low-Rank Adaptation (LoRA) has become a widely adopted technique in
text-to-image diffusion models, enabling the personalisation of visual concepts
such as characters, styles, and objects. However, existing approaches struggle
to effectively compose multiple LoRA adapters, particularly in open-ended
settings where the number and nature of required skills are not known in
advance. In this work, we present LoRAtorio, a novel train-free framework for
multi-LoRA composition that leverages intrinsic model behaviour. Our method is
motivated by two key observations: (1) LoRA adapters trained on narrow domains
produce denoised outputs that diverge from the base model, and (2) when
operating out-of-distribution, LoRA outputs show behaviour closer to the base
model than when conditioned in distribution. The balance between these two
observations allows for exceptional performance in the single LoRA scenario,
which nevertheless deteriorates when multiple LoRAs are loaded. Our method
operates in the latent space by dividing it into spatial patches and computing
cosine similarity between each patch's predicted noise and that of the base
model. These similarities are used to construct a spatially-aware weight
matrix, which guides a weighted aggregation of LoRA outputs. To address domain
drift, we further propose a modification to classifier-free guidance that
incorporates the base model's unconditional score into the composition. We
extend this formulation to a dynamic module selection setting, enabling
inference-time selection of relevant LoRA adapters from a large pool. LoRAtorio
achieves state-of-the-art performance, showing up to a 1.3% improvement in
ClipScore and a 72.43% win rate in GPT-4V pairwise evaluations, and generalises
effectively to multiple latent diffusion models.

</details>


### [93] [Is ChatGPT-5 Ready for Mammogram VQA?](https://arxiv.org/abs/2508.11628)
*Qiang Li,Shansong Wang,Mingzhe Hu,Mojtaba Safari,Zachary Eidex,Xiaofeng Yang*

Main category: cs.CV

TL;DR: GPT-5在乳腺X光视觉问答任务中表现优于GPT-4o，但仍不及人类专家和领域专用模型。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-5和GPT-4o在乳腺X光筛查任务中的表现，探索通用大语言模型在临床影像应用中的潜力。

Method: 在四个公开乳腺X光数据集（EMBED、InBreast、CMMD、CBIS-DDSM）上测试BI-RADS评估、异常检测和恶性分类任务。

Result: GPT-5在各项任务中表现最佳，但敏感性和特异性低于人类专家。

Conclusion: GPT-5在乳腺X光筛查中显示潜力，但需进一步领域优化才能用于高风险临床应用。

Abstract: Mammogram visual question answering (VQA) integrates image interpretation
with clinical reasoning and has potential to support breast cancer screening.
We systematically evaluated the GPT-5 family and GPT-4o model on four public
mammography datasets (EMBED, InBreast, CMMD, CBIS-DDSM) for BI-RADS assessment,
abnormality detection, and malignancy classification tasks. GPT-5 consistently
was the best performing model but lagged behind both human experts and
domain-specific fine-tuned models. On EMBED, GPT-5 achieved the highest scores
among GPT variants in density (56.8%), distortion (52.5%), mass (64.5%),
calcification (63.5%), and malignancy (52.8%) classification. On InBreast, it
attained 36.9% BI-RADS accuracy, 45.9% abnormality detection, and 35.0%
malignancy classification. On CMMD, GPT-5 reached 32.3% abnormality detection
and 55.0% malignancy accuracy. On CBIS-DDSM, it achieved 69.3% BI-RADS
accuracy, 66.0% abnormality detection, and 58.2% malignancy accuracy. Compared
with human expert estimations, GPT-5 exhibited lower sensitivity (63.5%) and
specificity (52.3%). While GPT-5 exhibits promising capabilities for screening
tasks, its performance remains insufficient for high-stakes clinical imaging
applications without targeted domain adaptation and optimization. However, the
tremendous improvements in performance from GPT-4o to GPT-5 show a promising
trend in the potential for general large language models (LLMs) to assist with
mammography VQA tasks.

</details>


### [94] [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)
*Yi-Fan Zhang,Xingyu Lu,Shukang Yin,Chaoyou Fu,Wei Chen,Xiao Hu,Bin Wen,Kaiyu Jiang,Changyi Liu,Tianke Zhang,Haonan Fan,Kaibing Chen,Jiankang Chen,Haojie Ding,Kaiyu Tang,Zhang Zhang,Liang Wang,Fan Yang,Tingting Gao,Guorui Zhou*

Main category: cs.CV

TL;DR: Thyme是一种新型多模态大语言模型（MLLM）范式，通过生成和执行代码实现图像处理和计算操作，显著提升了感知和推理任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前开源模型在图像处理和逻辑推理能力上不如专有模型（如O3），Thyme旨在填补这一空白。

Method: 采用两阶段训练策略：先通过SFT学习代码生成，再用RL优化决策；提出GRPO-ATS算法平衡推理探索与代码执行精度。

Result: 在近20个基准测试中表现优异，尤其在复杂推理和高分辨率感知任务中。

Conclusion: Thyme通过代码自主操作扩展了多模态模型的推理能力，为开源社区提供了强大工具。

Abstract: Following OpenAI's introduction of the ``thinking with images'' concept,
recent efforts have explored stimulating the use of visual information in the
reasoning process to enhance model performance in perception and reasoning
tasks. However, to the best of our knowledge, no open-source work currently
offers a feature set as rich as proprietary models (O3), which can perform
diverse image manipulations and simultaneously enhance logical reasoning
capabilities through code. In this paper, we make a preliminary attempt in this
direction by introducing Thyme (Think Beyond Images), a novel paradigm for
enabling MLLMs to transcend existing ``think with images'' approaches by
autonomously generating and executing diverse image processing and
computational operations via executable code. This approach not only
facilitates a rich, on-the-fly set of image manipulations (e.g., cropping,
rotation, contrast enhancement) but also allows for mathematical computations,
all while maintaining high autonomy in deciding when and how to apply these
operations. We activate this capability through a two-stage training strategy:
an initial SFT on a curated dataset of 500K samples to teach code generation,
followed by a RL phase to refine decision-making. For the RL stage, we manually
collect and design high-resolution question-answer pairs to increase the
learning difficulty, and we propose GRPO-ATS (Group Relative Policy
Optimization with Adaptive Temperature Sampling), an algorithm that applies
distinct temperatures to text and code generation to balance reasoning
exploration with code execution precision. We conduct extensive experimental
analysis and ablation studies. Comprehensive evaluations on nearly 20
benchmarks show that Thyme yields significant and consistent performance gains,
particularly in challenging high-resolution perception and complex reasoning
tasks.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [95] [GenFlowRL: Shaping Rewards with Generative Object-Centric Flow in Visual Reinforcement Learning](https://arxiv.org/abs/2508.11049)
*Kelin Yu,Sheng Zhang,Harshit Soora,Furong Huang,Heng Huang,Pratap Tokekar,Ruohan Gao*

Main category: cs.RO

TL;DR: GenFlowRL通过从多样化的跨体现数据集中提取生成的流来塑造奖励，从而学习通用且鲁棒的策略。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在机器人学习中依赖生成数据质量，且缺乏环境反馈，难以处理细粒度操作。视频强化学习受限于视频生成的不确定性和大规模数据集收集的挑战。

Method: GenFlowRL从多样化的跨体现数据集中提取生成的流，利用低维、以对象为中心的特征学习策略。

Result: 在10个操作任务的仿真和真实跨体现评估中，GenFlowRL表现出色，能有效利用生成的对象中心流提取的操作特征。

Conclusion: GenFlowRL通过生成的对象中心流提取特征，在多样化和挑战性场景中表现优异。

Abstract: Recent advances have shown that video generation models can enhance robot
learning by deriving effective robot actions through inverse dynamics. However,
these methods heavily depend on the quality of generated data and struggle with
fine-grained manipulation due to the lack of environment feedback. While
video-based reinforcement learning improves policy robustness, it remains
constrained by the uncertainty of video generation and the challenges of
collecting large-scale robot datasets for training diffusion models. To address
these limitations, we propose GenFlowRL, which derives shaped rewards from
generated flow trained from diverse cross-embodiment datasets. This enables
learning generalizable and robust policies from diverse demonstrations using
low-dimensional, object-centric features. Experiments on 10 manipulation tasks,
both in simulation and real-world cross-embodiment evaluations, demonstrate
that GenFlowRL effectively leverages manipulation features extracted from
generated object-centric flow, consistently achieving superior performance
across diverse and challenging scenarios. Our Project Page:
https://colinyu1.github.io/genflowrl

</details>


### [96] [Scene Graph-Guided Proactive Replanning for Failure-Resilient Embodied Agent](https://arxiv.org/abs/2508.11286)
*Che Rin Yu,Daewon Chae,Dabin Seo,Sangwon Lee,Hyeongwoo Im,Jinkyu Kim*

Main category: cs.RO

TL;DR: 论文提出了一种主动重规划框架，通过比较当前场景图与参考场景图，在子任务边界检测并纠正潜在失败，提升机器人任务成功率。


<details>
  <summary>Details</summary>
Motivation: 人类能根据环境状态调整行为，而机器人常因缺乏适应性导致失败。现有方法多在失败后响应，效率低。主动重规划可预防失败，但当前方案依赖人工规则和大量监督。

Method: 构建当前RGB-D观测的场景图与成功演示的参考图对比，在子任务边界激活轻量推理模块诊断不匹配并调整计划。

Result: 在AI2-THOR模拟器中验证，该方法能在执行失败前检测语义和空间不匹配，显著提高任务成功率和鲁棒性。

Conclusion: 提出的主动重规划框架通过场景图对比和轻量推理，有效预防机器人任务失败，提升自主性。

Abstract: When humans perform everyday tasks, we naturally adjust our actions based on
the current state of the environment. For instance, if we intend to put
something into a drawer but notice it is closed, we open it first. However,
many autonomous robots lack this adaptive awareness. They often follow
pre-planned actions that may overlook subtle yet critical changes in the scene,
which can result in actions being executed under outdated assumptions and
eventual failure. While replanning is critical for robust autonomy, most
existing methods respond only after failures occur, when recovery may be
inefficient or infeasible. While proactive replanning holds promise for
preventing failures in advance, current solutions often rely on manually
designed rules and extensive supervision. In this work, we present a proactive
replanning framework that detects and corrects failures at subtask boundaries
by comparing scene graphs constructed from current RGB-D observations against
reference graphs extracted from successful demonstrations. When the current
scene fails to align with reference trajectories, a lightweight reasoning
module is activated to diagnose the mismatch and adjust the plan. Experiments
in the AI2-THOR simulator demonstrate that our approach detects semantic and
spatial mismatches before execution failures occur, significantly improving
task success and robustness.

</details>


### [97] [Relative Position Matters: Trajectory Prediction and Planning with Polar Representation](https://arxiv.org/abs/2508.11492)
*Bozhou Zhang,Nan Song,Bingzhao Gao,Li Zhang*

Main category: cs.RO

TL;DR: 论文提出了一种基于极坐标的轨迹预测与规划方法Polaris，解决了传统笛卡尔坐标系在建模动态环境中相对关系时的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在笛卡尔坐标系中建模车辆与周围元素的相对关系时效果不佳，无法自然捕捉距离和方向的影响。

Method: 采用极坐标系表示位置，通过专用编码和优化模块显式建模距离和方向变化，提出Polaris方法。

Result: 在Argoverse 2和nuPlan基准测试中，Polaris实现了最先进的性能。

Conclusion: 极坐标表示能更直观有效地建模动态环境中的相对关系，提升轨迹预测与规划的准确性。

Abstract: Trajectory prediction and planning in autonomous driving are highly
challenging due to the complexity of predicting surrounding agents' movements
and planning the ego agent's actions in dynamic environments. Existing methods
encode map and agent positions and decode future trajectories in Cartesian
coordinates. However, modeling the relationships between the ego vehicle and
surrounding traffic elements in Cartesian space can be suboptimal, as it does
not naturally capture the varying influence of different elements based on
their relative distances and directions. To address this limitation, we adopt
the Polar coordinate system, where positions are represented by radius and
angle. This representation provides a more intuitive and effective way to model
spatial changes and relative relationships, especially in terms of distance and
directional influence. Based on this insight, we propose Polaris, a novel
method that operates entirely in Polar coordinates, distinguishing itself from
conventional Cartesian-based approaches. By leveraging the Polar
representation, this method explicitly models distance and direction variations
and captures relative relationships through dedicated encoding and refinement
modules, enabling more structured and spatially aware trajectory prediction and
planning. Extensive experiments on the challenging prediction (Argoverse 2) and
planning benchmarks (nuPlan) demonstrate that Polaris achieves state-of-the-art
performance.

</details>


### [98] [Visual Perception Engine: Fast and Flexible Multi-Head Inference for Robotic Vision Tasks](https://arxiv.org/abs/2508.11584)
*Jakub Łucki,Jonathan Becktor,Georgios Georgakis,Robert Royce,Shehryar Khattak*

Main category: cs.RO

TL;DR: VPEngine是一个模块化框架，通过共享基础模型和多任务并行处理，显著提升资源受限机器人平台上的视觉多任务处理效率。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的机器人平台上部署多个机器学习模型时出现的计算冗余、内存占用大和集成复杂的问题。

Method: 利用共享的基础模型提取图像表示，并通过并行运行的任务特定模型头高效共享这些表示，避免不必要的GPU-CPU内存传输。

Result: 使用DINOv2作为基础模型，实现了3倍的速度提升，并在NVIDIA Jetson Orin AGX上达到≥50 Hz的实时性能。

Conclusion: VPEngine通过高效GPU利用和动态任务优先级调整，为机器人社区提供了一个可扩展且易于使用的解决方案。

Abstract: Deploying multiple machine learning models on resource-constrained robotic
platforms for different perception tasks often results in redundant
computations, large memory footprints, and complex integration challenges. In
response, this work presents Visual Perception Engine (VPEngine), a modular
framework designed to enable efficient GPU usage for visual multitasking while
maintaining extensibility and developer accessibility. Our framework
architecture leverages a shared foundation model backbone that extracts image
representations, which are efficiently shared, without any unnecessary GPU-CPU
memory transfers, across multiple specialized task-specific model heads running
in parallel. This design eliminates the computational redundancy inherent in
feature extraction component when deploying traditional sequential models while
enabling dynamic task prioritization based on application demands. We
demonstrate our framework's capabilities through an example implementation
using DINOv2 as the foundation model with multiple task (depth, object
detection and semantic segmentation) heads, achieving up to 3x speedup compared
to sequential execution. Building on CUDA Multi-Process Service (MPS), VPEngine
offers efficient GPU utilization and maintains a constant memory footprint
while allowing per-task inference frequencies to be adjusted dynamically during
runtime. The framework is written in Python and is open source with ROS2 C++
(Humble) bindings for ease of use by the robotics community across diverse
robotic platforms. Our example implementation demonstrates end-to-end real-time
performance at $\geq$50 Hz on NVIDIA Jetson Orin AGX for TensorRT optimized
models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [99] [StyleMM: Stylized 3D Morphable Face Model via Text-Driven Aligned Image Translation](https://arxiv.org/abs/2508.11203)
*Seungmi Lee,Kwan Yun,Junyong Noh*

Main category: cs.GR

TL;DR: StyleMM是一个基于文本描述构建风格化3D可变形模型（3DMM）的新框架，通过扩散模型生成风格化目标图像，并保留关键面部属性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在风格化3DMM时难以保持面部身份、对齐和表情的一致性，StyleMM旨在解决这一问题。

Method: 结合预训练的网格变形网络和纹理生成器，利用扩散模型生成风格化目标图像，并通过图像训练实现3D风格迁移。

Result: StyleMM在面部多样性和风格化能力上优于现有方法，支持对形状、表情和纹理参数的显式控制。

Conclusion: StyleMM通过保留关键面部属性，实现了高效的风格化3DMM生成，具有广泛的应用潜力。

Abstract: We introduce StyleMM, a novel framework that can construct a stylized 3D
Morphable Model (3DMM) based on user-defined text descriptions specifying a
target style. Building upon a pre-trained mesh deformation network and a
texture generator for original 3DMM-based realistic human faces, our approach
fine-tunes these models using stylized facial images generated via text-guided
image-to-image (i2i) translation with a diffusion model, which serve as
stylization targets for the rendered mesh. To prevent undesired changes in
identity, facial alignment, or expressions during i2i translation, we introduce
a stylization method that explicitly preserves the facial attributes of the
source image. By maintaining these critical attributes during image
stylization, the proposed approach ensures consistent 3D style transfer across
the 3DMM parameter space through image-based training. Once trained, StyleMM
enables feed-forward generation of stylized face meshes with explicit control
over shape, expression, and texture parameters, producing meshes with
consistent vertex connectivity and animatability. Quantitative and qualitative
evaluations demonstrate that our approach outperforms state-of-the-art methods
in terms of identity-level facial diversity and stylization capability. The
code and videos are available at
[kwanyun.github.io/stylemm_page](kwanyun.github.io/stylemm_page).

</details>


### [100] [SPG: Style-Prompting Guidance for Style-Specific Content Creation](https://arxiv.org/abs/2508.11476)
*Qian Liang,Zichong Chen,Yang Zhou,Hui Huang*

Main category: cs.GR

TL;DR: 提出了一种名为Style-Prompting Guidance (SPG)的新采样策略，用于生成特定风格的图像，结合Classifier-Free Guidance (CFG)实现语义保真和风格一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的文本到图像扩散模型在生成与文本提示对齐的图像方面表现优异，但控制输出图像的视觉风格仍具挑战性。

Method: SPG通过构建风格噪声向量，并利用其与无条件噪声的方向偏差引导扩散过程朝向目标风格分布。

Result: 实验表明，SPG在风格一致性方面优于现有方法，且兼容ControlNet和IPAdapter等可控框架。

Conclusion: SPG是一种简单、鲁棒且广泛适用的方法，有效解决了风格控制问题。

Abstract: Although recent text-to-image (T2I) diffusion models excel at aligning
generated images with textual prompts, controlling the visual style of the
output remains a challenging task. In this work, we propose Style-Prompting
Guidance (SPG), a novel sampling strategy for style-specific image generation.
SPG constructs a style noise vector and leverages its directional deviation
from unconditional noise to guide the diffusion process toward the target style
distribution. By integrating SPG with Classifier-Free Guidance (CFG), our
method achieves both semantic fidelity and style consistency. SPG is simple,
robust, and compatible with controllable frameworks like ControlNet and
IPAdapter, making it practical and widely applicable. Extensive experiments
demonstrate the effectiveness and generality of our approach compared to
state-of-the-art methods. Code is available at
https://github.com/Rumbling281441/SPG.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [101] [LD-LAudio-V1: Video-to-Long-Form-Audio Generation Extension with Dual Lightweight Adapters](https://arxiv.org/abs/2508.11074)
*Haomin Zhang,Kristin Qi,Shuxin Yang,Zihao Chen,Chaofan Ding,Xinhan Di*

Main category: cs.SD

TL;DR: LD-LAudio-V1是一种扩展的视频到音频生成模型，通过双轻量适配器实现长音频生成，并发布了干净的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注短音频生成或依赖噪声数据集，限制了长视频到音频合成的质量。

Method: 引入双轻量适配器，扩展现有模型，并发布干净的数据集。

Result: 在多个指标上显著提升，如FD、KL、IS等，减少了拼接伪影和时间不一致性。

Conclusion: LD-LAudio-V1在长视频到音频生成中表现优异，数据集将促进进一步研究。

Abstract: Generating high-quality and temporally synchronized audio from video content
is essential for video editing and post-production tasks, enabling the creation
of semantically aligned audio for silent videos. However, most existing
approaches focus on short-form audio generation for video segments under 10
seconds or rely on noisy datasets for long-form video-to-audio zsynthesis. To
address these limitations, we introduce LD-LAudio-V1, an extension of
state-of-the-art video-to-audio models and it incorporates dual lightweight
adapters to enable long-form audio generation. In addition, we release a clean
and human-annotated video-to-audio dataset that contains pure sound effects
without noise or artifacts. Our method significantly reduces splicing artifacts
and temporal inconsistencies while maintaining computational efficiency.
Compared to direct fine-tuning with short training videos, LD-LAudio-V1
achieves significant improvements across multiple metrics: $FD_{\text{passt}}$
450.00 $\rightarrow$ 327.29 (+27.27%), $FD_{\text{panns}}$ 34.88 $\rightarrow$
22.68 (+34.98%), $FD_{\text{vgg}}$ 3.75 $\rightarrow$ 1.28 (+65.87%),
$KL_{\text{panns}}$ 2.49 $\rightarrow$ 2.07 (+16.87%), $KL_{\text{passt}}$ 1.78
$\rightarrow$ 1.53 (+14.04%), $IS_{\text{panns}}$ 4.17 $\rightarrow$ 4.30
(+3.12%), $IB_{\text{score}}$ 0.25 $\rightarrow$ 0.28 (+12.00%),
$Energy\Delta10\text{ms}$ 0.3013 $\rightarrow$ 0.1349 (+55.23%),
$Energy\Delta10\text{ms(vs.GT)}$ 0.0531 $\rightarrow$ 0.0288 (+45.76%), and
$Sem.\,Rel.$ 2.73 $\rightarrow$ 3.28 (+20.15%). Our dataset aims to facilitate
further research in long-form video-to-audio generation and is available at
https://github.com/deepreasonings/long-form-video2audio.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [102] [Fluid Dynamics and Domain Reconstruction from Noisy Flow Images Using Physics-Informed Neural Networks and Quasi-Conformal Mapping](https://arxiv.org/abs/2508.11216)
*Han Zhang,Xue-Cheng Tai,Jean-Michel Morel,Raymond H. Chan*

Main category: math.NA

TL;DR: 该论文提出了一种基于物理信息神经网络和几何优化的血流图像去噪方法，通过交替求解流体和几何子问题，实现了高质量的血流图像重建。


<details>
  <summary>Details</summary>
Motivation: 血流成像在医学诊断和治疗规划中至关重要，但获取高质量的血流图像仍具挑战性，尤其是存在噪声和伪影的情况下。

Method: 将去噪任务建模为优化问题，分解为流体子问题（使用物理信息神经网络重建速度场）和几何子问题（通过拟共形映射优化流动区域），并采用交替迭代求解。

Result: 实验表明，该方法在合成和真实数据上均能有效去噪，且对噪声水平具有鲁棒性。

Conclusion: 该方法通过结合物理约束和几何优化，显著提升了血流图像的质量，为医学应用提供了可靠工具。

Abstract: Blood flow imaging provides important information for hemodynamic behavior
within the vascular system and plays an essential role in medical diagnosis and
treatment planning. However, obtaining high-quality flow images remains a
significant challenge. In this work, we address the problem of denoising flow
images that may suffer from artifacts due to short acquisition times or
device-induced errors. We formulate this task as an optimization problem, where
the objective is to minimize the discrepancy between the modeled velocity
field, constrained to satisfy the Navier-Stokes equations, and the observed
noisy velocity data. To solve this problem, we decompose it into two
subproblems: a fluid subproblem and a geometry subproblem. The fluid subproblem
leverages a Physics-Informed Neural Network to reconstruct the velocity field
from noisy observations, assuming a fixed domain. The geometry subproblem aims
to infer the underlying flow region by optimizing a quasi-conformal mapping
that deforms a reference domain. These two subproblems are solved in an
alternating Gauss-Seidel fashion, iteratively refining both the velocity field
and the domain. Upon convergence, the framework yields a high-quality
reconstruction of the flow image. We validate the proposed method through
experiments on synthetic flow data in a converging channel geometry under
varying levels of Gaussian noise, and on real-like flow data in an aortic
geometry with signal-dependent noise. The results demonstrate the effectiveness
and robustness of the approach. Additionally, ablation studies are conducted to
assess the influence of key hyperparameters.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [103] [Failures to Surface Harmful Contents in Video Large Language Models](https://arxiv.org/abs/2508.10974)
*Yuxin Cao,Wei Song,Derui Wang,Jingling Xue,Jin Song Dong*

Main category: cs.MM

TL;DR: VideoLLMs存在安全漏洞，可能忽略视频中的有害内容，原因包括稀疏帧采样、空间信息丢失和编码-解码脱节。


<details>
  <summary>Details</summary>
Motivation: 揭示VideoLLMs在处理有害内容时的设计缺陷，以提高模型安全性。

Method: 通过根因分析发现三个设计缺陷，并设计零查询黑盒攻击验证。

Result: 评估显示，90%以上的情况下VideoLLMs会忽略有害内容。

Conclusion: 当前VideoLLMs设计存在根本性漏洞，需改进采样策略和编码机制。

Abstract: Video Large Language Models (VideoLLMs) are increasingly deployed on numerous
critical applications, where users rely on auto-generated summaries while
casually skimming the video stream. We show that this interaction hides a
critical safety gap: if harmful content is embedded in a video, either as
full-frame inserts or as small corner patches, state-of-the-art VideoLLMs
rarely mention the harmful content in the output, despite its clear visibility
to human viewers. A root-cause analysis reveals three compounding design flaws:
(1) insufficient temporal coverage resulting from the sparse, uniformly spaced
frame sampling used by most leading VideoLLMs, (2) spatial information loss
introduced by aggressive token downsampling within sampled frames, and (3)
encoder-decoder disconnection, whereby visual cues are only weakly utilized
during text generation. Leveraging these insights, we craft three zero-query
black-box attacks, aligning with these flaws in the processing pipeline. Our
large-scale evaluation across five leading VideoLLMs shows that the harmfulness
omission rate exceeds 90% in most cases. Even when harmful content is clearly
present in all frames, these models consistently fail to identify it. These
results underscore a fundamental vulnerability in current VideoLLMs' designs
and highlight the urgent need for sampling strategies, token compression, and
decoding mechanisms that guarantee semantic coverage rather than speed alone.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [104] [Model Interpretability and Rationale Extraction by Input Mask Optimization](https://arxiv.org/abs/2508.11388)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: 提出了一种基于梯度优化和正则化的新方法，用于生成神经网络预测的提取性解释，适用于文本和图像输入。


<details>
  <summary>Details</summary>
Motivation: 随着神经网络在自然语言处理和计算机视觉领域的快速发展，解释这些黑盒模型预测的需求日益增长。

Method: 通过梯度优化和新的正则化方案，对输入进行掩码处理，生成满足充分性、全面性和紧凑性的解释。

Result: 该方法在文本和图像分类任务中均能生成高质量的解释，表明自然语言处理中的理性提取条件可推广到其他输入类型。

Conclusion: 该方法无需训练专用模型，仅基于训练好的分类器即可实现理性提取，弥合了模型可解释性与理性提取之间的差距。

Abstract: Concurrent to the rapid progress in the development of neural-network based
models in areas like natural language processing and computer vision, the need
for creating explanations for the predictions of these black-box models has
risen steadily. We propose a new method to generate extractive explanations for
predictions made by neural networks, that is based on masking parts of the
input which the model does not consider to be indicative of the respective
class. The masking is done using gradient-based optimization combined with a
new regularization scheme that enforces sufficiency, comprehensiveness and
compactness of the generated explanation, three properties that are known to be
desirable from the related field of rationale extraction in natural language
processing. In this way, we bridge the gap between model interpretability and
rationale extraction, thereby proving that the latter of which can be performed
without training a specialized model, only on the basis of a trained
classifier. We further apply the same method to image inputs and obtain high
quality explanations for image classifications, which indicates that the
conditions proposed for rationale extraction in natural language processing are
more broadly applicable to different input types.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [105] [Allen: Rethinking MAS Design through Step-Level Policy Autonomy](https://arxiv.org/abs/2508.11294)
*Qiangong Zhou,Zhiting Wang,Mingyou Yao,Zongyang Liu*

Main category: cs.MA

TL;DR: Allen是一个新型多智能体系统（MAS），旨在提升策略自主性并平衡协作效率、任务监督和人工监管。


<details>
  <summary>Details</summary>
Motivation: 解决当前MAS设计中策略自主性不足和复杂网络拓扑中协作效率与可控性的平衡问题。

Method: 通过重新定义MAS中的基本执行单元，构建四层状态架构（任务、阶段、智能体、步骤），实现拓扑优化与可控进度的统一。

Result: Allen实现了前所未有的策略自主性，同时平衡了协作结构的可控性。

Conclusion: Allen为多智能体系统设计提供了新的解决方案，代码已开源。

Abstract: We introduce a new Multi-Agent System (MAS) - Allen, designed to address two
core challenges in current MAS design: (1) improve system's policy autonomy,
empowering agents to dynamically adapt their behavioral strategies, and (2)
achieving the trade-off between collaborative efficiency, task supervision, and
human oversight in complex network topologies.
  Our core insight is to redefine the basic execution unit in the MAS, allowing
agents to autonomously form different patterns by combining these units. We
have constructed a four-tier state architecture (Task, Stage, Agent, Step) to
constrain system behavior from both task-oriented and execution-oriented
perspectives. This achieves a unification of topological optimization and
controllable progress.
  Allen grants unprecedented Policy Autonomy, while making a trade-off for the
controllability of the collaborative structure. The project code has been open
source at: https://github.com/motern88/Allen

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [106] [Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.10993)
*Basile Lewandowski,Robert Birke,Lydia Y. Chen*

Main category: cs.LG

TL;DR: 本文提出了首个模型选择框架M&C，帮助用户从模型平台中选择最适合目标数据域的预训练T2I模型，无需对所有模型进行微调。


<details>
  <summary>Details</summary>
Motivation: 预训练T2I模型的公开共享促进了模型的民主化，但用户面临如何选择最适合目标数据域的模型的挑战。

Method: M&C框架通过构建匹配图（包含模型和数据集节点，以及模型-数据和数据-数据边）来预测微调后的最佳模型。

Result: 在10个T2I模型和32个数据集上的评估显示，M&C在61.3%的情况下成功预测最佳模型，其余情况下也能预测性能接近的模型。

Conclusion: M&C框架为用户提供了一种高效选择预训练T2I模型的方法，显著减少了微调成本。

Abstract: Text-to-image (T2I) models based on diffusion and transformer architectures
advance rapidly. They are often pretrained on large corpora, and openly shared
on a model platform, such as HuggingFace. Users can then build up AI
applications, e.g., generating media contents, by adopting pretrained T2I
models and fine-tuning them on the target dataset. While public pretrained T2I
models facilitate the democratization of the models, users face a new
challenge: which model can be best fine-tuned based on the target data domain?
Model selection is well addressed in classification tasks, but little is known
in (pretrained) T2I models and their performance indication on the target
domain. In this paper, we propose the first model selection framework, M&C,
which enables users to efficiently choose a pretrained T2I model from a model
platform without exhaustively fine-tuning them all on the target dataset. The
core of M&C is a matching graph, which consists of: (i) nodes of available
models and profiled datasets, and (ii) edges of model-data and data-data pairs
capturing the fine-tuning performance and data similarity, respectively. We
then build a model that, based on the inputs of model/data feature, and,
critically, the graph embedding feature, extracted from the matching graph,
predicts the model achieving the best quality after fine-tuning for the target
domain. We evaluate M&C on choosing across ten T2I models for 32 datasets
against three baselines. Our results show that M&C successfully predicts the
best model for fine-tuning in 61.3% of the cases and a closely performing model
for the rest.

</details>


### [107] [Boosting the Robustness-Accuracy Trade-off of SNNs by Robust Temporal Self-Ensemble](https://arxiv.org/abs/2508.11279)
*Jihang Wang,Dongcheng Zhao,Ruolin Chen,Qian Zhang,Yi Zeng*

Main category: cs.LG

TL;DR: 本文提出了一种名为RTE的训练框架，通过时间集成提升SNN的对抗鲁棒性，减少对抗扰动的时间传递性。


<details>
  <summary>Details</summary>
Motivation: SNN在节能和类脑计算方面具有潜力，但其对抗扰动的脆弱性尚未被充分理解。

Method: 提出RTE框架，通过统一损失函数和随机采样策略优化子网络的鲁棒性。

Result: RTE在多个基准测试中表现优于现有方法，重塑了SNN的内部鲁棒性景观。

Conclusion: 研究强调了时间结构在对抗学习中的重要性，为构建鲁棒的SNN模型提供了理论基础。

Abstract: Spiking Neural Networks (SNNs) offer a promising direction for
energy-efficient and brain-inspired computing, yet their vulnerability to
adversarial perturbations remains poorly understood. In this work, we revisit
the adversarial robustness of SNNs through the lens of temporal ensembling,
treating the network as a collection of evolving sub-networks across discrete
timesteps. This formulation uncovers two critical but underexplored
challenges-the fragility of individual temporal sub-networks and the tendency
for adversarial vulnerabilities to transfer across time. To overcome these
limitations, we propose Robust Temporal self-Ensemble (RTE), a training
framework that improves the robustness of each sub-network while reducing the
temporal transferability of adversarial perturbations. RTE integrates both
objectives into a unified loss and employs a stochastic sampling strategy for
efficient optimization. Extensive experiments across multiple benchmarks
demonstrate that RTE consistently outperforms existing training methods in
robust-accuracy trade-off. Additional analyses reveal that RTE reshapes the
internal robustness landscape of SNNs, leading to more resilient and temporally
diversified decision boundaries. Our study highlights the importance of
temporal structure in adversarial learning and offers a principled foundation
for building robust spiking models.

</details>


### [108] [Robust Convolution Neural ODEs via Contractivity-promoting regularization](https://arxiv.org/abs/2508.11432)
*Muhammad Zakwan,Liang Xu,Giancarlo Ferrari-Trecate*

Main category: cs.LG

TL;DR: 提出了一种通过收缩理论提升卷积神经常微分方程（NODEs）鲁棒性的方法，并通过正则化项在训练中实现收缩性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的脆弱性使其容易受到输入噪声和对抗攻击的影响，因此需要提升其鲁棒性。

Method: 利用收缩理论，通过在训练中引入涉及系统动力学雅可比矩阵的正则化项，或针对特定NODEs的权重正则化项，来诱导收缩性。

Result: 在MNIST和FashionMNIST数据集上的实验表明，该方法能有效提升模型对噪声和攻击的鲁棒性。

Conclusion: 通过收缩理论和正则化方法，可以显著提升卷积NODEs的鲁棒性，适用于噪声和对抗攻击场景。

Abstract: Neural networks can be fragile to input noise and adversarial attacks.
  In this work, we consider Convolutional Neural Ordinary Differential
Equations (NODEs), a family of continuous-depth neural networks represented by
dynamical systems, and propose to use contraction theory to improve their
robustness.
  For a contractive dynamical system two trajectories starting from different
initial conditions converge to each other exponentially fast.
  Contractive Convolutional NODEs can enjoy increased robustness as slight
perturbations of the features do not cause a significant change in the output.
  Contractivity can be induced during training by using a regularization term
involving the Jacobian of the system dynamics.
  To reduce the computational burden, we show that it can also be promoted
using carefully selected weight regularization terms for a class of NODEs with
slope-restricted activation functions.
  The performance of the proposed regularizers is illustrated through benchmark
image classification tasks on MNIST and FashionMNIST datasets, where images are
corrupted by different kinds of noise and attacks.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [109] [The Role of Radiographic Knee Alignment in Knee Replacement Outcomes and Opportunities for Artificial Intelligence-Driven Assessment](https://arxiv.org/abs/2508.10941)
*Zhisen Hu,David S. Johnson,Aleksei Tiulpin,Timothy F. Cootes,Claudia Lindner*

Main category: eess.IV

TL;DR: 本文综述了膝关节置换术（TKR）的预后评估及其与膝关节对齐生物标志物的关系，探讨了AI在膝关节对齐测量中的应用及未来方向。


<details>
  <summary>Details</summary>
Motivation: 膝关节骨关节炎（OA）的普遍性及其治疗手段TKR的预后难以预测，膝关节对齐是影响预后的关键因素之一。现有研究多关注OA诊断和MRI分割，而缺乏对TKR预后相关对齐生物标志物的探讨。

Method: 本文首先回顾了TKR预后的评分协议及潜在的对齐生物标志物，然后讨论了基于AI的膝关节对齐测量方法。

Result: 总结了现有AI方法在膝关节对齐测量中的应用，并指出其对TKR预后预测的潜力。

Conclusion: 未来研究应进一步探索膝关节对齐生物标志物与TKR预后的关系，并优化AI技术以提高预测准确性。

Abstract: Prevalent knee osteoarthritis (OA) imposes substantial burden on health
systems with no cure available. Its ultimate treatment is total knee
replacement (TKR). Complications from surgery and recovery are difficult to
predict in advance, and numerous factors may affect them. Radiographic knee
alignment is one of the key factors that impacts TKR outcomes, affecting
outcomes such as postoperative pain or function. Recently, artificial
intelligence (AI) has been introduced to the automatic analysis of knee
radiographs, for example, to automate knee alignment measurements. Existing
review articles tend to focus on knee OA diagnosis and segmentation of bones or
cartilages in MRI rather than exploring knee alignment biomarkers for TKR
outcomes and their assessment. In this review, we first examine the current
scoring protocols for evaluating TKR outcomes and potential knee alignment
biomarkers associated with these outcomes. We then discuss existing AI-based
approaches for generating knee alignment biomarkers from knee radiographs, and
explore future directions for knee alignment assessment and TKR outcome
prediction.

</details>


### [110] [Deep Learning-Based Automated Segmentation of Uterine Myomas](https://arxiv.org/abs/2508.11010)
*Tausifa Jan Saleem,Mohammad Yaqub*

Main category: eess.IV

TL;DR: 该论文提出利用公开数据集UMD建立子宫肌瘤自动分割的基线，以解决现有方法依赖私有数据集的问题。


<details>
  <summary>Details</summary>
Motivation: 子宫肌瘤是女性生殖系统常见的良性肿瘤，MRI分割过程耗时且易受专家差异影响，需要自动化方法。

Method: 利用公开的UMD数据集，采用深度学习算法进行子宫肌瘤的自动分割。

Result: 通过公开数据集建立了自动分割的基线，便于标准化评估和未来研究。

Conclusion: 使用公开数据集UMD为子宫肌瘤自动分割提供了标准化基准，推动了该领域的研究。

Abstract: Uterine fibroids (myomas) are the most common benign tumors of the female
reproductive system, particularly among women of childbearing age. With a
prevalence exceeding 70%, they pose a significant burden on female reproductive
health. Clinical symptoms such as abnormal uterine bleeding, infertility,
pelvic pain, and pressure-related discomfort play a crucial role in guiding
treatment decisions, which are largely influenced by the size, number, and
anatomical location of the fibroids. Magnetic Resonance Imaging (MRI) is a
non-invasive and highly accurate imaging modality commonly used by clinicians
for the diagnosis of uterine fibroids. Segmenting uterine fibroids requires a
precise assessment of both the uterus and fibroids on MRI scans, including
measurements of volume, shape, and spatial location. However, this process is
labor intensive and time consuming and subjected to variability due to intra-
and inter-expert differences at both pre- and post-treatment stages. As a
result, there is a critical need for an accurate and automated segmentation
method for uterine fibroids. In recent years, deep learning algorithms have
shown re-markable improvements in medical image segmentation, outperforming
traditional methods. These approaches offer the potential for fully automated
segmentation. Several studies have explored the use of deep learning models to
achieve automated segmentation of uterine fibroids. However, most of the
previous work has been conducted using private datasets, which poses challenges
for validation and comparison between studies. In this study, we leverage the
publicly available Uterine Myoma MRI Dataset (UMD) to establish a baseline for
automated segmentation of uterine fibroids, enabling standardized evaluation
and facilitating future research in this domain.

</details>


### [111] [HistoViT: Vision Transformer for Accurate and Scalable Histopathological Cancer Diagnosis](https://arxiv.org/abs/2508.11181)
*Faisal Ahmed*

Main category: eess.IV

TL;DR: 提出了一种基于Transformer的深度学习框架，用于组织病理学图像中的多类肿瘤分类，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现代病理学中癌症诊断的准确性和可扩展性挑战，特别是针对乳腺癌、前列腺癌、骨癌和宫颈癌等复杂组织学变异性高的恶性肿瘤。

Method: 采用微调的Vision Transformer (ViT)架构，简化预处理流程，将全切片图像转换为PyTorch张量并进行数据标准化。

Result: 在四个基准数据集上表现优异，分类准确率分别为乳腺癌99.32%、前列腺癌96.92%、骨癌95.28%、宫颈癌96.94%，AUC均超过99%。

Conclusion: 基于Transformer的架构在数字病理学中具有鲁棒性、泛化性和临床潜力，为可靠、自动化和可解释的癌症诊断系统提供了重要进展。

Abstract: Accurate and scalable cancer diagnosis remains a critical challenge in modern
pathology, particularly for malignancies such as breast, prostate, bone, and
cervical, which exhibit complex histological variability. In this study, we
propose a transformer-based deep learning framework for multi-class tumor
classification in histopathological images. Leveraging a fine-tuned Vision
Transformer (ViT) architecture, our method addresses key limitations of
conventional convolutional neural networks, offering improved performance,
reduced preprocessing requirements, and enhanced scalability across tissue
types. To adapt the model for histopathological cancer images, we implement a
streamlined preprocessing pipeline that converts tiled whole-slide images into
PyTorch tensors and standardizes them through data normalization. This ensures
compatibility with the ViT architecture and enhances both convergence stability
and overall classification performance. We evaluate our model on four benchmark
datasets: ICIAR2018 (breast), SICAPv2 (prostate), UT-Osteosarcoma (bone), and
SipakMed (cervical) dataset -- demonstrating consistent outperformance over
existing deep learning methods. Our approach achieves classification accuracies
of 99.32%, 96.92%, 95.28%, and 96.94% for breast, prostate, bone, and cervical
cancers respectively, with area under the ROC curve (AUC) scores exceeding 99%
across all datasets. These results confirm the robustness, generalizability,
and clinical potential of transformer-based architectures in digital pathology.
Our work represents a significant advancement toward reliable, automated, and
interpretable cancer diagnosis systems that can alleviate diagnostic burdens
and improve healthcare outcomes.

</details>


### [112] [Efficient Image-to-Image Schrödinger Bridge for CT Field of View Extension](https://arxiv.org/abs/2508.11211)
*Zhenhao Li,Long Yang,Xiaojie Yin,Haijun Yu,Jiazhou Wang,Hongbin Han,Weigang Hu,Yixing Huang*

Main category: eess.IV

TL;DR: 提出了一种基于Schrödinger Bridge扩散模型的高效CT视野扩展框架，显著提升了重建速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统CT扫描在视野受限时会导致数据截断和伪影，现有重建算法难以恢复准确解剖结构，扩散模型虽先进但计算成本高且推理速度慢。

Method: 采用图像到图像的Schrödinger Bridge（I²SB）扩散模型，直接学习有限视野与扩展视野图像之间的随机映射，避免了传统扩散模型从纯高斯噪声合成的迭代过程。

Result: 在模拟噪声数据和真实数据上，I²SB的均方根误差分别为49.8HU和152.0HU，优于现有扩散模型；单步推理速度达0.19秒/切片，比cDDPM快700倍。

Conclusion: I²SB在准确性和效率上均表现优异，适合实时或临床部署。

Abstract: Computed tomography (CT) is a cornerstone imaging modality for non-invasive,
high-resolution visualization of internal anatomical structures. However, when
the scanned object exceeds the scanner's field of view (FOV), projection data
are truncated, resulting in incomplete reconstructions and pronounced artifacts
near FOV boundaries. Conventional reconstruction algorithms struggle to recover
accurate anatomy from such data, limiting clinical reliability. Deep learning
approaches have been explored for FOV extension, with diffusion generative
models representing the latest advances in image synthesis. Yet, conventional
diffusion models are computationally demanding and slow at inference due to
their iterative sampling process. To address these limitations, we propose an
efficient CT FOV extension framework based on the image-to-image Schr\"odinger
Bridge (I$^2$SB) diffusion model. Unlike traditional diffusion models that
synthesize images from pure Gaussian noise, I$^2$SB learns a direct stochastic
mapping between paired limited-FOV and extended-FOV images. This direct
correspondence yields a more interpretable and traceable generative process,
enhancing anatomical consistency and structural fidelity in reconstructions.
I$^2$SB achieves superior quantitative performance, with root-mean-square error
(RMSE) values of 49.8\,HU on simulated noisy data and 152.0HU on real data,
outperforming state-of-the-art diffusion models such as conditional denoising
diffusion probabilistic models (cDDPM) and patch-based diffusion methods.
Moreover, its one-step inference enables reconstruction in just 0.19s per 2D
slice, representing over a 700-fold speedup compared to cDDPM (135s) and
surpassing diffusionGAN (0.58s), the second fastest. This combination of
accuracy and efficiency makes I$^2$SB highly suitable for real-time or clinical
deployment.

</details>


### [113] [Guiding WaveMamba with Frequency Maps for Image Debanding](https://arxiv.org/abs/2508.11331)
*Xinyi Wang,Smaranda Tasmoc,Nantheera Anantrasirichai,Angeliki Katsenou*

Main category: eess.IV

TL;DR: 提出了一种基于小波状态空间模型和频率掩码图的条带修复方法，有效抑制条带伪影并保留高频细节。


<details>
  <summary>Details</summary>
Motivation: 现代编解码器在低比特率压缩时容易在平滑区域（如天空）产生条带伪影，影响视觉质量，尤其是在用户生成内容中。

Method: 采用小波状态空间模型和频率掩码图进行条带修复，并对比开源方法的性能。

Result: 在公开数据集上，所提方法优于现有技术（DBI值为0.082），且能保留图像纹理。

Conclusion: 该方法在条带修复中表现优异，代码和补充材料已开源。

Abstract: Compression at low bitrates in modern codecs often introduces banding
artifacts, especially in smooth regions such as skies. These artifacts degrade
visual quality and are common in user-generated content due to repeated
transcoding. We propose a banding restoration method that employs the Wavelet
State Space Model and a frequency masking map to preserve high-frequency
details. Furthermore, we provide a benchmark of open-source banding restoration
methods and evaluate their performance on two public banding image datasets.
Experimentation on the available datasets suggests that the proposed
post-processing approach effectively suppresses banding compared to the
state-of-the-art method (a DBI value of 0.082 on BAND-2k) while preserving
image textures. Visual inspections of the results confirm this. Code and
supplementary material are available at:
https://github.com/xinyiW915/Debanding-PCS2025.

</details>


### [114] [AnatoMaskGAN: GNN-Driven Slice Feature Fusion and Noise Augmentation for Medical Semantic Image Synthesis](https://arxiv.org/abs/2508.11375)
*Zonglin Wu,Yule Xue,Qianxiang Hu,Yaoyao Feng,Yuqi Ma,Shanxiong Chen*

Main category: eess.IV

TL;DR: AnatoMaskGAN通过嵌入切片相关空间特征、引入多样图像增强策略和优化深度特征学习，显著提升了复杂医学图像的合成性能。


<details>
  <summary>Details</summary>
Motivation: 现有GAN方法在复杂扫描中缺乏空间一致性，且通常只能生成一对一图像，限制了数据增强和分析的效果。

Method: 设计了基于GNN的强相关切片特征融合模块、三维空间噪声注入策略和灰度纹理分类器，以全面捕捉解剖细节并增强结构多样性。

Result: 在L2R-OASIS和L2R-Abdomen CT数据集上，PSNR和SSIM指标均优于现有最佳模型，证明了其在重建精度和感知质量上的优越性。

Conclusion: AnatoMaskGAN的核心设计模块均对性能有显著贡献，验证了其在医学图像合成中的独立价值和整体有效性。

Abstract: Medical semantic-mask synthesis boosts data augmentation and analysis, yet
most GAN-based approaches still produce one-to-one images and lack spatial
consistency in complex scans. To address this, we propose AnatoMaskGAN, a novel
synthesis framework that embeds slice-related spatial features to precisely
aggregate inter-slice contextual dependencies, introduces diverse
image-augmentation strategies, and optimizes deep feature learning to improve
performance on complex medical images. Specifically, we design a GNN-based
strongly correlated slice-feature fusion module to model spatial relationships
between slices and integrate contextual information from neighboring slices,
thereby capturing anatomical details more comprehensively; we introduce a
three-dimensional spatial noise-injection strategy that weights and fuses
spatial features with noise to enhance modeling of structural diversity; and we
incorporate a grayscale-texture classifier to optimize grayscale distribution
and texture representation during generation. Extensive experiments on the
public L2R-OASIS and L2R-Abdomen CT datasets show that AnatoMaskGAN raises PSNR
on L2R-OASIS to 26.50 dB (0.43 dB higher than the current state of the art) and
achieves an SSIM of 0.8602 on L2R-Abdomen CT--a 0.48 percentage-point gain over
the best model, demonstrating its superiority in reconstruction accuracy and
perceptual quality. Ablation studies that successively remove the slice-feature
fusion module, spatial 3D noise-injection strategy, and grayscale-texture
classifier reveal that each component contributes significantly to PSNR, SSIM,
and LPIPS, further confirming the independent value of each core design in
enhancing reconstruction accuracy and perceptual quality.

</details>


### [115] [LKFMixer: Exploring Large Kernel Feature For Efficient Image Super-Resolution](https://arxiv.org/abs/2508.11391)
*Yinggan Tang,Quanwei Hu*

Main category: eess.IV

TL;DR: LKFMixer是一种纯CNN模型，通过大卷积核模拟自注意力机制，提升图像超分辨率性能，同时保持轻量化。


<details>
  <summary>Details</summary>
Motivation: 解决自注意力机制在图像超分辨率中计算开销大的问题，提出轻量化替代方案。

Method: 使用31x31大卷积核扩大感受野，通过坐标分解减少参数；引入SFMB和FSB模块优化特征提取。

Result: 在Manga109数据集上，LKFMixer-L比SwinIR-light PSNR提升0.6dB，推理速度快5倍。

Conclusion: LKFMixer在性能和效率上均优于现有方法，为轻量化超分辨率提供了新思路。

Abstract: The success of self-attention (SA) in Transformer demonstrates the importance
of non-local information to image super-resolution (SR), but the huge computing
power required makes it difficult to implement lightweight models. To solve
this problem, we propose a pure convolutional neural network (CNN) model,
LKFMixer, which utilizes large convolutional kernel to simulate the ability of
self-attention to capture non-local features. Specifically, we increase the
kernel size to 31 to obtain the larger receptive field as possible, and reduce
the parameters and computations by coordinate decomposition. Meanwhile, a
spatial feature modulation block (SFMB) is designed to enhance the focus of
feature information on both spatial and channel dimension. In addition, by
introducing feature selection block (FSB), the model can adaptively adjust the
weights between local features and non-local features. Extensive experiments
show that the proposed LKFMixer family outperform other state-of-the-art (SOTA)
methods in terms of SR performance and reconstruction quality. In particular,
compared with SwinIR-light on Manga109 dataset, LKFMixer-L achieves 0.6dB PSNR
improvement at $\times$4 scale, while the inference speed is $\times$5 times
faster. The code is available at https://github.com/Supereeeee/LKFMixer.

</details>


### [116] [Subcortical Masks Generation in CT Images via Ensemble-Based Cross-Domain Label Transfer](https://arxiv.org/abs/2508.11450)
*Augustine X. W. Lee,Pak-Hei Yeung,Jagath C. Rajapakse*

Main category: eess.IV

TL;DR: 提出一种自动集成框架，利用MRI模型为CT扫描生成高质量皮层下分割标签，填补了CT数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 解决CT扫描中皮层下分割数据不足的问题，促进脑损伤和神经退行性疾病的计算机辅助诊断。

Method: 通过集成MRI模型，构建鲁棒的集成管道，应用于未标注的MRI-CT配对数据。

Result: 实验表明框架性能优越，生成的CT数据集提升了分割模型的性能。

Conclusion: 开源了代码、数据集和模型，推动了CT皮层下分割的研究。

Abstract: Subcortical segmentation in neuroimages plays an important role in
understanding brain anatomy and facilitating computer-aided diagnosis of
traumatic brain injuries and neurodegenerative disorders. However, training
accurate automatic models requires large amounts of labelled data. Despite the
availability of publicly available subcortical segmentation datasets for
Magnetic Resonance Imaging (MRI), a significant gap exists for Computed
Tomography (CT). This paper proposes an automatic ensemble framework to
generate high-quality subcortical segmentation labels for CT scans by
leveraging existing MRI-based models. We introduce a robust ensembling pipeline
to integrate them and apply it to unannotated paired MRI-CT data, resulting in
a comprehensive CT subcortical segmentation dataset. Extensive experiments on
multiple public datasets demonstrate the superior performance of our proposed
framework. Furthermore, using our generated CT dataset, we train segmentation
models that achieve improved performance on related segmentation tasks. To
facilitate future research, we make our source code, generated dataset, and
trained models publicly available at
https://github.com/SCSE-Biomedical-Computing-Group/CT-Subcortical-Segmentation,
marking the first open-source release for CT subcortical segmentation to the
best of our knowledge.

</details>


### [117] [Semi-Supervised Learning with Online Knowledge Distillation for Skin Lesion Classification](https://arxiv.org/abs/2508.11511)
*Siyamalan Manivannan*

Main category: eess.IV

TL;DR: 提出了一种结合集成学习和在线知识蒸馏的半监督深度学习方法，用于皮肤病变分类，减少对大量标注数据的依赖，并在资源受限环境中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖全监督学习，需要大量标注数据，获取成本高。本研究旨在通过半监督学习减轻标注负担。

Method: 训练卷积神经网络集成模型，通过在线知识蒸馏将集成模型的洞察传递给单个模型，提升整体性能。

Result: 在ISIC 2018和2019数据集上表现优于现有方法，知识蒸馏后的单个模型性能优于独立训练模型。

Conclusion: 该方法通过集成学习和在线知识蒸馏，显著减少对标注数据的需求，同时提供高效的皮肤病变分类解决方案。

Abstract: Deep Learning has emerged as a promising approach for skin lesion analysis.
However, existing methods mostly rely on fully supervised learning, requiring
extensive labeled data, which is challenging and costly to obtain. To alleviate
this annotation burden, this study introduces a novel semi-supervised deep
learning approach that integrates ensemble learning with online knowledge
distillation for enhanced skin lesion classification. Our methodology involves
training an ensemble of convolutional neural network models, using online
knowledge distillation to transfer insights from the ensemble to its members.
This process aims to enhance the performance of each model within the ensemble,
thereby elevating the overall performance of the ensemble itself.
Post-training, any individual model within the ensemble can be deployed at test
time, as each member is trained to deliver comparable performance to the
ensemble. This is particularly beneficial in resource-constrained environments.
Experimental results demonstrate that the knowledge-distilled individual model
performs better than independently trained models. Our approach demonstrates
superior performance on both the \emph{International Skin Imaging
Collaboration} 2018 and 2019 public benchmark datasets, surpassing current
state-of-the-art results. By leveraging ensemble learning and online knowledge
distillation, our method reduces the need for extensive labeled data while
providing a more resource-efficient solution for skin lesion classification in
real-world scenarios.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [118] [Temporally-Similar Structure-Aware Spatiotemporal Fusion of Satellite Images](https://arxiv.org/abs/2508.11259)
*Ryosuke Isono,Shunsuke Ono*

Main category: eess.SP

TL;DR: TSSTF是一种新颖的时空融合框架，通过TGTV和TGEC机制提升卫星图像的空间结构细节保留能力，并在噪声条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决现有时空融合方法在噪声环境下无法捕捉精细空间结构的问题。

Method: 引入TGTV和TGEC机制，将时空融合任务建模为约束优化问题，并采用预条件原始对偶分裂方法求解。

Result: 在无噪声条件下与现有方法相当，在噪声条件下表现更优。

Conclusion: TSSTF在噪声环境下具有优越性能，并提供了可复现的参数推荐。

Abstract: This paper proposes a novel spatiotemporal (ST) fusion framework for
satellite images, named Temporally-Similar Structure-Aware ST fusion (TSSTF).
ST fusion is a promising approach to address the trade-off between the spatial
and temporal resolution of satellite images. In real-world scenarios, observed
satellite images are severely degraded by noise due to measurement equipment
and environmental conditions. Consequently, some recent studies have focused on
enhancing the robustness of ST fusion methods against noise. However, existing
noise-robust ST fusion approaches often fail to capture fine spatial structure,
leading to oversmoothing and artifacts. To address this issue, TSSTF introduces
two key mechanisms: Temporally-Guided Total Variation (TGTV) and
Temporally-Guided Edge Constraint (TGEC). TGTV is a novel regularization
function that promotes spatial piecewise smoothness while preserving structural
details, guided by a reference high spatial resolution image acquired on a
nearby date. TGEC enforces consistency in edge locations between two temporally
adjacent images, while allowing for spectral variations. We formulate the ST
fusion task as a constrained optimization problem incorporating TGTV and TGEC,
and develop an efficient algorithm based on a preconditioned primal-dual
splitting method. Experimental results demonstrate that TSSTF performs
comparably to state-of-the-art methods under noise-free conditions and
outperforms them under noisy conditions. Additionally, we provide a
comprehensive set of recommended parameter values that consistently yield high
performance across diverse target regions and noise conditions, aiming to
enhance reproducibility and practical utility.

</details>
