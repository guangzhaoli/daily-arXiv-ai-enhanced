<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 64]
- [eess.IV](#eess.IV) [Total: 4]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.LG](#cs.LG) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Australian Supermarket Object Set (ASOS): A Benchmark Dataset of Physical Objects and 3D Models for Robotics and Computer Vision](https://arxiv.org/abs/2509.09720)
*Akansel Cosgun,Lachlan Chumbley,Benjamin J. Meyer*

Main category: cs.CV

TL;DR: ASOS是一个包含50种常见超市商品的3D纹理网格数据集，专为机器人和计算机视觉基准测试设计，强调真实性和可获取性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多依赖合成模型或难以获取的专业物品，缺乏真实世界应用的实用性和可访问性。

Method: 采用运动恢复结构技术，通过高分辨率成像生成水密3D网格，包含10个不同类别的日常超市商品。

Result: 创建了一个成本效益高、易于获取的3D物体数据集，涵盖多种形状、尺寸和重量的日常物品。

Conclusion: ASOS数据集因其真实性和可访问性，在物体检测、姿态估计和机器人应用基准测试中具有重要价值。

Abstract: This paper introduces the Australian Supermarket Object Set (ASOS), a
comprehensive dataset comprising 50 readily available supermarket items with
high-quality 3D textured meshes designed for benchmarking in robotics and
computer vision applications. Unlike existing datasets that rely on synthetic
models or specialized objects with limited accessibility, ASOS provides a
cost-effective collection of common household items that can be sourced from a
major Australian supermarket chain. The dataset spans 10 distinct categories
with diverse shapes, sizes, and weights. 3D meshes are acquired by a
structure-from-motion techniques with high-resolution imaging to generate
watertight meshes. The dataset's emphasis on accessibility and real-world
applicability makes it valuable for benchmarking object detection, pose
estimation, and robotics applications.

</details>


### [2] [A Multimodal RAG Framework for Housing Damage Assessment: Collaborative Optimization of Image Encoding and Policy Vector Retrieval](https://arxiv.org/abs/2509.09721)
*Jiayi Miao,Dingxin Lu,Zhuqi Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的多模态检索增强生成框架(MM-RAG)，用于灾后房屋损坏评估，通过双分支编码器和跨模态交互提升检索准确性和分类性能。


<details>
  <summary>Details</summary>
Motivation: 灾后准确的房屋损坏评估对保险理赔和资源规划至关重要，需要处理图像和文本多模态信息来理解建筑损坏特征和匹配保险政策。

Method: 采用双分支编码器结构：图像分支使用ResNet+Transformer提取建筑损坏特征，文本分支使用BERT检索器处理文本向量化；集成跨模态交互模块通过多头注意力实现语义对齐；引入模态注意力门控机制动态控制生成过程中的视觉证据和文本先验信息。

Result: 在检索准确性和损坏严重程度分类指标上表现出优异性能，Top-1检索准确率提升了9.6%。

Conclusion: 该MM-RAG框架通过端到端训练和多任务优化，实现了图像理解和政策匹配的协同学习，为灾后房屋损坏评估提供了有效的多模态解决方案。

Abstract: After natural disasters, accurate evaluations of damage to housing are
important for insurance claims response and planning of resources. In this
work, we introduce a novel multimodal retrieval-augmented generation (MM-RAG)
framework. On top of classical RAG architecture, we further the framework to
devise a two-branch multimodal encoder structure that the image branch employs
a visual encoder composed of ResNet and Transformer to extract the
characteristic of building damage after disaster, and the text branch harnesses
a BERT retriever for the text vectorization of posts as well as insurance
policies and for the construction of a retrievable restoration index. To impose
cross-modal semantic alignment, the model integrates a cross-modal interaction
module to bridge the semantic representation between image and text via
multi-head attention. Meanwhile, in the generation module, the introduced modal
attention gating mechanism dynamically controls the role of visual evidence and
text prior information during generation. The entire framework takes end-to-end
training, and combines the comparison loss, the retrieval loss and the
generation loss to form multi-task optimization objectives, and achieves image
understanding and policy matching in collaborative learning. The results
demonstrate superior performance in retrieval accuracy and classification index
on damage severity, where the Top-1 retrieval accuracy has been improved by
9.6%.

</details>


### [3] [Improving MLLM Historical Record Extraction with Test-Time Image](https://arxiv.org/abs/2509.09722)
*Taylor Archibald,Tony Martinez*

Main category: cs.CV

TL;DR: 提出了一种基于LLM的集成框架，通过多图像增强和序列对齐技术提升噪声历史文档转录准确率4个百分点


<details>
  <summary>Details</summary>
Motivation: 解决噪声历史文档转录中LLM模型的不稳定性问题，提高转录准确性并提供置信度评估

Method: 使用Gemini 2.0 Flash对多个增强图像变体进行转录，采用自定义Needleman Wunsch风格对齐器融合输出生成共识转录和置信度分数

Result: 在622份宾夕法尼亚州死亡记录数据集上，相比单次转录基线准确率提升4个百分点，发现填充和模糊增强最有效

Conclusion: 该方法简单、可扩展，可立即部署到其他文档集合和转录模型，为历史文档数字化提供实用解决方案

Abstract: We present a novel ensemble framework that stabilizes LLM based text
extraction from noisy historical documents. We transcribe multiple augmented
variants of each image with Gemini 2.0 Flash and fuse these outputs with a
custom Needleman Wunsch style aligner that yields both a consensus
transcription and a confidence score. We present a new dataset of 622
Pennsylvania death records, and demonstrate our method improves transcription
accuracy by 4 percentage points relative to a single shot baseline. We find
that padding and blurring are the most useful for improving accuracy, while
grid warp perturbations are best for separating high and low confidence cases.
The approach is simple, scalable, and immediately deployable to other document
collections and transcription models.

</details>


### [4] [MITS: A Large-Scale Multimodal Benchmark Dataset for Intelligent Traffic Surveillance](https://arxiv.org/abs/2509.09730)
*Kaikai Zhao,Zhaoxiang Liu,Peng Wang,Xin Wang,Zhicheng Ma,Yajun Xu,Wenjing Zhang,Yibing Nan,Kai Wang,Shiguo Lian*

Main category: cs.CV

TL;DR: MITS是首个专门为智能交通监控设计的大规模多模态基准数据集，包含17万张真实交通监控图像和500万条指令问答对，显著提升了多模态大模型在交通监控任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的通用多模态大模型在智能交通监控领域表现有限，主要缺乏专门的多模态数据集来支持该领域的任务训练和评估。

Method: 通过系统化的数据生成流程，收集17万张真实交通监控图像，标注8个主要类别和24个子类别，生成高质量图像描述和500万条指令问答对，涵盖5个关键交通监控任务。

Result: 在MITS数据集上微调后，主流多模态大模型性能显著提升：LLaVA-1.5从0.494提升到0.905（+83.2%），LLaVA-1.6从0.678到0.921（+35.8%），Qwen2-VL从0.584到0.926（+58.6%），Qwen2.5-VL从0.732到0.930（+27.0%）。

Conclusion: MITS数据集填补了智能交通监控领域多模态数据的空白，为开发交通专用多模态应用提供了高质量资源，显著推动了智能交通监控和多模态大模型研究的发展。

Abstract: General-domain large multimodal models (LMMs) have achieved significant
advances in various image-text tasks. However, their performance in the
Intelligent Traffic Surveillance (ITS) domain remains limited due to the
absence of dedicated multimodal datasets. To address this gap, we introduce
MITS (Multimodal Intelligent Traffic Surveillance), the first large-scale
multimodal benchmark dataset specifically designed for ITS. MITS includes
170,400 independently collected real-world ITS images sourced from traffic
surveillance cameras, annotated with eight main categories and 24 subcategories
of ITS-specific objects and events under diverse environmental conditions.
Additionally, through a systematic data generation pipeline, we generate
high-quality image captions and 5 million instruction-following visual
question-answer pairs, addressing five critical ITS tasks: object and event
recognition, object counting, object localization, background analysis, and
event reasoning. To demonstrate MITS's effectiveness, we fine-tune mainstream
LMMs on this dataset, enabling the development of ITS-specific applications.
Experimental results show that MITS significantly improves LMM performance in
ITS applications, increasing LLaVA-1.5's performance from 0.494 to 0.905
(+83.2%), LLaVA-1.6's from 0.678 to 0.921 (+35.8%), Qwen2-VL's from 0.584 to
0.926 (+58.6%), and Qwen2.5-VL's from 0.732 to 0.930 (+27.0%). We release the
dataset, code, and models as open-source, providing high-value resources to
advance both ITS and LMM research.

</details>


### [5] [Decomposing Visual Classification: Assessing Tree-Based Reasoning in VLMs](https://arxiv.org/abs/2509.09732)
*Sary Elmansoury,Islam Mesabah,Gerrit Großmann,Peter Neigel,Raj Bhalwankar,Daniel Kondermann,Sebastian J. Vollmer*

Main category: cs.CV

TL;DR: 研究探讨结构化树状推理是否能提升视觉语言模型在细粒度分类任务中的性能，但发现标准零样本提示方法仍然优于树状推理方法


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在零样本视觉分类方面表现出色，但在细粒度任务和大规模分层标签空间中的性能尚未得到充分研究，需要探索结构化推理方法的效果

Method: 引入基于决策树的框架，将分类分解为可解释的决策过程，在细粒度(GTSRB)和粗粒度(CIFAR-10)数据集上评估，并探索使用LLM生成的类别和图像描述来增强树状提示

Result: 模型在理解树知识方面达到98.2%准确率，但树状推理始终逊于标准零样本提示。添加图像描述后，树状方法和零样本方法的性能都得到了提升

Conclusion: 研究揭示了结构化推理在视觉分类中的局限性，为设计更可解释的视觉语言模型系统提供了重要见解

Abstract: Vision language models (VLMs) excel at zero-shot visual classification, but
their performance on fine-grained tasks and large hierarchical label spaces is
understudied. This paper investigates whether structured, tree-based reasoning
can enhance VLM performance. We introduce a framework that decomposes
classification into interpretable decisions using decision trees and evaluates
it on fine-grained (GTSRB) and coarse-grained (CIFAR-10) datasets. Although the
model achieves 98.2% accuracy in understanding the tree knowledge, tree-based
reasoning consistently underperforms standard zero-shot prompting. We also
explore enhancing the tree prompts with LLM-generated classes and image
descriptions to improve alignment. The added description enhances the
performance of the tree-based and zero-shot methods. Our findings highlight
limitations of structured reasoning in visual classification and offer insights
for designing more interpretable VLM systems.

</details>


### [6] [World Modeling with Probabilistic Structure Integration](https://arxiv.org/abs/2509.09737)
*Klemen Kotar,Wanhee Lee,Rahul Venkatesh,Honglin Chen,Daniel Bear,Jared Watrous,Simon Kim,Khai Loong Aw,Lilian Naing Chen,Stefan Stojanov,Kevin Feigelis,Imran Thobani,Alex Durango,Khaled Jedoui,Atlas Kazemian,Dan Yamins*

Main category: cs.CV

TL;DR: PSI系统通过三步骤循环学习可控的世界模型：概率预测构建概率图模型，结构提取通过因果推断提取低维特征，集成将这些特征作为新token类型反馈训练，在视频数据上实现多种预测和理解任务。


<details>
  <summary>Details</summary>
Motivation: 构建能够从数据中学习丰富可控、灵活提示的世界模型，支持对各种变量的条件分布建模，并提取有意义的底层结构来增强模型能力。

Method: 三步骤循环：1)概率预测构建随机访问自回归序列模型；2)结构提取通过因果推断零样本提取低维特征；3)集成将结构转换为新token类型并反馈训练。

Result: 在1.4万亿token互联网视频数据上训练，实现了视频预测和理解推理，提取了最先进的光流、自监督深度和对象分割，并通过结构集成实现了预测改进。

Conclusion: PSI系统通过循环增强机制有效提升了世界模型的建模能力和控制灵活性，创建了类似LLM的通用提示语言，在视频理解任务中表现出色。

Abstract: We present Probabilistic Structure Integration (PSI), a system for learning
richly controllable and flexibly promptable world models from data. PSI
consists of a three-step cycle. The first step, Probabilistic prediction,
involves building a probabilistic graphical model Psi of the data, in the form
of a random-access autoregressive sequence model. Psi supports a complete set
of learned conditional distributions describing the dependence of any variables
in the data on any other set of variables. In step 2, Structure extraction, we
show how to extract underlying low-dimensional properties in the data,
corresponding to a diverse set of meaningful "intermediate structures", in a
zero-shot fashion via causal inference on Psi. Step 3, Integration, completes
the cycle by converting these structures into new token types that are then
continually mixed back into the training diet as conditioning signals and
prediction targets. Each such cycle augments the capabilities of Psi, both
allowing it to model the underlying data better, and creating new control
handles -- akin to an LLM-like universal prompting language. We train an
instance of Psi on 1.4 trillion tokens of internet video data; we use it to
perform a variety of useful video prediction and understanding inferences; we
extract state-of-the-art optical flow, self-supervised depth and object
segmentation; and we use these structures to support a full cycle of predictive
improvements.

</details>


### [7] [Images in Motion?: A First Look into Video Leakage in Collaborative Deep Learning](https://arxiv.org/abs/2509.09742)
*Md Fazle Rasul,Alanood Alqobaisi,Bruhadeshwar Bezawada,Indrakshi Ray*

Main category: cs.CV

TL;DR: 联邦学习中梯度反演攻击对视频数据的隐私泄露风险分析，发现特征提取器提供更强保护但攻击仍可能成功，超分辨率技术可提升攻击效果


<details>
  <summary>Details</summary>
Motivation: 联邦学习通过只交换模型更新来保护隐私，但梯度反演攻击可以从中反推原始数据。虽然图像、文本和表格数据的风险已知，但视频数据的隐私泄露风险尚未被研究

Method: 评估两种视频分类方法：使用预训练特征提取器的方法和处理原始视频帧的简单变换方法。测试在不同参考帧访问权限下的攻击效果，并应用图像超分辨率技术提升重建质量

Result: 特征提取器对梯度反演攻击具有更强抵抗力，但分类器复杂度不足时仍可能泄露。超分辨率技术能显著提升攻击者重建视频的质量，攻击在零参考帧、单参考帧和多参考帧场景下均有效

Conclusion: 联邦学习中的视频数据泄露是真实存在的威胁，特征提取器提供保护但并非绝对安全，需要进一步研究泄露发生的具体条件和防护措施

Abstract: Federated learning (FL) allows multiple entities to train a shared model
collaboratively. Its core, privacy-preserving principle is that participants
only exchange model updates, such as gradients, and never their raw, sensitive
data. This approach is fundamental for applications in domains where privacy
and confidentiality are important. However, the security of this very mechanism
is threatened by gradient inversion attacks, which can reverse-engineer private
training data directly from the shared gradients, defeating the purpose of FL.
While the impact of these attacks is known for image, text, and tabular data,
their effect on video data remains an unexamined area of research. This paper
presents the first analysis of video data leakage in FL using gradient
inversion attacks. We evaluate two common video classification approaches: one
employing pre-trained feature extractors and another that processes raw video
frames with simple transformations. Our initial results indicate that the use
of feature extractors offers greater resilience against gradient inversion
attacks. We also demonstrate that image super-resolution techniques can enhance
the frames extracted through gradient inversion attacks, enabling attackers to
reconstruct higher-quality videos. Our experiments validate this across
scenarios where the attacker has access to zero, one, or more reference frames
from the target environment. We find that although feature extractors make
attacks more challenging, leakage is still possible if the classifier lacks
sufficient complexity. We, therefore, conclude that video data leakage in FL is
a viable threat, and the conditions under which it occurs warrant further
investigation.

</details>


### [8] [A Co-Training Semi-Supervised Framework Using Faster R-CNN and YOLO Networks for Object Detection in Densely Packed Retail Images](https://arxiv.org/abs/2509.09750)
*Hossein Yazdanjouei,Arash Mansouri,Mohammad Shokouhifar*

Main category: cs.CV

TL;DR: 提出一种用于密集零售环境物体检测的半监督协同训练框架，结合Faster R-CNN和YOLO进行伪标签交换，集成多种分类器提高鲁棒性，通过元启发式算法优化超参数，在SKU-110k数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决密集零售环境中有限标注数据和复杂条件（遮挡、重叠物体）带来的挑战，减少人工标注成本，适应零售场景中频繁的产品和布局变化。

Method: 采用半监督协同训练框架，结合Faster R-CNN（ResNet骨干）进行精确定位和YOLO（Darknet骨干）捕捉全局上下文，通过伪标签交换机制；集成XGBoost、随机森林和SVM进行分类；使用元启发式算法优化超参数。

Result: 在SKU-110k数据集上表现出强大的性能，提高了在遮挡和重叠物体场景中的检测精度，证明了框架的可扩展性和实用性。

Conclusion: 该框架有效降低了人工标注依赖，减少了标注成本，能够很好地适应零售环境的变化，适用于自动化库存跟踪、产品监控和结账系统等实际零售应用。

Abstract: This study proposes a semi-supervised co-training framework for object
detection in densely packed retail environments, where limited labeled data and
complex conditions pose major challenges. The framework combines Faster R-CNN
(utilizing a ResNet backbone) for precise localization with YOLO (employing a
Darknet backbone) for global context, enabling mutual pseudo-label exchange
that improves accuracy in scenes with occlusion and overlapping objects. To
strengthen classification, it employs an ensemble of XGBoost, Random Forest,
and SVM, utilizing diverse feature representations for higher robustness.
Hyperparameters are optimized using a metaheuristic-driven algorithm, enhancing
precision and efficiency across models. By minimizing reliance on manual
labeling, the approach reduces annotation costs and adapts effectively to
frequent product and layout changes common in retail. Experiments on the
SKU-110k dataset demonstrate strong performance, highlighting the scalability
and practicality of the proposed framework for real-world retail applications
such as automated inventory tracking, product monitoring, and checkout systems.

</details>


### [9] [Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging](https://arxiv.org/abs/2509.09785)
*Moslem Yazdanpanah,Ali Bahri,Mehrdad Noori,Sahar Dastani,Gustavo Adolfo Vargas Hakim,David Osowiechi,Ismail Ben Ayed,Christian Desrosiers*

Main category: cs.CV

TL;DR: 提出Token Purging (PG)方法，一种无需反向传播的测试时自适应方法，通过移除受域偏移影响严重的token来提高3D点云分类性能


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分类中由于分布偏移导致的性能下降问题，现有TTA方法需要迭代更新且计算成本高

Method: 提出两种变体：PG-SP（利用源域统计信息）和PG-SF（完全无源域版本，依赖CLS-token驱动自适应），在注意力层前移除受域偏移影响严重的token

Result: 在ModelNet40-C、ShapeNet-C和ScanObjectNN-C数据集上，PG-SP比最先进的无反向传播方法平均准确率高10.3%，PG-SF在无源域自适应方面创下新基准，速度提升12.4倍，内存效率提升5.5倍

Conclusion: Token Purging是一种高效、内存友好的测试时自适应方法，适用于实际部署，无需反向传播即可实现鲁棒的性能提升

Abstract: Test-time adaptation (TTA) is crucial for mitigating performance degradation
caused by distribution shifts in 3D point cloud classification. In this work,
we introduce Token Purging (PG), a novel backpropagation-free approach that
removes tokens highly affected by domain shifts before they reach attention
layers. Unlike existing TTA methods, PG operates at the token level, ensuring
robust adaptation without iterative updates. We propose two variants: PG-SP,
which leverages source statistics, and PG-SF, a fully source-free version
relying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,
ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of
+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,
while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is
12.4 times faster and 5.5 times more memory efficient than our baseline, making
it suitable for real-world deployment. Code is available at
\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}

</details>


### [10] [Fine-Grained Cross-View Localization via Local Feature Matching and Monocular Depth Priors](https://arxiv.org/abs/2509.09792)
*Zimin Xia,Chenghao Xu,Alexandre Alahi*

Main category: cs.CV

TL;DR: 提出了一种准确且高度可解释的细粒度跨视角定位方法，通过匹配地面图像与参考航空图像的局部特征来估计3自由度姿态，避免了传统BEV转换中的信息损失问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法将地面图像转换为鸟瞰图(BEV)表示时，由于透视失真或高度信息压缩会导致信息损失，从而降低与航空视图的对齐质量。

Method: 直接在航空图像和地面图像之间建立对应关系，仅使用单目深度先验将匹配的关键点提升到BEV空间。支持度量和相对深度，采用尺度感知的Procrustes对齐来估计相机姿态。

Result: 实验结果表明，该方法仅需相机姿态的弱监督就能学习准确的局部特征对应关系，在跨区域泛化和未知方向等挑战性条件下实现了优越的定位性能。

Conclusion: 该方法具有灵活性，可与各种相对深度模型兼容且无需针对每个模型进行微调，结合强大的定位性能，使其非常适合实际部署。

Abstract: We propose an accurate and highly interpretable fine-grained cross-view
localization method that estimates the 3 Degrees of Freedom pose of a
ground-level image by matching its local features with a reference aerial
image. Previous methods typically transform the ground image into a bird's-eye
view (BEV) representation and then align it with the aerial image for
localization. However, this transformation often leads to information loss due
to perspective distortion or compression of height information, thereby
degrading alignment quality with the aerial view. In contrast, our method
directly establishes correspondences between ground and aerial images and lifts
only the matched keypoints to BEV space using monocular depth prior. Notably,
modern depth predictors can provide reliable metric depth when the test samples
are similar to the training data. When the depth distribution differs, they
still produce consistent relative depth, i.e., depth accurate up to an unknown
scale. Our method supports both metric and relative depth. It employs a
scale-aware Procrustes alignment to estimate the camera pose from the
correspondences and optionally recover the scale when using relative depth.
Experimental results demonstrate that, with only weak supervision on camera
pose, our method learns accurate local feature correspondences and achieves
superior localization performance under challenging conditions, such as
cross-area generalization and unknown orientation. Moreover, our method is
compatible with various relative depth models without requiring per-model
finetuning. This flexibility, combined with strong localization performance,
makes it well-suited for real-world deployment.

</details>


### [11] [Early Detection of Visual Impairments at Home Using a Smartphone Red-Eye Reflex Test](https://arxiv.org/abs/2509.09808)
*Judith Massmann,Alexander Lichtenstein,Francisco M. López*

Main category: cs.CV

TL;DR: 利用智能手机和AI技术开发移动应用KidsVisionCheck，通过红眼反射图像进行儿童视力筛查，准确率达90%


<details>
  <summary>Details</summary>
Motivation: 传统Bruckner测试需要眼科医生在临床环境中进行，希望利用智能手机和AI技术使其更加普及和便捷

Method: 基于深度神经网络模型，使用眼科医生收集和标注的儿童瞳孔图像进行训练

Result: 在未见测试数据上达到90%的准确率，无需专业设备即可提供高可靠性性能

Conclusion: 这项工作是实现全球范围内可及的儿童视力筛查和早期干预的重要第一步

Abstract: Numerous visual impairments can be detected in red-eye reflex images from
young children. The so-called Bruckner test is traditionally performed by
ophthalmologists in clinical settings. Thanks to the recent technological
advances in smartphones and artificial intelligence, it is now possible to
recreate the Bruckner test using a mobile device. In this paper, we present a
first study conducted during the development of KidsVisionCheck, a free
application that can perform vision screening with a mobile device using
red-eye reflex images. The underlying model relies on deep neural networks
trained on children's pupil images collected and labeled by an ophthalmologist.
With an accuracy of 90% on unseen test data, our model provides highly reliable
performance without the necessity of specialist equipment. Furthermore, we can
identify the optimal conditions for data collection, which can in turn be used
to provide immediate feedback to the users. In summary, this work marks a first
step toward accessible pediatric vision screenings and early intervention for
vision abnormalities worldwide.

</details>


### [12] [DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception](https://arxiv.org/abs/2509.09828)
*Tim Broedermannn,Christos Sakaridis,Luigi Piccinelli,Wim Abbeloos,Luc Van Gool*

Main category: cs.CV

TL;DR: 提出了一种基于深度引导的多模态融合方法DGFusion，通过整合深度信息实现条件感知融合，在自动驾驶语义感知任务中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有的传感器融合方法在空间维度上对传感器数据采用统一处理方式，在挑战性条件下性能受限。需要一种能够根据空间变化可靠性动态调整传感器融合的方法

Method: 提出DGFusion网络，将多模态分割作为多任务问题处理，利用激光雷达测量作为输入和深度学习真值。通过辅助深度头学习深度感知特征，编码为空间变化的局部深度token，结合全局条件token动态调整传感器融合

Result: 在MUSES和DELIVER数据集上实现了最先进的全景分割和语义分割性能

Conclusion: 深度引导的多模态融合方法能够有效适应传感器在不同空间位置的可信度变化，特别是在恶劣条件下处理稀疏和噪声激光雷达数据时表现出色

Abstract: Robust semantic perception for autonomous vehicles relies on effectively
combining multiple sensors with complementary strengths and weaknesses.
State-of-the-art sensor fusion approaches to semantic perception often treat
sensor data uniformly across the spatial extent of the input, which hinders
performance when faced with challenging conditions. By contrast, we propose a
novel depth-guided multimodal fusion method that upgrades condition-aware
fusion by integrating depth information. Our network, DGFusion, poses
multimodal segmentation as a multi-task problem, utilizing the lidar
measurements, which are typically available in outdoor sensor suites, both as
one of the model's inputs and as ground truth for learning depth. Our
corresponding auxiliary depth head helps to learn depth-aware features, which
are encoded into spatially varying local depth tokens that condition our
attentive cross-modal fusion. Together with a global condition token, these
local depth tokens dynamically adapt sensor fusion to the spatially varying
reliability of each sensor across the scene, which largely depends on depth. In
addition, we propose a robust loss for our depth, which is essential for
learning from lidar inputs that are typically sparse and noisy in adverse
conditions. Our method achieves state-of-the-art panoptic and semantic
segmentation performance on the challenging MUSES and DELIVER datasets. Code
and models will be available at https://github.com/timbroed/DGFusion

</details>


### [13] [Patch-based Automatic Rosacea Detection Using the ResNet Deep Learning Framework](https://arxiv.org/abs/2509.09841)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出基于图像块的ResNet-18深度学习方法用于酒渣鼻自动检测，通过局部图像块分析实现高精度诊断并保护患者隐私


<details>
  <summary>Details</summary>
Motivation: 酒渣鼻作为慢性炎症性皮肤病需要早期精确检测以提高治疗效果，传统全图像方法存在隐私泄露和诊断效率问题

Method: 使用ResNet-18深度学习框架，从面部图像提取不同尺寸、形状和位置的各种图像块，评估局部视觉信息对模型性能的影响

Result: 实验表明基于图像块的方法在准确性和敏感性上达到或超过全图像方法，同时增强模型鲁棒性和可解释性

Conclusion: 提出的图像块策略能够引导深度学习模型关注临床相关区域，保护患者隐私，为自动化皮肤病诊断提供实用见解

Abstract: Rosacea, which is a chronic inflammatory skin condition that manifests with
facial redness, papules, and visible blood vessels, often requirs precise and
early detection for significantly improving treatment effectiveness. This paper
presents new patch-based automatic rosacea detection strategies using the
ResNet-18 deep learning framework. The contributions of the proposed strategies
come from the following aspects. First, various image pateches are extracted
from the facial images of people in different sizes, shapes, and locations.
Second, a number of investigation studies are carried out to evaluate how the
localized visual information influences the deep learing model performance.
Third, thorough experiments are implemented to reveal that several patch-based
automatic rosacea detection strategies achieve competitive or superior accuracy
and sensitivity than the full-image based methods. And finally, the proposed
patch-based strategies, which use only localized patches, inherently preserve
patient privacy by excluding any identifiable facial features from the data.
The experimental results indicate that the proposed patch-based strategies
guide the deep learning model to focus on clinically relevant regions, enhance
robustness and interpretability, and protect patient privacy. As a result, the
proposed strategies offer practical insights for improving automated
dermatological diagnostics.

</details>


### [14] [Privacy-Preserving Automated Rosacea Detection Based on Medically Inspired Region of Interest Selection](https://arxiv.org/abs/2509.09844)
*Chengyu Yang,Rishik Reddy Yesgari,Chengjun Liu*

Main category: cs.CV

TL;DR: 提出了一种基于临床先验知识和合成数据的隐私保护型玫瑰痤疮自动检测方法，通过构建红色通道掩码聚焦诊断相关区域，在真实测试数据上表现优于全脸基线模型


<details>
  <summary>Details</summary>
Motivation: 玫瑰痤疮是一种常见但诊断不足的炎症性皮肤病，传统自动检测面临症状弥散、标注数据稀缺以及面部图像隐私问题等挑战

Method: 首先构建基于红色通道强度的固定掩码来聚焦脸颊、鼻子和前额等诊断相关区域，然后使用ResNet-18深度学习模型在掩码后的合成图像上进行训练

Result: 在真实世界测试数据上，该方法在准确率、召回率和F1分数方面显著优于全脸基线模型

Conclusion: 合成数据和临床先验知识可以共同实现准确且符合伦理的皮肤病AI系统，特别适用于远程医疗和大规模筛查等隐私敏感应用

Abstract: Rosacea is a common but underdiagnosed inflammatory skin condition that
primarily affects the central face and presents with subtle redness, pustules,
and visible blood vessels. Automated detection remains challenging due to the
diffuse nature of symptoms, the scarcity of labeled datasets, and privacy
concerns associated with using identifiable facial images. A novel
privacy-preserving automated rosacea detection method inspired by clinical
priors and trained entirely on synthetic data is presented in this paper.
Specifically, the proposed method, which leverages the observation that rosacea
manifests predominantly through central facial erythema, first constructs a
fixed redness-informed mask by selecting regions with consistently high red
channel intensity across facial images. The mask thus is able to focus on
diagnostically relevant areas such as the cheeks, nose, and forehead and
exclude identity-revealing features. Second, the ResNet-18 deep learning
method, which is trained on the masked synthetic images, achieves superior
performance over the full-face baselines with notable gains in terms of
accuracy, recall and F1 score when evaluated using the real-world test data.
The experimental results demonstrate that the synthetic data and clinical
priors can jointly enable accurate and ethical dermatological AI systems,
especially for privacy sensitive applications in telemedicine and large-scale
screening.

</details>


### [15] [Investigating the Impact of Various Loss Functions and Learnable Wiener Filter for Laparoscopic Image Desmoking](https://arxiv.org/abs/2509.09849)
*Chengyu Yang,Chengjun Liu*

Main category: cs.CV

TL;DR: 对ULW腹腔镜图像去烟雾框架进行全面的消融研究，评估U-Net主干、复合损失函数（MSE、SSIM、感知损失）和可学习维纳滤波器模块各组成部分的有效性和必要性


<details>
  <summary>Details</summary>
Motivation: 需要严格评估ULW框架中各个组件的有效性和必要性，通过系统性的消融研究来理解每个部分对整个框架性能的具体贡献

Method: 采用系统消融研究方法：(1)移除可学习维纳滤波器模块；(2)选择性使用复合损失函数中的各个损失项（MSE、SSIM、感知损失）。在公开的配对腹腔镜图像数据集上进行基准测试

Result: 使用定量指标（SSIM、PSNR、MSE、CIEDE-2000）和定性视觉比较对所有变体进行性能评估

Conclusion: 通过全面的消融分析验证了ULW框架中各组件的重要性，为腹腔镜图像去烟雾技术的优化提供了重要见解

Abstract: To rigorously assess the effectiveness and necessity of individual components
within the recently proposed ULW framework for laparoscopic image desmoking,
this paper presents a comprehensive ablation study. The ULW approach combines a
U-Net based backbone with a compound loss function that comprises mean squared
error (MSE), structural similarity index (SSIM) loss, and perceptual loss. The
framework also incorporates a differentiable, learnable Wiener filter module.
In this study, each component is systematically ablated to evaluate its
specific contribution to the overall performance of the whole framework. The
analysis includes: (1) removal of the learnable Wiener filter, (2) selective
use of individual loss terms from the composite loss function. All variants are
benchmarked on a publicly available paired laparoscopic images dataset using
quantitative metrics (SSIM, PSNR, MSE and CIEDE-2000) alongside qualitative
visual comparisons.

</details>


### [16] [WAVE-DETR Multi-Modal Visible and Acoustic Real-Life Drone Detector](https://arxiv.org/abs/2509.09859)
*Razvan Stefanescu,Ethan Oh,Ruben Vazquez,Chris Mesterharm,Constantin Serban,Ritu Chadha*

Main category: cs.CV

TL;DR: WAVE-DETR是一个结合可见光和声学信号的多模态无人机检测器，使用Deformable DETR和Wav2Vec2架构，在ARDrone数据集上通过门控融合机制将mAP提升了11.1%-15.3%。


<details>
  <summary>Details</summary>
Motivation: 为了解决在复杂环境条件下无人机检测的鲁棒性问题，结合视觉和声学信息来提高检测性能。

Method: 基于Deformable DETR和Wav2Vec2架构，开发了四种融合配置（门控机制、线性层、MLP和交叉注意力），将声学嵌入与多分辨率特征映射融合。

Result: 门控融合方法表现最佳，在ARDrone数据集上，小尺寸无人机的mAP提升了11.1%-15.3%，中大型无人机也有3.27%-5.84%的整体提升。

Conclusion: 多模态融合（特别是门控融合）能显著提升无人机检测性能，特别是在小尺寸目标检测方面效果显著。

Abstract: We introduce a multi-modal WAVE-DETR drone detector combining visible RGB and
acoustic signals for robust real-life UAV object detection. Our approach fuses
visual and acoustic features in a unified object detector model relying on the
Deformable DETR and Wav2Vec2 architectures, achieving strong performance under
challenging environmental conditions. Our work leverage the existing
Drone-vs-Bird dataset and the newly generated ARDrone dataset containing more
than 7,500 synchronized images and audio segments. We show how the acoustic
information is used to improve the performance of the Deformable DETR object
detector on the real ARDrone dataset. We developed, trained and tested four
different fusion configurations based on a gated mechanism, linear layer, MLP
and cross attention. The Wav2Vec2 acoustic embeddings are fused with the multi
resolution feature mappings of the Deformable DETR and enhance the object
detection performance over all drones dimensions. The best performer is the
gated fusion approach, which improves the mAP of the Deformable DETR object
detector on our in-distribution and out-of-distribution ARDrone datasets by
11.1% to 15.3% for small drones across all IoU thresholds between 0.5 and 0.9.
The mAP scores for medium and large drones are also enhanced, with overall
gains across all drone sizes ranging from 3.27% to 5.84%.

</details>


### [17] [Surrogate Supervision for Robust and Generalizable Deformable Image Registration](https://arxiv.org/abs/2509.09869)
*Yihao Liu,Junyu Chen,Lianrui Zuo,Shuwen Wei,Brian D. Boyd,Carmen Andreescu,Olusola Ajilore,Warren D. Taylor,Aaron Carass,Bennett A. Landman*

Main category: cs.CV

TL;DR: 提出了一种名为"替代监督"的训练范式，通过将空间变换应用于替代图像来解耦输入域和监督域，从而提升深度学习图像配准网络的鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 基于深度学习的可变形图像配准虽然精度高，但对输入图像特性变化（如伪影、视野不匹配、模态差异）敏感，需要开发能提高配准网络鲁棒性和泛化性的通用训练方法。

Method: 引入替代监督方法，通过将估计的空间变换应用于替代图像，使输入域与监督域解耦。这样可以在异质输入上进行训练，同时确保在相似性定义良好的域中进行监督计算。

Result: 在三个代表性应用中（抗伪影脑MR配准、掩模无关肺CT配准、多模态MR配准）均表现出对输入变化的强韧性，包括不均匀场、不一致视野和模态差异，同时在良好处理的数据上保持高性能。

Conclusion: 替代监督为训练鲁棒且可泛化的深度学习配准模型提供了原则性框架，且不增加复杂度，为医学图像配准在多样化生物医学成像场景中的更广泛应用提供了实用途径。

Abstract: Objective: Deep learning-based deformable image registration has achieved
strong accuracy, but remains sensitive to variations in input image
characteristics such as artifacts, field-of-view mismatch, or modality
difference. We aim to develop a general training paradigm that improves the
robustness and generalizability of registration networks. Methods: We introduce
surrogate supervision, which decouples the input domain from the supervision
domain by applying estimated spatial transformations to surrogate images. This
allows training on heterogeneous inputs while ensuring supervision is computed
in domains where similarity is well defined. We evaluate the framework through
three representative applications: artifact-robust brain MR registration,
mask-agnostic lung CT registration, and multi-modal MR registration. Results:
Across tasks, surrogate supervision demonstrated strong resilience to input
variations including inhomogeneity field, inconsistent field-of-view, and
modality differences, while maintaining high performance on well-curated data.
Conclusions: Surrogate supervision provides a principled framework for training
robust and generalizable deep learning-based registration models without
increasing complexity. Significance: Surrogate supervision offers a practical
pathway to more robust and generalizable medical image registration, enabling
broader applicability in diverse biomedical imaging scenarios.

</details>


### [18] [An Autoencoder and Vision Transformer-based Interpretability Analysis of the Differences in Automated Staging of Second and Third Molars](https://arxiv.org/abs/2509.09911)
*Barkin Buyukcakir,Jannick De Tobel,Patrick Thevissen,Dirk Vandermeulen,Peter Claes*

Main category: cs.CV

TL;DR: 提出结合卷积自编码器和Vision Transformer的框架，在牙齿年龄估计中同时提升性能和模型透明度，解决了第三磨牙性能差距的数据中心问题。


<details>
  <summary>Details</summary>
Motivation: 深度学习在法医年龄估计等高风险应用中因'黑盒'特性而受限，需要既提高准确性又提供可解释性的方法。

Method: 使用卷积自编码器(AE)结合Vision Transformer(ViT)，通过分析潜在空间度量和图像重建来提供多方面的诊断洞察。

Result: 分类准确率显著提升：牙齿37从0.712提高到0.815，牙齿38从0.462提高到0.543，并识别出牙齿38数据集内类内形态变异性的数据问题。

Conclusion: 该框架证明了单一可解释性模式的不足，提供了同时增强准确性和模型不确定性的方法论，成为支持法医年龄估计专家决策的更可靠工具。

Abstract: The practical adoption of deep learning in high-stakes forensic applications,
such as dental age estimation, is often limited by the 'black box' nature of
the models. This study introduces a framework designed to enhance both
performance and transparency in this context. We use a notable performance
disparity in the automated staging of mandibular second (tooth 37) and third
(tooth 38) molars as a case study. The proposed framework, which combines a
convolutional autoencoder (AE) with a Vision Transformer (ViT), improves
classification accuracy for both teeth over a baseline ViT, increasing from
0.712 to 0.815 for tooth 37 and from 0.462 to 0.543 for tooth 38. Beyond
improving performance, the framework provides multi-faceted diagnostic
insights. Analysis of the AE's latent space metrics and image reconstructions
indicates that the remaining performance gap is data-centric, suggesting high
intra-class morphological variability in the tooth 38 dataset is a primary
limiting factor. This work highlights the insufficiency of relying on a single
mode of interpretability, such as attention maps, which can appear anatomically
plausible yet fail to identify underlying data issues. By offering a
methodology that both enhances accuracy and provides evidence for why a model
may be uncertain, this framework serves as a more robust tool to support expert
decision-making in forensic age estimation.

</details>


### [19] [SCoDA: Self-supervised Continual Domain Adaptation](https://arxiv.org/abs/2509.09935)
*Chirayu Agrawal,Snehasis Mukherjee*

Main category: cs.CV

TL;DR: SCoDA提出了一种新的无源域自适应方法，通过自监督预训练和几何流形对齐，在不需要源域数据的情况下显著提升了目标域适应性能。


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法依赖全监督预训练的源模型，并使用余弦相似度对齐特征，这会导致源模型潜在流形的几何信息丢失。

Method: 使用自监督预训练的教师模型初始化，结合实例级特征匹配和空间相似性损失进行几何流形对齐，通过EMA更新教师参数防止灾难性遗忘。

Result: 在基准数据集上的大量实验表明，SCoDA显著优于最先进的SFDA方法。

Conclusion: 自监督预训练和几何流形对齐的结合为无源域自适应提供了更有效的解决方案，避免了传统方法的信息丢失问题。

Abstract: Source-Free Domain Adaptation (SFDA) addresses the challenge of adapting a
model to a target domain without access to the data of the source domain.
Prevailing methods typically start with a source model pre-trained with full
supervision and distill the knowledge by aligning instance-level features.
However, these approaches, relying on cosine similarity over L2-normalized
feature vectors, inadvertently discard crucial geometric information about the
latent manifold of the source model. We introduce Self-supervised Continual
Domain Adaptation (SCoDA) to address these limitations. We make two key
departures from standard practice: first, we avoid the reliance on supervised
pre-training by initializing the proposed framework with a teacher model
pre-trained entirely via self-supervision (SSL). Second, we adapt the principle
of geometric manifold alignment to the SFDA setting. The student is trained
with a composite objective combining instance-level feature matching with a
Space Similarity Loss. To combat catastrophic forgetting, the teacher's
parameters are updated via an Exponential Moving Average (EMA) of the student's
parameters. Extensive experiments on benchmark datasets demonstrate that SCoDA
significantly outperforms state-of-the-art SFDA methods.

</details>


### [20] [Segment Anything for Cell Tracking](https://arxiv.org/abs/2509.09943)
*Zhu Chen,Mert Edgü,Er Jin,Johannes Stegmaier*

Main category: cs.CV

TL;DR: 提出基于SAM2的零样本细胞追踪框架，无需人工标注训练数据，在2D和3D显微视频中实现竞争性精度


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习方法依赖昂贵人工标注、泛化性差的问题，应对细胞分裂、低信噪比、边界模糊等挑战

Method: 将Segment Anything 2 (SAM2)基础模型集成到追踪流程中，作为完全无监督方法，不依赖特定训练数据集

Result: 在2D和大规模3D延时显微视频中达到竞争性精度，无需数据集特定适配

Conclusion: 该方法消除了对训练数据的依赖，具有良好的跨数据集泛化能力，为细胞追踪提供了新的解决方案

Abstract: Tracking cells and detecting mitotic events in time-lapse microscopy image
sequences is a crucial task in biomedical research. However, it remains highly
challenging due to dividing objects, low signal-tonoise ratios, indistinct
boundaries, dense clusters, and the visually similar appearance of individual
cells. Existing deep learning-based methods rely on manually labeled datasets
for training, which is both costly and time-consuming. Moreover, their
generalizability to unseen datasets remains limited due to the vast diversity
of microscopy data. To overcome these limitations, we propose a zero-shot cell
tracking framework by integrating Segment Anything 2 (SAM2), a large foundation
model designed for general image and video segmentation, into the tracking
pipeline. As a fully-unsupervised approach, our method does not depend on or
inherit biases from any specific training dataset, allowing it to generalize
across diverse microscopy datasets without finetuning. Our approach achieves
competitive accuracy in both 2D and large-scale 3D time-lapse microscopy videos
while eliminating the need for dataset-specific adaptation.

</details>


### [21] [Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation](https://arxiv.org/abs/2509.09946)
*Vu-Minh Le,Thao-Anh Tran,Duc Huy Do,Xuan Canh Do,Huong Ninh,Hai Tran*

Main category: cs.CV

TL;DR: 提出了一种将2D多摄像头跟踪系统扩展到3D空间的方法，利用深度信息重建目标点云并通过聚类和偏航角优化来恢复3D边界框，在AI City Challenge中获得第三名


<details>
  <summary>Details</summary>
Motivation: 现有的多目标多摄像头跟踪系统主要基于2D空间，而3D跟踪需要从零开始重建所有组件，这对现有系统来说不可行。需要一种方法能够利用现有2D系统并扩展到3D空间

Method: 利用深度信息在点云空间中重建目标，通过聚类和偏航角优化恢复3D边界框，引入增强的在线数据关联机制，利用目标的局部ID一致性跨帧分配全局ID

Result: 在2025 AI City Challenge的3D MTMC数据集上进行评估，在排行榜上获得第三名的成绩

Conclusion: 该方法成功实现了将现有2D多摄像头跟踪系统扩展到3D空间的能力，证明了在不完全重建系统的情况下实现3D多目标跟踪的可行性

Abstract: Multi-Target Multi-Camera Tracking (MTMC) is an essential computer vision
task for automating large-scale surveillance. With camera calibration and depth
information, the targets in the scene can be projected into 3D space, offering
unparalleled levels of automatic perception of a 3D environment. However,
tracking in the 3D space requires replacing all 2D tracking components from the
ground up, which may be infeasible for existing MTMC systems. In this paper, we
present an approach for extending any online 2D multi-camera tracking system
into 3D space by utilizing depth information to reconstruct a target in
point-cloud space, and recovering its 3D box through clustering and yaw
refinement following tracking. We also introduced an enhanced online data
association mechanism that leverages the target's local ID consistency to
assign global IDs across frames. The proposed framework is evaluated on the
2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on the
leaderboard.

</details>


### [22] [Zero-Shot Referring Expression Comprehension via Visual-Language True/False Verification](https://arxiv.org/abs/2509.09958)
*Jeffrey Liu,Rongbin Hu*

Main category: cs.CV

TL;DR: 提出一种零样本的指代表达理解方法，通过将任务重新定义为视觉语言验证问题，无需特定训练即可达到竞争性性能


<details>
  <summary>Details</summary>
Motivation: 传统REC方法需要任务特定的训练，本文旨在探索零样本工作流是否能够达到类似或更好的性能

Method: 将REC重新定义为框级别的视觉语言验证：使用通用检测器生成候选框，然后用通用VLM对每个区域进行True/False查询验证

Result: 在RefCOCO、RefCOCO+和RefCOCOg数据集上超越了零样本和训练过的GroundingDINO基线，验证方法显著优于基于选择的方法

Conclusion: 工作流设计而非任务特定的预训练是驱动零样本REC性能的关键因素

Abstract: Referring Expression Comprehension (REC) is usually addressed with
task-trained grounding models. We show that a zero-shot workflow, without any
REC-specific training, can achieve competitive or superior performance. Our
approach reformulates REC as box-wise visual-language verification: given
proposals from a COCO-clean generic detector (YOLO-World), a general-purpose
VLM independently answers True/False queries for each region. This simple
procedure reduces cross-box interference, supports abstention and multiple
matches, and requires no fine-tuning. On RefCOCO, RefCOCO+, and RefCOCOg, our
method not only surpasses a zero-shot GroundingDINO baseline but also exceeds
reported results for GroundingDINO trained on REC and GroundingDINO+CRG.
Controlled studies with identical proposals confirm that verification
significantly outperforms selection-based prompting, and results hold with open
VLMs. Overall, we show that workflow design, rather than task-specific
pretraining, drives strong zero-shot REC performance.

</details>


### [23] [Augment to Segment: Tackling Pixel-Level Imbalance in Wheat Disease and Pest Segmentation](https://arxiv.org/abs/2509.09961)
*Tianqi Wei,Xin Yu,Zhi Chen,Scott Chapman,Zi Huang*

Main category: cs.CV

TL;DR: 提出随机投影复制粘贴(RPCP)增强技术来解决小麦叶部病虫害分割中的极端像素不平衡问题，通过提取稀有病虫害区域并应用几何变换和随机投影滤波来改善分割性能。


<details>
  <summary>Details</summary>
Motivation: 小麦叶部病虫害分割中，虫害通常只占标注像素的极小部分，这种极端像素级不平衡会导致模型对常见类别过拟合，对稀有类别学习不足，从而影响整体分割性能。

Method: 从标注训练图像中提取稀有虫害斑块，应用随机几何变换模拟变化，将变换后的斑块粘贴到适当区域避免与现有病变重叠，并对粘贴区域应用随机投影滤波器以细化局部特征并确保与背景自然融合。

Result: 实验表明该方法显著提高了虫害类别的分割性能，同时保持甚至略微提高了其他类别的准确率。

Conclusion: 针对性增强技术能有效缓解极端像素不平衡问题，为农业分割问题提供简单而有效的解决方案。

Abstract: Accurate segmentation of foliar diseases and insect damage in wheat is
crucial for effective crop management and disease control. However, the insect
damage typically occupies only a tiny fraction of annotated pixels. This
extreme pixel-level imbalance poses a significant challenge to the segmentation
performance, which can result in overfitting to common classes and insufficient
learning of rare classes, thereby impairing overall performance. In this paper,
we propose a Random Projected Copy-and-Paste (RPCP) augmentation technique to
address the pixel imbalance problem. Specifically, we extract rare
insect-damage patches from annotated training images and apply random geometric
transformations to simulate variations. The transformed patches are then pasted
in appropriate regions while avoiding overlaps with lesions or existing damaged
regions. In addition, we apply a random projection filter to the pasted
regions, refining local features and ensuring a natural blend with the new
background. Experiments show that our method substantially improves
segmentation performance on the insect damage class, while maintaining or even
slightly enhancing accuracy on other categories. Our results highlight the
effectiveness of targeted augmentation in mitigating extreme pixel imbalance,
offering a straightforward yet effective solution for agricultural segmentation
problems.

</details>


### [24] [An HMM-based framework for identity-aware long-term multi-object tracking from sparse and uncertain identification: use case on long-term tracking in livestock](https://arxiv.org/abs/2509.09962)
*Anne Marthe Sophie Ngo Bibinbe,Chiron Bang,Patrick Gagnon,Jamie Ahloy-Dallaire,Eric R. Paquet*

Main category: cs.CV

TL;DR: 提出基于隐马尔可夫模型的新框架，利用偶发身份信息改进长期多目标跟踪性能，在生猪追踪数据集和标准基准测试中验证有效性


<details>
  <summary>Details</summary>
Motivation: 现有多目标跟踪方法在长时间跟踪中因身份切换导致性能下降，而实际应用（如畜牧业）中可获得偶发的身份识别信息

Method: 使用隐马尔可夫模型框架结合不确定身份信息和跟踪数据，整合偶发身份识别（如喂食站识别）来改进跟踪

Result: 在10分钟生猪跟踪数据集上，即使只有21次喂食站识别，也能提高ByteTrack的F1分数；在MOT17和MOT20基准测试中也验证了性能提升

Conclusion: 该方法能有效利用偶发身份信息提升长期多目标跟踪性能，对识别不确定性具有鲁棒性，性能随身份提供频率增加而提高

Abstract: The need for long-term multi-object tracking (MOT) is growing due to the
demand for analyzing individual behaviors in videos that span several minutes.
Unfortunately, due to identity switches between objects, the tracking
performance of existing MOT approaches decreases over time, making them
difficult to apply for long-term tracking. However, in many real-world
applications, such as in the livestock sector, it is possible to obtain
sporadic identifications for some of the animals from sources like feeders. To
address the challenges of long-term MOT, we propose a new framework that
combines both uncertain identities and tracking using a Hidden Markov Model
(HMM) formulation. In addition to providing real-world identities to animals,
our HMM framework improves the F1 score of ByteTrack, a leading MOT approach
even with re-identification, on a 10 minute pig tracking dataset with 21
identifications at the pen's feeding station. We also show that our approach is
robust to the uncertainty of identifications, with performance increasing as
identities are provided more frequently. The improved performance of our HMM
framework was also validated on the MOT17 and MOT20 benchmark datasets using
both ByteTrack and FairMOT. The code for this new HMM framework and the new
10-minute pig tracking video dataset are available at:
https://github.com/ngobibibnbe/uncertain-identity-aware-tracking

</details>


### [25] [Event Camera Guided Visual Media Restoration & 3D Reconstruction: A Survey](https://arxiv.org/abs/2509.09971)
*Aupendu Kar,Vishnu Raj,Guan-Ming Su*

Main category: cs.CV

TL;DR: 事件相机与传统帧相机的融合技术综述，重点关注深度学习在视频恢复和3D重建中的应用，包括时空增强方法和公开数据集资源


<details>
  <summary>Details</summary>
Motivation: 事件相机具有低延迟、低功耗和高捕获率的优势，但其与传统帧相机的融合技术发展迅速，需要系统性的综述来总结最新进展并指导未来研究方向

Method: 系统回顾深度学习在图像/视频增强和恢复方面的主要贡献，从时间维度（帧插值、运动去模糊）和空间维度（超分辨率、低光/HDR增强、伪影减少）两个维度进行分析，并探讨3D重建领域的进展

Result: 总结了事件相机与传统帧相机融合在各种视频恢复任务中的显著优势，整理了公开可用的数据集资源，为可重复研究和基准测试提供了基础

Conclusion: 通过整合最新进展和见解，本综述旨在激发进一步研究，特别是在深度学习与事件相机系统结合方面，推动高级视觉媒体恢复和增强技术的发展

Abstract: Event camera sensors are bio-inspired sensors which asynchronously capture
per-pixel brightness changes and output a stream of events encoding the
polarity, location and time of these changes. These systems are witnessing
rapid advancements as an emerging field, driven by their low latency, reduced
power consumption, and ultra-high capture rates. This survey explores the
evolution of fusing event-stream captured with traditional frame-based capture,
highlighting how this synergy significantly benefits various video restoration
and 3D reconstruction tasks. The paper systematically reviews major deep
learning contributions to image/video enhancement and restoration, focusing on
two dimensions: temporal enhancement (such as frame interpolation and motion
deblurring) and spatial enhancement (including super-resolution, low-light and
HDR enhancement, and artifact reduction). This paper also explores how the 3D
reconstruction domain evolves with the advancement of event driven fusion.
Diverse topics are covered, with in-depth discussions on recent works for
improving visual quality under challenging conditions. Additionally, the survey
compiles a comprehensive list of openly available datasets, enabling
reproducible research and benchmarking. By consolidating recent progress and
insights, this survey aims to inspire further research into leveraging event
camera systems, especially in combination with deep learning, for advanced
visual media restoration and enhancement.

</details>


### [26] [ISTASTrack: Bridging ANN and SNN via ISTA Adapter for RGB-Event Tracking](https://arxiv.org/abs/2509.09977)
*Siying Liu,Zikai Wang,Hanle Zheng,Yifan Hu,Xilin Wang,Qingkai Yang,Jibin Wu,Hao Guo,Lei Deng*

Main category: cs.CV

TL;DR: ISTASTrack是首个基于Transformer的ANN-SNN混合RGB-Event跟踪器，通过ISTA适配器实现双向特征交互，在多个基准测试中达到最先进性能


<details>
  <summary>Details</summary>
Motivation: 现有ANN网络难以充分利用事件流的稀疏和异步特性，ANN-SNN混合架构虽然前景广阔，但跨异构范式的特征融合仍然是一个挑战

Method: 采用双分支模型：视觉Transformer提取RGB空间上下文，脉冲Transformer捕捉事件流时空动态；设计基于稀疏表示理论的ISTA适配器进行双向特征交互，并加入时序下采样注意力模块对齐特征

Result: 在FE240hz、VisEvent、COESOT和FELT等RGB-Event跟踪基准测试中实现了最先进的性能，同时保持高能效

Conclusion: ISTASTrack证明了ANN-SNN混合设计在鲁棒视觉跟踪中的有效性和实用性，为跨模态特征融合提供了新思路

Abstract: RGB-Event tracking has become a promising trend in visual object tracking to
leverage the complementary strengths of both RGB images and dynamic spike
events for improved performance. However, existing artificial neural networks
(ANNs) struggle to fully exploit the sparse and asynchronous nature of event
streams. Recent efforts toward hybrid architectures combining ANNs and spiking
neural networks (SNNs) have emerged as a promising solution in RGB-Event
perception, yet effectively fusing features across heterogeneous paradigms
remains a challenge. In this work, we propose ISTASTrack, the first
transformer-based \textbf{A}NN-\textbf{S}NN hybrid \textbf{Track}er equipped
with \textbf{ISTA} adapters for RGB-Event tracking. The two-branch model
employs a vision transformer to extract spatial context from RGB inputs and a
spiking transformer to capture spatio-temporal dynamics from event streams. To
bridge the modality and paradigm gap between ANN and SNN features, we
systematically design a model-based ISTA adapter for bidirectional feature
interaction between the two branches, derived from sparse representation theory
by unfolding the iterative shrinkage thresholding algorithm. Additionally, we
incorporate a temporal downsampling attention module within the adapter to
align multi-step SNN features with single-step ANN features in the latent
space, improving temporal fusion. Experimental results on RGB-Event tracking
benchmarks, such as FE240hz, VisEvent, COESOT, and FELT, have demonstrated that
ISTASTrack achieves state-of-the-art performance while maintaining high energy
efficiency, highlighting the effectiveness and practicality of hybrid ANN-SNN
designs for robust visual tracking. The code is publicly available at
https://github.com/lsying009/ISTASTrack.git.

</details>


### [27] [FLARE-SSM: Deep State Space Models with Influence-Balanced Loss for 72-Hour Solar Flare Prediction](https://arxiv.org/abs/2509.09988)
*Yusuke Takagi,Shunya Nagashima,Komei Sugiura*

Main category: cs.CV

TL;DR: 提出基于多深度状态空间模型的太阳耀斑预测方法，引入FLARE损失函数解决类别不平衡问题，在72小时耀斑等级预测任务上超越基线方法


<details>
  <summary>Details</summary>
Motivation: 当前太阳耀斑预测性能不足，现有方法难以有效处理耀斑类别间的严重不平衡问题，需要更准确可靠的预测方法来减轻对关键基础设施的潜在影响

Method: 使用多深度状态空间模型构建预测框架，提出频率和局部边界感知可靠性损失（FLARE损失）来改善类别不平衡下的预测性能和可靠性

Result: 在覆盖11年太阳活动周期的多波长太阳图像数据集上实验，在Gandin-Murphy-Gerrity分数和真实技能统计量两个标准指标上均优于基线方法

Conclusion: 该方法有效解决了太阳耀斑预测中的类别不平衡问题，提高了预测性能和可靠性，为太阳耀斑预警提供了更准确的技术支持

Abstract: Accurate and reliable solar flare predictions are essential to mitigate
potential impacts on critical infrastructure. However, the current performance
of solar flare forecasting is insufficient. In this study, we address the task
of predicting the class of the largest solar flare expected to occur within the
next 72 hours. Existing methods often fail to adequately address the severe
class imbalance across flare classes. To address this issue, we propose a solar
flare prediction model based on multiple deep state space models. In addition,
we introduce the frequency & local-boundary-aware reliability loss (FLARE loss)
to improve predictive performance and reliability under class imbalance.
Experiments were conducted on a multi-wavelength solar image dataset covering a
full 11-year solar activity cycle. As a result, our method outperformed
baseline approaches in terms of both the Gandin-Murphy-Gerrity score and the
true skill statistic, which are standard metrics in terms of the performance
and reliability.

</details>


### [28] [TUNI: Real-time RGB-T Semantic Segmentation with Unified Multi-Modal Feature Extraction and Cross-Modal Feature Fusion](https://arxiv.org/abs/2509.10005)
*Xiaodong Guo,Tong Liu,Yike Li,Zi'ang Lin,Zhihong Deng*

Main category: cs.CV

TL;DR: TUNI是一个统一的RGB-热成像语义分割模型，通过预训练编码器同时进行多模态特征提取和跨模态融合，实现了更紧凑的架构和实时推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有RGB-热成像分割模型中热特征提取有限、跨模态融合效果不佳以及编码器冗余导致的实时效率低下的问题。

Method: 提出TUNI模型，采用多层堆叠块组成的RGB-T编码器，通过大规模RGB和伪热数据预训练，实现特征提取和融合的统一处理。同时引入RGB-T局部模块，使用自适应余弦相似度选择性地强调跨模态的一致性和差异性局部特征。

Result: 在FMB、PST900和CART数据集上达到与最先进模型相当的性能，同时参数量和计算成本更低，在Jetson Orin NX上实现27 FPS的推理速度。

Conclusion: TUNI通过统一的编码器架构和局部特征融合机制，有效解决了RGB-热成像分割中的特征提取和融合问题，在保持高性能的同时实现了实时部署能力。

Abstract: RGB-thermal (RGB-T) semantic segmentation improves the environmental
perception of autonomous platforms in challenging conditions. Prevailing models
employ encoders pre-trained on RGB images to extract features from both RGB and
infrared inputs, and design additional modules to achieve cross-modal feature
fusion. This results in limited thermal feature extraction and suboptimal
cross-modal fusion, while the redundant encoders further compromises the
model's real-time efficiency. To address the above issues, we propose TUNI,
with an RGB-T encoder consisting of multiple stacked blocks that simultaneously
perform multi-modal feature extraction and cross-modal fusion. By leveraging
large-scale pre-training with RGB and pseudo-thermal data, the RGB-T encoder
learns to integrate feature extraction and fusion in a unified manner. By
slimming down the thermal branch, the encoder achieves a more compact
architecture. Moreover, we introduce an RGB-T local module to strengthen the
encoder's capacity for cross-modal local feature fusion. The RGB-T local module
employs adaptive cosine similarity to selectively emphasize salient consistent
and distinct local features across RGB-T modalities. Experimental results show
that TUNI achieves competitive performance with state-of-the-art models on FMB,
PST900 and CART, with fewer parameters and lower computational cost. Meanwhile,
it achieves an inference speed of 27 FPS on a Jetson Orin NX, demonstrating its
real-time capability in deployment. Codes are available at
https://github.com/xiaodonguo/TUNI.

</details>


### [29] [Few-Part-Shot Font Generation](https://arxiv.org/abs/2509.10006)
*Masaki Akiba,Shumpei Takezaki,Daichi Haraguchi,Seiichi Uchida*

Main category: cs.CV

TL;DR: 提出基于部分设计元素的少部件字体生成模型，只需输入部分字形而非完整字符即可生成整套字体


<details>
  <summary>Details</summary>
Motivation: 传统少样本字体生成需要完整字符形状，效率较低。本文旨在通过部分设计元素提高字体创建效率，并探索局部设计细节对整体字符结构的影响

Method: 设计新颖的少部件字体生成模型，以部分形状（partial shapes）作为输入，而非传统方法所需的完整字符形状

Result: 模型不仅提高了字体创建效率，还揭示了局部设计细节如何影响单个字符的整体结构

Conclusion: 该方法为字体设计提供了更高效的解决方案，同时深化了对字体设计中局部与整体关系的理解

Abstract: This paper proposes a novel model of few-part-shot font generation, which
designs an entire font based on a set of partial design elements, i.e., partial
shapes. Unlike conventional few-shot font generation, which requires entire
character shapes for a couple of character classes, our approach only needs
partial shapes as input. The proposed model not only improves the efficiency of
font creation but also provides insights into how partial design details
influence the entire structure of the individual characters.

</details>


### [30] [Efficient and Accurate Downfacing Visual Inertial Odometry](https://arxiv.org/abs/2509.10021)
*Jonas Kühne,Christian Vogt,Michele Magno,Luca Benini*

Main category: cs.CV

TL;DR: 提出了一种针对微型和纳米无人机优化的高效视觉惯性里程计(VIO)流水线，在超低功耗RISC-V SoC上实现实时VIO，相比基准流水线平均RMSE降低3.65倍


<details>
  <summary>Details</summary>
Motivation: 传统高精度VIO流水线需要强大计算系统，而微控制器上的轻量级实现精度不足，需要为微型无人机开发兼顾精度和效率的解决方案

Method: 采用先进特征检测和跟踪方法(SuperPoint、PX4FLOW、ORB)，进行优化和量化，结合刚体运动模型减少估计误差，特别优化平面运动场景

Result: 在GAP9低功耗SoC上，使用ORB特征跟踪器时平均RMSE降低达3.65倍；PX4FLOW在移动速度低于24像素/帧时达到与ORB相当的跟踪精度但运行时间更短

Conclusion: 该设计成功弥合了高精度VIO流水线与轻量级实现之间的差距，为微型无人机提供了既高效又准确的实时VIO解决方案

Abstract: Visual Inertial Odometry (VIO) is a widely used computer vision method that
determines an agent's movement through a camera and an IMU sensor. This paper
presents an efficient and accurate VIO pipeline optimized for applications on
micro- and nano-UAVs. The proposed design incorporates state-of-the-art feature
detection and tracking methods (SuperPoint, PX4FLOW, ORB), all optimized and
quantized for emerging RISC-V-based ultra-low-power parallel systems on chips
(SoCs). Furthermore, by employing a rigid body motion model, the pipeline
reduces estimation errors and achieves improved accuracy in planar motion
scenarios. The pipeline's suitability for real-time VIO is assessed on an
ultra-low-power SoC in terms of compute requirements and tracking accuracy
after quantization. The pipeline, including the three feature tracking methods,
was implemented on the SoC for real-world validation. This design bridges the
gap between high-accuracy VIO pipelines that are traditionally run on
computationally powerful systems and lightweight implementations suitable for
microcontrollers. The optimized pipeline on the GAP9 low-power SoC demonstrates
an average reduction in RMSE of up to a factor of 3.65x over the baseline
pipeline when using the ORB feature tracker. The analysis of the computational
complexity of the feature trackers further shows that PX4FLOW achieves on-par
tracking accuracy with ORB at a lower runtime for movement speeds below 24
pixels/frame.

</details>


### [31] [Hierarchical MLANet: Multi-level Attention for 3D Face Reconstruction From Single Images](https://arxiv.org/abs/2509.10024)
*Danling Cao*

Main category: cs.CV

TL;DR: 提出基于卷积神经网络的层次化多级注意力网络(MLANet)，从单张野外图像重建3D人脸模型，预测几何、纹理、姿态和光照参数


<details>
  <summary>Details</summary>
Motivation: 野外2D图像重建3D人脸模型应用广泛但面临缺乏标注数据和真实环境复杂性的挑战

Method: 使用预训练层次化骨干网络，在2D人脸特征提取不同阶段引入多级注意力机制，采用半监督训练策略结合3DMM参数和可微分渲染器实现端到端训练

Result: 在AFLW2000-3D和MICC Florence数据集上进行了比较和消融实验，在3D人脸重建和对齐任务上进行了定量和定性评估

Conclusion: 提出的方法在3D人脸重建任务中表现出有效性

Abstract: Recovering 3D face models from 2D in-the-wild images has gained considerable
attention in the computer vision community due to its wide range of potential
applications. However, the lack of ground-truth labeled datasets and the
complexity of real-world environments remain significant challenges. In this
chapter, we propose a convolutional neural network-based approach, the
Hierarchical Multi-Level Attention Network (MLANet), for reconstructing 3D face
models from single in-the-wild images. Our model predicts detailed facial
geometry, texture, pose, and illumination parameters from a single image.
Specifically, we employ a pre-trained hierarchical backbone network and
introduce multi-level attention mechanisms at different stages of 2D face image
feature extraction. A semi-supervised training strategy is employed,
incorporating 3D Morphable Model (3DMM) parameters from publicly available
datasets along with a differentiable renderer, enabling an end-to-end training
process. Extensive experiments, including both comparative and ablation
studies, were conducted on two benchmark datasets, AFLW2000-3D and MICC
Florence, focusing on 3D face reconstruction and 3D face alignment tasks. The
effectiveness of the proposed method was evaluated both quantitatively and
qualitatively.

</details>


### [32] [LaV-CoT: Language-Aware Visual CoT with Multi-Aspect Reward Optimization for Real-World Multilingual VQA](https://arxiv.org/abs/2509.10026)
*Jing Huang,Zhiya Tan,Shutao Gong,Fanwei Zeng,Jianshu Li*

Main category: cs.CV

TL;DR: LaV-CoT是一个语言感知的视觉思维链框架，通过多阶段推理管道和多方面奖励优化，显著提升多语言视觉问答性能，在多个基准测试中超越开源和商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖文本思维链，对多语言多模态推理支持有限，限制了在实际应用中的部署。需要一种能够同时处理视觉和语言信息的推理框架。

Method: 设计了包含文本摘要、语言识别、空间对象级字幕和逐步逻辑推理的多阶段推理管道。采用自动化数据标注方法生成多语言CoT注释，结合监督微调和语言感知组相对策略优化的两阶段训练范式。

Result: 在MMMB、Multilingual MMBench和MTVQA等数据集上，比相似规模的开源基线准确率提升约9.5%，甚至超越规模大2倍的模型约2.6%，表现优于GPT-4o-0513和Gemini-2.5-flash等先进专有模型。

Conclusion: LaV-CoT通过语言感知的视觉思维链框架有效解决了多语言多模态推理问题，在实际应用中表现出色，具有工业部署价值。

Abstract: As large vision language models (VLMs) advance, their capabilities in
multilingual visual question answering (mVQA) have significantly improved.
Chain-of-thought (CoT) reasoning has been proven to enhance interpretability
and complex reasoning. However, most existing approaches rely primarily on
textual CoT and provide limited support for multilingual multimodal reasoning,
constraining their deployment in real-world applications. To address this gap,
we introduce \textbf{LaV-CoT}, the first Language-aware Visual CoT framework
with Multi-Aspect Reward Optimization. LaV-CoT incorporates an interpretable
multi-stage reasoning pipeline consisting of Text Summary with Bounding Box
(BBox), Language Identification, Spatial Object-level Captioning, and
Step-by-step Logical Reasoning. Following this reasoning pipeline, we design an
automated data curation method that generates multilingual CoT annotations
through iterative generation, correction, and refinement, enabling scalable and
high-quality training data. To improve reasoning and generalization, LaV-CoT
adopts a two-stage training paradigm combining Supervised Fine-Tuning (SFT)
with Language-aware Group Relative Policy Optimization (GRPO), guided by
verifiable multi-aspect rewards including language consistency, structural
accuracy, and semantic alignment. Extensive evaluations on public datasets
including MMMB, Multilingual MMBench, and MTVQA show that LaV-CoT achieves up
to \(\sim\)9.5\% accuracy improvements over open-source baselines of similar
size and even surpasses models with 2$\times$ larger scales by \(\sim\)2.6\%.
Moreover, LaV-CoT outperforms advanced proprietary models such as GPT-4o-0513
and Gemini-2.5-flash. We further conducted an online A/B test to validate our
method on real-world data, highlighting its effectiveness for industrial
deployment. Our code is available at this link:
\href{https://github.com/HJNVR/LaV-CoT}

</details>


### [33] [Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings for Improved Diffusion Generation](https://arxiv.org/abs/2509.10058)
*Sung-Lin Tsai,Bo-Lun Huang,Yu Ting Shen,Cheng Yu Yeo,Chiang Tseng,Bo-Kai Ruan,Wen-Sheng Lien,Hong-Han Shuai*

Main category: cs.CV

TL;DR: 提出无需训练的框架，利用大语言模型解析模糊颜色描述并改进文本嵌入，提升文本到图像生成中的颜色准确性


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在处理复杂颜色术语时存在困难，常产生与人类意图不符的图像，现有方法无法系统性地解决颜色描述模糊问题

Method: 使用大语言模型解析颜色提示的模糊性，然后在CIELAB色彩空间中基于颜色术语的空间关系精炼文本嵌入，指导颜色混合操作

Result: 实验结果表明该方法提高了颜色对齐度且不影响图像质量，弥合了文本语义与视觉生成之间的差距

Conclusion: 该训练无关框架能够精确渲染模糊颜色描述下的色彩，无需额外训练或参考图像即可提升颜色保真度

Abstract: Accurate color alignment in text-to-image (T2I) generation is critical for
applications such as fashion, product visualization, and interior design, yet
current diffusion models struggle with nuanced and compound color terms (e.g.,
Tiffany blue, lime green, hot pink), often producing images that are misaligned
with human intent. Existing approaches rely on cross-attention manipulation,
reference images, or fine-tuning but fail to systematically resolve ambiguous
color descriptions. To precisely render colors under prompt ambiguity, we
propose a training-free framework that enhances color fidelity by leveraging a
large language model (LLM) to disambiguate color-related prompts and guiding
color blending operations directly in the text embedding space. Our method
first employs a large language model (LLM) to resolve ambiguous color terms in
the text prompt, and then refines the text embeddings based on the spatial
relationships of the resulting color terms in the CIELAB color space. Unlike
prior methods, our approach improves color accuracy without requiring
additional training or external reference images. Experimental results
demonstrate that our framework improves color alignment without compromising
image quality, bridging the gap between text semantics and visual generation.

</details>


### [34] [Multimodal Mathematical Reasoning Embedded in Aerial Vehicle Imagery: Benchmarking, Analysis, and Exploration](https://arxiv.org/abs/2509.10059)
*Yue Zhou,Litong Feng,Mengcheng Lan,Xue Yang,Qingyun Li,Yiping Ke,Xue Jiang,Wayne Zhang*

Main category: cs.CV

TL;DR: AVI-Math是首个评估无人机影像中多模态数学推理能力的基准数据集，包含3,773个高质量车辆相关问题，涵盖6个数学领域和20个主题。研究发现当前视觉语言模型在该领域的数学推理能力存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在无人机遥感任务中的数学推理能力尚未得到充分测试，而数学推理对于精确距离计算、面积估算和空间分析等任务至关重要。

Method: 构建了AVI-Math数据集，包含从不同高度和多个无人机角度采集的3,773个车辆相关问题，涵盖几何、逻辑和代数等6个数学领域。对14个主流视觉语言模型进行了全面评估，并探索了思维链提示和微调技术。

Result: 尽管现有视觉语言模型在之前的基准测试中表现良好，但在AVI-Math的推理任务上表现不佳，显示出当前模型在数学推理能力方面的显著局限性。思维链提示和微调技术显示出解决这些推理挑战的潜力。

Conclusion: 研究不仅揭示了视觉语言模型在数学推理方面的局限性，还为推进基于无人机的可信视觉语言模型在实际应用中的发展提供了有价值的见解。数据集和代码将公开发布。

Abstract: Mathematical reasoning is critical for tasks such as precise distance and
area computations, trajectory estimations, and spatial analysis in unmanned
aerial vehicle (UAV) based remote sensing, yet current vision-language models
(VLMs) have not been adequately tested in this domain. To address this gap, we
introduce AVI-Math, the first benchmark to rigorously evaluate multimodal
mathematical reasoning in aerial vehicle imagery, moving beyond simple counting
tasks to include domain-specific knowledge in areas such as geometry, logic,
and algebra. The dataset comprises 3,773 high-quality vehicle-related questions
captured from UAV views, covering 6 mathematical subjects and 20 topics. The
data, collected at varying altitudes and from multiple UAV angles, reflects
real-world UAV scenarios, ensuring the diversity and complexity of the
constructed mathematical problems. In this paper, we benchmark 14 prominent
VLMs through a comprehensive evaluation and demonstrate that, despite their
success on previous multimodal benchmarks, these models struggle with the
reasoning tasks in AVI-Math. Our detailed analysis highlights significant
limitations in the mathematical reasoning capabilities of current VLMs and
suggests avenues for future research. Furthermore, we explore the use of
Chain-of-Thought prompting and fine-tuning techniques, which show promise in
addressing the reasoning challenges in AVI-Math. Our findings not only expose
the limitations of VLMs in mathematical reasoning but also offer valuable
insights for advancing UAV-based trustworthy VLMs in real-world applications.
The code, and datasets will be released at
https://github.com/VisionXLab/avi-math

</details>


### [35] [BEVTraj: Map-Free End-to-End Trajectory Prediction in Bird's-Eye View with Deformable Attention and Sparse Goal Proposals](https://arxiv.org/abs/2509.10080)
*Minsang Kong,Myeongjun Kim,Sang Gu Kang,Sang Hun Lee*

Main category: cs.CV

TL;DR: BEVTraj是一种新颖的轨迹预测框架，直接在鸟瞰图空间利用实时传感器数据进行轨迹预测，无需依赖预建高清地图，通过可变形注意力和稀疏目标候选提议模块实现端到端预测。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预建高清地图或实时地图构建模块，但预建地图局限于特定区域且无法适应瞬时变化，而地图构建模块可能遗漏关键场景细节或引入错误影响预测性能。

Method: 提出BEVTraj框架，直接在BEV空间操作，使用可变形注意力从密集BEV特征中提取相关上下文，并引入稀疏目标候选提议模块实现完全端到端预测，无需后处理步骤。

Result: 大量实验表明，BEVTraj在性能上可与最先进的基于高清地图的模型相媲美，同时通过消除对预建地图的依赖提供了更大的灵活性。

Conclusion: BEVTraj框架成功克服了传统方法对预建地图的依赖，实现了灵活且高性能的轨迹预测，为自动驾驶系统提供了更实用的解决方案。

Abstract: In autonomous driving, trajectory prediction is essential for ensuring safe
and efficient navigation. To improve prediction accuracy, recent approaches
often rely on pre-built high-definition (HD) maps or real-time local map
construction modules to incorporate static environmental information. However,
pre-built HD maps are limited to specific regions and cannot adapt to transient
changes. In addition, local map construction modules, which recognize only
predefined elements, may fail to capture critical scene details or introduce
errors that degrade prediction performance. To overcome these limitations, we
propose Bird's-Eye View Trajectory Prediction (BEVTraj), a novel trajectory
prediction framework that operates directly in the bird's-eye view (BEV) space
utilizing real-time sensor data without relying on any pre-built maps. The
BEVTraj leverages deformable attention to efficiently extract relevant context
from dense BEV features. Furthermore, we introduce a Sparse Goal Candidate
Proposal (SGCP) module, which enables full end-to-end prediction without
requiring any post-processing steps. Extensive experiments demonstrate that the
BEVTraj achieves performance comparable to state-of-the-art HD map-based models
while offering greater flexibility by eliminating the dependency on pre-built
maps. The source code is available at https://github.com/Kongminsang/bevtraj.

</details>


### [36] [Leveraging Multi-View Weak Supervision for Occlusion-Aware Multi-Human Parsing](https://arxiv.org/abs/2509.10093)
*Laura Bragagnolo,Matteo Terreran,Leonardo Barcellona,Stefano Ghidoni*

Main category: cs.CV

TL;DR: 提出基于多视角信息的训练框架，通过弱监督实例分割和多视角一致性损失，提升遮挡场景下的多人解析性能


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体重叠遮挡场景下表现不佳，而多视角信息可以从不同角度分离重叠的人体

Method: 使用弱监督的人类实例分割和多视角一致性损失，结合半自动标注策略从多视角RGB+D数据和3D人体骨架生成标注

Result: 在遮挡场景下相比基线模型获得了4.20%的相对性能提升

Conclusion: 多视角信息训练框架能有效改善遮挡条件下的多人解析效果

Abstract: Multi-human parsing is the task of segmenting human body parts while
associating each part to the person it belongs to, combining instance-level and
part-level information for fine-grained human understanding. In this work, we
demonstrate that, while state-of-the-art approaches achieved notable results on
public datasets, they struggle considerably in segmenting people with
overlapping bodies. From the intuition that overlapping people may appear
separated from a different point of view, we propose a novel training framework
exploiting multi-view information to improve multi-human parsing models under
occlusions. Our method integrates such knowledge during the training process,
introducing a novel approach based on weak supervision on human instances and a
multi-view consistency loss. Given the lack of suitable datasets in the
literature, we propose a semi-automatic annotation strategy to generate human
instance segmentation masks from multi-view RGB+D data and 3D human skeletons.
The experiments demonstrate that the approach can achieve up to a 4.20\%
relative improvement on human parsing over the baseline model in occlusion
scenarios.

</details>


### [37] [VARCO-VISION-2.0 Technical Report](https://arxiv.org/abs/2509.10105)
*Young-rok Cha,Jeongho Ju,SunYoung Park,Jong-Hyeon Lee,Younghyun Yu,Youngjune Kim*

Main category: cs.CV

TL;DR: VARCO-VISION-2.0 是一个开源的韩英双语视觉语言模型，相比前代模型能力提升，支持多图像理解和布局感知OCR，在14B和1.7B两个规模版本上都有优异表现。


<details>
  <summary>Details</summary>
Motivation: 开发一个强大的双语视觉语言模型，支持韩语和英语，具备多图像理解能力，并能在保持核心语言能力的同时提升安全性。

Method: 采用四阶段课程训练和内存高效技术，通过偏好优化提升安全性，训练出支持多图像理解和布局感知OCR的双语模型。

Result: 模型在基准测试中表现出强大的空间定位能力，14B模型在OpenCompass VLM排行榜中位列同规模模型第8名，同时发布了适用于设备部署的1.7B轻量版本。

Conclusion: VARCO-VISION-2.0系列模型推动了双语视觉语言模型的发展，为实际应用提供了14B完整版和1.7B轻量版两个选择，可在Hugging Face获取。

Abstract: We introduce VARCO-VISION-2.0, an open-weight bilingual vision-language model
(VLM) for Korean and English with improved capabilities compared to the
previous model VARCO-VISION-14B. The model supports multi-image understanding
for complex inputs such as documents, charts, and tables, and delivers
layoutaware OCR by predicting both textual content and its spatial location.
Trained with a four-stage curriculum with memory-efficient techniques, the
model achieves enhanced multimodal alignment, while preserving core language
abilities and improving safety via preference optimization. Extensive benchmark
evaluations demonstrate strong spatial grounding and competitive results for
both languages, with the 14B model achieving 8th place on the OpenCompass VLM
leaderboard among models of comparable scale. Alongside the 14B-scale model, we
release a 1.7B version optimized for on-device deployment. We believe these
models advance the development of bilingual VLMs and their practical
applications. Two variants of VARCO-VISION-2.0 are available at Hugging Face: a
full-scale 14B model and a lightweight 1.7B model.

</details>


### [38] [A Lightweight Ensemble-Based Face Image Quality Assessment Method with Correlation-Aware Loss](https://arxiv.org/abs/2509.10114)
*MohammadAli Hamidi,Hadi Amirpour,Luigi Atzori,Christian Timmerer*

Main category: cs.CV

TL;DR: 提出了一种轻量级的人脸图像质量评估方法，通过集成MobileNetV3-Small和ShuffleNetV2网络，结合相关性感知损失函数，在保持高精度的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的人脸图像质量评估方法要么是通用图像质量评估技术无法捕捉人脸特定退化，要么是计算密集型模型限制了实际应用，需要一种既准确又高效的解决方案。

Method: 集成两个紧凑卷积神经网络（MobileNetV3-Small和ShuffleNetV2），通过简单平均进行预测级融合，并使用结合MSE和Pearson相关正则化的相关性感知损失函数（MSECorrLoss）。

Result: 在VQualA基准测试中取得了SRCC 0.9829和PLCC 0.9894的高相关性分数，同时满足计算效率约束。

Conclusion: 该方法在准确性和计算成本之间实现了良好平衡，适合实际部署，为人脸识别系统提供了高效的图像质量评估解决方案。

Abstract: Face image quality assessment (FIQA) plays a critical role in face
recognition and verification systems, especially in uncontrolled, real-world
environments. Although several methods have been proposed, general-purpose
no-reference image quality assessment techniques often fail to capture
face-specific degradations. Meanwhile, state-of-the-art FIQA models tend to be
computationally intensive, limiting their practical applicability. We propose a
lightweight and efficient method for FIQA, designed for the perceptual
evaluation of face images in the wild. Our approach integrates an ensemble of
two compact convolutional neural networks, MobileNetV3-Small and ShuffleNetV2,
with prediction-level fusion via simple averaging. To enhance alignment with
human perceptual judgments, we employ a correlation-aware loss (MSECorrLoss),
combining mean squared error (MSE) with a Pearson correlation regularizer. Our
method achieves a strong balance between accuracy and computational cost,
making it suitable for real-world deployment. Experiments on the VQualA FIQA
benchmark demonstrate that our model achieves a Spearman rank correlation
coefficient (SRCC) of 0.9829 and a Pearson linear correlation coefficient
(PLCC) of 0.9894, remaining within competition efficiency constraints.

</details>


### [39] [Realism Control One-step Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2509.10122)
*Zongliang Wu,Siming Zheng,Peng-Tao Jiang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出RCOD框架，通过潜在域分组策略和退化感知采样，在单步扩散模型中实现保真度与真实感的灵活权衡控制，提升真实图像超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 传统单步扩散方法在真实图像超分辨率任务中缺乏灵活的控制机制来平衡保真度和真实感这两个竞争目标，而多步方法可以通过调整采样步数来实现这种平衡。

Method: 提出Realism Controlled One-step Diffusion (RCOD)框架，包含：1) 潜在域分组策略实现噪声预测阶段的显式控制；2) 退化感知采样策略对齐蒸馏正则化；3) 视觉提示注入模块替代文本提示。

Result: 在定量指标和视觉质量上都优于最先进的单步扩散方法，同时保持计算效率，在推理阶段具有灵活的真实感控制能力。

Conclusion: RCOD框架成功解决了单步扩散模型在保真度-真实感权衡方面的局限性，为真实图像超分辨率任务提供了高效且可控的解决方案。

Abstract: Pre-trained diffusion models have shown great potential in real-world image
super-resolution (Real-ISR) tasks by enabling high-resolution reconstructions.
While one-step diffusion (OSD) methods significantly improve efficiency
compared to traditional multi-step approaches, they still have limitations in
balancing fidelity and realism across diverse scenarios. Since the OSDs for SR
are usually trained or distilled by a single timestep, they lack flexible
control mechanisms to adaptively prioritize these competing objectives, which
are inherently manageable in multi-step methods through adjusting sampling
steps. To address this challenge, we propose a Realism Controlled One-step
Diffusion (RCOD) framework for Real-ISR. RCOD provides a latent domain grouping
strategy that enables explicit control over fidelity-realism trade-offs during
the noise prediction phase with minimal training paradigm modifications and
original training data. A degradation-aware sampling strategy is also
introduced to align distillation regularization with the grouping strategy and
enhance the controlling of trade-offs. Moreover, a visual prompt injection
module is used to replace conventional text prompts with degradation-aware
visual tokens, enhancing both restoration accuracy and semantic consistency.
Our method achieves superior fidelity and perceptual quality while maintaining
computational efficiency. Extensive experiments demonstrate that RCOD
outperforms state-of-the-art OSD methods in both quantitative metrics and
visual qualities, with flexible realism control capabilities in the inference
stage. The code will be released.

</details>


### [40] [Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment](https://arxiv.org/abs/2509.10134)
*Rini Smita Thakur,Rajeev Ranjan Dwivedi,Vinod K Kurmi*

Main category: cs.CV

TL;DR: Grad-CL是一个新颖的源自由域适应框架，用于视盘和视杯的精确分割，通过梯度引导的伪标签细化和对比学习策略，在不访问源数据的情况下实现跨域分割性能的鲁棒适应。


<details>
  <summary>Details</summary>
Motivation: 解决视盘和视杯分割模型在不同成像协议或条件下性能显著下降的问题，特别是在无法访问原始源数据的情况下实现域适应。

Method: 采用两阶段方法：第一阶段通过梯度机制提取类别特定特征，进行不确定性量化和原型估计以细化伪标签；第二阶段使用基于余弦相似度的对比损失来增强视杯和视盘特征之间的类间可分性。

Result: 在具有挑战性的跨域眼底成像数据集上，Grad-CL超越了最先进的无监督和源自由域适应方法，实现了卓越的分割精度和改善的边界描绘。

Conclusion: Grad-CL框架能够有效解决视盘和视杯分割中的域适应问题，为眼科疾病的早期诊断和管理提供了可靠的解决方案。

Abstract: Accurate segmentation of the optic disc and cup is critical for the early
diagnosis and management of ocular diseases such as glaucoma. However,
segmentation models trained on one dataset often suffer significant performance
degradation when applied to target data acquired under different imaging
protocols or conditions. To address this challenge, we propose
\textbf{Grad-CL}, a novel source-free domain adaptation framework that
leverages a pre-trained source model and unlabeled target data to robustly
adapt segmentation performance without requiring access to the original source
data. Grad-CL combines a gradient-guided pseudolabel refinement module with a
cosine similarity-based contrastive learning strategy. In the first stage,
salient class-specific features are extracted via a gradient-based mechanism,
enabling more accurate uncertainty quantification and robust prototype
estimation for refining noisy pseudolabels. In the second stage, a contrastive
loss based on cosine similarity is employed to explicitly enforce inter-class
separability between the gradient-informed features of the optic cup and disc.
Extensive experiments on challenging cross-domain fundus imaging datasets
demonstrate that Grad-CL outperforms state-of-the-art unsupervised and
source-free domain adaptation methods, achieving superior segmentation accuracy
and improved boundary delineation. Project and code are available at
https://visdomlab.github.io/GCL/.

</details>


### [41] [Scalable Training for Vector-Quantized Networks with 100% Codebook Utilization](https://arxiv.org/abs/2509.10140)
*Yifan Chang,Jie Qin,Limeng Qiao,Xiaofeng Wang,Zheng Zhu,Lin Ma,Xingang Wang*

Main category: cs.CV

TL;DR: 提出了VQBridge方法解决VQ训练不稳定的问题，实现了100%码本使用率，显著提升了图像重建和生成性能


<details>
  <summary>Details</summary>
Motivation: 解决向量量化(VQ)训练中的不稳定性问题，包括straight-through估计偏差、滞后更新和稀疏码本梯度等挑战

Method: 提出VQBridge投影器，采用压缩-处理-恢复的管道优化码向量，结合学习退火实现稳定有效的码本训练

Result: 实现了100%码本使用率，在262k大码本上仍保持全使用率，达到SOTA重建性能，显著提升图像生成质量

Conclusion: 高质量的分词器对自回归图像生成至关重要，FVQ方法有效、可扩展且通用性强

Abstract: Vector quantization (VQ) is a key component in discrete tokenizers for image
generation, but its training is often unstable due to straight-through
estimation bias, one-step-behind updates, and sparse codebook gradients, which
lead to suboptimal reconstruction performance and low codebook usage. In this
work, we analyze these fundamental challenges and provide a simple yet
effective solution. To maintain high codebook usage in VQ networks (VQN) during
learning annealing and codebook size expansion, we propose VQBridge, a robust,
scalable, and efficient projector based on the map function method. VQBridge
optimizes code vectors through a compress-process-recover pipeline, enabling
stable and effective codebook training. By combining VQBridge with learning
annealing, our VQN achieves full (100%) codebook usage across diverse codebook
configurations, which we refer to as FVQ (FullVQ). Through extensive
experiments, we demonstrate that FVQ is effective, scalable, and generalizable:
it attains 100% codebook usage even with a 262k-codebook, achieves
state-of-the-art reconstruction performance, consistently improves with larger
codebooks, higher vector channels, or longer training, and remains effective
across different VQ variants. Moreover, when integrated with LlamaGen, FVQ
significantly enhances image generation performance, surpassing visual
autoregressive models (VAR) by 0.5 and diffusion models (DiT) by 0.2 rFID,
highlighting the importance of high-quality tokenizers for strong
autoregressive image generation.

</details>


### [42] [LayerLock: Non-collapsing Representation Learning with Progressive Freezing](https://arxiv.org/abs/2509.10156)
*Goker Erdogan,Nikhil Parthasarathy,Catalin Ionescu,Drew Hudson,Alexander Lerchner,Andrew Zisserman,Mehdi Sajjadi,Joao Carreira*

Main category: cs.CV

TL;DR: LayerLock是一种通过渐进层冻结实现从像素预测到潜在预测过渡的自监督视觉表示学习方法，可加速MAE训练并避免表示崩溃


<details>
  <summary>Details</summary>
Motivation: 观察到ViT层在视频MAE训练中按深度顺序收敛（浅层先收敛，深层后收敛），希望利用这一现象来优化训练过程

Method: 通过明确的进度表逐步冻结模型层，实现从像素预测到潜在预测的渐进过渡

Result: 在高达40亿参数的大型模型上应用LayerLock，在4DS感知套件上的结果超过了非潜在掩码预测方法

Conclusion: LayerLock是一种简单有效的自监督学习方法，能够显著加速MAE训练并提升性能

Abstract: We introduce LayerLock, a simple yet effective approach for self-supervised
visual representation learning, that gradually transitions from pixel to latent
prediction through progressive layer freezing. First, we make the observation
that during training of video masked-autoencoding (MAE) models, ViT layers
converge in the order of their depth: shallower layers converge early, deeper
layers converge late. We then show that this observation can be exploited to
accelerate standard MAE by progressively freezing the model according to an
explicit schedule, throughout training. Furthermore, this same schedule can be
used in a simple and scalable approach to latent prediction that does not
suffer from "representation collapse". We apply our proposed approach,
LayerLock, to large models of up to 4B parameters with results surpassing those
of non-latent masked prediction on the 4DS perception suite.

</details>


### [43] [On the Geometric Accuracy of Implicit and Primitive-based Representations Derived from View Rendering Constraints](https://arxiv.org/abs/2509.10241)
*Elias De Smijter,Renaud Detry,Christophe De Vleeschouwer*

Main category: cs.CV

TL;DR: 本文系统比较了隐式和显式新视角合成方法在空间3D重建中的表现，发现外观嵌入虽能提升光度保真度，但无法改善几何精度，且凸面喷溅比高斯喷溅具有更紧凑的表达优势。


<details>
  <summary>Details</summary>
Motivation: 研究外观嵌入在空间基3D物体重建中对几何精度的影响，特别是针对空间机器人应用中几何精度的关键需求。

Method: 使用SPEED+数据集，比较K-Planes、高斯喷溅和凸面喷溅三种方法，分析外观嵌入对几何精度和表示效率的作用。

Result: 外观嵌入主要减少显式方法所需基元数量而非提升几何保真度；凸面喷溅相比高斯喷溅能产生更紧凑且无杂乱的表示。

Conclusion: 外观嵌入在几何中心任务中存在局限性，空间场景中重建质量与表示效率需要权衡，凸面喷溅在安全关键应用中具有优势。

Abstract: We present the first systematic comparison of implicit and explicit Novel
View Synthesis methods for space-based 3D object reconstruction, evaluating the
role of appearance embeddings. While embeddings improve photometric fidelity by
modeling lighting variation, we show they do not translate into meaningful
gains in geometric accuracy - a critical requirement for space robotics
applications. Using the SPEED+ dataset, we compare K-Planes, Gaussian
Splatting, and Convex Splatting, and demonstrate that embeddings primarily
reduce the number of primitives needed for explicit methods rather than
enhancing geometric fidelity. Moreover, convex splatting achieves more compact
and clutter-free representations than Gaussian splatting, offering advantages
for safety-critical applications such as interaction and collision avoidance.
Our findings clarify the limits of appearance embeddings for geometry-centric
tasks and highlight trade-offs between reconstruction quality and
representation efficiency in space scenarios.

</details>


### [44] [GAMMA: Generalizable Alignment via Multi-task and Manipulation-Augmented Training for AI-Generated Image Detection](https://arxiv.org/abs/2509.10250)
*Haozhen Yan,Yan Hong,Suning Lang,Jiahui Zhan,Yikun Ji,Yujie Gao,Jun Lan,Huijia Zhu,Weiqiang Wang,Jianfu Zhang*

Main category: cs.CV

TL;DR: GAMMA是一个新的AI生成图像检测框架，通过减少领域偏差和增强语义对齐，在未见过的生成模型上实现了更好的泛化性能


<details>
  <summary>Details</summary>
Motivation: 现有AI生成图像检测器过度依赖特定生成模型的伪影特征，在面对新生成模型时泛化能力有限

Method: 采用多样化的图像操作策略（如修复式操作和语义保持扰动），多任务监督（双分割头和分类头），以及反向交叉注意力机制

Result: 在GenImage基准测试上准确率提升5.8%，对GPT-4o等新发布生成模型保持强鲁棒性

Conclusion: GAMMA通过减少领域偏差和增强语义一致性，显著提升了AI生成图像检测的泛化能力

Abstract: With generative models becoming increasingly sophisticated and diverse,
detecting AI-generated images has become increasingly challenging. While
existing AI-genereted Image detectors achieve promising performance on
in-distribution generated images, their generalization to unseen generative
models remains limited. This limitation is largely attributed to their reliance
on generation-specific artifacts, such as stylistic priors and compression
patterns. To address these limitations, we propose GAMMA, a novel training
framework designed to reduce domain bias and enhance semantic alignment. GAMMA
introduces diverse manipulation strategies, such as inpainting-based
manipulation and semantics-preserving perturbations, to ensure consistency
between manipulated and authentic content. We employ multi-task supervision
with dual segmentation heads and a classification head, enabling pixel-level
source attribution across diverse generative domains. In addition, a reverse
cross-attention mechanism is introduced to allow the segmentation heads to
guide and correct biased representations in the classification branch. Our
method achieves state-of-the-art generalization performance on the GenImage
benchmark, imporving accuracy by 5.8%, but also maintains strong robustness on
newly released generative model such as GPT-4o.

</details>


### [45] [Robustness and Diagnostic Performance of Super-Resolution Fetal Brain MRI](https://arxiv.org/abs/2509.10257)
*Ema Masterl,Tina Vipotnik Vesnaver,Žiga Špiclin*

Main category: cs.CV

TL;DR: 比较三种胎儿脑MRI超分辨率重建方法(NiftyMIC、SVRTK、NeSVoR)在140例扫描中的性能，发现NeSVoR重建成功率最高(>90%)，尽管不同方法产生体积测量差异，但不影响脑室扩大诊断分类性能。


<details>
  <summary>Details</summary>
Motivation: 胎儿脑MRI的多视角2D切片采集存在分辨率低、运动伪影和3D解剖信息不足的问题，现有超分辨率重建方法在病理病例中的比较性能及其对下游分析和诊断任务的影响尚未充分探索。

Method: 应用三种先进SRR方法(NiftyMIC、SVRTK、NeSVoR)处理140例胎儿脑MRI扫描(包括健康对照和脑室扩大病理病例)，使用BoUNTi算法分割重建后的高分辨率体积，提取9个主要脑结构体积，评估视觉质量、重建成功率、体积测量一致性和诊断分类性能。

Result: NeSVoR在健康组和病理组均表现出最高且最一致的重建成功率(>90%)；不同SRR方法之间存在显著的体积估计差异；但SRR方法的选择不影响脑室扩大的分类性能。

Conclusion: NeSVoR表现出最强的鲁棒性，尽管SRR引起的体积测量存在变异性，但诊断性能具有韧性，表明不同重建方法产生的体积差异不会影响临床诊断结果。

Abstract: Fetal brain MRI relies on rapid multi-view 2D slice acquisitions to reduce
motion artifacts caused by fetal movement. However, these stacks are typically
low resolution, may suffer from motion corruption, and do not adequately
capture 3D anatomy. Super-resolution reconstruction (SRR) methods aim to
address these limitations by combining slice-to-volume registration and
super-resolution techniques to generate high-resolution (HR) 3D volumes. While
several SRR methods have been proposed, their comparative performance -
particularly in pathological cases - and their influence on downstream
volumetric analysis and diagnostic tasks remain underexplored. In this study,
we applied three state-of-the-art SRR method - NiftyMIC, SVRTK, and NeSVoR - to
140 fetal brain MRI scans, including both healthy controls (HC) and
pathological cases (PC) with ventriculomegaly (VM). Each HR reconstruction was
segmented using the BoUNTi algorithm to extract volumes of nine principal brain
structures. We evaluated visual quality, SRR success rates, volumetric
measurement agreement, and diagnostic classification performance. NeSVoR
demonstrated the highest and most consistent reconstruction success rate (>90%)
across both HC and PC groups. Although significant differences in volumetric
estimates were observed between SRR methods, classification performance for VM
was not affected by the choice of SRR method. These findings highlight NeSVoR's
robustness and the resilience of diagnostic performance despite SRR-induced
volumetric variability.

</details>


### [46] [Mask Consistency Regularization in Object Removal](https://arxiv.org/abs/2509.10259)
*Hua Yuan,Jin Yuan,Yicheng Jiang,Yao Zhang,Xin Geng,Yong Rui*

Main category: cs.CV

TL;DR: 提出Mask Consistency Regularization (MCR)训练策略，通过扩张和重塑两种掩码扰动来解决目标移除任务中的掩码幻觉和掩码形状偏差问题。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在图像修复中的目标移除任务存在两个关键挑战：掩码幻觉（在掩码区域内生成无关内容）和掩码形状偏差（生成内容模仿掩码形状而非周围内容）。

Method: 提出MCR训练策略，在训练过程中引入扩张掩码和重塑掩码两种扰动，强制这些扰动分支的输出与原始掩码保持一致。扩张掩码帮助模型输出与周围内容对齐，重塑掩码鼓励模型打破掩码形状偏差。

Result: 实验证明MCR显著减少了幻觉和掩码形状偏差，在目标移除任务中实现了更好的性能。

Conclusion: MCR策略能够产生更鲁棒和上下文连贯的图像修复结果，有效解决了目标移除任务中的核心挑战。

Abstract: Object removal, a challenging task within image inpainting, involves
seamlessly filling the removed region with content that matches the surrounding
context. Despite advancements in diffusion models, current methods still face
two critical challenges. The first is mask hallucination, where the model
generates irrelevant or spurious content inside the masked region, and the
second is mask-shape bias, where the model fills the masked area with an object
that mimics the mask's shape rather than surrounding content. To address these
issues, we propose Mask Consistency Regularization (MCR), a novel training
strategy designed specifically for object removal tasks. During training, our
approach introduces two mask perturbations: dilation and reshape, enforcing
consistency between the outputs of these perturbed branches and the original
mask. The dilated masks help align the model's output with the surrounding
content, while reshaped masks encourage the model to break the mask-shape bias.
This combination of strategies enables MCR to produce more robust and
contextually coherent inpainting results. Our experiments demonstrate that MCR
significantly reduces hallucinations and mask-shape bias, leading to improved
performance in object removal.

</details>


### [47] [MagicMirror: A Large-Scale Dataset and Benchmark for Fine-Grained Artifacts Assessment in Text-to-Image Generation](https://arxiv.org/abs/2509.10260)
*Jia Wang,Jie Hu,Xiaoqi Ma,Hanghang Ma,Yanbing Zeng,Xiaoming Wei*

Main category: cs.CV

TL;DR: MagicMirror是一个全面的文本到图像生成伪影评估框架，包含首个大规模人工标注数据集MagicData340K、基于VLM的评估模型MagicAssessor，以及自动化基准测试MagicBench，揭示了当前顶级T2I模型仍存在严重伪影问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型在遵循指令和美学方面取得显著进展，但普遍存在解剖学和结构缺陷等物理伪影，严重降低感知质量并限制应用。缺乏系统化和细粒度的评估框架来应对这些多样复杂的伪影问题。

Method: 1) 建立生成图像伪影的详细分类体系；2) 人工标注340K图像的大规模数据集MagicData340K；3) 训练Vision-Language模型MagicAssessor进行详细评估；4) 设计新颖的数据采样策略和多级奖励系统用于GRPO训练；5) 构建自动化基准测试MagicBench评估T2I模型伪影。

Result: 评估发现即使像GPT-image-1这样的顶级模型也持续受到显著伪影的困扰，表明伪影减少是未来T2I发展的关键前沿。

Conclusion: MagicMirror框架填补了T2I生成伪影评估的空白，为系统化评估和改进图像生成质量提供了重要工具，强调了解决伪影问题对提升T2I模型实用性的重要性。

Abstract: Text-to-image (T2I) generation has achieved remarkable progress in
instruction following and aesthetics. However, a persistent challenge is the
prevalence of physical artifacts, such as anatomical and structural flaws,
which severely degrade perceptual quality and limit application. Given the
diversity and complexity of these artifacts, a systematic and fine-grained
evaluation framework is required, which is lacking in current benchmarks. To
fill this gap, we introduce MagicMirror, a comprehensive framework for
artifacts assessment. We first establish a detailed taxonomy of generated image
artifacts. Guided by this taxonomy, we manually annotate MagicData340K, the
first human-annotated large-scale dataset of 340K generated images with
fine-grained artifact labels. Building on this dataset, we train MagicAssessor,
a Vision-Language Model (VLM) that provides detailed assessments and
corresponding labels. To overcome challenges like class imbalance and reward
hacking, we design a novel data sampling strategy and a multi-level reward
system for Group Relative Policy Optimization (GRPO). Finally, we leverage
MagicAssessor to construct MagicBench, an automated benchmark for evaluating
the image artifacts of current T2I models. Our evaluation with MagicBench
reveals that despite their widespread adoption, even top-tier models like
GPT-image-1 are consistently plagued by significant artifacts, highlighting
artifact reduction as a critical frontier for future T2I development. Project
page: https://wj-inf.github.io/MagicMirror-page/.

</details>


### [48] [SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion](https://arxiv.org/abs/2509.10266)
*Wenfang Wu,Tingting Yuan,Yupeng Li,Daling Wang,Xiaoming Fu*

Main category: cs.CV

TL;DR: SignClip是一个新的手语翻译框架，通过融合手势和唇部运动特征，并采用分层对比学习来提升翻译准确性，在PHOENIX14T数据集上超越了之前的SOTA模型。


<details>
  <summary>Details</summary>
Motivation: 当前手语翻译方法主要关注手势信号，忽视了唇部运动等非手动线索，而这些线索对于区分视觉相似的手势和传达语言信息至关重要。

Method: 提出SignClip框架，融合空间手势和唇部运动特征，引入分层对比学习框架，包含多级对齐目标，确保手语-唇部和视觉-文本模态间的语义一致性。

Result: 在两个基准数据集PHOENIX14T和How2Sign上进行了广泛实验。在PHOENIX14T的无注释设置下，BLEU-4从24.32提升到24.71，ROUGE从46.57提升到48.38，超越了之前的SOTA模型SpaMo。

Conclusion: SignClip通过有效融合手动和非手动线索，显著提升了手语翻译的准确性，证明了唇部运动特征在手语翻译中的重要性。

Abstract: Sign language translation (SLT) aims to translate natural language from sign
language videos, serving as a vital bridge for inclusive communication. While
recent advances leverage powerful visual backbones and large language models,
most approaches mainly focus on manual signals (hand gestures) and tend to
overlook non-manual cues like mouthing. In fact, mouthing conveys essential
linguistic information in sign languages and plays a crucial role in
disambiguating visually similar signs. In this paper, we propose SignClip, a
novel framework to improve the accuracy of sign language translation. It fuses
manual and non-manual cues, specifically spatial gesture and lip movement
features. Besides, SignClip introduces a hierarchical contrastive learning
framework with multi-level alignment objectives, ensuring semantic consistency
across sign-lip and visual-text modalities. Extensive experiments on two
benchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of our
approach. For example, on PHOENIX14T, in the Gloss-free setting, SignClip
surpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from
24.32 to 24.71, and ROUGE from 46.57 to 48.38.

</details>


### [49] [Detecting Text Manipulation in Images using Vision Language Models](https://arxiv.org/abs/2509.10278)
*Vidit Vidit,Pavel Korshunov,Amir Mohammadi,Christophe Ecabert,Ketan Kotwal,Sébastien Marcel*

Main category: cs.CV

TL;DR: 本文分析开源和闭源视觉语言模型在文本篡改检测方面的性能，发现开源模型正在接近但仍落后于闭源模型如GPT-4o，并揭示了图像篡改检测模型在文本篡改检测中存在泛化问题。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型研究主要关注图像篡改检测，而文本篡改检测的研究相对缺失，本文旨在填补这一知识空白。

Method: 通过在多个文本篡改数据集上测试闭源和开源视觉语言模型，包括真实场景文本和模拟现实滥用的伪造ID卡场景。

Result: 开源模型性能正在提升但仍落后于闭源模型；专门用于图像篡改检测的模型在文本篡改检测上存在泛化问题。

Conclusion: 文本篡改检测是一个重要但被忽视的研究领域，需要开发更专门的模型来应对现实世界中的文本篡改挑战。

Abstract: Recent works have shown the effectiveness of Large Vision Language Models
(VLMs or LVLMs) in image manipulation detection. However, text manipulation
detection is largely missing in these studies. We bridge this knowledge gap by
analyzing closed- and open-source VLMs on different text manipulation datasets.
Our results suggest that open-source models are getting closer, but still
behind closed-source ones like GPT- 4o. Additionally, we benchmark image
manipulation detection-specific VLMs for text manipulation detection and show
that they suffer from the generalization problem. We benchmark VLMs for
manipulations done on in-the-wild scene texts and on fantasy ID cards, where
the latter mimic a challenging real-world misuse.

</details>


### [50] [MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection](https://arxiv.org/abs/2509.10282)
*Gang Li,Tianjiao Chen,Mingle Zhou,Min Li,Delong Han,Jin Wan*

Main category: cs.CV

TL;DR: MCL-AD是一个新颖的多模态协作学习框架，利用点云、RGB图像和文本语义实现零样本3D异常检测，通过多模态提示学习和协作调制机制取得了最先进的性能


<details>
  <summary>Details</summary>
Motivation: 现有方法大多只关注点云数据，忽略了RGB图像和文本先验等互补模态提供的丰富语义线索，限制了零样本3D异常检测的效果

Method: 提出多模态提示学习机制（MPLM）增强模态内表示能力和模态间协作学习，使用对象无关的解耦文本提示和多模态对比损失；设计协作调制机制（CMM）联合调制RGB图像引导和点云引导分支

Result: 大量实验证明MCL-AD框架在零样本3D异常检测中实现了最先进的性能

Conclusion: 多模态协作学习能够有效提升零样本3D异常检测效果，充分利用不同模态的互补信息是关键

Abstract: Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objects
without relying on labeled training data, making it especially valuable in
scenarios constrained by data scarcity, privacy, or high annotation cost.
However, most existing methods focus exclusively on point clouds, neglecting
the rich semantic cues available from complementary modalities such as RGB
images and texts priors. This paper introduces MCL-AD, a novel framework that
leverages multimodal collaboration learning across point clouds, RGB images,
and texts semantics to achieve superior zero-shot 3D anomaly detection.
Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) that
enhances the intra-modal representation capability and inter-modal
collaborative learning by introducing an object-agnostic decoupled text prompt
and a multimodal contrastive loss. In addition, a collaborative modulation
mechanism (CMM) is proposed to fully leverage the complementary representations
of point clouds and RGB images by jointly modulating the RGB image-guided and
point cloud-guided branches. Extensive experiments demonstrate that the
proposed MCL-AD framework achieves state-of-the-art performance in ZS-3D
anomaly detection.

</details>


### [51] [Adversarial robustness through Lipschitz-Guided Stochastic Depth in Neural Networks](https://arxiv.org/abs/2509.10298)
*Laith Nayal,Mahmoud Mousatat,Bader Rasheed*

Main category: cs.CV

TL;DR: 提出基于Lipschitz约束的随机深度方法，通过深度相关的DropPath概率控制网络有效Lipschitz常数，在保持精度的同时提升对抗鲁棒性并减少计算量


<details>
  <summary>Details</summary>
Motivation: 深度神经网络和Vision Transformers在计算机视觉中表现优异但对抗扰动脆弱，现有防御方法计算成本高或缺乏形式化保证

Method: Lipschitz引导的随机深度方法，drop概率随深度增加以控制网络有效Lipschitz常数，对深层进行正则化

Result: 在CIFAR-10和ViT-Tiny上实验表明，该方法保持接近基线精度，提升FGSM、PGD-20和AutoAttack下的鲁棒性，相比基线和线性DropPath显著减少FLOPs

Conclusion: 深度相关的DropPath调度能有效平衡模型精度、鲁棒性和计算效率，为对抗防御提供新思路

Abstract: Deep neural networks and Vision Transformers achieve state-of-the-art
performance in computer vision but are highly vulnerable to adversarial
perturbations. Standard defenses often incur high computational cost or lack
formal guarantees. We propose a Lipschitz-guided stochastic depth (DropPath)
method, where drop probabilities increase with depth to control the effective
Lipschitz constant of the network. This approach regularizes deeper layers,
improving robustness while preserving clean accuracy and reducing computation.
Experiments on CIFAR-10 with ViT-Tiny show that our custom depth-dependent
schedule maintains near-baseline clean accuracy, enhances robustness under
FGSM, PGD-20, and AutoAttack, and significantly reduces FLOPs compared to
baseline and linear DropPath schedules.

</details>


### [52] [A Stochastic Birth-and-Death Approach for Street Furniture Geolocation in Urban Environments](https://arxiv.org/abs/2509.10310)
*Evan Murphy,Marco Viola,Vladimir A. Krylov*

Main category: cs.CV

TL;DR: 提出基于能量图的概率框架，用于复杂城市环境中街道家具的精确定位，结合地理空间信息提高定位精度。


<details>
  <summary>Details</summary>
Motivation: 解决城市公共基础设施监控和维护中的精确定位问题，为地方当局和私人利益相关者提供有效的资产映射解决方案。

Method: 采用基于能量图的概率框架，编码物体位置的空间可能性，结合GIS图层、道路地图等外部地理空间信息，使用随机生死优化算法推断最可能的资产配置。

Result: 通过在都柏林市中心街灯基础设施数据集上的真实模拟验证，展示了该方法在城市资产映射中的可扩展性和准确性。

Conclusion: 该方法为城市资产定位提供了有效的概率框架，能够整合多种地理空间信息，实现精确且可扩展的街道家具定位。

Abstract: In this paper we address the problem of precise geolocation of street
furniture in complex urban environments, which is a critical task for effective
monitoring and maintenance of public infrastructure by local authorities and
private stakeholders. To this end, we propose a probabilistic framework based
on energy maps that encode the spatial likelihood of object locations.
Representing the energy in a map-based geopositioned format allows the
optimisation process to seamlessly integrate external geospatial information,
such as GIS layers, road maps, or placement constraints, which improves
contextual awareness and localisation accuracy. A stochastic birth-and-death
optimisation algorithm is introduced to infer the most probable configuration
of assets. We evaluate our approach using a realistic simulation informed by a
geolocated dataset of street lighting infrastructure in Dublin city centre,
demonstrating its potential for scalable and accurate urban asset mapping. The
implementation of the algorithm will be made available in the GitHub repository
https://github.com/EMurphy0108/SBD_Street_Furniture.

</details>


### [53] [Compute Only 16 Tokens in One Timestep: Accelerating Diffusion Transformers with Cluster-Driven Feature Caching](https://arxiv.org/abs/2509.10312)
*Zhixin Zheng,Xinyu Wang,Chang Zou,Shaobo Wang,Linfeng Zhang*

Main category: cs.CV

TL;DR: 提出Cluster-Driven Feature Caching (ClusCa)方法，通过在空间维度对token进行聚类，仅计算每个聚类中的一个token并将其信息传播给其他token，实现扩散变换器的高效加速。


<details>
  <summary>Details</summary>
Motivation: 现有特征缓存方法主要利用扩散模型的时间相似性，但忽略了空间维度的相似性，导致计算效率仍有提升空间。

Method: ClusCa方法在每一时间步对token进行空间聚类，每个聚类只计算一个代表性token，然后将信息传播给聚类内其他token，可减少90%以上的token数量。

Result: 在DiT、FLUX和HunyuanVideo上验证了有效性，实现了4.96倍加速，ImageReward达到99.49%，甚至超过原始模型0.51%。

Conclusion: ClusCa提供了与现有特征缓存正交且互补的加速视角，无需训练即可直接应用于任何扩散变换器，显著提升生成效率。

Abstract: Diffusion transformers have gained significant attention in recent years for
their ability to generate high-quality images and videos, yet still suffer from
a huge computational cost due to their iterative denoising process. Recently,
feature caching has been introduced to accelerate diffusion transformers by
caching the feature computation in previous timesteps and reusing it in the
following timesteps, which leverage the temporal similarity of diffusion models
while ignoring the similarity in the spatial dimension. In this paper, we
introduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and
complementary perspective for previous feature caching. Specifically, ClusCa
performs spatial clustering on tokens in each timestep, computes only one token
in each cluster and propagates their information to all the other tokens, which
is able to reduce the number of tokens by over 90%. Extensive experiments on
DiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image
and text-to-video generation. Besides, it can be directly applied to any
diffusion transformer without requirements for training. For instance, ClusCa
achieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing
the original model by 0.51%. The code is available at
https://github.com/Shenyi-Z/Cache4Diffusion.

</details>


### [54] [I-Segmenter: Integer-Only Vision Transformer for Efficient Semantic Segmentation](https://arxiv.org/abs/2509.10334)
*Jordan Sassoon,Michal Szczepanski,Martyna Poreba*

Main category: cs.CV

TL;DR: I-Segmenter是首个完全整数化的ViT分割框架，通过量化技术显著降低模型大小和计算成本，同时保持与FP32基线相近的精度


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在语义分割中表现优异，但在资源受限设备上部署困难，量化技术能提高效率但传统ViT分割模型在低精度下表现脆弱

Method: 基于Segmenter架构，系统替换浮点操作为整数操作；提出λ-ShiftGELU激活函数处理长尾分布；移除L2归一化层；用最近邻上采样替换双线性插值

Result: 在保持与FP32基线5.1%精度差距内，模型大小减少3.8倍，推理速度提升1.2倍；单样本PTQ也能获得竞争性精度

Conclusion: I-Segmenter为ViT分割模型的现实部署提供了实用解决方案，在效率和精度间取得了良好平衡

Abstract: Vision Transformers (ViTs) have recently achieved strong results in semantic
segmentation, yet their deployment on resource-constrained devices remains
limited due to their high memory footprint and computational cost. Quantization
offers an effective strategy to improve efficiency, but ViT-based segmentation
models are notoriously fragile under low precision, as quantization errors
accumulate across deep encoder-decoder pipelines. We introduce I-Segmenter, the
first fully integer-only ViT segmentation framework. Building on the Segmenter
architecture, I-Segmenter systematically replaces floating-point operations
with integer-only counterparts. To further stabilize both training and
inference, we propose $\lambda$-ShiftGELU, a novel activation function that
mitigates the limitations of uniform quantization in handling long-tailed
activation distributions. In addition, we remove the L2 normalization layer and
replace bilinear interpolation in the decoder with nearest neighbor upsampling,
ensuring integer-only execution throughout the computational graph. Extensive
experiments show that I-Segmenter achieves accuracy within a reasonable margin
of its FP32 baseline (5.1 % on average), while reducing model size by up to
3.8x and enabling up to 1.2x faster inference with optimized runtimes. Notably,
even in one-shot PTQ with a single calibration image, I-Segmenter delivers
competitive accuracy, underscoring its practicality for real-world deployment.

</details>


### [55] [GARD: Gamma-based Anatomical Restoration and Denoising for Retinal OCT](https://arxiv.org/abs/2509.10341)
*Botond Fazekas,Thomas Pinetz,Guilherme Aresta,Taha Emre,Hrvoje Bogunovic*

Main category: cs.CV

TL;DR: GARD是一种基于伽马扩散模型的OCT图像去噪方法，通过噪声减少保真项和加速推理，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: OCT图像存在散斑噪声问题，传统去噪方法难以平衡噪声消除与解剖结构保留的需求，需要更准确的统计建模和更好的去噪效果。

Method: 提出GARD方法，使用去噪扩散伽马模型（而非传统高斯模型）更准确建模散斑统计特性，引入噪声减少保真项指导去噪过程，并采用DDIM框架加速推理。

Result: 在配对噪声和低噪声OCT B扫描数据集上，GARD在PSNR、SSIM和MSE指标上显著优于传统去噪方法和先进深度学习模型，定性结果显示边缘更清晰、解剖细节保留更好。

Conclusion: GARD通过伽马扩散模型和噪声减少保真项，有效解决了OCT图像去噪中噪声消除与结构保留的平衡问题，提供了优异的去噪性能。

Abstract: Optical Coherence Tomography (OCT) is a vital imaging modality for diagnosing
and monitoring retinal diseases. However, OCT images are inherently degraded by
speckle noise, which obscures fine details and hinders accurate interpretation.
While numerous denoising methods exist, many struggle to balance noise
reduction with the preservation of crucial anatomical structures. This paper
introduces GARD (Gamma-based Anatomical Restoration and Denoising), a novel
deep learning approach for OCT image despeckling that leverages the strengths
of diffusion probabilistic models. Unlike conventional diffusion models that
assume Gaussian noise, GARD employs a Denoising Diffusion Gamma Model to more
accurately reflect the statistical properties of speckle. Furthermore, we
introduce a Noise-Reduced Fidelity Term that utilizes a pre-processed,
less-noisy image to guide the denoising process. This crucial addition prevents
the reintroduction of high-frequency noise. We accelerate the inference process
by adapting the Denoising Diffusion Implicit Model framework to our Gamma-based
model. Experiments on a dataset with paired noisy and less-noisy OCT B-scans
demonstrate that GARD significantly outperforms traditional denoising methods
and state-of-the-art deep learning models in terms of PSNR, SSIM, and MSE.
Qualitative results confirm that GARD produces sharper edges and better
preserves fine anatomical details.

</details>


### [56] [GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography](https://arxiv.org/abs/2509.10344)
*Yuexi Du,Lihui Chen,Nicha C. Dvornek*

Main category: cs.CV

TL;DR: 提出了GLAM模型，通过几何引导的全局和局部对齐方法，解决乳腺X光片多视图关系建模问题，在多个数据集上优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺X光片视觉语言模型忽略了医学图像特有的多视图关系特性，无法像放射科医生那样同时分析双侧视图，导致预测性能受限。

Method: GLAM模型利用乳腺X光片多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉和视觉-语言的对比学习，学习局部跨视图对齐和细粒度局部特征。

Result: 在EMBED数据集上预训练后，GLAM模型在多个数据集的不同设置下均优于基线方法。

Conclusion: 通过几何引导的多视图对齐方法能够有效提升乳腺X光片视觉语言模型的性能，为医学图像分析提供了新的解决方案。

Abstract: Mammography screening is an essential tool for early detection of breast
cancer. The speed and accuracy of mammography interpretation have the potential
to be improved with deep learning methods. However, the development of a
foundation visual language model (VLM) is hindered by limited data and domain
differences between natural and medical images. Existing mammography VLMs,
adapted from natural images, often ignore domain-specific characteristics, such
as multi-view relationships in mammography. Unlike radiologists who analyze
both views together to process ipsilateral correspondence, current methods
treat them as independent images or do not properly model the multi-view
correspondence learning, losing critical geometric context and resulting in
suboptimal prediction. We propose GLAM: Global and Local Alignment for
Multi-view mammography for VLM pretraining using geometry guidance. By
leveraging the prior knowledge about the multi-view imaging process of
mammograms, our model learns local cross-view alignments and fine-grained local
features through joint global and local, visual-visual, and visual-language
contrastive learning. Pretrained on EMBED [14], one of the largest open
mammography datasets, our model outperforms baselines across multiple datasets
under different settings.

</details>


### [57] [Towards Understanding Visual Grounding in Visual Language Models](https://arxiv.org/abs/2509.10345)
*Georgios Pantazopoulos,Eda B. Özyiğit*

Main category: cs.CV

TL;DR: 这篇综述论文回顾了现代通用视觉语言模型(VLMs)中视觉定位的研究现状，包括其重要性、核心组件、实际应用、评估方法以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 视觉定位能力使模型能够根据文本描述识别视觉输入中的特定区域，这种能力对于广泛的跨模态应用至关重要，包括指代表达理解、细粒度视觉问答、视觉上下文描述以及环境控制等。

Method: 论文采用综述研究方法，首先阐述视觉定位在VLMs中的重要性，然后分析现代视觉定位模型的核心组件开发范式，并考察其实际应用场景和评估指标。

Result: 论文系统梳理了视觉定位领域的代表性工作，深入探讨了视觉定位、多模态思维链和推理之间的多维相互关系，为领域研究提供了全面的理论框架。

Conclusion: 视觉定位是VLMs发展的关键能力，论文分析了该领域面临的挑战，并提出了未来研究的潜在方向，为相关研究提供了重要的参考和指导。

Abstract: Visual grounding refers to the ability of a model to identify a region within
some visual input that matches a textual description. Consequently, a model
equipped with visual grounding capabilities can target a wide range of
applications in various domains, including referring expression comprehension,
answering questions pertinent to fine-grained details in images or videos,
caption visual context by explicitly referring to entities, as well as low and
high-level control in simulated and real environments. In this survey paper, we
review representative works across the key areas of research on modern
general-purpose vision language models (VLMs). We first outline the importance
of grounding in VLMs, then delineate the core components of the contemporary
paradigm for developing grounded models, and examine their practical
applications, including benchmarks and evaluation metrics for grounded
multimodal generation. We also discuss the multifaceted interrelations among
visual grounding, multimodal chain-of-thought, and reasoning in VLMs. Finally,
we analyse the challenges inherent to visual grounding and suggest promising
directions for future research.

</details>


### [58] [Immunizing Images from Text to Image Editing via Adversarial Cross-Attention](https://arxiv.org/abs/2509.10359)
*Matteo Trippodo,Federico Becattini,Lorenzo Seidenari*

Main category: cs.CV

TL;DR: 提出针对文本图像编辑系统的Attention Attack攻击方法，通过自动生成图像描述破坏跨注意力机制，无需了解编辑方法或提示即可有效降低编辑性能


<details>
  <summary>Details</summary>
Motivation: 现有文本图像编辑方法容易受到对抗攻击，但现有攻击方法存在局限性，需要探索针对视觉组件的新型攻击策略

Method: 使用源图像的自动生成描述作为编辑提示的代理，破坏文本提示与视觉表示之间的跨注意力对齐，提出Caption Similarity和语义IoU两种新评估指标

Result: 在TEDBench++基准测试中，该攻击显著降低了编辑性能且保持不可感知性

Conclusion: Attention Attack是一种有效的对抗攻击方法，揭示了文本图像编辑系统的安全漏洞，提出的评估指标能更好衡量攻击效果

Abstract: Recent advances in text-based image editing have enabled fine-grained
manipulation of visual content guided by natural language. However, such
methods are susceptible to adversarial attacks. In this work, we propose a
novel attack that targets the visual component of editing methods. We introduce
Attention Attack, which disrupts the cross-attention between a textual prompt
and the visual representation of the image by using an automatically generated
caption of the source image as a proxy for the edit prompt. This breaks the
alignment between the contents of the image and their textual description,
without requiring knowledge of the editing method or the editing prompt.
Reflecting on the reliability of existing metrics for immunization success, we
propose two novel evaluation strategies: Caption Similarity, which quantifies
semantic consistency between original and adversarial edits, and semantic
Intersection over Union (IoU), which measures spatial layout disruption via
segmentation masks. Experiments conducted on the TEDBench++ benchmark
demonstrate that our attack significantly degrades editing performance while
remaining imperceptible.

</details>


### [59] [Efficient Learned Image Compression Through Knowledge Distillation](https://arxiv.org/abs/2509.10366)
*Fabien Allemand,Attilio Fiandrotti,Sumanta Chaudhuri,Alaa Eddine Mazouz*

Main category: cs.CV

TL;DR: 该论文研究通过知识蒸馏技术降低基于神经网络的图像压缩方法的计算资源需求，使其更适合资源受限平台上的实时应用。


<details>
  <summary>Details</summary>
Motivation: 神经网络图像压缩方法虽然性能优于传统编解码器，但计算资源需求过高，无法在资源受限平台上实现实时应用，这阻碍了其主流部署。

Method: 采用知识蒸馏训练范式，让较小的神经网络通过部分学习更大、更复杂模型的输出来获得更好的性能，而不是独立训练。

Result: 研究表明知识蒸馏可有效应用于图像压缩任务：适用于不同架构大小、实现不同图像质量/比特率权衡，并能节省处理和能源资源。

Conclusion: 知识蒸馏为神经网络图像压缩提供了资源优化的有效途径，未来可探索不同教师模型和替代损失函数的影响，并可扩展到基于Transformer的模型。

Abstract: Learned image compression sits at the intersection of machine learning and
image processing. With advances in deep learning, neural network-based
compression methods have emerged. In this process, an encoder maps the image to
a low-dimensional latent space, which is then quantized, entropy-coded into a
binary bitstream, and transmitted to the receiver. At the receiver end, the
bitstream is entropy-decoded, and a decoder reconstructs an approximation of
the original image. Recent research suggests that these models consistently
outperform conventional codecs. However, they require significant processing
power, making them unsuitable for real-time use on resource-constrained
platforms, which hinders their deployment in mainstream applications. This
study aims to reduce the resource requirements of neural networks used for
image compression by leveraging knowledge distillation, a training paradigm
where smaller neural networks, partially trained on the outputs of larger, more
complex models, can achieve better performance than when trained independently.
Our work demonstrates that knowledge distillation can be effectively applied to
image compression tasks: i) across various architecture sizes, ii) to achieve
different image quality/bit rate tradeoffs, and iii) to save processing and
energy resources. This approach introduces new settings and hyperparameters,
and future research could explore the impact of different teacher models, as
well as alternative loss functions. Knowledge distillation could also be
extended to transformer-based models. The code is publicly available at:
https://github.com/FABallemand/PRIM .

</details>


### [60] [Ordinality of Visible-Thermal Image Intensities for Intrinsic Image Decomposition](https://arxiv.org/abs/2509.10388)
*Zeqing Leo Yuan,Mani Ramanagopal,Aswin C. Sankaranarayanan,Srinivasa G. Narasimhan*

Main category: cs.CV

TL;DR: 提出一种无需训练的方法，利用可见光和热成像图像对进行本征图像分解，通过热成像吸收特性建立光强与反射率/阴影的序数关系，实现自监督优化。


<details>
  <summary>Details</summary>
Motivation: 解决本征图像分解缺乏真实世界ground-truth数据的长期挑战，现有方法依赖合成数据或有限标注，难以扩展到户外场景。

Method: 利用热成像相机检测被吸收光产生的热量，建立可见光与热成像强度间的序数关系，推导出反射率和阴影的序数约束，通过自监督方式优化神经网络。

Result: 在自然光和人工光照下进行定量评估，在多种户外场景进行定性实验，结果显示性能优于现有基于学习的方法。

Conclusion: 该方法为获取真实世界序数监督提供了可扩展的途径，此前通过人工标注是无法实现的，展现了在户外场景本征图像分解中的优越性能。

Abstract: Decomposing an image into its intrinsic photometric factors--shading and
reflectance--is a long-standing challenge due to the lack of extensive
ground-truth data for real-world scenes. Recent methods rely on synthetic data
or sparse annotations for limited indoor and even fewer outdoor scenes. We
introduce a novel training-free approach for intrinsic image decomposition
using only a pair of visible and thermal images. We leverage the principle that
light not reflected from an opaque surface is absorbed and detected as heat by
a thermal camera. This allows us to relate the ordinalities between visible and
thermal image intensities to the ordinalities of shading and reflectance, which
can densely self-supervise an optimizing neural network to recover shading and
reflectance. We perform quantitative evaluations with known reflectance and
shading under natural and artificial lighting, and qualitative experiments
across diverse outdoor scenes. The results demonstrate superior performance
over recent learning-based models and point toward a scalable path to curating
real-world ordinal supervision, previously infeasible via manual labeling.

</details>


### [61] [Compressed Video Quality Enhancement: Classifying and Benchmarking over Standards](https://arxiv.org/abs/2509.10407)
*Xiem HoangVan,Dang BuiDinh,Sang NguyenQuang,Wen-Hsiao Peng*

Main category: cs.CV

TL;DR: 本文提出了压缩视频质量增强(CVQE)领域的系统性综述，建立了新的分类法、统一基准框架，并分析了性能与复杂度的权衡关系。


<details>
  <summary>Details</summary>
Motivation: 现有CVQE综述存在分类不系统、缺乏跨编码标准的架构比较分析、基准测试实践不完善等问题，需要建立更全面的评估体系。

Method: 提出了三方面贡献：1）基于架构范式、编码标准和压缩域特征利用的新分类法；2）集成现代压缩协议和标准测试序列的统一基准框架；3）系统分析重建性能与计算复杂度的权衡关系。

Result: 建立了CVQE领域的系统性评估基础，为研究人员和从业者提供了一致的评估标准和模型选择指导。

Conclusion: 该综述为CVQE研究建立了系统性的分类和评估框架，识别了性能与复杂度的关键权衡，并指出了未来研究的 promising 方向。

Abstract: Compressed video quality enhancement (CVQE) is crucial for improving user
experience with lossy video codecs like H.264/AVC, H.265/HEVC, and H.266/VVC.
While deep learning based CVQE has driven significant progress, existing
surveys still suffer from limitations: lack of systematic classification
linking methods to specific standards and artifacts, insufficient comparative
analysis of architectural paradigms across coding types, and underdeveloped
benchmarking practices. To address these gaps, this paper presents three key
contributions. First, it introduces a novel taxonomy classifying CVQE methods
across architectural paradigms, coding standards, and compressed-domain feature
utilization. Second, it proposes a unified benchmarking framework integrating
modern compression protocols and standard test sequences for fair
multi-criteria evaluation. Third, it provides a systematic analysis of the
critical trade-offs between reconstruction performance and computational
complexity observed in state-of-the-art methods and highlighting promising
directions for future research. This comprehensive review aims to establish a
foundation for consistent assessment and informed model selection in CVQE
research and deployment.

</details>


### [62] [Multimodal SAM-adapter for Semantic Segmentation](https://arxiv.org/abs/2509.10408)
*Iacopo Curti,Pierluigi Zama Ramirez,Alioscia Petrelli,Luigi Di Stefano*

Main category: cs.CV

TL;DR: MM SAM-adapter是一个新颖的多模态语义分割框架，通过适配器网络将融合的多模态特征注入SAM的RGB特征中，在保持RGB特征强泛化能力的同时选择性利用辅助模态信息，在多个挑战性基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割方法在恶劣条件（如光照不足、遮挡、恶劣天气）下表现不佳，需要多模态方法整合辅助传感器数据（如LiDAR、红外）来提供互补信息增强鲁棒性。

Method: 提出MM SAM-adapter框架，使用适配器网络将融合的多模态特征注入Segment Anything Model (SAM)的RGB特征中，实现选择性利用辅助模态信息。

Result: 在DeLiVER、FMB和MUSES三个挑战性基准测试中取得state-of-the-art性能，在RGB-easy和RGB-hard子集上都优于竞争方法。

Conclusion: 多模态适配能有效提升场景理解的鲁棒性，MM SAM-adapter实现了多模态信息的平衡高效利用，在有利和不利条件下都表现出色。

Abstract: Semantic segmentation, a key task in computer vision with broad applications
in autonomous driving, medical imaging, and robotics, has advanced
substantially with deep learning. Nevertheless, current approaches remain
vulnerable to challenging conditions such as poor lighting, occlusions, and
adverse weather. To address these limitations, multimodal methods that
integrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,
providing complementary information that enhances robustness. In this work, we
present MM SAM-adapter, a novel framework that extends the capabilities of the
Segment Anything Model (SAM) for multimodal semantic segmentation. The proposed
method employs an adapter network that injects fused multimodal features into
SAM's rich RGB features. This design enables the model to retain the strong
generalization ability of RGB features while selectively incorporating
auxiliary modalities only when they contribute additional cues. As a result, MM
SAM-adapter achieves a balanced and efficient use of multimodal information. We
evaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,
where MM SAM-adapter delivers state-of-the-art performance. To further analyze
modality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hard
subsets. Results consistently demonstrate that our framework outperforms
competing methods in both favorable and adverse conditions, highlighting the
effectiveness of multimodal adaptation for robust scene understanding. The code
is available at the following link:
https://github.com/iacopo97/Multimodal-SAM-Adapter.

</details>


### [63] [InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis](https://arxiv.org/abs/2509.10441)
*Tao Han,Wanghan Xu,Junchao Gong,Xiaoyu Yue,Song Guo,Luping Zhou,Lei Bai*

Main category: cs.CV

TL;DR: InfGen是一种基于潜在扩散模型的第二代方法，用一步生成器替代VAE解码器，实现任意分辨率图像生成，将4K图像生成时间从100+秒缩短到10秒以内


<details>
  <summary>Details</summary>
Motivation: 解决当前扩散模型分辨率增加时计算需求二次增长的问题，为生产者和消费者提供跨设备一致的视觉体验

Method: 将扩散模型生成的固定潜在表示作为内容表征，使用紧凑生成潜在和一步生成器解码任意分辨率图像，替代传统VAE解码器

Result: 能够将多种模型升级到任意高分辨率时代，显著降低计算复杂度，4K图像生成时间减少到10秒以下

Conclusion: InfGen简化了任意分辨率图像生成过程，无需重新训练扩散模型，可应用于使用相同潜在空间的任何模型

Abstract: Arbitrary resolution image generation provides a consistent visual experience
across devices, having extensive applications for producers and consumers.
Current diffusion models increase computational demand quadratically with
resolution, causing 4K image generation delays over 100 seconds. To solve this,
we explore the second generation upon the latent diffusion models, where the
fixed latent generated by diffusion models is regarded as the content
representation and we propose to decode arbitrary resolution images with a
compact generated latent using a one-step generator. Thus, we present the
\textbf{InfGen}, replacing the VAE decoder with the new generator, for
generating images at any resolution from a fixed-size latent without retraining
the diffusion models, which simplifies the process, reducing computational
complexity and can be applied to any model using the same latent space.
Experiments show InfGen is capable of improving many models into the arbitrary
high-resolution era while cutting 4K image generation time to under 10 seconds.

</details>


### [64] [SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets](https://arxiv.org/abs/2509.10453)
*Emily Kaczmarek,Justin Szeto,Brennan Nichyporuk,Tal Arbel*

Main category: cs.CV

TL;DR: 本研究将三种最先进的时序自监督学习方法应用于3D脑MRI分析，通过处理变长输入和鲁棒空间特征学习，在阿尔茨海默病预测任务中超越了监督学习性能。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病预测中深度学习模型面临标注数据稀缺、跨数据集泛化能力差以及对不同扫描数量和时距的适应性不足等问题。

Method: 采用时序自监督学习（SSL）方法，包括时序顺序预测和对比学习，添加了处理变长输入和学习鲁棒空间特征的扩展模块，在四个公开数据集（3,161名患者）上进行预训练。

Result: 在七个下游任务中的六个任务上，自监督学习方法均优于监督学习，表现出跨任务和不同输入图像数量及时间间隔的适应性和泛化能力。

Conclusion: 时序自监督学习方法在阿尔茨海默病预测中表现出色，具有强大的临床适用性和鲁棒性能，代码和模型已公开。

Abstract: Alzheimer's disease is a progressive, neurodegenerative disorder that causes
memory loss and cognitive decline. While there has been extensive research in
applying deep learning models to Alzheimer's prediction tasks, these models
remain limited by lack of available labeled data, poor generalization across
datasets, and inflexibility to varying numbers of input scans and time
intervals between scans. In this study, we adapt three state-of-the-art
temporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,
and add novel extensions designed to handle variable-length inputs and learn
robust spatial features. We aggregate four publicly available datasets
comprising 3,161 patients for pre-training, and show the performance of our
model across multiple Alzheimer's prediction tasks including diagnosis
classification, conversion detection, and future conversion prediction.
Importantly, our SSL model implemented with temporal order prediction and
contrastive learning outperforms supervised learning on six out of seven
downstream tasks. It demonstrates adaptability and generalizability across
tasks and number of input images with varying time intervals, highlighting its
capacity for robust performance across clinical applications. We release our
code and model publicly at https://github.com/emilykaczmarek/SSL-AD.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [65] [Automated Tuning for Diffusion Inverse Problem Solvers without Generative Prior Retraining](https://arxiv.org/abs/2509.09880)
*Yaşar Utku Alçalar,Junno Yun,Mehmet Akçakaya*

Main category: eess.IV

TL;DR: 提出了ZADS方法，通过自适应调整保真度权重来优化基于扩散模型的MRI重建，无需重新训练扩散先验


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在加速MRI重建中性能严重依赖精心调整的保真度权重，特别是在快速采样计划下，现有启发式或固定权重方法无法适应不同的测量条件和时间步计划

Method: ZADS是一种测试时优化方法，将去噪过程视为固定展开采样器，仅使用欠采样测量以自监督方式优化保真度权重，可适应任意噪声计划

Result: 在fastMRI膝盖数据集上的实验表明，ZADS始终优于传统压缩感知和最近的基于扩散的方法，能够跨不同噪声计划和采集设置提供高保真重建

Conclusion: ZADS通过自适应权重调整有效解决了扩散模型在逆问题中的权重依赖问题，为加速MRI重建提供了更鲁棒的解决方案

Abstract: Diffusion/score-based models have recently emerged as powerful generative
priors for solving inverse problems, including accelerated MRI reconstruction.
While their flexibility allows decoupling the measurement model from the
learned prior, their performance heavily depends on carefully tuned data
fidelity weights, especially under fast sampling schedules with few denoising
steps. Existing approaches often rely on heuristics or fixed weights, which
fail to generalize across varying measurement conditions and irregular timestep
schedules. In this work, we propose Zero-shot Adaptive Diffusion Sampling
(ZADS), a test-time optimization method that adaptively tunes fidelity weights
across arbitrary noise schedules without requiring retraining of the diffusion
prior. ZADS treats the denoising process as a fixed unrolled sampler and
optimizes fidelity weights in a self-supervised manner using only undersampled
measurements. Experiments on the fastMRI knee dataset demonstrate that ZADS
consistently outperforms both traditional compressed sensing and recent
diffusion-based methods, showcasing its ability to deliver high-fidelity
reconstructions across varying noise schedules and acquisition settings.

</details>


### [66] [Drone-Based Multispectral Imaging and Deep Learning for Timely Detection of Branched Broomrape in Tomato Farms](https://arxiv.org/abs/2509.09972)
*Mohammadreza Narimani,Alireza Pourreza,Ali Moghimi,Mohsen Mesgaran,Parastoo Farajpoor,Hamid Jafarbiglu*

Main category: eess.IV

TL;DR: 本研究结合无人机多光谱影像和LSTM深度学习网络，使用SMOTE技术处理类别不平衡，实现了对番茄寄生植物分枝列当的早期检测，最高准确率达到88.37%，召回率达到95.37%。


<details>
  <summary>Details</summary>
Motivation: 分枝列当对加州番茄产业构成严重威胁，其地下生命周期使得早期检测困难，传统化学防治方法成本高、环境有害且效果有限。

Method: 在已知感染列当的番茄农场进行实验，使用基于生长度日数(GDD)确定五个关键生长阶段，通过无人机获取多光谱图像并提取番茄冠层反射率数据，结合LSTM网络和SMOTE过采样技术进行时序分析。

Result: 在897 GDD时检测准确率达79.09%，召回率70.36%；整合所有生长阶段并使用SMOTE增强后，准确率提升至88.37%，召回率达到95.37%。

Conclusion: 时序多光谱分析和LSTM网络在早期列当检测方面具有强大潜力，无人机多光谱传感与深度学习结合可为精准农业提供有力工具，减少损失并提高番茄生产的可持续性。

Abstract: This study addresses the escalating threat of branched broomrape (Phelipanche
ramosa) to California's tomato industry, which supplies over 90 percent of U.S.
processing tomatoes. The parasite's largely underground life cycle makes early
detection difficult, while conventional chemical controls are costly,
environmentally harmful, and often ineffective. To address this, we combined
drone-based multispectral imagery with Long Short-Term Memory (LSTM) deep
learning networks, using the Synthetic Minority Over-sampling Technique (SMOTE)
to handle class imbalance. Research was conducted on a known broomrape-infested
tomato farm in Woodland, Yolo County, CA, across five key growth stages
determined by growing degree days (GDD). Multispectral images were processed to
isolate tomato canopy reflectance. At 897 GDD, broomrape could be detected with
79.09 percent overall accuracy and 70.36 percent recall without integrating
later stages. Incorporating sequential growth stages with LSTM improved
detection substantially. The best-performing scenario, which integrated all
growth stages with SMOTE augmentation, achieved 88.37 percent overall accuracy
and 95.37 percent recall. These results demonstrate the strong potential of
temporal multispectral analysis and LSTM networks for early broomrape
detection. While further real-world data collection is needed for practical
deployment, this study shows that UAV-based multispectral sensing coupled with
deep learning could provide a powerful precision agriculture tool to reduce
losses and improve sustainability in tomato production.

</details>


### [67] [Polarization Denoising and Demosaicking: Dataset and Baseline Method](https://arxiv.org/abs/2509.10098)
*Muhamad Daniel Ariff Bin Abdul Rahman,Yusuke Monno,Masayuki Tanaka,Masatoshi Okutomi*

Main category: eess.IV

TL;DR: 提出了用于偏振去噪和去马赛克的新数据集和方法，包含40个真实场景和三种噪声条件，采用先去噪后去马赛克的信号处理方法。


<details>
  <summary>Details</summary>
Motivation: 现有的偏振去马赛克研究主要集中在无噪声情况，由于缺乏合适的评估数据集和可靠的基线方法，联合偏振去噪和去马赛克的研究很少。

Method: 采用先去噪后去马赛克的方法，基于成熟的信号处理组件构建可复现的方法，并使用包含噪声马赛克输入和无噪声完整图像对的新数据集。

Result: 实验结果表明，该方法相比其他替代方法具有更高的图像重建性能，提供了一个可靠的基线。

Conclusion: 该研究填补了偏振去噪和去马赛克联合任务的空白，为后续研究提供了高质量的数据集和有效的基线方法。

Abstract: A division-of-focal-plane (DoFP) polarimeter enables us to acquire images
with multiple polarization orientations in one shot and thus it is valuable for
many applications using polarimetric information. The image processing pipeline
for a DoFP polarimeter entails two crucial tasks: denoising and demosaicking.
While polarization demosaicking for a noise-free case has increasingly been
studied, the research for the joint task of polarization denoising and
demosaicking is scarce due to the lack of a suitable evaluation dataset and a
solid baseline method. In this paper, we propose a novel dataset and method for
polarization denoising and demosaicking. Our dataset contains 40 real-world
scenes and three noise-level conditions, consisting of pairs of noisy mosaic
inputs and noise-free full images. Our method takes a
denoising-then-demosaicking approach based on well-accepted signal processing
components to offer a reproducible method. Experimental results demonstrate
that our method exhibits higher image reconstruction performance than other
alternative methods, offering a solid baseline.

</details>


### [68] [Multi-pathology Chest X-ray Classification with Rejection Mechanisms](https://arxiv.org/abs/2509.10348)
*Yehudit Aperstein,Amit Tzahar,Alon Gottlib,Tal Verber,Ravit Shagan Damti,Alexander Apartsin*

Main category: eess.IV

TL;DR: 基于DenseNet-121的胸片诊断不确定性感知框架，通过熵拒绝和置信区间拒绝机制提高多标签分类的可靠性


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医疗影像任务中过度自信存在风险，特别是在胸片多病理检测中，需要提高预测不确定性识别能力

Method: 使用DenseNet-121骨干网络，结合熵拒绝和置信区间拒绝两种选择性预测机制，采用分位数校准程序调整拒绝阈值

Result: 在三个大型公共数据集上实验表明，选择性拒绝改善了诊断准确性和覆盖范围的权衡，熵拒绝方法在所有病理上获得最高平均AUC

Conclusion: 选择性预测机制可集成到AI辅助诊断工作流中，为临床环境中深度学习的安全部署提供了实用步骤

Abstract: Overconfidence in deep learning models poses a significant risk in
high-stakes medical imaging tasks, particularly in multi-label classification
of chest X-rays, where multiple co-occurring pathologies must be detected
simultaneously. This study introduces an uncertainty-aware framework for chest
X-ray diagnosis based on a DenseNet-121 backbone, enhanced with two selective
prediction mechanisms: entropy-based rejection and confidence interval-based
rejection. Both methods enable the model to abstain from uncertain predictions,
improving reliability by deferring ambiguous cases to clinical experts. A
quantile-based calibration procedure is employed to tune rejection thresholds
using either global or class-specific strategies. Experiments conducted on
three large public datasets (PadChest, NIH ChestX-ray14, and MIMIC-CXR)
demonstrate that selective rejection improves the trade-off between diagnostic
accuracy and coverage, with entropy-based rejection yielding the highest
average AUC across all pathologies. These results support the integration of
selective prediction into AI-assisted diagnostic workflows, providing a
practical step toward safer, uncertainty-aware deployment of deep learning in
clinical settings.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [69] [Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images](https://arxiv.org/abs/2509.09952)
*Zhi Ying,Boxiang Rong,Jingyu Wang,Maoyuan Xu*

Main category: cs.GR

TL;DR: 提出了一种新颖的两阶段生成-估计框架，用于高质量PBR材质生成，通过扩散模型合成可平铺纹理，再通过链式分解方案预测SVBRDF通道，实现了高效、高质量且用户可控的材质生成。


<details>
  <summary>Details</summary>
Motivation: 传统材质创建和重建需要艺术家大量时间和专业知识，现有基于视觉基础模型的方法在质量、灵活性和用户控制方面存在不足。

Method: 两阶段框架：1) 生成阶段使用微调扩散模型合成与用户输入对齐的着色可平铺纹理图像；2) 估计阶段采用链式分解方案，通过单步图像条件扩散模型顺序预测SVBRDF通道。

Result: 在材质生成和估计方面表现出优越性能，对生成纹理和真实照片都具有强鲁棒性，支持文本到材质、图像到材质、结构引导生成和材质编辑等多种应用。

Conclusion: 该方法提供了高效、高质量且灵活的PBR材质生成解决方案，在多个应用场景中展现出卓越的性能和实用性。

Abstract: Material creation and reconstruction are crucial for appearance modeling but
traditionally require significant time and expertise from artists. While recent
methods leverage visual foundation models to synthesize PBR materials from
user-provided inputs, they often fall short in quality, flexibility, and user
control. We propose a novel two-stage generate-and-estimate framework for PBR
material generation. In the generation stage, a fine-tuned diffusion model
synthesizes shaded, tileable texture images aligned with user input. In the
estimation stage, we introduce a chained decomposition scheme that sequentially
predicts SVBRDF channels by passing previously extracted representation as
input into a single-step image-conditional diffusion model. Our method is
efficient, high quality, and enables flexible user control. We evaluate our
approach against existing material generation and estimation methods,
demonstrating superior performance. Our material estimation method shows strong
robustness on both generated textures and in-the-wild photographs. Furthermore,
we highlight the flexibility of our framework across diverse applications,
including text-to-material, image-to-material, structure-guided generation, and
material editing.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [70] [HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario](https://arxiv.org/abs/2509.10096)
*Saeed Saadatnejad,Reyhaneh Hosseininejad,Jose Barreiros,Katherine M. Tsui,Alexandre Alahi*

Main category: cs.RO

TL;DR: 提出了HHI-Assist数据集和基于Transformer的扩散模型，用于预测人机交互中的运动姿态，提升辅助机器人的响应能力


<details>
  <summary>Details</summary>
Motivation: 劳动力短缺和人口老龄化需要辅助机器人，但准确的运动预测在物理交互场景中仍具挑战性

Method: 收集人-人交互运动捕捉数据集，开发条件Transformer去噪扩散模型来预测交互代理的姿态

Result: 模型有效捕捉了护理者和被护理者之间的耦合动态，相比基线有改进，对未见场景有强泛化能力

Conclusion: 通过推进交互感知的运动预测和引入新数据集，显著增强了机器人辅助策略的潜力

Abstract: The increasing labor shortage and aging population underline the need for
assistive robots to support human care recipients. To enable safe and
responsive assistance, robots require accurate human motion prediction in
physical interaction scenarios. However, this remains a challenging task due to
the variability of assistive settings and the complexity of coupled dynamics in
physical interactions. In this work, we address these challenges through two
key contributions: (1) HHI-Assist, a dataset comprising motion capture clips of
human-human interactions in assistive tasks; and (2) a conditional
Transformer-based denoising diffusion model for predicting the poses of
interacting agents. Our model effectively captures the coupled dynamics between
caregivers and care receivers, demonstrating improvements over baselines and
strong generalization to unseen scenarios. By advancing interaction-aware
motion prediction and introducing a new dataset, our work has the potential to
significantly enhance robotic assistance policies. The dataset and code are
available at: https://sites.google.com/view/hhi-assist/home

</details>


### [71] [GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation](https://arxiv.org/abs/2509.10454)
*Hang Yin,Haoyu Wei,Xiuwei Xu,Wenxuan Guo,Jie Zhou,Jiwen Lu*

Main category: cs.RO

TL;DR: 提出无需训练的视觉语言导航框架，通过将指令分解为空间约束并构建图优化问题，实现连续环境中的零样本导航


<details>
  <summary>Details</summary>
Motivation: 现有零样本VLN方法主要针对离散环境或需要在连续模拟器中进行无监督训练，难以泛化到真实世界场景，需要开发无需训练即可在连续环境中工作的框架

Method: 将导航指导构建为图约束优化问题，分解指令为显式空间约束，构建空间约束库，通过约束求解器确定路径点位置，处理无解或多解情况时使用导航树和回溯机制

Result: 在标准基准测试中相比最先进的零样本VLN方法显著提升了成功率和导航效率，真实世界实验显示能有效泛化到新环境和指令集

Conclusion: 该框架为构建更鲁棒和自主的导航系统铺平了道路，实现了无需训练即可在连续环境中进行零样本视觉语言导航

Abstract: In this paper, we propose a training-free framework for vision-and-language
navigation (VLN). Existing zero-shot VLN methods are mainly designed for
discrete environments or involve unsupervised training in continuous simulator
environments, which makes it challenging to generalize and deploy them in
real-world scenarios. To achieve a training-free framework in continuous
environments, our framework formulates navigation guidance as graph constraint
optimization by decomposing instructions into explicit spatial constraints. The
constraint-driven paradigm decodes spatial semantics through constraint
solving, enabling zero-shot adaptation to unseen environments. Specifically, we
construct a spatial constraint library covering all types of spatial
relationship mentioned in VLN instructions. The human instruction is decomposed
into a directed acyclic graph, with waypoint nodes, object nodes and edges,
which are used as queries to retrieve the library to build the graph
constraints. The graph constraint optimization is solved by the constraint
solver to determine the positions of waypoints, obtaining the robot's
navigation path and final goal. To handle cases of no solution or multiple
solutions, we construct a navigation tree and the backtracking mechanism.
Extensive experiments on standard benchmarks demonstrate significant
improvements in success rate and navigation efficiency compared to
state-of-the-art zero-shot VLN methods. We further conduct real-world
experiments to show that our framework can effectively generalize to new
environments and instruction sets, paving the way for a more robust and
autonomous navigation framework.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [72] [LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios](https://arxiv.org/abs/2509.09926)
*Jiahao Chen,Zhiyuan Huang,Yurou Liu,Bing Su*

Main category: cs.LG

TL;DR: 该论文提出了LoFT框架，通过参数高效微调基础模型来解决长尾半监督学习问题，并在开放世界场景下扩展为LoFT-OW，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有长尾半监督学习方法通常从零开始训练模型，容易产生过自信和低质量伪标签的问题。基础模型的微调范式可以生成更可靠的伪标签，但尚未在长尾半监督学习中得到充分探索。

Method: 提出LoFT框架，通过参数高效微调基础模型来处理长尾半监督学习问题。进一步扩展到开放世界场景（LoFT-OW），处理未标记数据中可能包含的分布外样本。

Result: 在多个基准测试中，该方法相比先前方法取得了优越性能，即使仅使用先前方法1%的未标记数据也能实现良好效果。

Conclusion: 基于基础模型微调的长尾半监督学习方法能够生成更可靠的伪标签，有效提升不平衡学习性能，特别是在开放世界场景下表现出色。

Abstract: Long-tailed learning has garnered increasing attention due to its wide
applicability in real-world scenarios. Among existing approaches, Long-Tailed
Semi-Supervised Learning (LTSSL) has emerged as an effective solution by
incorporating a large amount of unlabeled data into the imbalanced labeled
dataset. However, most prior LTSSL methods are designed to train models from
scratch, which often leads to issues such as overconfidence and low-quality
pseudo-labels. To address these challenges, we extend LTSSL into the foundation
model fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed
semi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate
that fine-tuned foundation models can generate more reliable pseudolabels,
thereby benefiting imbalanced learning. Furthermore, we explore a more
practical setting by investigating semi-supervised learning under open-world
conditions, where the unlabeled data may include out-of-distribution (OOD)
samples. To handle this problem, we propose LoFT-OW (LoFT under Open-World
scenarios) to improve the discriminative ability. Experimental results on
multiple benchmarks demonstrate that our method achieves superior performance
compared to previous approaches, even when utilizing only 1\% of the unlabeled
data compared with previous works.

</details>


### [73] [Adaptive Token Merging for Efficient Transformer Semantic Communication at the Edge](https://arxiv.org/abs/2509.09955)
*Omar Erak,Omar Alhussein,Hatem Abou-Zeid,Mehdi Bennis,Sami Muhaidat*

Main category: cs.LG

TL;DR: 提出一种无需训练的自适应token合并框架，通过动态合并语义冗余的token来压缩transformer表示，在保持精度的同时显著降低计算和通信成本


<details>
  <summary>Details</summary>
Motivation: 大规模transformer模型在资源受限的边缘设备上部署面临高计算和通信成本问题，需要一种无需重新训练的高效压缩方法

Method: 基于每层相似度阈值选择性合并语义冗余token，将合并策略发现建模为多目标优化问题，使用贝叶斯优化获得精度、推理成本和通信成本之间的帕累托最优权衡

Result: 在ImageNet分类上以30%更少的FLOPs和低于20%的原始通信成本达到未修改transformer的精度；在视觉问答任务中以不到三分之一计算量和十分之一带宽实现与完整LLaVA模型相当的性能

Conclusion: 该框架为资源受限的边缘智能场景提供了实用且通用的transformer模型部署解决方案，具有跨信道条件的鲁棒性和固有的隐私保护优势

Abstract: Large-scale transformers are central to modern semantic communication, yet
their high computational and communication costs hinder deployment on
resource-constrained edge devices. This paper introduces a training-free
framework for adaptive token merging, a novel mechanism that compresses
transformer representations at runtime by selectively merging semantically
redundant tokens under per-layer similarity thresholds. Unlike prior
fixed-ratio reduction, our approach couples merging directly to input
redundancy, enabling data-dependent adaptation that balances efficiency and
task relevance without retraining. We cast the discovery of merging strategies
as a multi-objective optimization problem and leverage Bayesian optimization to
obtain Pareto-optimal trade-offs between accuracy, inference cost, and
communication cost. On ImageNet classification, we match the accuracy of the
unmodified transformer with 30\% fewer floating-point operations per second and
under 20\% of the original communication cost, while for visual question
answering our method achieves performance competitive with the full LLaVA model
at less than one-third of the compute and one-tenth of the bandwidth. Finally,
we show that our adaptive merging is robust across varying channel conditions
and provides inherent privacy benefits, substantially degrading the efficacy of
model inversion attacks. Our framework provides a practical and versatile
solution for deploying powerful transformer models in resource-limited edge
intelligence scenarios.

</details>
