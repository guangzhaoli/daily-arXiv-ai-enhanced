<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 4]
- [physics.optics](#physics.optics) [Total: 1]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个基于视频一致性模型（VCMs）的零样本/即插即用高分辨率视频恢复求解器，通过绕过自动微分需求，在保证测量一致性和时间平滑性的同时，仅需少量神经网络评估即可实现最先进的视频重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像扩散模型的零样本逆求解器在图像恢复任务中表现出色，但直接应用于视频恢复时会出现时间不一致性问题。需要开发能够同时恢复空间细节和捕捉时间依赖关系的视频恢复方法。

Method: 利用视频一致性模型（VCMs）作为先验，提出LVTINO框架，通过条件机制绕过自动微分需求，仅需少量神经网络评估即可实现高质量视频重建。

Result: 在多种视频逆问题上的实验表明，LVTINO相比基于图像LDM的逐帧方法在感知质量和重建保真度上有显著提升，同时在计算效率方面也建立了新的基准。

Conclusion: LVTINO成功解决了高分辨率视频恢复中的时间一致性问题，为视频逆问题求解提供了新的有效解决方案，在重建质量和计算效率方面均优于现有方法。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出一种基于风格提取的三阶段训练图像生成方法，通过风格编码器和投影层实现细粒度文本引导的风格化图像生成


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中细粒度风格难以用自然语言精确描述和控制的问题，以及风格参考图像的引导信息难以与传统文本条件对齐的挑战

Method: 使用风格编码器和风格投影层将风格表示与文本表示对齐，采用三阶段训练方法，构建包含图像-风格标签-文本描述三元组的Style30k-captions数据集

Result: 实现了在不改变下游生成模型结构框架的情况下，从单张风格参考图像中提取细粒度风格表示并注入生成主体

Conclusion: 该方法能够最大化预训练生成模型的生成能力，实现细粒度可控的风格化图像生成

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文提出了一个用于技能学习中挣扎行为检测的数据集EvoStruggle，包含61.68小时视频记录和5,385个标注的挣扎时段，通过时序动作定位模型验证了挣扎检测在跨任务和跨活动中的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集没有关注技能学习过程中挣扎行为如何随时间演变，而理解这种演变对于确定用户当前学习阶段和开发有效辅助系统至关重要。

Method: 收集了76名参与者完成18个任务的视频数据，任务分为四类活动（打结、折纸、七巧板、洗牌），每个任务重复五次以捕捉技能演变。将挣扎检测定义为时序动作定位问题，使用TAL模型进行检测。

Result: 模型在跨任务泛化时达到34.56%的平均mAP，跨活动泛化时达到19.24%的mAP，表明挣扎是跨技能任务的可迁移概念，但检测性能仍有提升空间。

Conclusion: 挣扎检测在技能学习中具有可迁移性，EvoStruggle数据集为研究技能获取过程中的挣扎演变提供了重要资源，未来需要在挣扎检测性能上进一步改进。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一种基于轻量级残差U-Net架构的紧凑高效基础模型，用于统一求解各类偏微分方程，相比现有基于复杂Transformer架构的PDE基础模型具有更少的参数和计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的PDE基础模型存在计算复杂和参数过多的问题，需要开发更轻量高效的架构来统一求解多种PDE。

Method: 采用轻量级残差U-Net架构，结合自回归预训练策略模拟数值求解器行为，在多样化流体动力学PDE数据集上进行预训练。

Result: SPUS在6个未见下游PDE任务上达到最先进的泛化性能，同时仅需少量参数和微调数据。

Conclusion: SPUS展示了作为参数高效基础模型解决多样化PDE系统的潜力，为PDE求解提供了更实用的解决方案。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过多样性约束直接优化多人生成中的身份多样性，解决了文本到图像模型在多人提示中重复面部、合并身份和计数错误的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的文本到图像模型在生成多人图像时存在身份重复、面部融合和人数计数错误的问题，这限制了模型在多人场景中的应用。

Method: DisCo通过组相对策略优化（GRPO）微调流匹配模型，使用组合奖励函数：（i）惩罚图像内面部相似性，（ii）抑制跨样本身份重复，（iii）确保准确人数计数，（iv）通过人类偏好评分保持视觉保真度。采用单阶段课程学习稳定训练复杂度扩展。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的唯一面部准确率和接近完美的全局身份分布，超越了开源和专有方法（如Gemini、GPT-Image），同时保持了有竞争力的感知质量。

Conclusion: DisCo提供了一个可扩展、无需额外标注的解决方案，解决了生成模型中长期存在的身份危机问题，为组合式多人生成设定了新基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一种新的视觉地理定位方法，通过将查询图像的视觉表示与学习的地理表示对齐，并构建层次化地理嵌入模型，在多个基准数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉地理定位方法在学习地理表示方面仍有改进空间，需要更有效地将视觉内容与地理位置信息相结合。

Method: 提出层次化地理嵌入模型，将世界建模为地理嵌入的层次结构，并开发了将查询图像的外观特征与其语义分割图高效融合的方法。

Result: 在5个基准数据集的25个评估指标中，有22个指标超越了现有最先进方法和大型视觉语言模型，达到了新的最佳性能。

Conclusion: 研究表明，地理表示和视觉表示的有效结合是提升视觉地理定位性能的关键因素。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是一种针对大型视觉语言模型（LVLM）的数据高效指令调优方法，通过聚类注意力矩阵的轨迹来去除训练数据冗余，能够在保持性能的同时大幅减少数据量。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在LVLM上的表现甚至不如随机选择，需要一种能够有效去除LVLM训练数据冗余的解决方案。

Method: 基于跨模态注意力矩阵相似性的理论分析，提出XMAS方法：通过聚类注意力矩阵的顶部奇异值轨迹，从聚类中平衡采样来构建训练子集。

Result: 实验表明XMAS可以丢弃LLaVA-665k数据集的50%和Vision-Flan数据集的85%，同时完全保持LLaVA-1.5-7B在10个下游基准上的性能，并将训练速度提升1.2倍。

Conclusion: XMAS是首个在LVLM数据选择中超越随机选择的原理性方法，实现了显著的数据减少和训练加速。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种用于向量量化图像生成的变分流匹配方法，通过在连续嵌入空间中学习类别后验，结合了连续方法的几何感知和离散方法的监督优势。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化图像生成方法在连续传输动态和离散监督之间存在脱节，Purrception旨在弥合这一差距，提供明确的类别监督同时保持连续传输动态。

Method: 该方法将变分流匹配适配到向量量化潜在空间，通过学习码本索引的类别后验，同时在连续嵌入空间中计算速度场，结合了连续方法的几何感知和离散方法的监督优势。

Result: 在ImageNet-1k 256x256生成任务上，Purrception比连续流匹配和离散流匹配基线收敛更快，同时达到了与最先进模型竞争的FID分数。

Conclusion: 变分流匹配能够有效桥接连续传输和离散监督，提高图像生成的训练效率，为向量量化生成提供了新的解决方案。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT生成合成对比增强CT图像，同时进行主动脉腔和血栓分割，在图像合成和分割任务上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT使用的碘对比剂存在肾毒性、过敏反应和环境危害等风险，现有方法采用多阶段流程导致误差累积且无法充分利用共享的解剖结构信息。

Method: 集成条件扩散模型与多任务学习的统一框架，共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据，实现端到端的图像合成和分割联合优化。

Result: 在264名患者数据上评估，图像合成PSNR达25.61dB，优于单任务CDM的23.80dB；分割任务中腔体Dice分数提升至0.89，血栓Dice分数提升至0.53，临床测量误差显著降低。

Conclusion: 该方法通过统一的多任务框架有效解决了对比剂使用风险问题，在图像质量和分割精度方面均优于现有技术，为临床AAA评估提供了更安全有效的解决方案。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出一个用于多模态内容分析的高效原型框架，将视频转换为可查询的知识图谱表示


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析计算成本高且工程复杂，现有预训练模型难以与视频等复杂数据融合

Method: 设计候选流程配方，结合预训练模型将视频转换为时序半结构化数据，再转为帧级索引知识图谱

Result: 创建了可查询的知识图谱表示，支持持续学习和动态集成新领域知识

Conclusion: 该框架为多模态内容分析提供了高效的原型设计解决方案

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过将网站功能逆向工程为可重用的工具，使Web代理能够使用高级操作（如搜索、过滤、排序）而不是脆弱的逐步UI交互，从而提高了浏览器自动化的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前Web代理方法依赖逐步UI交互和大量LLM推理，在动态布局和长任务中容易失败。人类则利用网站提供的高级功能，因此需要一种更鲁棒的自动化方法。

Method: WALT框架逆向工程网站的潜在功能，将其转化为可调用的工具，包括发现、通信和内容管理操作。代理只需调用工具而不需要逐步推理UI交互。

Result: 在VisualWebArena和WebArena基准测试中，WALT实现了更高的成功率，使用更少的步骤和更少的LLM依赖推理。

Conclusion: WALT建立了一个鲁棒且可推广的浏览器自动化范式，将计算负担从脆弱的逐步推理转移到可靠的工具调用上。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一种用于半监督分割的框架，通过拓扑一致性机制来识别和保留有意义的语义结构，特别适用于密集分布的病理图像分析。


<details>
  <summary>Details</summary>
Motivation: 在半监督分割中，从未标记数据中捕获有意义的语义结构至关重要，尤其是在目标密集分布的病理图像分析中，这具有挑战性。

Method: 利用通过随机dropout和时间训练快照获得的多个扰动预测，强制这些不同输出之间的拓扑一致性。引入了一种新颖的匹配策略，结合空间重叠和全局结构对齐，以最小化预测之间的差异。

Result: 大量实验表明，该方法有效减少了拓扑错误，产生了更鲁棒和准确的分割结果，这对于可靠的下游分析至关重要。

Conclusion: 所提出的方法通过拓扑一致性机制和创新的匹配策略，在半监督分割中取得了显著效果，特别是在密集分布的病理图像分析中表现优异。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: 提出了Diffusion-LPO框架，将列表式偏好优化应用于扩散模型，通过Plackett-Luce模型扩展DPO目标，在图像生成、编辑和个性化偏好对齐任务中优于基于成对比较的DPO方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于成对偏好的DPO方法在扩散模型中应用有限，而人类对图像的偏好反馈通常包含隐含的排序信息，能比成对比较更精确地表达人类偏好。

Method: Diffusion-LPO框架：给定文本描述，将用户反馈聚合成图像排序列表，基于Plackett-Luce模型推导列表式DPO目标，鼓励每个样本优于其所有排名更低的替代图像。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等任务中，Diffusion-LPO在视觉质量和偏好对齐方面一致优于成对DPO基线方法。

Conclusion: 列表式偏好优化能更有效地利用人类反馈中的排序信息，为扩散模型与人类偏好对齐提供了更精确的优化框架。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯自回归的统一多模态大语言模型，通过Mixture-of-Transformers架构将预训练的视觉理解模型扩展为生成能力，在单一的下一个token预测框架内实现图像理解和生成。


<details>
  <summary>Details</summary>
Motivation: 构建统一的多模态大语言模型面临挑战：混合方法结合连续嵌入与扩散或基于流的目标，能产生高质量图像但破坏了自回归范式；纯自回归方法在离散视觉token上统一文本和图像预测，但常在语义对齐和像素级保真度之间权衡。

Method: 采用Mixture-of-Transformers架构，提出语义到像素的离散表示，将紧凑的语义token与细粒度像素token集成，在序列长度仅增加7.9%的情况下实现强语言对齐和精确的视觉细节描述。

Result: 在多个多模态基准测试中，Bridge在理解和生成基准上均取得竞争性或更优的结果，同时相比先前的统一MLLMs需要更少的训练数据和更短的训练时间。

Conclusion: Bridge成功构建了一个纯自回归的统一多模态大语言模型，在保持自回归范式的同时实现了高质量的图像理解和生成能力。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出结合CNN和贝叶斯深度学习的混合模型，用于小数据集的口腔癌分类，通过不确定性量化提高模型可靠性和泛化能力


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在口腔癌诊断中依赖大数据集，在医疗资源匮乏地区难以应用，且存在过度自信和可靠性不足的问题

Method: 使用卷积神经网络与贝叶斯深度学习相结合的混合模型，采用变分推断进行不确定性量化，在智能手机拍摄的彩色图像上进行训练

Result: 在相似分布测试集上达到94%准确率，在真实世界图像上达到88%准确率（传统CNN为72.94%），且能正确识别不确定性

Conclusion: 贝叶斯推断在小数据环境下能有效提升口腔癌早期诊断的模型可靠性和泛化能力

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 提出了一种名为CADTrans的源自由域自适应方法，通过构建辅助域和多重一致性策略来解决现有方法在处理硬样本和域偏差方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 源自由域自适应（SFDA）面临无法直接访问源域数据的挑战，现有方法主要关注在目标域中评估与源域相似的不变特征，但这些方法容易受到硬样本和域偏差的影响。

Method: CADTrans方法包含三个核心部分：1）辅助域模块从中间聚合的全局注意力中获得多样化表示；2）通过多重一致性策略获得不变特征表示来区分简单和硬样本；3）构建条件多核最大均值差异（CMK-MMD）策略来对齐硬样本到相应的简单样本。

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等多个基准数据集上进行了广泛实验，证明了所提方法实现了显著的性能提升。

Conclusion: CADTrans通过构建域一致性的不变特征表示，有效解决了SFDA中硬样本和域偏差的问题，在各种基准测试中表现出优越性能。

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 该研究开发了一个系统，利用BLV用户的历史视觉问题来指导多模态大语言模型生成更相关的图像描述，而不是默认的全面描述。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在视觉解释应用中往往提供冗长且不相关的描述，导致BLV用户需要筛选大量无关信息，效率低下。

Method: 系统通过识别图像中相似的视觉上下文（使用VizWiz-LF数据集），利用相关的历史问题来指导MLLM生成更符合BLV用户需求的描述。

Result: 评估显示，上下文感知的描述在76.1%的情况下能预测并回答用户问题，并在54.4%的比较中被优先选择。

Conclusion: 利用历史视觉问题指导MLLM可以生成更相关和高效的描述，提升BLV用户的体验。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 开发了ImageNet-Think数据集，基于ImageNet21k的25万张图像，包含结构化思维标记和答案，用于训练和评估多模态推理模型


<details>
  <summary>Details</summary>
Motivation: 帮助开发具有明确推理能力的视觉语言模型，促进对多模态推理机制的理解

Method: 使用GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506两个先进VLMs生成合成数据集，每张图像配有两对思维-答案序列

Result: 创建了包含逐步推理过程和最终描述性答案的多模态推理数据集

Conclusion: 该数据集将公开可用，支持推理/思维多模态VLM的研究发展

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络在感知矩阵的零空间低维投影中寻找解，而不是在图像域施加结构约束，提高了逆问题重建的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统逆问题求解方法通常忽略感知矩阵零空间的特定结构信息，导致重建效果受限。作者希望利用零空间的结构特性来设计更有效的正则化方法。

Method: NPN方法通过神经网络学习感知矩阵零空间的低维投影，将解约束在该投影空间中。该方法与现有重建框架兼容，可作为传统图像域先验的补充。

Result: 理论分析证明了收敛性和重建精度保证。实验结果表明，NPN在各种感知矩阵和逆问题中都能显著提升重建质量，包括压缩感知、去模糊、超分辨率、CT和MRI等。

Conclusion: NPN提供了一种新颖且有效的正则化策略，通过利用零空间结构信息，为成像逆问题提供了可解释且灵活的先验建模方法。

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种自动化基因组解释模块，将原始DNA序列转化为可解释的决策，适用于医疗自动化和机器人系统。该框架结合混沌游戏表示和概念瓶颈模型，通过生物学概念进行预测，并采用多种可靠性增强技术。


<details>
  <summary>Details</summary>
Motivation: 解决基因组医学中自动化决策系统的可靠性和可解释性问题，为机器人系统和临床自动化提供可靠基础。

Method: 结合混沌游戏表示和概念瓶颈模型，通过GC含量、CpG密度、k-mer基序等生物学概念进行预测，采用概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准等技术。

Result: 在HIV亚型分类任务上达到最先进性能，具有优异的概念预测保真度和更好的成本效益权衡。

Conclusion: 该工作为基因组医学中的机器人和临床自动化建立了可靠基础，填补了可解释基因组建模与自动化决策之间的空白。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1是一个推理增强的视觉-语言-动作模型，通过强化学习从可验证奖励和组相对策略优化来系统优化推理和执行能力，在泛化性和真实世界性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏明确的逐步推理，直接输出最终动作而不考虑可操作性约束和几何关系，且后训练流程很少强化推理质量。

Method: 提出VLA-R1模型，集成RLVR（从可验证奖励的强化学习）和GRPO（组相对策略优化），设计基于RLVR的后训练策略，创建VLA-CoT-13K高质量数据集提供思维链监督。

Result: 在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比现有VLA方法实现了更优的泛化性和真实世界性能。

Conclusion: VLA-R1通过系统优化推理和执行能力，解决了当前VLA模型的局限性，并在多个平台上验证了其优越性能。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种用于微距摄影的联合去模糊和3D重建方法，通过多视角模糊图像联合优化物体的清晰3D模型和每个像素的散焦模糊核


<details>
  <summary>Details</summary>
Motivation: 微距摄影中的散焦模糊问题严重阻碍了被拍摄物体的清晰成像和高质量3D重建，传统方法需要大量图像和标注，且目前没有针对微距摄影的多视角3D重建方法

Method: 采用可微分渲染方法自监督优化3D模型和散焦模糊核，从多视角模糊图像出发，联合优化物体的清晰3D模型和每个像素的散焦模糊核

Result: 实验表明，从少量多视角图像出发，该方法不仅能实现高质量图像去模糊，还能恢复高保真度的3D外观

Conclusion: 该联合优化框架有效解决了微距摄影中的散焦模糊问题，为小物体和细节物体的高质量3D重建提供了可行方案

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个新颖的单步扩散模型，通过重新定义运动去模糊为扩散过程，训练一致性模型实现高效高保真去模糊。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在图像去模糊任务中推理时间过长和保真度不足的问题。

Method: 将运动去模糊重新定义为扩散过程，每个时间步对应渐进模糊图像，训练一致性模型使所有时间步对齐到清晰图像；集成Kernel ControlNet进行模糊核估计，引入自适应时间步预测。

Result: 在全参考指标上取得优异性能，超越先前基于扩散的方法，与其他最先进模型性能相当。

Conclusion: FideDiff为预训练扩散模型在高保真图像恢复任务中的应用提供了新方向，为实际工业应用中的扩散模型发展建立了坚实基础。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个用于青铜器铭文识别的两阶段检测-识别流程，并开发了LadderMoE方法来处理多域变异性及长尾分布问题，在大型青铜器铭文数据集上取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文是早期中国文字的重要阶段，对考古和历史研究至关重要，但自动识别面临严重视觉退化、多域变异性（照片、拓片、描摹）和极端长尾字符分布的挑战。

Method: 构建了包含22454张全页图像和198598个标注字符的大规模青铜器铭文数据集；开发了两阶段检测-识别流程，先定位铭文再转录单个字符；提出了LadderMoE方法，在预训练的CLIP编码器基础上添加梯子式MoE适配器，实现动态专家专业化和更强的鲁棒性。

Result: 在单字符和全页识别任务上的综合实验表明，该方法显著优于最先进的场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上都实现了更高的准确率。

Conclusion: 这些结果为青铜器铭文识别和下游考古分析奠定了坚实基础。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程实现参数高效的领域自适应方法，只需训练少量参数即可达到与全参数微调相当的性能


<details>
  <summary>Details</summary>
Motivation: 现有UDA方法需要为每个源-目标对微调整个骨干网络，导致参数数量和存储需求线性增长，且无法重用训练好的骨干参数

Method: 在骨干网络前添加领域特定的视觉重编程层，生成视觉提示作为纹理偏置来调整输入图像的风格，使用多目标函数优化域内和域间分布差异

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，超越PDA基线1.6%准确率且参数减少54%

Conclusion: VirDA实现了参数高效的领域自适应，在保持高性能的同时大幅减少参数需求，为可重用骨干网络提供了可行方案

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 提出了离散面部编码（DFE），一种无监督的数据驱动方法，通过残差向量量化变分自编码器从3D网格序列学习紧凑且可解释的面部表情字典，替代传统FACS系统。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情编码系统（如FACS）存在覆盖范围有限和人工标注成本高的问题，需要一种更有效、可扩展的替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，通过RVQ-VAE编码为离散令牌序列，每个令牌捕获特定的可重用面部变形模式。

Result: DFE在压力检测、人格预测和抑郁检测三个心理学任务中，基于简单词袋模型的性能优于FACS和MAE等强基线模型。

Conclusion: DFE作为FACS的可扩展替代方案，具有更广泛的面部表情覆盖范围，在心理学和情感计算应用中具有重要潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM是一种针对保形变形的非刚性结构运动新方法，通过图优化框架进行点对点重建，准确计算局部保形尺度，并采用并行可分离迭代优化策略和自监督学习框架生成密集3D点云。


<details>
  <summary>Details</summary>
Motivation: 解决单目视觉可变形SLAM中的映射挑战，现有方法依赖严格假设（如局部平面表面或局部线性变形）且无法恢复保形尺度，需要消除这些约束并提高深度估计精度。

Method: 使用2D选择的图像扭曲进行点对点重建，通过图优化框架优化；采用并行可分离迭代优化策略解决问题的敏感性；结合自监督学习的编码器-解码器网络生成密集3D点云。

Result: 在合成和真实数据集上的仿真和实验结果表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM方法成功消除了现有方法的约束，能够准确计算局部保形尺度，实现更精确的深度估计，并在各种数据集上表现出优越性能。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 提出UniVerse框架，将鲁棒重建分解为恢复和重建两个子任务，利用视频扩散模型处理不一致的多视图图像，实现更好的3D场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖密集观测来优化模型参数，难以处理图像不一致问题。本文旨在解决从不一致多视图图像中重建3D场景的挑战。

Method: UniVerse框架首先将不一致图像转换为初始视频，然后使用专门设计的视频扩散模型将其恢复为一致图像，最后从恢复后的图像重建3D场景。

Result: 在合成和真实数据集上的实验表明，该方法在鲁棒重建方面具有强大的泛化能力和优越性能，还能控制重建3D场景的风格。

Conclusion: UniVerse通过解耦恢复和重建任务，利用扩散模型学习的大规模场景先验，有效处理了多样化的图像不一致问题，为鲁棒3D重建提供了新思路。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出轻量级端到端模板匹配框架，将匹配任务重构为联合定位和几何回归，解决传统方法效率低和深度学习缺乏几何建模的问题。


<details>
  <summary>Details</summary>
Motivation: 工业检测和组件对齐任务中，传统模板匹配方法枚举角度和尺度效率低，而深度学习模型缺乏对几何姿态的显式建模，难以实际部署。

Method: 使用模板感知动态卷积模块动态注入模板特征，结合深度可分离卷积和像素洗牌实现高效匹配，采用旋转-剪切增强策略进行无几何标注训练，并加入轻量级优化模块提升精度。

Result: 3.07M参数模型在复合变换下达到高精度和14ms推理速度，在小模板和多目标场景中表现出强鲁棒性。

Conclusion: 该方法适用于实时工业应用部署，在精度和效率方面均优于现有方法。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了首个自适应像素推理框架，通过动态确定像素级操作需求，解决视觉语言模型在细粒度视觉理解中的效率和精度问题。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理需要精确理解细粒度视觉元素的任务时表现不佳，主要由于图像编码过程中的信息丢失或对关键区域关注不足。现有方法引入像素级信息但容易过度使用，导致效率低下和无关细节干扰。

Method: 首先应用操作感知的监督微调建立文本推理和视觉操作的基础能力，然后设计基于模型自身响应反馈的rollout引导强化学习框架，使VLM能根据查询难度动态决定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，模型在HR-Bench 4K上达到73.4%的准确率，同时工具使用率仅为20.1%，相比之前方法在提高准确率的同时减少了66.5%的工具使用。

Conclusion: 该自适应像素推理框架能够有效平衡视觉细节获取和计算效率，显著提升VLM在细粒度视觉理解任务中的性能表现。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了一种基于增强敏感性的风险评分框架（ASRS），用于识别容易出错的胸部X光片病例，通过测量图像旋转后的嵌入变化来评估模型稳定性，提高医疗AI的公平性和安全性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸部X光片解读中表现良好，但在患者亚组间存在准确性不均的问题，现有错误检测方法难以处理分布内的细微错误，需要更有效的错误识别手段。

Method: ASRS框架应用临床合理的旋转（±15°/±30°），使用RAD-DINO编码器测量嵌入变化，通过敏感性评分将样本分为稳定性四分位数。

Result: 高敏感性病例的召回率显著降低（-0.2到-0.3），尽管AUROC和置信度较高，表明该方法能有效识别模型可能出错的病例。

Conclusion: ASRS提供了一种无需标签的选择性预测和临床医生审查方法，有助于改善医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过集成多个风格化参考到预训练图像到视频模型，实现高保真风格化和强时间一致性的视频风格化


<details>
  <summary>Details</summary>
Motivation: 传统帧级图像风格化方法会破坏时间一致性，而专门训练视频风格化模型需要配对视频数据且计算成本高，因此需要一种无需训练的高质量视频风格化解决方案

Method: 集成多个风格化参考到预训练I2V模型，使用高频补偿约束内容布局和运动，结合基于光流的运动线索在低显著性区域保留风格纹理

Result: FreeViS在风格化保真度和时间一致性方面优于现有基线方法，获得强人类偏好

Conclusion: 该训练免费流程为高质量、时间一致性的视频风格化提供了实用且经济的解决方案

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench是一个用于评估多模态大语言模型在医学图像质量评估中感知和推理能力的综合基准，包含2600个感知查询和708个推理评估，涵盖5种成像模态和40多个质量属性。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法受限于标量评分指标，无法反映专家评估中的人类推理过程，需要建立更符合人类推理范式的评估基准。

Method: 设计MedQ-Bench基准，包含MedQ-Perception（低层感知能力评估）和MedQ-Reasoning（无参考和比较推理任务），采用多维度评判协议评估模型输出，并与放射科医生进行人机对齐验证。

Result: 评估14个最先进MLLM发现模型具有初步但不稳定的感知和推理能力，准确率不足以可靠临床使用。

Conclusion: MLLM在医学图像质量评估中需要针对性优化，MedQ-Bench有望推动进一步探索并释放MLLM在医学图像质量评估中的潜力。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一个能够在单次前向传播中预测完整遮挡和深度排序的网络，仅需RGB图像输入，无需昂贵的标签或分割掩码。


<details>
  <summary>Details</summary>
Motivation: 现有视觉模型在理解实例级几何关系时依赖昂贵的输入格式（如类别标签、分割掩码）和二次方计算量的推理成本，限制了实际应用。

Method: InstaFormer通过对象查询与潜在掩码描述符之间的交互，语义化表示相同对象并携带互补信息，实现全遮挡和深度排序预测。

Result: 该方法在基准测试和消融实验中表现出有效性，代码和模型已开源。

Conclusion: InstaFormer提供了一种高效且无需昂贵输入的实例级几何关系理解解决方案，具有实际应用价值。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一种基于Transformer的神经风格迁移框架，通过金字塔位置编码和强化学习优化，实现了高效的高分辨率图像风格化处理。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时存在效率瓶颈，需要更高效的解决方案来实现实时高质量的艺术渲染。

Method: 提出了PyramidStyler框架，包含金字塔位置编码（PPE）来捕获多尺度特征，并引入强化学习动态优化风格化过程，加速收敛。

Result: 在4000轮训练后，内容损失降低62.6%（至2.07），风格损失降低57.4%（至0.86），推理时间1.39秒；使用强化学习后进一步改善（内容2.03，风格0.75），推理时间仅1.40秒。

Conclusion: 该方法实现了实时高质量的艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个用于大规模3D高斯泼溅的负载平衡高效框架，通过深度感知分区和优化策略解决了现有方法在大型无界场景中的负载不平衡和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有分治方法在扩展3DGS到大型场景时存在两个主要瓶颈：(i)分区负载严重不平衡，(ii)粗到细管道未能有效利用粗阶段，导致高开销。

Method: 提出深度感知分区方法将预处理时间从小时级降至分钟级，优化策略平衡可见高斯分布，并采用可见性裁剪和选择性致密化两种轻量级技术降低训练成本。

Result: 在大规模城市和户外数据集上的评估显示，LoBE-GS比最先进基线实现高达2倍的端到端训练加速，同时保持重建质量并扩展到传统3DGS无法处理的场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS管道，有效解决了负载平衡和效率问题，为大型无界场景的实时高保真3D重建提供了可行解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两个创新方法，解决长视频生成中的长期依赖建模和错误累积问题


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临双重挑战：需要捕捉长期依赖关系，同时防止自回归解码中的错误累积

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐，减少推理过程中的错误传播

Result: 实现了分钟级的时间一致性，计算复杂度保持线性，显著提升了长视频生成的上下文一致性和可靠性

Conclusion: MemoryPack和Direct Forcing相结合，大大增强了自回归视频模型的实用性和可靠性

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该论文提出了一种用于3D物体检测器分类任务置信度校准的方法，通过引入两个正则化损失项来改善主类和次类预测的校准效果，并在多个模型上验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，精确的物体检测和不确定性估计对于系统的自我感知和安全运行至关重要。现有方法主要关注主类预测的校准，而忽略了完整预测分布的重要性。

Method: 提出了两个辅助正则化损失项：一个针对主类预测的校准，另一个针对完整预测向量的校准。评估了多种后处理和训练时方法，结合等渗回归实现了最佳校准效果。

Result: 在CenterPoint和PillarNet模型上，结合完整类预测校准损失项和等渗回归的方法在主类和次类预测上都取得了最佳校准效果。但DSVT-Pillar模型无法使用相同方法同时校准主类和次类预测。

Conclusion: 完整的预测分布校准对于3D物体检测器的置信度校准至关重要，提出的方法在多个模型上验证了有效性，但不同模型可能需要特定的校准策略。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个利用预训练扩散模型进行行人搜索的新框架，通过三个专门模块解决现有方法中特征提取和优化冲突的问题，在CUHK-SYSU和PRW数据集上达到新的最优性能。


<details>
  <summary>Details</summary>
Motivation: 现有行人搜索方法主要使用ImageNet预训练骨干网络，可能无法有效捕捉复杂空间上下文和细粒度身份线索。同时，检测和重识别任务共享骨干特征会导致优化目标冲突，影响特征质量。

Method: 提出DiffPS框架，包含三个模块：1）扩散引导区域提议网络（DGRPN）用于改进行人定位；2）多尺度频率细化网络（MSFRN）缓解形状偏差；3）语义自适应特征聚合网络（SFAN）利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW数据集上实现了新的最优性能。

Conclusion: DiffPS通过利用扩散先验知识有效解决了行人搜索中检测和重识别任务的优化冲突问题，证明了扩散模型在该任务中的有效性。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: 提出了FMU网络，首次将流匹配技术融入HSI重建，通过深度展开框架结合生成先验，显著提升了压缩感知HSI重建质量。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像成本高且重建困难，现有压缩感知系统如CASSI在重建时面临严重退化问题，难以恢复精细光谱细节。

Method: 提出Flow-Matching-guided Unfolding网络（FMU），在深度展开框架中嵌入流匹配的生成先验，并引入平均速度损失来增强学习动态的全局一致性。

Result: 在模拟和真实数据集上的大量实验表明，FMU在重建质量上显著优于现有方法。

Conclusion: FMU成功结合了基于优化方法的可解释性和流匹配的生成能力，为HSI重建提供了更鲁棒和准确的解决方案。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的双列直插封装（DIP）自动缺陷检测系统，使用ConSinGAN生成训练数据，YOLOv7模型在准确率和检测时间上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统工业组件缺陷检测耗时费力，给质检人员带来沉重负担，且难以管理产品质量。

Method: 使用数字相机光学和深度学习模型，通过ConSinGAN生成合适大小的数据集，比较YOLOv3、v4、v7、v9四种模型，并开发SCADA系统和传感器架构。

Result: YOLOv7结合ConSinGAN在准确率达到95.50%，检测时间为285毫秒，明显优于其他YOLO版本和基于阈值的方法。

Conclusion: 提出的自动缺陷检测系统可轻松应用于多种缺陷类型或缺陷数据不足的情况。

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: FoundAD是一个基于预训练视觉基础模型的少样本异常检测方法，通过学习非线性投影算子来识别图像中的异常区域，在少样本条件下实现多类别异常检测。


<details>
  <summary>Details</summary>
Motivation: 工业安全检查中的少样本异常检测面临样本不足的挑战，特别是在类别不可知条件下。预训练视觉基础模型通过学习大量数据获得了对正常图像分布的通用理解，可以利用这种能力来改进异常检测。

Method: 利用预训练视觉基础模型学习图像嵌入，发现图像中的异常数量与学习到的嵌入差异直接相关。通过学习非线性投影算子将图像投影到自然图像流形上，该算子作为有效工具来表征和识别图像中的分布外区域。

Result: 大量实验表明，该方法支持多类别检测，在性能上具有竞争力，同时使用的参数数量显著少于先前方法。通过与多个基础编码器（包括最新的DINOv3）的评估验证了方法的有效性。

Conclusion: 这种方法拓宽了对基础特征的理解视角，并推动了少样本异常检测领域的发展，为工业安全检查提供了更简化的解决方案。

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT是一种基于Vision Transformer的语义分割方法，通过可训练的聚类模块和再生器模块来降低计算复杂度，同时保持分割精度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在机器人系统上的实际应用受到其二次注意力复杂度的限制。现有的token合并方法适用于分类任务，但不适合密集预测任务如语义分割。

Method: 在Vision Transformer骨干网络基础上，引入可训练的聚类模块，根据分割掩码的伪聚类指导合并相似token，然后通过再生器模块恢复细节信息供下游头使用。

Result: 在三个不同数据集上实现了高达2.18倍的GFLOPs减少和1.64倍的推理加速，同时保持了可比较的分割精度。

Conclusion: ClustViT有效解决了Vision Transformer在密集预测任务中的计算效率问题，为实际机器人应用提供了可行的解决方案。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一种统一的多模态大语言模型范式，通过将视觉补丁嵌入作为可解码令牌，使模型能够直接生成文本和多样化的视觉输出，在视觉感知和理解任务中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM方法依赖间接表示（如将坐标生成文本）进行视觉任务，这限制了性能并阻碍了密集预测任务如分割。需要一种能够直接生成视觉输出的统一方法。

Method: 提出Patch-as-Decodable Token (PaDT)范式，使用视觉参考令牌(VRTs)从查询图像的视觉补丁嵌入中派生，与LLM的文本输出令牌交错。轻量级解码器将LLM输出转换为检测、分割和定位预测。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终实现最先进的性能，即使与显著更大的MLLM模型相比也是如此。

Conclusion: PaDT通过统一的视觉-文本生成范式，有效解决了MLLM在视觉任务中的局限性，为多模态理解提供了更直接和高效的解决方案。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本文提出TriAlignXA可解释AI框架，通过'三角信任指数'解决农产品电商的'信任赤字'问题，平衡质量分级中的'不可能三角'矛盾。


<details>
  <summary>Details</summary>
Motivation: 在线果蔬电商存在'信任赤字'，因为数字交易无法提供直接的感官质量感知。传统绝对分级标准在生物特性、时效性和经济可行性之间存在矛盾。

Method: 构建'信任金字塔'模型，提出'三角信任指数'量化权衡。设计TriAlignXA框架，包含生物自适应引擎、时效优化引擎和经济优化引擎，通过'预映射机制'将流程数据编码为QR码实现透明化。

Result: 分级任务实验显示准确率显著高于基线模型，实证验证了框架在解决'不可能三角'方面的平衡能力。

Conclusion: 研究为构建可信赖的在线农产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 4DGS-Craft是一个用于4D高斯泼溅编辑的一致性和交互式框架，通过4D感知的InstructPix2Pix模型、多视图网格模块和高斯选择机制解决视图、时间和非编辑区域一致性问题，并利用LLM模块理解用户意图。


<details>
  <summary>Details</summary>
Motivation: 现有的4D高斯泼溅编辑方法在视图一致性、时间一致性、非编辑区域一致性以及处理复杂文本指令方面存在挑战，需要开发更一致和可控的编辑框架。

Method: 1. 引入4D感知的InstructPix2Pix模型，整合4D VGGT几何特征；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出高斯选择机制仅优化编辑区域的高斯；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 与相关工作相比，该方法实现了更一致和可控的4D场景编辑，能够处理复杂的用户指令并保持编辑质量。

Conclusion: 4DGS-Craft框架通过结合几何特征提取、多视图一致性和智能用户交互，显著提升了4D高斯泼溅编辑的性能和实用性。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: Pure-Pass (PP)是一种像素级掩码机制，通过识别纯像素来避免昂贵计算，集成到ATD-light模型后实现了更优的超分辨率性能和参数效率。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗、空间灵活性不足等局限性，需要更精细的像素级处理机制。

Method: 提出Pure-Pass机制，利用固定颜色中心点将像素分类，对纯像素进行豁免计算，实现细粒度、空间灵活的自适应掩码。

Result: PP-ATD-light在节省相似计算量的情况下，在重建质量和参数效率方面均优于CAMixer-ATD-light。

Conclusion: 像素级Pure-Pass机制能有效提升超分辨率模型的性能效率平衡，为轻量级图像重建提供了新思路。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 本研究利用GPT-4o多模态能力自动生成牙科全景X光片上的颌骨囊肿发现，通过构建自校正循环结构输出框架(SLSO)提高准确性


<details>
  <summary>Details</summary>
Motivation: 改进牙科影像分析中AI生成的准确性，减少幻觉现象，提高结构化输出的可靠性

Method: 采用10步流程处理22例颌骨囊肿病例，包括图像输入分析、结构化数据生成、牙号提取和一致性检查、检测到不一致时迭代再生、发现生成及后续重构和一致性验证

Result: SLSO框架相比传统CoT方法在多个评估项目上提高了输出准确性，牙号、牙齿移动和牙根吸收的改善率分别为66.9%、33.3%和28.6%，成功案例中经过最多5次再生后实现了结构化输出

Conclusion: SLSO框架强制负性发现描述，抑制幻觉，提高了牙号识别准确性，但识别跨多牙的广泛病变仍有限制，需要进一步改进以实现实用的发现生成系统

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net是一种3D自编码器架构，仅使用LiDAR点云从真实交通环境中提取高效特征。该架构通过减少编码器层数和简化跳跃连接，在不使用大量资源的情况下实现高性能，并能准确重建原始点云。


<details>
  <summary>Details</summary>
Motivation: 现有的最先进架构需要大量资源，而LiLa-Net旨在通过更简洁的架构实现高效的点云特征编码和重建，特别是在真实交通环境中的应用。

Method: 采用3D自编码器架构，利用跳跃连接概念提升性能。关键改进包括减少编码器层数、简化跳跃连接，同时在跳跃连接携带的信息和潜在编码之间实现有效平衡。

Result: 模型能够准确重建原始点云，并在跳跃连接和潜在编码之间达到有效平衡，提高了重建质量而不影响性能。模型还表现出强大的泛化能力，能成功重建与原始交通环境无关的物体。

Conclusion: LiLa-Net通过简化的架构实现了高效的点云特征编码和重建，在资源使用和性能之间取得了良好平衡，并展示了优秀的泛化能力，适用于真实交通环境中的LiDAR数据处理。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: kabr-tools是一个开源工具包，利用无人机视频和机器学习实现多物种行为自动监测，相比传统方法显著提升了行为分析的粒度和规模。


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法范围有限、耗时费力，难以评估跨景观的行为响应，需要可扩展的方法来量化复杂多维行为模式。

Method: 集成无人机视频与机器学习系统，通过目标检测、跟踪和行为分类生成行为指标，包括时间预算、行为转换、社会互动、栖息地关联和群体组成动态。

Result: 相比地面方法，无人机观测减少15%的可见性损失，捕获更多转换且精度和连续性更高；验证了斑马行为模式差异和物种间空间隔离。

Conclusion: kabr-tools实现了规模化自动行为监测，为生态系统研究、保护和生态监测提供了强大工具。

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一种基于3D高斯飞溅的语义感知3D形状和纹理变形框架，通过网格引导实现高保真几何和外观建模，无需预定义同胚映射或标记数据。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖点云或需要预定义同胚映射的局限性，实现无监督的语义对应和结构保持的3D变形。

Method: 使用网格引导的3D高斯飞溅进行高保真建模，通过统一变形策略将3D高斯锚定到重建网格面片，结合拓扑感知约束保持纹理保真度。

Result: 在TexMorph基准测试中，颜色一致性误差(ΔE)降低22.2%，EI降低26.2%，显著优于现有2D/3D方法。

Conclusion: 该框架在保持局部细节和全局语义一致性的同时，实现了高质量的3D形状和纹理变形，为无监督3D变形提供了有效解决方案。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，通过扩散模型仅依赖旋转测量实现零样本泛化的人体姿态估计


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的方法依赖位置和旋转测量，但位置测量受用户体型影响大，导致跨用户泛化能力差

Method: 将姿态估计建模为逆问题，使用预训练扩散模型仅基于旋转测量，通过似然项引导生成最可能解释稀疏测量的姿态序列

Result: 提出的InPose方法能够零样本泛化到不同用户

Conclusion: 通过仅依赖旋转测量并结合似然引导，实现了对任意用户的高质量姿态估计

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: VGDM：基于视觉引导扩散模型的脑肿瘤检测与分割框架，结合Transformer和扩散模型提升脑肿瘤分割性能


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面能力有限，限制了复杂肿瘤结构的分割性能。扩散模型在医学图像生成和边界细化方面显示出强大潜力

Method: 提出VGDM框架，在扩散过程核心嵌入视觉Transformer，利用全局上下文推理和迭代去噪来增强体积精度和边界精度。Transformer骨干网络能更有效建模整个MRI体积的空间关系，扩散细化减轻体素级误差并恢复细粒度肿瘤细节

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似性和Hausdorff距离指标上获得一致提升

Conclusion: 这种混合设计为神经肿瘤学提供了改进鲁棒性和可扩展性的途径，超越了传统的U-Net基线，展示了Transformer引导扩散模型在推进肿瘤分割技术前沿的潜力

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发了一个可扩展的深度学习管道，从历史地图中提取城市区域，创建了法国1925-1950年期间首个全国尺度的城市足迹数据集。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏全国范围的数字城市足迹数据，对1970年代前法国历史城市扩张的定量分析受到限制。

Method: 采用双通道U-Net方法处理历史地图的高辐射度和风格复杂性，第一通道生成初步地图识别混淆区域，第二通道使用精炼数据集和二值化输出来最小化辐射噪声。

Result: 处理了覆盖法国本土的941个高分辨率图块，最终马赛克达到73%的整体准确率，有效捕捉了多样化的城市模式。

Conclusion: 公开发布了代码、训练数据集和全国城市栅格数据，以支持未来长期城市化动态研究。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 分析了腹腔镜胆囊切除术视频中基于点跟踪的失败模式，比较了点跟踪与分割掩码初始化的性能，发现点跟踪对于手术工具具有竞争力，但对于解剖目标表现较差。


<details>
  <summary>Details</summary>
Motivation: 了解在复杂手术环境中基于点跟踪的可靠性和失败情况，因为点跟踪提供了一种高效低成本的替代方案，但其在手术视频中的表现尚未得到充分研究。

Method: 系统分析腹腔镜胆囊切除术视频中基于点跟踪的失败模式，重点关注胆囊、抓钳和L型电钩三个手术目标，并与分割掩码初始化方法进行性能比较。

Result: 点跟踪对于手术工具具有竞争力，但对于解剖目标（如胆囊）表现不佳，主要由于组织相似性和模糊边界导致跟踪失败。

Conclusion: 通过定性分析揭示了影响跟踪结果的关键因素，并提供了选择与放置跟踪点的实用建议，以改进手术视频分析中的跟踪性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出FFREEDG新任务：在服务器预训练后，仅使用客户端未标注数据进行联邦学习，不重新访问源数据。提出FRIEREN框架，利用视觉语言模型和弱-强一致性学习解决该任务。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法要么不现实地假设客户端有标注数据，要么未能充分利用现代视觉基础模型的能力，特别是在处理域偏移和未标注数据时存在挑战。

Method: FRIEREN框架整合视觉和语言模态，使用CLIP文本嵌入指导的视觉语言解码器改进语义消歧，采用弱-强一致性学习策略在伪标签上进行鲁棒的本地训练。

Result: 在合成到真实、清晰到恶劣天气基准测试中，该方法有效解决了新任务，性能与现有域泛化和适应方法相当，为未来研究设立了强基线。

Conclusion: 该工作提出了具有挑战性的FFREEDG任务，并展示了FRIEREN框架在该任务上的有效性，为联邦学习中的域适应问题提供了新思路。

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个基于提示的框架，利用动作中心知识从冻结的视觉语言模型中提取更准确和可解释的推理，用于视频异常检测。


<details>
  <summary>Details</summary>
Motivation: 现有的提示方法过于抽象，忽略了细粒度的人-物交互或动作语义，而这些对于定义监控视频中的复杂异常至关重要。

Method: 提出结构化提示框架，将提示组织成语义连贯的组别（如暴力、财产犯罪、公共安全），并制定细粒度的指导性问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上的实验显示，ASK-Hint持续提升AUC，相比微调和无训练方法都达到了最先进的性能。

Conclusion: 该框架强调了提示粒度的重要性，并为可解释的视频异常检测提供了一个无需训练且可泛化的解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify通过使用学生亲和网络和几何引导池化模块，利用潜在几何信息净化2D VLM生成的3D点特征，在仅使用1.5%训练数据的情况下达到或超越最先进的3D语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决2D视觉语言模型特征向3D语义分割转移时存在的权衡问题：直接投影会导致噪声和碎片化预测，而强制几何一致性需要昂贵的训练流程和大规模标注3D数据。

Method: 提出GeoPurify方法，使用小型学生亲和网络从3D自监督教师模型中提取几何先验来净化2D VLM生成的3D点特征，并在推理时使用几何引导池化模块进一步去噪。

Result: 在主要3D基准测试上的广泛实验表明，GeoPurify在仅使用约1.5%训练数据的情况下达到或超越最先进性能。

Conclusion: GeoPurify有效缓解了2D语义与3D几何结构之间的权衡问题，实现了卓越的数据效率，证明了利用潜在几何信息的有效性。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出了一种基于猪耳静脉模式的非侵入性生物识别方法，用于替代传统不可靠的耳标和芯片识别，在混合品种猪群中达到98.12%的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 传统猪只识别方法（耳标、芯片）不可靠、成本高且主要针对纯种猪，不适合小规模农户使用。

Method: 使用智能手机和简单背光采集800张猪耳图像，开发多阶段计算机视觉流程增强静脉可见度，提取结构和空间特征生成生物特征签名，并用机器学习模型分类。

Result: 支持向量机（SVM）模型在混合品种猪群中达到98.12%的识别精度，从图像处理到分类平均耗时8.3秒，适合实时农场部署。

Conclusion: 该系统通过永久生物标记替代易损的物理标识，为农民提供经济高效、无压力的动物识别方法，证实了耳静脉生物识别在数字化畜牧管理中的实用性。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出一种多类别密度图估计框架，使用Twins金字塔视觉transformer和专用多类计数头，在密集遮挡场景中优于现有方法，并应用于生物多样性监测


<details>
  <summary>Details</summary>
Motivation: 解决密集遮挡场景中离散检测方法失效的问题，需要多类别物体计数方法

Method: 采用Twins金字塔视觉transformer主干网络和基于多尺度解码的多类计数头，添加分割类别的焦点模块抑制类别间干扰

Result: 在VisDrone和iSAID基准测试中显著优于现有方法（MAE降低33%、43%和64%），比YOLOv11更适合密集场景

Conclusion: 该方法通过区域损失将多类人群计数扩展到新领域，展示了在生物多样性监测中的潜力，为保护工作提供可扩展的生态洞察

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过优化交叉注意力图来实现视频生成中的时间控制，让用户能够精确指定视觉元素在序列中出现的时间。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型缺乏细粒度的时间控制能力，无法让用户指定特定视觉元素在生成序列中的出现时间。

Method: 利用文本到视频扩散模型中的交叉注意力图，通过新颖的优化方法引导概念的时间安排，采用三个互补原则：通过相关性对齐时间形状、通过能量放大可见性区域、通过熵保持空间焦点。

Result: TempoControl能够在保持高质量和多样性的同时实现精确的时间控制，在单对象和多对象的时间重排序、动作和音频对齐生成等多种视频生成应用中表现出色。

Conclusion: 该方法为视频生成提供了有效的时间控制解决方案，无需修改模型架构或重新训练。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 论文提出RewardMap框架，通过密集奖励信号和多阶段强化学习解决多模态大语言模型在细粒度视觉推理任务中的稀疏奖励和不稳定优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在结构化信息丰富的空间推理任务（如交通地图）中表现不佳，标准强化学习面临稀疏奖励和优化不稳定问题。

Method: 构建ReasonMap-Plus数据集提供密集奖励信号，提出RewardMap多阶段RL框架，包含难度感知奖励设计和从简单感知到复杂推理的多阶段训练策略。

Result: 在ReasonMap和ReasonMap-Plus上的实验显示RewardMap各组件均带来性能提升，组合使用效果最佳，在6个基准测试中平均提升3.47%。

Conclusion: RewardMap有效提升了MLLMs的视觉理解和推理能力，证明其在细粒度视觉推理任务中的实用性和科学性价值。

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个利用FLUX强大先验进行基于拖拽图像编辑的框架，通过区域化编辑范式和仿射变换解决DiT特征结构化不足的问题，在DragBench-DR和ReD Bench基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型从UNet-based DDPMs转向更可扩展的DiT with flow matching（如SD3.5、FLUX），生成先验显著增强，但基于拖拽的编辑尚未受益于这些更强的先验。传统方法在目标区域存在扭曲问题。

Method: 1. 引入区域化编辑范式，使用仿射变换实现更丰富一致的特征监督；2. 集成预训练开放域个性化适配器（如IP-Adapter）增强主体一致性；3. 通过梯度掩码硬约束保持背景保真度；4. 利用多模态大语言模型解决任务歧义。

Result: 在DragBench-DR和ReD Bench基准上的广泛实验表明，DragFlow超越了基于点和基于区域的基线方法，在基于拖拽的图像编辑中达到了新的最先进水平。

Conclusion: DragFlow成功利用FLUX的丰富先验进行拖拽编辑，通过区域化方法和多模态增强解决了DiT特征结构化不足的问题，为基于拖拽的图像编辑设立了新的技术标杆。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 本文提出F2C方法，通过从关键帧选择扩展到关键片段选择来提升视频大语言模型的性能，在保持计算预算不变的情况下动态平衡空间分辨率和片段长度。


<details>
  <summary>Details</summary>
Motivation: 解决视频大语言模型中存在的"大海捞针"问题：原始视频帧产生的大量视觉标记会耗尽模型的上下文窗口，而现有的帧选择方法会丢失重要的时间动态信息。

Method: 提出关键片段选择方法，将选择范围从孤立的关键帧扩展到短时态连贯的片段；采用自适应分辨率策略动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试上，F2C方法比均匀采样方法分别提升了8.1%（Video-MME）、5.6%（LongVideoBench）和10.3%（MLVU）。

Conclusion: 保持时间连贯性在帧选择中至关重要，为视频大语言模型在实际视频理解应用中的扩展提供了实用路径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 该研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元（IMU）在13种临床相关日常活动中的性能，发现MotionAGFormer模型表现最佳，揭示了两种技术在成本、可访问性和精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和可穿戴传感器的发展，需要在真实世界条件下准确评估人体运动，这对于远程医疗、运动科学和康复至关重要。

Method: 利用VIDIMU数据集，比较了MotionAGFormer、MotionBERT、MMPose 2D-to-3D姿态提升和NVIDIA BodyTrack等深度学习框架与基于IMU数据的关节角度估计，采用OpenSim逆向运动学方法。

Result: MotionAGFormer表现最优，总体RMSE为9.27°±4.80°，MAE为7.86°±4.18°，Pearson相关系数为0.86±0.15，决定系数R²为0.67±0.28。

Conclusion: 两种技术都适用于实验室外运动学评估，但存在成本、可访问性和精度方面的权衡，为研究人员和临床医生开发远程医疗解决方案提供了指导。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift提出了一种新的fMRI解码方法，通过结合AutoKL和CLIP适配器的扩散模型，实现了跨被试的高质量视觉刺激重建，仅需1小时训练即可达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决fMRI数据解码中跨被试重建的挑战，包括被试间神经表征差异和大脑对复杂视觉输入的抽象语义编码问题。

Method: 集成互补适配器的扩散模型：AutoKL处理低级特征，CLIP处理语义特征。CLIP适配器在Stable Diffusion生成图像和COCO标题对上训练，模拟高级视觉皮层编码。跨被试泛化采用预训练+微调策略，仅更新17%参数。

Result: 在轻量级GPU（三块RTX 4090）上仅需1小时训练即可达到最先进性能，优于现有方法。

Conclusion: NeuroSwift为跨被试fMRI解码提供了一种高效准确的解决方案，显著降低了计算需求并提高了重建质量。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一个自训练框架，通过细粒度线索联合优化CLIP的视觉和文本表示，在13个细粒度基准测试中平均准确率提升2.90%。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度图像分类任务中表现受限，因为其依赖粗粒度的全局特征而忽略了空间精度。现有方法通过对齐大语言模型描述与CLIP的[CLS]标记来注入细粒度知识，但这种方法缺乏空间精确性。

Method: 提出Saliency-Oriented Attention Pooling (SOAP)和TokenFusion模块，构建基于显著性的[FG]标记并与全局[CLS]标记融合；引入双头LLM派生分类器（冻结分类器和可学习分类器）；开发动态知识聚合方法结合固定先验和演化logits迭代优化伪标签。

Result: 在13个细粒度基准测试中实现平均2.90%的准确率提升，仅需轻量级适应。

Conclusion: microCLIP成功挖掘了CLIP中的潜在细粒度信号，通过联合优化视觉和文本表示显著提升了细粒度分类性能。

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个通过GRPO微调多模态大语言模型的视频真实性检测器，能提供高精度判断和可解释推理，在现有基准上实现SOTA零样本性能


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频技术的快速发展，急需有效的检测工具来应对错误信息和声誉损害等社会风险，同时检测模型需要提供可解释的说明以确保监管机构和最终用户的透明度

Method: 使用GRPO（组相对策略优化）微调Qwen-VL多模态大语言模型，专门针对时间伪影和生成复杂度的两个奖励模型进行训练，构建了包含14万真实和AI生成视频的挑战性数据集

Result: VidGuard-R1在现有基准上实现了最先进的零样本性能，额外训练后准确率超过95%，案例研究显示其预测具有精确且可解释的推理

Conclusion: VidGuard-R1通过结合多模态大语言模型和GRPO优化，成功解决了AI生成视频检测的准确性和可解释性挑战，为视频真实性检测提供了有效解决方案

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出一种简单有效的方法来缓解长视频生成中的质量退化问题，无需长视频监督或重新训练。通过利用教师模型的丰富知识为自生成长视频中的采样片段提供指导，保持时间一致性，可扩展视频长度达教师能力的20倍。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了革命性进展，但基于Transformer架构的计算成本过高，特别是在生成长视频时。现有的自回归方法通过从短时双向教师模型蒸馏，但学生模型在超出训练范围时会出现明显的质量退化问题。

Method: 利用教师模型的知识为自生成长视频中的采样片段提供指导。通过自强制机制，在保持时间一致性的同时避免过度曝光和错误累积问题，无需重新计算重叠帧。

Result: 能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上。在标准基准测试和改进基准测试中，该方法在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效解决了长视频生成中的质量退化问题，展示了在无需额外监督的情况下扩展视频生成长度的能力，为长序列生成提供了新的解决方案。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一种基于物理引导的视频生成方法，通过两阶段训练策略实现逼真的刚体控制、交互和效果，在合成场景和真实场景中均显著改善了物体交互质量。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物理合理的物体交互方面仍存在困难，缺乏基于物理的控制机制。本文旨在解决这一限制，为机器人学和具身决策提供更好的世界模拟器。

Method: 提出KineMask方法，采用两阶段训练策略：通过物体掩码逐步移除未来运动监督，训练视频扩散模型。结合低级运动控制和高级文本条件，通过预测性场景描述实现复杂动态现象的合成。

Result: 在合成简单交互场景上训练后，在真实场景中显著改善了物体交互效果。实验表明KineMask在可比模型规模下实现了显著改进，消融研究验证了低级和高级条件在视频扩散模型中的互补作用。

Conclusion: KineMask通过物理引导的视频生成方法，有效解决了当前模型在物理交互方面的局限性，为视频生成提供了更可靠的控制机制，代码和模型将公开可用。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 提出多模态细粒度动作模拟方法，解决现有视频模型在精细控制方面的不足，通过整合本体感觉、运动感觉、力触觉和肌肉激活等多模态感官信息，提高模拟精度和实用性。


<details>
  <summary>Details</summary>
Motivation: 当前视频模型缺乏精细控制能力，无法满足通用家庭机器人对实时精细运动控制的需求，特别是在处理精细任务和紧急情况时。

Method: 开发特征学习范式，对齐多模态感官信息同时保留各模态独特信息；提出正则化方案增强动作轨迹特征在复杂交互动态中的因果性表示。

Result: 实验表明多模态感官整合提高了模拟精度，减少了时间漂移；大量消融研究和下游应用验证了方法的有效性和实用性。

Conclusion: 多模态细粒度动作模拟方法能够有效解决机器人精细控制问题，为通用家庭机器人的实际应用提供了可行方案。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA采用Native Sparse Attention技术解决视频理解中的长上下文问题，通过硬件感知的混合注意力机制，在Qwen2.5-VL模型上实现128K token的长视频理解能力提升


<details>
  <summary>Details</summary>
Motivation: 当前多模态语言模型在视频理解中受限于上下文长度，容易错过关键过渡帧，难以在长时间尺度上保持连贯性

Method: 将Native Sparse Attention适配到视频语言模型中，采用硬件感知的混合注意力方法：文本保持密集注意力，视频使用稀疏注意力，在216K视频指令数据集上进行端到端训练

Result: 相比token压缩和无需训练的稀疏基线，VideoNSA在长视频理解、时序推理和空间基准测试上表现更优，能够可靠扩展到128K token

Conclusion: 研究发现：1）可靠扩展到128K token；2）固定预算下的最优全局-局部注意力分配；3）任务依赖的分支使用模式；4）可学习的组合稀疏注意力有助于诱导动态注意力汇聚点

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过根据分辨率大小重新校准去噪器的噪声水平，解决了扩散模型在不同分辨率下生成质量不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供开箱即用的经济高效替代方案，因为模型在训练分辨率之外的分辨率上生成质量较差。

Method: 提出NoiseShift方法，识别噪声调度器在不同分辨率下具有不等的感知效果这一关键洞察，通过重新校准去噪器的噪声水平来匹配分辨率大小，无需修改模型架构或采样计划。

Result: 在Stable Diffusion 3、Stable Diffusion 3.5和Flux-Dev上应用NoiseShift后，低分辨率生成质量显著提升：在LAION-COCO数据集上FID分别提升15.89%、8.56%和2.44%，在CelebA数据集上分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift有效缓解了分辨率相关的伪影，提升了低分辨率图像生成质量，为扩散模型提供了更好的分辨率泛化能力。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该论文研究从视频预测动态物理属性的任务，重点关注需要时间信息推断的属性：弹性、粘度和动态摩擦。


<details>
  <summary>Details</summary>
Motivation: 动机是开发能够从视频中推断动态物理属性的方法，这对于理解物理世界和机器人交互等应用具有重要意义。

Method: 方法包括：(i) 为每个物理属性收集新的视频数据集；(ii) 探索三种推断方法：基于经典计算机视觉的oracle方法、使用预训练视频生成和自监督模型的简单读取机制、以及多模态大语言模型的提示策略。

Result: 结果显示，生成式或自监督训练的视频基础模型性能相似，但落后于oracle方法；MLLMs目前性能较差，但通过合适的提示可以改善。

Conclusion: 结论是视频基础模型在动态物理属性预测方面有潜力，但需要进一步改进，特别是多模态大语言模型的性能需要通过更好的提示策略来提升。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出声音对象检测任务，通过多模态对象感知框架从第一人称视角视频中学习对象与声音的关联，使用分割掩码和槽注意力视觉编码器实现对象中心的建模。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互产生独特声音，研究模型能否区分不同物体交互的声音（如勺子敲击木地板与地毯），评估模型将声音与相关物体关联的能力。

Method: 开发自动管道计算交互对象的分割掩码，引导模型关注信息最丰富的交互区域；使用槽注意力视觉编码器强化对象先验；基于第一人称视角视频进行多模态学习。

Result: 在新任务和现有多模态动作理解任务上达到最先进性能。

Conclusion: 提出的对象感知框架能有效链接声音与物体，在声音对象检测任务中表现优异。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 提出了一种针对3D高斯泼溅（3DGS）的密度引导毒化攻击方法，通过向低密度区域注入高斯点来嵌入视角依赖的虚假对象，同时引入自适应噪声策略破坏多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 随着NeRF和3DGS等3D场景表示方法在新视角合成中的广泛应用，研究其安全漏洞变得至关重要。本文旨在分析3DGS对图像级毒化攻击的鲁棒性。

Method: 使用核密度估计（KDE）识别低密度区域，策略性地注入高斯点来嵌入视角依赖的虚假对象；采用自适应噪声策略破坏多视角一致性；提出基于KDE的评估协议来系统评估攻击难度。

Result: 大量实验表明，该方法在攻击效果上优于现有最先进技术，能够有效嵌入可见的虚假对象同时最小化对正常视角的影响。

Conclusion: 该研究揭示了3DGS的潜在安全漏洞，提出的密度引导毒化方法和评估协议为未来3D场景表示的安全性研究提供了重要基准。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架，通过随机最优控制来引导文本到图像模型的采样过程，解决多主体生成中的属性泄漏、身份纠缠和主体遗漏问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在单主体提示上表现良好，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要系统性的解决方案。

Method: 基于流匹配和随机最优控制理论，提出了两种架构无关的算法：测试时控制器和伴随匹配微调方法，通过控制基速度场来实现主体解缠。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL等模型上，两种算法都能显著提升多主体对齐效果，同时保持基础模型的风格特征。

Conclusion: 该工作为多主体文本到图像生成提供了首个理论框架和实用算法，FOCUS方法在多个模型上实现了最先进的多主体保真度。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [78] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音频-视频生成范式，通过块状跨模态融合的双DiT模块，将两种模态建模为单一生成过程，实现自然同步，无需单独管道或后处理对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的音频-视频生成方法通常依赖复杂的多阶段架构或声音和视觉的顺序合成，需要更简洁统一的生成范式。

Method: 使用块状跨模态融合的双DiT模块，通过联合训练相同的视频和音频塔，在庞大视频语料库上进行时间（通过缩放RoPE嵌入）和语义（通过双向交叉注意力）的块状交换。

Result: 模型能够生成具有自然语音和准确、上下文匹配音效的电影级视频片段，实现电影级的故事讲述。

Conclusion: Ovi提供了一个统一的音频-视频生成框架，通过跨模态融合实现了高质量的同步生成，为电影级内容创作提供了有效工具。

Abstract: Audio-video generation has often relied on complex multi-stage architectures
or sequential synthesis of sound and visuals. We introduce Ovi, a unified
paradigm for audio-video generation that models the two modalities as a single
generative process. By using blockwise cross-modal fusion of twin-DiT modules,
Ovi achieves natural synchronization and removes the need for separate
pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion
modeling, we initialize an audio tower with an architecture identical to that
of a strong pretrained video model. Trained from scratch on hundreds of
thousands of hours of raw audio, the audio tower learns to generate realistic
sound effects, as well as speech that conveys rich speaker identity and
emotion. Fusion is obtained by jointly training the identical video and audio
towers via blockwise exchange of timing (via scaled-RoPE embeddings) and
semantics (through bidirectional cross-attention) on a vast video corpus. Our
model enables cinematic storytelling with natural speech and accurate,
context-matched sound effects, producing movie-grade video clips. All the
demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [79] [Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare](https://arxiv.org/abs/2510.01194)
*Juan Barrientos,Michaelle Pérez,Douglas González,Favio Reyna,Julio Fajardo,Andrea Lara*

Main category: cs.HC

TL;DR: 本文提出了一种人机协作的AI系统，帮助助产士在资源匮乏地区通过盲扫协议获取诊断相关的胎儿图像，通过AI识别关键帧来减轻专家负担。


<details>
  <summary>Details</summary>
Motivation: 解决低收入国家农村地区产科超声检查资源有限的问题，特别是在孕产妇保健服务不足的地区扩大产前影像检查的可及性。

Method: 开发了一个结合分类模型和基于网络平台的AI系统，采用盲扫协议，允许助产士使用低成本POCUS设备采集视频，AI自动识别关键帧供专家审查。

Result: 系统在非专家采集的盲扫视频中能够有效识别标准胎儿平面，现场评估显示系统具有良好的可用性和低认知负荷。

Conclusion: 该人机协作AI系统有潜力在服务不足地区扩大产前影像检查的覆盖范围，提高孕产妇保健服务质量。

Abstract: Access to obstetric ultrasound is often limited in low-resource settings,
particularly in rural areas of low- and middle-income countries. This work
proposes a human-in-the-loop artificial intelligence (AI) system designed to
assist midwives in acquiring diagnostically relevant fetal images using blind
sweep protocols. The system incorporates a classification model along with a
web-based platform for asynchronous specialist reviews. By identifying key
frames in blind sweep studies, the AI system allows specialists to concentrate
on interpretation rather than having to review entire videos. To evaluate its
performance, blind sweep videos captured by a small group of soft-trained
midwives using a low-cost Point-of-Care Ultrasound (POCUS) device were
analyzed. The system demonstrated promising results in identifying standard
fetal planes from sweeps made by non-experts. A field evaluation indicated good
usability and a low cognitive workload, suggesting that it has the potential to
expand access to prenatal imaging in underserved regions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [80] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 这篇综述论文系统性地调研了基于深度学习的从2D MRI进行3D形状重建的方法，重点分析了四种主要方法：点云、基于网格、形状感知和体积模型，并讨论了临床应用、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在医学影像中的重要性日益增加，需要为研究人员提供一个关于当前3D MRI重建方法的结构化概述，以识别推动深度学习向更稳健、可泛化和临床影响力解决方案发展的机会。

Method: 采用综述研究方法，系统分析四种主要3D重建方法：点云模型、基于网格的方法、形状感知模型和体积模型，对每种方法的技术基础、局限性和应用进行全面评估。

Result: 提供了从心脏到神经再到肺部成像的广泛应用概述，分析了模型在病变解剖中的临床适用性，以及训练和测试数据的影响，同时考察了公开数据集、计算需求和评估指标。

Conclusion: 该综述为研究人员提供了当前3D重建方法的结构化概览，并强调了包括多模态集成和跨模态框架在内的新兴研究方向，旨在推动深度学习在医学影像重建领域的发展。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [81] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 提出基于低秩表示和向量量化的新型压缩重建框架，解决神经网络压缩中解码器计算瓶颈问题


<details>
  <summary>Details</summary>
Motivation: 现有神经压缩方法虽然压缩率高，但基于卷积的解码器复杂度高、计算成本大，阻碍了实际应用

Method: 在带有向量量化的自编码器中引入低秩表示，通过对学习到的图像潜在表示进行一系列计算高效的低秩操作来实现高质量数据重建

Result: 大幅降低神经压缩/重建解码阶段的计算开销，基本消除解码器计算瓶颈，同时保持图像输出的高保真度

Conclusion: 该方法有效解决了神经压缩中的解码器瓶颈问题，为实际应用提供了可行的解决方案

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [82] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出了一种自适应门控融合网络（AGFN），通过基于信息熵和模态重要性的双门融合机制自适应调整特征权重，有效提升多模态情感分析性能。


<details>
  <summary>Details</summary>
Motivation: 传统的简单融合技术无法有效处理模态质量变化（如噪声、缺失或语义冲突），导致在辨别细微情感时性能不佳。

Method: 使用双门融合机制，基于信息熵和模态重要性自适应调整特征权重，在单模态编码和跨模态交互后优先选择信息量大的线索。

Result: 在CMU-MOSI和CMU-MOSEI数据集上的实验表明，AGFN在准确率上显著优于强基线，能有效辨别细微情感且性能稳健。

Conclusion: AGFN通过减少特征位置与预测误差之间的相关性，降低了模型对特定位置的依赖，从而学习到更广泛的特征分布，增强了泛化能力。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [83] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出一种无监督动态特征选择(DFS)方法，用于增强视觉任务中的潜在表示，通过去除图像中的误导性或冗余信息来提高模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示常受到噪声或无关特征的影响，这会降低模型性能和泛化能力。需要一种无需标注数据的方法来优化特征选择。

Method: 开发了无监督动态特征选择(DFS)框架，针对每个实例识别并移除图像中的误导或冗余信息，确保只有最相关特征贡献于潜在空间。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在各种任务（包括聚类和图像生成）中实现了显著的泛化性能提升，同时计算成本增加极小。

Conclusion: 无监督DFS方法能有效增强潜在表示，提高模型鲁棒性和泛化性能，且具有广泛适用性。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [84] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的Granular-GRPO框架，通过细粒度奖励评估和多粒度优势集成来解决基于流的生成模型中强化学习的奖励信号稀疏和窄化问题，显著提升了偏好对齐效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在探索潜在高价值样本时有效，但由于稀疏和窄化的奖励信号导致偏好对齐效果不佳。

Method: 提出G²RPO框架，包含奇异随机采样策略支持逐步随机探索，以及多粒度优势集成模块消除固定粒度去噪偏差。

Result: 在各种奖励模型上的实验表明，G²RPO显著优于现有的基于流的GRPO基线方法。

Conclusion: G²RPO框架通过精确全面的奖励评估和鲁棒的多粒度优势集成，有效提升了生成模型与人类偏好的对齐效果。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [85] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 提出了一种用于离散扩散基础模型的后验采样方法——锚定后验采样(APS)，通过量化期望和锚定重掩码技术解决现有离散扩散后验采样方法的局限性，在线性和非线性逆问题上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型提供了统一框架来建模分类数据，具有更快推理、更精细控制和无需训练即可进行贝叶斯推理等优势，但现有离散扩散后验采样方法面临信号稀疏、应用受限和维度灾难等挑战。

Method: 基于掩码扩散基础模型，引入两个关键创新：在离散嵌入空间中使用量化期望实现类梯度指导，以及锚定重掩码实现自适应解码。

Result: 在标准基准测试中，该方法在线性和非线性逆问题上都达到了离散扩散采样器的最先进性能，并在无需训练的风格化和文本引导编辑任务中展示了优势。

Conclusion: APS方法成功克服了现有离散扩散后验采样方法的局限性，为离散扩散模型在逆问题求解和图像编辑等任务中的应用提供了有效解决方案。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [86] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: 提出Concept Neuron Selection (CNS)方法，通过识别扩散模型中与目标概念相关的神经元，在增量学习中实现个性化，同时避免灾难性遗忘并保持零样本生成能力。


<details>
  <summary>Details</summary>
Motivation: 在现实应用中，增量更新扩散模型具有实用性但计算挑战大，需要一种有效的方法在持续学习场景下进行个性化。

Method: CNS方法独特识别扩散模型中与目标概念相关的神经元，以增量方式微调概念神经元，并联合保留先前概念的知识。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中都优于先前方法。

Conclusion: CNS实现了无融合操作，减少了持续个性化的内存存储和处理时间，是一种简单而有效的持续学习策略。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [87] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: 提出Equilibrium Matching (EqM)框架，通过平衡动力学视角构建生成模型，放弃传统扩散/流模型中的非平衡时间条件动态，学习隐式能量景观的平衡梯度


<details>
  <summary>Details</summary>
Motivation: 传统扩散和流模型依赖非平衡时间条件动态，EqM旨在通过平衡动力学视角提供更统一的生成建模框架

Method: EqM学习隐式能量景观的平衡梯度，在推理时采用基于优化的采样过程，通过梯度下降获得样本，支持可调步长、自适应优化器和自适应计算

Result: 在ImageNet 256×256上达到FID 1.90，超越扩散/流模型性能，并能处理部分噪声图像去噪、OOD检测和图像合成等任务

Conclusion: EqM通过用统一平衡景观替代时间条件速度，在流模型和能量基模型之间建立更紧密联系，为优化驱动推理提供简单途径

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [88] [MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging](https://arxiv.org/abs/2510.01298)
*Berker Demirel,Marco Fumero,Theofanis Karaletsos,Francesco Locatello*

Main category: q-bio.QM

TL;DR: MorphGen是一个基于扩散模型的荧光显微镜图像生成模型，能够跨多种细胞类型和扰动进行可控生成，在生物一致性方面优于现有方法


<details>
  <summary>Details</summary>
Motivation: 加速基于高内涵图像的药物发现和基因编辑检测，需要能够模拟细胞对干预反应的硅内模型

Method: 使用扩散模型生成完整的荧光通道图像，通过对齐损失将表示与OpenPhenom的表型嵌入匹配，保留细胞器特异性细节

Result: MorphGen的FID分数比现有最优方法MorphoDiff低35%以上，能够生成生物一致的真实图像

Conclusion: MorphGen通过联合生成完整荧光通道和与生物基础模型对齐，实现了更精细的形态学分析，为生物解释提供了重要工具

Abstract: Simulating in silico cellular responses to interventions is a promising
direction to accelerate high-content image-based assays, critical for advancing
drug discovery and gene editing. To support this, we introduce MorphGen, a
state-of-the-art diffusion-based generative model for fluorescent microscopy
that enables controllable generation across multiple cell types and
perturbations. To capture biologically meaningful patterns consistent with
known cellular morphologies, MorphGen is trained with an alignment loss to
match its representations to the phenotypic embeddings of OpenPhenom, a
state-of-the-art biological foundation model. Unlike prior approaches that
compress multichannel stains into RGB images -- thus sacrificing
organelle-specific detail -- MorphGen generates the complete set of fluorescent
channels jointly, preserving per-organelle structures and enabling a
fine-grained morphological analysis that is essential for biological
interpretation. We demonstrate biological consistency with real images via
CellProfiler features, and MorphGen attains an FID score over $35\%$ lower than
the prior state-of-the-art MorphoDiff, which only generates RGB images for a
single cell type. Code is available at https://github.com/czi-ai/MorphGen.

</details>


### [89] [A Multicentric Dataset for Training and Benchmarking Breast Cancer Segmentation in H&E Slides](https://arxiv.org/abs/2510.02037)
*Carlijn Lems,Leslie Tessier,John-Melle Bokhorst,Mart van Rijthoven,Witali Aswolinskiy,Matteo Pozzi,Natalie Klubickova,Suzanne Dintzis,Michela Campora,Maschenka Balkenhol,Peter Bult,Joey Spronck,Thomas Detone,Mattia Barbareschi,Enrico Munari,Giuseppe Bogina,Jelle Wesseling,Esther H. Lips,Francesco Ciompi,Frédérique Meeuwsen,Jeroen van der Laak*

Main category: q-bio.QM

TL;DR: BEETLE数据集是一个用于乳腺癌全玻片图像多类语义分割的数据集，包含587个活检和切除样本，覆盖所有分子亚型和组织学分级，特别关注现有数据集中代表性不足的形态学特征。


<details>
  <summary>Details</summary>
Motivation: 现有的乳腺癌分割数据集缺乏形态学多样性，无法支持模型泛化性和在异质患者队列中进行稳健的生物标志物验证。

Method: 通过多种标注策略收集了四个类别的注释：浸润性上皮、非浸润性上皮、坏死和其他，特别关注导管原位癌和分散性小叶肿瘤细胞等形态学特征。

Result: BEETLE数据集包含来自三个临床中心和两个公共数据集的587个样本，使用七种扫描仪数字化，提供了精心策划的多中心外部评估集。

Conclusion: 该数据集的多样性和与乳腺癌自动生物标志物量化领域的相关性确保了其高重用潜力，为乳腺癌分割模型提供了标准化基准测试。

Abstract: Automated semantic segmentation of whole-slide images (WSIs) stained with
hematoxylin and eosin (H&E) is essential for large-scale artificial
intelligence-based biomarker analysis in breast cancer. However, existing
public datasets for breast cancer segmentation lack the morphological diversity
needed to support model generalizability and robust biomarker validation across
heterogeneous patient cohorts. We introduce BrEast cancEr hisTopathoLogy
sEgmentation (BEETLE), a dataset for multiclass semantic segmentation of
H&E-stained breast cancer WSIs. It consists of 587 biopsies and resections from
three collaborating clinical centers and two public datasets, digitized using
seven scanners, and covers all molecular subtypes and histological grades.
Using diverse annotation strategies, we collected annotations across four
classes - invasive epithelium, non-invasive epithelium, necrosis, and other -
with particular focus on morphologies underrepresented in existing datasets,
such as ductal carcinoma in situ and dispersed lobular tumor cells. The
dataset's diversity and relevance to the rapidly growing field of automated
biomarker quantification in breast cancer ensure its high potential for reuse.
Finally, we provide a well-curated, multicentric external evaluation set to
enable standardized benchmarking of breast cancer segmentation models.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [90] [JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator](https://arxiv.org/abs/2510.01213)
*Tao Han,Ang Li,Qinyu Chen,Chang Gao*

Main category: eess.SP

TL;DR: JaneEye是一种基于事件相机的低功耗眼动追踪硬件加速器，专为XR可穿戴设备设计，通过简化的神经网络架构和硬件优化，实现了高精度（2.45像素误差）和低延迟（0.5毫秒）的性能。


<details>
  <summary>Details</summary>
Motivation: 传统帧式眼动追踪系统在XR应用中难以满足高精度、低延迟和低功耗的要求，而事件相机具有高时间分辨率和低功耗的优势，适合用于可穿戴设备的眼动追踪。

Method: 提出了一种超轻量级神经网络架构，包含新颖的ConvJANET层（仅保留遗忘门的简化ConvLSTM），将计算复杂度减半；采用自定义线性近似激活函数和定点量化；通过软硬件协同设计实现12纳米ASIC芯片。

Result: 在3ET+数据集上达到2.45像素误差，仅使用17.6K参数，事件帧率高达1250 Hz；ASIC芯片在400 MHz频率下运行，端到端延迟0.5毫秒（相当于2000 FPS），能效为18.9 μJ/帧。

Conclusion: JaneEye为下一代XR可穿戴设备设定了低功耗高性能眼动追踪解决方案的新基准，展示了事件相机和硬件加速器在眼动追踪领域的巨大潜力。

Abstract: Eye tracking has become a key technology for gaze-based interactions in
Extended Reality (XR). However, conventional frame-based eye-tracking systems
often fall short of XR's stringent requirements for high accuracy, low latency,
and energy efficiency. Event cameras present a compelling alternative, offering
ultra-high temporal resolution and low power consumption. In this paper, we
present JaneEye, an energy-efficient event-based eye-tracking hardware
accelerator designed specifically for wearable devices, leveraging sparse,
high-temporal-resolution event data. We introduce an ultra-lightweight neural
network architecture featuring a novel ConvJANET layer, which simplifies the
traditional ConvLSTM by retaining only the forget gate, thereby halving
computational complexity without sacrificing temporal modeling capability. Our
proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+
dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To
further enhance hardware efficiency, we employ custom linear approximations of
activation functions (hardsigmoid and hardtanh) and fixed-point quantization.
Through software-hardware co-design, our 12-nm ASIC implementation operates at
400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames
Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a
new benchmark in low-power, high-performance eye-tracking solutions suitable
for integration into next-generation XR wearables.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [91] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了一种新的视频帧插值质量评估指标PSNR_DIV，通过运动散度加权增强PSNR，在保持高效率的同时显著提升了与人类感知的相关性。


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值质量评估指标（如PSNR、SSIM、LPIPS）忽略了时间一致性，而专门针对视频帧插值的指标如FloLPIPS又存在计算效率低的问题，限制了实际应用。

Method: 基于运动散度加权的PSNR改进方法，该方法源自档案胶片修复技术，通过检测运动场中的奇异性来加权图像误差。

Result: 在BVI-VFI数据集上评估显示，PSNR_DIV相比FloLPIPS在Pearson线性相关系数上提升0.09，同时速度提升2.5倍，内存使用减少4倍，且在不同内容类别和运动估计器下表现稳定。

Conclusion: PSNR_DIV的高效性和准确性使其能够快速评估视频帧插值质量，并可作为神经网络训练中的损失函数，具有实际应用价值。

Abstract: Video frame interpolation is a fundamental tool for temporal video
enhancement, but existing quality metrics struggle to evaluate the perceptual
impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and
LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored
towards video frame interpolation, like FloLPIPS, have been developed but
suffer from computational inefficiency that limits their practical application.
We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric
that enhances PSNR through motion divergence weighting, a technique adapted
from archival film restoration where it was developed to detect temporal
inconsistencies. Our approach highlights singularities in motion fields which
is then used to weight image errors. Evaluation on the BVI-VFI dataset (180
sequences across multiple frame rates, resolutions and interpolation methods)
shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant
improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while
being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains
consistent across all content categories and are robust to the motion estimator
used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast
quality evaluation and practical use as a loss function for training neural
networks for video frame interpolation tasks. An implementation of our metric
is available at www.github.com/conalld/psnr-div.

</details>


### [92] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: M2M是一种针对结构化噪声的零样本去噪框架，通过生成伪独立子图像对和自适应滤波来消除各向异性相关噪声，无需训练数据即可实现有效去噪。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以处理具有强各向异性相关的结构化噪声，数据驱动方法需要大量高质量标签且泛化能力有限，而零样本方法仅适用于独立同分布噪声。

Method: 提出Median2Median框架，采用方向性插值和广义中值滤波生成伪独立子图像对，通过随机分配策略扩大采样空间并消除系统偏差，适用于Noise2Noise训练。

Result: 在真实模拟研究中，M2M在独立同分布噪声下与最先进的零样本方法表现相当，在相关噪声下始终优于现有方法。

Conclusion: M2M为结构化噪声抑制提供了一种高效、无需数据的解决方案，是超越严格独立同分布假设的有效零样本去噪的第一步。

Abstract: Image denoising is a fundamental problem in computer vision and medical
imaging. However, real-world images are often degraded by structured noise with
strong anisotropic correlations that existing methods struggle to remove. Most
data-driven approaches rely on large datasets with high-quality labels and
still suffer from limited generalizability, whereas existing zero-shot methods
avoid this limitation but remain effective only for independent and identically
distributed (i.i.d.) noise. To address this gap, we propose Median2Median
(M2M), a zero-shot denoising framework designed for structured noise. M2M
introduces a novel sampling strategy that generates pseudo-independent
sub-image pairs from a single noisy input. This strategy leverages directional
interpolation and generalized median filtering to adaptively exclude values
distorted by structured artifacts. To further enlarge the effective sampling
space and eliminate systematic bias, a randomized assignment strategy is
employed, ensuring that the sampled sub-image pairs are suitable for
Noise2Noise training. In our realistic simulation studies, M2M performs on par
with state-of-the-art zero-shot methods under i.i.d. noise, while consistently
outperforming them under correlated noise. These findings establish M2M as an
efficient, data-free solution for structured noise suppression and mark the
first step toward effective zero-shot denoising beyond the strict i.i.d.
assumption.

</details>


### [93] [GFSR-Net: Guided Focus via Segment-Wise Relevance Network for Interpretable Deep Learning in Medical Imaging](https://arxiv.org/abs/2510.01919)
*Jhonatan Contreras,Thomas Bocklitz*

Main category: eess.IV

TL;DR: GFSR-Net是一种提高医学图像分析可解释性的方法，通过少量人工标注引导模型关注诊断相关区域，减少对无关模式的依赖。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中取得了显著成功，但其在临床实践中的应用受到可解释性不足的限制。模型可能做出正确预测但无法解释其推理过程，或依赖与疾病无关的图像区域或现实世界中不存在的视觉线索，这会降低信任度并增加误诊风险。

Method: GFSR-Net使用少量人工标注来近似人类在图像中的直观关注区域，无需精确边界或详尽标记。在训练过程中，模型学习将其关注点与这些区域对齐，逐步强调具有诊断意义的特征。这种方法适用于不同类型的自然和医学图像，包括胸部X光片、视网膜扫描和皮肤病图像。

Result: 实验表明，GFSR在保持可比或更优准确性的同时，产生的显著图能更好地反映人类期望，减少对无关模式的依赖，并提高对自动化诊断工具的信心。

Conclusion: GFSR-Net通过引导模型关注诊断相关区域，提高了医学图像分析的可解释性和可靠性，有助于增强临床实践中对深度学习模型的信任。

Abstract: Deep learning has achieved remarkable success in medical image analysis,
however its adoption in clinical practice is limited by a lack of
interpretability. These models often make correct predictions without
explaining their reasoning. They may also rely on image regions unrelated to
the disease or visual cues, such as annotations, that are not present in
real-world conditions. This can reduce trust and increase the risk of
misleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance
Network (GFSR-Net), an approach designed to improve interpretability and
reliability in medical imaging. GFSR-Net uses a small number of human
annotations to approximate where a person would focus within an image
intuitively, without requiring precise boundaries or exhaustive markings,
making the process fast and practical. During training, the model learns to
align its focus with these areas, progressively emphasizing features that carry
diagnostic meaning. This guidance works across different types of natural and
medical images, including chest X-rays, retinal scans, and dermatological
images. Our experiments demonstrate that GFSR achieves comparable or superior
accuracy while producing saliency maps that better reflect human expectations.
This reduces the reliance on irrelevant patterns and increases confidence in
automated diagnostic tools.

</details>


### [94] [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](https://arxiv.org/abs/2510.02109)
*Jong Bum Won,Wesley De Neve,Joris Vankerschaver,Utku Ozbulak*

Main category: eess.IV

TL;DR: 该论文提出了SpurBreast数据集，这是一个专门设计的乳腺MRI数据集，旨在研究深度神经网络在医学影像中学习虚假相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在医学影像中表现出色，但容易学习虚假相关性而非有意义的医学模式。现有医学影像数据集缺乏系统研究此问题的能力，主要受限于许可限制和患者数据不足。

Method: 通过分析100多个患者、设备和成像协议特征，识别出两个主要的虚假信号：磁场强度（全局特征）和图像方向（局部特征）。创建了包含和不包含虚假相关性的数据集分割，用于评估模型性能。

Result: 研究表明DNNs会利用这些非临床信号，在验证集上获得高准确率，但在无偏测试数据上泛化能力差。

Conclusion: SpurBreast数据集为研究人员提供了系统研究临床相关和不相关特征、不确定性估计、对抗鲁棒性和泛化策略的平台。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in medical
imaging, yet their real-world deployment remains challenging due to spurious
correlations, where models can learn non-clinical features instead of
meaningful medical patterns. Existing medical imaging datasets are not designed
to systematically study this issue, largely due to restrictive licensing and
limited supplementary patient data. To address this gap, we introduce
SpurBreast, a curated breast MRI dataset that intentionally incorporates
spurious correlations to evaluate their impact on model performance. Analyzing
over 100 features involving patient, device, and imaging protocol, we identify
two dominant spurious signals: magnetic field strength (a global feature
influencing the entire image) and image orientation (a local feature affecting
spatial alignment). Through controlled dataset splits, we demonstrate that DNNs
can exploit these non-clinical signals, achieving high validation accuracy
while failing to generalize to unbiased test data. Alongside these two datasets
containing spurious correlations, we also provide benchmark datasets without
spurious correlations, allowing researchers to systematically investigate
clinically relevant and irrelevant features, uncertainty estimation,
adversarial robustness, and generalization strategies. Models and datasets are
available at https://github.com/utkuozbulak/spurbreast.

</details>


### [95] [Measurement-Guided Consistency Model Sampling for Inverse Problems](https://arxiv.org/abs/2510.02208)
*Amirreza Tanevardi,Pooria Abbas Rad Moghadam,Sajjad Amini*

Main category: eess.IV

TL;DR: 提出了一种针对逆问题重建的改进一致性采样方法，通过测量一致性机制引导采样随机性，在保持高效性的同时提高重建质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在解决逆成像问题时依赖缓慢的多步采样，一致性模型虽然能实现高质量单步生成，但在逆问题中的应用尚未充分探索

Method: 修改一致性采样方法，将采样器的随机性通过测量一致性机制与测量算子关联，强制重建结果与测量数据保持一致性

Result: 在Fashion-MNIST和LSUN Bedroom数据集上的实验显示，在感知和像素级指标上均有稳定提升，仅需少量步骤即可获得竞争性或更优的重建结果

Conclusion: 该方法有效结合了逆问题重建的测量一致性和一致性模型的高效生成能力，实现了高质量快速重建

Abstract: Diffusion models have become powerful generative priors for solving inverse
imaging problems, but their reliance on slow multi-step sampling limits
practical deployment. Consistency models address this bottleneck by enabling
high-quality generation in a single or only a few steps, yet their direct
adaptation to inverse problems is underexplored. In this paper, we present a
modified consistency sampling approach tailored for inverse problem
reconstruction: the sampler's stochasticity is guided by a
measurement-consistency mechanism tied to the measurement operator, which
enforces fidelity to the acquired measurements while retaining the efficiency
of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom
datasets demonstrate consistent improvements in perceptual and pixel-level
metrics, including Fr\'echet Inception Distance, Kernel Inception Distance,
peak signal-to-noise ratio, and structural similarity index measure, compared
to baseline consistency sampling, yielding competitive or superior
reconstructions with only a handful of steps.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [96] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个从多视角视频创建3D人体化身的框架，通过基于物质点法的物理模拟器实现高真实感的服装动态建模和逼真渲染，在动态建模精度、渲染质量和鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的3D人体化身方法在建模宽松服装的物理动态时存在精度不足或对新动画输入鲁棒性差的问题，需要更准确和鲁棒的动态建模方案。

Method: 使用基于物质点法的定制化模拟器，结合各向异性本构模型和新颖的碰撞处理算法来建模复杂服装变形和身体接触；结合可渲染的规范化身，使用带准阴影的3D高斯溅射进行高保真渲染。

Result: MPMAvatar在动态建模精度、渲染精度、鲁棒性和效率方面显著优于现有最先进的基于物理的化身方法，并能以零样本方式泛化到未见过的交互场景。

Conclusion: 该框架成功解决了宽松服装物理动态建模的挑战，实现了高真实感的3D人体化身创建，为物理逼真动画提供了有效的解决方案。

Abstract: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [97] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一种针对感兴趣区域的高效3D高斯泼溅重建方法，通过对象感知框架提升局部细节质量并减少模型大小


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在场景中均匀分配资源，导致感兴趣区域的细节有限且模型规模过大

Method: 提出ROI-GS框架，包括对象引导的相机选择、针对性对象训练以及将高保真对象重建无缝集成到全局场景中

Result: 实验显示ROI-GS显著提升局部质量（PSNR最高提升2.96 dB），模型大小减少约17%，训练速度更快

Conclusion: ROI-GS在保持实时性能的同时，能够优先处理选定对象的更高分辨率细节，优于现有方法

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

</details>


### [98] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种可重光照框架，将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，实现了更高质量的镜面物体重建和重光照效果。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法通常依赖简化的BRDF模型或耦合漫反射和镜面反射的参数化，限制了材料恢复的准确性和重光照保真度。镜面物体的形状、材料属性和光照难以分离是长期存在的挑战。

Method: 将微表面BRDF与高光-光泽度参数化集成到2D高斯泼溅中，采用延迟着色。使用基于扩散的表面法线和漫反射颜色先验指导早期优化，通过粗到细的环境图优化加速收敛并保持高动态范围镜面反射。

Result: 在复杂镜面场景上的实验表明，该方法实现了高质量的几何和材料重建，与现有高斯泼溅方法相比，在新光照下提供了更真实和一致的重光照效果。

Conclusion: 该框架通过物理一致的材质分解和有效的优化策略，显著提升了镜面物体的重建和重光照质量，为神经渲染领域提供了新的解决方案。

Abstract: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [99] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了如何利用领域专家提供的精炼知识来开发有效的智能辅导系统，重点关注通过可解释AI自动生成课程和基于专家制定课程的自适应辅导系统。


<details>
  <summary>Details</summary>
Motivation: AI教育社区经常忽视领域专家提供的精炼知识在创建有效辅导系统中的作用，本文旨在强调这一被忽视的重要资源。

Method: 提出了两种方法：1）利用可解释AI技术结合专家指定的解题规则自动生成课程；2）基于专家制定的学习课程开发自适应辅导系统。

Result: 通过传粉昆虫识别辅导系统的案例研究，证明了这些方法的可行性和重要性。

Conclusion: 专家精炼知识在开发智能教育系统中具有重要价值，能够提升学习体验并提高系统开发效率。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [100] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 本文提出了一种基于LLM引导的硬负样本生成框架VaPR，用于解决合成偏好标注中的噪声问题，显著提升了大型视觉语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的偏好微调方法（如DPO）忽视了合成偏好标注中普遍存在的风格和长度偏差噪声，这限制了大型视觉语言模型与人类偏好的对齐效果。

Method: 开发了基于LLM引导的硬负样本响应编辑框架，生成具有针对性错误但保持风格和长度相似性的拒绝响应，构建了包含30K高质量样本的VaPR数据集。

Result: 在十个基准测试中，VaPR模型实现了显著性能提升：LLaVA平均提升6.5%，Qwen2VL提升4.0%，Qwen2.5VL提升1.5%，特别是在推理任务上表现突出。同时有效减少了二元问题中的"是"回答倾向。

Conclusion: VaPR框架不仅显著提升了LVLM性能，还展示了良好的可扩展性和泛化能力，使用开源LLM作为编辑器也能达到接近GPT-4o的性能水平。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [101] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR（可验证奖励的强化学习）虽然旨在提升大语言模型的推理能力，但研究发现其反而会缩小推理边界。本文通过分析RLVR的学习动态，揭示了负干扰和赢家通吃现象，并提出了一种针对低似然问题的数据筛选算法，显著提升了Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: RLVR方法在提升大语言模型推理能力时出现推理边界缩小的悖论现象，本文旨在深入探究这一问题的成因及其学习动态机制。

Method: 通过理论和实证分析RLVR的学习动态，识别出负干扰和赢家通吃两个关键现象，并基于此提出了一种专注于低似然问题的数据筛选算法。

Result: 实验表明，所提出的数据筛选算法能有效缓解RLVR的负干扰问题，在多个数学推理基准测试中显著提升了Pass@k性能。

Conclusion: RLVR的失败源于标准RL目标中的固有策略采样导致模型收敛于狭窄的解策略，而专注于低似然问题的数据筛选方法能有效改善这一局限性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [102] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN) 方法通过生成多个代理轨迹并使用行为叙述进行选择，显著提高了计算机使用代理在复杂任务中的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了在长期复杂任务中的应用。

Method: bBoN 方法通过生成多个代理轨迹，使用描述代理轨迹的行为叙述来进行选择，实现广泛探索和原则性轨迹选择。

Result: 在 OSWorld 上达到 69.9% 的新 SOTA，接近人类水平 72%，在 WindowsAgentArena 和 AndroidWorld 上展示出强泛化能力。

Conclusion: 有效扩展计算机使用代理需要结构化的轨迹理解和选择，bBoN 提供了一个实用的框架来实现这一目标。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [103] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 提出了首个基于扩散模型的光子带结构生成方法，使用transformer编码器和潜在扩散模型从输入结构生成对应的光子带图，解决了传统数值方法计算成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 光子晶体在光子技术和量子技术中至关重要，但计算光子带图需要求解大量Maxwell方程，数值计算成本高昂，特别是在逆向设计优化循环中。

Method: 将transformer编码器与潜在扩散模型相结合，transformer提取输入结构的上下文嵌入，扩散模型生成对应的光子带图。

Result: 开发了能够生成光子带图的方法，并展示了transformer和扩散模型在捕捉光子学中复杂干涉和散射现象方面的适用性。

Conclusion: 该方法为光子学领域提供了新的代理建模策略，能够扩展到任意三维结构，为光子带图计算提供了高效的替代方案。

Abstract: Photonic crystals enable fine control over light propagation at the
nanoscale, and thus play a central role in the development of photonic and
quantum technologies. Photonic band diagrams (BDs) are a key tool to
investigate light propagation into such inhomogeneous structured materials.
However, computing BDs requires solving Maxwell's equations across many
configurations, making it numerically expensive, especially when embedded in
optimization loops for inverse design techniques, for example. To address this
challenge, we introduce the first approach for BD generation based on diffusion
models, with the capacity to later generalize and scale to arbitrary three
dimensional structures. Our method couples a transformer encoder, which
extracts contextual embeddings from the input structure, with a latent
diffusion model to generate the corresponding BD. In addition, we provide
insights into why transformers and diffusion models are well suited to capture
the complex interference and scattering phenomena inherent to photonics, paving
the way for new surrogate modeling strategies in this domain.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [104] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一个视觉语言导航系统，通过微调预训练的扩散模型进行路径规划，生成视觉路径掩码，再由轻量级策略执行轨迹，在真实世界评估中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在机器人导航任务中难以迁移的问题，因为动作空间差异和预训练目标不匹配阻碍了在机器人任务中的应用。

Method: 使用预训练的扩散模型进行微调，生成路径掩码作为视觉规划，配合轻量级的行为克隆策略将视觉规划转化为可执行轨迹。通过自监督跟踪模型和VLM增强的标注进行训练，避免人工标注。

Result: 在真实世界评估中，VENTURA在物体到达、障碍物避让和地形偏好任务上优于最先进的基础模型基线，成功率提高33%，碰撞率降低54%，并能泛化到未见过的任务组合。

Conclusion: VENTURA展示了在开放世界环境中遵循自然语言指令生成多样化机器人行为的能力，并展现出新兴的组合能力。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [105] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个用于机器人复杂双手操作的数据收集框架，通过VR遥操作和传感器化控制器将人类演示转移到机器人上，实现了主动的自我中心感知记录。


<details>
  <summary>Details</summary>
Motivation: 为了解决复杂双手操作任务的机器人学习问题，需要高质量的人类演示数据。传统方法往往缺乏便携性和对视觉注意力的捕捉，限制了机器人在真实世界中的泛化能力。

Method: 开发了便携式VR遥操作套件，包含传感器化控制器来镜像机器人末端执行器，通过精确姿态对齐桥接人机运动学。系统采用沉浸式3D模型渲染、自包含可穿戴计算机和高效校准方法，并记录操作者的头部运动来捕捉视觉注意力与操作的关联。

Result: 在六个挑战性双手任务上评估，仅使用ActiveUMI数据训练的策略在分布内任务上达到70%的平均成功率，在新物体和新环境中保持56%的成功率，显示出强大的泛化能力。

Conclusion: 便携式数据收集系统结合学习的主动感知，为创建可泛化且能力强的真实世界机器人策略提供了有效且可扩展的途径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [106] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout是一个用于3D室内布局合成的新框架，通过解耦和协调物理与语义细化来解决传统方法的泛化性不足问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法由于固定数据集而泛化能力差，而基于LLM和VLM的方法虽然语义丰富但缺乏灵活有效的细化机制，导致布局不理想。

Method: 开发了DisCo-Layout框架，包含语义细化工具(SRT)修正抽象对象关系，物理细化工具(PRT)通过网格匹配算法解决空间问题，并采用多智能体框架协调这些工具。

Result: 实验表明DisCo-Layout在生成真实、连贯且可泛化的3D室内布局方面达到最先进性能。

Conclusion: DisCo-Layout通过解耦和协调物理与语义细化，有效解决了3D室内布局合成的关键挑战，代码将公开可用。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [107] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 论文研究通过显式地将策略与相机外参条件化来实现视角不变模仿学习，发现该方法能显著提升行为克隆策略在不同视角下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习策略在视角变化时性能下降，因为它们可能依赖静态背景中的视觉线索来推断相机姿态，当工作空间几何或相机位置发生变化时这种捷径会失效。

Method: 使用Plucker嵌入对每个像素的光线进行编码，将策略与相机外参条件化，并在RoboSuite和ManiSkill中引入六个操作任务来评估视角变化下的策略鲁棒性。

Result: 实验表明，没有外参条件的策略在固定场景中依赖背景线索推断相机姿态，但在视角变化时性能崩溃；而条件化外参的策略恢复了性能，实现了仅使用RGB的鲁棒控制。

Conclusion: 显式条件化相机外参可以显著提高模仿学习策略的视角不变性，提供无需深度信息的鲁棒RGB控制方案。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [108] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本文提出了一种在低资源多模态环境中通过模型融合技术来提升多模态模型在纯语言任务上的性能的方法，解决了多模态模型在语法等语言任务上表现不佳的问题。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的视觉语言模型参数量巨大且训练数据远超儿童语言习得的数据量，存在与人类语言学习过程的差异。同时多模态模型在纯语言任务上表现不佳，需要解决这一性能下降问题。

Method: 使用发展合理的数据集在低资源设置下开发纯语言和多模态模型，并通过加权线性插值进行模型融合，将多模态模型与纯语言模型的参数进行融合。

Result: 多模态模型在BabyLM基准测试中表现优于之前的基线，但确实在纯语言语法任务上表现不佳。模型融合技术在一定程度上缓解了这个问题，同时保持了多模态性能。

Conclusion: 模型融合是解决多模态模型在纯语言任务上性能下降的有效方法，能够在保持多模态能力的同时提升语言任务表现，为构建更平衡的多模态模型提供了可行方案。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [109] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个用于系统化基准测试、分析和解释视觉语言模型（VLMs）的工具包，支持从开源VLMs的前向传播过程中提取任意层的中间输出。


<details>
  <summary>Details</summary>
Motivation: 为了促进对视觉语言模型的深入理解和改进，需要一种能够统一提取和分析模型中间表示的工具。

Method: 提供基于YAML配置的统一接口，抽象模型特定复杂性，支持16种最先进的基干VLMs及其30多个变体，并可扩展支持新模型。

Result: 通过两个简单分析实验展示了工具的使用，揭示了VLMs在不同层和目标概念上的隐藏表示存在系统性差异。

Conclusion: VLM-Lens作为开源项目发布，旨在加速社区对VLMs的理解和改进工作。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [110] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是一种基于ZK-SNARKs的图像生成模型水印系统，能够在保护模型权重和生成提示等敏感信息的同时，提供可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型变得强大且易用，合成媒体的真实性、所有权和滥用问题日益严重。传统水印方法要么降低图像质量，要么容易被移除，或需要访问模型内部机密信息，无法满足安全可扩展部署的需求。

Method: 提出选择性层ZK电路创建(SL-ZKCC)方法，选择性地将图像生成模型的关键层转换为电路，显著减少证明生成时间。生成的ZK-SNARK证明通过最低有效位(LSB)隐写术不可察觉地嵌入到生成图像中。

Result: 该系统在GAN和扩散模型上进行了验证，提供了一个安全、模型无关的可信AI图像生成流水线。

Conclusion: ZK-WAGON为图像生成模型提供了一种安全、可验证的水印解决方案，能够在保护隐私的同时确保生成内容的可信度。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [111] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 该研究揭示了预训练视频模型在社交感知方面与人类认知的差距，发现基于文本的模型比视觉模型更接近人类相似性判断。通过人类行为数据微调视频模型，可以有效缩小这一差距并提升社交属性的编码能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索现代视频和语言模型是否能捕捉人类对社交视频的相似性感知，以及如何利用人类行为数据来改进模型的社会认知能力。

Method: 引入包含49,000多个三元组相似性判断的新基准数据集，使用新颖的混合三元组-RSA目标函数，结合低秩适应(LoRA)技术对TimeSformer视频模型进行微调。

Result: 微调后的视频模型在保持视频上显著提升了与人类感知的对齐度，方差分解显示模型既增加了与语言嵌入的共享方差，又解释了语言模型未捕获的独特方差。线性探针测试表明微调增强了社交情感属性的编码能力。

Conclusion: 预训练视频模型在社交识别方面存在明显差距，基于人类相似性的行为引导微调能够有效塑造视频表示，使其更接近人类的社会感知。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it
remains unclear whether state-of-the-art AI models encode the same similarity
structure. We study (Q1) whether modern video and language models capture
human-perceived similarity in social videos, and (Q2) how to instill this
structure into models using human behavioral data. To address this, we
introduce a new benchmark of over 49,000 odd-one-out similarity judgments on
250 three-second video clips of social interactions, and discover a modality
gap: despite the task being visual, caption-based language embeddings align
better with human similarity than any pretrained video model. We close this gap
by fine-tuning a TimeSformer video model on these human judgments with our
novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning
pairwise distances to human similarity. This fine-tuning protocol yields
significantly improved alignment with human perceptions on held-out videos in
terms of both explained variance and odd-one-out triplet accuracy. Variance
partitioning shows that the fine-tuned video model increases shared variance
with language embeddings and explains additional unique variance not captured
by the language model. Finally, we test transfer via linear probes and find
that human-similarity fine-tuning strengthens the encoding of social-affective
attributes (intimacy, valence, dominance, communication) relative to the
pretrained baseline. Overall, our findings highlight a gap in pretrained video
models' social recognition and demonstrate that behavior-guided fine-tuning
shapes video representations toward human social perception.

</details>


### [112] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: 提出了MIG-Vis方法，利用扩散模型可视化并验证神经潜在子空间中编码的视觉语义属性，发现了高阶视觉皮层中结构化的语义表征。


<details>
  <summary>Details</summary>
Motivation: 理解高阶视觉区域中神经群体如何编码以物体为中心的视觉信息是计算神经科学的核心挑战。现有方法无法直接揭示神经群体本身的组织结构。

Method: 使用变分自编码器从神经群体中推断出群体间解耦的神经潜在子空间，然后提出互信息引导的扩散合成程序来可视化每个潜在组编码的特定视觉语义特征。

Result: 在两只猕猴的颞下皮层多会话神经发放数据集上验证，发现方法识别出的神经潜在组对多种视觉特征具有清晰的语义选择性，包括物体姿态、类别间转换和类内内容。

Conclusion: 这些发现为高阶视觉皮层中结构化的语义表征提供了直接、可解释的证据，并推进了对其编码原理的理解。

Abstract: Understanding how neural populations in higher visual areas encode
object-centered visual information remains a central challenge in computational
neuroscience. Prior works have investigated representational alignment between
artificial neural networks and the visual cortex. Nevertheless, these findings
are indirect and offer limited insights to the structure of neural populations
themselves. Similarly, decoding-based methods have quantified semantic features
from neural populations but have not uncovered their underlying organizations.
This leaves open a scientific question: "how feature-specific visual
information is distributed across neural populations in higher visual areas,
and whether it is organized into structured, semantically meaningful
subspaces." To tackle this problem, we present MIG-Vis, a method that leverages
the generative power of diffusion models to visualize and validate the
visual-semantic attributes encoded in neural latent subspaces. Our method first
uses a variational autoencoder to infer a group-wise disentangled neural latent
subspace from neural populations. Subsequently, we propose a mutual information
(MI)-guided diffusion synthesis procedure to visualize the specific
visual-semantic features encoded by each latent group. We validate MIG-Vis on
multi-session neural spiking datasets from the inferior temporal (IT) cortex of
two macaques. The synthesized results demonstrate that our method identifies
neural latent groups with clear semantic selectivity to diverse visual
features, including object pose, inter-category transformations, and
intra-class content. These findings provide direct, interpretable evidence of
structured semantic representation in the higher visual cortex and advance our
understanding of its encoding principles.

</details>
