<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.LG](#cs.LG) [Total: 4]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Vision-Based Perception for Autonomous Vehicles in Off-Road Environment Using Deep Learning](https://arxiv.org/abs/2509.19378)
*Nelson Alves Ferreira Neto*

Main category: cs.CV

TL;DR: 提出了一种用于非铺装道路和越野环境的感知系统CMSNet，能够实时分割障碍物和可通行地面，并在恶劣条件下工作。


<details>
  <summary>Details</summary>
Motivation: 为露天矿场和发展中国家的自动驾驶提供低延迟智能系统，解决非均匀地形和恶劣环境下的导航问题。

Method: 使用可配置模块化分割网络（CMSNet）框架，通过TensorRT、C++和CUDA优化实现实时推理，并提出了包含近12,000张图像的新数据集Kamino。

Result: 在两个数据集上的实证实验验证了系统的有效性，实现了恶劣条件下的实时语义分割。

Conclusion: CMSNet系统为非铺装道路和越野环境下的自动驾驶提供了有效的感知解决方案，特别是在恶劣能见度条件下表现出色。

Abstract: Low-latency intelligent systems are required for autonomous driving on
non-uniform terrain in open-pit mines and developing countries. This work
proposes a perception system for autonomous vehicles on unpaved roads and
off-road environments, capable of navigating rough terrain without a predefined
trail. The Configurable Modular Segmentation Network (CMSNet) framework is
proposed, facilitating different architectural arrangements. CMSNet
configurations were trained to segment obstacles and trafficable ground on new
images from unpaved/off-road scenarios with adverse conditions (night, rain,
dust). We investigated applying deep learning to detect drivable regions
without explicit track boundaries, studied algorithm behavior under visibility
impairment, and evaluated field tests with real-time semantic segmentation. A
new dataset, Kamino, is presented with almost 12,000 images from an operating
vehicle with eight synchronized cameras. The Kamino dataset has a high number
of labeled pixels compared to similar public collections and includes images
from an off-road proving ground emulating a mine under adverse visibility. To
achieve real-time inference, CMSNet CNN layers were methodically removed and
fused using TensorRT, C++, and CUDA. Empirical experiments on two datasets
validated the proposed system's effectiveness.

</details>


### [2] [Overview of LifeCLEF Plant Identification task 2020](https://arxiv.org/abs/2509.19402)
*Herve Goeau,Pierre Bonnet,Alexis Joly*

Main category: cs.CV

TL;DR: LifeCLEF 2020植物识别挑战评估了利用植物标本馆数据改进热带地区植物自动识别的可行性，通过跨域分类任务结合标本馆标本和野外照片进行训练。


<details>
  <summary>Details</summary>
Motivation: 解决热带地区植物识别数据不足问题，利用丰富的植物标本馆数字化资源来提升自动化识别系统在生物多样性丰富但数据稀缺地区的性能。

Method: 采用跨域分类方法，训练集包含数十万份植物标本馆标本和数千张野外照片，测试集仅包含野外照片，旨在学习两个域之间的映射关系。

Result: 挑战基于南美洲圭亚那地盾地区的约1000种植物数据集进行，该地区拥有世界最丰富的植物多样性之一。

Conclusion: 植物标本馆收藏可以为数据稀缺的热带地区植物自动识别提供有价值的补充数据源，跨域学习方法有望解决训练数据分布不均的问题。

Abstract: Automated identification of plants has improved considerably thanks to the
recent progress in deep learning and the availability of training data with
more and more photos in the field. However, this profusion of data only
concerns a few tens of thousands of species, mostly located in North America
and Western Europe, much less in the richest regions in terms of biodiversity
such as tropical countries. On the other hand, for several centuries, botanists
have collected, catalogued and systematically stored plant specimens in
herbaria, particularly in tropical regions, and the recent efforts by the
biodiversity informatics community made it possible to put millions of
digitized sheets online. The LifeCLEF 2020 Plant Identification challenge (or
"PlantCLEF 2020") was designed to evaluate to what extent automated
identification on the flora of data deficient regions can be improved by the
use of herbarium collections. It is based on a dataset of about 1,000 species
mainly focused on the South America's Guiana Shield, an area known to have one
of the greatest diversity of plants in the world. The challenge was evaluated
as a cross-domain classification task where the training set consist of several
hundred thousand herbarium sheets and few thousand of photos to enable learning
a mapping between the two domains. The test set was exclusively composed of
photos in the field. This paper presents the resources and assessments of the
conducted evaluation, summarizes the approaches and systems employed by the
participating research groups, and provides an analysis of the main outcomes.

</details>


### [3] [iFinder: Structured Zero-Shot Vision-Based LLM Grounding for Dash-Cam Video Reasoning](https://arxiv.org/abs/2509.19552)
*Manyi Yao,Bingbing Zhuang,Sparsh Garg,Amit Roy-Chowdhury,Christian Shelton,Manmohan Chandraker,Abhishek Aich*

Main category: cs.CV

TL;DR: iFinder是一个结构化语义理解框架，通过将行车记录仪视频转换为分层可解释数据结构，让LLM能够进行领域特定的推理，显著提升事故推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言模型在行车记录仪视频分析中存在空间推理、因果推断和可解释性不足的问题，特别是在只有视觉模态的情况下。

Method: 采用模块化、无需训练的方法，使用预训练视觉模型提取关键线索（物体姿态、车道位置、物体轨迹），并组织成层次化结构，结合三块提示策略实现逐步推理。

Result: 在四个公开行车记录仪基准测试中，iFinder显著优于端到端视频-语言模型，事故推理准确率提升高达39%。

Conclusion: 通过领域特定的表示方法，iFinder为零样本、可解释的行车视频理解提供了可靠替代方案。

Abstract: Grounding large language models (LLMs) in domain-specific tasks like post-hoc
dash-cam driving video analysis is challenging due to their general-purpose
training and lack of structured inductive biases. As vision is often the sole
modality available for such analysis (i.e., no LiDAR, GPS, etc.), existing
video-based vision-language models (V-VLMs) struggle with spatial reasoning,
causal inference, and explainability of events in the input video. To this end,
we introduce iFinder, a structured semantic grounding framework that decouples
perception from reasoning by translating dash-cam videos into a hierarchical,
interpretable data structure for LLMs. iFinder operates as a modular,
training-free pipeline that employs pretrained vision models to extract
critical cues -- object pose, lane positions, and object trajectories -- which
are hierarchically organized into frame- and video-level structures. Combined
with a three-block prompting strategy, it enables step-wise, grounded reasoning
for the LLM to refine a peer V-VLM's outputs and provide accurate reasoning.
Evaluations on four public dash-cam video benchmarks show that iFinder's
proposed grounding with domain-specific cues, especially object orientation and
global context, significantly outperforms end-to-end V-VLMs on four zero-shot
driving benchmarks, with up to 39% gains in accident reasoning accuracy. By
grounding LLMs with driving domain-specific representations, iFinder offers a
zero-shot, interpretable, and reliable alternative to end-to-end V-VLMs for
post-hoc driving video understanding.

</details>


### [4] [CURE: Centroid-guided Unsupervised Representation Erasure for Facial Recognition Systems](https://arxiv.org/abs/2509.19562)
*Fnu Shivam,Nima Najafzadeh,Yenumula Reddy,Prashnna Gyawali*

Main category: cs.CV

TL;DR: CURE是首个用于人脸识别系统的无监督遗忘框架，无需身份标签即可有效移除特定样本，同时保持整体模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前人脸识别系统广泛使用但存在隐私问题，现有机器遗忘技术依赖监督方法需要身份标签，这在隐私受限或大规模嘈杂数据集中往往不可得。

Method: 提出CURE（Centroid-guided Unsupervised Representation Erasure）框架，通过质心引导的无监督表示擦除方法，无需身份标签即可实现遗忘。同时提出新的评估指标UES（Unlearning Efficiency Score）来平衡遗忘和保留稳定性。

Result: CURE显著优于现有遗忘方法的无监督变体。通过将低质量图像指定为遗忘集进行质量感知遗忘，证明了其可用性和优势，并突出了图像质量在机器遗忘中的作用。

Conclusion: CURE填补了无监督机器遗忘的空白，为人脸识别系统提供了有效的隐私保护解决方案，同时提出的UES指标解决了当前评估指标的不足。

Abstract: In the current digital era, facial recognition systems offer significant
utility and have been widely integrated into modern technological
infrastructures; however, their widespread use has also raised serious privacy
concerns, prompting regulations that mandate data removal upon request. Machine
unlearning has emerged as a powerful solution to address this issue by
selectively removing the influence of specific user data from trained models
while preserving overall model performance. However, existing machine
unlearning techniques largely depend on supervised techniques requiring
identity labels, which are often unavailable in privacy-constrained situations
or in large-scale, noisy datasets. To address this critical gap, we introduce
CURE (Centroid-guided Unsupervised Representation Erasure), the first
unsupervised unlearning framework for facial recognition systems that operates
without the use of identity labels, effectively removing targeted samples while
preserving overall performance. We also propose a novel metric, the Unlearning
Efficiency Score (UES), which balances forgetting and retention stability,
addressing shortcomings in the current evaluation metrics. CURE significantly
outperforms unsupervised variants of existing unlearning methods. Additionally,
we conducted quality-aware unlearning by designating low-quality images as the
forget set, demonstrating its usability and benefits, and highlighting the role
of image quality in machine unlearning.

</details>


### [5] [Synthesizing Artifact Dataset for Pixel-level Detection](https://arxiv.org/abs/2509.19589)
*Dennis Menn,Feng Liang,Diana Marculescu*

Main category: cs.CV

TL;DR: 提出了一种通过自动在干净合成图像中注入伪影的方法来生成像素级标注，无需人工标注即可训练伪影检测器，在ConvNeXt和Swin-T上分别实现了13.2%和3.7%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 训练伪影检测器需要昂贵的像素级人工标注，缺乏标注数据限制了检测器性能。传统的伪标注方法存在噪声标签问题，导致性能不佳。

Method: 设计了一个伪影污染管道，自动在高质量合成图像的预定区域注入伪影，从而无需人工标注即可生成像素级标注数据。

Result: 该方法训练的伪影检测器在人类标注数据上验证，相比基线方法在ConvNeXt上提升13.2%，在Swin-T上提升3.7%。

Conclusion: 这项工作为实现可扩展的像素级伪影标注数据集迈出了重要一步，将世界知识整合到伪影检测中。

Abstract: Artifact detectors have been shown to enhance the performance of
image-generative models by serving as reward models during fine-tuning. These
detectors enable the generative model to improve overall output fidelity and
aesthetics. However, training the artifact detector requires expensive
pixel-level human annotations that specify the artifact regions. The lack of
annotated data limits the performance of the artifact detector. A naive
pseudo-labeling approach-training a weak detector and using it to annotate
unlabeled images-suffers from noisy labels, resulting in poor performance. To
address this, we propose an artifact corruption pipeline that automatically
injects artifacts into clean, high-quality synthetic images on a predetermined
region, thereby producing pixel-level annotations without manual labeling. The
proposed method enables training of an artifact detector that achieves
performance improvements of 13.2% for ConvNeXt and 3.7% for Swin-T, as verified
on human-labeled data, compared to baseline approaches. This work represents an
initial step toward scalable pixel-level artifact annotation datasets that
integrate world knowledge into artifact detection.

</details>


### [6] [Parameter-Efficient Multi-Task Learning via Progressive Task-Specific Adaptation](https://arxiv.org/abs/2509.19602)
*Neeraj Gangwar,Anshuka Rangi,Rishabh Deshmukh,Holakou Rahmanian,Yesh Dattatreya,Nickvash Kani*

Main category: cs.CV

TL;DR: 提出了一种渐进式任务特定的多任务适配方法，通过在不同层共享适配器模块并逐渐增加任务特异性，减少任务冲突，同时使用梯度相似性来分配相似任务到共享模块。


<details>
  <summary>Details</summary>
Motivation: 参数高效微调方法在单任务学习中表现良好，但在多任务学习中会加剧任务干扰和负迁移问题，因为可训练参数数量有限。

Method: 在预训练模型中引入适配器模块，这些模块在初始层共享于所有任务，在后续层逐渐变得更加任务特定。同时提出基于梯度的任务相似性计算方法，用于将相似任务分配到共享适配器模块。

Result: 在PASCAL和NYUD-v2数据集上的实验表明，该方法仅需五分之一的训练参数就能超越完全微调的多任务模型，在参数效率多任务学习方面超越了当前最先进方法。

Conclusion: 该渐进式任务特定多任务适配方法能有效减少任务冲突，实现更好的多任务学习性能，同时保持参数效率。

Abstract: Parameter-efficient fine-tuning methods have emerged as a promising solution
for adapting pre-trained models to various downstream tasks. While these
methods perform well in single-task learning, extending them to multi-task
learning exacerbates common challenges, such as task interference and negative
transfer, due to the limited number of trainable parameters. To address these
issues, we introduce progressive task-specific multi-task adaptation, a novel
parameter-efficient approach for multi-task learning. This approach introduces
adapter modules in a pre-trained model such that these modules are shared
across all tasks in the initial layers and become progressively more
task-specific in the later layers. The motivation is to reduce the conflicts
among tasks by allowing transfer learning across all tasks in the initial
layers and enabling task-specific learning toward the prediction heads.
Additionally, we propose a gradient-based approach for computing task
similarity and use this measure to allocate similar tasks to the shared adapter
modules. Our task similarity method introduces minimal overhead in the
pipeline. We evaluate our approach by adapting the Swin Transformer for dense
prediction tasks. Experiments on the PASCAL and NYUD-v2 datasets demonstrate
that our approach outperforms a fully fine-tuned multi-task model while
requiring only one-fifth of the trainable parameters. This approach achieves
better relative improvement to single-task fine-tuning while reducing the
number of trainable parameters and surpasses the current state-of-the-art
methods for parameter-efficient multi-task learning.

</details>


### [7] [Raw-JPEG Adapter: Efficient Raw Image Compression with JPEG](https://arxiv.org/abs/2509.19624)
*Mahmoud Afifi,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: RawJPEG Adapter是一种轻量级、可学习、可逆的预处理管道，将原始图像适配为标准JPEG压缩，通过存储在JPEG注释字段中的紧凑参数实现准确的原始重建。


<details>
  <summary>Details</summary>
Motivation: 原始数据保留完整传感器信息，但DNG等格式存储需求大；JPEG格式压缩效率高且兼容性好，但不适合存储原始数据。需要一种方法在保持JPEG优势的同时支持原始数据重建。

Method: 应用空间和可选频域变换，将紧凑参数存储在JPEG注释字段中，构建可逆的预处理管道来适配原始图像进行JPEG压缩。

Result: 在多个数据集上实验表明，该方法比直接JPEG存储具有更高保真度，支持其他编解码器，在压缩比和重建精度之间提供良好平衡。

Conclusion: RawJPEG Adapter为受限场景下的原始数据存储提供了实用解决方案，实现了JPEG格式的高效压缩与原始数据准确重建的平衡。

Abstract: Digital cameras digitize scene light into linear raw representations, which
the image signal processor (ISP) converts into display-ready outputs. While raw
data preserves full sensor information--valuable for editing and vision
tasks--formats such as Digital Negative (DNG) require large storage, making
them impractical in constrained scenarios. In contrast, JPEG is a widely
supported format, offering high compression efficiency and broad compatibility,
but it is not well-suited for raw storage. This paper presents RawJPEG Adapter,
a lightweight, learnable, and invertible preprocessing pipeline that adapts raw
images for standard JPEG compression. Our method applies spatial and optional
frequency-domain transforms, with compact parameters stored in the JPEG comment
field, enabling accurate raw reconstruction. Experiments across multiple
datasets show that our method achieves higher fidelity than direct JPEG
storage, supports other codecs, and provides a favorable trade-off between
compression ratio and reconstruction accuracy.

</details>


### [8] [The Impact of 2D Segmentation Backbones on Point Cloud Predictions Using 4D Radar](https://arxiv.org/abs/2509.19644)
*William L. Muckelroy III,Mohammed Alsakabi,John M. Dolan,Ozan K. Tonguz*

Main category: cs.CV

TL;DR: 研究更高容量的分割骨干网络对雷达生成LiDAR质量的影响，发现最优分割骨干可提升23.7%性能


<details>
  <summary>Details</summary>
Motivation: LiDAR成本高昂限制了高级自动驾驶系统的普及，需要寻找替代方案。已有研究使用4D雷达生成LiDAR点云，本研究旨在优化分割骨干网络以提升点云质量

Method: 使用更高容量的分割骨干网络来改进基于4D雷达生成LiDAR点云的神经网络，在RaDelft数据集上进行训练和评估

Result: 研究发现过高容量的模型反而会降低性能，但找到最优分割骨干网络可带来23.7%的性能提升

Conclusion: 分割骨干网络的容量选择对雷达生成LiDAR点云的质量至关重要，存在一个最优容量点，超过该点反而会损害性能

Abstract: LiDAR's dense, sharp point cloud (PC) representations of the surrounding
environment enable accurate perception and significantly improve road safety by
offering greater scene awareness and understanding. However, LiDAR's high cost
continues to restrict the broad adoption of high-level Autonomous Driving (AD)
systems in commercially available vehicles. Prior research has shown progress
towards circumventing the need for LiDAR by training a neural network, using
LiDAR point clouds as ground truth (GT), to produce LiDAR-like 3D point clouds
using only 4D Radars. One of the best examples is a neural network created to
train a more efficient radar target detector with a modular 2D convolutional
neural network (CNN) backbone and a temporal coherence network at its core that
uses the RaDelft dataset for training (see arXiv:2406.04723). In this work, we
investigate the impact of higher-capacity segmentation backbones on the quality
of the produced point clouds. Our results show that while very high-capacity
models may actually hurt performance, an optimal segmentation backbone can
provide a 23.7% improvement over the state-of-the-art (SOTA).

</details>


### [9] [Bias in the Picture: Benchmarking VLMs with Social-Cue News Images and LLM-as-Judge Assessment](https://arxiv.org/abs/2509.19659)
*Aravind Narayanan,Vahid Reza Khazaie,Shaina Raza*

Main category: cs.CV

TL;DR: 该论文提出了一个新闻图像基准测试，用于评估大型视觉语言模型在视觉上下文影响下产生的社会偏见风险，发现视觉线索会系统性导致模型输出偏见，尤其在性别和职业属性上风险较高。


<details>
  <summary>Details</summary>
Motivation: 研究大型视觉语言模型在吸收和再现有害社会刻板印象方面的风险，特别是当图像中存在年龄、性别、种族、服装或职业等视觉线索时。

Method: 构建包含1,343个图像-问题对的新闻图像基准测试，标注真实答案和人口统计属性，使用最先进的VLMs进行评估，并采用LLM作为评判者进行人工验证。

Result: 视觉上下文在开放设置中会系统性影响模型输出；不同属性和模型的偏见流行程度不同，性别和职业风险最高；更高的忠实度不一定对应更低的偏见。

Conclusion: 研究揭示了VLMs的社会偏见风险，发布了基准测试、评估标准和代码，以支持可复现和公平意识的多模态评估。

Abstract: Large vision-language models (VLMs) can jointly interpret images and text,
but they are also prone to absorbing and reproducing harmful social stereotypes
when visual cues such as age, gender, race, clothing, or occupation are
present. To investigate these risks, we introduce a news-image benchmark
consisting of 1,343 image-question pairs drawn from diverse outlets, which we
annotated with ground-truth answers and demographic attributes (age, gender,
race, occupation, and sports). We evaluate a range of state-of-the-art VLMs and
employ a large language model (LLM) as judge, with human verification. Our
findings show that: (i) visual context systematically shifts model outputs in
open-ended settings; (ii) bias prevalence varies across attributes and models,
with particularly high risk for gender and occupation; and (iii) higher
faithfulness does not necessarily correspond to lower bias. We release the
benchmark prompts, evaluation rubric, and code to support reproducible and
fairness-aware multimodal assessment.

</details>


### [10] [MoTiC: Momentum Tightness and Contrast for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2509.19664)
*Zeyu He,Shuai Huang,Yuwu Lu,Ming Zhao*

Main category: cs.CV

TL;DR: 本文提出MoTiC框架，通过贝叶斯分析和对比学习解决FSCIL中因数据稀缺导致的新类别原型估计偏差问题，在三个基准测试上取得SOTA性能


<details>
  <summary>Details</summary>
Motivation: FSCIL面临学习新类别时样本稀缺和保持旧类别知识的双重挑战，现有方法使用冻结特征提取器和类别平均原型，但新类别原型因数据稀缺存在显著估计偏差

Method: 提出MoTiC框架：1）贝叶斯分析将新类别先验与旧类别统计对齐以减少方差；2）大规模对比学习增强跨类别特征紧密度；3）动量自监督和虚拟类别注入先验信息

Result: 在三个FSCIL基准测试上取得最先进性能，特别是在细粒度任务CUB-200上表现优异

Conclusion: MoTiC能够有效减少估计偏差，提高增量学习的鲁棒性，构建具有丰富表示和增强类间凝聚力的特征空间

Abstract: Few-Shot Class-Incremental Learning (FSCIL) must contend with the dual
challenge of learning new classes from scarce samples while preserving old
class knowledge. Existing methods use the frozen feature extractor and
class-averaged prototypes to mitigate against catastrophic forgetting and
overfitting. However, new-class prototypes suffer significant estimation bias
due to extreme data scarcity, whereas base-class prototypes benefit from
sufficient data. In this work, we theoretically demonstrate that aligning the
new-class priors with old-class statistics via Bayesian analysis reduces
variance and improves prototype accuracy. Furthermore, we propose large-scale
contrastive learning to enforce cross-category feature tightness. To further
enrich feature diversity and inject prior information for new-class prototypes,
we integrate momentum self-supervision and virtual categories into the Momentum
Tightness and Contrast framework (MoTiC), constructing a feature space with
rich representations and enhanced interclass cohesion. Experiments on three
FSCIL benchmarks produce state-of-the-art performances, particularly on the
fine-grained task CUB-200, validating our method's ability to reduce estimation
bias and improve incremental learning robustness.

</details>


### [11] [Deep Learning for Clouds and Cloud Shadow Segmentation in Methane Satellite and Airborne Imaging Spectroscopy](https://arxiv.org/abs/2509.19665)
*Manuel Perez-Carrasco,Maya Nasr,Sebastien Roche,Chris Chan Miller,Zhan Zhang,Core Francisco Park,Eleanor Walker,Cecilia Garraffo,Douglas Finkbeiner,Ritesh Gautam,Steven Wofsy*

Main category: cs.CV

TL;DR: 使用机器学习方法检测高分辨率遥感影像中的云和云影，传统方法存在空间连贯性和边界定义问题，深度学习模型显著提升检测质量，其中UNet保持空间结构最佳，SCAN在卫星数据上表现最优。


<details>
  <summary>Details</summary>
Motivation: 云和云影检测是准确反演大气甲烷浓度的重要前提，对于MethaneSAT等新一代高光谱遥感任务至关重要，因为云和云影会影响甲烷排放的量化精度。

Method: 评估了传统方法（迭代逻辑回归ILR和多层感知器MLP）与先进深度学习架构（UNet和光谱通道注意力网络SCAN），重点关注空间连贯性和边界检测能力。

Result: 传统方法在空间连贯性和边界定义方面表现不佳，深度学习模型大幅改进检测质量：UNet在保持空间结构方面最优，SCAN在捕捉精细边界细节方面表现突出，特别是在MethaneSAT数据上超越UNet。

Conclusion: 深度学习架构为云和云影筛查提供了稳健、可扩展的解决方案，能够增强现有和下一代高光谱任务的甲烷排放量化能力，光谱注意力机制对卫星特定特征具有重要价值。

Abstract: Effective cloud and cloud shadow detection is a critical prerequisite for
accurate retrieval of concentrations of atmospheric methane or other trace
gases in hyperspectral remote sensing. This challenge is especially pertinent
for MethaneSAT and for its airborne companion mission, MethaneAIR. In this
study, we use machine learning methods to address the cloud and cloud shadow
detection problem for sensors with these high spatial resolutions instruments.
Cloud and cloud shadows in remote sensing data need to be effectively screened
out as they bias methane retrievals in remote sensing imagery and impact the
quantification of emissions. We deploy and evaluate conventional techniques
including Iterative Logistic Regression (ILR) and Multilayer Perceptron (MLP),
with advanced deep learning architectures, namely UNet and a Spectral Channel
Attention Network (SCAN) method. Our results show that conventional methods
struggle with spatial coherence and boundary definition, affecting the
detection of clouds and cloud shadows. Deep learning models substantially
improve detection quality: UNet performs best in preserving spatial structure,
while SCAN excels at capturing fine boundary details. Notably, SCAN surpasses
UNet on MethaneSAT data, underscoring the benefits of incorporating spectral
attention for satellite specific features. This in depth assessment of various
disparate machine learning techniques demonstrates the strengths and
effectiveness of advanced deep learning architectures in providing robust,
scalable solutions for clouds and cloud shadow screening towards enhancing
methane emission quantification capacity of existing and next generation
hyperspectral missions. Our data and code is publicly available at
https://doi.org/10.7910/DVN/IKLZOJ

</details>


### [12] [Enhancing Transformer-Based Vision Models: Addressing Feature Map Anomalies Through Novel Optimization Strategies](https://arxiv.org/abs/2509.19687)
*Sumit Mamtani*

Main category: cs.CV

TL;DR: 提出了两种轻量级优化技术（STA和ANF）来改善ViT的特征图质量，解决结构化噪声问题，提升下游任务性能


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在特征图中存在结构化噪声，这会阻碍分割和深度估计等下游应用的效果

Method: 提出两种架构无关的方法：1）结构化令牌增强（STA）- 通过空间扰动增强令牌多样性；2）自适应噪声过滤（ANF）- 在Transformer层间应用可学习的在线去噪

Result: 在ImageNet、Ade20k和NYUv2等标准基准测试中，实验结果显示视觉质量和任务性能得到一致改善

Conclusion: 所提出的STA和ANF方法能够有效提升ViT的视觉质量和下游任务性能，具有实际应用价值

Abstract: Vision Transformers (ViTs) have demonstrated superior performance across a
wide range of computer vision tasks. However, structured noise artifacts in
their feature maps hinder downstream applications such as segmentation and
depth estimation. We propose two novel and lightweight optimisation techniques-
Structured Token Augmentation (STA) and Adaptive Noise Filtering (ANF)- to
improve interpretability and mitigate these artefacts. STA enhances token
diversity through spatial perturbations during tokenisation, while ANF applies
learnable inline denoising between transformer layers. These methods are
architecture-agnostic and evaluated across standard benchmarks, including
ImageNet, Ade20k, and NYUv2. Experimental results show consistent improvements
in visual quality and task performance, highlighting the practical
effectiveness of our approach.

</details>


### [13] [From Prompt to Progression: Taming Video Diffusion Models for Seamless Attribute Transition](https://arxiv.org/abs/2509.19690)
*Ling Lo,Kelvin C. K. Chan,Wen-Huang Cheng,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 提出了一种通过帧级引导实现平滑属性过渡的视频生成方法，并创建了CAT-Bench基准测试


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理复杂时间变化时，特别是生成具有渐进属性过渡的视频时表现不佳，常见的提示插值方法无法有效处理渐进属性过渡

Method: 在去噪过程中引入帧级引导，为每个噪声潜在空间构建数据特定的过渡方向，逐帧引导从初始属性到最终属性的渐进转变，同时保持视频的运动动态

Result: 实验结果表明该方法优于现有基线，实现了视觉保真度、与文本提示的对齐以及无缝属性过渡

Conclusion: 提出的方法简单有效，能够扩展现有模型实现平滑一致的属性过渡，并发布了代码和CAT-Bench基准

Abstract: Existing models often struggle with complex temporal changes, particularly
when generating videos with gradual attribute transitions. The most common
prompt interpolation approach for motion transitions often fails to handle
gradual attribute transitions, where inconsistencies tend to become more
pronounced. In this work, we propose a simple yet effective method to extend
existing models for smooth and consistent attribute transitions, through
introducing frame-wise guidance during the denoising process. Our approach
constructs a data-specific transitional direction for each noisy latent,
guiding the gradual shift from initial to final attributes frame by frame while
preserving the motion dynamics of the video. Moreover, we present the
Controlled-Attribute-Transition Benchmark (CAT-Bench), which integrates both
attribute and motion dynamics, to comprehensively evaluate the performance of
different models. We further propose two metrics to assess the accuracy and
smoothness of attribute transitions. Experimental results demonstrate that our
approach performs favorably against existing baselines, achieving visual
fidelity, maintaining alignment with text prompts, and delivering seamless
attribute transitions. Code and CATBench are released:
https://github.com/lynn-ling-lo/Prompt2Progression.

</details>


### [14] [Anatomically Constrained Transformers for Cardiac Amyloidosis Classification](https://arxiv.org/abs/2509.19691)
*Alexander Thorley,Agis Chartsias,Jordan Strom,Roberto Lang,Jeremy Slivnick,Jamie O'Driscoll,Rajan Sharma,Dipak Kotecha,Jinming Duan,Alberto Gomez*

Main category: cs.CV

TL;DR: 该论文提出了一种基于解剖学约束的Transformer模型，用于心脏淀粉样变性（CA）分类，通过将模型注意力限制在心肌区域来提高分类性能并提供可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络分类方法无法保证基于临床相关特征进行分类，而定量特征方法虽然能确保临床相关性但可能忽略其他有用信息。

Method: 将心肌区域表示为变形点集和对应图像块作为输入标记，约束Transformer模型仅关注心肌区域，并在自监督预训练中仅掩码和重建解剖块。

Result: 相比全视频Transformer，该方法在CA分类任务上实现了更高的性能，并提供分类仅关注回声解剖区域的明确保证。

Conclusion: 通过将Transformer和预训练任务约束在CA影像特征定位的心肌区域，可以提高分类性能并提供可解释的注意力可视化。

Abstract: Cardiac amyloidosis (CA) is a rare cardiomyopathy, with typical abnormalities
in clinical measurements from echocardiograms such as reduced global
longitudinal strain of the myocardium. An alternative approach for detecting CA
is via neural networks, using video classification models such as convolutional
neural networks. These models process entire video clips, but provide no
assurance that classification is based on clinically relevant features known to
be associated with CA. An alternative paradigm for disease classification is to
apply models to quantitative features such as strain, ensuring that the
classification relates to clinically relevant features. Drawing inspiration
from this approach, we explicitly constrain a transformer model to the
anatomical region where many known CA abnormalities occur -- the myocardium,
which we embed as a set of deforming points and corresponding sampled image
patches into input tokens. We show that our anatomical constraint can also be
applied to the popular self-supervised learning masked autoencoder
pre-training, where we propose to mask and reconstruct only anatomical patches.
We show that by constraining both the transformer and pre-training task to the
myocardium where CA imaging features are localized, we achieve increased
performance on a CA classification task compared to full video transformers.
Our model provides an explicit guarantee that the classification is focused on
only anatomical regions of the echo, and enables us to visualize transformer
attention scores over the deforming myocardium.

</details>


### [15] [Learning to Stop: Reinforcement Learning for Efficient Patient-Level Echocardiographic Classification](https://arxiv.org/abs/2509.19694)
*Woo-Jin Cho Kim,Jorge Oliveira,Arian Beqiri,Alex Thorley,Jordan Strom,Jamie O'Driscoll,Rajan Sharma,Jeremy Slivnick,Roberto Lang,Alberto Gomez,Agisilaos Chartsias*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的优化方法，用于选择最佳的心脏超声视频片段子集，以提高疾病分类性能，同时减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的心脏超声自动分析方法要么使用单个片段（忽略其他片段的补充信息），要么使用所有片段（计算成本高且不利于临床采用），需要一种平衡性能和效率的方法。

Method: 通过强化学习训练一个智能体，学习在减少疾病分类不确定性和停止处理（当分类置信度足够时）之间做出决策；同时提出可学习的基于注意力的聚合方法，灵活融合多个片段的信息。

Result: 在检测心脏淀粉样变性的任务中，仅使用30%的片段就达到了0.91的AUC，超过了使用所有片段和其他基准方法的性能。

Conclusion: 该方法能够有效选择最优片段子集，在保持高性能的同时显著降低计算负担，具有临床应用的潜力。

Abstract: Guidelines for transthoracic echocardiographic examination recommend the
acquisition of multiple video clips from different views of the heart,
resulting in a large number of clips. Typically, automated methods, for
instance disease classifiers, either use one clip or average predictions from
all clips. Relying on one clip ignores complementary information available from
other clips, while using all clips is computationally expensive and may be
prohibitive for clinical adoption.
  To select the optimal subset of clips that maximize performance for a
specific task (image-based disease classification), we propose a method
optimized through reinforcement learning. In our method, an agent learns to
either keep processing view-specific clips to reduce the disease classification
uncertainty, or stop processing if the achieved classification confidence is
sufficient. Furthermore, we propose a learnable attention-based aggregation
method as a flexible way of fusing information from multiple clips. The
proposed method obtains an AUC of 0.91 on the task of detecting cardiac
amyloidosis using only 30% of all clips, exceeding the performance achieved
from using all clips and from other benchmarks.

</details>


### [16] [Towards Robust In-Context Learning for Medical Image Segmentation via Data Synthesis](https://arxiv.org/abs/2509.19711)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Chenfei Ye,Hanyang Peng,Jianfeng Cao,Ting Ma*

Main category: cs.CV

TL;DR: SynthICL是一个基于领域随机化的医学图像分割数据合成框架，通过利用真实世界数据集的解剖先验生成多样化且分布合适的医学数据，以解决上下文学习中数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 随着上下文学习在通用医学图像分割中的兴起，对大规模多样化数据集的需求加剧了数据稀缺问题。现有数据合成方法难以同时实现高数据多样性和适合医学数据的领域分布。

Method: 提出SynthICL框架，基于领域随机化技术，利用真实世界数据集的解剖先验确保真实性，生成多样化解剖结构以覆盖广泛数据分布，并显式建模主体间变异以创建适合ICL的数据队列。

Result: 在四个保留数据集上的实验验证了框架有效性，使用SynthICL数据训练的模型在平均Dice系数上实现了高达63%的性能提升，并显著增强了对未见解剖领域的泛化能力。

Conclusion: 该工作有助于缓解基于ICL的分割方法的数据瓶颈，为构建鲁棒模型铺平了道路。

Abstract: The rise of In-Context Learning (ICL) for universal medical image
segmentation has introduced an unprecedented demand for large-scale, diverse
datasets for training, exacerbating the long-standing problem of data scarcity.
While data synthesis offers a promising solution, existing methods often fail
to simultaneously achieve both high data diversity and a domain distribution
suitable for medical data. To bridge this gap, we propose \textbf{SynthICL}, a
novel data synthesis framework built upon domain randomization. SynthICL
ensures realism by leveraging anatomical priors from real-world datasets,
generates diverse anatomical structures to cover a broad data distribution, and
explicitly models inter-subject variations to create data cohorts suitable for
ICL. Extensive experiments on four held-out datasets validate our framework's
effectiveness, showing that models trained with our data achieve performance
gains of up to 63\% in average Dice and substantially enhanced generalization
to unseen anatomical domains. Our work helps mitigate the data bottleneck for
ICL-based segmentation, paving the way for robust models. Our code and the
generated dataset are publicly available at
https://github.com/jiesihu/Neuroverse3D.

</details>


### [17] [VIMD: Monocular Visual-Inertial Motion and Depth Estimation](https://arxiv.org/abs/2509.19713)
*Saimouli Katragadda,Guoquan Huang*

Main category: cs.CV

TL;DR: 提出了一种单目视觉惯性运动和深度学习框架VIMD，通过利用MSCKF运动跟踪来估计密集度量深度，采用逐像素尺度迭代优化而非全局仿射模型，具有高度模块化和强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 准确高效的密集度量深度估计对于机器人和XR中的3D视觉感知至关重要，需要解决现有方法在资源受限环境下的部署问题。

Method: 基于MSCKF的单目视觉惯性运动跟踪，利用多视图信息迭代优化逐像素尺度，而不是全局拟合不变的仿射模型，框架高度模块化可与多种深度估计骨干网络兼容。

Result: 在TartanAir和VOID数据集上进行了广泛评估，并在AR Table数据集上展示了零样本泛化能力，即使在每张图像仅10-20个稀疏深度点的情况下也能实现优异的准确性和鲁棒性。

Conclusion: VIMD为资源受限环境提供了实用的解决方案，其鲁棒性能和强泛化能力在广泛场景中具有重要应用潜力。

Abstract: Accurate and efficient dense metric depth estimation is crucial for 3D visual
perception in robotics and XR. In this paper, we develop a monocular
visual-inertial motion and depth (VIMD) learning framework to estimate dense
metric depth by leveraging accurate and efficient MSCKF-based monocular
visual-inertial motion tracking. At the core the proposed VIMD is to exploit
multi-view information to iteratively refine per-pixel scale, instead of
globally fitting an invariant affine model as in the prior work. The VIMD
framework is highly modular, making it compatible with a variety of existing
depth estimation backbones. We conduct extensive evaluations on the TartanAir
and VOID datasets and demonstrate its zero-shot generalization capabilities on
the AR Table dataset. Our results show that VIMD achieves exceptional accuracy
and robustness, even with extremely sparse points as few as 10-20 metric depth
points per image. This makes the proposed VIMD a practical solution for
deployment in resource constrained settings, while its robust performance and
strong generalization capabilities offer significant potential across a wide
range of scenarios.

</details>


### [18] [Frequency-domain Multi-modal Fusion for Language-guided Medical Image Segmentation](https://arxiv.org/abs/2509.19719)
*Bo Yu,Jianhua Yang,Zetao Du,Yan Huang,Chenglong Li,Liang Wang*

Main category: cs.CV

TL;DR: 提出了FMISeg模型，通过频域多模态交互实现语言引导的医学图像分割，解决了视觉-语言模态间的语义鸿沟问题，在肺感染区域分割任务上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效融合医学图像和临床文本报告，无法充分增强视觉特征表示和消除语义无关信息，导致分割性能不理想。复杂的病灶形态变化和视觉-语言模态间的语义鸿沟是主要挑战。

Method: FMISeg是一个晚期融合模型，在解码器中建立语言特征和频域视觉特征的交互。包含两个核心模块：频域特征双向交互模块(FFBI)用于增强视觉表示，语言引导频域特征交互模块(LFFI)用于在语言信息指导下抑制语义无关的视觉特征。

Result: 在QaTa-COV19和MosMedData+数据集上的实验表明，该方法在定性和定量评估上都优于当前最先进的方法。

Conclusion: FMISeg通过频域多模态交互有效解决了语言引导医学图像分割中的关键问题，为肺感染疾病的自动诊断提供了更准确的解决方案。

Abstract: Automatically segmenting infected areas in radiological images is essential
for diagnosing pulmonary infectious diseases. Recent studies have demonstrated
that the accuracy of the medical image segmentation can be improved by
incorporating clinical text reports as semantic guidance. However, the complex
morphological changes of lesions and the inherent semantic gap between
vision-language modalities prevent existing methods from effectively enhancing
the representation of visual features and eliminating semantically irrelevant
information, ultimately resulting in suboptimal segmentation performance. To
address these problems, we propose a Frequency-domain Multi-modal Interaction
model (FMISeg) for language-guided medical image segmentation. FMISeg is a late
fusion model that establishes interaction between linguistic features and
frequency-domain visual features in the decoder. Specifically, to enhance the
visual representation, our method introduces a Frequency-domain Feature
Bidirectional Interaction (FFBI) module to effectively fuse frequency-domain
features. Furthermore, a Language-guided Frequency-domain Feature Interaction
(LFFI) module is incorporated within the decoder to suppress semantically
irrelevant visual features under the guidance of linguistic information.
Experiments on QaTa-COV19 and MosMedData+ demonstrated that our method
outperforms the state-of-the-art methods qualitatively and quantitatively.

</details>


### [19] [PolGS: Polarimetric Gaussian Splatting for Fast Reflective Surface Reconstruction](https://arxiv.org/abs/2509.19726)
*Yufei Han,Bowen Tie,Heng Guo,Youwei Lyu,Si Li,Boxin Shi,Yunpeng Jia,Zhanyu Ma*

Main category: cs.CV

TL;DR: 提出PolGS方法，将偏振约束集成到3D高斯泼溅框架中，实现10分钟内快速重建复杂反射表面


<details>
  <summary>Details</summary>
Motivation: 现有3DGS方法在复杂反射表面重建质量上落后于隐式神经表示，需要提升反射材质的重建效果

Method: 通过偏振约束分离镜面反射和漫反射分量，在3D高斯泼溅框架中集成偏振信息

Result: 在合成和真实数据集上的实验验证了方法的有效性，显著提升了反射表面的重建质量

Conclusion: PolGS方法能够快速高质量地重建复杂反射表面，为实时虚拟现实应用提供有效解决方案

Abstract: Efficient shape reconstruction for surfaces with complex reflectance
properties is crucial for real-time virtual reality. While 3D Gaussian
Splatting (3DGS)-based methods offer fast novel view rendering by leveraging
their explicit surface representation, their reconstruction quality lags behind
that of implicit neural representations, particularly in the case of recovering
surfaces with complex reflective reflectance. To address these problems, we
propose PolGS, a Polarimetric Gaussian Splatting model allowing fast reflective
surface reconstruction in 10 minutes. By integrating polarimetric constraints
into the 3DGS framework, PolGS effectively separates specular and diffuse
components, enhancing reconstruction quality for challenging reflective
materials. Experimental results on the synthetic and real-world dataset
validate the effectiveness of our method.

</details>


### [20] [CAMILA: Context-Aware Masking for Image Editing with Language Alignment](https://arxiv.org/abs/2509.19731)
*Hyunseung Kim,Chiho Choi,Srikanth Malla,Sai Prahladh Padmanabhan,Saurabh Bagchi,Joon Hee Choi*

Main category: cs.CV

TL;DR: CAMILA是一种上下文感知的图像编辑方法，通过验证指令与图像的上下文一致性，只对可行指令进行编辑，忽略不可行或矛盾的指令，从而提高编辑质量和语义对齐。


<details>
  <summary>Details</summary>
Motivation: 现有图像编辑模型盲目遵循所有用户指令，即使指令不可行或矛盾，导致输出无意义。需要一种能识别和忽略不可行指令的方法。

Method: 提出CAMILA方法，通过上下文感知掩码和语言对齐技术，验证指令与图像的上下文一致性，只对相关区域应用可行编辑。

Result: 在单指令和多指令图像编辑数据集上评估，CAMILA在性能和语义对齐方面优于最先进模型，能有效处理复杂指令挑战并保持图像完整性。

Conclusion: CAMILA通过上下文感知验证机制，显著提升了图像编辑的可靠性和质量，为处理复杂和不可行指令提供了有效解决方案。

Abstract: Text-guided image editing has been allowing users to transform and synthesize
images through natural language instructions, offering considerable
flexibility. However, most existing image editing models naively attempt to
follow all user instructions, even if those instructions are inherently
infeasible or contradictory, often resulting in nonsensical output. To address
these challenges, we propose a context-aware method for image editing named as
CAMILA (Context-Aware Masking for Image Editing with Language Alignment).
CAMILA is designed to validate the contextual coherence between instructions
and the image, ensuring that only relevant edits are applied to the designated
regions while ignoring non-executable instructions. For comprehensive
evaluation of this new method, we constructed datasets for both single- and
multi-instruction image editing, incorporating the presence of infeasible
requests. Our method achieves better performance and higher semantic alignment
than state-of-the-art models, demonstrating its effectiveness in handling
complex instruction challenges while preserving image integrity.

</details>


### [21] [Robust RGB-T Tracking via Learnable Visual Fourier Prompt Fine-tuning and Modality Fusion Prompt Generation](https://arxiv.org/abs/2509.19733)
*Hongtao Yang,Bineng Zhong,Qihua Liang,Zhiruo Zhu,Yaozong Zheng,Ning Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于傅里叶变换的视觉提示跟踪方法（VFPTrack），通过在频域和空间域结合学习模态相关提示，改进了RGB-热成像跟踪的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于参数高效微调的RGB-T跟踪方法仅依赖空间域信息作为提示，忽视了频域信息在提示学习中的重要作用，导致性能不佳。

Method: 提出VFPTrack方法，包含对称特征提取编码器、视觉傅里叶提示和模态融合提示生成器。通过FFT获取频域提示，结合空间域视觉提示，并生成融合模态提示实现跨模态特征交互。

Result: 在三个主流RGB-T跟踪基准测试上的广泛实验表明，该方法展现出卓越的性能。

Conclusion: 结合频域和空间域信息的视觉傅里叶提示学习方法能够有效提升RGB-T跟踪的性能，实现跨模态特征的充分交互。

Abstract: Recently, visual prompt tuning is introduced to RGB-Thermal (RGB-T) tracking
as a parameter-efficient finetuning (PEFT) method. However, these PEFT-based
RGB-T tracking methods typically rely solely on spatial domain information as
prompts for feature extraction. As a result, they often fail to achieve optimal
performance by overlooking the crucial role of frequency-domain information in
prompt learning. To address this issue, we propose an efficient Visual Fourier
Prompt Tracking (named VFPTrack) method to learn modality-related prompts via
Fast Fourier Transform (FFT). Our method consists of symmetric feature
extraction encoder with shared parameters, visual fourier prompts, and Modality
Fusion Prompt Generator that generates bidirectional interaction prompts
through multi-modal feature fusion. Specifically, we first use a frozen feature
extraction encoder to extract RGB and thermal infrared (TIR) modality features.
Then, we combine the visual prompts in the spatial domain with the frequency
domain prompts obtained from the FFT, which allows for the full extraction and
understanding of modality features from different domain information. Finally,
unlike previous fusion methods, the modality fusion prompt generation module we
use combines features from different modalities to generate a fused modality
prompt. This modality prompt is interacted with each individual modality to
fully enable feature interaction across different modalities. Extensive
experiments conducted on three popular RGB-T tracking benchmarks show that our
method demonstrates outstanding performance.

</details>


### [22] [Rectified Decoupled Dataset Distillation: A Closer Look for Fair and Comprehensive Evaluation](https://arxiv.org/abs/2509.19743)
*Xinhao Zhong,Shuoyang Sun,Xulin Gu,Chenyang Zhu,Bin Chen,Yaowei Wang*

Main category: cs.CV

TL;DR: 本文提出RD³方法，系统研究数据集蒸馏中不同后评估设置对测试准确率的影响，揭示性能差异主要源于评估不一致而非方法本身，并建立标准化基准。


<details>
  <summary>Details</summary>
Motivation: 现有解耦数据集蒸馏方法存在后评估协议不一致的问题，阻碍了该领域的发展。需要系统研究评估设置的影响，并为公平比较建立标准。

Method: 提出Rectified Decoupled Dataset Distillation (RD³)方法，系统分析不同后评估设置的影响，识别改进策略，建立标准化评估基准。

Result: 分析发现性能差异主要源于评估不一致而非方法质量差异，识别出提高蒸馏数据集有效性的通用策略。

Conclusion: RD³为数据集蒸馏研究提供了公平可复现比较的基础，揭示了评估标准化的重要性。

Abstract: Dataset distillation aims to generate compact synthetic datasets that enable
models trained on them to achieve performance comparable to those trained on
full real datasets, while substantially reducing storage and computational
costs. Early bi-level optimization methods (e.g., MTT) have shown promising
results on small-scale datasets, but their scalability is limited by high
computational overhead. To address this limitation, recent decoupled dataset
distillation methods (e.g., SRe$^2$L) separate the teacher model pre-training
from the synthetic data generation process. These methods also introduce random
data augmentation and epoch-wise soft labels during the post-evaluation phase
to improve performance and generalization. However, existing decoupled
distillation methods suffer from inconsistent post-evaluation protocols, which
hinders progress in the field. In this work, we propose Rectified Decoupled
Dataset Distillation (RD$^3$), and systematically investigate how different
post-evaluation settings affect test accuracy. We further examine whether the
reported performance differences across existing methods reflect true
methodological advances or stem from discrepancies in evaluation procedures.
Our analysis reveals that much of the performance variation can be attributed
to inconsistent evaluation rather than differences in the intrinsic quality of
the synthetic data. In addition, we identify general strategies that improve
the effectiveness of distilled datasets across settings. By establishing a
standardized benchmark and rigorous evaluation protocol, RD$^3$ provides a
foundation for fair and reproducible comparisons in future dataset distillation
research.

</details>


### [23] [nnFilterMatch: A Unified Semi-Supervised Learning Framework with Uncertainty-Aware Pseudo-Label Filtering for Efficient Medical Segmentation](https://arxiv.org/abs/2509.19746)
*Yi Yang*

Main category: cs.CV

TL;DR: 提出nnFilterMatch框架，将半监督学习与主动学习机制结合，在单次训练中实现高效的医学图像分割，仅需5%-20%标注数据即可达到全监督模型性能


<details>
  <summary>Details</summary>
Motivation: 传统SSL-AL混合方法需要迭代重训练，计算开销大且难以在临床应用中扩展，需要一种更高效的标注策略

Method: 在nnU-Net框架中集成SSL与基于熵的伪标签过滤机制(FilterMatch)，通过选择性排除高置信度伪标签来避免重训练循环

Result: 在多个临床分割基准测试中，仅使用5%-20%标注数据即可达到或超过全监督模型性能

Conclusion: 该工作提供了一种可扩展的端到端学习策略，显著降低医学图像分割的标注需求而不影响准确性

Abstract: Semi-supervised learning (SSL) has emerged as a promising paradigm in medical
image segmentation, offering competitive performance while substantially
reducing the need for extensive manual annotation. When combined with active
learning (AL), these strategies further minimize annotation burden by
selectively incorporating the most informative samples. However, conventional
SSL_AL hybrid approaches often rely on iterative and loop-based retraining
cycles after each annotation round, incurring significant computational
overhead and limiting scalability in clinical applications. In this study, we
present a novel, annotation-efficient, and self-adaptive deep segmentation
framework that integrates SSL with entropy-based pseudo-label filtering
(FilterMatch), an AL-inspired mechanism, within the single-pass nnU-Net
training segmentation framework (nnFilterMatch). By selectively excluding
high-confidence pseudo-labels during training, our method circumvents the need
for retraining loops while preserving the benefits of uncertainty-guided
learning. We validate the proposed framework across multiple clinical
segmentation benchmarks and demonstrate that it achieves performance comparable
to or exceeding fully supervised models, even with only 5\%--20\% labeled data.
This work introduces a scalable, end-to-end learning strategy for reducing
annotation demands in medical image segmentation without compromising accuracy.
Code is available here: https://github.com/Ordi117/nnFilterMatch.git.

</details>


### [24] [Talking Head Generation via AU-Guided Landmark Prediction](https://arxiv.org/abs/2509.19749)
*Shao-Yu Chang,Jingyi Xu,Hieu Le,Dimitris Samaras*

Main category: cs.CV

TL;DR: 提出了一种基于面部动作单元（AUs）的音频驱动说话头生成的两阶段框架，通过显式地将AUs映射到2D面部关键点实现精细的表情控制


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖情感标签或隐式的AU条件控制，缺乏物理基础和逐帧的精确表情控制能力

Method: 第一阶段使用变分运动生成器从音频和AU强度预测时间连贯的关键点序列；第二阶段使用基于扩散的合成器根据关键点和参考图像生成逼真的唇同步视频

Result: 在MEAD数据集上的实验表明，该方法在多个指标上优于现有最先进基线方法

Conclusion: 显式的AU到关键点建模方法在表达性说话头生成中具有显著优势，能够提高表情准确性、时间稳定性和视觉真实感

Abstract: We propose a two-stage framework for audio-driven talking head generation
with fine-grained expression control via facial Action Units (AUs). Unlike
prior methods relying on emotion labels or implicit AU conditioning, our model
explicitly maps AUs to 2D facial landmarks, enabling physically grounded,
per-frame expression control. In the first stage, a variational motion
generator predicts temporally coherent landmark sequences from audio and AU
intensities. In the second stage, a diffusion-based synthesizer generates
realistic, lip-synced videos conditioned on these landmarks and a reference
image. This separation of motion and appearance improves expression accuracy,
temporal stability, and visual realism. Experiments on the MEAD dataset show
that our method outperforms state-of-the-art baselines across multiple metrics,
demonstrating the effectiveness of explicit AU-to-landmark modeling for
expressive talking head generation.

</details>


### [25] [ExpFace: Exponential Angular Margin Loss for Deep Face Recognition](https://arxiv.org/abs/2509.19753)
*Jinhui Zheng,Xueyuan Gong*

Main category: cs.CV

TL;DR: 提出ExpFace损失函数，通过角度指数项作为边界，在角度空间中中心区域施加更大惩罚、边缘区域施加较小惩罚，强调干净样本并抑制噪声样本，在面部识别任务中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于边界的softmax损失（如SphereFace、CosFace、ArcFace）虽然增强了类内紧凑性和类间可分性，但忽视了噪声样本的影响。研究发现干净样本主要聚集在角度空间中心区域，而噪声样本倾向于向边缘区域偏移。

Method: 提出指数角度边界损失（ExpFace），在角度空间中引入角度指数项作为边界，中心区域施加较大惩罚，边缘区域施加较小惩罚，从而强调干净样本并抑制噪声样本。

Result: 实验表明ExpFace不仅避免了SphereFace的训练不稳定性和ArcFace的非单调性问题，而且在角度空间中具有与决策边界一致的惩罚方式，实现了SOTA性能。

Conclusion: ExpFace通过角度指数边界设计有效处理噪声样本问题，在面部识别任务中表现出优越性能，为未来研究提供了新的损失函数设计思路。

Abstract: Face recognition is an open-set problem requiring high discriminative power
to ensure that intra-class distances remain smaller than inter-class distances.
Margin-based softmax losses, such as SphereFace, CosFace, and ArcFace, have
been widely adopted to enhance intra-class compactness and inter-class
separability, yet they overlook the impact of noisy samples. By examining the
distribution of samples in the angular space, we observe that clean samples
predominantly cluster in the center region, whereas noisy samples tend to shift
toward the peripheral region. Motivated by this observation, we propose the
Exponential Angular Margin Loss (ExpFace), which introduces an angular
exponential term as the margin. This design applies a larger penalty in the
center region and a smaller penalty in the peripheral region within the angular
space, thereby emphasizing clean samples while suppressing noisy samples. We
present a unified analysis of ExpFace and classical margin-based softmax losses
in terms of margin embedding forms, similarity curves, and gradient curves,
showing that ExpFace not only avoids the training instability of SphereFace and
the non-monotonicity of ArcFace, but also exhibits a similarity curve that
applies penalties in the same manner as the decision boundary in the angular
space. Extensive experiments demonstrate that ExpFace achieves state-of-the-art
performance. To facilitate future research, we have released the source code
at: https://github.com/dfr-code/ExpFace.

</details>


### [26] [Logics-Parsing Technical Report](https://arxiv.org/abs/2509.19760)
*Xiangyang Chen,Shuzhao Li,Xiuwen Zhu,Yongfan Chen,Fan Yang,Cheng Fang,Lin Qu,Xiaoxiao Xu,Hu Wei,Minggang Wu*

Main category: cs.CV

TL;DR: Logics-Parsing是一个基于大型视觉语言模型的端到端文档解析方法，通过强化学习优化复杂布局分析和阅读顺序推理，在多种文档类型上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 传统端到端文档解析方法在处理复杂文档类型（如多栏报纸、海报）时，由于缺乏显式的文档布局分析和阅读顺序推理阶段，性能受限。

Method: 提出Logics-Parsing模型，结合LVLM和强化学习，设计精心奖励机制来优化布局分析和阅读顺序推理，并扩展支持化学公式和手写中文等多样化数据类型。

Result: 在包含1,078页PDF图像的LogicsParsingBench基准测试中，模型在多样化文档分析场景下表现出有效性和SOTA性能。

Conclusion: 该方法通过强化学习增强的LVLM模型，显著提升了复杂文档的解析能力，并建立了全面的评估基准。

Abstract: Recent advances in Large Vision-Language models (LVLM) have spurred
significant progress in document parsing task. Compared to traditional
pipeline-based methods, end-to-end paradigms have shown their excellence in
converting PDF images into structured outputs through integrated Optical
Character Recognition (OCR), table recognition, mathematical formula
recognition and so on. However, the absence of explicit analytical stages for
document layouts and reading orders limits the LVLM's capability in handling
complex document types such as multi-column newspapers or posters. To address
this limitation, we propose in this report Logics-Parsing: an end-to-end
LVLM-based model augmented with reinforcement learning. Our model incorporates
meticulously designed reward mechanisms to optimize complex layout analysis and
reading order inference. In addition, we expand the model's versatility by
incorporating diverse data types such as chemical formulas and handwritten
Chinese characters into supervised fine-tuning. Finally, to enable rigorous
evaluation of our approach, we introduce LogicsParsingBench, a curated set of
1,078 page-level PDF images spanning nine major categories and over twenty
sub-categories, which will be released later. Comprehensive experiments
conducted on LogicsParsingBench have validated the efficacy and
State-of-the-art (SOTA) performance of our proposed model across diverse
document analysis scenarios. Project Page:
https://github.com/alibaba/Logics-Parsing

</details>


### [27] [Sex-based Bias Inherent in the Dice Similarity Coefficient: A Model Independent Analysis for Multiple Anatomical Structures](https://arxiv.org/abs/2509.19778)
*Hartmut Häntze,Myrthe Buser,Alessa Hering,Lisa C. Adams,Keno K. Bressem*

Main category: cs.CV

TL;DR: 该研究揭示了Dice相似系数(DSC)在医学图像分割评估中存在的性别偏见问题，即相同的分割错误在女性较小的器官上会导致更低的DSC评分，从而造成不公平的评估结果。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注模型或数据集的性别差异，但尚未探讨DSC评估指标本身可能引入的性别偏见。由于器官大小存在性别差异，相同的分割错误在不同性别的个体上会产生不同的DSC评分。

Method: 在50名参与者的MRI标注上应用相同大小的合成错误，确保性别间的可比性。通过模拟边界偏移等错误来量化DSC和标准化DSC的性别差异。

Result: 即使是1mm的最小边界偏移也会产生系统性DSC性别差异：小型结构差异约0.03，中型结构约0.01，只有大型结构（如肺和肝）几乎不受影响。

Conclusion: 使用DSC作为评估指标时不应期望男女获得相同分数，因为该指标本身存在偏见。认识到这一因素对于医学图像分析的公平评估至关重要。

Abstract: Overlap-based metrics such as the Dice Similarity Coefficient (DSC) penalize
segmentation errors more heavily in smaller structures. As organ size differs
by sex, this implies that a segmentation error of equal magnitude may result in
lower DSCs in women due to their smaller average organ volumes compared to men.
While previous work has examined sex-based differences in models or datasets,
no study has yet investigated the potential bias introduced by the DSC itself.
This study quantifies sex-based differences of the DSC and the normalized DSC
in an idealized setting independent of specific models. We applied
equally-sized synthetic errors to manual MRI annotations from 50 participants
to ensure sex-based comparability. Even minimal errors (e.g., a 1 mm boundary
shift) produced systematic DSC differences between sexes. For small structures,
average DSC differences were around 0.03; for medium-sized structures around
0.01. Only large structures (i.e., lungs and liver) were mostly unaffected,
with sex-based DSC differences close to zero. These findings underline that
fairness studies using the DSC as an evaluation metric should not expect
identical scores between men and women, as the metric itself introduces bias. A
segmentation model may perform equally well across sexes in terms of error
magnitude, even if observed DSC values suggest otherwise. Importantly, our work
raises awareness of a previously underexplored source of sex-based differences
in segmentation performance. One that arises not from model behavior, but from
the metric itself. Recognizing this factor is essential for more accurate and
fair evaluations in medical image analysis.

</details>


### [28] [EfficienT-HDR: An Efficient Transformer-Based Framework via Multi-Exposure Fusion for HDR Reconstruction](https://arxiv.org/abs/2509.19779)
*Yu-Shen Huang,Tzu-Han Chen,Cheng-Yen Hsiao,Shaou-Gang Miaou*

Main category: cs.CV

TL;DR: 本文提出了一种轻量级Vision Transformer架构，专门用于边缘设备上的HDR图像重建，解决了传统多曝光融合方法的高计算成本和鬼影伪影问题。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上实现高质量HDR成像对智能监控和自动驾驶等下游任务至关重要，但现有方法面临计算成本高和鬼影伪影的双重瓶颈。

Method: 基于Context-Aware Vision Transformer，将输入图像转换为YCbCr色彩空间分离亮度和色度信息，使用Intersection-Aware Adaptive Fusion模块抑制鬼影，并引入Inverted Residual Embedding、Dynamic Tanh和Enhanced Multi-Scale Dilated Convolution实现轻量化设计。

Result: 实验结果显示，主版本相比基线减少了约67%的FLOPS，在CPU上推理速度提升5倍以上，在边缘设备上提升2.5倍，同时保持高质量的图像重建效果。

Conclusion: 该方法为边缘设备提供了高效且无鬼影的HDR成像解决方案，在各种动态场景中展现出良好的通用性和实用性。

Abstract: Achieving high-quality High Dynamic Range (HDR) imaging on
resource-constrained edge devices is a critical challenge in computer vision,
as its performance directly impacts downstream tasks such as intelligent
surveillance and autonomous driving. Multi-Exposure Fusion (MEF) is a
mainstream technique to achieve this goal; however, existing methods generally
face the dual bottlenecks of high computational costs and ghosting artifacts,
hindering their widespread deployment. To this end, this study proposes a
light-weight Vision Transformer architecture designed explicitly for HDR
reconstruction to overcome these limitations. This study is based on the
Context-Aware Vision Transformer and begins by converting input images to the
YCbCr color space to separate luminance and chrominance information. It then
employs an Intersection-Aware Adaptive Fusion (IAAF) module to suppress
ghosting effectively. To further achieve a light-weight design, we introduce
Inverted Residual Embedding (IRE), Dynamic Tanh (DyT), and propose Enhanced
Multi-Scale Dilated Convolution (E-MSDC) to reduce computational complexity at
multiple levels. Our study ultimately contributes two model versions: a main
version for high visual quality and a light-weight version with advantages in
computational efficiency, both of which achieve an excellent balance between
performance and image quality. Experimental results demonstrate that, compared
to the baseline, the main version reduces FLOPS by approximately 67% and
increases inference speed by more than fivefold on CPU and 2.5 times on an edge
device. These results confirm that our method provides an efficient and
ghost-free HDR imaging solution for edge devices, demonstrating versatility and
practicality across various dynamic scenarios.

</details>


### [29] [BiTAA: A Bi-Task Adversarial Attack for Object Detection and Depth Estimation via 3D Gaussian Splatting](https://arxiv.org/abs/2509.19793)
*Yixun Zhang,Feng Zhou,Jianqin Yin*

Main category: cs.CV

TL;DR: BiTAA是一种基于3D高斯泼溅的双任务对抗攻击方法，能够同时破坏目标检测和偏置单目深度估计，揭示了自动驾驶多任务感知中的安全风险。


<details>
  <summary>Details</summary>
Motivation: 现有的2D/3D攻击方法存在任务孤岛问题，缺乏可控深度偏置机制，且没有标准化协议来量化跨任务迁移，导致检测与深度估计之间的交互关系未被充分探索。

Method: 基于3D高斯泼溅构建双模型攻击框架，支持全图像和补丁设置，兼容常见检测器和深度估计器，采用复合损失函数耦合检测抑制和符号可控的深度偏置。

Result: 实验显示BiTAA能实现一致的跨任务性能下降，并揭示了从检测到深度和从深度到检测迁移之间存在明显的不对称性。

Conclusion: 该方法凸显了多任务相机感知的实际风险，为自动驾驶场景中的跨任务感知防御提供了动机。

Abstract: Camera-based perception is critical to autonomous driving yet remains
vulnerable to task-specific adversarial manipulations in object detection and
monocular depth estimation. Most existing 2D/3D attacks are developed in task
silos, lack mechanisms to induce controllable depth bias, and offer no
standardized protocol to quantify cross-task transfer, leaving the interaction
between detection and depth underexplored. We present BiTAA, a bi-task
adversarial attack built on 3D Gaussian Splatting that yields a single
perturbation capable of simultaneously degrading detection and biasing
monocular depth. Specifically, we introduce a dual-model attack framework that
supports both full-image and patch settings and is compatible with common
detectors and depth estimators, with optional expectation-over-transformation
(EOT) for physical reality. In addition, we design a composite loss that
couples detection suppression with a signed, magnitude-controlled log-depth
bias within regions of interest (ROIs) enabling controllable near or far
misperception while maintaining stable optimization across tasks. We also
propose a unified evaluation protocol with cross-task transfer metrics and
real-world evaluations, showing consistent cross-task degradation and a clear
asymmetry between Det to Depth and from Depth to Det transfer. The results
highlight practical risks for multi-task camera-only perception and motivate
cross-task-aware defenses in autonomous driving scenarios.

</details>


### [30] [StrCGAN: A Generative Framework for Stellar Image Restoration](https://arxiv.org/abs/2509.19805)
*Shantanusinh Parmar*

Main category: cs.CV

TL;DR: StrCGAN是一种生成模型，用于增强低分辨率天体摄影图像，通过3D卷积层、多光谱融合和天体物理正则化模块来重建高保真度的天体图像。


<details>
  <summary>Details</summary>
Motivation: 传统模型如CycleGAN在图像到图像转换中仅限于2D映射，且容易扭曲恒星和星系的形态。小望远镜观测数据（如MobilTelesco数据集）的分辨率和质量有限，难以重建高保真度的天体图像。

Method: 在CycleGAN框架基础上扩展了三个关键创新：3D卷积层捕捉体积空间相关性、多光谱融合对齐光学和近红外域、天体物理正则化模块保护恒星形态。使用多任务全天巡天数据作为地面真值参考来指导训练。

Result: StrCGAN生成的图像不仅在视觉上更清晰，而且在物理上更一致，在天体物理图像增强任务中优于标准GAN模型。

Conclusion: StrCGAN通过结合3D卷积、多光谱融合和天体物理正则化，成功解决了传统模型在天体图像增强中的局限性，能够生成视觉和物理上都更优的重建结果。

Abstract: We introduce StrCGAN (Stellar Cyclic GAN), a generative model designed to
enhance low-resolution astrophotography images. Our goal is to reconstruct
high-fidelity ground truth-like representations of celestial objects, a task
that is challenging due to the limited resolution and quality of
small-telescope observations such as the MobilTelesco dataset. Traditional
models such as CycleGAN provide a foundation for image-to-image translation but
are restricted to 2D mappings and often distort the morphology of stars and
galaxies. To overcome these limitations, we extend the CycleGAN framework with
three key innovations: 3D convolutional layers to capture volumetric spatial
correlations, multi-spectral fusion to align optical and near-infrared (NIR)
domains, and astrophysical regularization modules to preserve stellar
morphology. Ground-truth references from multi-mission all-sky surveys spanning
optical to NIR guide the training process, ensuring that reconstructions remain
consistent across spectral bands. Together, these components allow StrCGAN to
generate reconstructions that are not only visually sharper but also physically
consistent, outperforming standard GAN models in the task of astrophysical
image enhancement.

</details>


### [31] [Adaptive Model Ensemble for Continual Learning](https://arxiv.org/abs/2509.19819)
*Yuchuan Mao,Zhi Gao,Xiaomeng Fan,Yuwei Wu,Yunde Jia,Chenchen Jing*

Main category: cs.CV

TL;DR: 提出了一种名为meta-weight-ensembler的自适应模型集成方法，通过元学习训练混合系数生成器来解决持续学习中的任务级和层级知识冲突问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型集成方法在持续学习中存在任务级和层级知识冲突问题，导致新旧任务的学习性能都受到影响。

Method: 使用元学习训练混合系数生成器，为每个层单独生成混合系数，自适应地融合不同任务的知识。

Result: 在多个持续学习数据集上的实验表明，该方法有效缓解了灾难性遗忘，并达到了最先进的性能。

Conclusion: meta-weight-ensembler可以灵活地与现有持续学习方法结合，提升其缓解灾难性遗忘的能力。

Abstract: Model ensemble is an effective strategy in continual learning, which
alleviates catastrophic forgetting by interpolating model parameters, achieving
knowledge fusion learned from different tasks. However, existing model ensemble
methods usually encounter the knowledge conflict issue at task and layer
levels, causing compromised learning performance in both old and new tasks. To
solve this issue, we propose meta-weight-ensembler that adaptively fuses
knowledge of different tasks for continual learning. Concretely, we employ a
mixing coefficient generator trained via meta-learning to generate appropriate
mixing coefficients for model ensemble to address the task-level knowledge
conflict. The mixing coefficient is individually generated for each layer to
address the layer-level knowledge conflict. In this way, we learn the prior
knowledge about adaptively accumulating knowledge of different tasks in a fused
model, achieving efficient learning in both old and new tasks.
Meta-weight-ensembler can be flexibly combined with existing continual learning
methods to boost their ability of alleviating catastrophic forgetting.
Experiments on multiple continual learning datasets show that
meta-weight-ensembler effectively alleviates catastrophic forgetting and
achieves state-of-the-art performance.

</details>


### [32] [ThinkFake: Reasoning in Multimodal Large Language Models for AI-Generated Image Detection](https://arxiv.org/abs/2509.19841)
*Tai-Ming Huang,Wei-Tung Lin,Kai-Lung Hua,Wen-Huang Cheng,Junichi Yamagishi,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: ThinkFake是一个基于推理的AI生成图像检测框架，使用多模态大语言模型和强化学习训练，能够进行逐步推理并产生可解释的结构化输出，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成图像的真实性不断提高，存在严重的错误信息和隐私侵犯风险，需要准确且可解释的检测方法。现有方法大多依赖无解释的二元分类或监督微调，泛化能力有限。

Method: 使用配备伪造推理提示的多模态大语言模型，通过Group Relative Policy Optimization强化学习训练，设计奖励函数，并引入结构化检测管道来提升推理质量和适应性。

Result: 在GenImage基准测试中超越现有最先进方法，在具有挑战性的LOKI基准测试中表现出强大的零样本泛化能力。

Conclusion: ThinkFake框架在AI生成图像检测方面具有有效性和鲁棒性，验证了基于推理的方法的优越性。

Abstract: The increasing realism of AI-generated images has raised serious concerns
about misinformation and privacy violations, highlighting the urgent need for
accurate and interpretable detection methods. While existing approaches have
made progress, most rely on binary classification without explanations or
depend heavily on supervised fine-tuning, resulting in limited generalization.
In this paper, we propose ThinkFake, a novel reasoning-based and generalizable
framework for AI-generated image detection. Our method leverages a Multimodal
Large Language Model (MLLM) equipped with a forgery reasoning prompt and is
trained using Group Relative Policy Optimization (GRPO) reinforcement learning
with carefully designed reward functions. This design enables the model to
perform step-by-step reasoning and produce interpretable, structured outputs.
We further introduce a structured detection pipeline to enhance reasoning
quality and adaptability. Extensive experiments show that ThinkFake outperforms
state-of-the-art methods on the GenImage benchmark and demonstrates strong
zero-shot generalization on the challenging LOKI benchmark. These results
validate our framework's effectiveness and robustness. Code will be released
upon acceptance.

</details>


### [33] [PersONAL: Towards a Comprehensive Benchmark for Personalized Embodied Agents](https://arxiv.org/abs/2509.19843)
*Filippo Ziliotto,Jelin Raphael Akkara,Alessandro Daniele,Lamberto Ballan,Luciano Serafini,Tommaso Campari*

Main category: cs.CV

TL;DR: PersONAL是一个用于研究具身AI个性化能力的基准测试，专注于个性化物体导航和定位任务，要求代理根据用户特定语义识别、检索和导航到与特定用户相关的物体。


<details>
  <summary>Details</summary>
Motivation: 当前具身AI在真实人类中心场景（如家庭环境）中部署面临挑战，主要困难在于难以建模个体人类偏好和行为。需要开发能够感知、推理和记忆个性化信息的具身代理。

Method: 构建了包含2000多个高质量场景的基准测试，涵盖30多个HM3D数据集中的真实感家庭环境。每个场景包含自然语言描述，明确关联物体与其所有者，支持两种评估模式：未知环境中的主动导航和已映射场景中的物体定位。

Result: 实验显示最先进基线模型与人类性能存在显著差距，表明当前具身代理在个性化信息处理能力上的不足。

Conclusion: PersONAL基准测试为开发能够处理个性化信息的具身代理铺平了道路，推动实现真实世界辅助机器人的发展。

Abstract: Recent advances in Embodied AI have enabled agents to perform increasingly
complex tasks and adapt to diverse environments. However, deploying such agents
in realistic human-centered scenarios, such as domestic households, remains
challenging, particularly due to the difficulty of modeling individual human
preferences and behaviors. In this work, we introduce PersONAL (PERSonalized
Object Navigation And Localization, a comprehensive benchmark designed to study
personalization in Embodied AI. Agents must identify, retrieve, and navigate to
objects associated with specific users, responding to natural-language queries
such as "find Lily's backpack". PersONAL comprises over 2,000 high-quality
episodes across 30+ photorealistic homes from the HM3D dataset. Each episode
includes a natural-language scene description with explicit associations
between objects and their owners, requiring agents to reason over user-specific
semantics. The benchmark supports two evaluation modes: (1) active navigation
in unseen environments, and (2) object grounding in previously mapped scenes.
Experiments with state-of-the-art baselines reveal a substantial gap to human
performance, highlighting the need for embodied agents capable of perceiving,
reasoning, and memorizing over personalized information; paving the way towards
real-world assistive robot.

</details>


### [34] [FreezeVLA: Action-Freezing Attacks against Vision-Language-Action Models](https://arxiv.org/abs/2509.19870)
*Xin Wang,Jie Li,Zejia Weng,Yixu Wang,Yifeng Gao,Tianyu Pang,Chao Du,Yan Teng,Yingchun Wang,Zuxuan Wu,Xingjun Ma,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 该论文提出了FreezeVLA攻击框架，揭示VLA模型存在"冻结"漏洞，攻击成功率高达76.2%，暴露了VLA模型的安全风险。


<details>
  <summary>Details</summary>
Motivation: VLA模型在机器人领域发展迅速，但其对抗攻击的安全性和鲁棒性研究不足，特别是可能导致机器人"冻结"忽略指令的漏洞。

Method: 提出FreezeVLA攻击框架，通过min-max双层优化生成和评估动作冻结攻击，在三个先进VLA模型和四个机器人基准上进行实验。

Result: FreezeVLA平均攻击成功率达76.2%，显著优于现有方法，且生成的对抗图像具有强迁移性，单个图像可跨不同语言提示可靠诱导瘫痪。

Conclusion: 研究揭示了VLA模型的关键安全风险，强调了开发鲁棒防御机制的紧迫性。

Abstract: Vision-Language-Action (VLA) models are driving rapid progress in robotics by
enabling agents to interpret multimodal inputs and execute complex,
long-horizon tasks. However, their safety and robustness against adversarial
attacks remain largely underexplored. In this work, we identify and formalize a
critical adversarial vulnerability in which adversarial images can "freeze" VLA
models and cause them to ignore subsequent instructions. This threat
effectively disconnects the robot's digital mind from its physical actions,
potentially inducing inaction during critical interventions. To systematically
study this vulnerability, we propose FreezeVLA, a novel attack framework that
generates and evaluates action-freezing attacks via min-max bi-level
optimization. Experiments on three state-of-the-art VLA models and four robotic
benchmarks show that FreezeVLA attains an average attack success rate of 76.2%,
significantly outperforming existing methods. Moreover, adversarial images
generated by FreezeVLA exhibit strong transferability, with a single image
reliably inducing paralysis across diverse language prompts. Our findings
expose a critical safety risk in VLA models and highlight the urgent need for
robust defense mechanisms.

</details>


### [35] [Adaptive Guidance Semantically Enhanced via Multimodal LLM for Edge-Cloud Object Detection](https://arxiv.org/abs/2509.19875)
*Yunqing Hu,Zheming Yang,Chang Zhao,Wen Ji*

Main category: cs.CV

TL;DR: 提出基于自适应引导的语义增强边云协同目标检测方法，利用MLLM在复杂场景下平衡检测精度与效率


<details>
  <summary>Details</summary>
Motivation: 传统目标检测方法在低光照、严重遮挡等复杂场景下性能下降，缺乏高层语义理解能力

Method: 通过指令微调使MLLM生成结构化场景描述，设计自适应映射机制将语义信息动态转换为边缘检测器参数调整信号，构建边云协同推理框架

Result: 在低光照和高度遮挡场景中，延迟降低79%以上，计算成本降低70%，同时保持检测精度

Conclusion: 该方法有效提升了复杂场景下的目标检测性能，实现了精度与效率的良好平衡

Abstract: Traditional object detection methods face performance degradation challenges
in complex scenarios such as low-light conditions and heavy occlusions due to a
lack of high-level semantic understanding. To address this, this paper proposes
an adaptive guidance-based semantic enhancement edge-cloud collaborative object
detection method leveraging Multimodal Large Language Models (MLLM), achieving
an effective balance between accuracy and efficiency. Specifically, the method
first employs instruction fine-tuning to enable the MLLM to generate structured
scene descriptions. It then designs an adaptive mapping mechanism that
dynamically converts semantic information into parameter adjustment signals for
edge detectors, achieving real-time semantic enhancement. Within an edge-cloud
collaborative inference framework, the system automatically selects between
invoking cloud-based semantic guidance or directly outputting edge detection
results based on confidence scores. Experiments demonstrate that the proposed
method effectively enhances detection accuracy and efficiency in complex
scenes. Specifically, it can reduce latency by over 79% and computational cost
by 70% in low-light and highly occluded scenes while maintaining accuracy.

</details>


### [36] [Generalized Shortest Path-based Superpixels for 3D Spherical Image Segmentation](https://arxiv.org/abs/2509.19895)
*Rémi Giraud,Rodrigo Borba Pinheiro,Yannick Berthoumieu*

Main category: cs.CV

TL;DR: 提出了SphSPS方法，专门用于360度球形或全向图像的超像素分割，通过考虑3D球形采集空间的几何结构来提高分割精度和形状规整性


<details>
  <summary>Details</summary>
Motivation: 随着广角图像采集设备的广泛使用和计算机视觉中对快速准确图像分析的需求增加，需要专门的方法来处理球形图像。现有的超像素分割方法主要针对标准2D平面图像设计，无法有效处理球形图像的几何失真问题

Method: SphSPS方法基于球形最短路径的超像素分割，通过将像素与超像素中心之间的最短路径概念推广到球形空间，快速提取相关聚类特征。同时还将全局规整性度量推广到球形空间

Result: 在参考的360度球形全景分割数据集和合成道路全向图像上验证，SphSPS方法在分割精度、噪声鲁棒性和规整性方面显著优于现有的平面和球形方法

Conclusion: SphSPS方法为360度图像的超像素应用提供了非常有价值的工具，通过考虑采集空间的几何结构有效提升了分割性能

Abstract: The growing use of wide angle image capture devices and the need for fast and
accurate image analysis in computer visions have enforced the need for
dedicated under-representation approaches. Most recent decomposition methods
segment an image into a small number of irregular homogeneous regions, called
superpixels. Nevertheless, these approaches are generally designed to segment
standard 2D planar images, i.e., captured with a 90o angle view without
distortion. In this work, we introduce a new general superpixel method called
SphSPS (for Spherical Shortest Path-based Superpixels)1 , dedicated to wide
360o spherical or omnidirectional images. Our method respects the geometry of
the 3D spherical acquisition space and generalizes the notion of shortest path
between a pixel and a superpixel center, to fastly extract relevant clustering
features. We demonstrate that considering the geometry of the acquisition space
to compute the shortest path enables to jointly improve the segmentation
accuracy and the shape regularity of superpixels. To evaluate this regularity
aspect, we also generalize a global regularity metric to the spherical space,
addressing the limitations of the only existing spherical compactness measure.
Finally, the proposed SphSPS method is validated on the reference 360o
spherical panorama segmentation dataset and on synthetic road omnidirectional
images. Our method significantly outperforms both planar and spherical
state-of-the-art approaches in terms of segmentation accuracy,robustness to
noise and regularity, providing a very interesting tool for superpixel-based
applications on 360o images.

</details>


### [37] [Efficient Cell Painting Image Representation Learning via Cross-Well Aligned Masked Siamese Network](https://arxiv.org/abs/2509.19896)
*Pin-Jui Huang,Yu-Hsuan Liao,SooHeon Kim,NoSeong Park,JongBae Park,DongMyung Shin*

Main category: cs.CV

TL;DR: 提出CWA-MSN框架，通过跨孔对齐的掩码孪生网络学习细胞图像表征，在数据量和参数更少的情况下显著优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有的自监督和对比学习方法需要大规模模型和大量数据，且难以处理细胞成像中的批次效应问题

Method: Cross-Well Aligned Masked Siamese Network (CWA-MSN)，通过跨孔对齐相同扰动细胞的嵌入，在掩码孪生架构中强制语义一致性

Result: 在基因-基因关系检索基准上，CWA-MSN比OpenPhenom提升29%，比CellCLIP提升9%，且训练数据更少（0.2M vs 2.2M），模型更小（22M vs 1.48B参数）

Conclusion: CWA-MSN是学习细胞图像表征的简单有效方法，在有限数据和参数预算下仍能实现高效表型建模

Abstract: Computational models that predict cellular phenotypic responses to chemical
and genetic perturbations can accelerate drug discovery by prioritizing
therapeutic hypotheses and reducing costly wet-lab iteration. However,
extracting biologically meaningful and batch-robust cell painting
representations remains challenging. Conventional self-supervised and
contrastive learning approaches often require a large-scale model and/or a huge
amount of carefully curated data, still struggling with batch effects. We
present Cross-Well Aligned Masked Siamese Network (CWA-MSN), a novel
representation learning framework that aligns embeddings of cells subjected to
the same perturbation across different wells, enforcing semantic consistency
despite batch effects. Integrated into a masked siamese architecture, this
alignment yields features that capture fine-grained morphology while remaining
data- and parameter-efficient. For instance, in a gene-gene relationship
retrieval benchmark, CWA-MSN outperforms the state-of-the-art publicly
available self-supervised (OpenPhenom) and contrastive learning (CellCLIP)
methods, improving the benchmark scores by +29\% and +9\%, respectively, while
training on substantially fewer data (e.g., 0.2M images for CWA-MSN vs. 2.2M
images for OpenPhenom) or smaller model size (e.g., 22M parameters for CWA-MSN
vs. 1.48B parameters for CellCLIP). Extensive experiments demonstrate that
CWA-MSN is a simple and effective way to learn cell image representation,
enabling efficient phenotype modeling even under limited data and parameter
budgets.

</details>


### [38] [Aerial-Ground Image Feature Matching via 3D Gaussian Splatting-based Intermediate View Rendering](https://arxiv.org/abs/2509.19898)
*Jiangxue Yu,Hui Wang,San Jiang,Xing Zhang,Dejin Zhang,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种通过生成中间视图来缓解视角畸变的地空图像特征匹配算法，能显著提高匹配数量和重建质量


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景3D建模中地空图像因视角差异大导致难以找到可靠对应点的问题

Method: 首先用SfM重建稀疏模型，然后用3D高斯泼溅生成中间视图，最后通过中间视图进行地空图像的特征匹配

Result: 实验证明该方法能显著增加初始和精炼匹配数量，实现准确的ISfM重建和完整的3DGS场景渲染

Conclusion: 该方法为地空图像提供了可靠的特征匹配解决方案，有效提升了3D建模质量

Abstract: The integration of aerial and ground images has been a promising solution in
3D modeling of complex scenes, which is seriously restricted by finding
reliable correspondences. The primary contribution of this study is a feature
matching algorithm for aerial and ground images, whose core idea is to generate
intermediate views to alleviate perspective distortions caused by the extensive
viewpoint changes. First, by using aerial images only, sparse models are
reconstructed through an incremental SfM (Structure from Motion) engine due to
their large scene coverage. Second, 3D Gaussian Splatting is then adopted for
scene rendering by taking as inputs sparse points and oriented images. For
accurate view rendering, a render viewpoint determination algorithm is designed
by using the oriented camera poses of aerial images, which is used to generate
high-quality intermediate images that can bridge the gap between aerial and
ground images. Third, with the aid of intermediate images, reliable feature
matching is conducted for match pairs from render-aerial and render-ground
images, and final matches can be generated by transmitting correspondences
through intermediate views. By using real aerial and ground datasets, the
validation of the proposed solution has been verified in terms of feature
matching and scene rendering and compared comprehensively with widely used
methods. The experimental results demonstrate that the proposed solution can
provide reliable feature matches for aerial and ground images with an obvious
increase in the number of initial and refined matches, and it can provide
enough matches to achieve accurate ISfM reconstruction and complete 3DGS-based
scene rendering.

</details>


### [39] [CapStARE: Capsule-based Spatiotemporal Architecture for Robust and Efficient Gaze Estimation](https://arxiv.org/abs/2509.19936)
*Miren Samaniego,Igor Rodriguez,Elena Lazkano*

Main category: cs.CV

TL;DR: CapStARE是一种基于胶囊的时空架构，用于视线估计，通过ConvNeXt主干、注意力路由胶囊形成和双GRU解码器实现高效的部分-整体推理和分离的时间建模，在多个数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决视线估计中复杂的时空动态建模问题，需要一种能够同时处理慢速和快速视线变化、并具有良好可解释性的高效架构。

Method: 采用ConvNeXt作为特征提取主干，通过注意力路由机制形成胶囊，使用双GRU解码器分别处理慢速和快速的视线动态变化，实现模块化设计。

Result: 在ETH-XGaze（3.36）、MPIIFaceGaze（2.65）、Gaze360（9.06）和RT-GENE（4.76）数据集上均达到或超越现有方法的最佳性能，同时保持实时推理速度（<10ms）和更少的参数。

Conclusion: CapStARE为交互式系统中的实时视线估计提供了一个实用且鲁棒的解决方案，具有良好的泛化能力和可解释性。

Abstract: We introduce CapStARE, a capsule-based spatio-temporal architecture for gaze
estimation that integrates a ConvNeXt backbone, capsule formation with
attention routing, and dual GRU decoders specialized for slow and rapid gaze
dynamics. This modular design enables efficient part-whole reasoning and
disentangled temporal modeling, achieving state-of-the-art performance on
ETH-XGaze (3.36) and MPIIFaceGaze (2.65) while maintaining real-time inference
(< 10 ms). The model also generalizes well to unconstrained conditions in
Gaze360 (9.06) and human-robot interaction scenarios in RT-GENE (4.76),
outperforming or matching existing methods with fewer parameters and greater
interpretability. These results demonstrate that CapStARE offers a practical
and robust solution for real-time gaze estimation in interactive systems. The
related code and results for this article can be found on:
https://github.com/toukapy/capsStare

</details>


### [40] [GS-RoadPatching: Inpainting Gaussians via 3D Searching and Placing for Driving Scenes](https://arxiv.org/abs/2509.19937)
*Guo Chen,Jiarun Liu,Sicong Du,Chenming Wu,Deqi Li,Shi-Sheng Huang,Guofeng Zhang,Sheng Yang*

Main category: cs.CV

TL;DR: GS-RoadPatching是一种基于3D高斯分布的驾驶场景补全方法，通过参考完全重建区域进行替代性修复，无需依赖2D扩散模型或重新训练高斯模型


<details>
  <summary>Details</summary>
Motivation: 现有3DGS修复方法依赖2D视角的扩散或GAN模型来预测缺失区域的外观或深度信息，需要处理跨模态的空间-时间一致性，且需要耗时的重新训练。驾驶场景中高度重复的模式适合在3DGS特征空间中进行结构匹配

Method: 构建特征嵌入的3DGS场景，采用多尺度局部上下文抽象方法，提出3D空间结构搜索找到候选补丁，然后进行简单有效的替代融合优化

Result: 在多个公开数据集上的实验表明，该方法在驾驶场景中达到了最先进的性能，质量和互操作性均优于基线方法，在通用场景中也展示了适用性

Conclusion: GS-RoadPatching提供了一种直接在3DGS模态下进行替代性场景修复和编辑的有效方法，摆脱了对2D跨模态一致性的依赖，无需重新训练高斯模型

Abstract: This paper presents GS-RoadPatching, an inpainting method for driving scene
completion by referring to completely reconstructed regions, which are
represented by 3D Gaussian Splatting (3DGS). Unlike existing 3DGS inpainting
methods that perform generative completion relying on 2D perspective-view-based
diffusion or GAN models to predict limited appearance or depth cues for missing
regions, our approach enables substitutional scene inpainting and editing
directly through the 3DGS modality, extricating it from requiring
spatial-temporal consistency of 2D cross-modals and eliminating the need for
time-intensive retraining of Gaussians. Our key insight is that the highly
repetitive patterns in driving scenes often share multi-modal similarities
within the implicit 3DGS feature space and are particularly suitable for
structural matching to enable effective 3DGS-based substitutional inpainting.
Practically, we construct feature-embedded 3DGS scenes to incorporate a patch
measurement method for abstracting local context at different scales and,
subsequently, propose a structural search method to find candidate patches in
3D space effectively. Finally, we propose a simple yet effective
substitution-and-fusion optimization for better visual harmony. We conduct
extensive experiments on multiple publicly available datasets to demonstrate
the effectiveness and efficiency of our proposed method in driving scenes, and
the results validate that our method achieves state-of-the-art performance
compared to the baseline methods in terms of both quality and interoperability.
Additional experiments in general scenes also demonstrate the applicability of
the proposed 3D inpainting strategy. The project page and code are available
at: https://shanzhaguoo.github.io/GS-RoadPatching/

</details>


### [41] [Interpreting ResNet-based CLIP via Neuron-Attention Decomposition](https://arxiv.org/abs/2509.19943)
*Edmund Bu,Yossi Gandelsman*

Main category: cs.CV

TL;DR: 提出了一种解释CLIP-ResNet神经元的新技术，通过将神经元对输出的贡献分解为单个计算路径来分析神经元-注意力头对，发现这些对可以用CLIP-ResNet图像-文本嵌入空间中的单一方向来近似，从而实现神经元的文本解释。


<details>
  <summary>Details</summary>
Motivation: 理解CLIP-ResNet中神经元的内部工作机制，探索神经网络的可解释性单元，并利用这些单元进行下游任务应用。

Method: 分析CLIP-ResNet中所有神经元对和后续注意力头的组合，将神经元-注意力头对近似为嵌入空间中的单一方向，通过文本关联进行解释，并利用稀疏性特征进行应用。

Result: 发现神经元-注意力头对可以用单一方向近似，且只有稀疏的神经元-头对具有显著贡献；在无训练语义分割任务中优于先前方法，并能有效监测数据集分布偏移。

Conclusion: 通过分析神经网络中的单个计算路径可以发现可解释单元，这些单元可以成功应用于下游任务，为神经网络可解释性研究提供了新思路。

Abstract: We present a novel technique for interpreting the neurons in CLIP-ResNet by
decomposing their contributions to the output into individual computation
paths. More specifically, we analyze all pairwise combinations of neurons and
the following attention heads of CLIP's attention-pooling layer. We find that
these neuron-head pairs can be approximated by a single direction in
CLIP-ResNet's image-text embedding space. Leveraging this insight, we interpret
each neuron-head pair by associating it with text. Additionally, we find that
only a sparse set of the neuron-head pairs have a significant contribution to
the output value, and that some neuron-head pairs, while polysemantic,
represent sub-concepts of their corresponding neurons. We use these
observations for two applications. First, we employ the pairs for training-free
semantic segmentation, outperforming previous methods for CLIP-ResNet. Second,
we utilize the contributions of neuron-head pairs to monitor dataset
distribution shifts. Our results demonstrate that examining individual
computation paths in neural networks uncovers interpretable units, and that
such units can be utilized for downstream tasks.

</details>


### [42] [When Words Can't Capture It All: Towards Video-Based User Complaint Text Generation with Multimodal Video Complaint Dataset](https://arxiv.org/abs/2509.19952)
*Sarmistha Das,R E Zera Marveen Lyngkhoi,Kirtan Jain,Vinayak Goyal,Sriparna Saha,Manish Gupta*

Main category: cs.CV

TL;DR: 本文提出了一个名为CoD-V的新任务，旨在通过视频生成投诉描述，以帮助用户更清晰地表达产品问题。作者构建了ComVID数据集，并开发了基于多模态RAG的VideoLLaMA2-7b模型来生成考虑用户情感状态的投诉描述。


<details>
  <summary>Details</summary>
Motivation: 现有可解释投诉挖掘研究存在局限性，用户难以通过文字清晰表达投诉内容，但可以轻松上传展示产品缺陷的视频。本文旨在解决用户表达投诉的困难，通过视频辅助生成更准确的投诉描述。

Method: 构建了包含1,175个投诉视频的ComVID数据集，提出了新的投诉保留(CR)评估指标，并开发了多模态检索增强生成(RAG)嵌入的VideoLLaMA2-7b模型，该模型能够考虑用户情感状态生成投诉描述。

Result: 对多个视频语言模型进行了全面评估，使用了METEOR、困惑度和Coleman-Liau可读性分数等多种评估指标。研究为新研究方向奠定了基础，为通过视频表达投诉提供了平台。

Conclusion: 本研究开创了通过视频生成投诉描述的新研究领域，为解决用户投诉表达困难提供了有效解决方案，数据集和资源已公开可用。

Abstract: While there exists a lot of work on explainable complaint mining,
articulating user concerns through text or video remains a significant
challenge, often leaving issues unresolved. Users frequently struggle to
express their complaints clearly in text but can easily upload videos depicting
product defects (e.g., vague text such as `worst product' paired with a
5-second video depicting a broken headphone with the right earcup). This paper
formulates a new task in the field of complaint mining to aid the common users'
need to write an expressive complaint, which is Complaint Description from
Videos (CoD-V) (e.g., to help the above user articulate her complaint about the
defective right earcup). To this end, we introduce ComVID, a video complaint
dataset containing 1,175 complaint videos and the corresponding descriptions,
also annotated with the emotional state of the complainer. Additionally, we
present a new complaint retention (CR) evaluation metric that discriminates the
proposed (CoD-V) task against standard video summary generation and description
tasks. To strengthen this initiative, we introduce a multimodal
Retrieval-Augmented Generation (RAG) embedded VideoLLaMA2-7b model, designed to
generate complaints while accounting for the user's emotional state. We conduct
a comprehensive evaluation of several Video Language Models on several tasks
(pre-trained and fine-tuned versions) with a range of established evaluation
metrics, including METEOR, perplexity, and the Coleman-Liau readability score,
among others. Our study lays the foundation for a new research direction to
provide a platform for users to express complaints through video. Dataset and
resources are available at: https://github.com/sarmistha-D/CoD-V.

</details>


### [43] [SynchroRaMa : Lip-Synchronized and Emotion-Aware Talking Face Generation via Multi-Modal Emotion Embedding](https://arxiv.org/abs/2509.19965)
*Phyo Thet Yee,Dimitrios Kollias,Sudeepta Mishra,Abhinav Dhall*

Main category: cs.CV

TL;DR: SynchroRaMa是一个新颖的音频驱动说话人脸生成框架，通过多模态情感嵌入和动态场景描述，解决了现有方法在情感表达和动态变化方面的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一模态（音频或图像）进行情感嵌入，难以捕捉细腻的情感线索；同时仅基于单张参考图像，无法有效表示跨时间的动态动作或属性变化。

Method: 整合多模态情感嵌入（文本情感分析+语音情感识别+音频价唤醒特征），包含音频到运动模块生成同步运动帧，并引入LLM生成的场景描述作为额外文本输入。

Result: 在基准数据集上的定量和定性实验表明，SynchroRaMa在图像质量、表情保持和运动真实性方面优于现有最优方法，用户研究也确认其在自然度、运动多样性和视频流畅度方面获得更高主观评分。

Conclusion: SynchroRaMa通过多模态融合和动态条件约束，显著提升了说话人脸生成的情感表达真实性和时间一致性，为自然的人机交互应用提供了有效解决方案。

Abstract: Audio-driven talking face generation has received growing interest,
particularly for applications requiring expressive and natural human-avatar
interaction. However, most existing emotion-aware methods rely on a single
modality (either audio or image) for emotion embedding, limiting their ability
to capture nuanced affective cues. Additionally, most methods condition on a
single reference image, restricting the model's ability to represent dynamic
changes in actions or attributes across time. To address these issues, we
introduce SynchroRaMa, a novel framework that integrates a multi-modal emotion
embedding by combining emotional signals from text (via sentiment analysis) and
audio (via speech-based emotion recognition and audio-derived valence-arousal
features), enabling the generation of talking face videos with richer and more
authentic emotional expressiveness and fidelity. To ensure natural head motion
and accurate lip synchronization, SynchroRaMa includes an audio-to-motion (A2M)
module that generates motion frames aligned with the input audio. Finally,
SynchroRaMa incorporates scene descriptions generated by Large Language Model
(LLM) as additional textual input, enabling it to capture dynamic actions and
high-level semantic attributes. Conditioning the model on both visual and
textual cues enhances temporal consistency and visual realism. Quantitative and
qualitative experiments on benchmark datasets demonstrate that SynchroRaMa
outperforms the state-of-the-art, achieving improvements in image quality,
expression preservation, and motion realism. A user study further confirms that
SynchroRaMa achieves higher subjective ratings than competing methods in
overall naturalness, motion diversity, and video smoothness. Our project page
is available at <https://novicemm.github.io/synchrorama>.

</details>


### [44] [OmniScene: Attention-Augmented Multimodal 4D Scene Understanding for Autonomous Driving](https://arxiv.org/abs/2509.19973)
*Pei Liu,Hongliang Lu,Haichao Liu,Haipeng Liu,Xin Liu,Ruoyu Yao,Shengbo Eben Li,Jun Ma*

Main category: cs.CV

TL;DR: 提出OmniScene框架，通过视觉语言模型和多模态融合实现类人的4D场景理解，在自动驾驶中超越现有方法


<details>
  <summary>Details</summary>
Motivation: 当前自动驾驶系统缺乏真正的场景理解能力，主要依赖基于深度的3D重建而非类人的三维场景认知

Method: 1. 提出OmniVLM视觉语言框架整合多视角和时序感知；2. 采用师生架构和知识蒸馏将文本表示嵌入3D实例特征；3. 提出分层融合策略自适应校准几何和语义特征的相对重要性

Result: 在nuScenes数据集上全面评估，在感知、预测、规划和视觉问答等任务中超越十余种SOTA模型，建立新基准

Conclusion: OmniScene框架实现了更类人的感知-理解-行动架构，为自动驾驶系统提供了真正的场景理解能力

Abstract: Human vision is capable of transforming two-dimensional observations into an
egocentric three-dimensional scene understanding, which underpins the ability
to translate complex scenes and exhibit adaptive behaviors. This capability,
however, remains lacking in current autonomous driving systems, where
mainstream approaches primarily rely on depth-based 3D reconstruction rather
than true scene understanding. To address this limitation, we propose a novel
human-like framework called OmniScene. First, we introduce the OmniScene
Vision-Language Model (OmniVLM), a vision-language framework that integrates
multi-view and temporal perception for holistic 4D scene understanding. Then,
harnessing a teacher-student OmniVLM architecture and knowledge distillation,
we embed textual representations into 3D instance features for semantic
supervision, enriching feature learning, and explicitly capturing human-like
attentional semantics. These feature representations are further aligned with
human driving behaviors, forming a more human-like
perception-understanding-action architecture. In addition, we propose a
Hierarchical Fusion Strategy (HFS) to address imbalances in modality
contributions during multimodal integration. Our approach adaptively calibrates
the relative significance of geometric and semantic features at multiple
abstraction levels, enabling the synergistic use of complementary cues from
visual and textual modalities. This learnable dynamic fusion enables a more
nuanced and effective exploitation of heterogeneous information. We evaluate
OmniScene comprehensively on the nuScenes dataset, benchmarking it against over
ten state-of-the-art models across various tasks. Our approach consistently
achieves superior results, establishing new benchmarks in perception,
prediction, planning, and visual question answering.

</details>


### [45] [CamPVG: Camera-Controlled Panoramic Video Generation with Epipolar-Aware Diffusion](https://arxiv.org/abs/2509.19979)
*Chenhao Ji,Chaohui Yu,Junyao Gao,Fan Wang,Cairong Zhao*

Main category: cs.CV

TL;DR: 提出了CamPVG，首个基于扩散模型的精确相机姿态引导的全景视频生成框架，解决了全景姿态表示和球面投影的复杂性挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注透视投影视频中的相机控制，而几何一致的全景视频生成仍然具有挑战性，主要由于全景姿态表示和球面投影的固有复杂性。

Method: 提出全景Plücker嵌入通过球面坐标变换编码相机外参，并引入球面极线模块通过沿极线自适应注意力掩码实现细粒度跨视图特征聚合。

Result: 大量实验表明，该方法能生成与相机轨迹一致的高质量全景视频，在全景视频生成方面远超现有方法。

Conclusion: CamPVG框架成功解决了全景视频生成的几何一致性难题，为精确相机控制的全景视频生成提供了有效解决方案。

Abstract: Recently, camera-controlled video generation has seen rapid development,
offering more precise control over video generation. However, existing methods
predominantly focus on camera control in perspective projection video
generation, while geometrically consistent panoramic video generation remains
challenging. This limitation is primarily due to the inherent complexities in
panoramic pose representation and spherical projection. To address this issue,
we propose CamPVG, the first diffusion-based framework for panoramic video
generation guided by precise camera poses. We achieve camera position encoding
for panoramic images and cross-view feature aggregation based on spherical
projection. Specifically, we propose a panoramic Pl\"ucker embedding that
encodes camera extrinsic parameters through spherical coordinate
transformation. This pose encoder effectively captures panoramic geometry,
overcoming the limitations of traditional methods when applied to
equirectangular projections. Additionally, we introduce a spherical epipolar
module that enforces geometric constraints through adaptive attention masking
along epipolar lines. This module enables fine-grained cross-view feature
aggregation, substantially enhancing the quality and consistency of generated
panoramic videos. Extensive experiments demonstrate that our method generates
high-quality panoramic videos consistent with camera trajectories, far
surpassing existing methods in panoramic video generation.

</details>


### [46] [SDE-DET: A Precision Network for Shatian Pomelo Detection in Complex Orchard Environments](https://arxiv.org/abs/2509.19990)
*Yihao Hu,Pan Wang,Xiaodong Bai,Shijie Cai,Hang Wang,Huazhong Liu,Aiping Yang,Xiangxiang Li,Meiping Ding,Hongyan Liu,Jianguo Yao*

Main category: cs.CV

TL;DR: 该论文提出了SDE-DET模型用于沙田柚检测，通过Star Block、Deformable Attention和Efficient Multi-Scale Attention机制解决复杂果园环境中的多尺度、遮挡和小目标检测问题，在STP-AgriData数据集上取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 沙田柚检测对于自动化采摘和成熟度分析至关重要，但在复杂果园环境中面临多尺度问题、树干和叶子遮挡、小目标检测等挑战。

Method: 构建STP-AgriData数据集，提出SDE-DET模型：使用Star Block获取高维信息而不增加计算开销；采用Deformable Attention增强遮挡条件下的检测能力；集成多个Efficient Multi-Scale Attention机制减少计算开销并提取深层视觉特征。

Result: SDE-DET在Precision、Recall、mAP@0.5、mAP@0.5:0.95和F1-score上分别达到0.883、0.771、0.838、0.497和0.823，优于Yolo系列和其他主流检测模型。

Conclusion: SDE-DET为沙田柚检测提供了可靠方法，为自动采摘机器人的进一步发展奠定了基础。

Abstract: Pomelo detection is an essential process for their localization, automated
robotic harvesting, and maturity analysis. However, detecting Shatian pomelo in
complex orchard environments poses significant challenges, including
multi-scale issues, obstructions from trunks and leaves, small object
detection, etc. To address these issues, this study constructs a custom dataset
STP-AgriData and proposes the SDE-DET model for Shatian pomelo detection.
SDE-DET first utilizes the Star Block to effectively acquire high-dimensional
information without increasing the computational overhead. Furthermore, the
presented model adopts Deformable Attention in its backbone, to enhance its
ability to detect pomelos under occluded conditions. Finally, multiple
Efficient Multi-Scale Attention mechanisms are integrated into our model to
reduce the computational overhead and extract deep visual representations,
thereby improving the capacity for small object detection. In the experiment,
we compared SDE-DET with the Yolo series and other mainstream detection models
in Shatian pomelo detection. The presented SDE-DET model achieved scores of
0.883, 0.771, 0.838, 0.497, and 0.823 in Precision, Recall, mAP@0.5,
mAP@0.5:0.95 and F1-score, respectively. SDE-DET has achieved state-of-the-art
performance on the STP-AgriData dataset. Experiments indicate that the SDE-DET
provides a reliable method for Shatian pomelo detection, laying the foundation
for the further development of automatic harvest robots.

</details>


### [47] [Improving Generalizability and Undetectability for Targeted Adversarial Attacks on Multimodal Pre-trained Models](https://arxiv.org/abs/2509.19994)
*Zhifang Zhang,Jiahan Zhang,Shengjie Zhou,Qi Wei,Shuo He,Feng Liu,Lei Feng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Proxy Targeted Attack (PTA)的新方法，用于改进多模态预训练模型中的目标对抗攻击，解决了现有攻击在泛化性和不可检测性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态预训练模型（如ImageBind）目标对抗攻击存在两个主要问题：1）在跨模态对齐任务中对部分已知或语义相似目标的泛化能力有限；2）容易被简单的异常检测方法检测到。

Method: 提出了Proxy Targeted Attack (PTA)方法，利用多个源模态和目标模态代理来优化目标对抗样本，确保其既能逃避防御检测，又能与多个潜在目标对齐。同时提供了理论分析来确保在满足不可检测性要求的同时实现最优泛化性。

Result: 实验结果表明，PTA方法能够在各种相关目标上实现高成功率，并且对多种异常检测方法保持不可检测性。

Conclusion: PTA方法有效解决了多模态预训练模型目标对抗攻击的泛化性和不可检测性问题，为模型安全性提供了重要改进。

Abstract: Multimodal pre-trained models (e.g., ImageBind), which align distinct data
modalities into a shared embedding space, have shown remarkable success across
downstream tasks. However, their increasing adoption raises serious security
concerns, especially regarding targeted adversarial attacks. In this paper, we
show that existing targeted adversarial attacks on multimodal pre-trained
models still have limitations in two aspects: generalizability and
undetectability. Specifically, the crafted targeted adversarial examples (AEs)
exhibit limited generalization to partially known or semantically similar
targets in cross-modal alignment tasks (i.e., limited generalizability) and can
be easily detected by simple anomaly detection methods (i.e., limited
undetectability). To address these limitations, we propose a novel method
called Proxy Targeted Attack (PTA), which leverages multiple source-modal and
target-modal proxies to optimize targeted AEs, ensuring they remain evasive to
defenses while aligning with multiple potential targets. We also provide
theoretical analyses to highlight the relationship between generalizability and
undetectability and to ensure optimal generalizability while meeting the
specified requirements for undetectability. Furthermore, experimental results
demonstrate that our PTA can achieve a high success rate across various related
targets and remain undetectable against multiple anomaly detection methods.

</details>


### [48] [Anomaly Detection by Clustering DINO Embeddings using a Dirichlet Process Mixture](https://arxiv.org/abs/2509.19997)
*Nico Schulthess,Ender Konukoglu*

Main category: cs.CV

TL;DR: 该论文提出了一种使用DINOv2嵌入和Dirichlet过程混合模型进行医学图像异常检测的无监督方法，相比基于内存库的方法显著降低了计算负担。


<details>
  <summary>Details</summary>
Motivation: 现有的基于内存库的异常检测方法在大型医学数据集上计算负担过重，需要一种更高效的无监督异常检测方案。

Method: 使用DINOv2基础模型提取特征嵌入，通过Dirichlet过程混合模型对正常特征分布进行建模，利用组件中心与嵌入的相似度作为异常评分函数。

Result: 实验表明该方法在医学图像异常检测基准上达到竞争性性能，同时推理时间至少减少一半，归一化DINOv2嵌入与解剖结构更对齐。

Conclusion: DPMM建模的DINOv2嵌入为医学图像异常检测提供了高效且有效的表示，即使模型在自然图像上训练也能很好地迁移到医学领域。

Abstract: In this work, we leverage informative embeddings from foundational models for
unsupervised anomaly detection in medical imaging. For small datasets, a
memory-bank of normative features can directly be used for anomaly detection
which has been demonstrated recently. However, this is unsuitable for large
medical datasets as the computational burden increases substantially.
Therefore, we propose to model the distribution of normative DINOv2 embeddings
with a Dirichlet Process Mixture model (DPMM), a non-parametric mixture model
that automatically adjusts the number of mixture components to the data at
hand. Rather than using a memory bank, we use the similarity between the
component centers and the embeddings as anomaly score function to create a
coarse anomaly segmentation mask. Our experiments show that through DPMM
embeddings of DINOv2, despite being trained on natural images, achieve very
competitive anomaly detection performance on medical imaging benchmarks and can
do this while at least halving the computation time at inference. Our analysis
further indicates that normalized DINOv2 embeddings are generally more aligned
with anatomical structures than unnormalized features, even in the presence of
anomalies, making them great representations for anomaly detection. The code is
available at https://github.com/NicoSchulthess/anomalydino-dpmm.

</details>


### [49] [Table Detection with Active Learning](https://arxiv.org/abs/2509.20003)
*Somraj Gautam,Nachiketa Purohit,Gaurav Harit*

Main category: cs.CV

TL;DR: 提出了一种结合不确定性和多样性策略的主动学习方法，用于表格检测任务，在有限标注预算下显著减少标注工作量并保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决目标检测任务中数据标注效率低下的问题，传统主动学习方法主要依赖不确定性选择，但结合多样性策略可以提高采样效率。

Method: 在表格检测任务中应用主动学习，结合不确定性和多样性策略选择最具信息量的样本，在CascadeTabNet和YOLOv9架构上评估。

Result: 在TableBank-LaTeX和TableBank-Word数据集上，主动学习方法显著优于随机采样，在相同标注预算下获得更高的mAP分数。

Conclusion: 主动学习方法能够有效减少标注成本，在有限预算下达到与全监督模型相当的性能，为数据高效标注提供了可行方案。

Abstract: Efficient data annotation remains a critical challenge in machine learning,
particularly for object detection tasks requiring extensive labeled data.
Active learning (AL) has emerged as a promising solution to minimize annotation
costs by selecting the most informative samples. While traditional AL
approaches primarily rely on uncertainty-based selection, recent advances
suggest that incorporating diversity-based strategies can enhance sampling
efficiency in object detection tasks. Our approach ensures the selection of
representative examples that improve model generalization. We evaluate our
method on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using
state-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our
results demonstrate that AL-based example selection significantly outperforms
random sampling, reducing annotation effort given a limited budget while
maintaining comparable performance to fully supervised models. Our method
achieves higher mAP scores within the same annotation budget.

</details>


### [50] [Does the Manipulation Process Matter? RITA: Reasoning Composite Image Manipulations via Reversely-Ordered Incremental-Transition Autoregression](https://arxiv.org/abs/2509.20006)
*Xuekang Zhu,Ji-Zhe Zhou,Kaiwen Feng,Chenfan Qu,Yunfei Wang,Liting Zhou,Jian liu*

Main category: cs.CV

TL;DR: 本文提出了RITA框架，将图像篡改定位重新定义为条件序列预测任务，通过分层预测方式解决传统方法维度坍塌问题，并在新基准HSIM上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像篡改定位方法采用一次性预测范式，忽略了篡改过程的时序性和层次性特征，导致维度坍塌和任务本质不匹配。

Method: 提出RITA框架，将篡改定位建模为条件序列预测任务，通过逐层预测方式显式建模编辑操作间的时序依赖和层次结构。

Result: 在传统基准和新构建的HSIM基准上均达到SOTA性能，HSS指标验证了方法的顺序性和层次对齐能力。

Conclusion: RITA为图像篡改定位提供了通用有效的范式，代码和数据集将公开。

Abstract: Image manipulations often entail a complex manipulation process, comprising a
series of editing operations to create a deceptive image, exhibiting
sequentiality and hierarchical characteristics. However, existing IML methods
remain manipulation-process-agnostic, directly producing localization masks in
a one-shot prediction paradigm without modeling the underlying editing steps.
This one-shot paradigm compresses the high-dimensional compositional space into
a single binary mask, inducing severe dimensional collapse, thereby creating a
fundamental mismatch with the intrinsic nature of the IML task.
  To address this, we are the first to reformulate image manipulation
localization as a conditional sequence prediction task, proposing the RITA
framework. RITA predicts manipulated regions layer-by-layer in an ordered
manner, using each step's prediction as the condition for the next, thereby
explicitly modeling temporal dependencies and hierarchical structures among
editing operations.
  To enable training and evaluation, we synthesize multi-step manipulation data
and construct a new benchmark HSIM. We further propose the HSS metric to assess
sequential order and hierarchical alignment. Extensive experiments show RITA
achieves SOTA on traditional benchmarks and provides a solid foundation for the
novel hierarchical localization task, validating its potential as a general and
effective paradigm. The code and dataset will be publicly available.

</details>


### [51] [PS3: A Multimodal Transformer Integrating Pathology Reports with Histology Images and Biological Pathways for Cancer Survival Prediction](https://arxiv.org/abs/2509.20022)
*Manahil Raza,Ayesha Azam,Talha Qaiser,Nasir Rajpoot*

Main category: cs.CV

TL;DR: 提出PS3模型，通过原型方法融合病理报告、全切片图像和转录组数据，在癌症生存预测任务中优于现有方法


<details>
  <summary>Details</summary>
Motivation: 现有多模态融合方法主要整合组织病理图像和基因组数据，但病理报告作为临床工作流的重要组成部分，包含专家解读和临床背景信息，可以进一步提升预后预测性能

Method: 使用原型方法生成平衡表示：病理报告诊断原型（提取诊断相关部分）、组织学原型（紧凑表示关键形态模式）、生物通路原型（编码转录组表达）。然后通过Transformer模型处理这些多模态标记，建模模态内和跨模态交互

Result: 在TCGA的六个数据集上评估，PS3模型在临床、单模态和多模态基线方法中表现最优

Conclusion: 病理报告的有效整合可以显著提升癌症生存预测性能，原型方法成功解决了多模态数据异质性问题

Abstract: Current multimodal fusion approaches in computational oncology primarily
focus on integrating multi-gigapixel histology whole slide images (WSIs) with
genomic or transcriptomic data, demonstrating improved survival prediction. We
hypothesize that incorporating pathology reports can further enhance prognostic
performance. Pathology reports, as essential components of clinical workflows,
offer readily available complementary information by summarizing
histopathological findings and integrating expert interpretations and clinical
context. However, fusing these modalities poses challenges due to their
heterogeneous nature. WSIs are high-dimensional, each containing several
billion pixels, whereas pathology reports consist of concise text summaries of
varying lengths, leading to potential modality imbalance. To address this, we
propose a prototype-based approach to generate balanced representations, which
are then integrated using a Transformer-based fusion model for survival
prediction that we term PS3 (Predicting Survival from Three Modalities).
Specifically, we present: (1) Diagnostic prototypes from pathology reports,
leveraging self-attention to extract diagnostically relevant sections and
standardize text representation; (2) Histological prototypes to compactly
represent key morphological patterns in WSIs; and (3) Biological pathway
prototypes to encode transcriptomic expressions, accurately capturing cellular
functions. PS3, the three-modal transformer model, processes the resulting
prototype-based multimodal tokens and models intra-modal and cross-modal
interactions across pathology reports, WSIs and transcriptomic data. The
proposed model outperforms state-of-the-art methods when evaluated against
clinical, unimodal and multimodal baselines on six datasets from The Cancer
Genome Atlas (TCGA). The code is available at: https://github.com/manahilr/PS3.

</details>


### [52] [Generative Adversarial Networks Applied for Privacy Preservation in Biometric-Based Authentication and Identification](https://arxiv.org/abs/2509.20024)
*Lubos Mjachky,Ivan Homoliak*

Main category: cs.CV

TL;DR: 提出了一种基于GAN的隐私保护认证方法，将人脸图像转换为视觉隐私域（如花朵或鞋子），在隐私域上训练分类器进行认证。


<details>
  <summary>Details</summary>
Motivation: 传统生物特征认证系统不允许用户控制数据使用方式，且数据可能泄露被滥用，需要保护个人隐私。

Method: 使用生成对抗网络（GAN）将人脸图像转换到视觉隐私域，然后在隐私域图像上训练认证分类器。

Result: 实验表明该方法对攻击具有鲁棒性，同时保持了有意义的实用性。

Conclusion: 该方法成功实现了隐私保护与认证效用的平衡，为生物特征认证提供了新的隐私保护解决方案。

Abstract: Biometric-based authentication systems are getting broadly adopted in many
areas. However, these systems do not allow participating users to influence the
way their data is used. Furthermore, the data may leak and can be misused
without the users' knowledge. In this paper, we propose a new authentication
method that preserves the privacy of individuals and is based on a generative
adversarial network (GAN). Concretely, we suggest using the GAN for translating
images of faces to a visually private domain (e.g., flowers or shoes).
Classifiers, which are used for authentication purposes, are then trained on
the images from the visually private domain. Based on our experiments, the
method is robust against attacks and still provides meaningful utility.

</details>


### [53] [Predictive Quality Assessment for Mobile Secure Graphics](https://arxiv.org/abs/2509.20028)
*Cas Steigstra,Sergey Milyaev,Shaodi You*

Main category: cs.CV

TL;DR: 提出了一个预测视频帧质量分数的轻量级模型框架，用于评估安全图形验证图像在下游验证任务中的可用性，解决了智能手机采集图像质量差导致的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: 智能手机采集安全图形验证图像时，由于用户拍摄条件不可控，导致高熵模式图像质量差，产生高误拒率，形成了显著的'可靠性差距'。

Method: 引入了一个预测性框架，通过轻量级模型预测视频帧的质量分数，判断其是否适合资源密集型的oracle模型进行验证。在包含32,000+图像的大规模数据集上使用重新情境化的FNMR和ISRR指标进行验证。

Result: 跨域分析发现，在冻结的ImageNet预训练网络上的轻量级探针比完全微调的模型能更好地泛化到未见过的打印技术，表明对于物理制造引起的域偏移，冻结的通用骨干网络比完全微调更稳健。

Conclusion: 对于现实世界的泛化，冻结的通用骨干网络在处理物理制造引起的域偏移时比完全微调更稳健，因为完全微调可能过度拟合源域的人工痕迹。

Abstract: The reliability of secure graphic verification, a key anti-counterfeiting
tool, is undermined by poor image acquisition on smartphones. Uncontrolled user
captures of these high-entropy patterns cause high false rejection rates,
creating a significant 'reliability gap'. To bridge this gap, we depart from
traditional perceptual IQA and introduce a framework that predictively
estimates a frame's utility for the downstream verification task. We propose a
lightweight model to predict a quality score for a video frame, determining its
suitability for a resource-intensive oracle model. Our framework is validated
using re-contextualized FNMR and ISRR metrics on a large-scale dataset of
32,000+ images from 105 smartphones. Furthermore, a novel cross-domain analysis
on graphics from different industrial printing presses reveals a key finding: a
lightweight probe on a frozen, ImageNet-pretrained network generalizes better
to an unseen printing technology than a fully fine-tuned model. This provides a
key insight for real-world generalization: for domain shifts from physical
manufacturing, a frozen general-purpose backbone can be more robust than full
fine-tuning, which can overfit to source-domain artifacts.

</details>


### [54] [SHMoAReg: Spark Deformable Image Registration via Spatial Heterogeneous Mixture of Experts and Attention Heads](https://arxiv.org/abs/2509.20073)
*Yuxi Zheng,Jianhui Feng,Tianran Li,Marius Staring,Yuchuan Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种名为SHMoAReg的新型专家引导的可变形图像配准网络，首次将混合专家机制引入DIR任务。该网络在编码器中使用混合注意力头，在解码器中使用空间异构混合专家机制，实现了更专业化的特征提取和异构的三维变形场预测。


<details>
  <summary>Details</summary>
Motivation: 当前基于编码器-解码器架构的深度学习方法在可变形图像配准中存在两个主要问题：缺乏专门针对配准任务的特征提取能力，以及在所有三个方向上均匀预测变形场。这限制了配准的准确性和适应性。

Method: SHMoAReg网络在编码器层中引入混合注意力头机制，动态选择每个图像标记的最佳注意力头组合；在解码器层中使用空间异构混合专家机制，为每个体素使用不同核大小的专家来异构预测三个方向的变形场。

Result: 在两个公开数据集上的实验显示，SHMoAReg相比各种方法都有持续改进，在腹部CT数据集的Dice分数从60.58%显著提高到65.58%。同时增强了模型可解释性，能够区分不同分辨率层内和层间专家的效用。

Conclusion: SHMoAReg是首个将MoE机制引入DIR任务的网络，通过专业化特征提取和异构变形预测，显著提高了配准性能，同时增强了模型的可解释性。

Abstract: Encoder-Decoder architectures are widely used in deep learning-based
Deformable Image Registration (DIR), where the encoder extracts multi-scale
features and the decoder predicts deformation fields by recovering spatial
locations. However, current methods lack specialized extraction of features
(that are useful for registration) and predict deformation jointly and
homogeneously in all three directions. In this paper, we propose a novel
expert-guided DIR network with Mixture of Experts (MoE) mechanism applied in
both encoder and decoder, named SHMoAReg. Specifically, we incorporate Mixture
of Attention heads (MoA) into encoder layers, while Spatial Heterogeneous
Mixture of Experts (SHMoE) into the decoder layers. The MoA enhances the
specialization of feature extraction by dynamically selecting the optimal
combination of attention heads for each image token. Meanwhile, the SHMoE
predicts deformation fields heterogeneously in three directions for each voxel
using experts with varying kernel sizes. Extensive experiments conducted on two
publicly available datasets show consistent improvements over various methods,
with a notable increase from 60.58% to 65.58% in Dice score for the abdominal
CT dataset. Furthermore, SHMoAReg enhances model interpretability by
differentiating experts' utilities across/within different resolution layers.
To the best of our knowledge, we are the first to introduce MoE mechanism into
DIR tasks. The code will be released soon.

</details>


### [55] [Unleashing the Potential of the Semantic Latent Space in Diffusion Models for Image Dehazing](https://arxiv.org/abs/2509.20091)
*Zizheng Yang,Hu Yu,Bing Li,Jinghao Zhang,Jie Huang,Feng Zhao*

Main category: cs.CV

TL;DR: 提出DiffLI^2D方法，利用预训练扩散模型的语义潜在空间进行图像去雾，避免重新训练扩散模型和迭代采样过程。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像去雾中具有强大生成能力，但存在计算负担大和推理步骤多的问题，限制了其广泛应用。

Method: 探索雾霾图像在预训练扩散模型语义潜在空间中的特性，将不同时间步的扩散潜在表示集成到精心设计的去雾网络中。

Result: 在多个数据集上的实验表明，该方法优于现有的图像去雾方法。

Conclusion: DiffLI^2D通过有效利用预训练扩散模型的信息表示，为将扩散模型引入图像去雾提供了新视角。

Abstract: Diffusion models have recently been investigated as powerful generative
solvers for image dehazing, owing to their remarkable capability to model the
data distribution. However, the massive computational burden imposed by the
retraining of diffusion models, coupled with the extensive sampling steps
during the inference, limit the broader application of diffusion models in
image dehazing. To address these issues, we explore the properties of hazy
images in the semantic latent space of frozen pre-trained diffusion models, and
propose a Diffusion Latent Inspired network for Image Dehazing, dubbed
DiffLI$^2$D. Specifically, we first reveal that the semantic latent space of
pre-trained diffusion models can represent the content and haze characteristics
of hazy images, as the diffusion time-step changes. Building upon this insight,
we integrate the diffusion latent representations at different time-steps into
a delicately designed dehazing network to provide instructions for image
dehazing. Our DiffLI$^2$D avoids re-training diffusion models and iterative
sampling process by effectively utilizing the informative representations
derived from the pre-trained diffusion models, which also offers a novel
perspective for introducing diffusion models to image dehazing. Extensive
experiments on multiple datasets demonstrate that the proposed method achieves
superior performance to existing image dehazing methods. Code is available at
https://github.com/aaaasan111/difflid.

</details>


### [56] [Hyperspectral Adapter for Semantic Segmentation with Vision Foundation Models](https://arxiv.org/abs/2509.20107)
*JuanaJuana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: 提出了一种新型高光谱适配器，利用预训练视觉基础模型有效学习高光谱数据，在自动驾驶数据集上实现最先进的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 当前高光谱语义分割方法性能不佳，因为它们依赖针对RGB输入优化的架构和学习框架。高光谱数据在复杂材料组成、变化光照等挑战性环境中具有鲁棒感知潜力。

Method: 设计包含光谱变换器和频谱感知空间先验模块的架构，提取丰富的空间-光谱特征。引入模态感知交互块，通过专用提取和注入机制有效整合高光谱表示和冻结的视觉Transformer特征。

Result: 在三个基准自动驾驶数据集上的广泛评估表明，该架构在使用高光谱输入时实现了最先进的语义分割性能，优于基于视觉和高光谱的分割方法。

Conclusion: 提出的高光谱适配器能够有效利用预训练视觉基础模型处理高光谱数据，为复杂环境下的机器人感知提供了强大解决方案。

Abstract: Hyperspectral imaging (HSI) captures spatial information along with dense
spectral measurements across numerous narrow wavelength bands. This rich
spectral content has the potential to facilitate robust robotic perception,
particularly in environments with complex material compositions, varying
illumination, or other visually challenging conditions. However, current HSI
semantic segmentation methods underperform due to their reliance on
architectures and learning frameworks optimized for RGB inputs. In this work,
we propose a novel hyperspectral adapter that leverages pretrained vision
foundation models to effectively learn from hyperspectral data. Our
architecture incorporates a spectral transformer and a spectrum-aware spatial
prior module to extract rich spatial-spectral features. Additionally, we
introduce a modality-aware interaction block that facilitates effective
integration of hyperspectral representations and frozen vision Transformer
features through dedicated extraction and injection mechanisms. Extensive
evaluations on three benchmark autonomous driving datasets demonstrate that our
architecture achieves state-of-the-art semantic segmentation performance while
directly using HSI inputs, outperforming both vision-based and hyperspectral
segmentation methods. We make the code available at
https://hyperspectraladapter.cs.uni-freiburg.de.

</details>


### [57] [A Simple Data Augmentation Strategy for Text-in-Image Scientific VQA](https://arxiv.org/abs/2509.20119)
*Belal Shoer,Yova Kementchedjhieva*

Main category: cs.CV

TL;DR: 论文提出了一种将科学图表和文本内容整合到单一图像中的新方法，通过合成数据集解决训练数据稀缺问题，并在多语言科学视觉问答任务上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 科学视觉问答对视觉语言模型具有挑战性，传统方法将图表和文本作为独立输入处理效果不佳。EXAMS-V虽然引入了将视觉和文本内容嵌入单一图像的新范式，但即使在零样本设置下，最先进的专有模型表现仍然较差，需要任务特定的微调。

Method: 为了解决"文本在图像中"格式训练数据稀缺的问题，作者通过将现有的独立图像-文本对转换为统一图像来合成新数据集。然后在一个小型多语言多模态模型上，使用合成数据和EXAMS-V的混合数据进行微调。

Result: 在13种语言上的实验表明，该方法带来了显著的性能提升，展示了强大的平均改进和跨语言迁移能力。

Conclusion: 通过合成数据集和微调策略，论文成功解决了科学视觉问答中多模态上下文整合的挑战，证明了该方法在多语言环境下的有效性。

Abstract: Scientific visual question answering poses significant challenges for
vision-language models due to the complexity of scientific figures and their
multimodal context. Traditional approaches treat the figure and accompanying
text (e.g., questions and answer options) as separate inputs. EXAMS-V
introduced a new paradigm by embedding both visual and textual content into a
single image. However, even state-of-the-art proprietary models perform poorly
on this setup in zero-shot settings, underscoring the need for task-specific
fine-tuning. To address the scarcity of training data in this "text-in-image"
format, we synthesize a new dataset by converting existing separate image-text
pairs into unified images. Fine-tuning a small multilingual multimodal model on
a mix of our synthetic data and EXAMS-V yields notable gains across 13
languages, demonstrating strong average improvements and cross-lingual
transfer.

</details>


### [58] [EchoBench: Benchmarking Sycophancy in Medical Large Vision-Language Models](https://arxiv.org/abs/2509.20146)
*Botai Yuan,Yutian Zhou,Yingjie Wang,Fushuo Huo,Yongcheng Jing,Li Shen,Ying Wei,Zhiqi Shen,Ziwei Liu,Tianwei Zhang,Jie Yang,Dacheng Tao*

Main category: cs.CV

TL;DR: EchoBench是一个用于评估医学大视觉语言模型中奉承行为的基准测试，发现现有模型普遍存在严重的奉承问题，即使表现最好的模型也存在45.98%的奉承率。


<details>
  <summary>Details</summary>
Motivation: 当前医学LVLM基准测试过度关注准确率，而忽视了模型的可靠性和安全性。奉承行为（模型不加批判地回应用户提供的信息）在临床高风险环境中可能带来严重后果。

Method: 开发了EchoBench基准测试，包含2,122张图像、18个科室和20种模态的90个提示，模拟患者、医学生和医生的偏见输入。评估了医疗专用、开源和专有LVLM模型。

Result: 所有模型都表现出显著的奉承行为：最佳专有模型Claude 3.7 Sonnet奉承率为45.98%，GPT-4.1达到59.15%，许多医疗专用模型超过95%奉承率。通过偏见类型、科室、感知粒度和模态的细粒度分析识别了影响因素。

Conclusion: 研究发现提高数据质量/多样性和加强领域知识可以减少奉承行为而不损害无偏见准确率。简单的提示级干预措施（负向提示、单样本、少样本）能有效减少奉承行为，为开发更安全可靠的医学LVLM提供了可行指导。

Abstract: Recent benchmarks for medical Large Vision-Language Models (LVLMs) emphasize
leaderboard accuracy, overlooking reliability and safety. We study sycophancy
-- models' tendency to uncritically echo user-provided information -- in
high-stakes clinical settings. We introduce EchoBench, a benchmark to
systematically evaluate sycophancy in medical LVLMs. It contains 2,122 images
across 18 departments and 20 modalities with 90 prompts that simulate biased
inputs from patients, medical students, and physicians. We evaluate
medical-specific, open-source, and proprietary LVLMs. All exhibit substantial
sycophancy; the best proprietary model (Claude 3.7 Sonnet) still shows 45.98%
sycophancy, and GPT-4.1 reaches 59.15%. Many medical-specific models exceed 95%
sycophancy despite only moderate accuracy. Fine-grained analyses by bias type,
department, perceptual granularity, and modality identify factors that increase
susceptibility. We further show that higher data quality/diversity and stronger
domain knowledge reduce sycophancy without harming unbiased accuracy. EchoBench
also serves as a testbed for mitigation: simple prompt-level interventions
(negative prompting, one-shot, few-shot) produce consistent reductions and
motivate training- and decoding-time strategies. Our findings highlight the
need for robust evaluation beyond accuracy and provide actionable guidance
toward safer, more trustworthy medical LVLMs.

</details>


### [59] [Smaller is Better: Enhancing Transparency in Vehicle AI Systems via Pruning](https://arxiv.org/abs/2509.20148)
*Sanish Suwal,Shaurya Garg,Dipkamal Bhusal,Michael Clifford,Nidhi Rastogi*

Main category: cs.CV

TL;DR: 本文系统研究了三种训练方法（自然训练、对抗训练和剪枝）对交通标志分类器事后解释质量的影响，发现剪枝能显著提高解释的可理解性和忠实性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖AI系统，但黑盒模型缺乏透明度，事后解释的质量和可靠性常受质疑，需要研究不同训练方法如何影响解释质量。

Method: 通过大量实证评估，比较自然训练、对抗训练和剪枝三种训练方法对交通标志分类器事后解释（使用显著图）的影响。

Result: 剪枝显著增强了解释的可理解性和忠实性，不仅提高了模型效率，还强制学习表示的稀疏性，导致更可解释和可靠的决策。

Conclusion: 剪枝是开发透明深度学习模型的有前景策略，特别适用于资源受限的车载AI系统。

Abstract: Connected and autonomous vehicles continue to heavily rely on AI systems,
where transparency and security are critical for trust and operational safety.
Post-hoc explanations provide transparency to these black-box like AI models
but the quality and reliability of these explanations is often questioned due
to inconsistencies and lack of faithfulness in representing model decisions.
This paper systematically examines the impact of three widely used training
approaches, namely natural training, adversarial training, and pruning, affect
the quality of post-hoc explanations for traffic sign classifiers. Through
extensive empirical evaluation, we demonstrate that pruning significantly
enhances the comprehensibility and faithfulness of explanations (using saliency
maps). Our findings reveal that pruning not only improves model efficiency but
also enforces sparsity in learned representation, leading to more interpretable
and reliable decisions. Additionally, these insights suggest that pruning is a
promising strategy for developing transparent deep learning models, especially
in resource-constrained vehicular AI systems.

</details>


### [60] [C$^2$MIL: Synchronizing Semantic and Topological Causalities in Multiple Instance Learning for Robust and Interpretable Survival Analysis](https://arxiv.org/abs/2509.20152)
*Min Cen,Zhenfeng Zhuang,Yuzhe Zhang,Min Zeng,Baptiste Magnier,Lequan Yu,Hong Zhang,Liansheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的双因果图基MIL模型C²MIL，通过语义因果干预和拓扑因果发现来解决H&E染色全玻片图像中语义偏差和拓扑噪声问题，提升生存分析的泛化性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图的MIL方法在H&E染色WSI生存分析中，由于染色和扫描变化导致的语义偏差以及不相关拓扑子图引入的噪声，会导致偏倚的幻灯片级表示，影响模型的可解释性和泛化能力。

Method: 提出了C²MIL模型，包含跨尺度自适应特征解缠模块进行语义因果干预，以及伯努利可微分因果子图采样方法进行拓扑因果发现，采用解缠监督和对比学习的联合优化策略。

Result: 实验证明C²MIL相比现有方法能持续提升泛化性和可解释性，可作为多种MIL基线的因果增强方法。

Conclusion: C²MIL通过双因果建模有效解决了WSI生存分析中的语义和拓扑偏差问题，为MIL方法提供了理论基础和实用增强方案。

Abstract: Graph-based Multiple Instance Learning (MIL) is widely used in survival
analysis with Hematoxylin and Eosin (H\&E)-stained whole slide images (WSIs)
due to its ability to capture topological information. However, variations in
staining and scanning can introduce semantic bias, while topological subgraphs
that are not relevant to the causal relationships can create noise, resulting
in biased slide-level representations. These issues can hinder both the
interpretability and generalization of the analysis. To tackle this, we
introduce a dual structural causal model as the theoretical foundation and
propose a novel and interpretable dual causal graph-based MIL model, C$^2$MIL.
C$^2$MIL incorporates a novel cross-scale adaptive feature disentangling module
for semantic causal intervention and a new Bernoulli differentiable causal
subgraph sampling method for topological causal discovery. A joint optimization
strategy combining disentangling supervision and contrastive learning enables
simultaneous refinement of both semantic and topological causalities.
Experiments demonstrate that C$^2$MIL consistently improves generalization and
interpretability over existing methods and can serve as a causal enhancement
for diverse MIL baselines. The code is available at
https://github.com/mimic0127/C2MIL.

</details>


### [61] [U-Mamba2-SSL for Semi-Supervised Tooth and Pulp Segmentation in CBCT](https://arxiv.org/abs/2509.20154)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: 提出U-Mamba2-SSL半监督学习框架，用于CBCT图像中牙齿和牙髓的自动分割，通过多阶段训练策略显著提升分割精度


<details>
  <summary>Details</summary>
Motivation: CBCT图像中牙齿和牙髓的手动分割需要专业知识和大量时间，亟需能够有效利用未标记数据的自动化算法

Method: 基于U-Mamba2模型构建半监督学习框架，采用多阶段训练：自监督预训练、一致性正则化利用未标记数据、带减权损失的伪标签策略

Result: 在验证集上获得平均得分0.872和DSC 0.969，表现出优越性能

Conclusion: U-Mamba2-SSL框架有效解决了CBCT图像分割问题，为临床治疗规划和诊断提供了可靠工具

Abstract: Accurate segmentation of teeth and pulp in Cone-Beam Computed Tomography
(CBCT) is vital for clinical applications like treatment planning and
diagnosis. However, this process requires extensive expertise and is
exceptionally time-consuming, highlighting the critical need for automated
algorithms that can effectively utilize unlabeled data. In this paper, we
propose U-Mamba2-SSL, a novel semi-supervised learning framework that builds on
the U-Mamba2 model and employs a multi-stage training strategy. The framework
first pre-trains U-Mamba2 in a self-supervised manner using a disruptive
autoencoder. It then leverages unlabeled data through consistency
regularization, where we introduce input and feature perturbations to ensure
stable model outputs. Finally, a pseudo-labeling strategy is implemented with a
reduced loss weighting to minimize the impact of potential errors. U-Mamba2-SSL
achieved an average score of 0.872 and a DSC of 0.969 on the validation
dataset, demonstrating the superior performance of our approach. The code is
available at https://github.com/zhiqin1998/UMamba2.

</details>


### [62] [Optical Ocean Recipes: Creating Realistic Datasets to Facilitate Underwater Vision Research](https://arxiv.org/abs/2509.20171)
*Patricia Schöntag,David Nakath,Judith Fischer,Rüdiger Röttgers,Kevin Köser*

Main category: cs.CV

TL;DR: 提出了一个名为“光学海洋配方”的框架，用于在受控水下条件下创建逼真的数据集，解决水下机器视觉评估缺乏可重复性和泛化性的问题。


<details>
  <summary>Details</summary>
Motivation: 水下机器视觉评估面临挑战，主要原因是缺乏能够考虑光学挑战（如颜色失真、散射和动态光照）的受控测试环境，且现有评估通常在特定光学水类型下进行，缺乏泛化性。

Method: 开发了光学海洋配方框架，使用校准的颜色和散射添加剂，在受控环境中创建逼真的水下数据集，支持可重复测试水成分对图像外观的影响。

Result: 提供了一个演示数据集，展示了该框架在水参数估计、图像恢复、分割、视觉SLAM和图像合成等多种视觉任务中的应用。

Conclusion: 光学海洋配方为水下机器视觉提供了一个独特且受控的测试框架，能够生成地面真实数据，促进算法评估和比较，数据集和评估代码将公开。

Abstract: The development and evaluation of machine vision in underwater environments
remains challenging, often relying on trial-and-error-based testing tailored to
specific applications. This is partly due to the lack of controlled,
ground-truthed testing environments that account for the optical challenges,
such as color distortion from spectrally variant light attenuation, reduced
contrast and blur from backscatter and volume scattering, and dynamic light
patterns from natural or artificial illumination. Additionally, the appearance
of ocean water in images varies significantly across regions, depths, and
seasons. However, most machine vision evaluations are conducted under specific
optical water types and imaging conditions, therefore often lack
generalizability. Exhaustive testing across diverse open-water scenarios is
technically impractical. To address this, we introduce the \textit{Optical
Ocean Recipes}, a framework for creating realistic datasets under controlled
underwater conditions. Unlike synthetic or open-water data, these recipes,
using calibrated color and scattering additives, enable repeatable and
controlled testing of the impact of water composition on image appearance.
Hence, this provides a unique framework for analyzing machine vision in
realistic, yet controlled underwater scenarios. The controlled environment
enables the creation of ground-truth data for a range of vision tasks,
including water parameter estimation, image restoration, segmentation, visual
SLAM, and underwater image synthesis. We provide a demonstration dataset
generated using the Optical Ocean Recipes and briefly demonstrate the use of
our system for two underwater vision tasks. The dataset and evaluation code
will be made available.

</details>


### [63] [Universal Camouflage Attack on Vision-Language Models for Autonomous Driving](https://arxiv.org/abs/2509.20196)
*Dehong Kong,Sifan Yu,Siyuan Liang,Jiawei Liang,Jianhou Gan,Aishan Liu,Wenqi Ren*

Main category: cs.CV

TL;DR: 提出了首个针对视觉语言建模自动驾驶系统的通用伪装攻击框架，通过特征空间优化生成物理可实现的伪装纹理，显著超越现有攻击方法。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法存在明显挑战：物理对抗攻击主要针对视觉模块，难以直接迁移到VLM-AD系统；针对VLM-AD的攻击大多集中在数字层面。需要解决这些挑战。

Method: 提出UCA框架，在特征空间操作生成物理可实现的伪装纹理。引入特征差异损失最大化干净和对抗图像的表征差异，采用多尺度学习策略和调整采样比例增强适应性。

Result: 广泛实验表明UCA能在各种VLM-AD模型和驾驶场景中诱导错误驾驶命令，显著超越现有最先进攻击方法（在3-P指标上提升30%）。在不同视角和动态条件下表现出强攻击鲁棒性。

Conclusion: UCA具有实际部署的高潜力，为VLM-AD系统的安全性研究提供了重要贡献。

Abstract: Visual language modeling for automated driving is emerging as a promising
research direction with substantial improvements in multimodal reasoning
capabilities. Despite its advanced reasoning abilities, VLM-AD remains
vulnerable to serious security threats from adversarial attacks, which involve
misleading model decisions through carefully crafted perturbations. Existing
attacks have obvious challenges: 1) Physical adversarial attacks primarily
target vision modules. They are difficult to directly transfer to VLM-AD
systems because they typically attack low-level perceptual components. 2)
Adversarial attacks against VLM-AD have largely concentrated on the digital
level. To address these challenges, we propose the first Universal Camouflage
Attack (UCA) framework for VLM-AD. Unlike previous methods that focus on
optimizing the logit layer, UCA operates in the feature space to generate
physically realizable camouflage textures that exhibit strong generalization
across different user commands and model architectures. Motivated by the
observed vulnerability of encoder and projection layers in VLM-AD, UCA
introduces a feature divergence loss (FDL) that maximizes the representational
discrepancy between clean and adversarial images. In addition, UCA incorporates
a multi-scale learning strategy and adjusts the sampling ratio to enhance its
adaptability to changes in scale and viewpoint diversity in real-world
scenarios, thereby improving training stability. Extensive experiments
demonstrate that UCA can induce incorrect driving commands across various
VLM-AD models and driving scenarios, significantly surpassing existing
state-of-the-art attack methods (improving 30\% in 3-P metrics). Furthermore,
UCA exhibits strong attack robustness under diverse viewpoints and dynamic
conditions, indicating high potential for practical deployment.

</details>


### [64] [PU-Gaussian: Point Cloud Upsampling using 3D Gaussian Representation](https://arxiv.org/abs/2509.20207)
*Mahmoud Khater,Mona Strauss,Philipp von Olshausen,Alexander Reiterer*

Main category: cs.CV

TL;DR: PU-Gaussian是一个基于各向异性3D高斯分布的点云上采样网络，通过局部几何建模和显式采样实现高质量稠密点云生成


<details>
  <summary>Details</summary>
Motivation: 现有方法在点云上采样时往往牺牲几何可解释性或对输入稀疏性的鲁棒性，需要一种既能保持几何结构又能处理稀疏噪声点云的方法

Method: 使用各向异性3D高斯分布建模每个点的局部邻域，在局部几何域中显式采样生成稠密点云，再通过细化网络调整分布均匀性和边缘清晰度

Result: 在PU1K和PUGAN数据集上达到最先进性能，代码和模型权重已公开

Conclusion: PU-Gaussian通过局部几何建模和显式采样策略，有效解决了稀疏噪声点云的上采样问题，在保持几何可解释性的同时提升了鲁棒性

Abstract: Point clouds produced by 3D sensors are often sparse and noisy, posing
challenges for tasks requiring dense and high-fidelity 3D representations.
Prior work has explored both implicit feature-based upsampling and
distance-function learning to address this, but often at the expense of
geometric interpretability or robustness to input sparsity. To overcome these
limitations, we propose PU-Gaussian, a novel upsampling network that models the
local neighborhood around each point using anisotropic 3D Gaussian
distributions. These Gaussians capture the underlying geometric structure,
allowing us to perform upsampling explicitly in the local geometric domain by
direct point sampling. The sampling process generates a dense, but coarse,
point cloud. A subsequent refinement network adjusts the coarse output to
produce a more uniform distribution and sharper edges. We perform extensive
testing on the PU1K and PUGAN datasets, demonstrating that PU-Gaussian achieves
state-of-the-art performance. We make code and model weights publicly available
at https://github.com/mvg-inatech/PU-Gaussian.git.

</details>


### [65] [ImageNet-trained CNNs are not biased towards texture: Revisiting feature reliance through controlled suppression](https://arxiv.org/abs/2509.20234)
*Tom Burgert,Oliver Stoll,Paolo Rota,Begüm Demir*

Main category: cs.CV

TL;DR: 本文挑战了CNN本质上是纹理偏见的假设，提出了一个领域无关的框架来量化特征依赖，发现CNN主要依赖局部形状特征而非纹理，且不同领域模型的特征依赖模式存在系统性差异。


<details>
  <summary>Details</summary>
Motivation: 重新审视Geirhos等人的冲突实验的局限性，挑战CNN天生纹理偏见的假设，探索更准确的特征依赖量化方法。

Method: 提出领域无关框架，通过系统性地抑制形状、纹理和颜色线索来量化特征依赖，避免强制选择冲突的混淆因素，在受控抑制条件下评估人类和神经网络。

Result: 发现CNN并非天生纹理偏见，而是主要依赖局部形状特征；现代训练策略或架构（ConvNeXt、ViTs）可显著缓解这种依赖；不同领域模型特征依赖模式不同：计算机视觉模型优先形状，医学影像模型强调颜色，遥感模型更强依赖纹理。

Conclusion: CNN的特征依赖模式比传统认知更复杂，且具有领域特异性，为理解深度学习的特征使用提供了新视角。

Abstract: The hypothesis that Convolutional Neural Networks (CNNs) are inherently
texture-biased has shaped much of the discourse on feature use in deep
learning. We revisit this hypothesis by examining limitations in the
cue-conflict experiment by Geirhos et al. To address these limitations, we
propose a domain-agnostic framework that quantifies feature reliance through
systematic suppression of shape, texture, and color cues, avoiding the
confounds of forced-choice conflicts. By evaluating humans and neural networks
under controlled suppression conditions, we find that CNNs are not inherently
texture-biased but predominantly rely on local shape features. Nonetheless,
this reliance can be substantially mitigated through modern training strategies
or architectures (ConvNeXt, ViTs). We further extend the analysis across
computer vision, medical imaging, and remote sensing, revealing that reliance
patterns differ systematically: computer vision models prioritize shape,
medical imaging models emphasize color, and remote sensing models exhibit a
stronger reliance towards texture. Code is available at
https://github.com/tomburgert/feature-reliance.

</details>


### [66] [An Anisotropic Cross-View Texture Transfer with Multi-Reference Non-Local Attention for CT Slice Interpolation](https://arxiv.org/abs/2509.20242)
*Kwang-Hyun Uhm,Hyunjun Cho,Sung-Hoo Hong,Seung-Won Jung*

Main category: cs.CV

TL;DR: 提出一种基于交叉视图纹理传递的CT切片插值方法，通过利用3D CT体积的各向异性特性，将高分辨率平面内纹理细节转移到低分辨率平面间图像中。


<details>
  <summary>Details</summary>
Motivation: 临床实践中CT图像通常具有较大的切片厚度，导致各向异性的CT体积，其中平面间分辨率远低于平面内分辨率。这种不一致的分辨率可能导致疾病诊断困难，现有方法未能充分利用3D CT体积的各向异性特性。

Method: 设计了一个独特框架，采用多参考非局部注意力模块，从多个平面内图像中提取有意义的特征来重建平面间高频细节。该方法将高分辨率平面内纹理细节作为参考，并将其转移到低分辨率平面间图像。

Result: 在公共CT数据集（包括真实配对基准）上的广泛实验表明，该方法在CT切片插值方面的性能显著优于现有竞争方法。

Conclusion: 验证了所提出框架的有效性，该方法能够充分利用3D CT体积的各向异性特性，显著提升CT切片插值质量。

Abstract: Computed tomography (CT) is one of the most widely used non-invasive imaging
modalities for medical diagnosis. In clinical practice, CT images are usually
acquired with large slice thicknesses due to the high cost of memory storage
and operation time, resulting in an anisotropic CT volume with much lower
inter-slice resolution than in-plane resolution. Since such inconsistent
resolution may lead to difficulties in disease diagnosis, deep learning-based
volumetric super-resolution methods have been developed to improve inter-slice
resolution. Most existing methods conduct single-image super-resolution on the
through-plane or synthesize intermediate slices from adjacent slices; however,
the anisotropic characteristic of 3D CT volume has not been well explored. In
this paper, we propose a novel cross-view texture transfer approach for CT
slice interpolation by fully utilizing the anisotropic nature of 3D CT volume.
Specifically, we design a unique framework that takes high-resolution in-plane
texture details as a reference and transfers them to low-resolution
through-plane images. To this end, we introduce a multi-reference non-local
attention module that extracts meaningful features for reconstructing
through-plane high-frequency details from multiple in-plane images. Through
extensive experiments, we demonstrate that our method performs significantly
better in CT slice interpolation than existing competing methods on public CT
datasets including a real-paired benchmark, verifying the effectiveness of the
proposed framework. The source code of this work is available at
https://github.com/khuhm/ACVTT.

</details>


### [67] [4D Driving Scene Generation With Stereo Forcing](https://arxiv.org/abs/2509.20251)
*Hao Lu,Zhuang Ma,Guangfeng Jiang,Wenhang Ge,Bohan Li,Yuzhan Cai,Wenzhao Zheng,Yunpeng Zhang,Yingcong Chen*

Main category: cs.CV

TL;DR: PhiGenesis是一个统一的4D场景生成框架，通过结合几何和时间一致性来生成动态驾驶场景，支持时间外推和空间新视角合成。


<details>
  <summary>Details</summary>
Motivation: 解决当前生成模型难以在没有逐场景优化的情况下合成支持时间外推和空间新视角合成的动态4D驾驶场景的问题。

Method: 采用两阶段方法：第一阶段使用预训练视频VAE和新型范围视图适配器进行前馈4D重建；第二阶段引入几何引导的视频扩散模型，使用渲染的历史4D场景作为先验来生成未来视图。

Result: 在外观和几何重建、时间生成和新视角合成任务中达到最先进性能，并在下游评估中表现优异。

Conclusion: PhiGenesis通过统一的框架成功解决了4D场景生成的挑战，实现了高质量的时间连贯性和空间一致性。

Abstract: Current generative models struggle to synthesize dynamic 4D driving scenes
that simultaneously support temporal extrapolation and spatial novel view
synthesis (NVS) without per-scene optimization. Bridging generation and novel
view synthesis remains a major challenge. We present PhiGenesis, a unified
framework for 4D scene generation that extends video generation techniques with
geometric and temporal consistency. Given multi-view image sequences and camera
parameters, PhiGenesis produces temporally continuous 4D Gaussian splatting
representations along target 3D trajectories. In its first stage, PhiGenesis
leverages a pre-trained video VAE with a novel range-view adapter to enable
feed-forward 4D reconstruction from multi-view images. This architecture
supports single-frame or video inputs and outputs complete 4D scenes including
geometry, semantics, and motion. In the second stage, PhiGenesis introduces a
geometric-guided video diffusion model, using rendered historical 4D scenes as
priors to generate future views conditioned on trajectories. To address
geometric exposure bias in novel views, we propose Stereo Forcing, a novel
conditioning strategy that integrates geometric uncertainty during denoising.
This method enhances temporal coherence by dynamically adjusting generative
influence based on uncertainty-aware perturbations. Our experimental results
demonstrate that our method achieves state-of-the-art performance in both
appearance and geometric reconstruction, temporal generation and novel view
synthesis (NVS) tasks, while simultaneously delivering competitive performance
in downstream evaluations. Homepage is at
\href{https://jiangxb98.github.io/PhiGensis}{PhiGensis}.

</details>


### [68] [A Versatile Foundation Model for AI-enabled Mammogram Interpretation](https://arxiv.org/abs/2509.20271)
*Fuxiang Huang,Jiayi Zhu,Yunfang Yu,Yu Xie,Yuan Guo,Qingcong Kong,Mingxiang Wu,Xinrui Jiang,Shu Yang,Jiabo Ma,Ziyi Liu,Zhe Xu,Zhixuan Chen,Yujie Tan,Zifan He,Luhui Mao,Xi Wang,Junlin Hou,Lei Zhang,Qiong Luo,Zhenhui Li,Herui Yao,Hao Chen*

Main category: cs.CV

TL;DR: VersaMammo是一个用于乳腺X线摄影的通用基础模型，通过两阶段预训练策略，在最大多机构乳腺X线摄影数据集上训练，在92个临床任务中取得最先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前乳腺X线摄影基础模型存在训练数据多样性不足、模型泛化能力有限以及缺乏全面临床评估等问题，限制了其临床转化应用。

Method: 采用两阶段预训练策略：首先通过自监督学习训练教师模型提取可迁移特征，然后结合监督学习和知识蒸馏将特征和临床知识转移到VersaMammo模型中。使用包含706,239张图像的21个来源的多机构数据集。

Result: 在包含92个具体任务的基准测试中，VersaMammo在68个内部任务中50个排名第一，24个外部验证任务中20个排名第一，平均排名分别为1.5和1.2。

Conclusion: VersaMammo展现出卓越的泛化能力和临床实用性，为实现可靠和可扩展的乳腺癌筛查和诊断提供了重要进展。

Abstract: Breast cancer is the most commonly diagnosed cancer and the leading cause of
cancer-related mortality in women globally. Mammography is essential for the
early detection and diagnosis of breast lesions. Despite recent progress in
foundation models (FMs) for mammogram analysis, their clinical translation
remains constrained by several fundamental limitations, including insufficient
diversity in training data, limited model generalizability, and a lack of
comprehensive evaluation across clinically relevant tasks. Here, we introduce
VersaMammo, a versatile foundation model for mammograms, designed to overcome
these limitations. We curated the largest multi-institutional mammogram dataset
to date, comprising 706,239 images from 21 sources. To improve generalization,
we propose a two-stage pre-training strategy to develop VersaMammo, a mammogram
foundation model. First, a teacher model is trained via self-supervised
learning to extract transferable features from unlabeled mammograms. Then,
supervised learning combined with knowledge distillation transfers both
features and clinical knowledge into VersaMammo. To ensure a comprehensive
evaluation, we established a benchmark comprising 92 specific tasks, including
68 internal tasks and 24 external validation tasks, spanning 5 major clinical
task categories: lesion detection, segmentation, classification, image
retrieval, and visual question answering. VersaMammo achieves state-of-the-art
performance, ranking first in 50 out of 68 specific internal tasks and 20 out
of 24 external validation tasks, with average ranks of 1.5 and 1.2,
respectively. These results demonstrate its superior generalization and
clinical utility, offering a substantial advancement toward reliable and
scalable breast cancer screening and diagnosis.

</details>


### [69] [A co-evolving agentic AI system for medical imaging analysis](https://arxiv.org/abs/2509.20279)
*Songhao Li,Jonathan Xu,Tiancheng Bao,Yuxuan Liu,Yuchen Liu,Yihang Liu,Lilin Wang,Wenhui Lei,Sheng Wang,Yinuo Xu,Yan Cui,Jialu Yao,Shunsuke Koga,Zhi Huang*

Main category: cs.CV

TL;DR: TissueLab是一个协同进化的智能AI系统，用于医学图像分析，通过标准化工具、实时专家反馈和主动学习，在病理学、放射学和空间组学领域实现最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗图像分析中智能AI的性能和应用受限，主要原因是缺乏强大的生态系统、工具集不足以及缺少实时交互式专家反馈。

Method: TissueLab整合了病理学、放射学和空间组学领域的工具工厂，通过标准化输入、输出和能力，让系统能够确定何时以及如何调用这些工具来解决研究和临床问题。系统支持实时分析，专家可以可视化中间结果并进行优化。

Result: 在涉及临床有意义量化（如分期、预后和治疗规划）的多样化任务中，TissueLab相比端到端视觉语言模型和其他智能AI系统（如GPT-5）实现了最先进的性能。系统能够通过主动学习在几分钟内对新疾病情境提供准确结果，无需大量数据集或长时间重新训练。

Conclusion: TissueLab作为一个可持续的开源生态系统发布，旨在加速医学图像计算研究和转化应用，同时为下一代医疗AI奠定基础。

Abstract: Agentic AI is rapidly advancing in healthcare and biomedical research.
However, in medical image analysis, their performance and adoption remain
limited due to the lack of a robust ecosystem, insufficient toolsets, and the
absence of real-time interactive expert feedback. Here we present "TissueLab",
a co-evolving agentic AI system that allows researchers to ask direct
questions, automatically plan and generate explainable workflows, and conduct
real-time analyses where experts can visualize intermediate results and refine
them. TissueLab integrates tool factories across pathology, radiology, and
spatial omics domains. By standardizing inputs, outputs, and capabilities of
diverse tools, the system determines when and how to invoke them to address
research and clinical questions. Across diverse tasks with clinically
meaningful quantifications that inform staging, prognosis, and treatment
planning, TissueLab achieves state-of-the-art performance compared with
end-to-end vision-language models (VLMs) and other agentic AI systems such as
GPT-5. Moreover, TissueLab continuously learns from clinicians, evolving toward
improved classifiers and more effective decision strategies. With active
learning, it delivers accurate results in unseen disease contexts within
minutes, without requiring massive datasets or prolonged retraining. Released
as a sustainable open-source ecosystem, TissueLab aims to accelerate
computational research and translational adoption in medical imaging while
establishing a foundation for the next generation of medical AI.

</details>


### [70] [HiPerformer: A High-Performance Global-Local Segmentation Model with Modular Hierarchical Fusion Strategy](https://arxiv.org/abs/2509.20280)
*Dayu Tan,Zhenpeng Xu,Yansen Su,Xin Peng,Chunhou Zheng,Weimin Zhong*

Main category: cs.CV

TL;DR: HiPerformer是一种用于医学图像分割的新型架构，通过动态并行融合多源特征和模块化层次设计，有效解决了传统CNN-Transformer混合架构中的特征不一致和信息丢失问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN-Transformer混合架构的主流方法采用简单的特征融合技术（如串行堆叠、端点连接或点加），难以处理特征间的不一致性，容易导致信息冲突和丢失。

Method: 1. 采用模块化层次架构动态并行融合多源特征；2. 设计局部-全局特征融合模块（LGFF）精确整合局部细节和全局语义信息；3. 提出渐进金字塔聚合模块（PPA）替代传统跳跃连接，增强多尺度特征表示并抑制噪声干扰。

Result: 在11个公共数据集上的实验表明，该方法优于现有分割技术，具有更高的分割精度和鲁棒性。

Conclusion: HiPerformer通过创新的模块化层次设计和特征融合机制，有效解决了医学图像分割中局部细节与全局上下文整合的挑战，实现了更全面的特征表示。

Abstract: Both local details and global context are crucial in medical image
segmentation, and effectively integrating them is essential for achieving high
accuracy. However, existing mainstream methods based on CNN-Transformer hybrid
architectures typically employ simple feature fusion techniques such as serial
stacking, endpoint concatenation, or pointwise addition, which struggle to
address the inconsistencies between features and are prone to information
conflict and loss. To address the aforementioned challenges, we innovatively
propose HiPerformer. The encoder of HiPerformer employs a novel modular
hierarchical architecture that dynamically fuses multi-source features in
parallel, enabling layer-wise deep integration of heterogeneous information.
The modular hierarchical design not only retains the independent modeling
capability of each branch in the encoder, but also ensures sufficient
information transfer between layers, effectively avoiding the degradation of
features and information loss that come with traditional stacking methods.
Furthermore, we design a Local-Global Feature Fusion (LGFF) module to achieve
precise and efficient integration of local details and global semantic
information, effectively alleviating the feature inconsistency problem and
resulting in a more comprehensive feature representation. To further enhance
multi-scale feature representation capabilities and suppress noise
interference, we also propose a Progressive Pyramid Aggregation (PPA) module to
replace traditional skip connections. Experiments on eleven public datasets
demonstrate that the proposed method outperforms existing segmentation
techniques, demonstrating higher segmentation accuracy and robustness. The code
is available at https://github.com/xzphappy/HiPerformer.

</details>


### [71] [PerFace: Metric Learning in Perceptual Facial Similarity for Enhanced Face Anonymization](https://arxiv.org/abs/2509.20281)
*Haruka Kumagai,Leslie Wöhler,Satoshi Ikehata,Kiyoharu Aizawa*

Main category: cs.CV

TL;DR: 本文提出了一种基于人类感知的人脸相似度度量方法，通过创建包含6400个三元组注释的数据集和度量学习来预测相似度，相比现有方法在人脸相似度预测和基于属性的人脸分类任务上都有显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着社会对隐私问题的关注度提高，人脸匿名化技术不断发展，包括人脸替换方法。但现有模型主要关注二元身份分类（是否为同一人），难以衡量"完全不同"与"高度相似但不同"等细微相似度差异。

Method: 提出基于人类感知的人脸相似度度量方法，创建包含6400个三元组注释的数据集，并采用度量学习来预测人脸相似度。

Result: 实验结果表明，该方法在人脸相似度预测和基于属性的人脸分类任务上都显著优于现有方法。

Conclusion: 该研究为平衡人脸替换中的匿名性和自然性提供了有效的相似度度量工具，能够更好地处理细微的人脸相似度差异。

Abstract: In response to rising societal awareness of privacy concerns, face
anonymization techniques have advanced, including the emergence of
face-swapping methods that replace one identity with another. Achieving a
balance between anonymity and naturalness in face swapping requires careful
selection of identities: overly similar faces compromise anonymity, while
dissimilar ones reduce naturalness. Existing models, however, focus on binary
identity classification "the same person or not", making it difficult to
measure nuanced similarities such as "completely different" versus "highly
similar but different." This paper proposes a human-perception-based face
similarity metric, creating a dataset of 6,400 triplet annotations and metric
learning to predict the similarity. Experimental results demonstrate
significant improvements in both face similarity prediction and attribute-based
face classification tasks over existing methods.

</details>


### [72] [FAST: Foreground-aware Diffusion with Accelerated Sampling Trajectory for Segmentation-oriented Anomaly Synthesis](https://arxiv.org/abs/2509.20295)
*Xichen Xu,Yanshu Wang,Jinbao Wang,Xiaoning Lei,Guoyang Xie,Guannan Jiang,Zhichao Lu*

Main category: cs.CV

TL;DR: FAST是一个面向工业异常分割的前景感知扩散框架，通过AIAS和FARM两个模块实现高效、高质量的异常合成，在多个工业基准测试中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 工业异常分割严重依赖像素级标注，但真实世界异常稀缺、多样且标注成本高。现有SIAS方法难以平衡采样效率和生成质量，且忽视异常与背景区域的统计差异。

Method: 提出FAST框架，包含两个核心模块：AIAS（异常感知加速采样）通过粗到细聚合加速反向过程，FARM（前景感知重建模块）在掩码前景区域自适应调整异常感知噪声。

Result: 在多个工业基准测试中，FAST在仅需10步采样的情况下，在后续分割任务中始终优于现有异常合成方法。

Conclusion: FAST通过前景感知的扩散框架成功解决了工业异常合成中的效率和质量平衡问题，为分割任务提供了可控、结构特定的异常合成方案。

Abstract: Industrial anomaly segmentation relies heavily on pixel-level annotations,
yet real-world anomalies are often scarce, diverse, and costly to label.
Segmentation-oriented industrial anomaly synthesis (SIAS) has emerged as a
promising alternative; however, existing methods struggle to balance sampling
efficiency and generation quality. Moreover, most approaches treat all spatial
regions uniformly, overlooking the distinct statistical differences between
anomaly and background areas. This uniform treatment hinders the synthesis of
controllable, structure-specific anomalies tailored for segmentation tasks. In
this paper, we propose FAST, a foreground-aware diffusion framework featuring
two novel modules: the Anomaly-Informed Accelerated Sampling (AIAS) and the
Foreground-Aware Reconstruction Module (FARM). AIAS is a training-free sampling
algorithm specifically designed for segmentation-oriented industrial anomaly
synthesis, which accelerates the reverse process through coarse-to-fine
aggregation and enables the synthesis of state-of-the-art segmentation-oriented
anomalies in as few as 10 steps. Meanwhile, FARM adaptively adjusts the
anomaly-aware noise within the masked foreground regions at each sampling step,
preserving localized anomaly signals throughout the denoising trajectory.
Extensive experiments on multiple industrial benchmarks demonstrate that FAST
consistently outperforms existing anomaly synthesis methods in downstream
segmentation tasks. We release the code at:
https://anonymous.4open.science/r/NeurIPS-938.

</details>


### [73] [A Comprehensive Evaluation of YOLO-based Deer Detection Performance on Edge Devices](https://arxiv.org/abs/2509.20318)
*Bishal Adhikari,Jiajia Li,Eric S. Michel,Jacob Dykes,Te-Ming Paul Tseng,Mary Love Tagert,Dong Chen*

Main category: cs.CV

TL;DR: 本文提出了一个针对农业中鹿类入侵检测的深度学习解决方案，通过创建公开数据集和评估多种YOLO模型在不同硬件平台上的性能，为实际部署提供指导。


<details>
  <summary>Details</summary>
Motivation: 传统鹿类入侵防控方法成本高、效果差，农业经济损失严重。现有研究缺乏专门数据集和实际部署可行性分析，阻碍了智能检测系统的发展。

Method: 创建包含3,095张标注图像的公开数据集；评估12个YOLO模型变体（v8、v9、v10、v11架构）；在高端GPU和边缘计算平台（Raspberry Pi 5、NVIDIA Jetson）上进行性能基准测试。

Result: Raspberry Pi无法实现实时检测，而NVIDIA Jetson在GPU加速下可达30+FPS；YOLOv11n、YOLOv8s、YOLOv9s在准确率（AP@.5>0.85）和计算效率（FPS>30）方面表现最佳。

Conclusion: 小型先进架构模型适合边缘部署，为农业鹿类检测提供了实用解决方案；公开数据集和代码将促进该领域进一步研究。

Abstract: The escalating economic losses in agriculture due to deer intrusion,
estimated to be in the hundreds of millions of dollars annually in the U.S.,
highlight the inadequacy of traditional mitigation strategies since these
methods are often labor-intensive, costly, and ineffective for modern farming
systems. To overcome this, there is a critical need for intelligent, autonomous
solutions which require accurate and efficient deer detection. But the progress
in this field is impeded by a significant gap in the literature, mainly the
lack of a domain-specific, practical dataset and limited study on the on-field
deployability of deer detection systems. Addressing this gap, this study
presents a comprehensive evaluation of state-of-the-art deep learning models
for deer detection in challenging real-world scenarios. The contributions of
this work are threefold. First, we introduce a curated, publicly available
dataset of 3,095 annotated images with bounding-box annotations of deer,
derived from the Idaho Cameratraps project. Second, we provide an extensive
comparative analysis of 12 model variants across four recent YOLO
architectures(v8, v9, v10, and v11). Finally, we benchmarked performance on a
high-end NVIDIA RTX 5090 GPU and evaluated on two representative edge computing
platforms: Raspberry Pi 5 and NVIDIA Jetson AGX Xavier. Results show that the
real-time detection is not feasible in Raspberry Pi without hardware-specific
model optimization, while NVIDIA Jetson provides greater than 30 FPS with
GPU-accelerated inference on 's' and 'n' series models. This study also reveals
that smaller, architecturally advanced models such as YOLOv11n, YOLOv8s, and
YOLOv9s offer the optimal balance of high accuracy (AP@.5 > 0.85) and
computational efficiency (FPS > 30). To support further research, both the
source code and datasets are publicly available at
https://github.com/WinnerBishal/track-the-deer.

</details>


### [74] [Efficient Encoder-Free Pose Conditioning and Pose Control for Virtual Try-On](https://arxiv.org/abs/2509.20343)
*Qi Li,Shuwen Qiu,Julien Han,Xingzi Xu,Mehmet Saygin Seyfioglu,Kee Kiat Koo,Karim Bouyarmane*

Main category: cs.CV

TL;DR: 该论文研究如何在虚拟试穿技术中融入姿态控制，通过空间拼接姿态数据（姿态图和骨架）来增强姿态保持和输出真实感，并提出混合掩码训练策略。


<details>
  <summary>Details</summary>
Motivation: 随着在线购物增长，虚拟试穿技术需求激增。姿态控制是确保产品与用户身体准确对齐的关键，但现有方法面临姿态表示选择、参数集成和姿态保持平衡等挑战。

Method: 基于基线VTON模型，通过空间拼接姿态数据（比较姿态图和骨架），不增加额外参数。引入混合掩码训练策略，结合细粒度掩码和边界框掩码。

Result: 实验表明，使用姿态图进行姿态拼接效果最佳，能提升姿态保持和输出真实感。混合掩码训练支持不同姿态条件下的灵活产品集成。

Conclusion: 通过简单的空间拼接姿态数据方法，可以有效增强虚拟试穿技术的姿态控制能力，混合掩码训练策略进一步提升了模型的灵活性。

Abstract: As online shopping continues to grow, the demand for Virtual Try-On (VTON)
technology has surged, allowing customers to visualize products on themselves
by overlaying product images onto their own photos. An essential yet
challenging condition for effective VTON is pose control, which ensures
accurate alignment of products with the user's body while supporting diverse
orientations for a more immersive experience. However, incorporating pose
conditions into VTON models presents several challenges, including selecting
the optimal pose representation, integrating poses without additional
parameters, and balancing pose preservation with flexible pose control.
  In this work, we build upon a baseline VTON model that concatenates the
reference image condition without external encoder, control network, or complex
attention layers. We investigate methods to incorporate pose control into this
pure concatenation paradigm by spatially concatenating pose data, comparing
performance using pose maps and skeletons, without adding any additional
parameters or module to the baseline model. Our experiments reveal that pose
stitching with pose maps yields the best results, enhancing both pose
preservation and output realism. Additionally, we introduce a mixed-mask
training strategy using fine-grained and bounding box masks, allowing the model
to support flexible product integration across varied poses and conditions.

</details>


### [75] [PhysCtrl: Generative Physics for Controllable and Physics-Grounded Video Generation](https://arxiv.org/abs/2509.20358)
*Chen Wang,Chuhao Chen,Yiming Huang,Zhiyang Dou,Yuan Liu,Jiatao Gu,Lingjie Liu*

Main category: cs.CV

TL;DR: PhysCtrl是一个物理基础的图像到视频生成框架，通过物理参数和力控制实现物理上合理的视频生成。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型虽然能生成逼真视频，但缺乏物理合理性和3D可控性，PhysCtrl旨在解决这些限制。

Method: 使用生成物理网络学习四种材料（弹性、沙子、塑料、刚性）的物理动力学分布，通过扩散模型在物理参数和应用力条件下训练，采用新颖的时空注意力块模拟粒子相互作用，并加入物理约束确保物理合理性。

Result: PhysCtrl生成逼真的物理基础运动轨迹，驱动图像到视频模型时能产生高保真、可控的视频，在视觉质量和物理合理性方面优于现有方法。

Conclusion: PhysCtrl框架成功实现了物理基础的可控视频生成，为视频生成领域提供了物理合理性的新解决方案。

Abstract: Existing video generation models excel at producing photo-realistic videos
from text or images, but often lack physical plausibility and 3D
controllability. To overcome these limitations, we introduce PhysCtrl, a novel
framework for physics-grounded image-to-video generation with physical
parameters and force control. At its core is a generative physics network that
learns the distribution of physical dynamics across four materials (elastic,
sand, plasticine, and rigid) via a diffusion model conditioned on physics
parameters and applied forces. We represent physical dynamics as 3D point
trajectories and train on a large-scale synthetic dataset of 550K animations
generated by physics simulators. We enhance the diffusion model with a novel
spatiotemporal attention block that emulates particle interactions and
incorporates physics-based constraints during training to enforce physical
plausibility. Experiments show that PhysCtrl generates realistic,
physics-grounded motion trajectories which, when used to drive image-to-video
models, yield high-fidelity, controllable videos that outperform existing
methods in both visual quality and physical plausibility. Project Page:
https://cwchenwang.github.io/physctrl

</details>


### [76] [EditVerse: Unifying Image and Video Editing and Generation with In-Context Learning](https://arxiv.org/abs/2509.20360)
*Xuan Ju,Tianyu Wang,Yuqian Zhou,He Zhang,Qing Liu,Nanxuan Zhao,Zhifei Zhang,Yijun Li,Yuanhao Cai,Shaoteng Liu,Daniil Pakhomov,Zhe Lin,Soo Ye Kim,Qiang Xu*

Main category: cs.CV

TL;DR: EditVerse是一个统一的图像和视频生成编辑框架，通过将文本、图像和视频表示为统一标记序列，实现跨模态知识迁移和灵活处理任意分辨率及时长的输入输出。


<details>
  <summary>Details</summary>
Motivation: 当前图像生成编辑已从任务特定转向统一框架，但视频生成编辑仍因架构限制和数据稀缺而碎片化。

Method: 使用统一标记序列表示所有模态，利用自注意力机制实现上下文学习；构建包含23.2万视频编辑样本的数据管道，结合大规模图像视频数据集进行联合训练。

Result: 在广泛实验和用户研究中，EditVerse达到最先进性能，超越现有开源和商业模型，并展现出跨模态的新兴编辑生成能力。

Conclusion: EditVerse证明了统一框架在图像视频生成编辑中的有效性，为解决视频编辑数据稀缺问题提供了可行方案。

Abstract: Recent advances in foundation models highlight a clear trend toward
unification and scaling, showing emergent capabilities across diverse domains.
While image generation and editing have rapidly transitioned from task-specific
to unified frameworks, video generation and editing remain fragmented due to
architectural limitations and data scarcity. In this work, we introduce
EditVerse, a unified framework for image and video generation and editing
within a single model. By representing all modalities, i.e., text, image, and
video, as a unified token sequence, EditVerse leverages self-attention to
achieve robust in-context learning, natural cross-modal knowledge transfer, and
flexible handling of inputs and outputs with arbitrary resolutions and
durations. To address the lack of video editing training data, we design a
scalable data pipeline that curates 232K video editing samples and combines
them with large-scale image and video datasets for joint training. Furthermore,
we present EditVerseBench, the first benchmark for instruction-based video
editing covering diverse tasks and resolutions. Extensive experiments and user
studies demonstrate that EditVerse achieves state-of-the-art performance,
surpassing existing open-source and commercial models, while exhibiting
emergent editing and generation abilities across modalities.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [77] [Frequency-Aware Ensemble Learning for BraTS 2025 Pediatric Brain Tumor Segmentation](https://arxiv.org/abs/2509.19353)
*Yuxiao Yi,Qingyao Zhuang,Zhi-Qin John Xu*

Main category: eess.IV

TL;DR: 提出集成nnU-Net、Swin UNETR和HFF-Net的方法用于儿科脑肿瘤分割，通过可调初始化尺度、迁移学习和频域分解等技术提升性能


<details>
  <summary>Details</summary>
Motivation: 儿科脑肿瘤分割面临罕见性和异质性挑战，但对临床诊断和治疗规划至关重要

Method: 集成三种网络：nnU-Net（可调初始化尺度控制复杂度）、Swin UNETR（从BraTS 2021预训练模型迁移学习）、HFF-Net（频域分解分离低频轮廓和高频纹理）

Result: 最终集成模型在BraTS-PED 2025挑战中取得Dice分数：ET 72.3%、NET 95.6%、CC 68.9%、ED 89.5%、TC 92.3%、WT 92.3%

Conclusion: 该集成方法有效解决了儿科脑肿瘤分割的挑战，为临床诊断提供了可靠的技术支持

Abstract: Pediatric brain tumor segmentation presents unique challenges due to the
rarity and heterogeneity of these malignancies, yet remains critical for
clinical diagnosis and treatment planning. We propose an ensemble approach
integrating nnU-Net, Swin UNETR, and HFF-Net for the BraTS-PED 2025 challenge.
Our method incorporates three key extensions: adjustable initialization scales
for optimal nnU-Net complexity control, transfer learning from BraTS 2021
pre-trained models to enhance Swin UNETR's generalization on pediatric dataset,
and frequency domain decomposition for HFF-Net to separate low-frequency tissue
contours from high-frequency texture details. Our final ensemble combines
nnU-Net ($\gamma=0.7$), fine-tuned Swin UNETR, and HFF-Net, achieving Dice
scores of 72.3% (ET), 95.6% (NET), 68.9% (CC), 89.5% (ED), 92.3% (TC), and
92.3% (WT), respectively.

</details>


### [78] [Ensuring Reliable Participation in Subjective Video Quality Tests Across Platforms](https://arxiv.org/abs/2509.20001)
*Babak Naderi,Ross Cutler*

Main category: eess.IV

TL;DR: 本文提出客观和主观检测器来识别远程桌面用户，并比较两个主流众包平台在现实测试条件和任务设计下的易受攻击性和缓解措施。


<details>
  <summary>Details</summary>
Motivation: 众包视频质量评估虽然比实验室研究更快速、成本更低，但存在工作者忽略指令或利用奖励机制的问题，特别是近期出现了利用视频元数据和远程桌面连接等复杂手段来偏置结果的情况。

Method: 开发客观和主观检测器来识别远程桌面用户，并在两个主流众包平台上进行现实测试条件和任务设计的比较研究。

Result: 研究发现众包平台存在远程桌面连接等漏洞，这些漏洞会偏置视频质量评估结果。

Conclusion: 需要有效的检测机制来确保众包视频质量评估的准确性和可靠性，特别是在面对日益复杂的作弊手段时。

Abstract: Subjective video quality assessment (VQA) is the gold standard for measuring
end-user experience across communication, streaming, and UGC pipelines. Beyond
high-validity lab studies, crowdsourcing offers accurate, reliable, faster, and
cheaper evaluation-but suffers from unreliable submissions by workers who
ignore instructions or game rewards. Recent tests reveal sophisticated exploits
of video metadata and rising use of remote-desktop (RD) connections, both of
which bias results. We propose objective and subjective detectors for RD users
and compare two mainstream crowdsourcing platforms on their susceptibility and
mitigation under realistic test conditions and task designs.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Design Insights and Comparative Evaluation of a Hardware-Based Cooperative Perception Architecture for Lane Change Prediction](https://arxiv.org/abs/2509.20218)
*Mohamed Manzour,Catherine M. Elias,Omar M. Shehata,Rubén Izquierdo,Miguel Ángel Sotelo*

Main category: cs.AI

TL;DR: 本文探讨了在混合交通中通过真实硬件部署实现协同车道变换预测，并分享了实施和测试过程中获得的实践经验。


<details>
  <summary>Details</summary>
Motivation: 现有的车道变换预测研究大多在模拟环境或预录制数据集上进行，依赖简化的假设，而实际部署相对较少且实践经验不足。

Method: 通过在混合交通中进行真实硬件部署，实施协同车道变换预测系统，并记录实施和测试过程中的挑战。

Result: 研究揭示了实际部署中的瓶颈、可靠性问题和操作约束，这些因素影响了系统的行为。

Conclusion: 通过记录这些经验，本研究为从事类似管道工作的研究人员提供了指导，强调了实际部署中的挑战和教训。

Abstract: Research on lane change prediction has gained attention in the last few
years. Most existing works in this area have been conducted in simulation
environments or with pre-recorded datasets, these works often rely on
simplified assumptions about sensing, communication, and traffic behavior that
do not always hold in practice. Real-world deployments of lane-change
prediction systems are relatively rare, and when they are reported, the
practical challenges, limitations, and lessons learned are often
under-documented. This study explores cooperative lane-change prediction
through a real hardware deployment in mixed traffic and shares the insights
that emerged during implementation and testing. We highlight the practical
challenges we faced, including bottlenecks, reliability issues, and operational
constraints that shaped the behavior of the system. By documenting these
experiences, the study provides guidance for others working on similar
pipelines.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [80] [Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models](https://arxiv.org/abs/2509.19595)
*Mohammad Saim,Phan Anh Duong,Cat Luong,Aniket Bhanderi,Tianyu Jiang*

Main category: cs.CL

TL;DR: 提出了ELENA框架，利用大型视觉语言模型生成多层次的体现情感叙事，重点关注情感反应中显著的身体部位，在面部遮挡图像中表现出色。


<details>
  <summary>Details</summary>
Motivation: 身体部位的情感反应包含丰富的情绪体验信息，但现有模型存在对脸部区域的偏见，需要开发能更好识别体现情感的方法。

Method: 使用最先进的大型视觉语言模型生成多层次的体现情感叙事，通过注意力图分析模型偏见，构建关注显著身体部位的情感描述框架。

Result: ELENA框架在面部遮挡图像的情感识别中表现优于基线方法，无需微调即可有效识别体现情感。

Conclusion: ELENA为跨视觉模态的体现情感分析开辟了新路径，丰富了情感感知环境下的建模能力。

Abstract: The embodiment of emotional reactions from body parts contains rich
information about our affective experiences. We propose a framework that
utilizes state-of-the-art large vision-language models (LVLMs) to generate
Embodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered
text outputs, primarily comprising descriptions that focus on the salient body
parts involved in emotional reactions. We also employ attention maps and
observe that contemporary models exhibit a persistent bias towards the facial
region. Despite this limitation, we observe that our employed framework can
effectively recognize embodied emotions in face-masked images, outperforming
baselines without any fine-tuning. ELENA opens a new trajectory for embodied
emotion analysis across the modality of vision and enriches modeling in an
affect-aware setting.

</details>


### [81] [CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition](https://arxiv.org/abs/2509.19768)
*Sina J. Semnani,Han Zhang,Xinyan He,Merve Tekgürler,Monica S. Lam*

Main category: cs.CL

TL;DR: CHURRO是一个专门用于历史文本识别的3B参数开放权重视觉语言模型，在历史文档识别任务上优于现有VLMs和OCR系统，且成本效益更高。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要针对现代标准化文本设计，无法有效处理历史文档中多样的语言脚本、不规则布局和常见退化问题，这限制了文化遗产的研究和保护。

Method: 开发了CHURRO-DS数据集（包含155个历史语料库、99,491页文档，涵盖22个世纪的文本遗产和46种语言集群），并基于此训练了3B参数的CHURRO模型。

Result: CHURRO在测试集上达到82.3%（印刷体）和70.1%（手写体）的标准化Levenshtein相似度，分别比第二好的Gemini 2.5 Pro模型高出1.4%和6.5%，同时成本效益提高15.5倍。

Conclusion: 通过发布模型和数据集，旨在推动社区驱动的研究，提高历史文本的可读性并加速学术研究。

Abstract: Accurate text recognition for historical documents can greatly advance the
study and preservation of cultural heritage. Existing vision-language models
(VLMs), however, are designed for modern, standardized texts and are not
equipped to read the diverse languages and scripts, irregular layouts, and
frequent degradation found in historical materials.
  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for
historical text recognition. The model is trained on CHURRO-DS, the largest
historical text recognition dataset to date. CHURRO-DS unifies 155 historical
corpora comprising 99,491 pages, spanning 22 centuries of textual heritage
across 46 language clusters, including historical variants and dead languages.
  We evaluate several open-weight and closed VLMs and optical character
recognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all
other VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and
70.1% (handwritten) normalized Levenshtein similarity, surpassing the
second-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being
15.5 times more cost-effective.
  By releasing the model and dataset, we aim to enable community-driven
research to improve the readability of historical texts and accelerate
scholarship.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [82] [TIMED: Adversarial and Autoregressive Refinement of Diffusion-Based Time Series Generation](https://arxiv.org/abs/2509.19638)
*MohammadReza EskandariNasab,Shah Muhammad Hamdi,Soukaina Filali Boubrahimi*

Main category: cs.LG

TL;DR: TIMED是一个统一的生成框架，用于合成高质量的时间序列数据，结合了扩散模型、监督网络和对抗训练来建模时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 真实时间序列数据通常稀缺、有噪声或收集成本高，而生成时间序列需要同时建模观测值的边际分布和时序依赖关系。

Method: TIMED框架整合了：1）去噪扩散概率模型（DDPM）捕捉全局结构；2）监督网络通过教师强制学习自回归依赖；3）Wasserstein批评器提供对抗反馈确保时序平滑性；4）最大均值差异（MMD）损失对齐特征空间分布。

Result: 在多个多元时间序列基准测试中，TIMED生成的序列比最先进的生成模型更真实且时序更连贯。

Conclusion: TIMED通过联合训练多个组件，有效捕捉时间序列数据的无条件和条件特征，生成质量优于现有方法。

Abstract: Generating high-quality synthetic time series is a fundamental yet
challenging task across domains such as forecasting and anomaly detection,
where real data can be scarce, noisy, or costly to collect. Unlike static data
generation, synthesizing time series requires modeling both the marginal
distribution of observations and the conditional temporal dependencies that
govern sequential dynamics. We propose TIMED, a unified generative framework
that integrates a denoising diffusion probabilistic model (DDPM) to capture
global structure via a forward-reverse diffusion process, a supervisor network
trained with teacher forcing to learn autoregressive dependencies through
next-step prediction, and a Wasserstein critic that provides adversarial
feedback to ensure temporal smoothness and fidelity. To further align the real
and synthetic distributions in feature space, TIMED incorporates a Maximum Mean
Discrepancy (MMD) loss, promoting both diversity and sample quality. All
components are built using masked attention architectures optimized for
sequence modeling and are trained jointly to effectively capture both
unconditional and conditional aspects of time series data. Experimental results
across diverse multivariate time series benchmarks demonstrate that TIMED
generates more realistic and temporally coherent sequences than
state-of-the-art generative models.

</details>


### [83] [C${}^2$Prompt: Class-aware Client Knowledge Interaction for Federated Continual Learning](https://arxiv.org/abs/2509.19674)
*Kunlun Xu,Yibo Feng,Jiangmeng Li,Yongsheng Qi,Jiahuan Zhou*

Main category: cs.LG

TL;DR: 提出C²Prompt方法解决联邦持续学习中提示方法存在的类间知识冲突问题，通过局部类分布补偿和类感知提示聚合增强类间知识一致性


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的联邦持续学习方法存在类间知识连贯性问题，包括客户端间类内分布差距和类间提示相关性不足，导致知识冲突和遗忘加剧

Method: 提出Class-aware Client Knowledge Interaction (C²Prompt)方法，包含局部类分布补偿机制(LCDC)减少客户端间类内分布差异，以及类感知提示聚合方案(CPA)选择性增强类相关知识聚合

Result: 在多个联邦持续学习基准测试上实现了最先进的性能

Conclusion: C²Prompt通过显式增强提示通信中的类间知识连贯性，有效缓解了联邦持续学习中的时空遗忘问题

Abstract: Federated continual learning (FCL) tackles scenarios of learning from
continuously emerging task data across distributed clients, where the key
challenge lies in addressing both temporal forgetting over time and spatial
forgetting simultaneously. Recently, prompt-based FCL methods have shown
advanced performance through task-wise prompt communication.In this study, we
underscore that the existing prompt-based FCL methods are prone to class-wise
knowledge coherence between prompts across clients. The class-wise knowledge
coherence includes two aspects: (1) intra-class distribution gap across
clients, which degrades the learned semantics across prompts, (2) inter-prompt
class-wise relevance, which highlights cross-class knowledge confusion. During
prompt communication, insufficient class-wise coherence exacerbates knowledge
conflicts among new prompts and induces interference with old prompts,
intensifying both spatial and temporal forgetting. To address these issues, we
propose a novel Class-aware Client Knowledge Interaction (C${}^2$Prompt) method
that explicitly enhances class-wise knowledge coherence during prompt
communication. Specifically, a local class distribution compensation mechanism
(LCDC) is introduced to reduce intra-class distribution disparities across
clients, thereby reinforcing intra-class knowledge consistency. Additionally, a
class-aware prompt aggregation scheme (CPA) is designed to alleviate
inter-class knowledge confusion by selectively strengthening class-relevant
knowledge aggregation. Extensive experiments on multiple FCL benchmarks
demonstrate that C${}^2$Prompt achieves state-of-the-art performance. Our
source code is available at
https://github.com/zhoujiahuan1991/NeurIPS2025-C2Prompt

</details>


### [84] [Predictive Coding-based Deep Neural Network Fine-tuning for Computationally Efficient Domain Adaptation](https://arxiv.org/abs/2509.20269)
*Matteo Cardoni,Sam Leroux*

Main category: cs.LG

TL;DR: 提出了一种结合反向传播和预测编码的混合训练方法，用于在动态环境中实现高效的设备端域适应。


<details>
  <summary>Details</summary>
Motivation: 由于传感器漂移或光照变化导致的输入数据分布变化，单一静态模型往往不足，需要持续模型适应。

Method: 首先使用反向传播离线训练深度神经网络获得高初始性能，然后使用预测编码进行在线适应，使模型能够恢复因输入数据分布变化而损失的准确性。

Result: 在MNIST和CIFAR-10数据集上的实验结果表明，该混合策略能够以较低的计算开销实现有效适应。

Conclusion: 该方法特别适合资源受限的边缘设备或未来的神经形态加速器，为在动态环境中维持模型性能提供了有前景的解决方案。

Abstract: As deep neural networks are increasingly deployed in dynamic, real-world
environments, relying on a single static model is often insufficient. Changes
in input data distributions caused by sensor drift or lighting variations
necessitate continual model adaptation. In this paper, we propose a hybrid
training methodology that enables efficient on-device domain adaptation by
combining the strengths of Backpropagation and Predictive Coding. The method
begins with a deep neural network trained offline using Backpropagation to
achieve high initial performance. Subsequently, Predictive Coding is employed
for online adaptation, allowing the model to recover accuracy lost due to
shifts in the input data distribution. This approach leverages the robustness
of Backpropagation for initial representation learning and the computational
efficiency of Predictive Coding for continual learning, making it particularly
well-suited for resource-constrained edge devices or future neuromorphic
accelerators. Experimental results on the MNIST and CIFAR-10 datasets
demonstrate that this hybrid strategy enables effective adaptation with a
reduced computational overhead, offering a promising solution for maintaining
model performance in dynamic environments.

</details>


### [85] [Video models are zero-shot learners and reasoners](https://arxiv.org/abs/2509.20328)
*Thaddäus Wiedemer,Yuxuan Li,Paul Vicol,Shixiang Shane Gu,Nick Matarese,Kevin Swersky,Been Kim,Priyank Jaini,Robert Geirhos*

Main category: cs.LG

TL;DR: 本文探讨了大型语言模型（LLM）的零样本能力如何启发视频模型的发展，认为视频模型可能像LLM一样走向通用视觉理解。通过Veo 3模型展示了其在未明确训练的任务上的广泛能力，表明视频模型正成为统一的通用视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: 受LLM从任务特定模型向通用基础模型转变的启发，研究视频模型是否也能实现类似的通用视觉理解能力。

Method: 使用Veo 3视频模型，测试其在多种未明确训练任务上的表现，包括物体分割、边缘检测、图像编辑、物理属性理解、物体功能识别、工具使用模拟等。

Result: Veo 3展示了广泛的零样本能力，能够解决视觉感知、建模和操作任务，甚至完成早期形式的视觉推理如迷宫和对称性求解。

Conclusion: 视频模型正朝着成为统一的通用视觉基础模型的方向发展，其涌现的零样本能力表明了这一趋势。

Abstract: The remarkable zero-shot capabilities of Large Language Models (LLMs) have
propelled natural language processing from task-specific models to unified,
generalist foundation models. This transformation emerged from simple
primitives: large, generative models trained on web-scale data. Curiously, the
same primitives apply to today's generative video models. Could video models be
on a trajectory towards general-purpose vision understanding, much like LLMs
developed general-purpose language understanding? We demonstrate that Veo 3 can
solve a broad variety of tasks it wasn't explicitly trained for: segmenting
objects, detecting edges, editing images, understanding physical properties,
recognizing object affordances, simulating tool use, and more. These abilities
to perceive, model, and manipulate the visual world enable early forms of
visual reasoning like maze and symmetry solving. Veo's emergent zero-shot
capabilities indicate that video models are on a path to becoming unified,
generalist vision foundation models.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [86] [AJAHR: Amputated Joint Aware 3D Human Mesh Recovery](https://arxiv.org/abs/2509.19939)
*Hyunjin Cho,Giyun Choi,Jongwon Choi*

Main category: cs.GR

TL;DR: 提出AJAHR框架解决现有3D人体网格重建方法对截肢个体的偏见问题，通过集成截肢分类器和合成数据集A3D来提升截肢个体的网格重建精度


<details>
  <summary>Details</summary>
Motivation: 现有方法假设标准人体结构，忽略了截肢等解剖学差异，导致对截肢个体应用时存在偏见，且缺乏合适的数据集

Method: 提出AJAHR框架，集成身体部位截肢分类器与网格重建网络联合训练，并创建合成数据集A3D提供多样化的截肢姿势数据

Result: 在保持对非截肢个体竞争力的同时，在截肢个体上取得了最先进的结果

Conclusion: 该方法有效解决了现有3D人体网格重建对截肢个体的偏见问题，为处理解剖学多样性提供了可行方案

Abstract: Existing human mesh recovery methods assume a standard human body structure,
overlooking diverse anatomical conditions such as limb loss. This assumption
introduces bias when applied to individuals with amputations - a limitation
further exacerbated by the scarcity of suitable datasets. To address this gap,
we propose Amputated Joint Aware 3D Human Mesh Recovery (AJAHR), which is an
adaptive pose estimation framework that improves mesh reconstruction for
individuals with limb loss. Our model integrates a body-part amputation
classifier, jointly trained with the mesh recovery network, to detect potential
amputations. We also introduce Amputee 3D (A3D), which is a synthetic dataset
offering a wide range of amputee poses for robust training. While maintaining
competitive performance on non-amputees, our approach achieves state-of-the-art
results for amputated individuals. Additional materials can be found at the
project webpage.

</details>


### [87] [MeshMosaic: Scaling Artist Mesh Generation via Local-to-Global Assembly](https://arxiv.org/abs/2509.19995)
*Rui Xu,Tianyang Xue,Qiujie Dong,Le Wan,Zhe Zhu,Peng Li,Zhiyang Dou,Cheng Lin,Shiqing Xin,Yuan Liu,Wenping Wang,Taku Komura*

Main category: cs.GR

TL;DR: MeshMosaic是一个局部到全局的艺术家网格生成框架，能够处理超过10万个三角形，大幅超越现有方法（通常只能处理约8000个面），通过分块生成和边界条件共享实现高分辨率网格的生成。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归生成模型在处理高三角形数量的艺术家设计网格时面临挑战，主要由于长序列瓶颈和有限量化分辨率，导致无法忠实再现精细几何细节和结构化密度模式。

Method: MeshMosaic首先将形状分割成多个面片，然后自回归地生成每个面片，利用共享边界条件来促进相邻区域之间的连贯性、对称性和无缝连接。通过单独量化每个面片，提高了网格密度和结构的对称性和组织性。

Result: 在多个公共数据集上的广泛实验表明，MeshMosaic在几何保真度和用户偏好方面显著优于最先进的方法，支持更好的细节表示和实际应用中的实用网格生成。

Conclusion: MeshMosaic通过局部到全局的分块生成策略，成功解决了高分辨率网格生成中的可扩展性问题，为实际应用提供了高质量的网格生成解决方案。

Abstract: Scaling artist-designed meshes to high triangle numbers remains challenging
for autoregressive generative models. Existing transformer-based methods suffer
from long-sequence bottlenecks and limited quantization resolution, primarily
due to the large number of tokens required and constrained quantization
granularity. These issues prevent faithful reproduction of fine geometric
details and structured density patterns. We introduce MeshMosaic, a novel
local-to-global framework for artist mesh generation that scales to over 100K
triangles--substantially surpassing prior methods, which typically handle only
around 8K faces. MeshMosaic first segments shapes into patches, generating each
patch autoregressively and leveraging shared boundary conditions to promote
coherence, symmetry, and seamless connectivity between neighboring regions.
This strategy enhances scalability to high-resolution meshes by quantizing
patches individually, resulting in more symmetrical and organized mesh density
and structure. Extensive experiments across multiple public datasets
demonstrate that MeshMosaic significantly outperforms state-of-the-art methods
in both geometric fidelity and user preference, supporting superior detail
representation and practical mesh generation for real-world applications.

</details>


### [88] [KSDiff: Keyframe-Augmented Speech-Aware Dual-Path Diffusion for Facial Animation](https://arxiv.org/abs/2509.20128)
*Tianle Lyu,Junchuan Zhao,Ye Wang*

Main category: cs.GR

TL;DR: KSDiff是一个基于扩散模型的音频驱动面部动画框架，通过双路径语音编码器和关键帧增强学习模块，实现了语音特征的细粒度解耦和关键动态帧的建模，在唇部同步和头部姿态自然度方面达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将语音特征视为整体表示，无法捕捉不同面部运动的细粒度驱动作用，同时忽略了具有强烈动态的关键帧建模的重要性。

Method: 提出KSDiff框架：1）双路径语音编码器（DPSE）从原始音频和文本中解耦表情相关和头部姿态相关特征；2）自回归关键帧建立学习（KEL）模块预测最显著的运动帧；3）双路径运动生成器合成连贯真实的面部运动。

Result: 在HDTF和VoxCeleb数据集上的实验表明，KSDiff在唇部同步精度和头部姿态自然度方面均优于现有方法，达到state-of-the-art性能。

Conclusion: 结合语音解耦和关键帧感知扩散的方法在说话头生成中具有显著有效性，能够产生更自然、更同步的面部动画。

Abstract: Audio-driven facial animation has made significant progress in multimedia
applications, with diffusion models showing strong potential for talking-face
synthesis. However, most existing works treat speech features as a monolithic
representation and fail to capture their fine-grained roles in driving
different facial motions, while also overlooking the importance of modeling
keyframes with intense dynamics. To address these limitations, we propose
KSDiff, a Keyframe-Augmented Speech-Aware Dual-Path Diffusion framework.
Specifically, the raw audio and transcript are processed by a Dual-Path Speech
Encoder (DPSE) to disentangle expression-related and head-pose-related
features, while an autoregressive Keyframe Establishment Learning (KEL) module
predicts the most salient motion frames. These components are integrated into a
Dual-path Motion generator to synthesize coherent and realistic facial motions.
Extensive experiments on HDTF and VoxCeleb demonstrate that KSDiff achieves
state-of-the-art performance, with improvements in both lip synchronization
accuracy and head-pose naturalness. Our results highlight the effectiveness of
combining speech disentanglement with keyframe-aware diffusion for talking-head
generation.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [89] [MultiSoundGen: Video-to-Audio Generation for Multi-Event Scenarios via SlowFast Contrastive Audio-Visual Pretraining and Direct Preference Optimization](https://arxiv.org/abs/2509.19999)
*Jianxuan Yang,Xiaoran Yang,Lipan Zhang,Xinyue Guo,Zhao Wang,Gongping Huang*

Main category: cs.MM

TL;DR: 本文提出MultiSoundGen框架，通过引入直接偏好优化和慢快对比视听预训练，解决了视频到音频转换在复杂多事件场景中的语义-时间对齐和音频质量优化问题。


<details>
  <summary>Details</summary>
Motivation: 当前视频到音频方法在复杂多事件场景中面临两大挑战：1）难以精确对齐复杂语义信息和快速动态特征；2）基础训练缺乏对语义-时间对齐和音频质量的定量偏好优化。

Method: 提出MultiSoundGen框架，包含两个创新：1）SF-CAVP（慢快对比视听预训练模型），采用统一双流架构显式对齐语义表示和动态特征；2）AVP-RPO（AVP排序偏好优化），将DPO方法引入V2A任务，使用SF-CAVP作为奖励模型量化关键语义-时间匹配。

Result: 实验表明MultiSoundGen在复杂多事件场景中达到最先进性能，在分布匹配、音频质量、语义对齐和时间同步方面均取得全面提升。

Conclusion: 该研究成功解决了V2A任务在复杂多事件场景中的核心限制，通过创新的SF-CAVP和AVP-RPO方法实现了显著的性能提升，代码和数据集将公开发布。

Abstract: Current video-to-audio (V2A) methods struggle in complex multi-event
scenarios (video scenarios involving multiple sound sources, sound events, or
transitions) due to two critical limitations. First, existing methods face
challenges in precisely aligning intricate semantic information together with
rapid dynamic features. Second, foundational training lacks quantitative
preference optimization for semantic-temporal alignment and audio quality. As a
result, it fails to enhance integrated generation quality in cluttered
multi-event scenes. To address these core limitations, this study proposes a
novel V2A framework: MultiSoundGen. It introduces direct preference
optimization (DPO) into the V2A domain, leveraging audio-visual pretraining
(AVP) to enhance performance in complex multi-event scenarios. Our
contributions include two key innovations: the first is SlowFast Contrastive
AVP (SF-CAVP), a pioneering AVP model with a unified dual-stream architecture.
SF-CAVP explicitly aligns core semantic representations and rapid dynamic
features of audio-visual data to handle multi-event complexity; second, we
integrate the DPO method into V2A task and propose AVP-Ranked Preference
Optimization (AVP-RPO). It uses SF-CAVP as a reward model to quantify and
prioritize critical semantic-temporal matches while enhancing audio quality.
Experiments demonstrate that MultiSoundGen achieves state-of-the-art (SOTA)
performance in multi-event scenarios, delivering comprehensive gains across
distribution matching, audio quality, semantic alignment, and temporal
synchronization. The complete code and dataset will be released soon.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [90] [HUNT: High-Speed UAV Navigation and Tracking in Unstructured Environments via Instantaneous Relative Frames](https://arxiv.org/abs/2509.19452)
*Alessandro Saviolo,Jeffrey Mao,Giuseppe Loianno*

Main category: cs.RO

TL;DR: HUNT是一个实时框架，将无人机在未知环境中的高速穿越、目标捕获和跟踪统一在一个相对导航框架中，解决了在感知退化和无全局定位条件下的搜索救援任务挑战。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在搜索救援任务中需要同时实现高速穿越未知非结构化环境和目标跟踪的难题，特别是在感知退化和缺乏全局定位的情况下。

Method: 提出HUNT框架，通过将导航目标直接定义在机载瞬时观测值（如姿态、高度、速度）上，实现统一的感知-控制流水线，在搜索阶段支持反应式高速飞行，检测到目标后无缝切换到跟踪模式。

Result: 在密集森林、集装箱场地和搜索救援场景中的户外实验表明，HUNT在全局方法失效的情况下仍能保持鲁棒自主性，成功跟踪车辆和人偶目标。

Conclusion: HUNT框架通过相对导航方法有效统一了无人机的高速穿越和目标跟踪能力，为在挑战性环境中实现自主搜索救援操作提供了可行解决方案。

Abstract: Search and rescue operations require unmanned aerial vehicles to both
traverse unknown unstructured environments at high speed and track targets once
detected. Achieving both capabilities under degraded sensing and without global
localization remains an open challenge. Recent works on relative navigation
have shown robust tracking by anchoring planning and control to a visible
detected object, but cannot address navigation when no target is in the field
of view. We present HUNT (High-speed UAV Navigation and Tracking), a real-time
framework that unifies traversal, acquisition, and tracking within a single
relative formulation. HUNT defines navigation objectives directly from onboard
instantaneous observables such as attitude, altitude, and velocity, enabling
reactive high-speed flight during search. Once a target is detected, the same
perception-control pipeline transitions seamlessly to tracking. Outdoor
experiments in dense forests, container compounds, and search-and-rescue
operations with vehicles and mannequins demonstrate robust autonomy where
global methods fail.

</details>


### [91] [ROPA: Synthetic Robot Pose Generation for RGB-D Bimanual Data Augmentation](https://arxiv.org/abs/2509.19454)
*Jason Chen,I-Chun Arthur Liu,Gaurav Sukhatme,Daniel Seita*

Main category: cs.RO

TL;DR: ROPA是一种离线模仿学习数据增强方法，通过微调Stable Diffusion合成第三人称RGB和RGB-D观察中的新机器人姿态，同时生成对应的关节空间动作标签，并采用约束优化确保双手操作场景中的物理一致性。


<details>
  <summary>Details</summary>
Motivation: 训练稳健的双手操作策略需要覆盖广泛机器人姿态、接触和场景上下文的演示数据，但收集多样且精确的真实世界演示成本高、耗时，限制了可扩展性。现有数据增强方法主要针对手腕相机设置或生成无配对动作的新图像，而针对第三人称RGB-D训练的新动作标签增强研究较少。

Method: 提出ROPA方法，微调Stable Diffusion合成第三人称RGB和RGB-D观察中的新机器人姿态，同时生成对应的关节空间动作标签，并采用约束优化确保双手操作场景中的物理一致性。

Result: 在5个模拟任务和3个真实世界任务上评估，2625次模拟试验和300次真实世界试验结果表明，ROPA优于基线方法和消融实验，显示出在第三人称双手操作中RGB和RGB-D数据增强的可扩展潜力。

Conclusion: ROPA方法在双手操作的数据增强方面表现出色，为第三人称RGB和RGB-D数据增强提供了有效的解决方案，具有可扩展性潜力。

Abstract: Training robust bimanual manipulation policies via imitation learning
requires demonstration data with broad coverage over robot poses, contacts, and
scene contexts. However, collecting diverse and precise real-world
demonstrations is costly and time-consuming, which hinders scalability. Prior
works have addressed this with data augmentation, typically for either
eye-in-hand (wrist camera) setups with RGB inputs or for generating novel
images without paired actions, leaving augmentation for eye-to-hand
(third-person) RGB-D training with new action labels less explored. In this
paper, we propose Synthetic Robot Pose Generation for RGB-D Bimanual Data
Augmentation (ROPA), an offline imitation learning data augmentation method
that fine-tunes Stable Diffusion to synthesize third-person RGB and RGB-D
observations of novel robot poses. Our approach simultaneously generates
corresponding joint-space action labels while employing constrained
optimization to enforce physical consistency through appropriate
gripper-to-object contact constraints in bimanual scenarios. We evaluate our
method on 5 simulated and 3 real-world tasks. Our results across 2625
simulation trials and 300 real-world trials demonstrate that ROPA outperforms
baselines and ablations, showing its potential for scalable RGB and RGB-D data
augmentation in eye-to-hand bimanual manipulation. Our project website is
available at: https://ropaaug.github.io/.

</details>


### [92] [Agentic Scene Policies: Unifying Space, Semantics, and Affordances for Robot Action](https://arxiv.org/abs/2509.19571)
*Sacha Morin,Kumaraditya Gupta,Mahtab Sandhu,Charlie Gauthier,Francesco Argenziano,Kirsty Ellis,Liam Paull*

Main category: cs.RO

TL;DR: ASP是一种基于场景表示的智能体框架，通过语义、空间和功能查询能力执行自然语言指令，相比端到端VLA模型在复杂指令和新场景中表现更好。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉-语言-动作模型在复杂指令和新场景下的执行困难问题，通过显式场景表示来桥接机器人指令与世界状态。

Method: 设计可查询的场景表示接口，利用语义、空间和功能查询能力，通过显式推理对象功能来执行复杂技能。

Result: 在桌面操作任务中优于VLA模型，能够处理房间级查询，通过功能引导导航和扩展场景表示实现复杂任务执行。

Conclusion: ASP框架展示了显式场景表示在机器人语言条件策略中的有效性，为处理开放词汇查询和复杂技能提供了新途径。

Abstract: Executing open-ended natural language queries is a core problem in robotics.
While recent advances in imitation learning and vision-language-actions models
(VLAs) have enabled promising end-to-end policies, these models struggle when
faced with complex instructions and new scenes. An alternative is to design an
explicit scene representation as a queryable interface between the robot and
the world, using query results to guide downstream motion planning. In this
work, we present Agentic Scene Policies (ASP), an agentic framework that
leverages the advanced semantic, spatial, and affordance-based querying
capabilities of modern scene representations to implement a capable
language-conditioned robot policy. ASP can execute open-vocabulary queries in a
zero-shot manner by explicitly reasoning about object affordances in the case
of more complex skills. Through extensive experiments, we compare ASP with VLAs
on tabletop manipulation problems and showcase how ASP can tackle room-level
queries through affordance-guided navigation, and a scaled-up scene
representation. (Project page:
https://montrealrobotics.ca/agentic-scene-policies.github.io/)

</details>


### [93] [EgoBridge: Domain Adaptation for Generalizable Imitation from Egocentric Human Data](https://arxiv.org/abs/2509.19626)
*Ryan Punamiya,Dhruv Patel,Patcharapong Aphiwetsa,Pranav Kuppili,Lawrence Y. Zhu,Simar Kareer,Judy Hoffman,Danfei Xu*

Main category: cs.RO

TL;DR: EgoBridge是一个统一的协同训练框架，通过领域适应方法对齐人类和机器人数据的策略潜在空间，显著提升了机器人模仿学习的性能。


<details>
  <summary>Details</summary>
Motivation: 人类自我中心体验数据为机器人模仿学习提供了丰富资源，但人类和机器人之间存在显著的视觉外观、传感器模态和运动学领域差距，阻碍了知识迁移。

Method: 使用最优传输（OT）测量联合策略潜在特征和动作的差异，学习既能在人类和机器人领域间对齐，又能保留策略学习关键的动作相关信息的观察表示。

Result: 在三个真实世界单臂和双手操作任务中，EgoBridge相比人类增强的跨具身基线实现了44%的绝对策略成功率提升，并能泛化到仅在人数据中出现的新对象、场景和任务。

Conclusion: EgoBridge通过领域适应的潜在空间对齐方法，有效解决了人类-机器人领域差距问题，为利用人类数据提升机器人模仿学习性能提供了可行方案。

Abstract: Egocentric human experience data presents a vast resource for scaling up
end-to-end imitation learning for robotic manipulation. However, significant
domain gaps in visual appearance, sensor modalities, and kinematics between
human and robot impede knowledge transfer. This paper presents EgoBridge, a
unified co-training framework that explicitly aligns the policy latent spaces
between human and robot data using domain adaptation. Through a measure of
discrepancy on the joint policy latent features and actions based on Optimal
Transport (OT), we learn observation representations that not only align
between the human and robot domain but also preserve the action-relevant
information critical for policy learning. EgoBridge achieves a significant
absolute policy success rate improvement by 44% over human-augmented
cross-embodiment baselines in three real-world single-arm and bimanual
manipulation tasks. EgoBridge also generalizes to new objects, scenes, and
tasks seen only in human data, where baselines fail entirely. Videos and
additional information can be found at https://ego-bridge.github.io

</details>


### [94] [Queryable 3D Scene Representation: A Multi-Modal Framework for Semantic Reasoning and Robotic Task Planning](https://arxiv.org/abs/2509.20077)
*Xun Li,Rodrigo Santa Cruz,Mingze Xi,Hu Zhang,Madhawa Perera,Ziwei Wang,Ahalya Ravendran,Brandon J. Matthews,Feng Xu,Matt Adcock,Dadong Wang,Jiajun Liu*

Main category: cs.RO

TL;DR: 提出了3D可查询场景表示（3D QSR）框架，通过融合多模态3D表示实现机器人对复杂3D环境的语义理解和任务规划


<details>
  <summary>Details</summary>
Motivation: 解决机器人理解高级人类指令并执行复杂任务时的场景理解挑战，需要将精确几何结构与丰富语义信息融合的智能地图

Method: 基于多媒体数据构建统一框架，整合三种互补3D表示：全景重建的3D一致新视图渲染和分割、3D点云的精确几何、3D场景图的结构化组织。采用以对象为中心的设计，结合大视觉语言模型实现语义查询能力

Result: 在Unity模拟环境和Replica数据集上的评估显示，该框架能有效支持场景理解，将高级语言指令转化为精确的机器人任务规划。在真实湿实验室数字孪生环境中的应急响应测试也验证了其有效性

Conclusion: 3D QSR框架成功实现了空间和语义推理的集成，能够有效促进场景理解并将高级人类指令转化为复杂3D环境中的精确机器人任务规划

Abstract: To enable robots to comprehend high-level human instructions and perform
complex tasks, a key challenge lies in achieving comprehensive scene
understanding: interpreting and interacting with the 3D environment in a
meaningful way. This requires a smart map that fuses accurate geometric
structure with rich, human-understandable semantics. To address this, we
introduce the 3D Queryable Scene Representation (3D QSR), a novel framework
built on multimedia data that unifies three complementary 3D representations:
(1) 3D-consistent novel view rendering and segmentation from panoptic
reconstruction, (2) precise geometry from 3D point clouds, and (3) structured,
scalable organization via 3D scene graphs. Built on an object-centric design,
the framework integrates with large vision-language models to enable semantic
queryability by linking multimodal object embeddings, and supporting
object-level retrieval of geometric, visual, and semantic information. The
retrieved data are then loaded into a robotic task planner for downstream
execution. We evaluate our approach through simulated robotic task planning
scenarios in Unity, guided by abstract language instructions and using the
indoor public dataset Replica. Furthermore, we apply it in a digital duplicate
of a real wet lab environment to test QSR-supported robotic task planning for
emergency response. The results demonstrate the framework's ability to
facilitate scene understanding and integrate spatial and semantic reasoning,
effectively translating high-level human instructions into precise robotic task
planning in complex 3D environments.

</details>


### [95] [VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation](https://arxiv.org/abs/2509.20322)
*Shaofeng Yin,Yanjie Ze,Hong-Xing Yu,C. Karen Liu,Jiajun Wu*

Main category: cs.RO

TL;DR: VisualMimic是一个视觉模拟到现实的框架，通过分层全身控制将自我中心视觉与人体机器人运动控制相结合，实现了在非结构化环境中的零样本迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖外部运动捕捉系统，要么无法在多样化任务中泛化，无法满足非结构化环境中人体机器人运动操作的感知与控制集成需求。

Method: 结合任务无关的低层关键点跟踪器（通过师生方案从人类运动数据训练）和任务特定的高层策略（从视觉和本体感觉输入生成关键点命令），通过向低层策略注入噪声和使用人类运动统计数据裁剪高层动作来确保训练稳定性。

Result: 实现了从模拟到真实人体机器人的零样本迁移，完成了多种运动操作任务（如箱子搬运、推、足球运球和踢球），并在室外环境中表现出强大的泛化能力。

Conclusion: VisualMimic框架成功解决了人体机器人在非结构化环境中视觉感知与全身控制的集成问题，展示了在真实世界中的实际应用潜力。

Abstract: Humanoid loco-manipulation in unstructured environments demands tight
integration of egocentric perception and whole-body control. However, existing
approaches either depend on external motion capture systems or fail to
generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real
framework that unifies egocentric vision with hierarchical whole-body control
for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint
tracker -- trained from human motion data via a teacher-student scheme -- with
a task-specific high-level policy that generates keypoint commands from visual
and proprioceptive input. To ensure stable training, we inject noise into the
low-level policy and clip high-level actions using human motion statistics.
VisualMimic enables zero-shot transfer of visuomotor policies trained in
simulation to real humanoid robots, accomplishing a wide range of
loco-manipulation tasks such as box lifting, pushing, football dribbling, and
kicking. Beyond controlled laboratory settings, our policies also generalize
robustly to outdoor environments. Videos are available at:
https://visualmimic.github.io .

</details>
