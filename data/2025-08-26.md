<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 195]
- [cs.MM](#cs.MM) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [cs.LG](#cs.LG) [Total: 12]
- [cs.GR](#cs.GR) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.AI](#cs.AI) [Total: 3]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 6]
- [cs.HC](#cs.HC) [Total: 2]
- [eess.IV](#eess.IV) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Towards High-Precision Depth Sensing via Monocular-Aided iToF and RGB Integration](https://arxiv.org/abs/2508.16579)
*Yansong Du,Yutong Deng,Yuting Zhou,Feiyu Jiao,Jian Song,Xun Guan*

Main category: cs.CV

TL;DR: 提出了一种新颖的iToF-RGB融合框架，解决iToF深度感知的低分辨率、有限视场和结构失真问题。


<details>
  <summary>Details</summary>
Motivation: iToF深度感知存在低空间分辨率、有限视场和复杂场景中的结构失真等固有局限性，需要一种方法来提升其性能。

Method: 通过几何校准和对齐模块将窄视场iToF深度图重投影到宽视场RGB坐标系，利用双编码器融合网络结合RGB图像和深度先验恢复细节并进行深度超分辨率。

Result: 在合成和真实数据集上的实验表明，该方法在准确性、结构一致性和视觉质量上显著优于现有方法。

Conclusion: 该框架通过跨模态结构线索和深度一致性约束，实现了深度精度提升、边缘锐化及视场无缝扩展。

Abstract: This paper presents a novel iToF-RGB fusion framework designed to address the
inherent limitations of indirect Time-of-Flight (iToF) depth sensing, such as
low spatial resolution, limited field-of-view (FoV), and structural distortion
in complex scenes. The proposed method first reprojects the narrow-FoV iToF
depth map onto the wide-FoV RGB coordinate system through a precise geometric
calibration and alignment module, ensuring pixel-level correspondence between
modalities. A dual-encoder fusion network is then employed to jointly extract
complementary features from the reprojected iToF depth and RGB image, guided by
monocular depth priors to recover fine-grained structural details and perform
depth super-resolution. By integrating cross-modal structural cues and depth
consistency constraints, our approach achieves enhanced depth accuracy,
improved edge sharpness, and seamless FoV expansion. Extensive experiments on
both synthetic and real-world datasets demonstrate that the proposed framework
significantly outperforms state-of-the-art methods in terms of accuracy,
structural consistency, and visual quality.

</details>


### [2] [CountLoop: Training-Free High-Instance Image Generation via Iterative Agent Guidance](https://arxiv.org/abs/2508.16644)
*Anindya Mondal,Ayan Banerjee,Sauradip Nag,Josep Lladós,Xiatian Zhu,Anjan Dutta*

Main category: cs.CV

TL;DR: CountLoop是一个无需训练的框架，通过迭代结构化反馈为扩散模型提供精确的实例控制，显著提高了复杂场景中对象计数的准确性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成精确对象实例数量的场景时不可靠，尤其是在复杂和高密度场景中。

Method: 采用迭代反馈机制，结合语言引导的规划器和批评器评估对象计数、空间布局和属性一致性，并引入实例驱动的注意力掩码和组合生成技术。

Result: 在多个基准测试中，CountLoop实现了高达98%的计数准确率，同时保持了空间保真度和视觉质量，优于基线方法。

Conclusion: CountLoop通过结构化反馈和实例驱动技术，显著提升了扩散模型在复杂场景中的实例控制能力。

Abstract: Diffusion models have shown remarkable progress in photorealistic image
synthesis, yet they remain unreliable for generating scenes with a precise
number of object instances, particularly in complex and high-density settings.
We present CountLoop, a training-free framework that provides diffusion models
with accurate instance control through iterative structured feedback. The
approach alternates between image generation and multimodal agent evaluation,
where a language-guided planner and critic assess object counts, spatial
arrangements, and attribute consistency. This feedback is then used to refine
layouts and guide subsequent generations. To further improve separation between
objects, especially in occluded scenes, we introduce instance-driven attention
masking and compositional generation techniques. Experiments on COCO Count, T2I
CompBench, and two new high-instance benchmarks show that CountLoop achieves
counting accuracy of up to 98% while maintaining spatial fidelity and visual
quality, outperforming layout-based and gradient-guided baselines with a score
of 0.97.

</details>


### [3] [Do VLMs Have Bad Eyes? Diagnosing Compositional Failures via Mechanistic Interpretability](https://arxiv.org/abs/2508.16652)
*Ashwath Vaithinathan Aravindan,Abha Jha,Mihir Kulkarni*

Main category: cs.CV

TL;DR: 研究发现CLIP视觉编码器中单个神经元的多特征表示（叠加）阻碍了其组合特征表示能力，影响了组合推理和对象绑定。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型（VLMs）在组合泛化和对象绑定方面失败的根本原因。

Method: 使用机制可解释性技术分析CLIP视觉编码器的MLP层神经元。

Result: 发现神经元的多特征表示直接阻碍了组合特征表示能力。

Conclusion: 本研究为揭示VLMs组合失败的机制根源提供了初步探索。

Abstract: Vision-Language Models (VLMs) have shown remarkable performance in
integrating visual and textual information for tasks such as image captioning
and visual question answering. However, these models struggle with
compositional generalization and object binding, which limit their ability to
handle novel combinations of objects and their attributes. Our work explores
the root causes of these failures using mechanistic interpretability
techniques. We show evidence that individual neurons in the MLP layers of
CLIP's vision encoder represent multiple features, and this "superposition"
directly hinders its compositional feature representation which consequently
affects compositional reasoning and object binding capabilities. We hope this
study will serve as an initial step toward uncovering the mechanistic roots of
compositional failures in VLMs. The code and supporting results can be found
https://github.com/Mystic-Slice/Do-VLMs-Have-Bad-Eyes .

</details>


### [4] [MSNav: Zero-Shot Vision-and-Language Navigation with Dynamic Memory and LLM Spatial Reasoning](https://arxiv.org/abs/2508.16654)
*Chenghao Liu,Zhimu Zhou,Jiachen Zhang,Minghao Zhang,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: MSNav框架通过融合记忆、空间和决策模块，解决了VLN任务中的空间推理、跨模态对齐和内存过载问题，显著提升了导航性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLN任务中单一LLM的端到端决策存在空间推理差、跨模态对齐弱和长任务内存过载等问题，需要系统性解决方案。

Method: 提出MSNav框架，包含动态地图记忆模块（解决内存过载）、空间推理模块（提升终点识别）和LLM决策模块（执行稳健动作）。

Result: 在R2R和REVERIE数据集上，MSNav在SR和SPL指标上达到最优性能，Qwen-Sp模型在I-O-S测试集上表现优于商业LLM。

Conclusion: MSNav通过模块化设计有效提升了VLN任务的鲁棒性和性能，为复杂导航任务提供了新思路。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to interpret natural
language instructions and navigate complex environments. Current approaches
often adopt a "black-box" paradigm, where a single Large Language Model (LLM)
makes end-to-end decisions. However, it is plagued by critical vulnerabilities,
including poor spatial reasoning, weak cross-modal grounding, and memory
overload in long-horizon tasks. To systematically address these issues, we
propose Memory Spatial Navigation(MSNav), a framework that fuses three modules
into a synergistic architecture, which transforms fragile inference into a
robust, integrated intelligence. MSNav integrates three modules: Memory Module,
a dynamic map memory module that tackles memory overload through selective node
pruning, enhancing long-range exploration; Spatial Module, a module for spatial
reasoning and object relationship inference that improves endpoint recognition;
and Decision Module, a module using LLM-based path planning to execute robust
actions. Powering Spatial Module, we also introduce an Instruction-Object-Space
(I-O-S) dataset and fine-tune the Qwen3-4B model into Qwen-Spatial (Qwen-Sp),
which outperforms leading commercial LLMs in object list extraction, achieving
higher F1 and NDCG scores on the I-O-S test set. Extensive experiments on the
Room-to-Room (R2R) and REVERIE datasets demonstrate MSNav's state-of-the-art
performance with significant improvements in Success Rate (SR) and Success
weighted by Path Length (SPL).

</details>


### [5] [Optimizing Hyper parameters in CNN for Soil Classification using PSO and Whale Optimization Algorithm](https://arxiv.org/abs/2508.16660)
*Yasir Nooruldeen Ibrahim,Fawziya Mahmood Ramo,Mahmood Siddeeq Qadir,Muna Jaffer Al-Shamdeen*

Main category: cs.CV

TL;DR: 使用卷积神经网络和群体优化算法（鲸鱼优化和粒子群优化）进行土壤分类，提高了分类准确性和性能。


<details>
  <summary>Details</summary>
Motivation: 土壤分类对土地管理、农业产出和环境问题解决至关重要，人工智能技术可提升分类效率和准确性。

Method: 结合卷积神经网络和群体优化算法（鲸鱼优化和粒子群优化）选择超参数，优化土壤图像分类。

Result: 采用准确率和F1分数评估，提出的方法在土壤分类中表现出高效性能。

Conclusion: 群体优化算法显著提升了卷积神经网络在土壤分类中的性能，为实际应用提供了有效解决方案。

Abstract: Classifying soil images contributes to better land management, increased
agricultural output, and practical solutions for environmental issues. The
development of various disciplines, particularly agriculture, civil
engineering, and natural resource management, is aided by understanding of soil
quality since it helps with risk reduction, performance improvement, and sound
decision-making . Artificial intelligence has recently been used in a number of
different fields. In this study, an intelligent model was constructed using
Convolutional Neural Networks to classify soil kinds, and machine learning
algorithms were used to enhance the performance of soil classification . To
achieve better implementation and performance of the Convolutional Neural
Networks algorithm and obtain valuable results for the process of classifying
soil type images, swarm algorithms were employed to obtain the best performance
by choosing Hyper parameters for the Convolutional Neural Networks network
using the Whale optimization algorithm and the Particle swarm optimization
algorithm, and comparing the results of using the two algorithms in the process
of multiple classification of soil types. The Accuracy and F1 measures were
adopted to test the system, and the results of the proposed work were efficient
result

</details>


### [6] [QA-VLM: Providing human-interpretable quality assessment for wire-feed laser additive manufacturing parts with Vision Language Models](https://arxiv.org/abs/2508.16661)
*Qiaojie Zheng,Jiucai Zhang,Joy Gockel,Michael B. Wakin,Craig Brice,Xiaoli Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型（VLM）的QA-VLM框架，用于增材制造中的图像质量评估，提供可解释的输出。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工操作且机器学习方法缺乏可解释性，限制了实际应用。

Method: 利用VLM的注意力机制和推理能力，结合领域知识，生成可解释的质量评估。

Result: 在24个DED-LW样本上验证，QA-VLM的解释质量和一致性优于现有VLM。

Conclusion: QA-VLM框架为增材制造提供了可信且可解释的质量评估方法。

Abstract: Image-based quality assessment (QA) in additive manufacturing (AM) often
relies heavily on the expertise and constant attention of skilled human
operators. While machine learning and deep learning methods have been
introduced to assist in this task, they typically provide black-box outputs
without interpretable justifications, limiting their trust and adoption in
real-world settings. In this work, we introduce a novel QA-VLM framework that
leverages the attention mechanisms and reasoning capabilities of
vision-language models (VLMs), enriched with application-specific knowledge
distilled from peer-reviewed journal articles, to generate human-interpretable
quality assessments. Evaluated on 24 single-bead samples produced by laser wire
direct energy deposition (DED-LW), our framework demonstrates higher validity
and consistency in explanation quality than off-the-shelf VLMs. These results
highlight the potential of our approach to enable trustworthy, interpretable
quality assessment in AM applications.

</details>


### [7] [The Loupe: A Plug-and-Play Attention Module for Amplifying Discriminative Features in Vision Transformers](https://arxiv.org/abs/2508.16663)
*Naren Sengodan*

Main category: cs.CV

TL;DR: The paper introduces The Loupe, a lightweight attention module for Fine-Grained Visual Classification (FGVC), improving model performance and interpretability without needing part-level annotations.


<details>
  <summary>Details</summary>
Motivation: FGVC requires precise identification of subtle visual cues, but existing models lack interpretability for critical applications like biodiversity monitoring and medical diagnostics.

Method: The Loupe is a plug-and-play attention module added to pre-trained backbones (e.g., Swin Transformer), trained end-to-end with a composite loss function to focus on discriminative object parts.

Result: On the CUB-200-2011 dataset, The Loupe boosts Swin-Base model accuracy from 85.40% to 88.06%, with attention maps localizing meaningful features.

Conclusion: The Loupe enhances both performance and interpretability in FGVC, providing a tool for trustworthy model decisions.

Abstract: Fine-Grained Visual Classification (FGVC) is a critical and challenging area
within computer vision, demanding the identification of highly subtle,
localized visual cues. The importance of FGVC extends to critical applications
such as biodiversity monitoring and medical diagnostics, where precision is
paramount. While large-scale Vision Transformers have achieved state-of-the-art
performance, their decision-making processes often lack the interpretability
required for trust and verification in such domains. In this paper, we
introduce The Loupe, a novel, lightweight, and plug-and-play attention module
designed to be inserted into pre-trained backbones like the Swin Transformer.
The Loupe is trained end-to-end with a composite loss function that implicitly
guides the model to focus on the most discriminative object parts without
requiring explicit part-level annotations. Our unique contribution lies in
demonstrating that a simple, intrinsic attention mechanism can act as a
powerful regularizer, significantly boosting performance while simultaneously
providing clear visual explanations. Our experimental evaluation on the
challenging CUB-200-2011 dataset shows that The Loupe improves the accuracy of
a Swin-Base model from 85.40% to 88.06%, a significant gain of 2.66%.
Crucially, our qualitative analysis of the learned attention maps reveals that
The Loupe effectively localizes semantically meaningful features, providing a
valuable tool for understanding and trusting the model's decision-making
process.

</details>


### [8] [COVID19 Prediction Based On CT Scans Of Lungs Using DenseNet Architecture](https://arxiv.org/abs/2508.16670)
*Deborup Sanyal*

Main category: cs.CV

TL;DR: 利用卷积神经网络分析COVID19患者的肺部CT扫描，预测感染严重程度。


<details>
  <summary>Details</summary>
Motivation: COVID19导致全球大流行，呼吸系统衰竭是主要死因，医疗资源紧张，急需准确预测病情严重程度的方法。

Method: 使用卷积神经网络模型分析患者肺部CT扫描数据。

Result: 模型能在一个月内预测患者是否需要插管或死亡等不良结果。

Conclusion: 该模型有助于医生快速判断COVID19患者病情严重程度，优化医疗资源分配。

Abstract: COVID19 took the world by storm since December 2019. A highly infectious
communicable disease, COVID19 is caused by the SARSCoV2 virus. By March 2020,
the World Health Organization (WHO) declared COVID19 as a global pandemic. A
pandemic in the 21st century after almost 100 years was something the world was
not prepared for, which resulted in the deaths of around 1.6 million people
worldwide. The most common symptoms of COVID19 were associated with the
respiratory system and resembled a cold, flu, or pneumonia. After extensive
research, doctors and scientists concluded that the main reason for lives being
lost due to COVID19 was failure of the respiratory system. Patients were dying
gasping for breath. Top healthcare systems of the world were failing badly as
there was an acute shortage of hospital beds, oxygen cylinders, and
ventilators. Many were dying without receiving any treatment at all. The aim of
this project is to help doctors decide the severity of COVID19 by reading the
patient's Computed Tomography (CT) scans of the lungs. Computer models are less
prone to human error, and Machine Learning or Neural Network models tend to
give better accuracy as training improves over time. We have decided to use a
Convolutional Neural Network model. Given that a patient tests positive, our
model will analyze the severity of COVID19 infection within one month of the
positive test result. The severity of the infection may be promising or
unfavorable (if it leads to intubation or death), based entirely on the CT
scans in the dataset.

</details>


### [9] [MedRepBench: A Comprehensive Benchmark for Medical Report Interpretation](https://arxiv.org/abs/2508.16674)
*Fangxin Shang,Yuan Xia,Dalu Yang,Yahui Wang,Binglin Yang*

Main category: cs.CV

TL;DR: MedRepBench是一个用于评估视觉语言模型（VLMs）在结构化医学报告理解中的性能的综合基准，包含1900份中文医学报告，支持客观和主观评估协议。


<details>
  <summary>Details</summary>
Motivation: 缺乏标准化基准来评估医学报告的结构化解释质量，尤其是在中文医疗环境中。

Method: 构建MedRepBench基准，包含多样化的医学报告，支持两种评估协议：客观评估和自动化主观评估。使用GRPO优化VLM性能。

Result: 优化后的VLM在召回率上提升了6%，但OCR+LLM流程存在布局盲区和延迟问题。

Conclusion: MedRepBench为医学报告理解提供了标准化评估工具，并揭示了完全基于视觉的报告理解的必要性。

Abstract: Medical report interpretation plays a crucial role in healthcare, enabling
both patient-facing explanations and effective information flow across clinical
systems. While recent vision-language models (VLMs) and large language models
(LLMs) have demonstrated general document understanding capabilities, there
remains a lack of standardized benchmarks to assess structured interpretation
quality in medical reports. We introduce MedRepBench, a comprehensive benchmark
built from 1,900 de-identified real-world Chinese medical reports spanning
diverse departments, patient demographics, and acquisition formats. The
benchmark is designed primarily to evaluate end-to-end VLMs for structured
medical report understanding. To enable controlled comparisons, we also include
a text-only evaluation setting using high-quality OCR outputs combined with
LLMs, allowing us to estimate the upper-bound performance when character
recognition errors are minimized. Our evaluation framework supports two
complementary protocols: (1) an objective evaluation measuring field-level
recall of structured clinical items, and (2) an automated subjective evaluation
using a powerful LLM as a scoring agent to assess factuality, interpretability,
and reasoning quality. Based on the objective metric, we further design a
reward function and apply Group Relative Policy Optimization (GRPO) to improve
a mid-scale VLM, achieving up to 6% recall gain. We also observe that the
OCR+LLM pipeline, despite strong performance, suffers from layout-blindness and
latency issues, motivating further progress toward robust, fully vision-based
report understanding.

</details>


### [10] [Two-Stage Framework for Efficient UAV-Based Wildfire Video Analysis with Adaptive Compression and Fire Source Detection](https://arxiv.org/abs/2508.16739)
*Yanbing Bai,Rui-Yang Ju,Lemeng Zhao,Junjie Hu,Jianchao Bi,Erick Mas,Shunichi Koshimura*

Main category: cs.CV

TL;DR: 提出了一种轻量级的两阶段框架，用于无人机平台的实时野火监测和火源检测，显著降低了计算成本并保持了高准确性。


<details>
  <summary>Details</summary>
Motivation: 由于无人机计算资源有限，无法独立运行大型模型进行实时分析，因此需要一种高效的方法来实现实时视频分析。

Method: 第一阶段使用策略网络和帧压缩技术减少冗余视频片段；第二阶段采用改进的YOLOv8模型定位火源。

Result: 实验表明，第一阶段在降低计算成本的同时保持了分类准确性，第二阶段在相似推理时间下实现了更高的检测准确性。

Conclusion: 该框架为无人机平台的实时野火监测提供了一种高效且准确的解决方案。

Abstract: Unmanned Aerial Vehicles (UAVs) have become increasingly important in
disaster emergency response by enabling real-time aerial video analysis. Due to
the limited computational resources available on UAVs, large models cannot be
run independently for real-time analysis. To overcome this challenge, we
propose a lightweight and efficient two-stage framework for real-time wildfire
monitoring and fire source detection on UAV platforms. Specifically, in Stage
1, we utilize a policy network to identify and discard redundant video clips
using frame compression techniques, thereby reducing computational costs. In
addition, we introduce a station point mechanism that leverages future frame
information within the sequential policy network to improve prediction
accuracy. In Stage 2, once the frame is classified as "fire", we employ the
improved YOLOv8 model to localize the fire source. We evaluate the Stage 1
method using the FLAME and HMDB51 datasets, and the Stage 2 method using the
Fire & Smoke dataset. Experimental results show that our method significantly
reduces computational costs while maintaining classification accuracy in Stage
1, and achieves higher detection accuracy with similar inference time in Stage
2 compared to baseline methods.

</details>


### [11] [CellEcoNet: Decoding the Cellular Language of Pathology with Deep Learning for Invasive Lung Adenocarcinoma Recurrence Prediction](https://arxiv.org/abs/2508.16742)
*Abdul Rehman Akbar,Usama Sajjad,Ziyu Su,Wencheng Li,Fei Xing,Jimmy Ruiz,Wei Chen,Muhammad Khalid Khan Niazi*

Main category: cs.CV

TL;DR: CellEcoNet是一种新型的空间感知深度学习框架，通过自然语言类比分析全切片图像，预测侵袭性肺腺癌患者的复发风险，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有工具无法准确识别需要辅助治疗的侵袭性肺腺癌患者，临床需求未得到满足。

Method: CellEcoNet将细胞视为单词，细胞邻域视为短语，组织架构视为句子，自动学习上下文依赖关系，捕捉空间相互作用。

Result: 在456张H&E染色全切片图像上，CellEcoNet的预测性能（AUC:77.8%，HR:9.54）优于IASLC分级系统和其他计算方法。

Conclusion: CellEcoNet不仅提供了预后工具，还通过解码肿瘤微环境的“语言”揭示了细胞变异如何编码复发风险。

Abstract: Despite surgical resection, ~70% of invasive lung adenocarcinoma (ILA)
patients recur within five years, and current tools fail to identify those
needing adjuvant therapy. To address this unmet clinical need, we introduce
CellEcoNet, a novel spatially aware deep learning framework that models whole
slide images (WSIs) through natural language analogy, defining a "language of
pathology," where cells act as words, cellular neighborhoods become phrases,
and tissue architecture forms sentences. CellEcoNet learns these
context-dependent meanings automatically, capturing how subtle variations and
spatial interactions derive recurrence risk. On a dataset of 456 H&E-stained
WSIs, CellEcoNet achieved superior predictive performance (AUC:77.8% HR:9.54),
outperforming IASLC grading system (AUC:71.4% HR:2.36), AJCC Stage (AUC:64.0%
HR:1.17) and state-of-the-art computational methods (AUCs:62.2-67.4%).
CellEcoNet demonstrated fairness and consistent performance across diverse
demographic and clinical subgroups. Beyond prognosis, CellEcoNet marks a
paradigm shift by decoding the tumor microenvironment's cellular "language" to
reveal how subtle cell variations encode recurrence risk.

</details>


### [12] [A Framework for Benchmarking Fairness-Utility Trade-offs in Text-to-Image Models via Pareto Frontiers](https://arxiv.org/abs/2508.16752)
*Marco N. Bochernitsan,Rodrigo C. Barros,Lucas S. Kupssinskü*

Main category: cs.CV

TL;DR: 提出了一种评估文本到图像生成模型公平性和效用的方法，通过Pareto最优前沿分析，发现大多数默认超参数配置在公平性和效用空间中被更优解支配。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成模型的公平性评估方法依赖定性判断或狭窄比较，难以同时评估公平性和效用，且缺乏可重复性。

Method: 使用Pareto最优前沿分析超参数配置，以Normalized Shannon Entropy和ClipScore分别评估公平性和效用。

Result: 评估了多种模型（如Stable Diffusion、Fair Diffusion等），发现大多数默认超参数配置在公平性和效用空间中被更优解支配。

Conclusion: 该方法能有效评估和优化文本到图像生成模型的公平性和效用，为可重复的公平性评估提供了新思路。

Abstract: Achieving fairness in text-to-image generation demands mitigating social
biases without compromising visual fidelity, a challenge critical to
responsible AI. Current fairness evaluation procedures for text-to-image models
rely on qualitative judgment or narrow comparisons, which limit the capacity to
assess both fairness and utility in these models and prevent reproducible
assessment of debiasing methods. Existing approaches typically employ ad-hoc,
human-centered visual inspections that are both error-prone and difficult to
replicate. We propose a method for evaluating fairness and utility in
text-to-image models using Pareto-optimal frontiers across hyperparametrization
of debiasing methods. Our method allows for comparison between distinct
text-to-image models, outlining all configurations that optimize fairness for a
given utility and vice-versa. To illustrate our evaluation method, we use
Normalized Shannon Entropy and ClipScore for fairness and utility evaluation,
respectively. We assess fairness and utility in Stable Diffusion, Fair
Diffusion, SDXL, DeCoDi, and FLUX text-to-image models. Our method shows that
most default hyperparameterizations of the text-to-image model are dominated
solutions in the fairness-utility space, and it is straightforward to find
better hyperparameters.

</details>


### [13] [WebMMU: A Benchmark for Multimodal Multilingual Website Understanding and Code Generation](https://arxiv.org/abs/2508.16763)
*Rabiul Awal,Mahsa Massoud,Aarash Feizi,Zichao Li,Suyuchen Wang,Christopher Pal,Aishwarya Agrawal,David Vazquez,Siva Reddy,Juan A. Rodriguez,Perouz Taslakian,Spandana Gella,Sai Rajeswar*

Main category: cs.CV

TL;DR: WebMMU是一个多语言基准测试，评估三个核心网络任务：网站视觉问答、代码编辑和设计到代码生成，揭示当前多模态大语言模型在复杂推理和跨语言能力上的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试将这些任务分开处理，而WebMMU通过真实世界数据统一评估模型在多步推理、元素定位和功能理解上的能力。

Method: 使用专家标注的真实网络数据，评估模型在视觉问答、代码编辑和设计到代码生成任务中的表现。

Result: 多模态大语言模型在基础信息提取上表现良好，但在推理、代码功能维护和多语言内容生成上存在困难。

Conclusion: 当前模型在多模态和跨语言推理方面存在局限，未来需改进以支持自动化网络开发任务。

Abstract: We present WebMMU, a multilingual benchmark that evaluates three core web
tasks: (1) website visual question answering, (2) code editing involving
HTML/CSS/JavaScript, and (3) mockup-to-code generation. Unlike prior benchmarks
that treat these tasks separately, WebMMU unifies them using expert-annotated,
real-world web data to assess models' abilities in complex multi-step
reasoning, precise element grounding, and functional UI comprehension and
coding. Our evaluation shows that while multimodal large language models
(MLLMs) perform well on basic information extraction, they struggle with
reasoning and grounding, editing code to preserve functionality, and generating
design-to-code that maintains hierarchy and supports multilingual content.
These findings reveal key limitations in current MLLMs and underscore the need
for improved multimodal and cross-lingual reasoning to build future web agents
capable of automating diverse web development tasks.

</details>


### [14] [Improving Performance, Robustness, and Fairness of Radiographic AI Models with Finely-Controllable Synthetic Data](https://arxiv.org/abs/2508.16783)
*Stefania L. Moroianu,Christian Bluethgen,Pierre Chambon,Mehdi Cherti,Jean-Benoit Delbrouck,Magdalini Paschali,Brandon Price,Judy Gichoya,Jenia Jitsev,Curtis P. Langlotz,Akshay S. Chaudhari*

Main category: cs.CV

TL;DR: RoentGen-v2是一个用于生成胸部X光片的文本到图像扩散模型，通过合成数据提升模型性能和公平性。


<details>
  <summary>Details</summary>
Motivation: 解决临床可部署深度学习模型在多样患者群体中性能和公平性的挑战。

Method: 提出RoentGen-v2模型，生成合成数据并用于监督预训练，再在真实数据上微调。

Result: 合成预训练使下游分类模型准确率提高6.5%，公平性差距减少19.3%。

Conclusion: 合成数据可提升医学深度学习的公平性和泛化能力。

Abstract: Achieving robust performance and fairness across diverse patient populations
remains a challenge in developing clinically deployable deep learning models
for diagnostic imaging. Synthetic data generation has emerged as a promising
strategy to address limitations in dataset scale and diversity. We introduce
RoentGen-v2, a text-to-image diffusion model for chest radiographs that enables
fine-grained control over both radiographic findings and patient demographic
attributes, including sex, age, and race/ethnicity. RoentGen-v2 is the first
model to generate clinically plausible images with demographic conditioning,
facilitating the creation of a large, demographically balanced synthetic
dataset comprising over 565,000 images. We use this large synthetic dataset to
evaluate optimal training pipelines for downstream disease classification
models. In contrast to prior work that combines real and synthetic data
naively, we propose an improved training strategy that leverages synthetic data
for supervised pretraining, followed by fine-tuning on real data. Through
extensive evaluation on over 137,000 chest radiographs from five institutions,
we demonstrate that synthetic pretraining consistently improves model
performance, generalization to out-of-distribution settings, and fairness
across demographic subgroups. Across datasets, synthetic pretraining led to a
6.5% accuracy increase in the performance of downstream classification models,
compared to a modest 2.7% increase when naively combining real and synthetic
data. We observe this performance improvement simultaneously with the reduction
of the underdiagnosis fairness gap by 19.3%. These results highlight the
potential of synthetic imaging to advance equitable and generalizable medical
deep learning under real-world data constraints. We open source our code,
trained models, and synthetic dataset at
https://github.com/StanfordMIMI/RoentGen-v2 .

</details>


### [15] [Towards Open-Vocabulary Multimodal 3D Object Detection with Attributes](https://arxiv.org/abs/2508.16812)
*Xinhao Xiang,Kuan-Chuan Peng,Suhas Lohit,Michael J. Jones,Jiawei Zhang*

Main category: cs.CV

TL;DR: OVODA是一个开放词汇的3D物体和属性检测框架，无需已知新类别的锚点尺寸，利用基础模型连接3D特征与文本语义，并在nuScenes和Argoverse 2数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测方法受限于闭集假设，难以识别现实场景中的新物体及其属性。

Method: OVODA通过基础模型特征拼接、提示调优策略和属性检测专用技术（如视角指定提示和水平翻转增强）实现开放词汇检测。

Result: 在未提供新类别锚点尺寸的条件下，OVODA在开放词汇3D物体检测中优于现有方法，并能成功识别物体属性。

Conclusion: OVODA为开放词汇3D物体和属性检测提供了有效解决方案，并发布了OVAD数据集以支持相关研究。

Abstract: 3D object detection plays a crucial role in autonomous systems, yet existing
methods are limited by closed-set assumptions and struggle to recognize novel
objects and their attributes in real-world scenarios. We propose OVODA, a novel
framework enabling both open-vocabulary 3D object and attribute detection with
no need to know the novel class anchor size. OVODA uses foundation models to
bridge the semantic gap between 3D features and texts while jointly detecting
attributes, e.g., spatial relationships, motion states, etc. To facilitate such
research direction, we propose OVAD, a new dataset that supplements existing 3D
object detection benchmarks with comprehensive attribute annotations. OVODA
incorporates several key innovations, including foundation model feature
concatenation, prompt tuning strategies, and specialized techniques for
attribute detection, including perspective-specified prompts and horizontal
flip augmentation. Our results on both the nuScenes and Argoverse 2 datasets
show that under the condition of no given anchor sizes of novel classes, OVODA
outperforms the state-of-the-art methods in open-vocabulary 3D object detection
while successfully recognizing object attributes. Our OVAD dataset is released
here: https://doi.org/10.5281/zenodo.16904069 .

</details>


### [16] [AIM 2025 Low-light RAW Video Denoising Challenge: Dataset, Methods and Results](https://arxiv.org/abs/2508.16830)
*Alexander Yakovenko,George Chakvetadze,Ilya Khrapov,Maksim Zhelezov,Dmitry Vatolin,Radu Timofte,Youngjin Oh,Junhyeong Kwon,Junyoung Park,Nam Ik Cho,Senyan Xu,Ruixuan Jiang,Long Peng,Xueyang Fu,Zheng-Jun Zha,Xiaoping Peng,Hansen Feng,Zhanyi Tie,Ziming Xia,Lizhi Wang*

Main category: cs.CV

TL;DR: AIM 2025挑战赛聚焦于低光RAW视频去噪，提出新基准数据集并评估多种方法。


<details>
  <summary>Details</summary>
Motivation: 解决低光条件下RAW视频的噪声问题，同时适应传感器特性和信号依赖噪声。

Method: 利用时间冗余性，在帧率和曝光时间限制下处理RAW序列，保留Bayer模式。

Result: 新数据集包含756个序列，14种传感器，9种条件，评估采用PSNR和SSIM。

Conclusion: 挑战赛为低光RAW视频去噪提供了基准和方法比较。

Abstract: This paper reviews the AIM 2025 (Advances in Image Manipulation) Low-Light
RAW Video Denoising Challenge. The task is to develop methods that denoise
low-light RAW video by exploiting temporal redundancy while operating under
exposure-time limits imposed by frame rate and adapting to sensor-specific,
signal-dependent noise. We introduce a new benchmark of 756 ten-frame sequences
captured with 14 smartphone camera sensors across nine conditions
(illumination: 1/5/10 lx; exposure: 1/24, 1/60, 1/120 s), with high-SNR
references obtained via burst averaging. Participants process linear RAW
sequences and output the denoised 10th frame while preserving the Bayer
pattern. Submissions are evaluated on a private test set using full-reference
PSNR and SSIM, with final ranking given by the mean of per-metric ranks. This
report describes the dataset, challenge protocol, and submitted approaches.

</details>


### [17] [Transformer-Based Neural Network for Transient Detection without Image Subtraction](https://arxiv.org/abs/2508.16844)
*Adi Inada,Masao Sako,Tatiana Acero-Cuellar,Federica Bianco*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的神经网络，用于天文图像中真实与虚假瞬变检测的准确分类，性能优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 传统CNN方法在天文图像处理中表现有限，需要计算昂贵的差异成像，因此需要更高效的架构。

Method: 采用Transformer架构，专注于搜索和模板图像的像素级比较，避免差异成像。

Result: 在DES的autoScan数据集上达到97.4%的分类准确率，且差异图像的性能随训练集增大而降低。

Conclusion: 该网络显著提升了大规模天文巡天中超新星检测的准确性和效率。

Abstract: We introduce a transformer-based neural network for the accurate
classification of real and bogus transient detections in astronomical images.
This network advances beyond the conventional convolutional neural network
(CNN) methods, widely used in image processing tasks, by adopting an
architecture better suited for detailed pixel-by-pixel comparison. The
architecture enables efficient analysis of search and template images only,
thus removing the necessity for computationally-expensive difference imaging,
while maintaining high performance. Our primary evaluation was conducted using
the autoScan dataset from the Dark Energy Survey (DES), where the network
achieved a classification accuracy of 97.4% and diminishing performance utility
for difference image as the size of the training set grew. Further experiments
with DES data confirmed that the network can operate at a similar level even
when the input images are not centered on the supernova candidate. These
findings highlight the network's effectiveness in enhancing both accuracy and
efficiency of supernova detection in large-scale astronomical surveys.

</details>


### [18] [NinA: Normalizing Flows in Action. Training VLA Models with Normalizing Flows](https://arxiv.org/abs/2508.16845)
*Denis Tarasov,Alexander Nikulin,Ilya Zisman,Albina Klepach,Nikita Lyubaykin,Andrei Polubarov,Alexander Derevyagin,Vladislav Kurenkov*

Main category: cs.CV

TL;DR: NinA（Normalizing Flows in Action）是一种快速且表达能力强的替代方案，用于替代基于扩散模型的Vision-Language-Action（VLA）解码器，显著减少推理时间。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在推理时需要多次迭代去噪步骤或下游技术来加速采样，限制了其在需要高频控制的现实场景中的实用性。

Method: NinA用Normalizing Flow（NF）替代扩散动作解码器，通过可逆变换实现一次性采样。

Result: NinA在相同训练条件下与基于扩散的解码器性能相当，但推理速度显著更快。

Conclusion: NinA为高效、高频的VLA控制提供了一条有前景的路径，且不牺牲性能。

Abstract: Recent advances in Vision-Language-Action (VLA) models have established a
two-component architecture, where a pre-trained Vision-Language Model (VLM)
encodes visual observations and task descriptions, and an action decoder maps
these representations to continuous actions. Diffusion models have been widely
adopted as action decoders due to their ability to model complex, multimodal
action distributions. However, they require multiple iterative denoising steps
at inference time or downstream techniques to speed up sampling, limiting their
practicality in real-world settings where high-frequency control is crucial. In
this work, we present NinA (Normalizing Flows in Action), a fast and expressive
alter- native to diffusion-based decoders for VLAs. NinA replaces the diffusion
action decoder with a Normalizing Flow (NF) that enables one-shot sampling
through an invertible transformation, significantly reducing inference time. We
integrate NinA into the FLOWER VLA architecture and fine-tune on the LIBERO
benchmark. Our experiments show that NinA matches the performance of its
diffusion-based counterpart under the same training regime, while achieving
substantially faster inference. These results suggest that NinA offers a
promising path toward efficient, high-frequency VLA control without
compromising performance.

</details>


### [19] [RF-PGS: Fully-structured Spatial Wireless Channel Representation with Planar Gaussian Splatting](https://arxiv.org/abs/2508.16849)
*Lihao Zhang,Zongtan Li,Haijian Sun*

Main category: cs.CV

TL;DR: RF-PGS是一种新型框架，通过稀疏路径损耗谱重建高保真无线电传播路径，显著提升了6G空间信道状态信息的建模效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 6G时代对大规模天线阵列和高精度空间信道状态信息的需求，传统方法在空间分辨率、效率和可扩展性方面存在挑战。

Method: RF-PGS采用Planar Gaussians作为几何基元，分两阶段训练：几何训练阶段实现密集、表面对齐的场景重建；RF训练阶段通过全结构化的无线电辐射和定制多视图损失建模无线电传播行为。

Result: 相比现有辐射场方法，RF-PGS显著提高了重建精度，降低了训练成本，并实现了无线信道的高效表示。

Conclusion: RF-PGS为可扩展的6G空间信道状态信息建模提供了实用解决方案。

Abstract: In the 6G era, the demand for higher system throughput and the implementation
of emerging 6G technologies require large-scale antenna arrays and accurate
spatial channel state information (Spatial-CSI). Traditional channel modeling
approaches, such as empirical models, ray tracing, and measurement-based
methods, face challenges in spatial resolution, efficiency, and scalability.
Radiance field-based methods have emerged as promising alternatives but still
suffer from geometric inaccuracy and costly supervision. This paper proposes
RF-PGS, a novel framework that reconstructs high-fidelity radio propagation
paths from only sparse path loss spectra. By introducing Planar Gaussians as
geometry primitives with certain RF-specific optimizations, RF-PGS achieves
dense, surface-aligned scene reconstruction in the first geometry training
stage. In the subsequent Radio Frequency (RF) training stage, the proposed
fully-structured radio radiance, combined with a tailored multi-view loss,
accurately models radio propagation behavior. Compared to prior radiance field
methods, RF-PGS significantly improves reconstruction accuracy, reduces
training costs, and enables efficient representation of wireless channels,
offering a practical solution for scalable 6G Spatial-CSI modeling.

</details>


### [20] [Gaussian Primitive Optimized Deformable Retinal Image Registration](https://arxiv.org/abs/2508.16852)
*Xin Tian,Jiazheng Wang,Yuxi Zhang,Xiang Chen,Renjiu Hu,Gaolei Li,Min Liu,Hang Zhang*

Main category: cs.CV

TL;DR: GPO是一种新型的可变形视网膜图像配准框架，通过高斯基元优化和结构化消息传递解决梯度信号不足的问题。


<details>
  <summary>Details</summary>
Motivation: 视网膜图像配准因大范围同质区域和稀疏但关键的血管特征导致梯度信号不足，传统学习方法难以应对。

Method: GPO通过提取关键点作为控制节点，建模为高斯基元，利用KNN高斯插值传播位移信号，构建全局一致的位移场。

Result: 在FIRE数据集上，GPO将目标配准误差从6.2px降至2.4px，AUC从0.770提升至0.938。

Conclusion: GPO通过优化梯度流和局部细节处理，显著提升了视网膜图像配准的精度和性能。

Abstract: Deformable retinal image registration is notoriously difficult due to large
homogeneous regions and sparse but critical vascular features, which cause
limited gradient signals in standard learning-based frameworks. In this paper,
we introduce Gaussian Primitive Optimization (GPO), a novel iterative framework
that performs structured message passing to overcome these challenges. After an
initial coarse alignment, we extract keypoints at salient anatomical structures
(e.g., major vessels) to serve as a minimal set of descriptor-based control
nodes (DCN). Each node is modelled as a Gaussian primitive with trainable
position, displacement, and radius, thus adapting its spatial influence to
local deformation scales. A K-Nearest Neighbors (KNN) Gaussian interpolation
then blends and propagates displacement signals from these information-rich
nodes to construct a globally coherent displacement field; focusing
interpolation on the top (K) neighbors reduces computational overhead while
preserving local detail. By strategically anchoring nodes in high-gradient
regions, GPO ensures robust gradient flow, mitigating vanishing gradient signal
in textureless areas. The framework is optimized end-to-end via a multi-term
loss that enforces both keypoint consistency and intensity alignment.
Experiments on the FIRE dataset show that GPO reduces the target registration
error from 6.2\,px to ~2.4\,px and increases the AUC at 25\,px from 0.770 to
0.938, substantially outperforming existing methods. The source code can be
accessed via https://github.com/xintian-99/GPOreg.

</details>


### [21] [Beyond Emotion Recognition: A Multi-Turn Multimodal Emotion Understanding and Reasoning Benchmark](https://arxiv.org/abs/2508.16859)
*Jinpeng Hu,Hongchang Shi,Chongyuan Dai,Zhuo Li,Peipei Song,Meng Wang*

Main category: cs.CV

TL;DR: 论文提出了一个多轮多模态情感理解与推理（MTMEUR）基准，包含1,451个现实场景视频和5,101个渐进问题，并提出了一个多智能体框架以提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注情感识别能力，忽略了情感推理的重要性，而情感推理对提升人机交互的自然性和有效性至关重要。

Method: 引入MTMEUR基准，并提出多智能体框架，每个智能体专注于特定方面（如背景、角色动态、事件细节）以增强推理能力。

Result: 实验表明，现有MLLMs在该任务上表现不佳，多智能体框架显示出潜力。

Conclusion: MTMEUR基准和多智能体框架为情感推理研究提供了新方向，现有模型仍需改进。

Abstract: Multimodal large language models (MLLMs) have been widely applied across
various fields due to their powerful perceptual and reasoning capabilities. In
the realm of psychology, these models hold promise for a deeper understanding
of human emotions and behaviors. However, recent research primarily focuses on
enhancing their emotion recognition abilities, leaving the substantial
potential in emotion reasoning, which is crucial for improving the naturalness
and effectiveness of human-machine interactions. Therefore, in this paper, we
introduce a multi-turn multimodal emotion understanding and reasoning (MTMEUR)
benchmark, which encompasses 1,451 video data from real-life scenarios, along
with 5,101 progressive questions. These questions cover various aspects,
including emotion recognition, potential causes of emotions, future action
prediction, etc. Besides, we propose a multi-agent framework, where each agent
specializes in a specific aspect, such as background context, character
dynamics, and event details, to improve the system's reasoning capabilities.
Furthermore, we conduct experiments with existing MLLMs and our agent-based
method on the proposed benchmark, revealing that most models face significant
challenges with this task.

</details>


### [22] [Delta-SVD: Efficient Compression for Personalized Text-to-Image Models](https://arxiv.org/abs/2508.16863)
*Tangyuan Zhang,Shangyu Chen,Qixiang Chen,Jianfei Cai*

Main category: cs.CV

TL;DR: Delta-SVD是一种无需训练的后处理压缩方法，用于减少DreamBooth微调后的存储开销，通过低秩分解和能量截断策略实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: DreamBooth等个性化文本到图像模型需要微调大规模扩散模型，导致存储开销大，难以维护多个主题特定模型。

Method: Delta-SVD利用奇异值分解（SVD）对权重增量进行低秩分解，并通过能量截断策略平衡压缩效率和重建保真度。

Result: 实验表明，Delta-SVD在CLIP分数、SSIM和FID等指标上几乎无损地实现了显著压缩。

Conclusion: Delta-SVD为个性化扩散模型提供了可扩展且高效的部署方案，适用于需要大规模存储和部署定制模型的实际应用。

Abstract: Personalized text-to-image models such as DreamBooth require fine-tuning
large-scale diffusion backbones, resulting in significant storage overhead when
maintaining many subject-specific models. We present Delta-SVD, a post-hoc,
training-free compression method that targets the parameter weights update
induced by DreamBooth fine-tuning. Our key observation is that these delta
weights exhibit strong low-rank structure due to the sparse and localized
nature of personalization. Delta-SVD first applies Singular Value Decomposition
(SVD) to factorize the weight deltas, followed by an energy-based rank
truncation strategy to balance compression efficiency and reconstruction
fidelity. The resulting compressed models are fully plug-and-play and can be
re-constructed on-the-fly during inference. Notably, the proposed approach is
simple, efficient, and preserves the original model architecture. Experiments
on a multiple subject dataset demonstrate that Delta-SVD achieves substantial
compression with negligible loss in generation quality measured by CLIP score,
SSIM and FID. Our method enables scalable and efficient deployment of
personalized diffusion models, making it a practical solution for real-world
applications that require storing and deploying large-scale subject
customizations.

</details>


### [23] [Do Multimodal LLMs See Sentiment?](https://arxiv.org/abs/2508.16873)
*Neemias B. da Silva,John Harrison,Rodrigo Minetto,Myriam R. Delgado,Bogdan T. Nassu,Thiago H. Silva*

Main category: cs.CV

TL;DR: 该论文提出了一种名为MLLMsent的框架，通过多模态大语言模型（MLLMs）从三个角度研究情感推理能力，并在实验中取得了优于现有方法的结果。


<details>
  <summary>Details</summary>
Motivation: 研究视觉内容如何传达情感，尤其是在社交媒体主导的在线互动时代，这一问题具有重要意义但具有挑战性。

Method: 提出MLLMsent框架，包括直接情感分类、结合预训练LLMs的情感分析，以及对情感标记图像描述的微调。

Result: 实验表明，该方法在多个基准测试中显著优于现有方法，最高提升达64.8%，并在跨数据集测试中表现优异。

Conclusion: 该研究展示了视觉推理方案在情感计算中的潜力，并为未来研究设立了新基准。

Abstract: Understanding how visual content communicates sentiment is critical in an era
where online interaction is increasingly dominated by this kind of media on
social platforms. However, this remains a challenging problem, as sentiment
perception is closely tied to complex, scene-level semantics. In this paper, we
propose an original framework, MLLMsent, to investigate the sentiment reasoning
capabilities of Multimodal Large Language Models (MLLMs) through three
perspectives: (1) using those MLLMs for direct sentiment classification from
images; (2) associating them with pre-trained LLMs for sentiment analysis on
automatically generated image descriptions; and (3) fine-tuning the LLMs on
sentiment-labeled image descriptions. Experiments on a recent and established
benchmark demonstrate that our proposal, particularly the fine-tuned approach,
achieves state-of-the-art results outperforming Lexicon-, CNN-, and
Transformer-based baselines by up to 30.9%, 64.8%, and 42.4%, respectively,
across different levels of evaluators' agreement and sentiment polarity
categories. Remarkably, in a cross-dataset test, without any training on these
new data, our model still outperforms, by up to 8.26%, the best runner-up,
which has been trained directly on them. These results highlight the potential
of the proposed visual reasoning scheme for advancing affective computing,
while also establishing new benchmarks for future research.

</details>


### [24] [AWM-Fuse: Multi-Modality Image Fusion for Adverse Weather via Global and Local Text Perception](https://arxiv.org/abs/2508.16881)
*Xilai Li,Huichun Liu,Xiaosong Li,Tao Ye,Zhenyu Kuang,Huafeng Li*

Main category: cs.CV

TL;DR: AWM-Fuse是一种多模态图像融合方法，通过全局和局部文本感知处理恶劣天气条件下的图像退化问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决恶劣天气导致的视觉信息丢失问题，并提升语义感知能力。

Method: 结合BLIP和ChatGPT生成的文本描述，通过全局和局部模块提取特征并约束图像生成。

Result: 在复杂天气条件和下游任务中表现优于现有方法。

Conclusion: AWM-Fuse通过文本感知有效提升了图像融合的语义对齐和细节捕捉能力。

Abstract: Multi-modality image fusion (MMIF) in adverse weather aims to address the
loss of visual information caused by weather-related degradations, providing
clearer scene representations. Although less studies have attempted to
incorporate textual information to improve semantic perception, they often lack
effective categorization and thorough analysis of textual content. In response,
we propose AWM-Fuse, a novel fusion method for adverse weather conditions,
designed to handle multiple degradations through global and local text
perception within a unified, shared weight architecture. In particular, a
global feature perception module leverages BLIP-produced captions to extract
overall scene features and identify primary degradation types, thus promoting
generalization across various adverse weather conditions. Complementing this,
the local module employs detailed scene descriptions produced by ChatGPT to
concentrate on specific degradation effects through concrete textual cues,
thereby capturing finer details. Furthermore, textual descriptions are used to
constrain the generation of fusion images, effectively steering the network
learning process toward better alignment with real semantic labels, thereby
promoting the learning of more meaningful visual features. Extensive
experiments demonstrate that AWM-Fuse outperforms current state-of-the-art
methods in complex weather conditions and downstream tasks. Our code is
available at https://github.com/Feecuin/AWM-Fuse.

</details>


### [25] [A Lightweight Convolution and Vision Transformer integrated model with Multi-scale Self-attention Mechanism](https://arxiv.org/abs/2508.16884)
*Yi Zhang,Lingxiao Wei,Bowei Zhang,Ziwei Liu,Kai Yi,Shu Hu*

Main category: cs.CV

TL;DR: SAEViT是一种轻量级ViT模型，通过稀疏注意力模块和通道交互前馈网络提升计算效率，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: ViT在计算成本和局部特征建模能力上存在不足，限制了其实际应用。

Method: 提出SAA模块进行自适应稀疏采样，CIFFN层增强通道间信息交换，并结合DWSConv块强化卷积特征。

Result: 在ImageNet-1K上达到76.3%和79.6%的Top-1准确率，仅需0.8和1.3 GFLOPs。

Conclusion: SAEViT为轻量级视觉任务提供了高效解决方案。

Abstract: Vision Transformer (ViT) has prevailed in computer vision tasks due to its
strong long-range dependency modelling ability. However, its large model size
with high computational cost and weak local feature modeling ability hinder its
application in real scenarios. To balance computation efficiency and
performance, we propose SAEViT (Sparse-Attention-Efficient-ViT), a lightweight
ViT based model with convolution blocks, in this paper to achieve efficient
downstream vision tasks. Specifically, SAEViT introduces a Sparsely Aggregated
Attention (SAA) module that performs adaptive sparse sampling based on image
redundancy and recovers the feature map via deconvolution operation, which
significantly reduces the computational complexity of attention operations. In
addition, a Channel-Interactive Feed-Forward Network (CIFFN) layer is developed
to enhance inter-channel information exchange through feature decomposition and
redistribution, mitigating redundancy in traditional feed-forward networks
(FNN). Finally, a hierarchical pyramid structure with embedded depth-wise
separable convolutional blocks (DWSConv) is devised to further strengthen
convolutional features. Extensive experiments on mainstream datasets show that
SAEViT achieves Top-1 accuracies of 76.3\% and 79.6\% on the ImageNet-1K
classification task with only 0.8 GFLOPs and 1.3 GFLOPs, respectively,
demonstrating a lightweight solution for various fundamental vision tasks.

</details>


### [26] [MDIQA: Unified Image Quality Assessment for Multi-dimensional Evaluation and Restoration](https://arxiv.org/abs/2508.16887)
*Shunyu Yao,Ming Liu,Zhilu Zhang,Zhaolin Wan,Zhilong Ji,Jinfeng Bai,Wangmeng Zuo*

Main category: cs.CV

TL;DR: 提出了一种多维图像质量评估（MDIQA）框架，通过多维度建模提升图像质量评估的准确性，并灵活应用于图像修复任务。


<details>
  <summary>Details</summary>
Motivation: 现有图像质量评估方法过于关注整体评分，忽略了人类从多个维度评估图像质量的事实。

Method: MDIQA框架通过技术（5个）和美学（4个）维度建模，分分支训练后融合特征生成最终评分。

Result: 实验表明MDIQA性能优越，并能灵活应用于图像修复任务。

Conclusion: MDIQA框架有效提升了图像质量评估的准确性，并支持个性化图像修复。

Abstract: Recent advancements in image quality assessment (IQA), driven by
sophisticated deep neural network designs, have significantly improved the
ability to approach human perceptions. However, most existing methods are
obsessed with fitting the overall score, neglecting the fact that humans
typically evaluate image quality from different dimensions before arriving at
an overall quality assessment. To overcome this problem, we propose a
multi-dimensional image quality assessment (MDIQA) framework. Specifically, we
model image quality across various perceptual dimensions, including five
technical and four aesthetic dimensions, to capture the multifaceted nature of
human visual perception within distinct branches. Each branch of our MDIQA is
initially trained under the guidance of a separate dimension, and the
respective features are then amalgamated to generate the final IQA score.
Additionally, when the MDIQA model is ready, we can deploy it for a flexible
training of image restoration (IR) models, enabling the restoration results to
better align with varying user preferences through the adjustment of perceptual
dimension weights. Extensive experiments demonstrate that our MDIQA achieves
superior performance and can be effectively and flexibly applied to image
restoration tasks. The code is available: https://github.com/YaoShunyu19/MDIQA.

</details>


### [27] [Structural Energy-Guided Sampling for View-Consistent Text-to-3D](https://arxiv.org/abs/2508.16917)
*Qing Zhang,Jinguang Tong,Jie Hong,Jing Zhang,Xuesong Li*

Main category: cs.CV

TL;DR: SEGS是一种无需训练、即插即用的框架，通过结构能量引导采样解决文本到3D生成中的Janus问题，提升多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 文本到3D生成中存在Janus问题，即物体从正面看正确但从其他角度出现几何重复或扭曲，原因是2D扩散先验中的视角偏差影响了3D优化。

Method: 提出SEGS框架，在采样时通过PCA子空间中的结构能量梯度引导去噪轨迹，增强多视角一致性。

Result: SEGS显著减少了Janus伪影，提升了几何对齐和视角一致性，且无需重新训练或修改权重。

Conclusion: SEGS是一种高效且无需训练的方法，有效解决了文本到3D生成中的视角一致性问题。

Abstract: Text-to-3D generation often suffers from the Janus problem, where objects
look correct from the front but collapse into duplicated or distorted geometry
from other angles. We attribute this failure to viewpoint bias in 2D diffusion
priors, which propagates into 3D optimization. To address this, we propose
Structural Energy-Guided Sampling (SEGS), a training-free, plug-and-play
framework that enforces multi-view consistency entirely at sampling time. SEGS
defines a structural energy in a PCA subspace of intermediate U-Net features
and injects its gradients into the denoising trajectory, steering geometry
toward the intended viewpoint while preserving appearance fidelity. Integrated
seamlessly into SDS/VSD pipelines, SEGS significantly reduces Janus artifacts,
achieving improved geometric alignment and viewpoint consistency without
retraining or weight modification.

</details>


### [28] [MSPCaps: A Multi-Scale Patchify Capsule Network with Cross-Agreement Routing for Visual Recognition](https://arxiv.org/abs/2508.16922)
*Yudong Hu,Yueju Han,Rui Sun,Jinke Ren*

Main category: cs.CV

TL;DR: MSPCaps是一种新型胶囊网络架构，通过多尺度特征学习和高效路由机制提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有CapsNet依赖单一高维特征图，忽略了多尺度特征的互补信息，且传统特征融合策略难以协调多尺度差异。

Method: MSPCaps包含多尺度ResNet骨干、Patchify胶囊层和跨协议路由块，实现多尺度特征提取与自适应路由。

Result: MSPCaps在分类准确率上显著优于基线方法，且具有高可扩展性和鲁棒性。

Conclusion: MSPCaps通过多尺度特征和高效路由机制，推动了特征表示学习的进步。

Abstract: Capsule Network (CapsNet) has demonstrated significant potential in visual
recognition by capturing spatial relationships and part-whole hierarchies for
learning equivariant feature representations. However, existing CapsNet and
variants often rely on a single high-level feature map, overlooking the rich
complementary information from multi-scale features. Furthermore, conventional
feature fusion strategies (e.g., addition and concatenation) struggle to
reconcile multi-scale feature discrepancies, leading to suboptimal
classification performance. To address these limitations, we propose the
Multi-Scale Patchify Capsule Network (MSPCaps), a novel architecture that
integrates multi-scale feature learning and efficient capsule routing.
Specifically, MSPCaps consists of three key components: a Multi-Scale ResNet
Backbone (MSRB), a Patchify Capsule Layer (PatchifyCaps), and Cross-Agreement
Routing (CAR) blocks. First, the MSRB extracts diverse multi-scale feature
representations from input images, preserving both fine-grained details and
global contextual information. Second, the PatchifyCaps partitions these
multi-scale features into primary capsules using a uniform patch size,
equipping the model with the ability to learn from diverse receptive fields.
Finally, the CAR block adaptively routes the multi-scale capsules by
identifying cross-scale prediction pairs with maximum agreement. Unlike the
simple concatenation of multiple self-routing blocks, CAR ensures that only the
most coherent capsules contribute to the final voting. Our proposed MSPCaps
achieves remarkable scalability and superior robustness, consistently
surpassing multiple baseline methods in terms of classification accuracy, with
configurations ranging from a highly efficient Tiny model (344.3K parameters)
to a powerful Large model (10.9M parameters), highlighting its potential in
advancing feature representation learning.

</details>


### [29] [LGE-Guided Cross-Modality Contrastive Learning for Gadolinium-Free Cardiomyopathy Screening in Cine CMR](https://arxiv.org/abs/2508.16927)
*Siqing Yuan,Yulin Wang,Zirui Cao,Yueyan Wang,Zehao Weng,Hui Wang,Lei Xu,Zixian Chen,Lei Chen,Zhong Xue,Dinggang Shen*

Main category: cs.CV

TL;DR: CC-CMR框架通过对比学习和跨模态对齐，实现无钆剂的心肌病筛查，准确率达94.3%，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 心肌病是心衰和猝死的主要原因，现有CMR依赖钆剂且解读复杂，难以大规模应用。

Method: 通过对比学习和跨模态对齐，将LGE序列的病理信息编码到cine CMR中，结合特征交互模块和自适应训练机制。

Result: 在多中心231例数据上，准确率达0.943（95% CI: 0.886-0.986），优于现有方法4.3%。

Conclusion: CC-CMR无需钆剂，临床适用性强，适合广泛人群和医疗环境。

Abstract: Cardiomyopathy, a principal contributor to heart failure and sudden cardiac
mortality, demands precise early screening. Cardiac Magnetic Resonance (CMR),
recognized as the diagnostic 'gold standard' through multiparametric protocols,
holds the potential to serve as an accurate screening tool. However, its
reliance on gadolinium contrast and labor-intensive interpretation hinders
population-scale deployment. We propose CC-CMR, a Contrastive Learning and
Cross-Modal alignment framework for gadolinium-free cardiomyopathy screening
using cine CMR sequences. By aligning the latent spaces of cine CMR and Late
Gadolinium Enhancement (LGE) sequences, our model encodes fibrosis-specific
pathology into cine CMR embeddings. A Feature Interaction Module concurrently
optimizes diagnostic precision and cross-modal feature congruence, augmented by
an uncertainty-guided adaptive training mechanism that dynamically calibrates
task-specific objectives to ensure model generalizability. Evaluated on
multi-center data from 231 subjects, CC-CMR achieves accuracy of 0.943 (95% CI:
0.886-0.986), outperforming state-of-the-art cine-CMR-only models by 4.3% while
eliminating gadolinium dependency, demonstrating its clinical viability for
wide range of populations and healthcare environments.

</details>


### [30] [Align 3D Representation and Text Embedding for 3D Content Personalization](https://arxiv.org/abs/2508.16932)
*Qi Song,Ziyuan Luo,Ka Chun Cheung,Simon See,Renjie Wan*

Main category: cs.CV

TL;DR: Invert3D是一种新型框架，通过将3D内容与文本嵌入空间对齐，实现高效的自然语言驱动的3D内容个性化，避免了昂贵的重新训练过程。


<details>
  <summary>Details</summary>
Motivation: 当前3D个性化方法依赖计算成本高的知识蒸馏，而2D图像个性化技术无法直接应用于3D内容。Invert3D旨在填补这一空白。

Method: 提出相机条件的3D到文本逆机制，将3D内容投影到与文本嵌入对齐的3D嵌入空间。

Result: 实验表明，Invert3D能够高效实现3D内容的个性化。

Conclusion: Invert3D为3D内容个性化提供了一种高效且无需重新训练的新方法。

Abstract: Recent advances in NeRF and 3DGS have significantly enhanced the efficiency
and quality of 3D content synthesis. However, efficient personalization of
generated 3D content remains a critical challenge. Current 3D personalization
approaches predominantly rely on knowledge distillation-based methods, which
require computationally expensive retraining procedures. To address this
challenge, we propose \textbf{Invert3D}, a novel framework for convenient 3D
content personalization. Nowadays, vision-language models such as CLIP enable
direct image personalization through aligned vision-text embedding spaces.
However, the inherent structural differences between 3D content and 2D images
preclude direct application of these techniques to 3D personalization. Our
approach bridges this gap by establishing alignment between 3D representations
and text embedding spaces. Specifically, we develop a camera-conditioned
3D-to-text inverse mechanism that projects 3D contents into a 3D embedding
aligned with text embeddings. This alignment enables efficient manipulation and
personalization of 3D content through natural language prompts, eliminating the
need for computationally retraining procedures. Extensive experiments
demonstrate that Invert3D achieves effective personalization of 3D content. Our
work is available at: https://github.com/qsong2001/Invert3D.

</details>


### [31] [Addressing Annotation Scarcity in Hyperspectral Brain Image Segmentation with Unsupervised Domain Adaptation](https://arxiv.org/abs/2508.16934)
*Tim Mach,Daniel Rueckert,Alex Berger,Laurin Lux,Ivan Ezhov*

Main category: cs.CV

TL;DR: 提出了一种用于分割高光谱脑图像中脑血管的新型深度学习框架，解决了标签稀缺的关键挑战。


<details>
  <summary>Details</summary>
Motivation: 标签稀缺问题阻碍了传统的监督训练方法，因此需要一种新的方法来解决这一挑战。

Method: 采用无监督域适应方法，结合少量专家标注的真实数据和未标注数据进行训练。

Result: 定量和定性评估表明，该方法显著优于现有最先进方法。

Conclusion: 证明了域适应在标签稀缺的生物医学成像任务中的有效性。

Abstract: This work presents a novel deep learning framework for segmenting cerebral
vasculature in hyperspectral brain images. We address the critical challenge of
severe label scarcity, which impedes conventional supervised training. Our
approach utilizes a novel unsupervised domain adaptation methodology, using a
small, expert-annotated ground truth alongside unlabeled data. Quantitative and
qualitative evaluations confirm that our method significantly outperforms
existing state-of-the-art approaches, demonstrating the efficacy of domain
adaptation for label-scarce biomedical imaging tasks.

</details>


### [32] [NAT: Learning to Attack Neurons for Enhanced Adversarial Transferability](https://arxiv.org/abs/2508.16937)
*Krishna Kanth Nakka,Alexandre Alahi*

Main category: cs.CV

TL;DR: NAT是一种针对特定神经元生成对抗扰动的方法，通过优化单个神经元而非整个嵌入层，显著提高了对抗样本的迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常优化整个嵌入层，导致少数神经元主导攻击效果，而其他神经元影响较小。NAT旨在通过针对单个神经元优化，更有效地破坏神经网络的核心单元。

Method: NAT通过训练生成器，针对特定神经元生成对抗扰动，而非整个嵌入层。实验覆盖了41个ImageNet模型和9个细粒度模型。

Result: NAT在跨模型和跨域设置中分别超过基线14%和4%，且在仅10次查询内实现了高欺骗率。

Conclusion: NAT通过神经元级优化显著提升了对抗样本的迁移性和攻击效果，为对抗攻击提供了新思路。

Abstract: The generation of transferable adversarial perturbations typically involves
training a generator to maximize embedding separation between clean and
adversarial images at a single mid-layer of a source model. In this work, we
build on this approach and introduce Neuron Attack for Transferability (NAT), a
method designed to target specific neuron within the embedding. Our approach is
motivated by the observation that previous layer-level optimizations often
disproportionately focus on a few neurons representing similar concepts,
leaving other neurons within the attacked layer minimally affected. NAT shifts
the focus from embedding-level separation to a more fundamental,
neuron-specific approach. We find that targeting individual neurons effectively
disrupts the core units of the neural network, providing a common basis for
transferability across different models. Through extensive experiments on 41
diverse ImageNet models and 9 fine-grained models, NAT achieves fooling rates
that surpass existing baselines by over 14\% in cross-model and 4\% in
cross-domain settings. Furthermore, by leveraging the complementary attacking
capabilities of the trained generators, we achieve impressive fooling rates
within just 10 queries. Our code is available at:
https://krishnakanthnakka.github.io/NAT/

</details>


### [33] [HieroAction: Hierarchically Guided VLM for Fine-Grained Action Analysis](https://arxiv.org/abs/2508.16942)
*Junhao Wu,Xiuer Gu,Zhiying Li,Yeying Jin,Yunfeng Diao,Zhiyu Li,Zhenbo Song,Xiaomei Zhang,Zhaoxin Fan*

Main category: cs.CV

TL;DR: HieroAction是一个视觉语言模型，通过逐步动作推理和分层策略学习，提供准确且可解释的人类动作评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅提供最终评分而缺乏解释，限制了实际应用。HieroAction旨在填补这一空白。

Method: 结合逐步动作推理（分步评估动作）和分层策略学习（优化子动作与整体质量的匹配）。

Result: 在多个基准数据集上表现优异，证明了其准确性和可解释性。

Conclusion: HieroAction通过结构化评估和强化学习，实现了高质量的动作评估。

Abstract: Evaluating human actions with clear and detailed feedback is important in
areas such as sports, healthcare, and robotics, where decisions rely not only
on final outcomes but also on interpretable reasoning. However, most existing
methods provide only a final score without explanation or detailed analysis,
limiting their practical applicability. To address this, we introduce
HieroAction, a vision-language model that delivers accurate and structured
assessments of human actions. HieroAction builds on two key ideas: (1) Stepwise
Action Reasoning, a tailored chain of thought process designed specifically for
action assessment, which guides the model to evaluate actions step by step,
from overall recognition through sub action analysis to final scoring, thus
enhancing interpretability and structured understanding; and (2) Hierarchical
Policy Learning, a reinforcement learning strategy that enables the model to
learn fine grained sub action dynamics and align them with high level action
quality, thereby improving scoring precision. The reasoning pathway structures
the evaluation process, while policy learning refines each stage through reward
based optimization. Their integration ensures accurate and interpretable
assessments, as demonstrated by superior performance across multiple benchmark
datasets. Code will be released upon acceptance.

</details>


### [34] [RPD-Diff: Region-Adaptive Physics-Guided Diffusion Model for Visibility Enhancement under Dense and Non-Uniform Haze](https://arxiv.org/abs/2508.16956)
*Ruicheng Zhang,Puxin Yan,Zeyu Zhang,Yicheng Chang,Hongyi Chen,Zhi Jin*

Main category: cs.CV

TL;DR: RPD-Diff是一种区域自适应物理引导的去雾扩散模型，通过物理引导的中间状态目标策略和雾感知去噪时间步预测器，有效处理密集和非均匀雾霾场景。


<details>
  <summary>Details</summary>
Motivation: 传统去雾方法在密集和非均匀雾霾条件下表现不佳，信息退化和空间异质性导致恢复效果不理想。

Method: 提出RPD-Diff模型，结合物理先验和动态调整的去噪时间步，优化扩散马尔可夫链。

Result: 在四个真实数据集上验证，RPD-Diff在复杂雾霾场景中达到最先进性能。

Conclusion: RPD-Diff通过物理引导和动态调整策略，显著提升了去雾效果，适用于复杂雾霾条件。

Abstract: Single-image dehazing under dense and non-uniform haze conditions remains
challenging due to severe information degradation and spatial heterogeneity.
Traditional diffusion-based dehazing methods struggle with insufficient
generation conditioning and lack of adaptability to spatially varying haze
distributions, which leads to suboptimal restoration. To address these
limitations, we propose RPD-Diff, a Region-adaptive Physics-guided Dehazing
Diffusion Model for robust visibility enhancement in complex haze scenarios.
RPD-Diff introduces a Physics-guided Intermediate State Targeting (PIST)
strategy, which leverages physical priors to reformulate the diffusion Markov
chain by generation target transitions, mitigating the issue of insufficient
conditioning in dense haze scenarios. Additionally, the Haze-Aware Denoising
Timestep Predictor (HADTP) dynamically adjusts patch-specific denoising
timesteps employing a transmission map cross-attention mechanism, adeptly
managing non-uniform haze distributions. Extensive experiments across four
real-world datasets demonstrate that RPD-Diff achieves state-of-the-art
performance in challenging dense and non-uniform haze scenarios, delivering
high-quality, haze-free images with superior detail clarity and color fidelity.

</details>


### [35] [Local Information Matters: A Rethink of Crowd Counting](https://arxiv.org/abs/2508.16970)
*Tianhang Pan,Xiuyi Jia*

Main category: cs.CV

TL;DR: 论文提出了一种新的拥挤人群计数模型设计原则，强调局部建模能力，并设计了LIMM模型，通过窗口分区和对比学习策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 重新思考拥挤人群计数任务中个体（人头）通常只占图像极小部分的特点，现有方法未关注此特性，导致模型设计不合理。

Method: 提出LIMM模型，采用窗口分区设计和窗口对比学习策略，增强局部建模能力，并加入全局注意力模块处理大尺寸个体。

Result: 在多个公开数据集上实验表明，模型在局部建模能力上有显著提升（如JHU-Crowd++高密度子集上MAE提升8.7%），同时保持对大尺寸个体的计数能力，达到SOTA性能。

Conclusion: LIMM模型通过强调局部建模能力，显著提升了拥挤人群计数的性能，验证了新设计原则的有效性。

Abstract: The motivation of this paper originates from rethinking an essential
characteristic of crowd counting: individuals (heads of humans) in the crowd
counting task typically occupy a very small portion of the image. This
characteristic has never been the focus of existing works: they typically use
the same backbone as other visual tasks and pursue a large receptive field.
This drives us to propose a new model design principle of crowd counting:
emphasizing local modeling capability of the model. We follow the principle and
design a crowd counting model named Local Information Matters Model (LIMM). The
main innovation lies in two strategies: a window partitioning design that
applies grid windows to the model input, and a window-wise contrastive learning
design to enhance the model's ability to distinguish between local density
levels. Moreover, a global attention module is applied to the end of the model
to handle the occasionally occurring large-sized individuals. Extensive
experiments on multiple public datasets illustrate that the proposed model
shows a significant improvement in local modeling capability (8.7\% in MAE on
the JHU-Crowd++ high-density subset for example), without compromising its
ability to count large-sized ones, which achieves state-of-the-art performance.
Code is available at: https://github.com/tianhangpan/LIMM.

</details>


### [36] [Robust Diagram Reasoning: A Framework for Enhancing LVLM Performance on Visually Perturbed Scientific Diagrams](https://arxiv.org/abs/2508.16972)
*Minghao Zhou,Rafael Souza,Yaqian Hu,Luming Che*

Main category: cs.CV

TL;DR: 论文提出了RDR框架，通过AMCV机制和两个新指标（PRS和VDC）来增强和评估LVLMs在视觉退化科学图表上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LVLMs在处理科学图表时缺乏对常见视觉干扰（如噪声、模糊和遮挡）的鲁棒性，现有基准测试未充分评估这一能力。

Method: RDR框架采用AMCV机制，生成多个扰动版本的图表，进行并行推理，并通过一致性自校正循环提升性能。

Result: 实验表明，即使是GPT-4V等先进LVLMs，在扰动输入下性能显著下降（Clean Accuracy 85.2% vs. PRS 72.1%）。

Conclusion: RDR框架和SciDiagram-Robust数据集填补了LVLMs在科学图表鲁棒性评估上的空白，为实际应用提供了重要工具。

Abstract: Large Language Models (LLMs) and their multimodal variants (LVLMs) hold
immense promise for scientific and engineering applications, particularly in
processing visual information like scientific diagrams. However, their
practical deployment is hindered by a critical lack of robustness to common
visual perturbations such as noise, blur, and occlusions, which are prevalent
in real-world scientific documents. Existing evaluation benchmarks largely
overlook this challenge, leaving the robust reasoning capabilities of LVLMs on
visually degraded scientific diagrams underexplored. To address this, we
introduce the Robust Diagram Reasoning (RDR) framework, a novel approach
designed to enhance and rigorously evaluate LVLMs' performance under such
conditions. At its core, RDR employs an Adaptive Multi-View & Consistency
Verification (AMCV) mechanism, which involves generating multiple perturbed
versions of a diagram, performing parallel inference, and then applying a
consistency-based self-correction loop. We also propose two new metrics,
Perturbation Robustness Score (PRS) and Visual Degradation Consistency (VDC),
to quantify robustness. Furthermore, we construct SciDiagram-Robust, the first
large-scale scientific diagram question-answering dataset specifically
augmented with diverse, programmatically generated visual perturbations. Our
extensive experiments demonstrate that even state-of-the-art closed-source
LVLMs like GPT-4V exhibit significant performance degradation when faced with
perturbed inputs (Clean Accuracy 85.2% vs. PRS 72.1%).

</details>


### [37] [Balanced Sharpness-Aware Minimization for Imbalanced Regression](https://arxiv.org/abs/2508.16973)
*Yahao Liu,Qin Wang,Lixin Duan,Wen Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为BSAM的方法，通过平衡损失锐度来解决回归任务中的不平衡分布问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现实数据通常呈现不平衡分布，导致回归模型在罕见目标值上表现不佳。论文将其重新定义为不平衡泛化问题。

Method: 提出BSAM方法，结合传统锐度感知最小化和目标重加权策略，以均匀化观察空间的泛化能力。

Result: 在年龄和深度估计等任务中，BSAM方法显著优于现有方法。

Conclusion: BSAM通过平衡泛化能力，有效解决了不平衡回归问题，具有理论和实验支持。

Abstract: Regression is fundamental in computer vision and is widely used in various
tasks including age estimation, depth estimation, target localization, \etc
However, real-world data often exhibits imbalanced distribution, making
regression models perform poorly especially for target values with rare
observations~(known as the imbalanced regression problem). In this paper, we
reframe imbalanced regression as an imbalanced generalization problem. To
tackle that, we look into the loss sharpness property for measuring the
generalization ability of regression models in the observation space. Namely,
given a certain perturbation on the model parameters, we check how model
performance changes according to the loss values of different target
observations. We propose a simple yet effective approach called Balanced
Sharpness-Aware Minimization~(BSAM) to enforce the uniform generalization
ability of regression models for the entire observation space. In particular,
we start from the traditional sharpness-aware minimization and then introduce a
novel targeted reweighting strategy to homogenize the generalization ability
across the observation space, which guarantees a theoretical generalization
bound. Extensive experiments on multiple vision regression tasks, including age
and depth estimation, demonstrate that our BSAM method consistently outperforms
existing approaches. The code is available
\href{https://github.com/manmanjun/BSAM_for_Imbalanced_Regression}{here}.

</details>


### [38] [Hierarchical Contextual Grounding LVLM: Enhancing Fine-Grained Visual-Language Understanding with Robust Grounding](https://arxiv.org/abs/2508.16974)
*Leilei Guo,Antonio Carlos Rivera,Peiyu Tang,Haoxuan Ren,Zheyu Song*

Main category: cs.CV

TL;DR: HCG-LVLM是一种新型视觉语言大模型，通过分层设计提升细粒度视觉语言理解能力，显著减少幻觉和推理错误。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言大模型在复杂场景中表现不足，如幻觉、推理错误和区域定位不精确，需要改进。

Method: 采用两层架构：全局上下文感知层和细粒度局部定位层，结合局部细节增强模块和语义一致性验证器。

Result: 在多个数据集上优于现有模型，如Flamingo、BLIP-2和MiniGPT-4，准确率更高且幻觉减少。

Conclusion: HCG-LVLM的分层设计有效提升了细粒度视觉语言理解和精确定位能力。

Abstract: Large Language Models (LLMs) and Vision-Language Large Models (LVLMs) have
achieved remarkable progress in natural language processing and multimodal
understanding. Despite their impressive generalization capabilities, current
LVLMs often exhibit insufficient robustness, proneness to hallucination, and
reasoning errors in complex real-world scenarios, particularly when precise
image region localization and fine-grained visual reasoning are required. To
address these limitations, we propose the Hierarchical Contextual Grounding
LVLM (HCG-LVLM), a novel architecture that mimics human coarse-to-fine
cognitive processing. HCG-LVLM employs a two-layered approach: a Global
Contextual Perception layer for initial broad understanding and a Fine-grained
Local Grounding layer. The latter incorporates a Local Detail Enhancement
Module to extract high-resolution features and a Semantic Consistency Validator
to ensure accurate, hallucination-free visual-language alignment. Through an
adaptive fusion mechanism, information from both layers is integrated for
robust and precise outputs. Extensive experiments on challenging datasets,
including GQA, A-OKVQA for fine-grained VQA, and RefCOCO/+/g for Referring
Expression Comprehension, demonstrate that HCG-LVLM consistently outperforms
state-of-the-art models such as Flamingo, BLIP-2, and MiniGPT-4. Our model
achieves superior accuracy and significantly reduces hallucination, validating
the effectiveness of its hierarchical design in enhancing fine-grained
visual-language understanding and precise grounding capabilities.

</details>


### [39] [Combating Digitally Altered Images: Deepfake Detection](https://arxiv.org/abs/2508.16975)
*Saksham Kumar,Rhythm Narang*

Main category: cs.CV

TL;DR: 提出了一种基于改进Vision Transformer（ViT）模型的Deepfake检测方法，通过数据增强和分层采样解决类别不平衡问题，在测试数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: Deepfake技术生成的超真实图像和视频对公众和相关机构构成挑战，需要一种可靠的检测方法。

Method: 使用改进的ViT模型，结合数据增强和分层采样技术，训练于OpenForensics数据集子集。

Result: 模型在测试数据集上表现出色，能够精确检测Deepfake图像。

Conclusion: 该方法在Deepfake检测领域达到了先进水平，具有实际应用潜力。

Abstract: The rise of Deepfake technology to generate hyper-realistic manipulated
images and videos poses a significant challenge to the public and relevant
authorities. This study presents a robust Deepfake detection based on a
modified Vision Transformer(ViT) model, trained to distinguish between real and
Deepfake images. The model has been trained on a subset of the OpenForensics
Dataset with multiple augmentation techniques to increase robustness for
diverse image manipulations. The class imbalance issues are handled by
oversampling and a train-validation split of the dataset in a stratified
manner. Performance is evaluated using the accuracy metric on the training and
testing datasets, followed by a prediction score on a random image of people,
irrespective of their realness. The model demonstrates state-of-the-art results
on the test dataset to meticulously detect Deepfake images.

</details>


### [40] [Preserving Domain Generalization in Fine-Tuning via Joint Parameter Selection](https://arxiv.org/abs/2508.16976)
*Bin Pan,Shiyu Shen,Zongbin Wang,Zhenwei Shi,Xia Xu*

Main category: cs.CV

TL;DR: JPS是一种参数高效的自适应方法，通过选择性微调预训练模型中的稀疏参数子集，平衡任务适应与泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决全微调预训练模型可能损害其固有泛化能力的问题。

Method: 提出联合参数选择（JPS），通过双操作符识别并更新跨源域梯度一致且显著的参数。

Result: JPS在多个基准测试中优于现有域泛化方法。

Conclusion: JPS通过稀疏参数更新有效保留了预训练模型的泛化能力，同时提升了任务性能。

Abstract: Domain generalization seeks to develop models trained on a limited set of
source domains that are capable of generalizing effectively to unseen target
domains. While the predominant approach leverages large-scale pre-trained
vision models as initialization, recent studies have highlighted that full
fine-tuning can compromise the intrinsic generalization capabilities of these
models. To address this limitation, parameter-efficient adaptation strategies
have emerged, wherein only a subset of model parameters is selectively
fine-tuned, thereby balancing task adaptation with the preservation of
generalization. Motivated by this paradigm, we introduce Joint Parameter
Selection (JPS), a novel method that restricts updates to a small, sparse
subset of parameters, thereby retaining and harnessing the generalization
strength of pre-trained models. Theoretically, we establish a generalization
error bound that explicitly accounts for the sparsity of parameter updates,
thereby providing a principled justification for selective fine-tuning.
Practically, we design a selection mechanism employing dual operators to
identify and update parameters exhibiting consistent and significant gradients
across all source domains. Extensive benchmark experiments demonstrate that JPS
achieves superior performance compared to state-of-the-art domain
generalization methods, substantiating both the efficiency and efficacy of the
proposed approach.

</details>


### [41] [HiCache: Training-free Acceleration of Diffusion Models via Hermite Polynomial-based Feature Caching](https://arxiv.org/abs/2508.16984)
*Liang Feng,Shikang Zheng,Jiacheng Liu,Yuqi Lin,Qinming Zhou,Peiliang Cai,Xinyu Wang,Junjie Chen,Chang Zou,Yue Ma,Linfeng Zhang*

Main category: cs.CV

TL;DR: HiCache是一个无需训练的加速框架，通过数学工具与经验特性对齐，显著提升扩散模型的推理速度，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在内容生成中表现出色，但迭代采样导致计算成本高昂。现有缓存方法因无法建模特征演化的复杂动态而损失质量。

Method: HiCache利用Hermite多项式（高斯相关过程的潜在最优基）进行特征导数近似，并引入双缩放机制确保数值稳定性和预测准确性。

Result: 实验显示HiCache在FLUX.1-dev上实现6.24倍加速，且生成质量超过基线，适用于文本到图像、视频生成和超分辨率任务。

Conclusion: HiCache通过理论优化和工程实现，显著提升了扩散模型的推理效率，同时保持高质量生成。

Abstract: Diffusion models have achieved remarkable success in content generation but
suffer from prohibitive computational costs due to iterative sampling. While
recent feature caching methods tend to accelerate inference through temporal
extrapolation, these methods still suffer from server quality loss due to the
failure in modeling the complex dynamics of feature evolution. To solve this
problem, this paper presents HiCache, a training-free acceleration framework
that fundamentally improves feature prediction by aligning mathematical tools
with empirical properties. Our key insight is that feature derivative
approximations in Diffusion Transformers exhibit multivariate Gaussian
characteristics, motivating the use of Hermite polynomials-the potentially
theoretically optimal basis for Gaussian-correlated processes. Besides, We
further introduce a dual-scaling mechanism that ensures numerical stability
while preserving predictive accuracy. Extensive experiments demonstrate
HiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding
baseline quality, maintaining strong performance across text-to-image, video
generation, and super-resolution tasks. Core implementation is provided in the
appendix, with complete code to be released upon acceptance.

</details>


### [42] [An Efficient Dual-Line Decoder Network with Multi-Scale Convolutional Attention for Multi-organ Segmentation](https://arxiv.org/abs/2508.17007)
*Riad Hassan,M. Rubaiyat Hossain Mondal,Sheikh Iqbal Ahamed,Fahad Mostafa,Md Mostafijur Rahman*

Main category: cs.CV

TL;DR: EDLDNet是一种高效的双线解码器分割网络，通过引入噪声解码器和多尺度注意力模块，在保持计算效率的同时显著提升了医学图像分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习分割方法在准确性和计算效率之间难以平衡，EDLDNet旨在填补这一空白。

Method: 采用噪声解码器、多尺度卷积注意力模块（MSCAMs）、注意力门（AGs）和上卷积块（UCBs），并结合基于突变的损失函数。

Result: 在四个公开医学影像数据集上表现优异，Synapse数据集上Dice分数达84.00%，比UNet提升13.89%，同时减少89.7%的MACs。

Conclusion: EDLDNet在泛化性、计算效率和鲁棒性方面表现突出，代码和数据已开源。

Abstract: Proper segmentation of organs-at-risk is important for radiation therapy,
surgical planning, and diagnostic decision-making in medical image analysis.
While deep learning-based segmentation architectures have made significant
progress, they often fail to balance segmentation accuracy with computational
efficiency. Most of the current state-of-the-art methods either prioritize
performance at the cost of high computational complexity or compromise accuracy
for efficiency. This paper addresses this gap by introducing an efficient
dual-line decoder segmentation network (EDLDNet). The proposed method features
a noisy decoder, which learns to incorporate structured perturbation at
training time for better model robustness, yet at inference time only the
noise-free decoder is executed, leading to lower computational cost.
Multi-Scale convolutional Attention Modules (MSCAMs), Attention Gates (AGs),
and Up-Convolution Blocks (UCBs) are further utilized to optimize feature
representation and boost segmentation performance. By leveraging multi-scale
segmentation masks from both decoders, we also utilize a mutation-based loss
function to enhance the model's generalization. Our approach outperforms SOTA
segmentation architectures on four publicly available medical imaging datasets.
EDLDNet achieves SOTA performance with an 84.00% Dice score on the Synapse
dataset, surpassing baseline model like UNet by 13.89% in Dice score while
significantly reducing Multiply-Accumulate Operations (MACs) by 89.7%. Compared
to recent approaches like EMCAD, our EDLDNet not only achieves higher Dice
score but also maintains comparable computational efficiency. The outstanding
performance across diverse datasets establishes EDLDNet's strong
generalization, computational efficiency, and robustness. The source code,
pre-processed data, and pre-trained weights will be available at
https://github.com/riadhassan/EDLDNet .

</details>


### [43] [Contrastive Prompt Clustering for Weakly Supervised Semantic Segmentation](https://arxiv.org/abs/2508.17009)
*Wangyu Wu,Zhenhong Chen,Xiaowen Ma,Wenqiao Zhang,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.CV

TL;DR: CPC是一种新的弱监督语义分割框架，利用LLMs生成类别簇并引入对比损失，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视类别间共享语义和细粒度区分，CPC旨在解决这一问题。

Method: CPC利用LLMs生成类别簇，并引入类感知的对比损失，实现层次化设计。

Result: 在PASCAL VOC 2012和MS COCO 2014上，CPC超越现有最优方法。

Conclusion: CPC通过结合粗粒度语义先验和细粒度边界，有效提升弱监督语义分割性能。

Abstract: Weakly Supervised Semantic Segmentation (WSSS) with image-level labels has
gained attention for its cost-effectiveness. Most existing methods emphasize
inter-class separation, often neglecting the shared semantics among related
categories and lacking fine-grained discrimination. To address this, we propose
Contrastive Prompt Clustering (CPC), a novel WSSS framework. CPC exploits Large
Language Models (LLMs) to derive category clusters that encode intrinsic
inter-class relationships, and further introduces a class-aware patch-level
contrastive loss to enforce intra-class consistency and inter-class separation.
This hierarchical design leverages clusters as coarse-grained semantic priors
while preserving fine-grained boundaries, thereby reducing confusion among
visually similar categories. Experiments on PASCAL VOC 2012 and MS COCO 2014
demonstrate that CPC surpasses existing state-of-the-art methods in WSSS.

</details>


### [44] [Fiducial Marker Splatting for High-Fidelity Robotics Simulations](https://arxiv.org/abs/2508.17012)
*Diram Tabaa,Gianni Di Caro*

Main category: cs.CV

TL;DR: 提出了一种结合高斯泼溅（GS）和结构化标记的混合框架，用于高效生成复杂场景中的标记，提升机器人定位和控制的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统基于网格的3D模拟在复杂环境中表现不佳，而现有神经渲染方法缺乏对机器人定位关键标记的支持。

Method: 开发了一种混合框架，结合GS的光照真实感和结构化标记表示，提出了一种高效生成标记的新算法。

Result: 实验表明，该方法在效率和姿态估计准确性上优于传统图像拟合技术，并在温室模拟中验证了其潜力。

Conclusion: 该框架在复杂环境中表现出色，为实际应用提供了重要价值。

Abstract: High-fidelity 3D simulation is critical for training mobile robots, but its
traditional reliance on mesh-based representations often struggle in complex
environments, such as densely packed greenhouses featuring occlusions and
repetitive structures. Recent neural rendering methods, like Gaussian Splatting
(GS), achieve remarkable visual realism but lack flexibility to incorporate
fiducial markers, which are essential for robotic localization and control. We
propose a hybrid framework that combines the photorealism of GS with structured
marker representations. Our core contribution is a novel algorithm for
efficiently generating GS-based fiducial markers (e.g., AprilTags) within
cluttered scenes. Experiments show that our approach outperforms traditional
image-fitting techniques in both efficiency and pose-estimation accuracy. We
further demonstrate the framework's potential in a greenhouse simulation. This
agricultural setting serves as a challenging testbed, as its combination of
dense foliage, similar-looking elements, and occlusions pushes the limits of
perception, thereby highlighting the framework's value for real-world
applications.

</details>


### [45] [Dual Orthogonal Guidance for Robust Diffusion-based Handwritten Text Generation](https://arxiv.org/abs/2508.17017)
*Konstantina Nikolaidou,George Retsinas,Giorgos Sfikas,Silvia Cascianelli,Rita Cucchiara,Marcus Liwicki*

Main category: cs.CV

TL;DR: 提出了一种名为DOG的新型采样引导策略，通过正交投影减少生成文本中的伪影，提升生成清晰度和风格多样性。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成手写文本时容易记忆训练样本，且在风格多样性和生成清晰度上表现不佳。

Method: 采用双正交引导（DOG）策略，结合三角调度控制引导强度。

Result: 在DiffusionPen和One-DM上实验表明，DOG提升了内容清晰度和风格多样性。

Conclusion: DOG是一种稳定且有效的引导策略，适用于手写文本生成任务。

Abstract: Diffusion-based Handwritten Text Generation (HTG) approaches achieve
impressive results on frequent, in-vocabulary words observed at training time
and on regular styles. However, they are prone to memorizing training samples
and often struggle with style variability and generation clarity. In
particular, standard diffusion models tend to produce artifacts or distortions
that negatively affect the readability of the generated text, especially when
the style is hard to produce. To tackle these issues, we propose a novel
sampling guidance strategy, Dual Orthogonal Guidance (DOG), that leverages an
orthogonal projection of a negatively perturbed prompt onto the original
positive prompt. This approach helps steer the generation away from artifacts
while maintaining the intended content, and encourages more diverse, yet
plausible, outputs. Unlike standard Classifier-Free Guidance (CFG), which
relies on unconditional predictions and produces noise at high guidance scales,
DOG introduces a more stable, disentangled direction in the latent space. To
control the strength of the guidance across the denoising process, we apply a
triangular schedule: weak at the start and end of denoising, when the process
is most sensitive, and strongest in the middle steps. Experimental results on
the state-of-the-art DiffusionPen and One-DM demonstrate that DOG improves both
content clarity and style variability, even for out-of-vocabulary words and
challenging writing styles.

</details>


### [46] [Probabilistic Temporal Masked Attention for Cross-view Online Action Detection](https://arxiv.org/abs/2508.17025)
*Liping Xie,Yang Tan,Shicheng Jing,Huimin Lu,Kanjian Zhang*

Main category: cs.CV

TL;DR: 提出了一种新的概率时间掩码注意力（PTMA）模型，用于解决在线动作检测（OAD）中视角变化导致的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 主流OAD模型对视频视角变化敏感，泛化能力受限。

Method: 利用概率建模生成跨视角的潜在压缩表示，结合GRU时间掩码注意力（TMA）单元增强信息交互。

Result: 在DAHLIA、IKEA ASM和Breakfast数据集上，PTMA在跨主体、跨视角和跨主体-视角评估协议中达到最优性能。

Conclusion: PTMA通过概率建模和多视角信息整合，显著提升了OAD任务的泛化能力。

Abstract: As a critical task in video sequence classification within computer vision,
Online Action Detection (OAD) has garnered significant attention. The
sensitivity of mainstream OAD models to varying video viewpoints often hampers
their generalization when confronted with unseen sources. To address this
limitation, we propose a novel Probabilistic Temporal Masked Attention (PTMA)
model, which leverages probabilistic modeling to derive latent compressed
representations of video frames in a cross-view setting. The PTMA model
incorporates a GRU-based temporal masked attention (TMA) cell, which leverages
these representations to effectively query the input video sequence, thereby
enhancing information interaction and facilitating autoregressive frame-level
video analysis. Additionally, multi-view information can be integrated into the
probabilistic modeling to facilitate the extraction of view-invariant features.
Experiments conducted under three evaluation protocols: cross-subject (cs),
cross-view (cv), and cross-subject-view (csv) show that PTMA achieves
state-of-the-art performance on the DAHLIA, IKEA ASM, and Breakfast datasets.

</details>


### [47] [A Novel Local Focusing Mechanism for Deepfake Detection Generalization](https://arxiv.org/abs/2508.17029)
*Mingliang Li,Lin Yuanbo Wu,Changhong Liu,Hanxi Li*

Main category: cs.CV

TL;DR: 提出了一种新的局部聚焦机制（LFM），用于提高跨域深度伪造检测的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度卷积网络的检测方法在跨类别和跨生成域时泛化能力差，主要原因是CNN的语义特征过拟合和全局平均池化（GAP）丢失局部伪造线索。

Method: LFM通过显式关注局部特征，结合显著性网络（SNet）和Top-K池化（TKP）模块选择最具信息量的局部模式，并引入两种正则化技术（RBLD和RKS）防止过拟合。

Result: LFM在准确率和平均精度上分别比现有最佳方法（NPR）提高了3.7%和2.8%，且在单GPU上达到1789 FPS的高效性能。

Conclusion: LFM为跨域深度伪造检测设定了新基准，代码已开源。

Abstract: The rapid advancement of deepfake generation techniques has intensified the
need for robust and generalizable detection methods. Existing approaches based
on reconstruction learning typically leverage deep convolutional networks to
extract differential features. However, these methods show poor generalization
across object categories (e.g., from faces to cars) and generation domains
(e.g., from GANs to Stable Diffusion), due to intrinsic limitations of deep
CNNs. First, models trained on a specific category tend to overfit to semantic
feature distributions, making them less transferable to other categories,
especially as network depth increases. Second, Global Average Pooling (GAP)
compresses critical local forgery cues into a single vector, thus discarding
discriminative patterns vital for real-fake classification. To address these
issues, we propose a novel Local Focus Mechanism (LFM) that explicitly attends
to discriminative local features for differentiating fake from real images. LFM
integrates a Salience Network (SNet) with a task-specific Top-K Pooling (TKP)
module to select the K most informative local patterns. To mitigate potential
overfitting introduced by Top-K pooling, we introduce two regularization
techniques: Rank-Based Linear Dropout (RBLD) and Random-K Sampling (RKS), which
enhance the model's robustness. LFM achieves a 3.7 improvement in accuracy and
a 2.8 increase in average precision over the state-of-the-art Neighboring Pixel
Relationships (NPR) method, while maintaining exceptional efficiency at 1789
FPS on a single NVIDIA A6000 GPU. Our approach sets a new benchmark for
cross-domain deepfake detection. The source code are available in
https://github.com/lmlpy/LFM.git

</details>


### [48] [F4-ITS: Fine-grained Feature Fusion for Food Image-Text Search](https://arxiv.org/abs/2508.17037)
*Raghul Asokan*

Main category: cs.CV

TL;DR: F4-ITS是一种无需训练的视觉语言模型框架，通过多模态特征融合和重排序机制，显著提升食物图像-文本匹配的检索性能。


<details>
  <summary>Details</summary>
Motivation: 数字食物内容的激增需要更精细的视觉理解和检索系统，以支持饮食监测、智能厨房等应用。

Method: 提出F4-ITS框架，包括多模态融合策略和基于特征的重新排序机制，利用VLM生成的文本描述增强检索效果。

Result: 在密集和稀疏标注场景下，top-1检索性能分别提升约10%和7.7%，top-k成分检索提升28.6%。

Conclusion: F4-ITS展示了在资源受限环境中，小模型通过文本融合也能媲美或超越大模型的潜力。

Abstract: The proliferation of digital food content has intensified the need for robust
and accurate systems capable of fine-grained visual understanding and
retrieval. In this work, we address the challenging task of food image-to-text
matching, a critical component in applications such as dietary monitoring,
smart kitchens, and restaurant automation. We propose F4-ITS: Fine-grained
Feature Fusion for Food Image-Text Search, a training-free, vision-language
model (VLM)-guided framework that significantly improves retrieval performance
through enhanced multi-modal feature representations. Our approach introduces
two key contributions: (1) a uni-directional(and bi-directional) multi-modal
fusion strategy that combines image embeddings with VLM-generated textual
descriptions to improve query expressiveness, and (2) a novel feature-based
re-ranking mechanism for top-k retrieval, leveraging predicted food ingredients
to refine results and boost precision. Leveraging open-source image-text
encoders, we demonstrate substantial gains over standard baselines - achieving
~10% and ~7.7% improvements in top-1 retrieval under dense and sparse caption
scenarios, and a ~28.6% gain in top-k ingredient-level retrieval. Additionally,
we show that smaller models (e.g., ViT-B/32) can match or outperform larger
counterparts (e.g., ViT-H, ViT-G, ViT-bigG) when augmented with textual fusion,
highlighting the effectiveness of our method in resource-constrained settings.
Code and test datasets will be made publicly available at:
https://github.com/mailcorahul/f4-its

</details>


### [49] [M3DMap: Object-aware Multimodal 3D Mapping for Dynamic Environments](https://arxiv.org/abs/2508.17044)
*Dmitry Yudin*

Main category: cs.CV

TL;DR: 提出了一种分类法和模块化方法M3DMap，用于构建多模态3D地图，适用于静态和动态场景。


<details>
  <summary>Details</summary>
Motivation: 解决动态环境中3D映射的挑战，缺乏通用的多模态数据表示方法。

Method: 提出分类法分析现有方法，并开发模块化方法M3DMap，包含分割跟踪、里程估计、地图构建和数据检索模块。

Result: 展示了M3DMap在多种任务中的优势，并验证了多模态数据和基础模型的积极效果。

Conclusion: M3DMap为动态环境中的3D映射提供了有效解决方案，并推动了多模态数据在3D映射中的应用。

Abstract: 3D mapping in dynamic environments poses a challenge for modern researchers
in robotics and autonomous transportation. There are no universal
representations for dynamic 3D scenes that incorporate multimodal data such as
images, point clouds, and text. This article takes a step toward solving this
problem. It proposes a taxonomy of methods for constructing multimodal 3D maps,
classifying contemporary approaches based on scene types and representations,
learning methods, and practical applications. Using this taxonomy, a brief
structured analysis of recent methods is provided. The article also describes
an original modular method called M3DMap, designed for object-aware
construction of multimodal 3D maps for both static and dynamic scenes. It
consists of several interconnected components: a neural multimodal object
segmentation and tracking module; an odometry estimation module, including
trainable algorithms; a module for 3D map construction and updating with
various implementations depending on the desired scene representation; and a
multimodal data retrieval module. The article highlights original
implementations of these modules and their advantages in solving various
practical tasks, from 3D object grounding to mobile manipulation. Additionally,
it presents theoretical propositions demonstrating the positive effect of using
multimodal data and modern foundational models in 3D mapping methods. Details
of the taxonomy and method implementation are available at
https://yuddim.github.io/M3DMap.

</details>


### [50] [Styleclone: Face Stylization with Diffusion Based Data Augmentation](https://arxiv.org/abs/2508.17045)
*Neeraj Matiyali,Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: StyleClone利用文本反演和扩散引导图像生成增强小规模风格数据集，训练快速图像转换网络，在速度和质量上优于扩散方法。


<details>
  <summary>Details</summary>
Motivation: 解决有限风格图像下高质量人脸风格化的问题。

Method: 结合文本反演和扩散引导生成多样风格样本，增强数据集后训练图像转换网络。

Result: 在多种风格上提升风格化质量、保留源图像内容并加速推理。

Conclusion: StyleClone在风格化任务中表现优异，同时提供了数据增强技术的系统评估。

Abstract: We present StyleClone, a method for training image-to-image translation
networks to stylize faces in a specific style, even with limited style images.
Our approach leverages textual inversion and diffusion-based guided image
generation to augment small style datasets. By systematically generating
diverse style samples guided by both the original style images and real face
images, we significantly enhance the diversity of the style dataset. Using this
augmented dataset, we train fast image-to-image translation networks that
outperform diffusion-based methods in speed and quality. Experiments on
multiple styles demonstrate that our method improves stylization quality,
better preserves source image content, and significantly accelerates inference.
Additionally, we provide a systematic evaluation of the augmentation techniques
and their impact on stylization performance.

</details>


### [51] [PVNet: Point-Voxel Interaction LiDAR Scene Upsampling Via Diffusion Models](https://arxiv.org/abs/2508.17050)
*Xianjing Cheng,Lintai Wu,Zuowen Wang,Junhui Hou,Jie Wen,Yong Xu*

Main category: cs.CV

TL;DR: PVNet是一种基于扩散模型的点-体素交互框架，用于无需密集监督的LiDAR点云上采样，首次支持任意上采样率。


<details>
  <summary>Details</summary>
Motivation: 解决LiDAR扫描数据稀疏性对3D场景理解的限制，现有方法难以泛化到复杂户外场景。

Method: 采用无分类器引导的DDPMs，结合点云和体素特征，设计体素完成模块和点-体素交互模块。

Result: 在多个基准测试中达到最先进性能。

Conclusion: PVNet是首个场景级点云上采样方法，支持任意上采样率，显著提升环境感知能力。

Abstract: Accurate 3D scene understanding in outdoor environments heavily relies on
high-quality point clouds. However, LiDAR-scanned data often suffer from
extreme sparsity, severely hindering downstream 3D perception tasks. Existing
point cloud upsampling methods primarily focus on individual objects, thus
demonstrating limited generalization capability for complex outdoor scenes. To
address this issue, we propose PVNet, a diffusion model-based point-voxel
interaction framework to perform LiDAR point cloud upsampling without dense
supervision. Specifically, we adopt the classifier-free guidance-based DDPMs to
guide the generation, in which we employ a sparse point cloud as the guiding
condition and the synthesized point clouds derived from its nearby frames as
the input. Moreover, we design a voxel completion module to refine and complete
the coarse voxel features for enriching the feature representation. In
addition, we propose a point-voxel interaction module to integrate features
from both points and voxels, which efficiently improves the environmental
perception capability of each upsampled point. To the best of our knowledge,
our approach is the first scene-level point cloud upsampling method supporting
arbitrary upsampling rates. Extensive experiments on various benchmarks
demonstrate that our method achieves state-of-the-art performance. The source
code will be available at https://github.com/chengxianjing/PVNet.

</details>


### [52] [DeltaFlow: An Efficient Multi-frame Scene Flow Estimation Method](https://arxiv.org/abs/2508.17054)
*Qingwen Zhang,Xiaomeng Zhu,Yushan Zhang,Yixi Cai,Olov Andersson,Patric Jensfelt*

Main category: cs.CV

TL;DR: DeltaFlow（ΔFlow）是一种轻量级3D框架，通过Δ方案高效捕捉运动线索，显著降低计算成本，并在多帧场景流估计中实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖两帧输入，忽略了时间域中的有价值信息，而多帧方法计算成本高。

Method: 提出DeltaFlow框架，采用Δ方案提取时间特征，引入类别平衡损失和实例一致性损失以解决类别不平衡和运动不一致问题。

Result: 在Argoverse 2和Waymo数据集上，DeltaFlow误差降低22%，推理速度快2倍，并表现出强大的跨域泛化能力。

Conclusion: DeltaFlow通过高效的时间特征提取和损失函数设计，显著提升了场景流估计的性能和效率。

Abstract: Previous dominant methods for scene flow estimation focus mainly on input
from two consecutive frames, neglecting valuable information in the temporal
domain. While recent trends shift towards multi-frame reasoning, they suffer
from rapidly escalating computational costs as the number of frames grows. To
leverage temporal information more efficiently, we propose DeltaFlow
($\Delta$Flow), a lightweight 3D framework that captures motion cues via a
$\Delta$ scheme, extracting temporal features with minimal computational cost,
regardless of the number of frames. Additionally, scene flow estimation faces
challenges such as imbalanced object class distributions and motion
inconsistency. To tackle these issues, we introduce a Category-Balanced Loss to
enhance learning across underrepresented classes and an Instance Consistency
Loss to enforce coherent object motion, improving flow accuracy. Extensive
evaluations on the Argoverse 2 and Waymo datasets show that $\Delta$Flow
achieves state-of-the-art performance with up to 22% lower error and $2\times$
faster inference compared to the next-best multi-frame supervised method, while
also demonstrating a strong cross-domain generalization ability. The code is
open-sourced at https://github.com/Kin-Zhang/DeltaFlow along with trained model
weights.

</details>


### [53] [REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage Generative Network Framework](https://arxiv.org/abs/2508.17061)
*Stefanos Pasios,Nikos Nikolaidis*

Main category: cs.CV

TL;DR: 论文提出了一种名为REGEN的双阶段生成网络框架，通过生成对抗网络增强游戏帧的逼真度，实现了实时推理且不牺牲视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现代视频游戏中的逼真度对玩家体验至关重要，但动态环境中实现实时逼真度仍具挑战性。

Method: 采用双阶段生成网络框架，将问题转化为配对的图像到图像翻译任务，使用轻量级方法训练。

Result: 在《侠盗猎车手V》中验证，视觉效果与现有方法相当，推理速度提升32.14倍。

Conclusion: REGEN框架在提升游戏帧逼真度的同时实现了实时性能，优于直接训练轻量级模型的方法。

Abstract: Photorealism is an important aspect of modern video games since it can shape
the player experience and simultaneously impact the immersion, narrative
engagement, and visual fidelity. Although recent hardware technological
breakthroughs, along with state-of-the-art rendering technologies, have
significantly improved the visual realism of video games, achieving true
photorealism in dynamic environments at real-time frame rates still remains a
major challenge due to the tradeoff between visual quality and performance. In
this short paper, we present a novel approach for enhancing the photorealism of
rendered game frames using generative adversarial networks. To this end, we
propose Real-time photorealism Enhancement in Games via a dual-stage gEnerative
Network framework (REGEN), which employs a robust unpaired image-to-image
translation model to produce semantically consistent photorealistic frames that
transform the problem into a simpler paired image-to-image translation task.
This enables training with a lightweight method that can achieve real-time
inference time without compromising visual quality. We demonstrate the
effectiveness of our framework on Grand Theft Auto V, showing that the approach
achieves visual results comparable to the ones produced by the robust unpaired
Im2Im method while improving inference speed by 32.14 times. Our findings also
indicate that the results outperform the photorealism-enhanced frames produced
by directly training a lightweight unpaired Im2Im translation method to
translate the video game frames towards the visual characteristics of
real-world images. Code, pre-trained models, and demos for this work are
available at: https://github.com/stefanos50/REGEN.

</details>


### [54] [SSG-Dit: A Spatial Signal Guided Framework for Controllable Video Generation](https://arxiv.org/abs/2508.17062)
*Peng Hu,Yu Gu,Liang Luo,Fuji Ren*

Main category: cs.CV

TL;DR: SSG-DiT是一种高效的可控视频生成框架，通过空间信号引导和双分支注意力机制，显著提升了语义一致性和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有模型在可控视频生成中难以保持语义一致性，导致生成内容与用户条件不符。

Method: 提出SSG-DiT框架，采用两阶段解耦设计：空间信号提示生成视觉提示，再通过SSG-Adapter注入冻结的视频DiT主干。

Result: 在VBench基准测试中，SSG-DiT在空间关系控制和整体一致性方面表现最优。

Conclusion: SSG-DiT通过空间信号引导和高效适配器设计，实现了高质量可控视频生成，解决了语义一致性问题。

Abstract: Controllable video generation aims to synthesize video content that aligns
precisely with user-provided conditions, such as text descriptions and initial
images. However, a significant challenge persists in this domain: existing
models often struggle to maintain strong semantic consistency, frequently
generating videos that deviate from the nuanced details specified in the
prompts. To address this issue, we propose SSG-DiT (Spatial Signal Guided
Diffusion Transformer), a novel and efficient framework for high-fidelity
controllable video generation. Our approach introduces a decoupled two-stage
process. The first stage, Spatial Signal Prompting, generates a spatially aware
visual prompt by leveraging the rich internal representations of a pre-trained
multi-modal model. This prompt, combined with the original text, forms a joint
condition that is then injected into a frozen video DiT backbone via our
lightweight and parameter-efficient SSG-Adapter. This unique design, featuring
a dual-branch attention mechanism, allows the model to simultaneously harness
its powerful generative priors while being precisely steered by external
spatial signals. Extensive experiments demonstrate that SSG-DiT achieves
state-of-the-art performance, outperforming existing models on multiple key
metrics in the VBench benchmark, particularly in spatial relationship control
and overall consistency.

</details>


### [55] [Proximal Vision Transformer: Enhancing Feature Representation through Two-Stage Manifold Geometry](https://arxiv.org/abs/2508.17081)
*Haoyu Yun,Hamid Krim*

Main category: cs.CV

TL;DR: 本文提出了一种结合ViT与近端工具的新框架，通过几何优化提升特征表示和分类性能。


<details>
  <summary>Details</summary>
Motivation: ViT的优化局限于建模图像内的局部关系，无法捕捉数据点间的全局几何关系。

Method: 通过ViT的自注意力机制构建流形的切丛，引入近端迭代定义切丛中的截面，实现全局特征对齐和优化。

Result: 实验结果表明，该方法在分类精度和数据分布上优于传统ViT。

Conclusion: 该框架通过几何优化显著提升了ViT的性能。

Abstract: The Vision Transformer (ViT) architecture has become widely recognized in
computer vision, leveraging its self-attention mechanism to achieve remarkable
success across various tasks. Despite its strengths, ViT's optimization remains
confined to modeling local relationships within individual images, limiting its
ability to capture the global geometric relationships between data points. To
address this limitation, this paper proposes a novel framework that integrates
ViT with the proximal tools, enabling a unified geometric optimization approach
to enhance feature representation and classification performance. In this
framework, ViT constructs the tangent bundle of the manifold through its
self-attention mechanism, where each attention head corresponds to a tangent
space, offering geometric representations from diverse local perspectives.
Proximal iterations are then introduced to define sections within the tangent
bundle and project data from tangent spaces onto the base space, achieving
global feature alignment and optimization. Experimental results confirm that
the proposed method outperforms traditional ViT in terms of classification
accuracy and data distribution.

</details>


### [56] [PD-Loss: Proxy-Decidability for Efficient Metric Learning](https://arxiv.org/abs/2508.17082)
*Pedro Silva,Guilherme A. L. Silva,Pablo Coelho,Vander Freitas,Gladston Moreira,David Menotii,Eduardo Luz*

Main category: cs.CV

TL;DR: PD-Loss是一种结合可学习代理和统计框架的新目标函数，用于高效优化嵌入空间，解决了现有方法的计算和全局分布优化问题。


<details>
  <summary>Details</summary>
Motivation: 现有深度度量学习方法（如成对损失）存在采样复杂和收敛慢的问题，而基于代理的方法虽可扩展但难以优化全局分布特性。

Method: 提出Proxy-Decidability Loss（PD-Loss），通过代理估计真实和冒名分布，结合可学习代理与统计框架d'，优化嵌入空间。

Result: 实验表明，PD-Loss在细粒度分类和人脸验证等任务中性能与最先进方法相当，同时提供了嵌入优化的新视角。

Conclusion: PD-Loss结合了代理方法的计算效率和D-Loss的分布可分离性，为分布感知的深度度量学习提供了可扩展的解决方案。

Abstract: Deep Metric Learning (DML) aims to learn embedding functions that map
semantically similar inputs to proximate points in a metric space while
separating dissimilar ones. Existing methods, such as pairwise losses, are
hindered by complex sampling requirements and slow convergence. In contrast,
proxy-based losses, despite their improved scalability, often fail to optimize
global distribution properties. The Decidability-based Loss (D-Loss) addresses
this by targeting the decidability index (d') to enhance distribution
separability, but its reliance on large mini-batches imposes significant
computational constraints. We introduce Proxy-Decidability Loss (PD-Loss), a
novel objective that integrates learnable proxies with the statistical
framework of d' to optimize embedding spaces efficiently. By estimating genuine
and impostor distributions through proxies, PD-Loss combines the computational
efficiency of proxy-based methods with the principled separability of D-Loss,
offering a scalable approach to distribution-aware DML. Experiments across
various tasks, including fine-grained classification and face verification,
demonstrate that PD-Loss achieves performance comparable to that of
state-of-the-art methods while introducing a new perspective on embedding
optimization, with potential for broader applications.

</details>


### [57] [GRASP: Geospatial pixel Reasoning viA Structured Policy learning](https://arxiv.org/abs/2508.17102)
*Chengjie Jiang,Yunqi Zhou,Jiafeng Yan,Jing Li*

Main category: cs.CV

TL;DR: GRASP是一个结构化策略学习框架，通过强化学习优化，无需密集像素监督，实现了在遥感任务中从自然语言指令生成分割掩码的先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有MLLM系统需要密集像素监督，成本高且对域外数据表现不佳。GRASP旨在通过强化学习减少训练参数并利用基础模型的强先验。

Method: GRASP结合多模态大语言模型生成边界框和点提示，再由预训练分割模型生成最终掩码，仅通过GRPO强化学习优化。

Result: 在域内和域外测试集上分别提升4%和54%，展示了强大的泛化能力。

Conclusion: GRASP证明了复杂地理空间分割行为可以通过弱空间线索的强化学习实现，且性能优越。

Abstract: Geospatial pixel reasoning is a nascent remote-sensing task that aims to
generate segmentation masks directly from natural-language instructions.
Prevailing MLLM-based systems co-train a language model and a mask decoder with
dense pixel supervision, which is expensive and often weak on out-of-domain
(OOD) data. We introduce GRASP, a structured policy-learning framework. In our
design, a multimodal large language model first emits task-relevant bounding
boxes and positive points from a vision-language instruction. These outputs are
then passed to a pre-trained segmentation model, which consumes them as prompts
to generate the final mask. Instead of supervised fine-tuning, we optimize the
system purely with reinforcement learning: the model is trained solely with
GRPO, guided by format rewards and accuracy rewards computed on boxes and
points (no mask supervision). This leverages strong priors in foundation
models, minimizes trainable parameters, and enables learning from inexpensive
annotations. We additionally curate GRASP-1k, which contains
reasoning-intensive queries, detailed reasoning traces, and fine-grained
segmentation annotations. Evaluations on both in-domain and out-of-domain test
sets show state-of-the-art results: about 4% improvement in-domain and up to
54% on OOD benchmarks. The experiment results evidence our model's robust
generalization and demonstrate that complex geospatial segmentation behaviors
can be learned via RL from weak spatial cues. Code and the dataset will be
released open-source.

</details>


### [58] [SugarcaneShuffleNet: A Very Fast, Lightweight Convolutional Neural Network for Diagnosis of 15 Sugarcane Leaf Diseases](https://arxiv.org/abs/2508.17107)
*Shifat E. Arman,Hasan Muhammad Abdullah,Syed Nazmus Sakib,RM Saiem,Shamima Nasrin Asha,Md Mehedi Hasan,Shahrear Bin Amin,S M Mahin Abrar*

Main category: cs.CV

TL;DR: 提出了一种轻量级模型SugarcaneShuffleNet和数据集SugarcaneLD-BD，用于甘蔗叶病分类，适用于资源受限地区。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在资源受限地区难以推广，缺乏高效、可解释的工具。

Method: 开发了轻量级模型SugarcaneShuffleNet，结合数据集SugarcaneLD-BD，并通过渐进式Web应用SugarcaneAI部署。

Result: 模型准确率达98.02%，F1分数0.98，推理时间4.14毫秒/图像，优于其他轻量级模型。

Conclusion: 该研究提供了高效、实用的甘蔗叶病分类工具，适用于低资源环境。

Abstract: Despite progress in AI-based plant diagnostics, sugarcane farmers in
low-resource regions remain vulnerable to leaf diseases due to the lack of
scalable, efficient, and interpretable tools. Many deep learning models fail to
generalize under real-world conditions and require substantial computational
resources, limiting their use in resource-constrained regions. In this paper,
we present SugarcaneLD-BD, a curated dataset for sugarcane leaf-disease
classification; SugarcaneShuffleNet, an optimized lightweight model for rapid
on-device diagnosis; and SugarcaneAI, a Progressive Web Application for field
deployment. SugarcaneLD-BD contains 638 curated images across five classes,
including four major sugarcane diseases, collected in Bangladesh under diverse
field conditions and verified by expert pathologists. To enhance diversity, we
combined SugarcaneLD-BD with two additional datasets, yielding a larger and
more representative corpus. Our optimized model, SugarcaneShuffleNet, offers
the best trade-off between speed and accuracy for real-time, on-device
diagnosis. This 9.26 MB model achieved 98.02% accuracy, an F1-score of 0.98,
and an average inference time of 4.14 ms per image. For comparison, we
fine-tuned five other lightweight convolutional neural networks: MnasNet,
EdgeNeXt, EfficientNet-Lite, MobileNet, and SqueezeNet via transfer learning
and Bayesian optimization. MnasNet and EdgeNeXt achieved comparable accuracy to
SugarcaneShuffleNet, but required significantly more parameters, memory, and
computation, limiting their suitability for low-resource deployment. We
integrate SugarcaneShuffleNet into SugarcaneAI, delivering Grad-CAM-based
explanations in the field. Together, these contributions offer a diverse
benchmark, efficient models for low-resource environments, and a practical tool
for sugarcane disease classification. It spans varied lighting, backgrounds and
devices used on-farm

</details>


### [59] [PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science](https://arxiv.org/abs/2508.17117)
*Syed Nazmus Sakib,Nafiul Haque,Mohammad Zabed Hossain,Shifat E. Arman*

Main category: cs.CV

TL;DR: PlantVillageVQA是一个基于PlantVillage图像库的大规模视觉问答数据集，旨在推动农业决策和分析的视觉语言模型发展。


<details>
  <summary>Details</summary>
Motivation: 为农业领域提供高质量的视觉问答数据集，以提升植物病害识别的诊断准确性并推动科学研究。

Method: 通过两阶段自动化流程生成问题-答案对：1) 基于图像元数据的模板合成；2) 多阶段语言重构，并由专家审核科学准确性和相关性。

Result: 数据集包含193,609个高质量QA对，覆盖55,448张图像、14种作物和38种病害，问题分为3个认知复杂度和9个类别。

Conclusion: PlantVillageVQA是一个公开、标准化且专家验证的数据集，有助于农业领域的科学研究和植物病害识别。

Abstract: PlantVillageVQA is a large-scale visual question answering (VQA) dataset
derived from the widely used PlantVillage image corpus. It was designed to
advance the development and evaluation of vision-language models for
agricultural decision-making and analysis. The PlantVillageVQA dataset
comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448
images spanning 14 crop species and 38 disease conditions. Questions are
organised into 3 levels of cognitive complexity and 9 distinct categories. Each
question category was phrased manually following expert guidance and generated
via an automated two-stage pipeline: (1) template-based QA synthesis from image
metadata and (2) multi-stage linguistic re-engineering. The dataset was
iteratively reviewed by domain experts for scientific accuracy and relevancy.
The final dataset was evaluated using three state-of-the-art models for quality
assessment. Our objective remains to provide a publicly available, standardised
and expert-verified database to enhance diagnostic accuracy for plant disease
identifications and advance scientific research in the agricultural domain. Our
dataset will be open-sourced at
https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.

</details>


### [60] [CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis](https://arxiv.org/abs/2508.17128)
*Mirza Mumtaz Zahoor,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 提出了一种名为CE-RS-SBCIT的混合框架，结合了残差和空间学习的CNN与Transformer模块，显著提升了脑肿瘤MRI分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤的早期检测和准确分类对诊断和治疗至关重要，但现有深度学习方法面临计算成本高、对MRI数据微小变化敏感等问题。

Method: 通过四种创新方法：SBCIT模块、定制残差和空间学习CNN、通道增强策略和空间注意力机制，整合局部和全局特征。

Result: 在Kaggle和Figshare的MRI数据集上，实现了98.30%的准确率、98.08%的敏感度、98.25%的F1分数和98.43%的精确度。

Conclusion: CE-RS-SBCIT框架在脑肿瘤分类中表现出色，解决了现有方法的局限性，为临床诊断提供了高效工具。

Abstract: Brain tumors remain among the most lethal human diseases, where early
detection and accurate classification are critical for effective diagnosis and
treatment planning. Although deep learning-based computer-aided diagnostic
(CADx) systems have shown remarkable progress. However, conventional
convolutional neural networks (CNNs) and Transformers face persistent
challenges, including high computational cost, sensitivity to minor contrast
variations, structural heterogeneity, and texture inconsistencies in MRI data.
Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integrating
residual and spatial learning-based CNNs with transformer-driven modules. The
proposed framework exploits local fine-grained and global contextual cues
through four core innovations: (i) a smoothing and boundary-based
CNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learning
CNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatial
attention mechanism. The developed SBCIT employs stem convolution and
contextual interaction transformer blocks with systematic smoothing and
boundary operations, enabling efficient global feature modeling. Moreover,
Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,
enrich the representation space, while the CE module amplifies discriminative
channels and mitigates redundancy. Furthermore, the spatial attention mechanism
selectively emphasizes subtle contrast and textural variations across tumor
classes. Extensive evaluation on challenging MRI datasets from Kaggle and
Figshare, encompassing glioma, meningioma, pituitary tumors, and healthy
controls, demonstrates superior performance, achieving 98.30% accuracy, 98.08%
sensitivity, 98.25% F1-score, and 98.43% precision.

</details>


### [61] [Structural Damage Detection Using AI Super Resolution and Visual Language Model](https://arxiv.org/abs/2508.17130)
*Catherine Hoier,Khandaker Mamun Ahmed*

Main category: cs.CV

TL;DR: 提出了一种基于无人机影像和AI技术的灾害损害评估框架，结合视频超分辨率模型和视觉语言模型，实现了高精度的建筑损害分类。


<details>
  <summary>Details</summary>
Motivation: 传统灾害损害评估方法耗时、昂贵且危险，难以在资源有限的环境中快速响应。

Method: 利用无人机影像、VRT视频超分辨率模型和Gemma3:27b视觉语言模型，构建了一个集成系统，用于提升低分辨率影像质量并分类建筑损害。

Result: 在土耳其地震和摩尔龙卷风数据上验证，分类准确率达到84.5%。

Conclusion: 该系统为非技术人员提供了高效的初步分析工具，提升了灾害管理的响应速度和效率。

Abstract: Natural disasters pose significant challenges to timely and accurate damage
assessment due to their sudden onset and the extensive areas they affect.
Traditional assessment methods are often labor-intensive, costly, and hazardous
to personnel, making them impractical for rapid response, especially in
resource-limited settings. This study proposes a novel, cost-effective
framework that leverages aerial drone footage, an advanced AI-based video
super-resolution model, Video Restoration Transformer (VRT), and Gemma3:27b, a
27 billion parameter Visual Language Model (VLM). This integrated system is
designed to improve low-resolution disaster footage, identify structural
damage, and classify buildings into four damage categories, ranging from
no/slight damage to total destruction, along with associated risk levels. The
methodology was validated using pre- and post-event drone imagery from the 2023
Turkey earthquakes (courtesy of The Guardian) and satellite data from the 2013
Moore Tornado (xBD dataset). The framework achieved a classification accuracy
of 84.5%, demonstrating its ability to provide highly accurate results.
Furthermore, the system's accessibility allows non-technical users to perform
preliminary analyses, thereby improving the responsiveness and efficiency of
disaster management efforts.

</details>


### [62] [Beyond Play and Pause: Turning GPT-4o Spatial Weakness into a Strength for In-Depth Interactive Video Learning](https://arxiv.org/abs/2508.17160)
*Sajad Goudarzi,Samaneh Zamanifard*

Main category: cs.CV

TL;DR: Untwist是一个AI驱动的交互式视频学习系统，允许用户通过提问或框选特定区域与视频内容互动，提供上下文感知的多模态响应。


<details>
  <summary>Details</summary>
Motivation: 传统视频学习被动且互动性差，现有AI工具缺乏实时、区域特定的交互能力。

Method: 结合GPT API和计算机视觉技术，通过视频预处理和实时交互架构，提升视频内容的理解和定位准确性。

Result: Untwist显著提高了视频内容的定位和解释准确性，将被动视频消费转变为互动学习体验。

Conclusion: Untwist有潜力通过AI驱动的交互提升学习参与度和理解力。

Abstract: Traditional video-based learning remains passive, offering limited
opportunities for users to engage dynamically with content. While current
AI-powered tools offer transcription and summarization, they lack real-time,
region-specific interaction capabilities. This paper introduces Untwist, an
AI-driven system that enables interactive video learning by allowing users to
ask questions about the entire video or specific regions using a bounding box,
receiving context-aware, multimodal responses. By integrating GPT APIs with
Computer Vision techniques, Untwist extracts, processes, and structures video
content to enhance comprehension. Our approach addresses GPT-4o spatial
weakness by leveraging annotated frames instead of raw coordinate data,
significantly improving accuracy in localizing and interpreting video content.
This paper describes the system architecture, including video pre-processing
and real-time interaction, and outlines how Untwist can transform passive video
consumption into an interactive, AI-driven learning experience with the
potential to enhance engagement and comprehension.

</details>


### [63] [Development of an isotropic segmentation model for medial temporal lobe subregions on anisotropic MRI atlas using implicit neural representation](https://arxiv.org/abs/2508.17171)
*Yue Li,Pulkit Khandelwal,Rohit Jena,Long Xie,Michael Duong,Amanda E. Denning,Christopher A. Brown,Laura E. M. Wisse,Sandhitsu R. Das,David A. Wolk,Paul A. Yushkevich*

Main category: cs.CV

TL;DR: 利用隐式神经表示方法结合T1和T2加权MRI的优势，提升MTL亚区分割精度，改善AD成像生物标志物的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于T2加权MRI的各向异性分辨率限制，难以准确提取MTL皮质亚区厚度，影响AD诊断和追踪的准确性。

Method: 结合T1和T2加权MRI的分辨率优势，通过隐式神经表示方法将各向异性空间图谱上采样至各向同性空间，开发各向同性MTL亚区分割模型。

Result: 各向同性模型提取的皮质亚区厚度在区分轻度认知障碍与认知正常参与者时更具显著性，且在纵向分析中表现更稳定。

Conclusion: 该方法在不增加图谱标注工作量的情况下提高了AD成像生物标志物的准确性，有助于更精确量化AD与脑萎缩的关系。

Abstract: Imaging biomarkers in magnetic resonance imaging (MRI) are important tools
for diagnosing and tracking Alzheimer's disease (AD). As medial temporal lobe
(MTL) is the earliest region to show AD-related hallmarks, brain atrophy caused
by AD can first be observed in the MTL. Accurate segmentation of MTL subregions
and extraction of imaging biomarkers from them are important. However, due to
imaging limitations, the resolution of T2-weighted (T2w) MRI is anisotropic,
which makes it difficult to accurately extract the thickness of cortical
subregions in the MTL. In this study, we used an implicit neural representation
method to combine the resolution advantages of T1-weighted and T2w MRI to
accurately upsample an MTL subregion atlas set from anisotropic space to
isotropic space, establishing a multi-modality, high-resolution atlas set.
Based on this atlas, we developed an isotropic MTL subregion segmentation
model. In an independent test set, the cortical subregion thickness extracted
using this isotropic model showed higher significance than an anisotropic
method in distinguishing between participants with mild cognitive impairment
and cognitively unimpaired (CU) participants. In longitudinal analysis, the
biomarkers extracted using isotropic method showed greater stability in CU
participants. This study improved the accuracy of AD imaging biomarkers without
increasing the amount of atlas annotation work, which may help to more
accurately quantify the relationship between AD and brain atrophy and provide
more accurate measures for disease tracking.

</details>


### [64] [VROOM - Visual Reconstruction over Onboard Multiview](https://arxiv.org/abs/2508.17172)
*Yajat Yadav,Varun Bharadwaj,Jathin Korrapati,Tanish Baranwal*

Main category: cs.CV

TL;DR: VROOM是一个利用赛车车载摄像头视频重建F1赛道3D模型的系统，通过处理高速运动和镜头切换等挑战，结合多种方法和技术，部分恢复了复杂环境中的赛道和车辆轨迹。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何仅利用车载摄像头视频进行可扩展的4D重建，尤其是在高速运动和复杂环境下。

Method: 结合DROID-SLAM、AnyCam和Monst3r等方法，采用掩码、时间分块和分辨率缩放等预处理技术。

Result: VROOM能够部分恢复复杂环境中的赛道和车辆轨迹。

Conclusion: 研究表明，利用车载视频进行4D重建在现实场景中是可行的。

Abstract: We introduce VROOM, a system for reconstructing 3D models of Formula 1
circuits using only onboard camera footage from racecars. Leveraging video data
from the 2023 Monaco Grand Prix, we address video challenges such as high-speed
motion and sharp cuts in camera frames. Our pipeline analyzes different methods
such as DROID-SLAM, AnyCam, and Monst3r and combines preprocessing techniques
such as different methods of masking, temporal chunking, and resolution scaling
to account for dynamic motion and computational constraints. We show that Vroom
is able to partially recover track and vehicle trajectories in complex
environments. These findings indicate the feasibility of using onboard video
for scalable 4D reconstruction in real-world settings. The project page can be
found at https://varun-bharadwaj.github.io/vroom, and our code is available at
https://github.com/yajatyadav/vroom.

</details>


### [65] [Advancing Weakly-Supervised Change Detection in Satellite Images via Adversarial Class Prompting](https://arxiv.org/abs/2508.17186)
*Zhenghui Zhao,Chen Wu,Di Wang,Hongruixuan Chen,Cuiqun Chen,Zhuo Zheng,Bo Du,Liangpei Zhang*

Main category: cs.CV

TL;DR: 提出了一种对抗性类提示（AdvCP）方法，通过对抗性提示挖掘和样本校正，解决弱监督变化检测中的背景噪声问题。


<details>
  <summary>Details</summary>
Motivation: 弱监督变化检测（WSCD）依赖图像级标签，但容易将背景变化误判为目标变化，需要解决这一噪声问题。

Method: AdvCP包括对抗性提示挖掘（通过错误标签激活特征映射）和对抗性样本校正（构建全局原型）。

Result: 实验表明AdvCP显著提升了性能，且适用于其他多类弱监督密集预测任务。

Conclusion: AdvCP能有效解决WSCD中的噪声问题，无需额外推理成本，具有广泛适用性。

Abstract: Weakly-Supervised Change Detection (WSCD) aims to distinguish specific object
changes (e.g., objects appearing or disappearing) from background variations
(e.g., environmental changes due to light, weather, or seasonal shifts) in
paired satellite images, relying only on paired image (i.e., image-level)
classification labels. This technique significantly reduces the need for dense
annotations required in fully-supervised change detection. However, as
image-level supervision only indicates whether objects have changed in a scene,
WSCD methods often misclassify background variations as object changes,
especially in complex remote-sensing scenarios. In this work, we propose an
Adversarial Class Prompting (AdvCP) method to address this co-occurring noise
problem, including two phases: a) Adversarial Prompt Mining: After each
training iteration, we introduce adversarial prompting perturbations, using
incorrect one-hot image-level labels to activate erroneous feature mappings.
This process reveals co-occurring adversarial samples under weak supervision,
namely background variation features that are likely to be misclassified as
object changes. b) Adversarial Sample Rectification: We integrate these
adversarially prompt-activated pixel samples into training by constructing an
online global prototype. This prototype is built from an exponentially weighted
moving average of the current batch and all historical training data. Our AdvCP
can be seamlessly integrated into current WSCD methods without adding
additional inference cost. Experiments on ConvNet, Transformer, and Segment
Anything Model (SAM)-based baselines demonstrate significant performance
enhancements. Furthermore, we demonstrate the generalizability of AdvCP to
other multi-class weakly-supervised dense prediction scenarios. Code is
available at https://github.com/zhenghuizhao/AdvCP

</details>


### [66] [MMCIG: Multimodal Cover Image Generation for Text-only Documents and Its Dataset Construction via Pseudo-labeling](https://arxiv.org/abs/2508.17199)
*Hyeyeon Kim,Sungwoo Han,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CV

TL;DR: 提出了一种新颖的封面图像生成任务，通过多模态伪标注方法构建高质量数据集，实验证明该方法优于仅文本或仅图像的伪标注方法。


<details>
  <summary>Details</summary>
Motivation: 现有数据集无法满足从纯文本生成摘要和对应图像的任务需求，因此提出低成本构建高质量数据集的方法。

Method: 通过收集含多图像和摘要的文档，排除事实不一致实例，独立排名图像和标题，选择排名第一的图像和标题作为伪标注，并移除含直接图像引用的文档。

Result: 多模态伪标注方法构建的数据集更精确，生成的图像质量高于仅文本或仅图像的方法。

Conclusion: 提出的多模态伪标注方法有效解决了任务需求，并展示了高质量图像生成的潜力。

Abstract: In this study, we introduce a novel cover image generation task that produces
both a concise summary and a visually corresponding image from a given
text-only document. Because no existing datasets are available for this task,
we propose a multimodal pseudo-labeling method to construct high-quality
datasets at low cost. We first collect documents that contain multiple images
with their captions, and their summaries by excluding factually inconsistent
instances. Our approach selects one image from the multiple images accompanying
the documents. Using the gold summary, we independently rank both the images
and their captions. Then, we annotate a pseudo-label for an image when both the
image and its corresponding caption are ranked first in their respective
rankings. Finally, we remove documents that contain direct image references
within texts. Experimental results demonstrate that the proposed multimodal
pseudo-labeling method constructs more precise datasets and generates higher
quality images than text- and image-only pseudo-labeling methods, which
consider captions and images separately. We release our code at:
https://github.com/HyeyeeonKim/MMCIG

</details>


### [67] [Multi-Agent Visual-Language Reasoning for Comprehensive Highway Scene Understanding](https://arxiv.org/abs/2508.17205)
*Yunxiang Yang,Ningning Xu,Jidong J. Yang*

Main category: cs.CV

TL;DR: 该论文提出了一种基于多专家策略的多智能体框架，用于高速公路场景的全面理解，通过结合领域知识生成任务特定的思维链提示，指导小型高效视觉语言模型进行多任务推理。


<details>
  <summary>Details</summary>
Motivation: 旨在解决高速公路场景中多种关键感知任务（如天气分类、路面湿滑评估和交通拥堵检测）的复杂性和计算效率问题。

Method: 采用混合专家策略，利用大型通用视觉语言模型生成任务特定的思维链提示，指导小型高效模型进行多模态推理。

Result: 实验结果表明，该框架在多样化的交通和环境条件下表现优异，并能与现有交通摄像头系统集成。

Conclusion: 该框架能够增强情境感知能力，并在资源受限的环境中提供及时警报，适用于高风险农村地区。

Abstract: This paper introduces a multi-agent framework for comprehensive highway scene
understanding, designed around a mixture-of-experts strategy. In this
framework, a large generic vision-language model (VLM), such as GPT-4o, is
contextualized with domain knowledge to generates task-specific
chain-of-thought (CoT) prompts. These fine-grained prompts are then used to
guide a smaller, efficient VLM (e.g., Qwen2.5-VL-7B) in reasoning over short
videos, along with complementary modalities as applicable. The framework
simultaneously addresses multiple critical perception tasks, including weather
classification, pavement wetness assessment, and traffic congestion detection,
achieving robust multi-task reasoning while balancing accuracy and
computational efficiency. To support empirical validation, we curated three
specialized datasets aligned with these tasks. Notably, the pavement wetness
dataset is multimodal, combining video streams with road weather sensor data,
highlighting the benefits of multimodal reasoning. Experimental results
demonstrate consistently strong performance across diverse traffic and
environmental conditions. From a deployment perspective, the framework can be
readily integrated with existing traffic camera systems and strategically
applied to high-risk rural locations, such as sharp curves, flood-prone
lowlands, or icy bridges. By continuously monitoring the targeted sites, the
system enhances situational awareness and delivers timely alerts, even in
resource-constrained environments.

</details>


### [68] [Multi-modal Knowledge Decomposition based Online Distillation for Biomarker Prediction in Breast Cancer Histopathology](https://arxiv.org/abs/2508.17213)
*Qibin Zhang,Xinyu Hao,Qiao Chen,Rui Xu,Fengyu Cong,Cheng Lu,Hongming Xu*

Main category: cs.CV

TL;DR: 提出了一种基于多模态知识分解的在线蒸馏方法，用于增强H&E染色组织病理学图像中的IHC生物标志物预测。


<details>
  <summary>Details</summary>
Motivation: 多模态数据（如基因组和病理信息）的同时获取常因成本或技术限制而困难，需解决此问题。

Method: 开发了两教师一学生模型，通过最小化MKD损失提取模态特定和模态通用特征，并应用SKD和CLOD促进模型间学习。

Result: 在TCGA-BRCA和QHSU数据集上，该方法在单模态数据下的IHC生物标志物预测中表现优异。

Conclusion: 该方法有效提升了单模态数据下的预测性能，为多模态数据融合提供了新思路。

Abstract: Immunohistochemical (IHC) biomarker prediction benefits from multi-modal data
fusion analysis. However, the simultaneous acquisition of multi-modal data,
such as genomic and pathological information, is often challenging due to cost
or technical limitations. To address this challenge, we propose an online
distillation approach based on Multi-modal Knowledge Decomposition (MKD) to
enhance IHC biomarker prediction in haematoxylin and eosin (H\&E) stained
histopathology images. This method leverages paired genomic-pathology data
during training while enabling inference using either pathology slides alone or
both modalities. Two teacher and one student models are developed to extract
modality-specific and modality-general features by minimizing the MKD loss. To
maintain the internal structural relationships between samples,
Similarity-preserving Knowledge Distillation (SKD) is applied. Additionally,
Collaborative Learning for Online Distillation (CLOD) facilitates mutual
learning between teacher and student models, encouraging diverse and
complementary learning dynamics. Experiments on the TCGA-BRCA and in-house QHSU
datasets demonstrate that our approach achieves superior performance in IHC
biomarker prediction using uni-modal data. Our code is available at
https://github.com/qiyuanzz/MICCAI2025_MKD.

</details>


### [69] [Deep Learning with Self-Attention and Enhanced Preprocessing for Precise Diagnosis of Acute Lymphoblastic Leukemia from Bone Marrow Smears in Hemato-Oncology](https://arxiv.org/abs/2508.17216)
*Md. Maruf,Md. Mahbubul Haque,Bishowjit Paul*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的自动化急性淋巴细胞白血病（ALL）诊断框架，结合预处理和注意力机制，显著提高了诊断准确率。


<details>
  <summary>Details</summary>
Motivation: ALL的早期准确诊断和分型对治疗至关重要，传统方法复杂且易出错，需要更高效、准确的自动化工具。

Method: 采用VGG19+多头自注意力（MHSA）块和Focal Loss，结合预处理流程，优化细胞形态特征表示。

Result: 增强的VGG19+MHSA模型达到99.25%的准确率，优于ResNet101基线（98.62%）。

Conclusion: 注意力增强的CNN结合针对性损失优化和预处理，为ALL诊断提供了高效、准确的自动化解决方案。

Abstract: Acute lymphoblastic leukemia (ALL) is a prevalent hematological malignancy in
both pediatric and adult populations. Early and accurate detection with precise
subtyping is essential for guiding therapy. Conventional workflows are complex,
time-consuming, and prone to human error. We present a deep learning framework
for automated ALL diagnosis from bone marrow smear images. The method combines
a robust preprocessing pipeline with convolutional neural networks (CNNs) to
standardize image quality and improve inference efficiency. As a key design, we
insert a multi-head self-attention (MHSA) block into a VGG19 backbone to model
long-range dependencies and contextual relationships among cellular features.
To mitigate class imbalance, we train with Focal Loss. Across evaluated
architectures, the enhanced VGG19+MHSA trained with Focal Loss achieves 99.25%
accuracy, surpassing a strong ResNet101 baseline (98.62%). These results
indicate that attention-augmented CNNs, coupled with targeted loss optimization
and preprocessing, yield more discriminative representations of leukemic cell
morphology. Our approach offers a highly accurate and computationally efficient
tool for automated ALL recognition and subtyping, with potential to accelerate
diagnostic workflows and support reliable decision-making in clinical settings.

</details>


### [70] [4D Visual Pre-training for Robot Learning](https://arxiv.org/abs/2508.17230)
*Chengkai Hou,Yanjie Ze,Yankai Fu,Zeyu Gao,Songbo Hu,Yue Yu,Shanghang Zhang,Huazhe Xu*

Main category: cs.CV

TL;DR: 论文提出了一种名为FVP的4D视觉预训练框架，旨在通过点云预测任务提升3D表示的性能，并在机器人任务中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉表示主要基于2D图像，忽略了世界的3D本质，而大规模3D数据的稀缺使得直接从网络数据中提取通用3D表示困难。因此，作者寻求一种能够提升所有3D表示的通用预训练框架。

Method: FVP框架将视觉预训练目标定义为点云预测问题，使用扩散模型建模预测模型，并在公开数据集上进行预训练。

Result: 在12个真实世界机器人任务中，FVP将3D Diffusion Policy的平均成功率提升了28%，并在模仿学习方法中达到最先进性能。此外，FVP在不同点云编码器和数据集上均表现出有效性。

Conclusion: FVP是一种有效的4D视觉预训练框架，能够显著提升3D表示的性能，并在机器人任务中展现出广泛适用性。

Abstract: General visual representations learned from web-scale datasets for robotics
have achieved great success in recent years, enabling data-efficient robot
learning on manipulation tasks; yet these pre-trained representations are
mostly on 2D images, neglecting the inherent 3D nature of the world. However,
due to the scarcity of large-scale 3D data, it is still hard to extract a
universal 3D representation from web datasets. Instead, we are seeking a
general visual pre-training framework that could improve all 3D representations
as an alternative. Our framework, called FVP, is a novel 4D Visual Pre-training
framework for real-world robot learning. FVP frames the visual pre-training
objective as a next-point-cloud-prediction problem, models the prediction model
as a diffusion model, and pre-trains the model on the larger public datasets
directly. Across twelve real-world manipulation tasks, FVP boosts the average
success rate of 3D Diffusion Policy (DP3) for these tasks by 28%. The FVP
pre-trained DP3 achieves state-of-the-art performance across imitation learning
methods. Moreover, the efficacy of FVP adapts across various point cloud
encoders and datasets. Finally, we apply FVP to the RDT-1B, a larger
Vision-Language-Action robotic model, enhancing its performance on various
robot tasks. Our project page is available at: https://4d-
visual-pretraining.github.io/.

</details>


### [71] [PersPose: 3D Human Pose Estimation with Perspective Encoding and Perspective Rotation](https://arxiv.org/abs/2508.17239)
*Xiaoyang Hao,Han Li*

Main category: cs.CV

TL;DR: PersPose通过Perspective Encoding和Perspective Rotation改进单目3D人体姿态估计，在多个数据集上达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅使用裁剪图像作为输入，忽略了相机内参和透视关系，导致深度估计不准确。

Method: 提出Perspective Encoding编码相机内参，Perspective Rotation减少透视畸变，结合为PersPose框架。

Result: 在3DPW、MPIINF-3DHP和Human3.6M数据集上表现优异，如3DPW的MPJPE降低7.54%。

Conclusion: PersPose通过结合相机内参和透视校正，显著提升了3D人体姿态估计的精度。

Abstract: Monocular 3D human pose estimation (HPE) methods estimate the 3D positions of
joints from individual images. Existing 3D HPE approaches often use the cropped
image alone as input for their models. However, the relative depths of joints
cannot be accurately estimated from cropped images without the corresponding
camera intrinsics, which determine the perspective relationship between 3D
objects and the cropped images. In this work, we introduce Perspective Encoding
(PE) to encode the camera intrinsics of the cropped images. Moreover, since the
human subject can appear anywhere within the original image, the perspective
relationship between the 3D scene and the cropped image differs significantly,
which complicates model fitting. Additionally, the further the human subject
deviates from the image center, the greater the perspective distortions in the
cropped image. To address these issues, we propose Perspective Rotation (PR), a
transformation applied to the original image that centers the human subject,
thereby reducing perspective distortions and alleviating the difficulty of
model fitting. By incorporating PE and PR, we propose a novel 3D HPE framework,
PersPose. Experimental results demonstrate that PersPose achieves
state-of-the-art (SOTA) performance on the 3DPW, MPIINF-3DHP, and Human3.6M
datasets. For example, on the in-the-wild dataset 3DPW, PersPose achieves an
MPJPE of 60.1 mm, 7.54% lower than the previous SOTA approach. Code is
available at: https://github.com/ KenAdamsJoseph/PersPose.

</details>


### [72] [CoViPAL: Layer-wise Contextualized Visual Token Pruning for Large Vision-Language Models](https://arxiv.org/abs/2508.17243)
*Zicong Tang,Ziyang Ma,Suqing Wang,Zuchao Li,Lefei Zhang,Hai Zhao,Yun Li,Qianren Wang*

Main category: cs.CV

TL;DR: CoViPAL是一种层级的上下文视觉令牌修剪方法，通过轻量级插件模块（PPM）在LVLM处理前预测并移除冗余视觉令牌，显著提升推理效率且不损失准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在浅层修剪冗余视觉令牌时因缺乏足够上下文信息而效果不佳，但许多视觉令牌在浅层即具有冗余性，可通过适当上下文信号安全修剪。

Method: 提出CoViPAL，采用轻量级、模型无关的PPM模块，在LVLM处理前预测并移除冗余视觉令牌。

Result: 在多个基准测试中，CoViPAL在相同令牌预算下优于无训练修剪方法，并在可比监督下超越基于训练的方法。

Conclusion: CoViPAL为LVLM提供了一种可扩展且高效的推理效率提升方案，无需牺牲准确性。

Abstract: Large Vision-Language Models (LVLMs) process multimodal inputs consisting of
text tokens and vision tokens extracted from images or videos. Due to the rich
visual information, a single image can generate thousands of vision tokens,
leading to high computational costs during the prefilling stage and significant
memory overhead during decoding. Existing methods attempt to prune redundant
vision tokens, revealing substantial redundancy in visual representations.
However, these methods often struggle in shallow layers due to the lack of
sufficient contextual information. We argue that many visual tokens are
inherently redundant even in shallow layers and can be safely and effectively
pruned with appropriate contextual signals. In this work, we propose CoViPAL, a
layer-wise contextualized visual token pruning method that employs a
Plug-and-Play Pruning Module (PPM) to predict and remove redundant vision
tokens before they are processed by the LVLM. The PPM is lightweight,
model-agnostic, and operates independently of the LVLM architecture, ensuring
seamless integration with various models. Extensive experiments on multiple
benchmarks demonstrate that CoViPAL outperforms training-free pruning methods
under equal token budgets and surpasses training-based methods with comparable
supervision. CoViPAL offers a scalable and efficient solution to improve
inference efficiency in LVLMs without compromising accuracy.

</details>


### [73] [Uncovering and Mitigating Destructive Multi-Embedding Attacks in Deepfake Proactive Forensics](https://arxiv.org/abs/2508.17247)
*Lixin Jia,Haiyang Sun,Zhiqing Guo,Yunfeng Diao,Dan Ma,Gaobo Yang*

Main category: cs.CV

TL;DR: 论文提出了一种对抗多嵌入攻击（MEA）的训练范式AIS，通过模拟攻击场景和引入稀疏稳定的水印表示，增强了现有方法的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展，个人隐私面临严重威胁。现有的主动取证方法依赖单一水印嵌入的假设，但在实际场景中可能因多次嵌入而失效。

Method: 提出Adversarial Interference Simulation（AIS）训练范式，通过模拟多嵌入攻击场景和设计稀疏稳定的损失函数，增强水印的鲁棒性。

Result: 实验表明，AIS显著提升了现有方法对多嵌入攻击的抵抗能力，确保原始水印在多次嵌入后仍可正确提取。

Conclusion: AIS是一种即插即用的训练范式，有效解决了多嵌入攻击对主动取证机制的威胁，为深度伪造防御提供了新思路。

Abstract: With the rapid evolution of deepfake technologies and the wide dissemination
of digital media, personal privacy is facing increasingly serious security
threats. Deepfake proactive forensics, which involves embedding imperceptible
watermarks to enable reliable source tracking, serves as a crucial defense
against these threats. Although existing methods show strong forensic ability,
they rely on an idealized assumption of single watermark embedding, which
proves impractical in real-world scenarios. In this paper, we formally define
and demonstrate the existence of Multi-Embedding Attacks (MEA) for the first
time. When a previously protected image undergoes additional rounds of
watermark embedding, the original forensic watermark can be destroyed or
removed, rendering the entire proactive forensic mechanism ineffective. To
address this vulnerability, we propose a general training paradigm named
Adversarial Interference Simulation (AIS). Rather than modifying the network
architecture, AIS explicitly simulates MEA scenarios during fine-tuning and
introduces a resilience-driven loss function to enforce the learning of sparse
and stable watermark representations. Our method enables the model to maintain
the ability to extract the original watermark correctly even after a second
embedding. Extensive experiments demonstrate that our plug-and-play AIS
training paradigm significantly enhances the robustness of various existing
methods against MEA.

</details>


### [74] [A biological vision inspired framework for machine perception of abutting grating illusory contours](https://arxiv.org/abs/2508.17254)
*Xiao Zhang,Kai-Fu Yang,Xian-Shi Zhang,Hong-Zhi You,Hong-Mei Yan,Yong-Jie Li*

Main category: cs.CV

TL;DR: 论文提出了一种名为ICPNet的新型深度网络，旨在解决深度神经网络（DNN）在感知错觉轮廓（如abutting grating）时与人类感知不一致的问题。通过多尺度特征投影（MFP）模块、特征交互注意力模块（FIAM）和边缘融合模块（EFM），ICPNet显著提升了模型对错觉轮廓的敏感性，并在实验中表现出优于现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前DNN在机器智能中表现优异，但在感知错觉轮廓时与人类感知存在差异，这限制了其与人类认知的对齐。论文旨在通过模拟视觉皮层电路设计新型网络，缩小这一差距。

Method: 提出ICPNet，包含多尺度特征投影（MFP）模块、特征交互注意力模块（FIAM）和边缘融合模块（EFM）。MFP提取多尺度特征，FIAM增强前馈与反馈特征的交互，EFM通过边缘检测任务注入形状约束。

Result: 在AG-MNIST和AG-Fashion-MNIST测试集上，ICPNet对abutting grating错觉轮廓的敏感性显著优于现有模型，top-1准确率有显著提升。

Conclusion: ICPNet在感知错觉轮廓方面取得了重要进展，为DNN模型实现人类水平智能迈出了一步。

Abstract: Higher levels of machine intelligence demand alignment with human perception
and cognition. Deep neural networks (DNN) dominated machine intelligence have
demonstrated exceptional performance across various real-world tasks.
Nevertheless, recent evidence suggests that DNNs fail to perceive illusory
contours like the abutting grating, a discrepancy that misaligns with human
perception patterns. Departing from previous works, we propose a novel deep
network called illusory contour perception network (ICPNet) inspired by the
circuits of the visual cortex. In ICPNet, a multi-scale feature projection
(MFP) module is designed to extract multi-scale representations. To boost the
interaction between feedforward and feedback features, a feature interaction
attention module (FIAM) is introduced. Moreover, drawing inspiration from the
shape bias observed in human perception, an edge detection task conducted via
the edge fusion module (EFM) injects shape constraints that guide the network
to concentrate on the foreground. We assess our method on the existing AG-MNIST
test set and the AG-Fashion-MNIST test sets constructed by this work.
Comprehensive experimental results reveal that ICPNet is significantly more
sensitive to abutting grating illusory contours than state-of-the-art models,
with notable improvements in top-1 accuracy across various subsets. This work
is expected to make a step towards human-level intelligence for DNN-based
models.

</details>


### [75] [SEER-VAR: Semantic Egocentric Environment Reasoner for Vehicle Augmented Reality](https://arxiv.org/abs/2508.17255)
*Yuzhi Lai,Shenghai Yuan,Peizheng Li,Jun Lou,Andreas Zell*

Main category: cs.CV

TL;DR: SEER-VAR是一个新颖的基于车辆的增强现实（AR）框架，结合了语义分解、上下文感知SLAM分支和LLM驱动的推荐系统，适用于动态驾驶场景。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常假设静态或单视图设置，无法满足动态驾驶场景的需求，SEER-VAR旨在解决这一问题。

Method: 通过深度引导的视觉-语言基础动态分离驾驶舱和道路场景，使用两个SLAM分支跟踪运动，并利用GPT模块生成上下文感知的AR覆盖。

Result: 实验表明，SEER-VAR在多样环境中实现了鲁棒的空间对齐和感知一致的AR渲染，提升了场景理解和覆盖相关性。

Conclusion: SEER-VAR为未来研究提供了有效基础，代码和数据集将开源。

Abstract: We present SEER-VAR, a novel framework for egocentric vehicle-based augmented
reality (AR) that unifies semantic decomposition, Context-Aware SLAM Branches
(CASB), and LLM-driven recommendation. Unlike existing systems that assume
static or single-view settings, SEER-VAR dynamically separates cabin and road
scenes via depth-guided vision-language grounding. Two SLAM branches track
egocentric motion in each context, while a GPT-based module generates
context-aware overlays such as dashboard cues and hazard alerts. To support
evaluation, we introduce EgoSLAM-Drive, a real-world dataset featuring
synchronized egocentric views, 6DoF ground-truth poses, and AR annotations
across diverse driving scenarios. Experiments demonstrate that SEER-VAR
achieves robust spatial alignment and perceptually coherent AR rendering across
varied environments. As one of the first to explore LLM-based AR recommendation
in egocentric driving, we address the lack of comparable systems through
structured prompting and detailed user studies. Results show that SEER-VAR
enhances perceived scene understanding, overlay relevance, and driver ease,
providing an effective foundation for future research in this direction. Code
and dataset will be made open source.

</details>


### [76] [ResLink: A Novel Deep Learning Architecture for Brain Tumor Classification with Area Attention and Residual Connections](https://arxiv.org/abs/2508.17259)
*Sumedha Arya,Nirmal Gaud*

Main category: cs.CV

TL;DR: ResLink是一种新型深度学习架构，用于CT扫描图像的脑肿瘤分类，结合了区域注意力机制和残差连接，准确率达95%。


<details>
  <summary>Details</summary>
Motivation: 脑肿瘤对神经功能有严重影响，早期准确诊断对治疗至关重要。

Method: ResLink采用多阶段卷积管道，结合注意力机制和残差连接，增强特征学习和空间理解。

Result: 模型在平衡数据集上训练，准确率达95%，表现出强泛化能力。

Conclusion: ResLink展示了在脑肿瘤分类中的潜力，为医学影像应用提供了高效技术。

Abstract: Brain tumors show significant health challenges due to their potential to
cause critical neurological functions. Early and accurate diagnosis is crucial
for effective treatment. In this research, we propose ResLink, a novel deep
learning architecture for brain tumor classification using CT scan images.
ResLink integrates novel area attention mechanisms with residual connections to
enhance feature learning and spatial understanding for spatially rich image
classification tasks. The model employs a multi-stage convolutional pipeline,
incorporating dropout, regularization, and downsampling, followed by a final
attention-based refinement for classification. Trained on a balanced dataset,
ResLink achieves a high accuracy of 95% and demonstrates strong
generalizability. This research demonstrates the potential of ResLink in
improving brain tumor classification, offering a robust and efficient technique
for medical imaging applications.

</details>


### [77] [CLIFF: Continual Learning for Incremental Flake Features in 2D Material Identification](https://arxiv.org/abs/2508.17261)
*Sankalp Pandey,Xuan Bac Nguyen,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 提出了一种新的持续学习框架CLIFF，用于二维材料薄片层分类，解决了光学显微镜下材料外观变化带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 自动化薄片层分类在量子硬件中至关重要，但不同材料的外观变化使得分类困难。

Method: 通过冻结主干和基础头，学习材料特定的提示、嵌入和增量头，结合提示池和余弦相似性门调制特征。

Result: CLIFF在保持高精度的同时，显著降低了遗忘率。

Conclusion: CLIFF是首个在二维材料领域系统研究持续学习的方法，具有实际应用潜力。

Abstract: Identifying quantum flakes is crucial for scalable quantum hardware; however,
automated layer classification from optical microscopy remains challenging due
to substantial appearance shifts across different materials. In this paper, we
propose a new Continual-Learning Framework for Flake Layer Classification
(CLIFF). To our knowledge, this is the first systematic study of continual
learning in the domain of two-dimensional (2D) materials. Our method enables
the model to differentiate between materials and their physical and optical
properties by freezing a backbone and base head trained on a reference
material. For each new material, it learns a material-specific prompt,
embedding, and a delta head. A prompt pool and a cosine-similarity gate
modulate features and compute material-specific corrections. Additionally, we
incorporate memory replay with knowledge distillation. CLIFF achieves
competitive accuracy with significantly lower forgetting than naive fine-tuning
and a prompt-based baseline.

</details>


### [78] [AdaGAT: Adaptive Guidance Adversarial Training for the Robustness of Deep Neural Networks](https://arxiv.org/abs/2508.17265)
*Zhenyu Liu,Huizhi Liang,Xinrun Li,Vaclav Snasel,Varun Ojha*

Main category: cs.CV

TL;DR: AdaGAT是一种动态调整引导模型训练状态的方法，通过两种损失函数提升学生模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有对抗蒸馏方法中，可学习的引导模型难以在共同训练中保持最优状态，影响知识传递效果。

Method: 提出AdaGAT方法，动态调整引导模型的训练状态，并设计两种损失函数以优化其参与反向传播。

Result: 在CIFAR-10等数据集上实验表明，AdaGAT能显著提升目标模型对抗攻击的鲁棒性。

Conclusion: AdaGAT通过动态调整引导模型，有效增强了学生模型的鲁棒性，优于多种基线方法。

Abstract: Adversarial distillation (AD) is a knowledge distillation technique that
facilitates the transfer of robustness from teacher deep neural network (DNN)
models to lightweight target (student) DNN models, enabling the target models
to perform better than only training the student model independently. Some
previous works focus on using a small, learnable teacher (guide) model to
improve the robustness of a student model. Since a learnable guide model starts
learning from scratch, maintaining its optimal state for effective knowledge
transfer during co-training is challenging. Therefore, we propose a novel
Adaptive Guidance Adversarial Training (AdaGAT) method. Our method, AdaGAT,
dynamically adjusts the training state of the guide model to install robustness
to the target model. Specifically, we develop two separate loss functions as
part of the AdaGAT method, allowing the guide model to participate more
actively in backpropagation to achieve its optimal state. We evaluated our
approach via extensive experiments on three datasets: CIFAR-10, CIFAR-100, and
TinyImageNet, using the WideResNet-34-10 model as the target model. Our
observations reveal that appropriately adjusting the guide model within a
certain accuracy range enhances the target model's robustness across various
adversarial attacks compared to a variety of baseline models.

</details>


### [79] [Spatial-Temporal Human-Object Interaction Detection](https://arxiv.org/abs/2508.17270)
*Xu Sun,Yunqing He,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了一种新的视频实例级人-物交互检测任务ST-HOID，用于区分细粒度交互及轨迹，并构建了首个数据集VidOR-HOID。


<details>
  <summary>Details</summary>
Motivation: 人-物交互（HOI）对以人为中心的视频内容理解至关重要。

Method: 结合目标轨迹检测模块和交互推理模块的新方法。

Result: 实验表明，该方法在多个基线任务上表现优于现有技术。

Conclusion: ST-HOID任务及方法在视频人-物交互检测中具有显著优势。

Abstract: In this paper, we propose a new instance-level human-object interaction
detection task on videos called ST-HOID, which aims to distinguish fine-grained
human-object interactions (HOIs) and the trajectories of subjects and objects.
It is motivated by the fact that HOI is crucial for human-centric video content
understanding. To solve ST-HOID, we propose a novel method consisting of an
object trajectory detection module and an interaction reasoning module.
Furthermore, we construct the first dataset named VidOR-HOID for ST-HOID
evaluation, which contains 10,831 spatial-temporal HOI instances. We conduct
extensive experiments to evaluate the effectiveness of our method. The
experimental results demonstrate that our method outperforms the baselines
generated by the state-of-the-art methods of image human-object interaction
detection, video visual relation detection and video human-object interaction
recognition.

</details>


### [80] [Deep Learning-Assisted Detection of Sarcopenia in Cross-Sectional Computed Tomography Imaging](https://arxiv.org/abs/2508.17275)
*Manish Bhardwaj,Huizhi Liang,Ashwin Sivaharan,Sandip Nandhra,Vaclav Snasel,Tamer El-Sayed,Varun Ojha*

Main category: cs.CV

TL;DR: 利用深度学习模型自动测量CT图像中的骨骼肌面积（SMA），以高效评估肌肉减少症（sarcopenia）。


<details>
  <summary>Details</summary>
Motivation: 肌肉减少症与不良手术结果相关，但传统评估方法耗时且增加临床负担，需要更高效的自动化解决方案。

Method: 采用迁移学习和自监督学习方法，利用标记和未标记的CT扫描数据集开发深度学习模型。

Result: 模型预测SMA的平均误差为±3%，预测掩模的平均Dice相似系数为93%。

Conclusion: 该方法为肌肉减少症的自动化评估和检测提供了可行路径。

Abstract: Sarcopenia is a progressive loss of muscle mass and function linked to poor
surgical outcomes such as prolonged hospital stays, impaired mobility, and
increased mortality. Although it can be assessed through cross-sectional
imaging by measuring skeletal muscle area (SMA), the process is time-consuming
and adds to clinical workloads, limiting timely detection and management;
however, this process could become more efficient and scalable with the
assistance of artificial intelligence applications. This paper presents
high-quality three-dimensional cross-sectional computed tomography (CT) images
of patients with sarcopenia collected at the Freeman Hospital, Newcastle upon
Tyne Hospitals NHS Foundation Trust. Expert clinicians manually annotated the
SMA at the third lumbar vertebra, generating precise segmentation masks. We
develop deep-learning models to measure SMA in CT images and automate this
task. Our methodology employed transfer learning and self-supervised learning
approaches using labelled and unlabeled CT scan datasets. While we developed
qualitative assessment models for detecting sarcopenia, we observed that the
quantitative assessment of SMA is more precise and informative. This approach
also mitigates the issue of class imbalance and limited data availability. Our
model predicted the SMA, on average, with an error of +-3 percentage points
against the manually measured SMA. The average dice similarity coefficient of
the predicted masks was 93%. Our results, therefore, show a pathway to full
automation of sarcopenia assessment and detection.

</details>


### [81] [MTNet: Learning modality-aware representation with transformer for RGBT tracking](https://arxiv.org/abs/2508.17280)
*Ruichao Hou,Boyue Xu,Tongwei Ren,Gangshan Wu*

Main category: cs.CV

TL;DR: MTNet是一种基于Transformer的多模态感知跟踪器，通过模态感知网络和Transformer融合网络提升RGB-T跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 常规的融合范式和固定跟踪模板限制了RGB-T跟踪中的特征交互，需要更灵活的方法来提升性能。

Method: 提出模态感知网络（包含CADM和SSPM模块）和Transformer融合网络，结合三叉预测头和动态更新策略。

Result: 在三个RGBT基准测试中表现优异，达到实时速度。

Conclusion: MTNet通过模态感知和全局依赖捕捉，显著提升了RGB-T跟踪的鲁棒性和准确性。

Abstract: The ability to learn robust multi-modality representation has played a
critical role in the development of RGBT tracking. However, the regular fusion
paradigm and the invariable tracking template remain restrictive to the feature
interaction. In this paper, we propose a modality-aware tracker based on
transformer, termed MTNet. Specifically, a modality-aware network is presented
to explore modality-specific cues, which contains both channel aggregation and
distribution module(CADM) and spatial similarity perception module (SSPM). A
transformer fusion network is then applied to capture global dependencies to
reinforce instance representations. To estimate the precise location and tackle
the challenges, such as scale variation and deformation, we design a trident
prediction head and a dynamic update strategy which jointly maintain a reliable
template for facilitating inter-frame communication. Extensive experiments
validate that the proposed method achieves satisfactory results compared with
the state-of-the-art competitors on three RGBT benchmarks while reaching
real-time speed.

</details>


### [82] [Quickly Tuning Foundation Models for Image Segmentation](https://arxiv.org/abs/2508.17283)
*Breenda Das,Lennart Purucker,Timur Carstensen,Frank Hutter*

Main category: cs.CV

TL;DR: QTT-SEG是一种基于元学习的自动化方法，用于快速优化SAM模型在特定领域的图像分割任务。


<details>
  <summary>Details</summary>
Motivation: SAM等基础模型在零样本图像分割中表现优异，但在特定领域任务中表现不足，且手动微调需要大量专业知识和努力。

Method: QTT-SEG利用元学习的成本和性能模型，在超过2亿种可能的配置中高效预测最佳参数，基于Quick-Tune框架实现。

Result: 在8个二分类和5个多分类数据集上，QTT-SEG在短时间内显著提升SAM的零样本性能，并在多数任务中超越AutoGluon Multimodal。

Conclusion: QTT-SEG展示了元学习在自动化模型适应特定分割任务中的潜力。

Abstract: Foundation models like SAM (Segment Anything Model) exhibit strong zero-shot
image segmentation performance, but often fall short on domain-specific tasks.
Fine-tuning these models typically requires significant manual effort and
domain expertise. In this work, we introduce QTT-SEG, a meta-learning-driven
approach for automating and accelerating the fine-tuning of SAM for image
segmentation. Built on the Quick-Tune hyperparameter optimization framework,
QTT-SEG predicts high-performing configurations using meta-learned cost and
performance models, efficiently navigating a search space of over 200 million
possibilities. We evaluate QTT-SEG on eight binary and five multiclass
segmentation datasets under tight time constraints. Our results show that
QTT-SEG consistently improves upon SAM's zero-shot performance and surpasses
AutoGluon Multimodal, a strong AutoML baseline, on most binary tasks within
three minutes. On multiclass datasets, QTT-SEG delivers consistent gains as
well. These findings highlight the promise of meta-learning in automating model
adaptation for specialized segmentation tasks. Code available at:
https://github.com/ds-brx/QTT-SEG/

</details>


### [83] [Explain Before You Answer: A Survey on Compositional Visual Reasoning](https://arxiv.org/abs/2508.17298)
*Fucai Ke,Joy Hsu,Zhixi Cai,Zixian Ma,Xin Zheng,Xindi Wu,Sukai Huang,Weiqing Wang,Pari Delir Haghighi,Gholamreza Haffari,Ranjay Krishna,Jiajun Wu,Hamid Rezatofighi*

Main category: cs.CV

TL;DR: 该论文是一篇关于组合视觉推理的综合综述，系统回顾了2023至2025年的260多篇论文，总结了核心定义、范式转变、基准测试及未来方向。


<details>
  <summary>Details</summary>
Motivation: 填补组合视觉推理文献的空白，为研究提供统一分类和历史路线图，推动下一代研究。

Method: 系统综述260+篇论文，分析五阶段范式转变，并分类60+基准测试及其指标。

Result: 总结了组合视觉推理的优势、挑战和未来方向，如认知对齐、数据效率和工具集成等。

Conclusion: 该综述为组合视觉推理研究提供了基础参考，并指出了未来研究方向。

Abstract: Compositional visual reasoning has emerged as a key research frontier in
multimodal AI, aiming to endow machines with the human-like ability to
decompose visual scenes, ground intermediate concepts, and perform multi-step
logical inference. While early surveys focus on monolithic vision-language
models or general multimodal reasoning, a dedicated synthesis of the rapidly
expanding compositional visual reasoning literature is still missing. We fill
this gap with a comprehensive survey spanning 2023 to 2025 that systematically
reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We
first formalize core definitions and describe why compositional approaches
offer advantages in cognitive alignment, semantic fidelity, robustness,
interpretability, and data efficiency. Next, we trace a five-stage paradigm
shift: from prompt-enhanced language-centric pipelines, through tool-enhanced
LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and
unified agentic VLMs, highlighting their architectural designs, strengths, and
limitations. We then catalog 60+ benchmarks and corresponding metrics that
probe compositional visual reasoning along dimensions such as grounding
accuracy, chain-of-thought faithfulness, and high-resolution perception.
Drawing on these analyses, we distill key insights, identify open challenges
(e.g., limitations of LLM-based reasoning, hallucination, a bias toward
deductive reasoning, scalable supervision, tool integration, and benchmark
limitations), and outline future directions, including world-model integration,
human-AI collaborative reasoning, and richer evaluation protocols. By offering
a unified taxonomy, historical roadmap, and critical outlook, this survey aims
to serve as a foundational reference and inspire the next generation of
compositional visual reasoning research.

</details>


### [84] [FoundDiff: Foundational Diffusion Model for Generalizable Low-Dose CT Denoising](https://arxiv.org/abs/2508.17299)
*Zhihao Chen,Qi Gao,Zilong Li,Junping Zhang,Yi Zhang,Jun Zhao,Hongming Shan*

Main category: cs.CV

TL;DR: FoundDiff是一种基于扩散模型的方法，用于统一和通用的低剂量CT去噪，适用于不同剂量水平和解剖区域。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的方法通常在特定剂量水平和解剖区域上训练，难以适应多样化的噪声特征和解剖异质性，限制了其在临床场景中的泛化性和鲁棒性。

Method: FoundDiff采用两阶段策略：(1) 剂量-解剖感知，通过DA-CLIP模型学习连续表示；(2) 自适应去噪，通过DA-Diff模型结合剂量和解剖嵌入进行去噪。

Result: 在两个公共LDCT数据集上的实验表明，FoundDiff在去噪性能和泛化能力上优于现有方法。

Conclusion: FoundDiff通过创新的两阶段策略，显著提升了低剂量CT去噪的泛化性和适应性。

Abstract: Low-dose computed tomography (CT) denoising is crucial for reduced radiation
exposure while ensuring diagnostically acceptable image quality. Despite
significant advancements driven by deep learning (DL) in recent years, existing
DL-based methods, typically trained on a specific dose level and anatomical
region, struggle to handle diverse noise characteristics and anatomical
heterogeneity during varied scanning conditions, limiting their
generalizability and robustness in clinical scenarios. In this paper, we
propose FoundDiff, a foundational diffusion model for unified and generalizable
LDCT denoising across various dose levels and anatomical regions. FoundDiff
employs a two-stage strategy: (i) dose-anatomy perception and (ii) adaptive
denoising. First, we develop a dose- and anatomy-aware contrastive language
image pre-training model (DA-CLIP) to achieve robust dose and anatomy
perception by leveraging specialized contrastive learning strategies to learn
continuous representations that quantify ordinal dose variations and identify
salient anatomical regions. Second, we design a dose- and anatomy-aware
diffusion model (DA-Diff) to perform adaptive and generalizable denoising by
synergistically integrating the learned dose and anatomy embeddings from DACLIP
into diffusion process via a novel dose and anatomy conditional block (DACB)
based on Mamba. Extensive experiments on two public LDCT datasets encompassing
eight dose levels and three anatomical regions demonstrate superior denoising
performance of FoundDiff over existing state-of-the-art methods and the
remarkable generalization to unseen dose levels. The codes and models are
available at https://github.com/hao1635/FoundDiff.

</details>


### [85] [PosBridge: Multi-View Positional Embedding Transplant for Identity-Aware Image Editing](https://arxiv.org/abs/2508.17302)
*Peilin Xiong,Junwen Chen,Honghui Yuan,Keiji Yanai*

Main category: cs.CV

TL;DR: PosBridge是一种无需训练的高效图像编辑框架，通过位置嵌入移植和Corner Centered Layout技术，实现用户指定对象的无缝插入。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型规模的扩大，训练成本急剧增加，需要无需训练且可扩展的编辑框架。

Method: 采用位置嵌入移植技术引导扩散模型复制参考对象的结构特征，并结合Corner Centered Layout输入FLUX.1-Fill模型。

Result: 实验表明，PosBridge在结构一致性、外观保真度和计算效率上优于主流基线。

Conclusion: PosBridge具有实用价值和广泛应用潜力。

Abstract: Localized subject-driven image editing aims to seamlessly integrate
user-specified objects into target scenes. As generative models continue to
scale, training becomes increasingly costly in terms of memory and computation,
highlighting the need for training-free and scalable editing frameworks.To this
end, we propose PosBridge an efficient and flexible framework for inserting
custom objects. A key component of our method is positional embedding
transplant, which guides the diffusion model to faithfully replicate the
structural characteristics of reference objects.Meanwhile, we introduce the
Corner Centered Layout, which concatenates reference images and the background
image as input to the FLUX.1-Fill model. During progressive denoising,
positional embedding transplant is applied to guide the noise distribution in
the target region toward that of the reference object. In this way, Corner
Centered Layout effectively directs the FLUX.1-Fill model to synthesize
identity-consistent content at the desired location. Extensive experiments
demonstrate that PosBridge outperforms mainstream baselines in structural
consistency, appearance fidelity, and computational efficiency, showcasing its
practical value and potential for broad adoption.

</details>


### [86] [First Place Solution to the MLCAS 2025 GWFSS Challenge: The Devil is in the Detail and Minority](https://arxiv.org/abs/2508.17305)
*Songliang Cao,Tianqi Hu,Hao Lu*

Main category: cs.CV

TL;DR: 本文介绍了在MLCAS 2025 GWFSS挑战赛中获胜的解决方案，通过针对小麦茎部的三个技术改进，取得了显著优势。


<details>
  <summary>Details</summary>
Motivation: 当前语义分割竞赛中，许多技巧已被集成到现有代码库中，因此关键在于专注于小麦本身的特性，尤其是茎部的问题。

Method: 提出了三个针对茎部的改进：动态上采样器SAPA、半监督引导蒸馏和测试时缩放策略。

Result: 这些改进使团队在竞赛中取得第一名，显著优于第二名。

Conclusion: 专注于问题本质（如小麦茎部）并针对性改进，是竞赛中脱颖而出的关键。

Abstract: In this report, we present our solution during the participation of the MLCAS
2025 GWFSS Challenge. This challenge hosts a semantic segmentation competition
specific to wheat plants, which requires to segment three wheat organs
including the head, leaf, and stem, and another background class. In 2025,
participating a segmentation competition is significantly different from that
in previous years where many tricks can play important roles. Nowadays most
segmentation tricks have been well integrated into existing codebases such that
our naive ViT-Adapter baseline has already achieved sufficiently good
performance. Hence, we believe the key to stand out among other competitors is
to focus on the problem nature of wheat per se. By probing visualizations, we
identify the key -- the stem matters. In contrast to heads and leaves, stems
exhibit fine structure and occupy only few pixels, which suffers from fragile
predictions and class imbalance. Building on our baseline, we present three
technical improvements tailored to stems: i) incorporating a dynamic upsampler
SAPA used to enhance detail delineation; ii) leveraging semi-supervised guided
distillation with stem-aware sample selection to mine the treasure beneath
unlabeled data; and iii) applying a test-time scaling strategy to zoom in and
segment twice the image. Despite being simple, the three improvements bring us
to the first place of the competition, outperforming the second place by clear
margins. Code and models will be released at
https://github.com/tiny-smart/gwfss25.

</details>


### [87] [Defending Deepfake via Texture Feature Perturbation](https://arxiv.org/abs/2508.17315)
*Xiao Zhang,Changfang Chen,Tianyi Wang*

Main category: cs.CV

TL;DR: 提出了一种基于面部纹理特征的主动Deepfake检测方法，通过在纹理区域插入不可见的扰动来干扰Deepfake生成。


<details>
  <summary>Details</summary>
Motivation: 由于Deepfake技术快速发展，现有被动检测方法难以应对高质量伪造内容，因此需要主动防御手段。

Method: 利用局部二值模式（LBP）提取纹理特征，采用双模型注意力策略生成和优化纹理扰动。

Result: 在CelebA-HQ和LFW数据集上验证了方法的有效性，能显著干扰Deepfake生成并产生明显视觉缺陷。

Conclusion: 该方法为主动Deepfake检测提供了高效且可扩展的解决方案。

Abstract: The rapid development of Deepfake technology poses severe challenges to
social trust and information security. While most existing detection methods
primarily rely on passive analyses, due to unresolvable high-quality Deepfake
contents, proactive defense has recently emerged by inserting invisible signals
in advance of image editing. In this paper, we introduce a proactive Deepfake
detection approach based on facial texture features. Since human eyes are more
sensitive to perturbations in smooth regions, we invisibly insert perturbations
within texture regions that have low perceptual saliency, applying localized
perturbations to key texture regions while minimizing unwanted noise in
non-textured areas. Our texture-guided perturbation framework first extracts
preliminary texture features via Local Binary Patterns (LBP), and then
introduces a dual-model attention strategy to generate and optimize texture
perturbations. Experiments on CelebA-HQ and LFW datasets demonstrate the
promising performance of our method in distorting Deepfake generation and
producing obvious visual defects under multiple attack models, providing an
efficient and scalable solution for proactive Deepfake detection.

</details>


### [88] [SpecGen: Neural Spectral BRDF Generation via Spectral-Spatial Tri-plane Aggregation](https://arxiv.org/abs/2508.17316)
*Zhenyu Jin,Wenjie Li,Zhanyu Ma,Heng Guo*

Main category: cs.CV

TL;DR: SpecGen是一种从单张RGB球体图像生成光谱BRDF的新方法，通过SSTA网络利用丰富的RGB BRDF数据提升光谱BRDF生成质量，实验显示其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统光谱提升方法将RGB图像转换为光谱图像，但缺乏光谱BRDF数据，限制了光谱图像渲染的灵活性。

Method: 提出SpecGen方法，结合SSTA网络建模波长和入射-出射方向的反射响应，利用RGB BRDF数据增强训练。

Result: 实验表明，该方法能准确重建光谱BRDF，并在高光谱图像重建中PSNR提升8 dB。

Conclusion: SpecGen通过单RGB图像生成光谱BRDF，为任意光照和形状的光谱渲染提供了新途径。

Abstract: Synthesizing spectral images across different wavelengths is essential for
photorealistic rendering. Unlike conventional spectral uplifting methods that
convert RGB images into spectral ones, we introduce SpecGen, a novel method
that generates spectral bidirectional reflectance distribution functions
(BRDFs) from a single RGB image of a sphere. This enables spectral image
rendering under arbitrary illuminations and shapes covered by the corresponding
material. A key challenge in spectral BRDF generation is the scarcity of
measured spectral BRDF data. To address this, we propose the Spectral-Spatial
Tri-plane Aggregation (SSTA) network, which models reflectance responses across
wavelengths and incident-outgoing directions, allowing the training strategy to
leverage abundant RGB BRDF data to enhance spectral BRDF generation.
Experiments show that our method accurately reconstructs spectral BRDFs from
limited spectral data and surpasses state-of-the-art methods in hyperspectral
image reconstruction, achieving an improvement of 8 dB in PSNR. Codes and data
will be released upon acceptance.

</details>


### [89] [Mind the (Language) Gap: Towards Probing Numerical and Cross-Lingual Limits of LVLMs](https://arxiv.org/abs/2508.17334)
*Somraj Gautam,Abhirama Subramanyam Penamakuri,Abhishek Bhandari,Gaurav Harit*

Main category: cs.CV

TL;DR: MMCRICBENCH-3K是一个用于评估视觉问答（VQA）在板球记分卡上的基准测试，旨在测试大型视觉语言模型（LVLMs）在复杂数值和跨语言推理上的表现。


<details>
  <summary>Details</summary>
Motivation: 设计该基准测试是为了评估LVLMs在半结构化表格图像上的数值推理和跨语言能力，揭示现有模型的局限性。

Method: 基准包含1,463张合成生成的板球记分卡图像（ODI、T20和Test格式）和1,500个英语问答对，分为英语和印地语两个子集。

Result: 实验表明，即使是GPT-4o和Qwen2.5VL等先进模型在英语子集上表现不佳，印地语子集上表现更差。

Conclusion: 该基准揭示了LVLMs在结构化视觉文本理解、数值推理和跨语言泛化方面的关键局限性，并公开数据集以促进相关研究。

Abstract: We introduce MMCRICBENCH-3K, a benchmark for Visual Question Answering (VQA)
on cricket scorecards, designed to evaluate large vision-language models
(LVLMs) on complex numerical and cross-lingual reasoning over semi-structured
tabular images. MMCRICBENCH-3K comprises 1,463 synthetically generated
scorecard images from ODI, T20, and Test formats, accompanied by 1,500 English
QA pairs. It includes two subsets: MMCRICBENCH-E-1.5K, featuring English
scorecards, and MMCRICBENCH-H-1.5K, containing visually similar Hindi
scorecards, with all questions and answers kept in English to enable controlled
cross-script evaluation. The task demands reasoning over structured numerical
data, multi-image context, and implicit domain knowledge. Empirical results
show that even state-of-the-art LVLMs, such as GPT-4o and Qwen2.5VL, struggle
on the English subset despite it being their primary training language and
exhibit a further drop in performance on the Hindi subset. This reveals key
limitations in structure-aware visual text understanding, numerical reasoning,
and cross-lingual generalization. The dataset is publicly available via Hugging
Face at https://huggingface.co/datasets/DIALab/MMCricBench, to promote LVLM
research in this direction.

</details>


### [90] [No Pixel Left Behind: A Detail-Preserving Architecture for Robust High-Resolution AI-Generated Image Detection](https://arxiv.org/abs/2508.17346)
*Lianrui Mu,Zou Xingze,Jianhong Bai,Jiaqi Hu,Wenjie Zheng,Jiangnan Ye,Jiedong Zhuang,Mudassar Ali,Jing Wang,Haoji Hu*

Main category: cs.CV

TL;DR: HiDA-Net 是一种新型框架，通过特征聚合模块和局部伪造定位模块，解决了高分辨率 AI 生成图像检测的挑战，并在新基准 HiRes-50K 上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法在低分辨率数据集上训练和评估，无法应对高分辨率图像的复杂性，导致信息丢失和检测性能下降。

Method: HiDA-Net 使用特征聚合模块（FAM）融合局部和全局特征，并引入 TFL 和 QFE 模块增强鲁棒性。

Result: 在 Chameleon 数据集上准确率提升 13%，在 HiRes-50K 上提升 10%。

Conclusion: HiDA-Net 通过保留原生分辨率细节和引入新模块，显著提升了高分辨率 AI 生成图像的检测性能。

Abstract: The rapid growth of high-resolution, meticulously crafted AI-generated images
poses a significant challenge to existing detection methods, which are often
trained and evaluated on low-resolution, automatically generated datasets that
do not align with the complexities of high-resolution scenarios. A common
practice is to resize or center-crop high-resolution images to fit standard
network inputs. However, without full coverage of all pixels, such strategies
risk either obscuring subtle, high-frequency artifacts or discarding
information from uncovered regions, leading to input information loss. In this
paper, we introduce the High-Resolution Detail-Aggregation Network (HiDA-Net),
a novel framework that ensures no pixel is left behind. We use the Feature
Aggregation Module (FAM), which fuses features from multiple full-resolution
local tiles with a down-sampled global view of the image. These local features
are aggregated and fused with global representations for final prediction,
ensuring that native-resolution details are preserved and utilized for
detection. To enhance robustness against challenges such as localized AI
manipulations and compression, we introduce Token-wise Forgery Localization
(TFL) module for fine-grained spatial sensitivity and JPEG Quality Factor
Estimation (QFE) module to disentangle generative artifacts from compression
noise explicitly. Furthermore, to facilitate future research, we introduce
HiRes-50K, a new challenging benchmark consisting of 50,568 images with up to
64 megapixels. Extensive experiments show that HiDA-Net achieves
state-of-the-art, increasing accuracy by over 13% on the challenging Chameleon
dataset and 10% on our HiRes-50K.

</details>


### [91] [DiCache: Let Diffusion Model Determine Its Own Cache](https://arxiv.org/abs/2508.17356)
*Jiazi Bu,Pengyang Ling,Yujie Zhou,Yibin Wang,Yuhang Zang,Tong Wu,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: DiCache是一种无需训练的自适应缓存策略，通过浅层在线探针和动态缓存轨迹对齐，显著提升了扩散模型的效率和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有缓存加速方法依赖预定义规则，难以适应扩散过程的动态性，导致泛化能力有限。

Method: DiCache包含在线探针分析方案和动态缓存轨迹对齐，实时确定缓存时机并优化多步缓存使用。

Result: 实验表明，DiCache在多个领先扩散模型上实现了更高的效率和视觉保真度。

Conclusion: DiCache通过自适应策略解决了缓存时机和使用问题，提升了扩散模型的性能。

Abstract: Recent years have witnessed the rapid development of acceleration techniques
for diffusion models, especially caching-based acceleration methods. These
studies seek to answer two fundamental questions: "When to cache" and "How to
use cache", typically relying on predefined empirical laws or dataset-level
priors to determine the timing of caching and utilizing handcrafted rules for
leveraging multi-step caches. However, given the highly dynamic nature of the
diffusion process, they often exhibit limited generalizability and fail on
outlier samples. In this paper, a strong correlation is revealed between the
variation patterns of the shallow-layer feature differences in the diffusion
model and those of final model outputs. Moreover, we have observed that the
features from different model layers form similar trajectories. Based on these
observations, we present DiCache, a novel training-free adaptive caching
strategy for accelerating diffusion models at runtime, answering both when and
how to cache within a unified framework. Specifically, DiCache is composed of
two principal components: (1) Online Probe Profiling Scheme leverages a
shallow-layer online probe to obtain a stable prior for the caching error in
real time, enabling the model to autonomously determine caching schedules. (2)
Dynamic Cache Trajectory Alignment combines multi-step caches based on
shallow-layer probe feature trajectory to better approximate the current
feature, facilitating higher visual quality. Extensive experiments validate
DiCache's capability in achieving higher efficiency and improved visual
fidelity over state-of-the-art methods on various leading diffusion models
including WAN 2.1, HunyuanVideo for video generation, and Flux for image
generation.

</details>


### [92] [Condition Weaving Meets Expert Modulation: Towards Universal and Controllable Image Generation](https://arxiv.org/abs/2508.17364)
*Guoqing Zhang,Xingtong Ge,Lu Shi,Xin Zhang,Muqing Xue,Wanru Xu,Yigang Cen*

Main category: cs.CV

TL;DR: 提出UniGen框架，通过CoMoE模块和WeaveNet机制解决多条件图像生成中的冗余和效率问题，实现高效且表达丰富的生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法为每种条件训练独立分支，导致模型冗余和计算资源浪费。

Method: 引入CoMoE模块聚合相似特征并分配专家模块，提出WeaveNet机制增强主干与条件分支的交互。

Result: 在Subjects-200K和MultiGen-20M数据集上表现优异，验证了方法的通用性和有效性。

Conclusion: UniGen框架在多条件图像生成任务中具有高效和表达丰富的优势。

Abstract: The image-to-image generation task aims to produce controllable images by
leveraging conditional inputs and prompt instructions. However, existing
methods often train separate control branches for each type of condition,
leading to redundant model structures and inefficient use of computational
resources. To address this, we propose a Unified image-to-image Generation
(UniGen) framework that supports diverse conditional inputs while enhancing
generation efficiency and expressiveness. Specifically, to tackle the widely
existing parameter redundancy and computational inefficiency in controllable
conditional generation architectures, we propose the Condition Modulated Expert
(CoMoE) module. This module aggregates semantically similar patch features and
assigns them to dedicated expert modules for visual representation and
conditional modeling. By enabling independent modeling of foreground features
under different conditions, CoMoE effectively mitigates feature entanglement
and redundant computation in multi-condition scenarios. Furthermore, to bridge
the information gap between the backbone and control branches, we propose
WeaveNet, a dynamic, snake-like connection mechanism that enables effective
interaction between global text-level control from the backbone and
fine-grained control from conditional branches. Extensive experiments on the
Subjects-200K and MultiGen-20M datasets across various conditional image
generation tasks demonstrate that our method consistently achieves
state-of-the-art performance, validating its advantages in both versatility and
effectiveness. The code has been uploaded to
https://github.com/gavin-gqzhang/UniGen.

</details>


### [93] [Lightweight Joint Optimization of General-Purpose Vision-Language Models and Retrievers for Medical Diagnosis](https://arxiv.org/abs/2508.17394)
*Nir Mazor,Tom Hope*

Main category: cs.CV

TL;DR: 本文提出了一种联合优化多模态检索器和LVLM的模型，用于医学诊断，相比标准RAG，其性能在临床多标签分类和视觉问答任务中表现优异，但仍存在改进空间。


<details>
  <summary>Details</summary>
Motivation: 通过检索医学文献和医院记录中的视觉信息，提高诊断准确性。

Method: 联合优化多模态检索器和LVLM，仅使用通用主干网络和轻量微调。

Result: 模型在多项任务中表现优异，但实际性能与理论上限仍有差距。

Conclusion: 未来方法需进一步优化以缩小性能差距。

Abstract: Clinical decision-making often involves interpreting images (e.g., radiology)
for making diagnoses. Retrieving relevant visual information from medical
literature and hospital records could enhance diagnostic accuracy. In this
paper, we develop a model in which a multimodal retriever is jointly optimized
with an LVLM for medical diagnosis, unlike standard RAG where LVLM error signal
is not propagated down to the retriever. We show that using only
general-purpose backbones, with only lightweight fine-tuning, our model is able
to achieve competitive results with medically-pretrained models across clinical
multi-label classification and visual question answering tasks. In a novel
analysis, we additionally find that in many cases different top retrieved
images each lead to different predictions for a given target, and that these
cases are empirically challenging for all models, even for non-retrieval
models. Our joint retrieval optimization significantly improves these
challenging cases over standard RAG. However, oracle analysis reveals that
while the correct diagnosis is frequently achievable using one of the top
retrieved images, in practice there is a large performance gap from the oracle,
and rerankers using frontier LVLMs do not close this gap -- leaving ample room
for improvement by future methods. Code will be made publicly available.

</details>


### [94] [Enhancing Underwater Images via Deep Learning: A Comparative Study of VGG19 and ResNet50-Based Approaches](https://arxiv.org/abs/2508.17397)
*Aoqi Li,Yanghui Song,Jichao Dao,Chengfu Yang*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的复杂水下场景图像增强方法，结合VGG19和ResNet50模型，实现多尺度多层次特征分析，并通过统一模型整合优势，提升图像增强效果。


<details>
  <summary>Details</summary>
Motivation: 解决复杂水下场景中图像增强的挑战性问题，提升视觉任务的准确性和实用性。

Method: 整合VGG19和ResNet50模型，进行多尺度多层次特征分析，构建统一模型。

Result: 通过PSNR、UCIQE和UIQM等指标定量评估，验证了方法的有效性，并在不同场景下分析了模型性能。

Conclusion: 该方法为复杂水下环境中的视觉增强任务提供了技术支持，并提出了模型优化和多模型融合等实用建议。

Abstract: This paper addresses the challenging problem of image enhancement in complex
underwater scenes by proposing a solution based on deep learning. The proposed
method skillfully integrates two deep convolutional neural network models,
VGG19 and ResNet50, leveraging their powerful feature extraction capabilities
to perform multi-scale and multi-level deep feature analysis of underwater
images. By constructing a unified model, the complementary advantages of the
two models are effectively integrated, achieving a more comprehensive and
accurate image enhancement effect.To objectively evaluate the enhancement
effect, this paper introduces image quality assessment metrics such as PSNR,
UCIQE, and UIQM to quantitatively compare images before and after enhancement
and deeply analyzes the performance of different models in different
scenarios.Furthermore, to improve the practicality and stability of the
underwater visual enhancement system, this paper also provides practical
suggestions from aspects such as model optimization, multi-model fusion, and
hardware selection, aiming to provide strong technical support for visual
enhancement tasks in complex underwater environments.

</details>


### [95] [MoCo: Motion-Consistent Human Video Generation via Structure-Appearance Decoupling](https://arxiv.org/abs/2508.17404)
*Haoyu Wang,Hao Tang,Donglin Di,Zhilu Zhang,Wangmeng Zuo,Feng Gao,Siwei Ma,Shiliang Zhang*

Main category: cs.CV

TL;DR: MoCo提出了一种将人类视频生成解耦为结构生成和外观生成的方法，解决了现有方法在一致性和物理合理性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成模型在人类动作一致性上表现不佳，且数据集局限于简单动作，无法满足复杂运动的需求。

Method: MoCo通过3D结构生成器生成动作序列，再合成外观，并引入Human-Aware Dynamic Control模块和密集跟踪约束。

Result: 实验表明，MoCo在生成真实且结构一致的人类视频上优于现有方法。

Conclusion: MoCo通过解耦生成过程和引入新模块，显著提升了人类视频生成的质量和多样性。

Abstract: Generating human videos with consistent motion from text prompts remains a
significant challenge, particularly for whole-body or long-range motion.
Existing video generation models prioritize appearance fidelity, resulting in
unrealistic or physically implausible human movements with poor structural
coherence. Additionally, most existing human video datasets primarily focus on
facial or upper-body motions, or consist of vertically oriented dance videos,
limiting the scope of corresponding generation methods to simple movements. To
overcome these challenges, we propose MoCo, which decouples the process of
human video generation into two components: structure generation and appearance
generation. Specifically, our method first employs an efficient 3D structure
generator to produce a human motion sequence from a text prompt. The remaining
video appearance is then synthesized under the guidance of the generated
structural sequence. To improve fine-grained control over sparse human
structures, we introduce Human-Aware Dynamic Control modules and integrate
dense tracking constraints during training. Furthermore, recognizing the
limitations of existing datasets, we construct a large-scale whole-body human
video dataset featuring complex and diverse motions. Extensive experiments
demonstrate that MoCo outperforms existing approaches in generating realistic
and structurally coherent human videos.

</details>


### [96] [E-BayesSAM: Efficient Bayesian Adaptation of SAM with Self-Optimizing KAN-Based Interpretation for Uncertainty-Aware Ultrasonic Segmentation](https://arxiv.org/abs/2508.17408)
*Bin Huang,Zhong Liu,Huiying Wen,Bingsheng Huang,Xin Chen,Shuo Li*

Main category: cs.CV

TL;DR: E-BayesSAM结合T-VBI和SO-KAN，解决了SAM在医学图像分割中的贝叶斯适应问题，提升了效率、准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决SAM在贝叶斯适应中的不稳定性、高计算成本和黑盒设计问题。

Method: 使用T-VBI进行高效贝叶斯适应，SO-KAN提升可解释性。

Result: 在五个超声数据集上实现实时推理、更高分割准确性和关键token识别。

Conclusion: E-BayesSAM统一了效率、可靠性和可解释性，推动了SAM在临床中的应用。

Abstract: Although the Segment Anything Model (SAM) has advanced medical image
segmentation, its Bayesian adaptation for uncertainty-aware segmentation
remains hindered by three key issues: (1) instability in Bayesian fine-tuning
of large pre-trained SAMs; (2) high computation cost due to SAM's massive
parameters; (3) SAM's black-box design limits interpretability. To overcome
these, we propose E-BayesSAM, an efficient framework combining Token-wise
Variational Bayesian Inference (T-VBI) for efficienty Bayesian adaptation and
Self-Optimizing Kolmogorov-Arnold Network (SO-KAN) for improving
interpretability. T-VBI innovatively reinterprets SAM's output tokens as
dynamic probabilistic weights and reparameterizes them as latent variables
without auxiliary training, enabling training-free VBI for uncertainty
estimation. SO-KAN improves token prediction with learnable spline activations
via self-supervised learning, providing insight to prune redundant tokens to
boost efficiency and accuracy. Experiments on five ultrasound datasets
demonstrated that E-BayesSAM achieves: (i) real-time inference (0.03s/image),
(ii) superior segmentation accuracy (average DSC: Pruned E-BayesSAM's 89.0\%
vs. E-BayesSAM's 88.0% vs. MedSAM's 88.3%), and (iii) identification of four
critical tokens governing SAM's decisions. By unifying efficiency, reliability,
and interpretability, E-BayesSAM bridges SAM's versatility with clinical needs,
advancing deployment in safety-critical medical applications. The source code
is available at https://github.com/mp31192/E-BayesSAM.

</details>


### [97] [Data Leakage in Visual Datasets](https://arxiv.org/abs/2508.17416)
*Patrick Ramos,Ryan Ramos,Noa Garcia*

Main category: cs.CV

TL;DR: 论文分析了视觉数据集中的数据泄漏问题，指出泄漏会损害模型评估的公平性，并通过图像检索技术证实了所有数据集都存在泄漏现象。


<details>
  <summary>Details</summary>
Motivation: 研究动机是发现并研究视觉数据集中因训练和评估数据重叠导致的数据泄漏问题，以确保模型评估的可靠性。

Method: 方法包括使用图像检索技术识别和分类数据泄漏的不同类型（如模态、覆盖范围和程度）。

Result: 结果表明所有分析的数据集都存在某种形式的泄漏，且无论严重程度如何，都会影响下游任务的模型评估可靠性。

Conclusion: 结论是数据泄漏普遍存在，且会显著影响模型评估的公平性和可靠性，需引起重视。

Abstract: We analyze data leakage in visual datasets. Data leakage refers to images in
evaluation benchmarks that have been seen during training, compromising fair
model evaluation. Given that large-scale datasets are often sourced from the
internet, where many computer vision benchmarks are publicly available, our
efforts are focused into identifying and studying this phenomenon. We
characterize visual leakage into different types according to its modality,
coverage, and degree. By applying image retrieval techniques, we unequivocally
show that all the analyzed datasets present some form of leakage, and that all
types of leakage, from severe instances to more subtle cases, compromise the
reliability of model evaluation in downstream tasks.

</details>


### [98] [Constrained Prompt Enhancement for Improving Zero-Shot Generalization of Vision-Language Models](https://arxiv.org/abs/2508.17417)
*Xiaojie Yin,Qilong Wang,Qinghua Hu*

Main category: cs.CV

TL;DR: 提出了一种新的约束提示增强（CPE）方法，通过构建全面的文本提示和紧凑的视觉提示来改善视觉-文本对齐，提升视觉语言模型的零样本泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在文本提示和视觉提示上存在语义不完整和噪声问题，导致视觉-文本对齐效果不佳。

Method: 方法包括拓扑引导的同义语义生成（TGSSG）和类别无关的判别区域选择（CADRS），分别优化文本和视觉提示。

Result: 通过测试时间适应（TTA）和最优传输（OT）的集合匹配策略，实现了有效的视觉-文本对齐。

Conclusion: CPE方法显著提升了视觉语言模型的零样本泛化能力。

Abstract: Vision-language models (VLMs) pre-trained on web-scale data exhibit promising
zero-shot generalization but often suffer from semantic misalignment due to
domain gaps between pre-training and downstream tasks. Existing approaches
primarily focus on text prompting with class-specific descriptions and
visual-text adaptation via aligning cropped image regions with textual
descriptions. However, they still face the issues of incomplete textual prompts
and noisy visual prompts. In this paper, we propose a novel constrained prompt
enhancement (CPE) method to improve visual-textual alignment by constructing
comprehensive textual prompts and compact visual prompts from the semantic
perspective. Specifically, our approach consists of two key components:
Topology-Guided Synonymous Semantic Generation (TGSSG) and Category-Agnostic
Discriminative Region Selection (CADRS). Textually, to address the issue of
incomplete semantic expression in textual prompts, our TGSSG first generates
synonymous semantic set for each category via large language models, and
constructs comprehensive textual prompts based on semantic ambiguity entropy
and persistent homology analysis. Visually, to mitigate the irrelevant visual
noise introduced by random cropping, our CADRS identifies discriminative
regions with activation maps outputted by a pre-trained vision model,
effectively filtering out noisy regions and generating compact visual prompts.
Given the comprehensive set of textual prompts and compact set of visual
prompts, we introduce two set-to-set matching strategies based on test-time
adaptation (TTA) and optimal transport (OT) to achieve effective visual-textual
alignment, and so improve zero-shot generalization of VLMs.

</details>


### [99] [Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search](https://arxiv.org/abs/2508.17427)
*Zhao Zheng,Jingfan Fan,Long Shao,Hong Song,Danni Ai,Tianyu Fu,Deqiang Xiao,Yongtian Wang,Jian Yang*

Main category: cs.CV

TL;DR: 提出了一种基于旋转分支定界的几何最大重叠配准框架，通过分解刚体变换并高效搜索最优参数，显著提升了点云配准的精度和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在高离群率下表现良好，但存在计算复杂度高或局部最优问题，需要更高效且准确的解决方案。

Method: 将刚体变换分解为旋转轴平移和2D变换，通过分支定界搜索最优旋转参数，并利用区间查询和扫描线算法高效求解。

Result: 在3DMatch、3DLoMatch和KITTI数据集上表现出优于现有方法的精度和效率。

Conclusion: 该方法在多项式时间复杂度和线性空间复杂度下实现了高精度配准，适用于实际应用。

Abstract: Point cloud registration based on correspondences computes the rigid
transformation that maximizes the number of inliers constrained within the
noise threshold. Current state-of-the-art (SOTA) methods employing spatial
compatibility graphs or branch-and-bound (BnB) search mainly focus on
registration under high outlier ratios. However, graph-based methods require at
least quadratic space and time complexity for graph construction, while
multi-stage BnB search methods often suffer from inaccuracy due to local optima
between decomposed stages. This paper proposes a geometric maximum overlapping
registration framework via rotation-only BnB search. The rigid transformation
is decomposed using Chasles' theorem into a translation along rotation axis and
a 2D rigid transformation. The optimal rotation axis and angle are searched via
BnB, with residual parameters formulated as range maximum query (RMQ) problems.
Firstly, the top-k candidate rotation axes are searched within a hemisphere
parameterized by cube mapping, and the translation along each axis is estimated
through interval stabbing of the correspondences projected onto that axis.
Secondly, the 2D registration is relaxed to 1D rotation angle search with 2D
RMQ of geometric overlapping for axis-aligned rectangles, which is solved
deterministically in polynomial time using sweep line algorithm with segment
tree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasets
demonstrate superior accuracy and efficiency over SOTA methods, while the time
complexity is polynomial and the space complexity increases linearly with the
number of points, even in the worst case.

</details>


### [100] [FedKLPR: Personalized Federated Learning for Person Re-Identification with Adaptive Pruning](https://arxiv.org/abs/2508.17431)
*Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.CV

TL;DR: FedKLPR是一种轻量级且通信高效的人重识别联邦学习框架，通过KL散度正则化、加权聚合、稀疏激活跳过和跨轮恢复等机制，显著减少通信开销并保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在人重识别中的应用面临统计异构性和高通信开销的挑战，需要一种既能保护隐私又能高效通信的解决方案。

Method: 提出FedKLPR框架，包含KL散度正则化损失（KLL）、KL散度剪枝加权聚合（KLPWA）、稀疏激活跳过（SAS）和跨轮恢复（CRR）四个关键组件。

Result: 在八个基准数据集上，FedKLPR显著减少通信开销（ResNet-50减少33%-38%，ResNet-34减少20%-40%），同时模型精度下降不超过1%。

Conclusion: FedKLPR有效解决了联邦学习在人重识别中的统计异构性和通信开销问题，为实际应用提供了高效且隐私保护的解决方案。

Abstract: Person re-identification (Re-ID) is a fundamental task in intelligent
surveillance and public safety. Federated learning (FL) offers a
privacy-preserving solution by enabling collaborative model training without
centralized data collection. However, applying FL to real-world re-ID systems
faces two major challenges: statistical heterogeneity across clients due to
non-IID data distributions, and substantial communication overhead caused by
frequent transmission of large-scale models. To address these issues, we
propose FedKLPR, a lightweight and communication-efficient federated learning
framework for person re-identification. FedKLPR introduces four key components.
First, the KL-Divergence Regularization Loss (KLL) constrains local models by
minimizing the divergence from the global feature distribution, effectively
mitigating the effects of statistical heterogeneity and improving convergence
stability under non-IID conditions. Secondly, KL-Divergence-Prune Weighted
Aggregation (KLPWA) integrates pruning ratio and distributional similarity into
the aggregation process, thereby improving the robustness of the global model
while significantly reducing communication overhead. Furthermore, sparse
Activation Skipping (SAS) mitigates the dilution of critical parameters during
the aggregation of pruned client models by excluding zero-valued weights from
the update process. Finally, Cross-Round Recovery (CRR) introduces a dynamic
pruning control mechanism that halts pruning when necessary, enabling deeper
compression while maintaining model accuracy. Experimental results on eight
benchmark datasets demonstrate that FedKLPR achieves significant communication
reduction. Compared with the state-of-the-art, FedKLPR reduces 33\%-38\%
communication cost on ResNet-50 and 20\%-40\% communication cost on ResNet-34,
while maintaining model accuracy within 1\% degradation.

</details>


### [101] [TinySR: Pruning Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2508.17434)
*Linwei Dong,Qingnan Fan,Yuhang Yu,Qi Zhang,Jinwei Chen,Yawei Luo,Changqing Zou*

Main category: cs.CV

TL;DR: TinySR是一种紧凑高效的扩散模型，专为实时图像超分辨率设计，通过动态块间激活和扩展-腐蚀策略优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决现有扩散模型在实时应用中计算开销高的问题，同时保持感知质量。

Method: 采用动态块间激活、扩展-腐蚀策略、VAE压缩、通道剪枝、注意力移除和轻量级SepConv等技术。

Result: 相比TSD-SR，TinySR实现了5.68倍加速和83%参数减少，同时保持高质量结果。

Conclusion: TinySR在保持性能的同时显著提升了计算效率，适用于实时应用。

Abstract: Real-world image super-resolution (Real-ISR) focuses on recovering
high-quality images from low-resolution inputs that suffer from complex
degradations like noise, blur, and compression. Recently, diffusion models
(DMs) have shown great potential in this area by leveraging strong generative
priors to restore fine details. However, their iterative denoising process
incurs high computational overhead, posing challenges for real-time
applications. Although one-step distillation methods, such as OSEDiff and
TSD-SR, offer faster inference, they remain fundamentally constrained by their
large, over-parameterized model architectures. In this work, we present TinySR,
a compact yet effective diffusion model specifically designed for Real-ISR that
achieves real-time performance while maintaining perceptual quality. We
introduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy
to facilitate more effective decision-making in depth pruning. We achieve VAE
compression through channel pruning, attention removal and lightweight SepConv.
We eliminate time- and prompt-related modules and perform pre-caching
techniques to further speed up the model. TinySR significantly reduces
computational cost and model size, achieving up to 5.68x speedup and 83%
parameter reduction compared to its teacher TSD-SR, while still providing high
quality results.

</details>


### [102] [An LLM-LVLM Driven Agent for Iterative and Fine-Grained Image Editing](https://arxiv.org/abs/2508.17435)
*Zihan Liang,Jiahao Sun,Haoran Ma*

Main category: cs.CV

TL;DR: RefineEdit-Agent是一个无需训练的智能代理框架，通过结合LLM和LVLM的能力，实现了复杂、迭代和上下文感知的图像编辑，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在细粒度、迭代图像编辑方面存在不足，如指令理解、上下文保留和反馈机制缺乏。

Method: 利用LLM进行规划，LVLM进行视觉理解和评估，构建闭环系统，包括指令解析、编辑规划、迭代编辑和反馈循环。

Result: 在LongBench-T2I-Edit基准上，RefineEdit-Agent平均得分3.67，优于其他基线方法。

Conclusion: RefineEdit-Agent通过智能代理设计，显著提升了编辑保真度和上下文保留能力。

Abstract: Despite the remarkable capabilities of text-to-image (T2I) generation models,
real-world applications often demand fine-grained, iterative image editing that
existing methods struggle to provide. Key challenges include granular
instruction understanding, robust context preservation during modifications,
and the lack of intelligent feedback mechanisms for iterative refinement. This
paper introduces RefineEdit-Agent, a novel, training-free intelligent agent
framework designed to address these limitations by enabling complex, iterative,
and context-aware image editing. RefineEdit-Agent leverages the powerful
planning capabilities of Large Language Models (LLMs) and the advanced visual
understanding and evaluation prowess of Vision-Language Large Models (LVLMs)
within a closed-loop system. Our framework comprises an LVLM-driven instruction
parser and scene understanding module, a multi-level LLM-driven editing planner
for goal decomposition, tool selection, and sequence generation, an iterative
image editing module, and a crucial LVLM-driven feedback and evaluation loop.
To rigorously evaluate RefineEdit-Agent, we propose LongBench-T2I-Edit, a new
benchmark featuring 500 initial images with complex, multi-turn editing
instructions across nine visual dimensions. Extensive experiments demonstrate
that RefineEdit-Agent significantly outperforms state-of-the-art baselines,
achieving an average score of 3.67 on LongBench-T2I-Edit, compared to 2.29 for
Direct Re-Prompting, 2.91 for InstructPix2Pix, 3.16 for GLIGEN-based Edit, and
3.39 for ControlNet-XL. Ablation studies, human evaluations, and analyses of
iterative refinement, backbone choices, tool usage, and robustness to
instruction complexity further validate the efficacy of our agentic design in
delivering superior edit fidelity and context preservation.

</details>


### [103] [Disentangled Geometry and Appearance for Efficient Multi-View Surface Reconstruction and Rendering](https://arxiv.org/abs/2508.17436)
*Qitong Zhang,Jieqing Feng*

Main category: cs.CV

TL;DR: 提出了一种基于显式网格表示和可微分光栅化的高效多视角表面重建方法，解决了传统神经渲染方法需要额外网格提取步骤的问题，显著提升了重建质量和应用范围。


<details>
  <summary>Details</summary>
Motivation: 传统神经渲染方法在多视角表面重建中需要额外的网格提取步骤，导致不便和低质量表面，限制了应用。

Method: 引入解耦的几何和外观模型，不依赖深度网络；构建神经变形场以增强几何学习；通过正则化约束几何特征；分离视图不变漫反射项以提高渲染效率。

Result: 方法在训练（4.84分钟）和渲染（0.023秒）速度上达到最优，重建质量与顶级方法相当，支持网格和纹理编辑。

Conclusion: 该方法在效率、质量和应用范围上具有显著优势，为多视角表面重建和渲染提供了有价值的解决方案。

Abstract: This paper addresses the limitations of neural rendering-based multi-view
surface reconstruction methods, which require an additional mesh extraction
step that is inconvenient and would produce poor-quality surfaces with mesh
aliasing, restricting downstream applications. Building on the explicit mesh
representation and differentiable rasterization framework, this work proposes
an efficient solution that preserves the high efficiency of this framework
while significantly improving reconstruction quality and versatility.
Specifically, we introduce a disentangled geometry and appearance model that
does not rely on deep networks, enhancing learning and broadening
applicability. A neural deformation field is constructed to incorporate global
geometric context, enhancing geometry learning, while a novel regularization
constrains geometric features passed to a neural shader to ensure its accuracy
and boost shading. For appearance, a view-invariant diffuse term is separated
and baked into mesh vertices, further improving rendering efficiency.
Experimental results demonstrate that the proposed method achieves
state-of-the-art training (4.84 minutes) and rendering (0.023 seconds) speeds,
with reconstruction quality that is competitive with top-performing methods.
Moreover, the method enables practical applications such as mesh and texture
editing, showcasing its versatility and application potential. This combination
of efficiency, competitive quality, and broad applicability makes our approach
a valuable contribution to multi-view surface reconstruction and rendering.

</details>


### [104] [Pixie: Fast and Generalizable Supervised Learning of 3D Physics from Pixels](https://arxiv.org/abs/2508.17437)
*Long Le,Ryan Lucas,Chen Wang,Chuhao Chen,Dinesh Jayaraman,Eric Eaton,Lingjie Liu*

Main category: cs.CV

TL;DR: PIXIE是一种新颖的神经网络方法，通过监督损失从3D视觉特征预测物理属性，速度快且泛化性强。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖缓慢的逐场景优化，限制了泛化性和应用。PIXIE旨在解决这一问题。

Method: PIXIE训练通用神经网络，结合高斯散射等静态场景表示，实现快速推理。

Result: PIXIE比测试时优化方法快1.46-4.39倍，且能零样本泛化到真实场景。

Conclusion: PIXIE在物理属性预测和仿真中表现出色，适用于虚拟世界构建。

Abstract: Inferring the physical properties of 3D scenes from visual information is a
critical yet challenging task for creating interactive and realistic virtual
worlds. While humans intuitively grasp material characteristics such as
elasticity or stiffness, existing methods often rely on slow, per-scene
optimization, limiting their generalizability and application. To address this
problem, we introduce PIXIE, a novel method that trains a generalizable neural
network to predict physical properties across multiple scenes from 3D visual
features purely using supervised losses. Once trained, our feed-forward network
can perform fast inference of plausible material fields, which coupled with a
learned static scene representation like Gaussian Splatting enables realistic
physics simulation under external forces. To facilitate this research, we also
collected PIXIEVERSE, one of the largest known datasets of paired 3D assets and
physic material annotations. Extensive evaluations demonstrate that PIXIE is
about 1.46-4.39x better and orders of magnitude faster than test-time
optimization methods. By leveraging pretrained visual features like CLIP, our
method can also zero-shot generalize to real-world scenes despite only ever
been trained on synthetic data. https://pixie-3d.github.io/

</details>


### [105] [Investigating Domain Gaps for Indoor 3D Object Detection](https://arxiv.org/abs/2508.17439)
*Zijing Zhao,Zhu Xu,Qingchao Chen,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 本文提出了一个跨数据集适应室内3D物体检测的基准，分析了不同领域差距对检测器的影响，并提供了改进适应性能的方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究在有限数据集上进行，训练和测试集分布相同，缺乏对跨数据集适应性的研究。

Method: 使用ScanNet、SUN RGB-D、3D Front等数据集，以及新提出的ProcTHOR-OD和ProcFront数据集，进行合成到真实、点云质量、布局和实例特征等适应场景的实验。

Result: 分析了不同领域差距对3D物体检测器的影响，并提出了改进适应性能的方法。

Conclusion: 为领域自适应室内3D物体检测提供了基线，希望未来能提出更具跨领域泛化能力的检测器。

Abstract: As a fundamental task for indoor scene understanding, 3D object detection has
been extensively studied, and the accuracy on indoor point cloud data has been
substantially improved. However, existing researches have been conducted on
limited datasets, where the training and testing sets share the same
distribution. In this paper, we consider the task of adapting indoor 3D object
detectors from one dataset to another, presenting a comprehensive benchmark
with ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposed
large-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.
Since indoor point cloud datasets are collected and constructed in different
ways, the object detectors are likely to overfit to specific factors within
each dataset, such as point cloud quality, bounding box layout and instance
features. We conduct experiments across datasets on different adaptation
scenarios including synthetic-to-real adaptation, point cloud quality
adaptation, layout adaptation and instance feature adaptation, analyzing the
impact of different domain gaps on 3D object detectors. We also introduce
several approaches to improve adaptation performances, providing baselines for
domain adaptive indoor 3D object detection, hoping that future works may
propose detectors with stronger generalization ability across domains. Our
project homepage can be found in
https://jeremyzhao1998.github.io/DAVoteNet-release/.

</details>


### [106] [Multi-Level LVLM Guidance for Untrimmed Video Action Recognition](https://arxiv.org/abs/2508.17442)
*Liyang Peng,Sihan Zhu,Yunjie Guo*

Main category: cs.CV

TL;DR: ECVT是一种新型视频Transformer架构，利用大型视觉语言模型（LVLM）提升复杂视频中的动作识别和定位能力，通过双分支设计和多粒度语义描述实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉细粒度动作、长期时间依赖和高级语义信息方面存在局限，ECVT旨在通过LVLM的语义理解能力解决这一问题。

Method: ECVT采用双分支设计：视频编码分支提取时空特征，跨模态引导分支利用LVLM生成多粒度语义描述，并通过自适应门控、跨模态注意力和事件图模块集成文本线索。

Result: 在ActivityNet v1.3和THUMOS14数据集上，ECVT分别达到40.5%的平均mAP和67.1%的mAP@0.5，优于现有方法。

Conclusion: ECVT通过结合LVLM和多模态融合机制，显著提升了视频动作识别和定位的性能，为复杂视频理解提供了新思路。

Abstract: Action recognition and localization in complex, untrimmed videos remain a
formidable challenge in computer vision, largely due to the limitations of
existing methods in capturing fine-grained actions, long-term temporal
dependencies, and high-level semantic information from low-level visual
features. This paper introduces the Event-Contextualized Video Transformer
(ECVT), a novel architecture that leverages the advanced semantic understanding
capabilities of Large Vision-Language Models (LVLMs) to bridge this gap. ECVT
employs a dual-branch design, comprising a Video Encoding Branch for
spatio-temporal feature extraction and a Cross-Modal Guidance Branch. The
latter utilizes an LVLM to generate multi-granularity semantic descriptions,
including Global Event Prompting for macro-level narrative and Temporal
Sub-event Prompting for fine-grained action details. These multi-level textual
cues are integrated into the video encoder's learning process through
sophisticated mechanisms such as adaptive gating for high-level semantic
fusion, cross-modal attention for fine-grained feature refinement, and an event
graph module for temporal context calibration. Trained end-to-end with a
comprehensive loss function incorporating semantic consistency and temporal
calibration terms, ECVT significantly enhances the model's ability to
understand video temporal structures and event logic. Extensive experiments on
ActivityNet v1.3 and THUMOS14 datasets demonstrate that ECVT achieves
state-of-the-art performance, with an average mAP of 40.5% on ActivityNet v1.3
and mAP@0.5 of 67.1% on THUMOS14, outperforming leading baselines.

</details>


### [107] [A Synthetic Dataset for Manometry Recognition in Robotic Applications](https://arxiv.org/abs/2508.17468)
*Pedro Antonio Rabelo Saraiva,Enzo Ferreira de Souza,Joao Manoel Herrera Pinheiro,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.CV

TL;DR: 提出了一种结合程序渲染和AI驱动视频生成的混合数据合成方法，用于解决工业环境中数据稀缺和高成本问题，并验证了其在目标检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 工业环境（如海上石油平台）中数据采集成本高且危险，阻碍了自主检测系统的发展。

Method: 使用BlenderProc生成带精确标注的光真实感图像，结合NVIDIA的Cosmos-Predict2模型合成物理合理的视频序列，混合真实与合成数据训练YOLO检测网络。

Result: 混合真实与合成数据的训练模型性能优于仅使用真实数据的模型，1:1混合比例效果最佳。

Conclusion: 合成数据优先的方法在安全关键和资源受限的工业应用中是一种高效、经济且可靠的解决方案。

Abstract: This work addresses the challenges of data scarcity and high acquisition
costs for training robust object detection models in complex industrial
environments, such as offshore oil platforms. The practical and economic
barriers to collecting real-world data in these hazardous settings often hamper
the development of autonomous inspection systems. To overcome this, in this
work we propose and validate a hybrid data synthesis pipeline that combines
procedural rendering with AI-driven video generation. Our methodology leverages
BlenderProc to create photorealistic images with precise annotations and
controlled domain randomization, and integrates NVIDIA's Cosmos-Predict2
world-foundation model to synthesize physically plausible video sequences with
temporal diversity, capturing rare viewpoints and adverse conditions. We
demonstrate that a YOLO-based detection network trained on a composite dataset,
blending real images with our synthetic data, achieves superior performance
compared to models trained exclusively on real-world data. Notably, a 1:1
mixture of real and synthetic data yielded the highest accuracy, surpassing the
real-only baseline. These findings highlight the viability of a synthetic-first
approach as an efficient, cost-effective, and safe alternative for developing
reliable perception systems in safety-critical and resource-constrained
industrial applications.

</details>


### [108] [T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image Generation](https://arxiv.org/abs/2508.17472)
*Kaiyue Sun,Rongyao Fang,Chengqi Duan,Xian Liu,Xihui Liu*

Main category: cs.CV

TL;DR: T2I-ReasonBench是一个评估文本到图像（T2I）模型推理能力的基准，包含四个维度，并提出两阶段评估协议。


<details>
  <summary>Details</summary>
Motivation: 现有T2I模型在复杂推理任务上的表现缺乏系统评估，因此需要开发一个全面的基准来填补这一空白。

Method: 提出T2I-ReasonBench基准，包含四个推理维度（习语解释、文本图像设计、实体推理和科学推理），并采用两阶段评估协议（推理准确性和图像质量）。

Result: 对多种T2I生成模型进行了基准测试，并提供了详细的性能分析。

Conclusion: T2I-ReasonBench为评估T2I模型的推理能力提供了系统工具，揭示了模型在不同推理任务上的表现差异。

Abstract: We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of
text-to-image (T2I) models. It consists of four dimensions: Idiom
Interpretation, Textual Image Design, Entity-Reasoning and
Scientific-Reasoning. We propose a two-stage evaluation protocol to assess the
reasoning accuracy and image quality. We benchmark various T2I generation
models, and provide comprehensive analysis on their performances.

</details>


### [109] [GraphMMP: A Graph Neural Network Model with Mutual Information and Global Fusion for Multimodal Medical Prognosis](https://arxiv.org/abs/2508.17478)
*Xuhao Shan,Ruiquan Ge,Jikui Liu,Linglong Wu,Chi Zhang,Siqi Liu,Wenjian Qin,Wenwen Min,Ahmed Elazab,Changmiao Wang*

Main category: cs.CV

TL;DR: GraphMMP是一种基于图神经网络的两阶段多模态预后模型，通过互信息构建特征图，并利用Mamba模块进行全局融合，显著提升了预后性能。


<details>
  <summary>Details</summary>
Motivation: 多模态医学数据分析中，如何有效建模异构数据模态间的复杂交互并捕捉局部和全局依赖关系是主要挑战。

Method: 提出GraphMMP模型，利用互信息构建特征图，并设计基于Mamba的全局融合模块。

Result: 在肝脏预后和METABRIC研究数据集上，GraphMMP表现优于现有方法。

Conclusion: GraphMMP在多模态医学预后任务中表现出色，为解决复杂数据交互问题提供了有效方案。

Abstract: In the field of multimodal medical data analysis, leveraging diverse types of
data and understanding their hidden relationships continues to be a research
focus. The main challenges lie in effectively modeling the complex interactions
between heterogeneous data modalities with distinct characteristics while
capturing both local and global dependencies across modalities. To address
these challenges, this paper presents a two-stage multimodal prognosis model,
GraphMMP, which is based on graph neural networks. The proposed model
constructs feature graphs using mutual information and features a global fusion
module built on Mamba, which significantly boosts prognosis performance.
Empirical results show that GraphMMP surpasses existing methods on datasets
related to liver prognosis and the METABRIC study, demonstrating its
effectiveness in multimodal medical prognosis tasks.

</details>


### [110] [Optimizing Multi-Modal Trackers via Sensitivity-aware Regularized Tuning](https://arxiv.org/abs/2508.17488)
*Zhiwen Chen,Jinjian Wu,Zhiyu Zhu,Yifan Zhang,Guangming Shi,Junhui Hou*

Main category: cs.CV

TL;DR: 提出了一种基于敏感度感知的调优框架，优化多模态跟踪器的预训练模型适应问题。


<details>
  <summary>Details</summary>
Motivation: 现有微调方法在自由度和限制之间难以平衡，导致塑性-稳定性权衡不佳。

Method: 通过分析预训练权重的切线空间和迁移敏感度，引入正则化项优化学习过程。

Result: 实验表明，该方法在多模态跟踪任务中优于现有技术。

Conclusion: 该方法显著提升了跨模态迁移能力，代码和模型已开源。

Abstract: This paper tackles the critical challenge of optimizing multi-modal trackers
by effectively adapting the pre-trained models for RGB data. Existing
fine-tuning paradigms oscillate between excessive freedom and over-restriction,
both leading to a suboptimal plasticity-stability trade-off. To mitigate this
dilemma, we propose a novel sensitivity-aware regularized tuning framework,
which delicately refines the learning process by incorporating intrinsic
parameter sensitivities. Through a comprehensive investigation from pre-trained
to multi-modal contexts, we identify that parameters sensitive to pivotal
foundational patterns and cross-domain shifts are primary drivers of this
issue. Specifically, we first analyze the tangent space of pre-trained weights
to measure and orient prior sensitivities, dedicated to preserving
generalization. Then, we further explore transfer sensitivities during the
tuning phase, emphasizing adaptability and stability. By incorporating these
sensitivities as regularization terms, our method significantly enhances the
transferability across modalities. Extensive experiments showcase the superior
performance of the proposed method, surpassing current state-of-the-art
techniques across various multi-modal tracking. The source code and models will
be publicly available at https://github.com/zhiwen-xdu/SRTrack.

</details>


### [111] [Social-MAE: A Transformer-Based Multimodal Autoencoder for Face and Voice](https://arxiv.org/abs/2508.17502)
*Hugo Bohy,Minh Tran,Kevin El Haddad,Thierry Dutoit,Mohammad Soleymani*

Main category: cs.CV

TL;DR: Social-MAE是一种基于CAV-MAE改进的预训练视听自编码器，用于社交行为的多模态感知，在情感识别等任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 人类社交行为本质上是多模态的，需要强大的视听模型来感知。

Method: 改进CAV-MAE以接收更多帧输入，并在VoxCeleb2数据集上自监督预训练。

Result: 在情感识别和笑声检测任务中达到SOTA，在性格估计中表现竞争性。

Conclusion: 领域内自监督预训练有效，模型在多模态社交任务中表现优异。

Abstract: Human social behaviors are inherently multimodal necessitating the
development of powerful audiovisual models for their perception. In this paper,
we present Social-MAE, our pre-trained audiovisual Masked Autoencoder based on
an extended version of Contrastive Audio-Visual Masked Auto-Encoder (CAV-MAE),
which is pre-trained on audiovisual social data. Specifically, we modify
CAV-MAE to receive a larger number of frames as input and pre-train it on a
large dataset of human social interaction (VoxCeleb2) in a self-supervised
manner. We demonstrate the effectiveness of this model by finetuning and
evaluating the model on different social and affective downstream tasks,
namely, emotion recognition, laughter detection and apparent personality
estimation. The model achieves state-of-the-art results on multimodal emotion
recognition and laughter recognition and competitive results for apparent
personality estimation, demonstrating the effectiveness of in-domain
self-supervised pre-training. Code and model weight are available here
https://github.com/HuBohy/SocialMAE.

</details>


### [112] [DinoTwins: Combining DINO and Barlow Twins for Robust, Label-Efficient Vision Transformers](https://arxiv.org/abs/2508.17509)
*Michael Podsiadly,Brendon K Lay*

Main category: cs.CV

TL;DR: 结合DINO和Barlow Twins技术，提出了一种在标签数据稀缺和计算资源有限情况下更高效的视觉模型训练方法。


<details>
  <summary>Details</summary>
Motivation: 解决无监督学习中DINO对数据增强敏感和Barlow Twins需要大批量的问题，结合两者优势提升模型性能。

Method: 将Barlow Twins的冗余减少目标与DINO的自蒸馏策略结合，在MS COCO数据集上训练混合模型。

Result: 混合模型在仅使用10%标签数据的情况下，性能与DINO相当，同时保持了强特征表示能力。

Conclusion: 该方法为资源受限环境下的视觉Transformer训练提供了高效且可扩展的解决方案。

Abstract: Training AI models to understand images without costly labeled data remains a
challenge. We combine two techniques--DINO (teacher-student learning) and
Barlow Twins (redundancy reduction)--to create a model that learns better with
fewer labels and less compute. While both DINO and Barlow Twins have
independently demonstrated strong performance in self-supervised learning, each
comes with limitations--DINO may be sensitive to certain augmentations, and
Barlow Twins often requires batch sizes too large to fit on consumer hardware.
By combining the redundancy-reduction objective of Barlow Twins with the
self-distillation strategy of DINO, we aim to leverage their complementary
strengths. We train a hybrid model on the MS COCO dataset using only 10\% of
labeled data for linear probing, and evaluate its performance against
standalone DINO and Barlow Twins implementations. Preliminary results show that
the combined approach achieves comparable loss and classification accuracy to
DINO while maintaining strong feature representations. Attention visualizations
further suggest improved semantic segmentation capability in the hybrid model.
This combined method offers a scalable, label-efficient alternative for
training ViTs in resource-constrained environments.

</details>


### [113] [OmniMRI: A Unified Vision--Language Foundation Model for Generalist MRI Interpretation](https://arxiv.org/abs/2508.17524)
*Xingxin He,Aurora Rofena,Ruimin Feng,Haozhe Liao,Zhaoye Zhou,Albert Jang,Fang Liu*

Main category: cs.CV

TL;DR: OmniMRI是一个统一的视觉-语言基础模型，旨在泛化整个MRI工作流程，整合图像和临床语言数据。


<details>
  <summary>Details</summary>
Motivation: MRI工作流程分散且缺乏通用性，现有方法通常局限于特定解剖或应用，且未整合临床语言信息。

Method: OmniMRI通过多阶段训练（自监督视觉预训练、视觉-语言对齐、多模态预训练和多任务指令调优）在大规模异构数据上训练。

Result: OmniMRI能够在一个架构中执行多种任务，包括MRI重建、分割、异常检测、诊断建议和报告生成。

Conclusion: OmniMRI有潜力将分散的MRI工作流程整合为一个可扩展的通用框架，推动成像与临床语言的统一。

Abstract: Magnetic Resonance Imaging (MRI) is indispensable in clinical practice but
remains constrained by fragmented, multi-stage workflows encompassing
acquisition, reconstruction, segmentation, detection, diagnosis, and reporting.
While deep learning has achieved progress in individual tasks, existing
approaches are often anatomy- or application-specific and lack generalizability
across diverse clinical settings. Moreover, current pipelines rarely integrate
imaging data with complementary language information that radiologists rely on
in routine practice. Here, we introduce OmniMRI, a unified vision-language
foundation model designed to generalize across the entire MRI workflow. OmniMRI
is trained on a large-scale, heterogeneous corpus curated from 60 public
datasets, over 220,000 MRI volumes and 19 million MRI slices, incorporating
image-only data, paired vision-text data, and instruction-response data. Its
multi-stage training paradigm, comprising self-supervised vision pretraining,
vision-language alignment, multimodal pretraining, and multi-task instruction
tuning, progressively equips the model with transferable visual
representations, cross-modal reasoning, and robust instruction-following
capabilities. Qualitative results demonstrate OmniMRI's ability to perform
diverse tasks within a single architecture, including MRI reconstruction,
anatomical and pathological segmentation, abnormality detection, diagnostic
suggestion, and radiology report generation. These findings highlight OmniMRI's
potential to consolidate fragmented pipelines into a scalable, generalist
framework, paving the way toward foundation models that unify imaging and
clinical language for comprehensive, end-to-end MRI interpretation.

</details>


### [114] [Minimal Solvers for Full DoF Motion Estimation from Asynchronous Tracks](https://arxiv.org/abs/2508.17537)
*Petr Hruby,Marc Pollefeys*

Main category: cs.CV

TL;DR: 论文提出了一种多项式近似方法，用于从异步点轨迹估计相机的平移和角速度，适用于滚动快门和事件相机。


<details>
  <summary>Details</summary>
Motivation: 解决从异步点轨迹估计相机速度的非多项式问题，适用于滚动快门和事件相机。

Method: 提出多项式近似方法，分类最小问题并确定其代数度，开发低阶最小求解器。

Result: 在合成和真实数据集上评估了求解器的性能。

Conclusion: 该方法有效，代码将公开。

Abstract: We address the problem of estimating both translational and angular velocity
of a camera from asynchronous point tracks, a formulation relevant to rolling
shutter and event cameras. Since the original problem is non-polynomial, we
propose a polynomial approximation, classify the resulting minimal problems,
and determine their algebraic degrees. Furthermore, we develop minimal solvers
for several problems with low degrees and evaluate them on synthetic and real
datasets. The code will be made publicly available.

</details>


### [115] [Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection](https://arxiv.org/abs/2508.17567)
*Daniel Frees,Moritz Bolling,Aditri Bhagirath*

Main category: cs.CV

TL;DR: 该论文研究了在医学影像分类任务中，使用RadImageNet和ImageNet预训练对模型性能的影响，并确定了最优的CNN架构。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据稀缺，限制了从头训练模型的性能，因此需要探索预训练方法以提升模型效果。

Method: 通过比较RadImageNet和ImageNet预训练，研究了不同CNN架构（如ResNet50和1维卷积分类器）在乳腺癌和ACL撕裂检测任务中的表现。

Result: 最佳模型在ACL撕裂检测和乳腺癌结节恶性检测中分别达到0.9969和0.9641的AUC值，但未发现RadImageNet预训练显著优于ImageNet。

Conclusion: 研究确定了最优的CNN架构和预训练策略，但RadImageNet预训练并未在所有任务中表现出显著优势。

Abstract: Modern computer vision models have proven to be highly useful for medical
imaging classification and segmentation tasks, but the scarcity of medical
imaging data often limits the efficacy of models trained from scratch. Transfer
learning has emerged as a pivotal solution to this, enabling the fine-tuning of
high-performance models on small data. Mei et al. (2022) found that
pre-training CNNs on a large dataset of radiologist-labeled images
(RadImageNet) enhanced model performance on downstream tasks compared to
ImageNet pretraining. The present work extends Mei et al. (2022) by conducting
a comprehensive investigation to determine optimal CNN architectures for breast
lesion malignancy detection and ACL tear detection, as well as performing
statistical analysis to compare the effect of RadImageNet and ImageNet
pre-training on downstream model performance. Our findings suggest that
1-dimensional convolutional classifiers with skip connections, ResNet50
pre-trained backbones, and partial backbone unfreezing yields optimal
downstream medical classification performance. Our best models achieve AUCs of
0.9969 for ACL tear detection and 0.9641 for breast nodule malignancy
detection, competitive with the results reported by Mei et al. (2022) and
surpassing other previous works. We do not find evidence confirming RadImageNet
pre-training to provide superior downstream performance for ACL tear and breast
lesion classification tasks.

</details>


### [116] [MetaGen: A DSL, Database, and Benchmark for VLM-Assisted Metamaterial Generation](https://arxiv.org/abs/2508.17568)
*Liane Makatura,Benjamin Jones,Siyuan Bian,Wojciech Matusik*

Main category: cs.CV

TL;DR: 论文提出了MetaDSL、MetaDB和MetaBench三个工具，用于解决超材料设计中的几何复杂性和行为映射问题。


<details>
  <summary>Details</summary>
Motivation: 超材料的设计因几何复杂性和行为映射的非直观性而困难，需要一种集成化的解决方案。

Method: 开发了MetaDSL（领域特定语言）、MetaDB（数据库）和MetaBench（基准测试套件），并结合视觉语言模型进行优化。

Result: 构建了包含15万+参数化设计的数据库，并通过案例研究验证了框架的有效性。

Conclusion: 该框架为超材料的设计和理解结构-表示-性能关系提供了重要基础。

Abstract: Metamaterials are micro-architected structures whose geometry imparts highly
tunable-often counter-intuitive-bulk properties. Yet their design is difficult
because of geometric complexity and a non-trivial mapping from architecture to
behaviour. We address these challenges with three complementary contributions.
(i) MetaDSL: a compact, semantically rich domain-specific language that
captures diverse metamaterial designs in a form that is both human-readable and
machine-parsable. (ii) MetaDB: a curated repository of more than 150,000
parameterized MetaDSL programs together with their
derivatives-three-dimensional geometry, multi-view renderings, and simulated
elastic properties. (iii) MetaBench: benchmark suites that test three core
capabilities of vision-language metamaterial assistants-structure
reconstruction, property-driven inverse design, and performance prediction. We
establish baselines by fine-tuning state-of-the-art vision-language models and
deploy an omni-model within an interactive, CAD-like interface. Case studies
show that our framework provides a strong first step toward integrated design
and understanding of structure-representation-property relationships.

</details>


### [117] [IDU: Incremental Dynamic Update of Existing 3D Virtual Environments with New Imagery Data](https://arxiv.org/abs/2508.17579)
*Meida Chen,Luis Leal,Yue Hu,Rong Liu,Butian Xiong,Andrew Feng,Jiuyi Xu,Yangming Shi*

Main category: cs.CV

TL;DR: 提出了增量动态更新（IDU）管道，通过少量新图像高效更新3D重建模型，适用于军事场景的动态环境。


<details>
  <summary>Details</summary>
Motivation: 军事组织需要频繁更新高分辨率3D虚拟环境，但传统方法耗时且成本高。

Method: 通过相机姿态估计、变化检测、3D生成AI模型和人工指导，逐步更新3D模型。

Result: 实验证明IDU管道显著减少了更新时间和人力成本。

Conclusion: IDU提供了一种高效、经济的解决方案，适用于快速变化的军事场景。

Abstract: For simulation and training purposes, military organizations have made
substantial investments in developing high-resolution 3D virtual environments
through extensive imaging and 3D scanning. However, the dynamic nature of
battlefield conditions-where objects may appear or vanish over time-makes
frequent full-scale updates both time-consuming and costly. In response, we
introduce the Incremental Dynamic Update (IDU) pipeline, which efficiently
updates existing 3D reconstructions, such as 3D Gaussian Splatting (3DGS), with
only a small set of newly acquired images. Our approach starts with camera pose
estimation to align new images with the existing 3D model, followed by change
detection to pinpoint modifications in the scene. A 3D generative AI model is
then used to create high-quality 3D assets of the new elements, which are
seamlessly integrated into the existing 3D model. The IDU pipeline incorporates
human guidance to ensure high accuracy in object identification and placement,
with each update focusing on a single new object at a time. Experimental
results confirm that our proposed IDU pipeline significantly reduces update
time and labor, offering a cost-effective and targeted solution for maintaining
up-to-date 3D models in rapidly evolving military scenarios.

</details>


### [118] [HERO: Hierarchical Extrapolation and Refresh for Efficient World Models](https://arxiv.org/abs/2508.17588)
*Quanjian Song,Xinyu Wang,Donghao Zhou,Jingyu Lin,Cunjian Chen,Yue Ma,Xiu Li*

Main category: cs.CV

TL;DR: HERO是一个无需训练的分层加速框架，针对高效世界模型设计，通过分层策略显著提升推理速度，同时保持高质量输出。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成驱动的世界模型中推理速度慢，现有加速技术直接应用会导致质量下降，因此需要一种专门针对世界模型的高效加速方法。

Method: HERO采用分层策略：(i)浅层使用块状刷新机制选择需重新计算的token；(ii)深层通过线性外推直接估计中间特征，绕过注意力模块和前馈网络的计算。

Result: 实验表明，HERO实现了1.73倍的加速，且质量下降最小，显著优于现有扩散加速方法。

Conclusion: HERO通过分层策略有效解决了世界模型推理效率问题，为高质量快速生成虚拟环境提供了可行方案。

Abstract: Generation-driven world models create immersive virtual environments but
suffer slow inference due to the iterative nature of diffusion models. While
recent advances have improved diffusion model efficiency, directly applying
these techniques to world models introduces limitations such as quality
degradation. In this paper, we present HERO, a training-free hierarchical
acceleration framework tailored for efficient world models. Owing to the
multi-modal nature of world models, we identify a feature coupling phenomenon,
wherein shallow layers exhibit high temporal variability, while deeper layers
yield more stable feature representations. Motivated by this, HERO adopts
hierarchical strategies to accelerate inference: (i) In shallow layers, a
patch-wise refresh mechanism efficiently selects tokens for recomputation. With
patch-wise sampling and frequency-aware tracking, it avoids extra metric
computation and remain compatible with FlashAttention. (ii) In deeper layers, a
linear extrapolation scheme directly estimates intermediate features. This
completely bypasses the computations in attention modules and feed-forward
networks. Our experiments show that HERO achieves a 1.73$\times$ speedup with
minimal quality degradation, significantly outperforming existing diffusion
acceleration methods.

</details>


### [119] [TinyGiantVLM: A Lightweight Vision-Language Architecture for Spatial Reasoning under Resource Constraints](https://arxiv.org/abs/2508.17595)
*Vinh-Thuan Ly,Hoang M. Truong,Xuan-Huong Nguyen*

Main category: cs.CV

TL;DR: TinyGiantVLM是一个轻量级、模块化的两阶段框架，用于解决仓库环境中细粒度空间关系推理的挑战，通过多模态特征融合和动态专家混合模块提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在复杂工业场景中难以理解3D布局和物体排列，需要一种更有效的空间推理方法。

Method: 采用两阶段训练策略，结合全局和区域级特征编码，使用Mixture-of-Experts模块动态融合多模态输入。

Result: 在AI City Challenge 2025中，64M参数模型排名第五，80M参数变体进一步提升了性能。

Conclusion: TinyGiantVLM在工业环境中有效结合视觉感知与空间理解，展示了轻量级模型的潜力。

Abstract: Reasoning about fine-grained spatial relationships in warehouse-scale
environments poses a significant challenge for existing vision-language models
(VLMs), which often struggle to comprehend 3D layouts, object arrangements, and
multimodal cues in real-world industrial settings. In this paper, we present
TinyGiantVLM, a lightweight and modular two-stage framework designed for
physical spatial reasoning, distinguishing itself from traditional geographic
reasoning in complex logistics scenes. Our approach encodes both global and
region-level features from RGB and depth modalities using pretrained visual
backbones. To effectively handle the complexity of high-modality inputs and
diverse question types, we incorporate a Mixture-of-Experts (MoE) fusion
module, which dynamically combines spatial representations to support
downstream reasoning tasks and improve convergence. Training is conducted in a
two-phase strategy: the first phase focuses on generating free-form answers to
enhance spatial reasoning ability, while the second phase uses normalized
answers for evaluation. Evaluated on Track 3 of the AI City Challenge 2025, our
64M-parameter base model achieved 5th place on the leaderboard with a score of
66.8861, demonstrating strong performance in bridging visual perception and
spatial understanding in industrial environments. We further present an
80M-parameter variant with expanded MoE capacity, which demonstrates improved
performance on spatial reasoning tasks.

</details>


### [120] [HotSpotter - Patterned Species Instance Recognition](https://arxiv.org/abs/2508.17605)
*Jonathan P. Crall,Charles V. Stewart,Tanya Y. Berger-Wolf,Daniel I. Rubenstein,Siva R. Sundaresan*

Main category: cs.CV

TL;DR: HotSpotter是一种快速、准确的算法，用于从标记数据库中识别个体动物，适用于多种物种。


<details>
  <summary>Details</summary>
Motivation: 开发一种不依赖物种的个体动物识别方法，以提高识别准确性和速度。

Method: 基于关键点（热点）提取和匹配的两种方法：顺序匹配和快速最近邻搜索。

Result: 在1000多张图像的数据库中，比现有方法更准确，且每张查询图像仅需几秒。

Conclusion: HotSpotter是一种高效、通用的个体动物识别算法。

Abstract: We present HotSpotter, a fast, accurate algorithm for identifying individual
animals against a labeled database. It is not species specific and has been
applied to Grevy's and plains zebras, giraffes, leopards, and lionfish. We
describe two approaches, both based on extracting and matching keypoints or
"hotspots". The first tests each new query image sequentially against each
database image, generating a score for each database image in isolation, and
ranking the results. The second, building on recent techniques for instance
recognition, matches the query image against the database using a fast nearest
neighbor search. It uses a competitive scoring mechanism derived from the Local
Naive Bayes Nearest Neighbor algorithm recently proposed for category
recognition. We demonstrate results on databases of more than 1000 images,
producing more accurate matches than published methods and matching each query
image in just a few seconds.

</details>


### [121] [A Weighted Vision Transformer-Based Multi-Task Learning Framework for Predicting ADAS-Cog Scores](https://arxiv.org/abs/2508.17613)
*Nur Amirah Abd Hamid,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: 提出了一种基于加权Vision Transformer的多任务学习框架，用于预测ADAS-Cog全局评分及其13个子评分，通过子评分特定的损失加权提升预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有方法多关注ADAS-Cog全局评分预测，忽视了子评分的预测价值，而子评分可能对全局评分有更大影响。

Method: 使用加权Vision Transformer的多任务学习框架，结合基线MRI扫描数据，预测24个月后的ADAS-Cog全局评分及其子评分。

Result: 结果表明，强加权策略对MRI模式更异质的MCI受试者效果更好，而中等加权对变异较低的CN受试者更有效。

Conclusion: 提出的框架为AD预后提供了一种灵活且可解释的端到端MRI学习方法，避免了均匀加权对关键子评分的利用不足。

Abstract: Prognostic modeling is essential for forecasting future clinical scores and
enabling early detection of Alzheimers disease (AD). While most existing
methods focus on predicting the ADAS-Cog global score, they often overlook the
predictive value of its 13 sub-scores, which reflect distinct cognitive
domains. Some sub-scores may exert greater influence on determining global
scores. Assigning higher loss weights to these clinically meaningful sub-scores
can guide the model to focus on more relevant cognitive domains, enhancing both
predictive accuracy and interpretability. In this study, we propose a weighted
Vision Transformer (ViT)-based multi-task learning (MTL) framework to jointly
predict the ADAS-Cog global score using baseline MRI scans and its 13
sub-scores at Month 24. Our framework integrates ViT as a feature extractor and
systematically investigates the impact of sub-score-specific loss weighting on
model performance. Results show that our proposed weighting strategies are
group-dependent: strong weighting improves performance for MCI subjects with
more heterogeneous MRI patterns, while moderate weighting is more effective for
CN subjects with lower variability. Our findings suggest that uniform weighting
underutilizes key sub-scores and limits generalization. The proposed framework
offers a flexible, interpretable approach to AD prognosis using end-to-end
MRI-based learning. (Github repo link will be provided after review)

</details>


### [122] [JCo-MVTON: Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free Virtual Try-on](https://arxiv.org/abs/2508.17614)
*Aowen Wang,Wei Li,Hao Luo,Mengxing Ao,Chenyu Zhu,Xinyang Li,Fan Wang*

Main category: cs.CV

TL;DR: JCo-MVTON是一种新型的虚拟试穿框架，通过结合扩散模型和多模态条件融合，解决了传统方法依赖人体掩码、控制粒度不足和泛化能力差的问题。


<details>
  <summary>Details</summary>
Motivation: 传统虚拟试穿系统依赖人体掩码，对服装属性的控制有限，且难以泛化到真实场景。

Method: 提出基于多模态扩散Transformer的框架，通过条件路径融合参考图像和目标服装图像，并改进位置编码和注意力掩码以实现精确对齐。

Result: 在公开基准测试中表现优异，显著超越现有方法，并在实际应用中展现出强大的泛化能力。

Conclusion: JCo-MVTON通过创新设计和数据集构建策略，实现了高性能和广泛适用性，为虚拟试穿领域提供了新思路。

Abstract: Virtual try-on systems have long been hindered by heavy reliance on human
body masks, limited fine-grained control over garment attributes, and poor
generalization to real-world, in-the-wild scenarios. In this paper, we propose
JCo-MVTON (Jointly Controllable Multi-Modal Diffusion Transformer for Mask-Free
Virtual Try-On), a novel framework that overcomes these limitations by
integrating diffusion-based image generation with multi-modal conditional
fusion. Built upon a Multi-Modal Diffusion Transformer (MM-DiT) backbone, our
approach directly incorporates diverse control signals -- such as the reference
person image and the target garment image -- into the denoising process through
dedicated conditional pathways that fuse features within the self-attention
layers. This fusion is further enhanced with refined positional encodings and
attention masks, enabling precise spatial alignment and improved garment-person
integration. To address data scarcity and quality, we introduce a bidirectional
generation strategy for dataset construction: one pipeline uses a mask-based
model to generate realistic reference images, while a symmetric ``Try-Off''
model, trained in a self-supervised manner, recovers the corresponding garment
images. The synthesized dataset undergoes rigorous manual curation, allowing
iterative improvement in visual fidelity and diversity. Experiments demonstrate
that JCo-MVTON achieves state-of-the-art performance on public benchmarks
including DressCode, significantly outperforming existing methods in both
quantitative metrics and human evaluations. Moreover, it shows strong
generalization in real-world applications, surpassing commercial systems.

</details>


### [123] [Improving Interpretability in Alzheimer's Prediction via Joint Learning of ADAS-Cog Scores](https://arxiv.org/abs/2508.17619)
*Nur Amirah Abd Hamid,Mohd Shahrizal Rusli,Muhammad Thaqif Iman Mohd Taufek,Mohd Ibrahim Shapiai,Daphne Teck Ching Lai*

Main category: cs.CV

TL;DR: 提出多任务学习框架，联合预测ADAS-Cog总分及其子项，提升总分预测准确性，并分析子项贡献。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注ADAS-Cog总分预测，忽视了子项对认知衰退的领域特异性预测价值。

Method: 使用Vision Transformer和Swin Transformer提取MRI特征，结合纵向临床数据建模认知进展。

Result: 子项学习提升总分预测，但部分关键子项预测误差高，源于临床特征主导模型。

Conclusion: 需改进多模态融合和自适应损失加权，以实现更平衡的学习和更稳健的AD预测框架。

Abstract: Accurate prediction of clinical scores is critical for early detection and
prognosis of Alzheimers disease (AD). While existing approaches primarily focus
on forecasting the ADAS-Cog global score, they often overlook the predictive
value of its sub-scores (13 items), which capture domain-specific cognitive
decline. In this study, we propose a multi task learning (MTL) framework that
jointly predicts the global ADAS-Cog score and its sub-scores (13 items) at
Month 24 using baseline MRI and longitudinal clinical scores from baseline and
Month 6. The main goal is to examine how each sub scores particularly those
associated with MRI features contribute to the prediction of the global score,
an aspect largely neglected in prior MTL studies. We employ Vision Transformer
(ViT) and Swin Transformer architectures to extract imaging features, which are
fused with longitudinal clinical inputs to model cognitive progression. Our
results show that incorporating sub-score learning improves global score
prediction. Subscore level analysis reveals that a small subset especially Q1
(Word Recall), Q4 (Delayed Recall), and Q8 (Word Recognition) consistently
dominates the predicted global score. However, some of these influential
sub-scores exhibit high prediction errors, pointing to model instability.
Further analysis suggests that this is caused by clinical feature dominance,
where the model prioritizes easily predictable clinical scores over more
complex MRI derived features. These findings emphasize the need for improved
multimodal fusion and adaptive loss weighting to achieve more balanced
learning. Our study demonstrates the value of sub score informed modeling and
provides insights into building more interpretable and clinically robust AD
prediction frameworks. (Github repo provided)

</details>


### [124] [Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes](https://arxiv.org/abs/2508.17634)
*Ryan Faulkner,Ian Reid,Simon Ratcliffe,Tat-Jun Chin*

Main category: cs.CV

TL;DR: 提出了一种基于重建的开放集分割方法，结合了Mamba架构的长距离依赖性和可扩展性，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 在户外场景中，LiDAR扫描会产生大规模点云数据，但训练数据外的异常对象难以避免，需要有效的开放集分割方法。

Method: 结合物体缺陷检测研究和Mamba架构，提出了一种基于重建的开放集分割方法。

Result: 该方法不仅提升了自身方法的性能，还能提升现有方法的性能，并在大规模点云数据上表现优异。

Conclusion: 该方法为户外场景的开放集分割提供了有效解决方案，并展示了Mamba架构的潜力。

Abstract: LiDAR scanning in outdoor scenes acquires accurate distance measurements over
wide areas, producing large-scale point clouds. Application examples for this
data include robotics, automotive vehicles, and land surveillance. During such
applications, outlier objects from outside the training data will inevitably
appear. Our research contributes a novel approach to open-set segmentation,
leveraging the learnings of object defect-detection research. We also draw on
the Mamba architecture's strong performance in utilising long-range
dependencies and scalability to large data. Combining both, we create a
reconstruction based approach for the task of outdoor scene open-set
segmentation. We show that our approach improves performance not only when
applied to our our own open-set segmentation method, but also when applied to
existing methods. Furthermore we contribute a Mamba based architecture which is
competitive with existing voxel-convolution based methods on challenging,
large-scale pointclouds.

</details>


### [125] [Wound3DAssist: A Practical Framework for 3D Wound Assessment](https://arxiv.org/abs/2508.17635)
*Remi Chierchia,Rodrigo Santa Cruz,Léo Lebrat,Yulia Arzhaeva,Mohammad Ali Armin,Jeremy Oorloff,Chuong Nguyen,Olivier Salvado,Clinton Fookes,David Ahmedt-Aristizabal*

Main category: cs.CV

TL;DR: Wound3DAssist是一个基于单目消费级视频的3D伤口评估框架，解决了传统2D方法的视角失真和深度测量问题。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口管理依赖主观且耗时的传统方法，现有2D数字视频测量存在视角失真和深度测量不足的问题。

Method: 通过智能手机视频生成3D模型，结合3D重建、伤口分割、组织分类和周围分析，实现自动化测量。

Result: 在数字模型、硅胶模型和真实患者中验证，显示毫米级精度和可靠的组织分析，评估时间少于20分钟。

Conclusion: Wound3DAssist为临床提供了高效、准确的3D伤口评估解决方案。

Abstract: Managing chronic wounds remains a major healthcare challenge, with clinical
assessment often relying on subjective and time-consuming manual documentation
methods. Although 2D digital videometry frameworks aided the measurement
process, these approaches struggle with perspective distortion, a limited field
of view, and an inability to capture wound depth, especially in anatomically
complex or curved regions. To overcome these limitations, we present
Wound3DAssist, a practical framework for 3D wound assessment using monocular
consumer-grade videos. Our framework generates accurate 3D models from short
handheld smartphone video recordings, enabling non-contact, automatic
measurements that are view-independent and robust to camera motion. We
integrate 3D reconstruction, wound segmentation, tissue classification, and
periwound analysis into a modular workflow. We evaluate Wound3DAssist across
digital models with known geometry, silicone phantoms, and real patients.
Results show that the framework supports high-quality wound bed visualization,
millimeter-level accuracy, and reliable tissue composition analysis. Full
assessments are completed in under 20 minutes, demonstrating feasibility for
real-world clinical use.

</details>


### [126] [Few-Shot Pattern Detection via Template Matching and Regression](https://arxiv.org/abs/2508.17636)
*Eunchan Jo,Dahyun Kang,Sanghyun Kim,Yunseon Choi,Minsu Cho*

Main category: cs.CV

TL;DR: 提出了一种基于模板匹配和回归的简单有效检测器TMR，用于解决少样本模式检测问题，并在新数据集RPINE上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在少样本对象计数和检测中表现有限，无法定位非对象模式，因此需要一种更通用的方法。

Method: 结合模板匹配和回归，通过少量可学习层保留目标样本的空间布局信息。

Result: 在RPINE、FSCD-147和FSCD-LVIS三个基准测试中优于现有方法，并展示了跨数据集的强泛化能力。

Conclusion: TMR方法简单有效，适用于多种模式检测任务，尤其在非对象模式上表现突出。

Abstract: We address the problem of few-shot pattern detection, which aims to detect
all instances of a given pattern, typically represented by a few exemplars,
from an input image. Although similar problems have been studied in few-shot
object counting and detection (FSCD), previous methods and their benchmarks
have narrowed patterns of interest to object categories and often fail to
localize non-object patterns. In this work, we propose a simple yet effective
detector based on template matching and regression, dubbed TMR. While previous
FSCD methods typically represent target exemplars as spatially collapsed
prototypes and lose structural information, we revisit classic template
matching and regression. It effectively preserves and leverages the spatial
layout of exemplars through a minimalistic structure with a small number of
learnable convolutional or projection layers on top of a frozen backbone We
also introduce a new dataset, dubbed RPINE, which covers a wider range of
patterns than existing object-centric datasets. Our method outperforms the
state-of-the-art methods on the three benchmarks, RPINE, FSCD-147, and
FSCD-LVIS, and demonstrates strong generalization in cross-dataset evaluation.

</details>


### [127] [Dynamic Embedding of Hierarchical Visual Features for Efficient Vision-Language Fine-Tuning](https://arxiv.org/abs/2508.17638)
*Xinyu Wei,Guoli Yang,Jialu Zhou,Mingyue Yang,Leqian Li,Kedi Zhang,Chunping Qiu*

Main category: cs.CV

TL;DR: DEHVF是一种基于动态嵌入和分层视觉特征融合的高效视觉语言微调方法，解决了现有方法中序列长度增加和视觉信息利用不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在融合视觉信息时导致输入序列长度显著增加，且忽略了视觉编码器的分层语义表示和细粒度视觉信息。

Method: 通过轻量级分层视觉融合器，动态选择和融合与LLM各层语义粒度对应的分层视觉特征，并将其嵌入到LLM的FFN中。

Result: 在多个VL基准测试中，DEHVF比现有参数高效微调方法（PEFT）具有更高的准确性，同时保持高效训练和推理。

Conclusion: DEHVF通过动态融合分层视觉特征，实现了跨模态信息的精确对齐和互补，避免了序列扩展问题。

Abstract: Large Vision-Language Models (LVLMs) commonly follow a paradigm that projects
visual features and then concatenates them with text tokens to form a unified
sequence input for Large Language Models (LLMs). However, this paradigm leads
to a significant increase in the length of the input sequence, resulting in
substantial computational overhead. Existing methods attempt to fuse visual
information into the intermediate layers of LLMs, which alleviate the sequence
length issue but often neglect the hierarchical semantic representations within
the model and the fine-grained visual information available in the shallower
visual encoding layers. To address this limitation, we propose DEHVF, an
efficient vision-language fine-tuning method based on dynamic embedding and
fusion of hierarchical visual features. Its core lies in leveraging the
inherent hierarchical representation characteristics of visual encoders and
language models. Through a lightweight hierarchical visual fuser, it
dynamically selects and fuses hierarchical features corresponding to semantic
granularity based on the internal representations of each layer in LLMs. The
fused layer-related visual features are then projected and aligned before being
directly embedded into the Feed-Forward Network (FFN) of the corresponding
layer in LLMs. This approach not only avoids sequence expansion but also
dynamically fuses multi-layer visual information. By fine-tuning only a small
number of parameters, DEHVF achieves precise alignment and complementarity of
cross-modal information at the same semantic granularity. We conducted
experiments across various VL benchmarks, including visual question answering
on ScienceQA and image captioning on COCO Captions. The results demonstrate
that DEHVF achieves higher accuracy than existing parameter-efficient
fine-tuning (PEFT) baselines while maintaining efficient training and
inference.

</details>


### [128] [HyTver: A Novel Loss Function for Longitudinal Multiple Sclerosis Lesion Segmentation](https://arxiv.org/abs/2508.17639)
*Dayan Perera,Ting Fung Fung,Vishnu Monn*

Main category: cs.CV

TL;DR: 论文提出了一种名为HyTver的新型混合损失函数，用于解决多发性硬化症病灶分割中的输入输出不平衡问题，同时保持其他指标的优异表现。


<details>
  <summary>Details</summary>
Motivation: 多发性硬化症病灶分割面临数据不平衡的挑战，现有损失函数（如Dice损失或交叉熵损失）未能有效解决这一问题，且存在计算复杂或性能不足的问题。

Method: 提出HyTver混合损失函数，通过结合多种损失函数的优势，优化分割性能并兼顾其他指标。

Result: HyTver在Dice得分上达到0.659，同时在距离指标上表现与其他流行函数相当。

Conclusion: HyTver是一种高效且稳定的损失函数，适用于多发性硬化症病灶分割任务。

Abstract: Longitudinal Multiple Sclerosis Lesion Segmentation is a particularly
challenging problem that involves both input and output imbalance in the data
and segmentation. Therefore in order to develop models that are practical, one
of the solutions is to develop better loss functions. Most models naively use
either Dice loss or Cross-Entropy loss or their combination without too much
consideration. However, one must select an appropriate loss function as the
imbalance can be mitigated by selecting a proper loss function. In order to
solve the imbalance problem, multiple loss functions were proposed that claimed
to solve it. They come with problems of their own which include being too
computationally complex due to hyperparameters as exponents or having
detrimental performance in metrics other than region-based ones. We propose a
novel hybrid loss called HyTver that achieves good segmentation performance
while maintaining performance in other metrics. We achieve a Dice score of
0.659 while also ensuring that the distance-based metrics are comparable to
other popular functions. In addition, we also evaluate the stability of the
loss functions when used on a pre- trained model and perform extensive
comparisons with other popular loss functions

</details>


### [129] [FloraSyntropy-Net: Scalable Deep Learning with Novel FloraSyntropy Archive for Large-Scale Plant Disease Diagnosis](https://arxiv.org/abs/2508.17653)
*Saif Ur Rehman Khan,Muhammad Nabeel Asim,Sebastian Vollmer,Andreas Dengel*

Main category: cs.CV

TL;DR: 论文提出了一种名为FloraSyntropy-Net的新型联邦学习框架，用于植物病害的早期诊断，并在大规模数据集上实现了高准确率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有AI模型在植物病害诊断中缺乏泛化能力，无法适应农业多样性，因此需要一种更通用的解决方案。

Method: 通过结合Memetic算法选择最优基础模型（DenseNet201）、新型Deep Block增强特征表示，以及客户端克隆策略，构建了联邦学习框架。

Result: 在FloraSyntropy数据集上达到96.38%的准确率，在无关的Pest数据集上达到99.84%的准确率。

Conclusion: 该研究不仅提供了新资源，还提出了一种高度泛化的框架，推动了农业AI的实际应用。

Abstract: Early diagnosis of plant diseases is critical for global food safety, yet
most AI solutions lack the generalization required for real-world agricultural
diversity. These models are typically constrained to specific species, failing
to perform accurately across the broad spectrum of cultivated plants. To
address this gap, we first introduce the FloraSyntropy Archive, a large-scale
dataset of 178,922 images across 35 plant species, annotated with 97 distinct
disease classes. We establish a benchmark by evaluating numerous existing
models on this archive, revealing a significant performance gap. We then
propose FloraSyntropy-Net, a novel federated learning framework (FL) that
integrates a Memetic Algorithm (MAO) for optimal base model selection
(DenseNet201), a novel Deep Block for enhanced feature representation, and a
client-cloning strategy for scalable, privacy-preserving training.
FloraSyntropy-Net achieves a state-of-the-art accuracy of 96.38% on the
FloraSyntropy benchmark. Crucially, to validate its generalization capability,
we test the model on the unrelated multiclass Pest dataset, where it
demonstrates exceptional adaptability, achieving 99.84% accuracy. This work
provides not only a valuable new resource but also a robust and highly
generalizable framework that advances the field towards practical, large-scale
agricultural AI applications.

</details>


### [130] [Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark](https://arxiv.org/abs/2508.17658)
*Yaolei Qi,Yikai Yang,Wenbo Peng,Shumei Miao,Yutao Hu,Guanyu Yang*

Main category: cs.CV

TL;DR: 本文提出了一种基于点云的管状结构补全方法（TSRNet），并建立了首个真实临床数据衍生的点云冠状动脉补全数据集（PC-CAC），显著提升了管状结构补全的性能。


<details>
  <summary>Details</summary>
Motivation: 现有分割算法在严重临床病例（如冠状动脉狭窄和血管闭塞）中难以处理结构不连续问题，影响诊断准确性，因此需要补全不连续结构。

Method: 提出TSRNet网络，结合细节保留特征提取器、多重密集细化策略和全局到局部损失函数，确保补全准确性。

Result: 在PC-CAC和两个公共数据集上的实验表明，TSRNet在多项评估指标上优于现有方法。

Conclusion: TSRNet为基于点云的管状结构重建设立了新基准，数据集和代码已开源。

Abstract: Complex tubular structures are essential in medical imaging and
computer-assisted diagnosis, where their integrity enhances anatomical
visualization and lesion detection. However, existing segmentation algorithms
struggle with structural discontinuities, particularly in severe clinical cases
such as coronary artery stenosis and vessel occlusions, which leads to
undesired discontinuity and compromising downstream diagnostic accuracy.
Therefore, it is imperative to reconnect discontinuous structures to ensure
their completeness. In this study, we explore the tubular structure completion
based on point cloud for the first time and establish a Point Cloud-based
Coronary Artery Completion (PC-CAC) dataset, which is derived from real
clinical data. This dataset provides a novel benchmark for tubular structure
completion. Additionally, we propose TSRNet, a Tubular Structure Reconnection
Network that integrates a detail-preservated feature extractor, a multiple
dense refinement strategy, and a global-to-local loss function to ensure
accurate reconnection while maintaining structural integrity. Comprehensive
experiments on our PC-CAC and two additional public datasets (PC-ImageCAS and
PC-PTR) demonstrate that our method consistently outperforms state-of-the-art
approaches across multiple evaluation metrics, setting a new benchmark for
point cloud-based tubular structure reconstruction. Our benchmark is available
at https://github.com/YaoleiQi/PCCAC.

</details>


### [131] [M^3-GloDets: Multi-Region and Multi-Scale Analysis of Fine-Grained Diseased Glomerular Detection](https://arxiv.org/abs/2508.17666)
*Tianyu Shi,Xinzi He,Kenji Ikemura,Mert R. Sabuncu,Yihe Yang,Ruining Deng*

Main category: cs.CV

TL;DR: M^3-GloDet框架用于评估肾小球检测模型，填补了现有研究对病变肾小球亚型检测的不足。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注正常肾小球或全局硬化，而病变肾小球亚型的检测因形态多样性和复杂性被忽视，影响临床诊断准确性。

Method: 提出M^3-GloDet框架，系统评估不同区域、尺度和类别的检测模型，包括传统和最新算法，并分析不同图像分辨率和区域大小的影响。

Result: 中等大小的图像块和适度放大倍数在上下文与效率之间取得最佳平衡，并减少过拟合。

Conclusion: 该研究为肾小球自动检测模型的优化和临床应用提供了实用建议，推动了数字病理学的发展。

Abstract: Accurate detection of diseased glomeruli is fundamental to progress in renal
pathology and underpins the delivery of reliable clinical diagnoses. Although
recent advances in computer vision have produced increasingly sophisticated
detection algorithms, the majority of research efforts have focused on normal
glomeruli or instances of global sclerosis, leaving the wider spectrum of
diseased glomerular subtypes comparatively understudied. This disparity is not
without consequence; the nuanced and highly variable morphological
characteristics that define these disease variants frequently elude even the
most advanced computational models. Moreover, ongoing debate surrounds the
choice of optimal imaging magnifications and region-of-view dimensions for
fine-grained glomerular analysis, adding further complexity to the pursuit of
accurate classification and robust segmentation.
  To bridge these gaps, we present M^3-GloDet, a systematic framework designed
to enable thorough evaluation of detection models across a broad continuum of
regions, scales, and classes. Within this framework, we evaluate both
long-standing benchmark architectures and recently introduced state-of-the-art
models that have achieved notable performance, using an experimental design
that reflects the diversity of region-of-interest sizes and imaging resolutions
encountered in routine digital renal pathology. As the results, we found that
intermediate patch sizes offered the best balance between context and
efficiency. Additionally, moderate magnifications enhanced generalization by
reducing overfitting. Through systematic comparison of these approaches on a
multi-class diseased glomerular dataset, our aim is to advance the
understanding of model strengths and limitations, and to offer actionable
insights for the refinement of automated detection strategies and clinical
workflows in the digital pathology domain.

</details>


### [132] [Hierarchical Vision-Language Learning for Medical Out-of-Distribution Detection](https://arxiv.org/abs/2508.17667)
*Runhe Lai,Xinhua Lu,Kanghao Chen,Qichao Chen,Wei-Shi Zheng,Ruixuan Wang*

Main category: cs.CV

TL;DR: 提出了一种基于视觉语言模型的新型OOD检测框架，通过多尺度视觉融合和硬伪OOD样本生成策略，显著提升了未知疾病的识别能力。


<details>
  <summary>Details</summary>
Motivation: 在可信赖的医疗诊断系统中，识别未知疾病（OOD检测）对避免误诊至关重要。

Method: 采用多尺度视觉融合策略和硬伪OOD样本生成方法，增强医学图像的细节表示。

Result: 在三个公共医学数据集上验证，性能优于现有方法。

Conclusion: 该框架为医疗OOD检测提供了高效解决方案，代码已开源。

Abstract: In trustworthy medical diagnosis systems, integrating out-of-distribution
(OOD) detection aims to identify unknown diseases in samples, thereby
mitigating the risk of misdiagnosis. In this study, we propose a novel OOD
detection framework based on vision-language models (VLMs), which integrates
hierarchical visual information to cope with challenging unknown diseases that
resemble known diseases. Specifically, a cross-scale visual fusion strategy is
proposed to couple visual embeddings from multiple scales. This enriches the
detailed representation of medical images and thus improves the discrimination
of unknown diseases. Moreover, a cross-scale hard pseudo-OOD sample generation
strategy is proposed to benefit OOD detection maximally. Experimental
evaluations on three public medical datasets support that the proposed
framework achieves superior OOD detection performance compared to existing
methods. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/HVL.

</details>


### [133] [Language-Guided Temporal Token Pruning for Efficient VideoLLM Processing](https://arxiv.org/abs/2508.17686)
*Yogesh Kumar*

Main category: cs.CV

TL;DR: LGTTP通过语言引导的时间令牌修剪，显著减少计算开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决视觉语言模型在长视频中因注意力机制二次复杂度导致的计算效率问题。

Method: 利用查询中的时间线索自适应修剪视频令牌，保留上下文连续性。

Result: 计算量减少65%，性能保留97-99%，在特定任务中性能提升。

Conclusion: LGTTP在保持性能的同时显著提升计算效率，适用于多种视频理解任务。

Abstract: Vision Language Models (VLMs) struggle with long-form videos due to the
quadratic complexity of attention mechanisms. We propose Language-Guided
Temporal Token Pruning (LGTTP), which leverages temporal cues from queries to
adaptively prune video tokens, preserving contextual continuity while reducing
computational overhead. Unlike uniform pruning or keyframe selection, LGTTP
retains higher token density in temporally relevant segments. Our
model-agnostic framework integrates with TimeChat and LLaVA-Video, achieving a
65% reduction in computation while preserving 97-99% of the original
performance. On QVHighlights, LGTTP improves HIT@1 by +9.5%, and on
Charades-STA, it retains 99.6% of R@1. It excels on queries with explicit
temporal markers and remains effective across general video understanding
tasks.

</details>


### [134] [Benchmarking Class Activation Map Methods for Explainable Brain Hemorrhage Classification on Hemorica Dataset](https://arxiv.org/abs/2508.17699)
*Z. Rafati,M. Hoseyni,J. Khoramdel,A. Nikoofard*

Main category: cs.CV

TL;DR: 该研究通过CAM技术评估了脑出血诊断的可解释性，发现HiResCAM和AblationCAM在定位和分割表现最佳。


<details>
  <summary>Details</summary>
Motivation: 提高深度学习模型在医学影像中的透明度和临床信任。

Method: 使用9种CAM算法在EfficientNetV2S的不同阶段进行定量评估，采用Dice、IoU等指标。

Result: HiResCAM在边界框对齐上表现最佳，AblationCAM在像素级分割上表现最好（Dice 0.57，IoU 0.40）。

Conclusion: 研究为脑出血检测提供了可重复的CAM方法基准，展示了XAI在临床诊断中的潜力。

Abstract: Explainable Artificial Intelligence (XAI) has become an essential component
of medical imaging research, aiming to increase transparency and clinical trust
in deep learning models. This study investigates brain hemorrhage diagnosis
with a focus on explainability through Class Activation Mapping (CAM)
techniques. A pipeline was developed to extract pixellevel segmentation and
detection annotations from classification models using nine state-of-the-art
CAM algorithms, applied across multiple network stages, and quantitatively
evaluated on the Hemorica dataset, which uniquely provides both slice-level
labels and high-quality segmentation masks. Metrics including Dice, IoU, and
pixel-wise overlap were employed to benchmark CAM variants. Results show that
the strongest localization performance occurred at stage 5 of EfficientNetV2S,
with HiResCAM yielding the highest bounding-box alignment and AblationCAM
achieving the best pixel-level Dice (0.57) and IoU (0.40), representing strong
accuracy given that models were trained solely for classification without
segmentation supervision. To the best of current knowledge, this is among the f
irst works to quantitatively compare CAM methods for brain hemorrhage
detection, establishing a reproducible benchmark and underscoring the potential
of XAI-driven pipelines for clinically meaningful AI-assisted diagnosis.

</details>


### [135] [CATformer: Contrastive Adversarial Transformer for Image Super-Resolution](https://arxiv.org/abs/2508.17708)
*Qinyi Tian,Spence Cox,Laura E. Dalton*

Main category: cs.CV

TL;DR: CATformer是一种结合扩散模型、对抗学习和对比学习的超分辨率神经网络，通过双分支架构提升图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 超分辨率技术能提升低分辨率图像质量，但现有方法在效率和视觉质量上仍有改进空间。

Method: CATformer采用双分支架构，主分支基于扩散模型逐步优化潜在表示，辅助分支通过对比学习增强噪声鲁棒性，最终融合并解码。

Result: 实验表明，CATformer在效率和视觉质量上优于现有基于Transformer和扩散模型的方法。

Conclusion: CATformer弥合了Transformer、扩散模型和GAN方法之间的性能差距，为扩散模型在超分辨率中的应用奠定了基础。

Abstract: Super-resolution remains a promising technique to enhance the quality of
low-resolution images. This study introduces CATformer (Contrastive Adversarial
Transformer), a novel neural network integrating diffusion-inspired feature
refinement with adversarial and contrastive learning. CATformer employs a
dual-branch architecture combining a primary diffusion-inspired transformer,
which progressively refines latent representations, with an auxiliary
transformer branch designed to enhance robustness to noise through learned
latent contrasts. These complementary representations are fused and decoded
using deep Residual-in-Residual Dense Blocks for enhanced reconstruction
quality. Extensive experiments on benchmark datasets demonstrate that CATformer
outperforms recent transformer-based and diffusion-inspired methods both in
efficiency and visual image quality. This work bridges the performance gap
among transformer-, diffusion-, and GAN-based methods, laying a foundation for
practical applications of diffusion-inspired transformers in super-resolution.

</details>


### [136] [NGD: Neural Gradient Based Deformation for Monocular Garment Reconstruction](https://arxiv.org/abs/2508.17712)
*Soham Dasgupta,Shanthika Naik,Preet Savalia,Sujay Kumar Ingle,Avinash Sharma*

Main category: cs.CV

TL;DR: NGD方法通过神经梯度变形和自适应网格重构，从单目视频中高质量重建动态服装，解决了现有方法在细节和变形上的不足。


<details>
  <summary>Details</summary>
Motivation: 动态服装重建因复杂动态和无约束特性而具挑战性，现有方法在细节建模和变形处理上存在局限。

Method: 提出NGD（神经梯度变形）方法，结合自适应网格重构策略和动态纹理映射，以捕捉高频细节和光影效果。

Result: 实验表明，NGD在质量和定量评估上显著优于现有方法，实现了高质量的服装重建。

Conclusion: NGD为动态服装重建提供了高效解决方案，尤其在细节和变形处理上表现优异。

Abstract: Dynamic garment reconstruction from monocular video is an important yet
challenging task due to the complex dynamics and unconstrained nature of the
garments. Recent advancements in neural rendering have enabled high-quality
geometric reconstruction with image/video supervision. However, implicit
representation methods that use volume rendering often provide smooth geometry
and fail to model high-frequency details. While template reconstruction methods
model explicit geometry, they use vertex displacement for deformation, which
results in artifacts. Addressing these limitations, we propose NGD, a Neural
Gradient-based Deformation method to reconstruct dynamically evolving textured
garments from monocular videos. Additionally, we propose a novel adaptive
remeshing strategy for modelling dynamically evolving surfaces like wrinkles
and pleats of the skirt, leading to high-quality reconstruction. Finally, we
learn dynamic texture maps to capture per-frame lighting and shadow effects. We
provide extensive qualitative and quantitative evaluations to demonstrate
significant improvements over existing SOTA methods and provide high-quality
garment reconstructions.

</details>


### [137] [F2RVLM: Boosting Fine-grained Fragment Retrieval for Multi-Modal Long-form Dialogue with Vision Language Model](https://arxiv.org/abs/2508.17714)
*Hanbo Bi,Zhiqiang Yuan,Zexi Jia,Jiapei Zhang,Chongyang Li,Peixiang Luo,Ying Deng,Xiaoyue Duan,Jinchao Zhang*

Main category: cs.CV

TL;DR: 论文提出了细粒度片段检索（FFR）任务，并构建了MLDR数据集和WeChat测试集。通过F2RVLM模型，结合两阶段训练和难度感知课程采样，显著提升了多模态长对话中的检索性能。


<details>
  <summary>Details</summary>
Motivation: 传统对话检索方法难以满足用户对长对话中语义连贯内容的检索需求，因此需要一种新的方法来定位多模态对话中的相关片段。

Method: 提出F2RVLM模型，采用两阶段训练（监督微调和GRPO强化学习）和难度感知课程采样，以提升检索性能。

Result: F2RVLM在领域内和真实场景中均优于现有视觉语言模型，展示了卓越的检索能力。

Conclusion: F2RVLM通过创新的训练方法和难度感知策略，有效解决了多模态长对话中的细粒度检索问题。

Abstract: Traditional dialogue retrieval aims to select the most appropriate utterance
or image from recent dialogue history. However, they often fail to meet users'
actual needs for revisiting semantically coherent content scattered across
long-form conversations. To fill this gap, we define the Fine-grained Fragment
Retrieval (FFR) task, requiring models to locate query-relevant fragments,
comprising both utterances and images, from multimodal long-form dialogues. As
a foundation for FFR, we construct MLDR, the longest-turn multimodal dialogue
retrieval dataset to date, averaging 25.45 turns per dialogue, with each
naturally spanning three distinct topics. To evaluate generalization in
real-world scenarios, we curate and annotate a WeChat-based test set comprising
real-world multimodal dialogues with an average of 75.38 turns. Building on
these resources, we explore existing generation-based Vision-Language Models
(VLMs) on FFR and observe that they often retrieve incoherent utterance-image
fragments. While optimized for generating responses from visual-textual inputs,
these models lack explicit supervision to ensure semantic coherence within
retrieved fragments. To this end, we propose F2RVLM, a generative retrieval
model trained in a two-stage paradigm: (1) supervised fine-tuning to inject
fragment-level retrieval knowledge, and (2) GRPO-based reinforcement learning
with multi-objective rewards promoting semantic precision, relevance, and
contextual coherence. To handle varying intra-fragment complexity, from locally
dense to sparsely distributed, we introduce difficulty-aware curriculum
sampling that ranks training instances by model-predicted difficulty and
gradually exposes the model to harder samples. This boosts reasoning ability in
long, multi-turn contexts. F2RVLM outperforms popular VLMs in both in-domain
and real-domain settings, demonstrating superior retrieval performance.

</details>


### [138] [Instant Preference Alignment for Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.17718)
*Yang Li,Songlin Yang,Xiaoxuan Han,Wei Wang,Jing Dong,Yueming Lyu,Ziyu Xue*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLM）的无训练框架，用于实时偏好对齐的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态偏好或微调，难以适应动态和细粒度的用户意图，需要一种实时且无需训练的方法。

Method: 框架分为偏好理解和偏好引导生成两部分：利用MLLM从参考图像提取偏好信号并丰富提示；通过全局关键词控制和局部区域感知调制引导扩散模型。

Result: 在Viper数据集和自建基准上，方法在定量指标和人工评估中均优于现有方法。

Conclusion: 该方法为基于对话的生成和MLLM-扩散模型整合开辟了新可能性。

Abstract: Text-to-image (T2I) generation has greatly enhanced creative expression, yet
achieving preference-aligned generation in a real-time and training-free manner
remains challenging. Previous methods often rely on static, pre-collected
preferences or fine-tuning, limiting adaptability to evolving and nuanced user
intents. In this paper, we highlight the need for instant preference-aligned
T2I generation and propose a training-free framework grounded in multimodal
large language model (MLLM) priors. Our framework decouples the task into two
components: preference understanding and preference-guided generation. For
preference understanding, we leverage MLLMs to automatically extract global
preference signals from a reference image and enrich a given prompt using
structured instruction design. Our approach supports broader and more
fine-grained coverage of user preferences than existing methods. For
preference-guided generation, we integrate global keyword-based control and
local region-aware cross-attention modulation to steer the diffusion model
without additional training, enabling precise alignment across both global
attributes and local elements. The entire framework supports multi-round
interactive refinement, facilitating real-time and context-aware image
generation. Extensive experiments on the Viper dataset and our collected
benchmark demonstrate that our method outperforms prior approaches in both
quantitative metrics and human evaluations, and opens up new possibilities for
dialog-based generation and MLLM-diffusion integration.

</details>


### [139] [Few-shot Human Action Anomaly Detection via a Unified Contrastive Learning Framework](https://arxiv.org/abs/2508.17726)
*Koichiro Kamide,Shunsuke Sakai,Shun Maeda,Chunzhi Gu,Chao Zhang*

Main category: cs.CV

TL;DR: 提出了一种统一框架用于人类动作异常检测（HAAD），通过对比学习和生成运动增强策略，解决了现有方法在少样本场景下的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要为每个动作类别单独训练模型，且需要大量正常样本，限制了其在数据稀缺或新类别频繁出现的实际场景中的适用性。

Method: 构建了一个类别无关的表示空间，通过对比学习进行异常检测，并引入基于扩散模型的生成运动增强策略以提高泛化能力和鲁棒性。

Result: 在HumanAct12数据集上的实验表明，该方法在少样本HAAD任务中，无论是已知还是未知类别，均表现出卓越的效果。

Conclusion: 该框架首次将生成运动增强策略与对比学习结合，显著提升了HAAD的效率和可扩展性。

Abstract: Human Action Anomaly Detection (HAAD) aims to identify anomalous actions
given only normal action data during training. Existing methods typically
follow a one-model-per-category paradigm, requiring separate training for each
action category and a large number of normal samples. These constraints hinder
scalability and limit applicability in real-world scenarios, where data is
often scarce or novel categories frequently appear. To address these
limitations, we propose a unified framework for HAAD that is compatible with
few-shot scenarios. Our method constructs a category-agnostic representation
space via contrastive learning, enabling AD by comparing test samples with a
given small set of normal examples (referred to as the support set). To improve
inter-category generalization and intra-category robustness, we introduce a
generative motion augmentation strategy harnessing a diffusion-based foundation
model for creating diverse and realistic training samples. Notably, to the best
of our knowledge, our work is the first to introduce such a strategy
specifically tailored to enhance contrastive learning for action AD. Extensive
experiments on the HumanAct12 dataset demonstrate the state-of-the-art
effectiveness of our approach under both seen and unseen category settings,
regarding training efficiency and model scalability for few-shot HAAD.

</details>


### [140] [Segmentation and Classification of Pap Smear Images for Cervical Cancer Detection Using Deep Learning](https://arxiv.org/abs/2508.17728)
*Nisreen Albzour,Sarah S. Lam*

Main category: cs.CV

TL;DR: 该研究提出了一种结合U-Net分割和分类模型的深度学习框架，用于提高宫颈癌早期诊断的性能。实验结果表明，分割对分类性能的提升有限。


<details>
  <summary>Details</summary>
Motivation: 宫颈癌是全球女性健康的重要威胁，早期检测至关重要，但传统方法耗时且易出错。

Method: 使用U-Net进行图像分割，并结合分类模型，利用Herlev Pap Smear数据集进行训练和评估。

Result: 分割图像对分类性能的提升有限（精度提高约0.41%，F1分数提高约1.30%）。

Conclusion: 该框架可作为临床辅助工具，帮助病理学家进行早期诊断，但分割对分类的贡献有限。

Abstract: Cervical cancer remains a significant global health concern and a leading
cause of cancer-related deaths among women. Early detection through Pap smear
tests is essential to reduce mortality rates; however, the manual examination
is time consuming and prone to human error. This study proposes a deep learning
framework that integrates U-Net for segmentation and a classification model to
enhance diagnostic performance. The Herlev Pap Smear Dataset, a publicly
available cervical cell dataset, was utilized for training and evaluation. The
impact of segmentation on classification performance was evaluated by comparing
the model trained on segmented images and another trained on non-segmented
images. Experimental results showed that the use of segmented images marginally
improved the model performance on precision (about 0.41 percent higher) and
F1-score (about 1.30 percent higher), which suggests a slightly more balanced
classification performance. While segmentation helps in feature extraction, the
results showed that its impact on classification performance appears to be
limited. The proposed framework offers a supplemental tool for clinical
applications, which may aid pathologists in early diagnosis.

</details>


### [141] [CMFDNet: Cross-Mamba and Feature Discovery Network for Polyp Segmentation](https://arxiv.org/abs/2508.17729)
*Feng Jiang,Zongfei Zhang,Xin Xu*

Main category: cs.CV

TL;DR: CMFDNet是一种创新的结肠息肉分割架构，通过CMD、MSA和FD模块解决了息肉形状多样、边界模糊和小尺寸息肉易忽略的问题，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在息肉分割中面临形状和大小变化大、边界模糊以及小尺寸息肉易忽略的问题，需要更有效的解决方案。

Method: 提出CMFDNet架构，包含CMD模块（减少模糊边界）、MSA模块（增强多几何形状识别）和FD模块（解决小尺寸息肉检测问题）。

Result: 在ETIS和ColonDB数据集上，CMFDNet的mDice分数分别比最佳现有方法高1.83%和1.55%。

Conclusion: CMFDNet通过创新模块设计显著提升了息肉分割性能，尤其在处理复杂和小尺寸息肉时表现突出。

Abstract: Automated colonic polyp segmentation is crucial for assisting doctors in
screening of precancerous polyps and diagnosis of colorectal neoplasms.
Although existing methods have achieved promising results, polyp segmentation
remains hindered by the following limitations,including: (1) significant
variation in polyp shapes and sizes, (2) indistinct boundaries between polyps
and adjacent tissues, and (3) small-sized polyps are easily overlooked during
the segmentation process. Driven by these practical difficulties, an innovative
architecture, CMFDNet, is proposed with the CMD module, MSA module, and FD
module. The CMD module, serving as an innovative decoder, introduces a
cross-scanning method to reduce blurry boundaries. The MSA module adopts a
multi-branch parallel structure to enhance the recognition ability for polyps
with diverse geometries and scale distributions. The FD module establishes
dependencies among all decoder features to alleviate the under-detection of
polyps with small-scale features. Experimental results show that CMFDNet
outperforms six SOTA methods used for comparison, especially on ETIS and
ColonDB datasets, where mDice scores exceed the best SOTA method by 1.83% and
1.55%, respectively.

</details>


### [142] [DroneKey: Drone 3D Pose Estimation in Image Sequences using Gated Key-representation and Pose-adaptive Learning](https://arxiv.org/abs/2508.17746)
*Seo-Bin Hwang,Yeong-Jun Cho*

Main category: cs.CV

TL;DR: DroneKey框架结合2D关键点检测和3D姿态估计，针对无人机螺旋桨的独特挑战，通过门控和和姿态自适应损失函数，实现了高精度和实时处理。


<details>
  <summary>Details</summary>
Motivation: 无人机螺旋桨作为关键点检测困难，现有方法难以应对其视觉相似性和姿态多样性。

Method: DroneKey结合2D关键点检测器和3D姿态估计器，使用门控和结合中间和紧凑表示，并引入姿态自适应Mahalanobis距离损失函数。

Result: 关键点检测AP达99.68%，3D姿态估计MAE-angle为10.62°，RMSE为0.221m，MAE-absolute为0.076m，实时处理44 FPS。

Conclusion: DroneKey在无人机关键点检测和3D姿态估计中表现出高精度和可靠性，代码和数据集已公开。

Abstract: Estimating the 3D pose of a drone is important for anti-drone systems, but
existing methods struggle with the unique challenges of drone keypoint
detection. Drone propellers serve as keypoints but are difficult to detect due
to their high visual similarity and diversity of poses. To address these
challenges, we propose DroneKey, a framework that combines a 2D keypoint
detector and a 3D pose estimator specifically designed for drones. In the
keypoint detection stage, we extract two key-representations (intermediate and
compact) from each transformer encoder layer and optimally combine them using a
gated sum. We also introduce a pose-adaptive Mahalanobis distance in the loss
function to ensure stable keypoint predictions across extreme poses. We built
new datasets of drone 2D keypoints and 3D pose to train and evaluate our
method, which have been publicly released. Experiments show that our method
achieves an AP of 99.68% (OKS) in keypoint detection, outperforming existing
methods. Ablation studies confirm that the pose-adaptive Mahalanobis loss
function improves keypoint prediction stability and accuracy. Additionally,
improvements in the encoder design enable real-time processing at 44 FPS. For
3D pose estimation, our method achieved an MAE-angle of 10.62{\deg}, an RMSE of
0.221m, and an MAE-absolute of 0.076m, demonstrating high accuracy and
reliability. The code and dataset are available at
https://github.com/kkanuseobin/DroneKey.

</details>


### [143] [From Global to Local: Social Bias Transfer in CLIP](https://arxiv.org/abs/2508.17750)
*Ryan Ramos,Yusuke Hirota,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 分析CLIP模型预训练中的社会偏见如何传播到下游任务，发现偏见测量和转移的不一致性。


<details>
  <summary>Details</summary>
Motivation: 研究CLIP模型预训练中的社会偏见如何影响下游任务，以促进更好的偏见缓解实践。

Method: 通过实证分析，比较全局和局部数据视图的偏见差异，并探索预训练模型与下游任务偏见的关联。

Result: 偏见测量依赖数据子集，偏见转移趋势不一致，预训练模型在下游任务中表示空间趋同。

Conclusion: 研究揭示了偏见行为的复杂性，为未来偏见缓解研究提供了参考。

Abstract: The recycling of contrastive language-image pre-trained (CLIP) models as
backbones for a large number of downstream tasks calls for a thorough analysis
of their transferability implications, especially their well-documented
reproduction of social biases and human stereotypes. How do such biases,
learned during pre-training, propagate to downstream applications like visual
question answering or image captioning? Do they transfer at all?
  We investigate this phenomenon, referred to as bias transfer in prior
literature, through a comprehensive empirical analysis. Firstly, we examine how
pre-training bias varies between global and local views of data, finding that
bias measurement is highly dependent on the subset of data on which it is
computed. Secondly, we analyze correlations between biases in the pre-trained
models and the downstream tasks across varying levels of pre-training bias,
finding difficulty in discovering consistent trends in bias transfer. Finally,
we explore why this inconsistency occurs, showing that under the current
paradigm, representation spaces of different pre-trained CLIPs tend to converge
when adapted for downstream tasks. We hope this work offers valuable insights
into bias behavior and informs future research to promote better bias
mitigation practices.

</details>


### [144] [CEIDM: A Controlled Entity and Interaction Diffusion Model for Enhanced Text-to-Image Generation](https://arxiv.org/abs/2508.17760)
*Mingyue Yang,Dianxi Shi,Jialu Zhou,Xinyu Wei,Leqian Li,Shaowu Yang,Chunping Qiu*

Main category: cs.CV

TL;DR: CEIDM是一种基于扩散模型的文本到图像生成方法，通过双重控制实体和交互关系，结合LLM挖掘隐式交互关系，并通过聚类和偏移方法优化交互动作，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成中实体及其复杂交互关系的控制问题，以生成更符合现实逻辑和交互合理的高质量图像。

Method: 1. 基于LLM的实体交互关系挖掘；2. 交互动作聚类与偏移方法；3. 实体控制网络设计，包括多尺度卷积和动态网络。

Result: 实验表明，CEIDM在实体控制和交互控制方面优于现有代表性方法。

Conclusion: CEIDM通过双重控制和优化方法，显著提升了文本到图像生成的质量和交互合理性。

Abstract: In Text-to-Image (T2I) generation, the complexity of entities and their
intricate interactions pose a significant challenge for T2I method based on
diffusion model: how to effectively control entity and their interactions to
produce high-quality images. To address this, we propose CEIDM, a image
generation method based on diffusion model with dual controls for entity and
interaction. First, we propose an entity interactive relationships mining
approach based on Large Language Models (LLMs), extracting reasonable and rich
implicit interactive relationships through chain of thought to guide diffusion
models to generate high-quality images that are closer to realistic logic and
have more reasonable interactive relationships. Furthermore, We propose an
interactive action clustering and offset method to cluster and offset the
interactive action features contained in each text prompts. By constructing
global and local bidirectional offsets, we enhance semantic understanding and
detail supplementation of original actions, making the model's understanding of
the concept of interactive "actions" more accurate and generating images with
more accurate interactive actions. Finally, we design an entity control network
which generates masks with entity semantic guidance, then leveraging
multi-scale convolutional network to enhance entity feature and dynamic network
to fuse feature. It effectively controls entities and significantly improves
image quality. Experiments show that the proposed CEIDM method is better than
the most representative existing methods in both entity control and their
interaction control.

</details>


### [145] [Robust Anomaly Detection in Industrial Environments via Meta-Learning](https://arxiv.org/abs/2508.17789)
*Muhammad Aqeel,Shakiba Sharifi,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: RAD框架结合归一化流和模型无关元学习，有效解决工业异常检测中的标签噪声问题，表现优异。


<details>
  <summary>Details</summary>
Motivation: 工业环境中训练数据常含错误标签，传统方法难以应对，需开发鲁棒异常检测方法。

Method: 采用双层优化策略，结合元学习和自适应L2正则化，利用归一化流进行异常评分。

Result: 在MVTec-AD和KSDD2数据集上，I-AUROC分别达95.4%和94.6%，噪声条件下仍保持高检测能力。

Conclusion: RAD对噪声数据具有强鲁棒性，适用于实际工业场景，是实用的异常检测解决方案。

Abstract: Anomaly detection is fundamental for ensuring quality control and operational
efficiency in industrial environments, yet conventional approaches face
significant challenges when training data contains mislabeled samples-a common
occurrence in real-world scenarios. This paper presents RAD, a robust anomaly
detection framework that integrates Normalizing Flows with Model-Agnostic
Meta-Learning to address the critical challenge of label noise in industrial
settings. Our approach employs a bi-level optimization strategy where
meta-learning enables rapid adaptation to varying noise conditions, while
uncertainty quantification guides adaptive L2 regularization to maintain model
stability. The framework incorporates multiscale feature processing through
pretrained feature extractors and leverages the precise likelihood estimation
capabilities of Normalizing Flows for robust anomaly scoring. Comprehensive
evaluation on MVTec-AD and KSDD2 datasets demonstrates superior performance,
achieving I-AUROC scores of 95.4% and 94.6% respectively under clean
conditions, while maintaining robust detection capabilities above 86.8% and
92.1% even when 50% of training samples are mislabeled. The results highlight
RAD's exceptional resilience to noisy training conditions and its ability to
detect subtle anomalies across diverse industrial scenarios, making it a
practical solution for real-world anomaly detection applications where perfect
data curation is challenging.

</details>


### [146] [Sketchpose: Learning to Segment Cells with Partial Annotations](https://arxiv.org/abs/2508.17798)
*Clément Cazorla,Nathanaël Munier,Renaud Morin,Pierre Weiss*

Main category: cs.CV

TL;DR: 提出了一种基于距离图的方法，支持部分标注对象，显著节省时间和资源，同时保持分割质量。


<details>
  <summary>Details</summary>
Motivation: 现有细胞分割网络依赖完全标注数据集，限制了训练集生成和迁移学习。

Method: 仍基于距离图，但支持部分标注对象。

Result: 在节俭学习、迁移学习和常规学习中表现良好，节省资源且不牺牲质量。

Conclusion: 方法有效且实用，已集成到用户友好的Napari插件中。

Abstract: The most popular networks used for cell segmentation (e.g. Cellpose,
Stardist, HoverNet,...) rely on a prediction of a distance map. It yields
unprecedented accuracy but hinges on fully annotated datasets. This is a
serious limitation to generate training sets and perform transfer learning. In
this paper, we propose a method that still relies on the distance map and
handles partially annotated objects. We evaluate the performance of the
proposed approach in the contexts of frugal learning, transfer learning and
regular learning on regular databases. Our experiments show that it can lead to
substantial savings in time and resources without sacrificing segmentation
quality. The proposed algorithm is embedded in a user-friendly Napari plugin.

</details>


### [147] [PoRe: Position-Reweighted Visual Token Pruning for Vision Language Models](https://arxiv.org/abs/2508.17807)
*Kai Zhao,Wubang Yuan,Alex Lingyu Hung,Dan Zeng*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解视觉语言模型中视觉令牌修剪的近期偏差，通过空间位置重新加权注意力分数。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中视觉令牌数量远多于文本令牌，且存在冗余。近期偏差导致修剪时过度保留图像底部令牌，影响修剪效果。

Method: 提出位置重新加权机制，根据视觉令牌的空间位置调整注意力分数，无需改变模型架构或额外训练。

Result: 实验表明，该方法显著提升了视觉令牌修剪的性能，且计算开销极小。

Conclusion: 该方法是一种即插即用的解决方案，可无缝集成到现有视觉令牌修剪框架中，有效缓解近期偏差。

Abstract: Vision-Language Models (VLMs) typically process a significantly larger number
of visual tokens compared to text tokens due to the inherent redundancy in
visual signals. Visual token pruning is a promising direction to reduce the
computational cost of VLMs by eliminating redundant visual tokens. The
text-visual attention score is a widely adopted criterion for visual token
pruning as it reflects the relevance of visual tokens to the text input.
However, many sequence models exhibit a recency bias, where tokens appearing
later in the sequence exert a disproportionately large influence on the model's
output. In VLMs, this bias manifests as inflated attention scores for tokens
corresponding to the lower regions of the image, leading to suboptimal pruning
that disproportionately retains tokens from the image bottom. In this paper, we
present an extremely simple yet effective approach to alleviate the recency
bias in visual token pruning. We propose a straightforward reweighting
mechanism that adjusts the attention scores of visual tokens according to their
spatial positions in the image. Our method, termed Position-reweighted Visual
Token Pruning, is a plug-and-play solution that can be seamlessly incorporated
into existing visual token pruning frameworks without any changes to the model
architecture or extra training. Extensive experiments on LVLMs demonstrate that
our method improves the performance of visual token pruning with minimal
computational overhead.

</details>


### [148] [UniSino: Physics-Driven Foundational Model for Universal CT Sinogram Standardization](https://arxiv.org/abs/2508.17816)
*Xingyu Ai,Shaoyu Wang,Zhiyuan Jia,Ao Xu,Hongming Shan,Jianhua Ma,Qiegen Liu*

Main category: cs.CV

TL;DR: UniSino是一种用于CT投影数据标准化的基础模型，通过直接在投影域处理数据，提高了对多种欠采样场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: CT成像中，欠采样和噪声会导致重建图像中的伪影和噪声，影响诊断准确性。传统方法缺乏对不同伪影类型的泛化能力。

Method: UniSino在投影域直接标准化数据，结合投影数据的物理特性进行训练，增强泛化能力。

Result: 实验表明，UniSino在单欠采样和混合欠采样情况下均表现出优异的重建质量和鲁棒性。

Conclusion: UniSino为CT投影数据标准化提供了通用且高效的解决方案，显著提升了重建图像的质量和诊断准确性。

Abstract: During raw-data acquisition in CT imaging, diverse factors can degrade the
collected sinograms, with undersampling and noise leading to severe artifacts
and noise in reconstructed images and compromising diagnostic accuracy.
Conventional correction methods rely on manually designed algorithms or fixed
empirical parameters, but these approaches often lack generalizability across
heterogeneous artifact types. To address these limitations, we propose UniSino,
a foundation model for universal CT sinogram standardization. Unlike existing
foundational models that operate in image domain, UniSino directly standardizes
data in the projection domain, which enables stronger generalization across
diverse undersampling scenarios. Its training framework incorporates the
physical characteristics of sinograms, enhancing generalization and enabling
robust performance across multiple subtasks spanning four benchmark datasets.
Experimental results demonstrate thatUniSino achieves superior reconstruction
quality both single and mixed undersampling case, demonstrating exceptional
robustness and generalization in sinogram enhancement for CT imaging. The code
is available at: https://github.com/yqx7150/UniSino.

</details>


### [149] [TemCoCo: Temporally Consistent Multi-modal Video Fusion with Visual-Semantic Collaboration](https://arxiv.org/abs/2508.17817)
*Meiqi Gong,Hao Zhang,Xunpeng Yi,Linfeng Tang,Jiayi Ma*

Main category: cs.CV

TL;DR: 提出首个显式结合时间建模与视觉-语义协作的视频融合框架，确保视觉保真、语义准确和时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法直接将静态帧融合技术用于视频任务，忽略时间依赖性，导致帧间不一致。

Method: 引入视觉-语义交互模块、时间协作模块和时间增强机制，结合新评估指标。

Result: 在公开数据集上表现优越。

Conclusion: 框架有效提升视频融合的视觉、语义和时间一致性。

Abstract: Existing multi-modal fusion methods typically apply static frame-based image
fusion techniques directly to video fusion tasks, neglecting inherent temporal
dependencies and leading to inconsistent results across frames. To address this
limitation, we propose the first video fusion framework that explicitly
incorporates temporal modeling with visual-semantic collaboration to
simultaneously ensure visual fidelity, semantic accuracy, and temporal
consistency. First, we introduce a visual-semantic interaction module
consisting of a semantic branch and a visual branch, with Dinov2 and VGG19
employed for targeted distillation, allowing simultaneous enhancement of both
the visual and semantic representations. Second, we pioneer integrate the video
degradation enhancement task into the video fusion pipeline by constructing a
temporal cooperative module, which leverages temporal dependencies to
facilitate weak information recovery. Third, to ensure temporal consistency, we
embed a temporal-enhanced mechanism into the network and devise a temporal loss
to guide the optimization process. Finally, we introduce two innovative
evaluation metrics tailored for video fusion, aimed at assessing the temporal
consistency of the generated fused videos. Extensive experimental results on
public video datasets demonstrate the superiority of our method. Our code is
released at https://github.com/Meiqi-Gong/TemCoCo.

</details>


### [150] [A Contrastive Learning-Guided Confident Meta-learning for Zero Shot Anomaly Detection](https://arxiv.org/abs/2508.17827)
*Muhammad Aqeel,Danijel Skocaj,Marco Cristani,Francesco Setti*

Main category: cs.CV

TL;DR: CoZAD是一种新型零样本异常检测框架，结合软置信学习、元学习和对比特征表示，解决了工业与医疗领域数据稀缺和标注成本高的问题。


<details>
  <summary>Details</summary>
Motivation: 工业与医疗领域的异常检测面临数据稀缺和标注成本高的挑战，尤其是在快速变化的环境中。

Method: CoZAD通过软置信学习为所有训练数据分配置信权重，保留边界信息并突出典型正常模式，结合IQR阈值量化数据不确定性和基于协方差的正则化量化模型不确定性，同时利用对比学习创建判别性特征空间。

Result: 在10个工业和医疗数据集的评估中，CoZAD在6/7的工业基准上表现最优，尤其在纹理丰富数据集（如DTD-Synthetic和BTAD）和像素级定位（如MVTec-AD）上显著优于现有方法。

Conclusion: CoZAD无需依赖视觉-语言对齐或模型集成，适用于资源受限环境，具有快速部署的优势。

Abstract: Industrial and medical anomaly detection faces critical challenges from data
scarcity and prohibitive annotation costs, particularly in evolving
manufacturing and healthcare settings. To address this, we propose CoZAD, a
novel zero-shot anomaly detection framework that integrates soft confident
learning with meta-learning and contrastive feature representation. Unlike
traditional confident learning that discards uncertain samples, our method
assigns confidence-based weights to all training data, preserving boundary
information while emphasizing prototypical normal patterns. The framework
quantifies data uncertainty through IQR-based thresholding and model
uncertainty via covariance based regularization within a Model-Agnostic
Meta-Learning. Contrastive learning creates discriminative feature spaces where
normal patterns form compact clusters, enabling rapid domain adaptation.
Comprehensive evaluation across 10 datasets spanning industrial and medical
domains demonstrates state-of-the-art performance, outperforming existing
methods on 6 out of 7 industrial benchmarks with notable improvements on
texture-rich datasets (99.2% I-AUROC on DTD-Synthetic, 97.2% on BTAD) and
pixellevel localization (96.3% P-AUROC on MVTec-AD). The framework eliminates
dependence on vision-language alignments or model ensembles, making it valuable
for resourceconstrained environments requiring rapid deployment.

</details>


### [151] [HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation](https://arxiv.org/abs/2508.17832)
*Xiping Wang,Yuxi Wang,Mengqi Zhou,Junsong Fan,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为HLG的分层布局生成方法，用于细粒度的3D室内场景生成，解决了现有方法在精细对象布局上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在粗粒度家具布局上取得进展，但难以捕捉细粒度对象布局，限制了生成场景的真实性和实用性。

Method: HLG采用从粗到细的分层方法，通过垂直和水平解耦构建分层布局，并结合可训练的布局优化网络解决放置问题。

Result: 实验表明，HLG在生成真实室内场景方面优于现有方法。

Conclusion: HLG推动了场景生成领域的发展，并为需要详细3D环境的应用开辟了新可能性。

Abstract: Realistic 3D indoor scene generation is crucial for virtual reality, interior
design, embodied intelligence, and scene understanding. While existing methods
have made progress in coarse-scale furniture arrangement, they struggle to
capture fine-grained object placements, limiting the realism and utility of
generated environments. This gap hinders immersive virtual experiences and
detailed scene comprehension for embodied AI applications. To address these
issues, we propose Hierarchical Layout Generation (HLG), a novel method for
fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine
hierarchical approach, refining scene layouts from large-scale furniture
placement to intricate object arrangements. Specifically, our fine-grained
layout alignment module constructs a hierarchical layout through vertical and
horizontal decoupling, effectively decomposing complex 3D indoor scenes into
multiple levels of granularity. Additionally, our trainable layout optimization
network addresses placement issues, such as incorrect positioning, orientation
errors, and object intersections, ensuring structurally coherent and physically
plausible scene generation. We demonstrate the effectiveness of our approach
through extensive experiments, showing superior performance in generating
realistic indoor scenes compared to existing methods. This work advances the
field of scene generation and opens new possibilities for applications
requiring detailed 3D environments. We will release our code upon publication
to encourage future research.

</details>


### [152] [SCOUT: Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive Data Selection](https://arxiv.org/abs/2508.17843)
*Weiqi Yan,Lvhai Chen,Shengchuan Zhang,Yan Zhang,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出了一种半监督伪装目标检测方法SCOUT，通过自适应数据增强与选择和文本融合模块，有效利用未标注数据，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 像素级标注成本高，现有半监督方法对未标注数据的利用不足，需改进。

Method: 引入自适应数据增强与选择模块（ADAS）和文本融合模块（TFM），结合对抗性增强和文本视觉交互。

Result: 在新建数据集RefTextCOD上实验，性能优于现有半监督方法，达到SOTA。

Conclusion: SCOUT通过改进未标注数据利用和引入文本信息，显著提升半监督伪装目标检测性能。

Abstract: The difficulty of pixel-level annotation has significantly hindered the
development of the Camouflaged Object Detection (COD) field. To save on
annotation costs, previous works leverage the semi-supervised COD framework
that relies on a small number of labeled data and a large volume of unlabeled
data. We argue that there is still significant room for improvement in the
effective utilization of unlabeled data. To this end, we introduce a
Semi-supervised Camouflaged Object Detection by Utilizing Text and Adaptive
Data Selection (SCOUT). It includes an Adaptive Data Augment and Selection
(ADAS) module and a Text Fusion Module (TFM). The ADSA module selects valuable
data for annotation through an adversarial augment and sampling strategy. The
TFM module further leverages the selected valuable data by combining
camouflage-related knowledge and text-visual interaction. To adapt to this
work, we build a new dataset, namely RefTextCOD. Extensive experiments show
that the proposed method surpasses previous semi-supervised methods in the COD
field and achieves state-of-the-art performance. Our code will be released at
https://github.com/Heartfirey/SCOUT.

</details>


### [153] [Diffusion-Based Data Augmentation for Medical Image Segmentation](https://arxiv.org/abs/2508.17844)
*Maham Nazir,Muhammad Aqeel,Francesco Setti*

Main category: cs.CV

TL;DR: DiffAug结合文本引导的扩散生成与自动分割验证，解决医学图像分割中罕见异常标注数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割模型因罕见异常标注数据稀缺而表现不佳，需一种新方法生成高质量异常样本。

Method: 利用基于医学文本描述和空间掩码的潜在扩散模型，通过修复正常图像生成异常样本，并通过潜在空间分割网络动态验证质量。

Result: 在三个医学影像基准测试中，DiffAug的Dice分数提升8-10%，假阴性率降低28%。

Conclusion: DiffAug通过文本引导生成和动态验证，显著提升罕见异常分割性能，适用于早期筛查应用。

Abstract: Medical image segmentation models struggle with rare abnormalities due to
scarce annotated pathological data. We propose DiffAug a novel framework that
combines textguided diffusion-based generation with automatic segmentation
validation to address this challenge. Our proposed approach uses latent
diffusion models conditioned on medical text descriptions and spatial masks to
synthesize abnormalities via inpainting on normal images. Generated samples
undergo dynamic quality validation through a latentspace segmentation network
that ensures accurate localization while enabling single-step inference. The
text prompts, derived from medical literature, guide the generation of diverse
abnormality types without requiring manual annotation. Our validation mechanism
filters synthetic samples based on spatial accuracy, maintaining quality while
operating efficiently through direct latent estimation. Evaluated on three
medical imaging benchmarks (CVC-ClinicDB, Kvasir-SEG, REFUGE2), our framework
achieves state-of-the-art performance with 8-10% Dice improvements over
baselines and reduces false negative rates by up to 28% for challenging cases
like small polyps and flat lesions critical for early detection in screening
applications.

</details>


### [154] [Alternating Training-based Label Smoothing Enhances Prompt Generalization](https://arxiv.org/abs/2508.17846)
*Yang Chen,Yanbin Wei,Ke Jin,Yi Kong,James Kwok,Yu Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种交替训练标签平滑（ATLaS）方法，结合类间和实例间软标签，提升提示调优的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示调优方法的泛化能力有限，而传统标签平滑（LS）反而会削弱其性能，因此需要一种新的方法结合LS与提示调优。

Method: 提出ATLaS方法，交替使用标准独热标签和LS生成的软标签进行训练，并引入类间软标签（CSL）和实例间软标签（ISL）提供额外信息。

Result: 实验表明，ATLaS结合CSL和ISL能显著提升提示调优的泛化性能，且与现有方法兼容。

Conclusion: ATLaS是一种高效且兼容性强的标签平滑方法，能够有效提升提示调优的泛化能力。

Abstract: Recent advances in pre-trained vision-language models have demonstrated
remarkable zero-shot generalization capabilities. To further enhance these
models' adaptability to various downstream tasks, prompt tuning has emerged as
a parameter-efficient fine-tuning method. However, despite its efficiency, the
generalization ability of prompt remains limited. In contrast, label smoothing
(LS) has been widely recognized as an effective regularization technique that
prevents models from becoming over-confident and improves their generalization.
This inspires us to explore the integration of LS with prompt tuning. However,
we have observed that the vanilla LS even weakens the generalization ability of
prompt tuning. To address this issue, we propose the Alternating Training-based
Label Smoothing (ATLaS) method, which alternately trains with standard one-hot
labels and soft labels generated by LS to supervise the prompt tuning.
Moreover, we introduce two types of efficient offline soft labels, including
Class-wise Soft Labels (CSL) and Instance-wise Soft Labels (ISL), to provide
inter-class or instance-class relationships for prompt tuning. The theoretical
properties of the proposed ATLaS method are analyzed. Extensive experiments
demonstrate that the proposed ATLaS method, combined with CSL and ISL,
consistently enhances the generalization performance of prompt tuning.
Moreover, the proposed ATLaS method exhibits high compatibility with prevalent
prompt tuning methods, enabling seamless integration into existing methods.

</details>


### [155] [Box-Level Class-Balanced Sampling for Active Object Detection](https://arxiv.org/abs/2508.17849)
*Jingyi Liao,Xun Xu,Chuan-Sheng Foo,Lile Cai*

Main category: cs.CV

TL;DR: 提出了一种针对目标检测的主动学习方法，通过类别平衡采样和任务感知软伪标签策略提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 减少目标检测中昂贵边界框标注的需求，解决早期模型在少数类别上表现不佳导致的伪标签类别不平衡问题。

Method: 采用类别平衡采样策略选择更多少数类别对象标注，并提出任务感知软伪标签策略提高伪标签准确性。

Result: 在公开基准数据集上实现了最先进的性能。

Conclusion: 所提方法能有效平衡训练数据并提升模型性能，适用于目标检测任务。

Abstract: Training deep object detectors demands expensive bounding box annotation.
Active learning (AL) is a promising technique to alleviate the annotation
burden. Performing AL at box-level for object detection, i.e., selecting the
most informative boxes to label and supplementing the sparsely-labelled image
with pseudo labels, has been shown to be more cost-effective than selecting and
labelling the entire image. In box-level AL for object detection, we observe
that models at early stage can only perform well on majority classes, making
the pseudo labels severely class-imbalanced. We propose a class-balanced
sampling strategy to select more objects from minority classes for labelling,
so as to make the final training data, \ie, ground truth labels obtained by AL
and pseudo labels, more class-balanced to train a better model. We also propose
a task-aware soft pseudo labelling strategy to increase the accuracy of pseudo
labels. We evaluate our method on public benchmarking datasets and show that
our method achieves state-of-the-art performance.

</details>


### [156] [VISA: Group-wise Visual Token Selection and Aggregation via Graph Summarization for Efficient MLLMs Inference](https://arxiv.org/abs/2508.17857)
*Pengfei Jiang,Hanjun Li,Linglan Zhao,Fei Chao,Ke Yan,Shouhong Ding,Rongrong Ji*

Main category: cs.CV

TL;DR: VISA是一种新颖的视觉令牌选择和聚合方法，用于解决多模态大语言模型中视觉令牌过多导致的推理效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）中过多的视觉令牌会导致推理效率低下，现有令牌修剪方法会丢失大量视觉信息。

Method: VISA包含图基视觉令牌聚合（VTA）模块和分组令牌选择策略（GTS），通过语义相似性聚合令牌信息并逐步提取视觉信息。

Result: 在LLaVA-1.5、LLaVA-NeXT和Video-LLaVA上的实验表明，VISA在性能和推理速度之间取得了更好的平衡。

Conclusion: VISA方法在保留更多视觉信息的同时压缩了视觉令牌，显著提升了推理效率。

Abstract: In this study, we introduce a novel method called group-wise \textbf{VI}sual
token \textbf{S}election and \textbf{A}ggregation (VISA) to address the issue
of inefficient inference stemming from excessive visual tokens in multimoal
large language models (MLLMs). Compared with previous token pruning approaches,
our method can preserve more visual information while compressing visual
tokens. We first propose a graph-based visual token aggregation (VTA) module.
VTA treats each visual token as a node, forming a graph based on semantic
similarity among visual tokens. It then aggregates information from removed
tokens into kept tokens based on this graph, producing a more compact visual
token representation. Additionally, we introduce a group-wise token selection
strategy (GTS) to divide visual tokens into kept and removed ones, guided by
text tokens from the final layers of each group. This strategy progressively
aggregates visual information, enhancing the stability of the visual
information extraction process. We conduct comprehensive experiments on
LLaVA-1.5, LLaVA-NeXT, and Video-LLaVA across various benchmarks to validate
the efficacy of VISA. Our method consistently outperforms previous methods,
achieving a superior trade-off between model performance and inference speed.
The code is available at https://github.com/mobiushy/VISA.

</details>


### [157] [AVAM: Universal Training-free Adaptive Visual Anchoring Embedded into Multimodal Large Language Model for Multi-image Question Answering](https://arxiv.org/abs/2508.17860)
*Kang Zeng,Guojin Zhong,Jintao Cheng,Jin Yuan,Zhiyong Li*

Main category: cs.CV

TL;DR: 提出了一种自适应视觉锚定策略，解决多图像VQA中的视觉冗余问题，并通过协作解码机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 多图像VQA中视觉冗余影响准确性和效率，现有方法缺乏灵活性且产生离散视觉片段。

Method: 自适应视觉锚定策略和协作解码机制。

Result: 实验验证了方法的有效性，显著提升了多种MLLMs的性能。

Conclusion: 方法简单通用，可无缝集成到现有MLLMs中，代码将公开。

Abstract: The advancement of Multimodal Large Language Models (MLLMs) has driven
significant progress in Visual Question Answering (VQA), evolving from Single
to Multi Image VQA (MVQA). However, the increased number of images in MVQA
inevitably introduces substantial visual redundancy that is irrelevant to
question answering, negatively impacting both accuracy and efficiency. To
address this issue, existing methods lack flexibility in controlling the number
of compressed visual tokens and tend to produce discrete visual fragments,
which hinder MLLMs' ability to comprehend images holistically. In this paper,
we propose a straightforward yet universal Adaptive Visual Anchoring strategy,
which can be seamlessly integrated into existing MLLMs, offering significant
accuracy improvements through adaptive compression. Meanwhile, to balance the
results derived from both global and compressed visual input, we further
introduce a novel collaborative decoding mechanism, enabling optimal
performance. Extensive experiments validate the effectiveness of our method,
demonstrating consistent performance improvements across various MLLMs. The
code will be publicly available.

</details>


### [158] [Camera Pose Refinement via 3D Gaussian Splatting](https://arxiv.org/abs/2508.17876)
*Lulu Hao,Lipu Zhou,Zhenzhong Wei,Xu Wang*

Main category: cs.CV

TL;DR: 提出了一种基于3D高斯泼溅（3DGS）的相机姿态优化框架GS-SMC，无需额外训练即可应用于不同场景，显著提升了姿态估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有相机姿态优化方法依赖特定描述符或网络，需重新训练或重建场景，缺乏几何约束导致精度不足。

Method: 利用3DGS渲染新视图，通过迭代优化和极几何约束优化相机姿态，支持灵活选择特征提取器和匹配器。

Result: 在7-Scenes和Cambridge数据集上，中位平移和旋转误差分别降低53.3%/56.9%和40.7%/53.2%。

Conclusion: GS-SMC提供了一种轻量级、无需训练的相机姿态优化方案，显著优于现有方法。

Abstract: Camera pose refinement aims at improving the accuracy of initial pose
estimation for applications in 3D computer vision. Most refinement approaches
rely on 2D-3D correspondences with specific descriptors or dedicated networks,
requiring reconstructing the scene again for a different descriptor or fully
retraining the network for each scene. Some recent methods instead infer pose
from feature similarity, but their lack of geometry constraints results in less
accuracy. To overcome these limitations, we propose a novel camera pose
refinement framework leveraging 3D Gaussian Splatting (3DGS), referred to as
GS-SMC. Given the widespread usage of 3DGS, our method can employ an existing
3DGS model to render novel views, providing a lightweight solution that can be
directly applied to diverse scenes without additional training or fine-tuning.
Specifically, we introduce an iterative optimization approach, which refines
the camera pose using epipolar geometric constraints among the query and
multiple rendered images. Our method allows flexibly choosing feature
extractors and matchers to establish these constraints. Extensive empirical
evaluations on the 7-Scenes and the Cambridge Landmarks datasets demonstrate
that our method outperforms state-of-the-art camera pose refinement approaches,
achieving 53.3% and 56.9% reductions in median translation and rotation errors
on 7-Scenes, and 40.7% and 53.2% on Cambridge.

</details>


### [159] [Edge-Enhanced Vision Transformer Framework for Accurate AI-Generated Image Detection](https://arxiv.org/abs/2508.17877)
*Dabbrata Das,Mahshar Yahan,Md Tareq Zaman,Md Rishadul Bayesh*

Main category: cs.CV

TL;DR: 提出了一种结合ViT和边缘处理模块的混合检测框架，用于高效识别AI生成图像，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: AI生成图像日益逼真，传统检测方法依赖全局特征，忽略细微结构且计算量大。

Method: 结合微调ViT和边缘处理模块，利用边缘差异图捕捉AI图像的平滑纹理和弱边缘特征。

Result: 在多个数据集上表现优异，CIFAKE上准确率达97.75%，F1分数97.77%。

Conclusion: 该方法轻量、可解释且高效，适用于实际内容验证和数字取证。

Abstract: The rapid advancement of generative models has led to a growing prevalence of
highly realistic AI-generated images, posing significant challenges for digital
forensics and content authentication. Conventional detection methods mainly
rely on deep learning models that extract global features, which often overlook
subtle structural inconsistencies and demand substantial computational
resources. To address these limitations, we propose a hybrid detection
framework that combines a fine-tuned Vision Transformer (ViT) with a novel
edge-based image processing module. The edge-based module computes variance
from edge-difference maps generated before and after smoothing, exploiting the
observation that AI-generated images typically exhibit smoother textures,
weaker edges, and reduced noise compared to real images. When applied as a
post-processing step on ViT predictions, this module enhances sensitivity to
fine-grained structural cues while maintaining computational efficiency.
Extensive experiments on the CIFAKE, Artistic, and Custom Curated datasets
demonstrate that the proposed framework achieves superior detection performance
across all benchmarks, attaining 97.75% accuracy and a 97.77% F1-score on
CIFAKE, surpassing widely adopted state-of-the-art models. These results
establish the proposed method as a lightweight, interpretable, and effective
solution for both still images and video frames, making it highly suitable for
real-world applications in automated content verification and digital
forensics.

</details>


### [160] [ISALux: Illumination and Segmentation Aware Transformer Employing Mixture of Experts for Low Light Image Enhancement](https://arxiv.org/abs/2508.17885)
*Raul Balmez,Alexandru Brateanu,Ciprian Orhei,Codruta Ancuti,Cosmin Ancuti*

Main category: cs.CV

TL;DR: ISALux是一种基于Transformer的低光照图像增强方法，通过结合光照和语义先验，提出了一种新的自注意力模块HISA-MSA，并采用MoE-FFN增强上下文学习，最终在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决低光照图像增强中光照和语义信息融合不足的问题，并应对现有方法在基准数据集上因不同光照模式导致的过拟合问题。

Method: 提出HISA-MSA模块，独立处理光照和语义特征，并通过MoE-FFN增强学习；引入LoRA技术减少过拟合。

Result: 在多个数据集上表现优异，与SOTA方法竞争。

Conclusion: ISALux通过结合光照和语义先验，有效提升了低光照图像增强的性能，并通过LoRA技术解决了过拟合问题。

Abstract: We introduce ISALux, a novel transformer-based approach for Low-Light Image
Enhancement (LLIE) that seamlessly integrates illumination and semantic priors.
Our architecture includes an original self-attention block, Hybrid Illumination
and Semantics-Aware Multi-Headed Self- Attention (HISA-MSA), which integrates
illumination and semantic segmentation maps for en- hanced feature extraction.
ISALux employs two self-attention modules to independently process illumination
and semantic features, selectively enriching each other to regulate luminance
and high- light structural variations in real-world scenarios. A Mixture of
Experts (MoE)-based Feed-Forward Network (FFN) enhances contextual learning,
with a gating mechanism conditionally activating the top K experts for
specialized processing. To address overfitting in LLIE methods caused by
distinct light patterns in benchmarking datasets, we enhance the HISA-MSA
module with low-rank matrix adaptations (LoRA). Extensive qualitative and
quantitative evaluations across multiple specialized datasets demonstrate that
ISALux is competitive with state-of-the-art (SOTA) methods. Addition- ally, an
ablation study highlights the contribution of each component in the proposed
model. Code will be released upon publication.

</details>


### [161] [UniAPO: Unified Multimodal Automated Prompt Optimization](https://arxiv.org/abs/2508.17890)
*Qipeng Zhu,Yanzhe Chen,Huasong Zhong,Yan Li,Jie Chen,Zhixin Zhang,Junping Zhang,Zhenheng Yang*

Main category: cs.CV

TL;DR: UniAPO是一种针对多模态自动提示优化的统一框架，解决了视觉标记膨胀和缺乏过程级监督的挑战，通过EM启发优化和短长期记忆机制提升效果。


<details>
  <summary>Details</summary>
Motivation: 现有自动提示优化方法在文本输入场景中有效，但在多模态任务中存在视觉标记膨胀和缺乏过程级监督的问题。

Method: 采用EM启发优化过程，解耦反馈建模和提示细化，并引入短长期记忆机制。

Result: 在文本、图像和视频基准测试中均取得一致性能提升。

Conclusion: UniAPO为高效且可迁移的提示优化建立了统一框架。

Abstract: Prompting is fundamental to unlocking the full potential of large language
models. To automate and enhance this process, automatic prompt optimization
(APO) has been developed, demonstrating effectiveness primarily in text-only
input scenarios. However, extending existing APO methods to multimodal tasks,
such as video-language generation introduces two core challenges: (i) visual
token inflation, where long visual token sequences restrict context capacity
and result in insufficient feedback signals; (ii) a lack of process-level
supervision, as existing methods focus on outcome-level supervision and
overlook intermediate supervision, limiting prompt optimization. We present
UniAPO: Unified Multimodal Automated Prompt Optimization, the first framework
tailored for multimodal APO. UniAPO adopts an EM-inspired optimization process
that decouples feedback modeling and prompt refinement, making the optimization
more stable and goal-driven. To further address the aforementioned challenges,
we introduce a short-long term memory mechanism: historical feedback mitigates
context limitations, while historical prompts provide directional guidance for
effective prompt optimization. UniAPO achieves consistent gains across text,
image, and video benchmarks, establishing a unified framework for efficient and
transferable prompt optimization.

</details>


### [162] [Designing Practical Models for Isolated Word Visual Speech Recognition](https://arxiv.org/abs/2508.17894)
*Iason Ioannis Panagos,Giorgos Sfikas,Christophoros Nikou*

Main category: cs.CV

TL;DR: 该论文提出了一种轻量级的视觉语音识别（VSR）系统，旨在降低硬件成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 当前VSR系统依赖深度神经网络，计算成本高，限制了在资源受限场景中的应用。

Method: 采用标准双网络设计范式，结合高效的图像分类模型和轻量级块设计，构建轻量级端到端架构。

Result: 在大型公开英语单词数据库上的实验表明，所提模型具有高效性和实用性。

Conclusion: 开发的轻量级VSR模型在降低资源需求的同时保持了高性能，适合实际应用。

Abstract: Visual speech recognition (VSR) systems decode spoken words from an input
sequence using only the video data. Practical applications of such systems
include medical assistance as well as human-machine interactions. A VSR system
is typically employed in a complementary role in cases where the audio is
corrupt or not available. In order to accurately predict the spoken words,
these architectures often rely on deep neural networks in order to extract
meaningful representations from the input sequence. While deep architectures
achieve impressive recognition performance, relying on such models incurs
significant computation costs which translates into increased resource demands
in terms of hardware requirements and results in limited applicability in
real-world scenarios where resources might be constrained. This factor prevents
wider adoption and deployment of speech recognition systems in more practical
applications. In this work, we aim to alleviate this issue by developing
architectures for VSR that have low hardware costs. Following the standard
two-network design paradigm, where one network handles visual feature
extraction and another one utilizes the extracted features to classify the
entire sequence, we develop lightweight end-to-end architectures by first
benchmarking efficient models from the image classification literature, and
then adopting lightweight block designs in a temporal convolution network
backbone. We create several unified models with low resource requirements but
strong recognition performance. Experiments on the largest public database for
English words demonstrate the effectiveness and practicality of our developed
models. Code and trained models will be made publicly available.

</details>


### [163] [EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images](https://arxiv.org/abs/2508.17916)
*Xinning Yao,Bo Liu,Bojian Li,Jingjing Wang,Jinghua Yue,Fugen Zhou*

Main category: cs.CV

TL;DR: EndoUFM是一种无监督单目深度估计框架，通过整合双基础模型和自适应微调策略，显著提升了内窥镜手术场景中的深度估计性能。


<details>
  <summary>Details</summary>
Motivation: 现有单目深度估计技术在内窥镜手术环境中因光照变化和复杂纹理表现不佳，而视觉基础模型在自然图像上训练导致领域适应性不足。

Method: EndoUFM结合了随机向量低秩适应（RVLoRA）和基于深度可分离卷积的残差块（Res-DSC），并设计了掩模引导平滑损失。

Result: 在多个数据集上验证，EndoUFM实现了最先进的性能，同时保持高效模型大小。

Conclusion: 该框架提升了手术中的空间感知，对增强现实和导航系统有重要意义。

Abstract: Depth estimation is a foundational component for 3D reconstruction in
minimally invasive endoscopic surgeries. However, existing monocular depth
estimation techniques often exhibit limited performance to the varying
illumination and complex textures of the surgical environment. While powerful
visual foundation models offer a promising solution, their training on natural
images leads to significant domain adaptability limitations and semantic
perception deficiencies when applied to endoscopy. In this study, we introduce
EndoUFM, an unsupervised monocular depth estimation framework that innovatively
integrating dual foundation models for surgical scenes, which enhance the depth
estimation performance by leveraging the powerful pre-learned priors. The
framework features a novel adaptive fine-tuning strategy that incorporates
Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a
Residual block based on Depthwise Separable Convolution (Res-DSC) to improve
the capture of fine-grained local features. Furthermore, we design a
mask-guided smoothness loss to enforce depth consistency within anatomical
tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and
EndoNeRF datasets confirm that our method achieves state-of-the-art performance
while maintaining an efficient model size. This work contributes to augmenting
surgeons' spatial perception during minimally invasive procedures, thereby
enhancing surgical precision and safety, with crucial implications for
augmented reality and navigation systems.

</details>


### [164] [Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation](https://arxiv.org/abs/2508.17924)
*Konstantin Egorov,Stepan Botman,Pavel Blinov,Galina Zubkova,Anton Ivaschenko,Alexander Kolsanov,Andrey Savchenko*

Main category: cs.CV

TL;DR: 论文介绍了一个大规模多视角视频数据集，用于远程光电容积描记术（rPPG）和健康生物标志物估计，解决了现有数据集规模小、隐私问题和多样性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有公开可用的rPPG数据集存在规模小、隐私问题和条件多样性不足的问题，限制了rPPG技术的发展。

Method: 论文提出了一个包含3600个同步视频记录的数据集，来自600名受试者，使用多台消费级相机在不同角度下捕捉，并配以100Hz PPG信号和多种健康指标。

Result: 基于该数据集训练的高效rPPG模型在跨数据集场景中表现优于现有方法。

Conclusion: 公开数据集和模型将显著加速AI医疗助手的发展。

Abstract: Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical
issues of existing publicly available datasets: small size, privacy concerns
with facial videos, and lack of diversity in conditions. The paper introduces a
novel comprehensive large-scale multi-view video dataset for rPPG and health
biomarkers estimation. Our dataset comprises 3600 synchronized video recordings
from 600 subjects, captured under varied conditions (resting and post-exercise)
using multiple consumer-grade cameras at different angles. To enable multimodal
analysis of physiological states, each recording is paired with a 100 Hz PPG
signal and extended health metrics, such as electrocardiogram, arterial blood
pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and
stress level. Using this data, we train an efficient rPPG model and compare its
quality with existing approaches in cross-dataset scenarios. The public release
of our dataset and model should significantly speed up the progress in the
development of AI medical assistants.

</details>


### [165] [See What You Need: Query-Aware Visual Intelligence through Reasoning-Perception Loops](https://arxiv.org/abs/2508.17932)
*Zixuan Dong,Baoyun Peng,Yufei Wang,Lin Liu,Xinxin Dong,Yunlong Cao,Xiaodong Wang*

Main category: cs.CV

TL;DR: CAVIA是一个无需训练的视频理解框架，通过动态协调推理和视觉注意力，显著提升了长视频问答系统的性能。


<details>
  <summary>Details</summary>
Motivation: 当前长视频问答系统采用刚性流程，将推理与感知解耦，导致信息丢失或计算效率低下。CAVIA旨在解决这一问题。

Method: CAVIA通过分层推理、跨模态语义桥接和置信度驱动的迭代合成，实现推理与感知的动态协调。

Result: 在EgoSchema、NExT-QA和IntentQA基准测试中，CAVIA分别取得了65.7%、76.1%和73.8%的准确率，显著优于现有方法。

Conclusion: 动态推理-感知协调为视频理解提供了可扩展的范式，CAVIA展示了其有效性。

Abstract: Human video comprehension demonstrates dynamic coordination between reasoning
and visual attention, adaptively focusing on query-relevant details. However,
current long-form video question answering systems employ rigid pipelines that
decouple reasoning from perception, leading to either information loss through
premature visual abstraction or computational inefficiency through exhaustive
processing. The core limitation lies in the inability to adapt visual
extraction to specific reasoning requirements, different queries demand
fundamentally different visual evidence from the same video content. In this
work, we present CAVIA, a training-free framework that revolutionizes video
understanding through reasoning, perception coordination. Unlike conventional
approaches where visual processing operates independently of reasoning, CAVIA
creates a closed-loop system where reasoning continuously guides visual
extraction based on identified information gaps. CAVIA introduces three
innovations: (1) hierarchical reasoning, guided localization to precise frames;
(2) cross-modal semantic bridging for targeted extraction; (3)
confidence-driven iterative synthesis. CAVIA achieves state-of-the-art
performance on challenging benchmarks: EgoSchema (65.7%, +5.3%), NExT-QA
(76.1%, +2.6%), and IntentQA (73.8%, +6.9%), demonstrating that dynamic
reasoning-perception coordination provides a scalable paradigm for video
understanding.

</details>


### [166] [Beam Geometry and Input Dimensionality: Impact on Sparse-Sampling Artifact Correction for Clinical CT with U-Nets](https://arxiv.org/abs/2508.17961)
*Tina Dorosti,Johannes Thalhammer,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 研究探讨了不同光束几何形状和数据维度对稀疏采样条纹伪影校正任务的影响，发现2D U-Net在轴向2D切片上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 旨在通过结合体积上下文信息改进伪影减少任务的模型性能。

Method: 使用22名受试者的CT扫描数据，模拟稀疏采样CT体积，训练和验证2D和3D U-Net模型，并比较不同数据维度的性能。

Result: 2D U-Net在轴向2D切片上的MSE和SSIM值最佳，优于2.5D和3D输入数据。

Conclusion: 2D U-Net在稀疏采样伪影校正任务中表现最优，尤其是在轴向2D切片上。

Abstract: This study aims to investigate the effect of various beam geometries and
dimensions of input data on the sparse-sampling streak artifact correction task
with U-Nets for clinical CT scans as a means of incorporating the volumetric
context into artifact reduction tasks to improve model performance. A total of
22 subjects were retrospectively selected (01.2016-12.2018) from the Technical
University of Munich's research hospital, TUM Klinikum rechts der Isar.
Sparsely-sampled CT volumes were simulated with the Astra toolbox for parallel,
fan, and cone beam geometries. 2048 views were taken as full-view scans. 2D and
3D U-Nets were trained and validated on 14, and tested on 8 subjects,
respectively. For the dimensionality study, in addition to the 512x512 2D CT
images, the CT scans were further pre-processed to generate a so-called '2.5D',
and 3D data: Each CT volume was divided into 64x64x64 voxel blocks. The 3D data
refers to individual 64-voxel blocks. An axial, coronal, and sagittal cut
through the center of each block resulted in three 64x64 2D patches that were
rearranged as a single 64x64x3 image, proposed as 2.5D data. Model performance
was assessed with the mean squared error (MSE) and structural similarity index
measure (SSIM). For all geometries, the 2D U-Net trained on axial 2D slices
results in the best MSE and SSIM values, outperforming the 2.5D and 3D input
data dimensions.

</details>


### [167] [SAIL-Recon: Large SfM by Augmenting Scene Regression with Localization](https://arxiv.org/abs/2508.17972)
*Junyuan Deng,Heng Li,Tao Xie,Weiqiang Ren,Qian Zhang,Ping Tan,Xiaoyang Guo*

Main category: cs.CV

TL;DR: SAIL-Recon是一种用于大规模SfM的前馈Transformer，通过增强场景回归网络的视觉定位能力，解决了现有方法处理大量输入图像的难题。


<details>
  <summary>Details</summary>
Motivation: 现有场景回归方法（如VGGT）在处理极端视角变化的图像时表现优异，但在处理大量输入图像时效果不佳。

Method: SAIL-Recon首先从一组锚点图像计算神经场景表示，然后微调回归网络以基于该表示重建所有输入图像。

Result: 实验表明，SAIL-Recon不仅高效扩展到大规模场景，还在相机姿态估计和新视角合成基准测试中达到最先进水平。

Conclusion: SAIL-Recon解决了大规模SfM问题，并在多个基准测试中表现优异，代码和模型已公开。

Abstract: Scene regression methods, such as VGGT, solve the Structure-from-Motion (SfM)
problem by directly regressing camera poses and 3D scene structures from input
images. They demonstrate impressive performance in handling images under
extreme viewpoint changes. However, these methods struggle to handle a large
number of input images. To address this problem, we introduce SAIL-Recon, a
feed-forward Transformer for large scale SfM, by augmenting the scene
regression network with visual localization capabilities. Specifically, our
method first computes a neural scene representation from a subset of anchor
images. The regression network is then fine-tuned to reconstruct all input
images conditioned on this neural scene representation. Comprehensive
experiments show that our method not only scales efficiently to large-scale
scenes, but also achieves state-of-the-art results on both camera pose
estimation and novel view synthesis benchmarks, including TUM-RGBD, CO3Dv2, and
Tanks & Temples. We will publish our model and code. Code and models are
publicly available at: https://hkust-sail.github.io/ sail-recon/.

</details>


### [168] [Enhanced Drift-Aware Computer Vision Architecture for Autonomous Driving](https://arxiv.org/abs/2508.17975)
*Md Shahi Amran Hossain,Abu Shad Ahammed,Sayeri Mukherjee,Roman Obermaisser*

Main category: cs.CV

TL;DR: 提出了一种混合计算机视觉架构，通过合成图像数据提升在数据漂移环境中的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶中高精度物体检测的需求，尤其是在恶劣天气或低光照条件下，ISO 8800标准的发布进一步强调了相关风险的管理。

Method: 结合YOLOv8快速检测和五层CNN验证的混合架构，使用合成图像数据进行训练。

Result: 在漂移增强的道路图像测试中，检测准确率提高了90%以上。

Conclusion: 混合模型在提升道路安全性方面具有显著潜力。

Abstract: The use of computer vision in automotive is a trending research in which
safety and security are a primary concern. In particular, for autonomous
driving, preventing road accidents requires highly accurate object detection
under diverse conditions. To address this issue, recently the International
Organization for Standardization (ISO) released the 8800 norm, providing
structured frameworks for managing associated AI relevant risks. However,
challenging scenarios such as adverse weather or low lighting often introduce
data drift, leading to degraded model performance and potential safety
violations. In this work, we present a novel hybrid computer vision
architecture trained with thousands of synthetic image data from the road
environment to improve robustness in unseen drifted environments. Our dual mode
framework utilized YOLO version 8 for swift detection and incorporated a
five-layer CNN for verification. The system functioned in sequence and improved
the detection accuracy by more than 90\% when tested with drift-augmented road
images. The focus was to demonstrate how such a hybrid model can provide better
road safety when working together in a hybrid structure.

</details>


### [169] [Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization](https://arxiv.org/abs/2508.17976)
*Keyang Zhang,Chenqi Kong,Hui Liu,Bo Ding,Xinghao Jiang,Haoliang Li*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Propose-Rectify框架，结合语义推理与法证分析，通过多尺度特征验证和增强分割模块，显著提升了图像篡改检测和定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 图像篡改技术的日益复杂需要更鲁棒的检测方法，现有MLLMs在低层次法证特征感知上存在不足。

Method: 采用两阶段框架：提案阶段利用法证适应的LLaVA模型生成初步分析；校正阶段通过多尺度法证特征分析和增强分割模块优化结果。

Result: 实验表明，该框架在多种数据集上实现了最先进的性能，具有出色的鲁棒性和泛化能力。

Conclusion: 通过结合多模态推理与法证方法，该框架显著提升了篡改检测和定位的准确性与可靠性。

Abstract: The increasing sophistication of image manipulation techniques demands robust
forensic solutions that can both reliably detect alterations and precisely
localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show
promise by leveraging world knowledge and semantic understanding for
context-aware detection, yet they struggle with perceiving subtle, low-level
forensic artifacts crucial for accurate manipulation localization. This paper
presents a novel Propose-Rectify framework that effectively bridges semantic
reasoning with forensic-specific analysis. In the proposal stage, our approach
utilizes a forensic-adapted LLaVA model to generate initial manipulation
analysis and preliminary localization of suspicious regions based on semantic
understanding and contextual reasoning. In the rectification stage, we
introduce a Forensics Rectification Module that systematically validates and
refines these initial proposals through multi-scale forensic feature analysis,
integrating technical evidence from several specialized filters. Additionally,
we present an Enhanced Segmentation Module that incorporates critical forensic
cues into SAM's encoded image embeddings, thereby overcoming inherent semantic
biases to achieve precise delineation of manipulated regions. By
synergistically combining advanced multimodal reasoning with established
forensic methodologies, our framework ensures that initial semantic proposals
are systematically validated and enhanced through concrete technical evidence,
resulting in comprehensive detection accuracy and localization precision.
Extensive experimental validation demonstrates state-of-the-art performance
across diverse datasets with exceptional robustness and generalization
capabilities.

</details>


### [170] [Fence off Anomaly Interference: Cross-Domain Distillation for Fully Unsupervised Anomaly Detection](https://arxiv.org/abs/2508.18007)
*Xinyue Liu,Jianyuan Wang,Biao Leng,Shuo Zhang*

Main category: cs.CV

TL;DR: 提出了一种基于知识蒸馏的跨域蒸馏框架（CDD），用于完全无监督异常检测（FUAD），通过域特定训练和跨域知识聚合提升性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在FUAD设置下可能导致学生模型学习异常表示，影响检测效果。

Method: 设计域特定训练和跨域知识聚合，通过伪正常特征指导全局学生学习跨域正常表示。

Result: 在MVTec AD和VisA数据集上显著优于基线方法。

Conclusion: CDD框架在FUAD设置下有效提升了异常检测性能。

Abstract: Fully Unsupervised Anomaly Detection (FUAD) is a practical extension of
Unsupervised Anomaly Detection (UAD), aiming to detect anomalies without any
labels even when the training set may contain anomalous samples. To achieve
FUAD, we pioneer the introduction of Knowledge Distillation (KD) paradigm based
on teacher-student framework into the FUAD setting. However, due to the
presence of anomalies in the training data, traditional KD methods risk
enabling the student to learn the teacher's representation of anomalies under
FUAD setting, thereby resulting in poor anomaly detection performance. To
address this issue, we propose a novel Cross-Domain Distillation (CDD)
framework based on the widely studied reverse distillation (RD) paradigm.
Specifically, we design a Domain-Specific Training, which divides the training
set into multiple domains with lower anomaly ratios and train a domain-specific
student for each. Cross-Domain Knowledge Aggregation is then performed, where
pseudo-normal features generated by domain-specific students collaboratively
guide a global student to learn generalized normal representations across all
samples. Experimental results on noisy versions of the MVTec AD and VisA
datasets demonstrate that our method achieves significant performance
improvements over the baseline, validating its effectiveness under FUAD
setting.

</details>


### [171] [Development of a Neural Network Model for Currency Detection to aid visually impaired people in Nigeria](https://arxiv.org/abs/2508.18012)
*Sochukwuma Nwokoye,Desmond Moru*

Main category: cs.CV

TL;DR: 利用神经网络技术帮助视障人士识别尼日利亚货币，准确率超过90%。


<details>
  <summary>Details</summary>
Motivation: 探索人工神经网络在辅助视障人士区分不同货币形式中的潜力，以简化商业交易。

Method: 构建包含3,468张图像的自定义数据集，并训练SSD神经网络模型。

Result: 系统能够准确识别尼日利亚货币，平均精度超过90%。

Conclusion: 该系统有望在辅助技术领域做出重要贡献，并提升视障人士的生活质量。

Abstract: Neural networks in assistive technology for visually impaired leverage
artificial intelligence's capacity to recognize patterns in complex data. They
are used for converting visual data into auditory or tactile representations,
helping the visually impaired understand their surroundings. The primary aim of
this research is to explore the potential of artificial neural networks to
facilitate the differentiation of various forms of cash for individuals with
visual impairments. In this study, we built a custom dataset of 3,468 images,
which was subsequently used to train an SSD neural network model. The proposed
system can accurately identify Nigerian cash, thereby streamlining commercial
transactions. The performance of the system in terms of accuracy was assessed,
and the Mean Average Precision score was over 90%. We believe that our system
has the potential to make a substantial contribution to the field of assistive
technology while also improving the quality of life of visually challenged
persons in Nigeria and beyond.

</details>


### [172] [Towards Continual Visual Anomaly Detection in the Medical Domain](https://arxiv.org/abs/2508.18013)
*Manuel Barusco,Francesco Borsatti,Nicola Beda,Davide Dalle Pezze,Gian Antonio Susto*

Main category: cs.CV

TL;DR: 本文首次探索了在医疗领域中应用持续学习（CL）的视觉异常检测（VAD）模型，提出PatchCoreCL方法，并在真实医疗数据集BMAD上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 医疗影像数据具有动态演变的特性，传统VAD模型在数据分布变化时性能下降，持续学习提供了一种适应性强的解决方案。

Method: 采用持续学习版本的PatchCore模型（PatchCoreCL），在BMAD数据集上进行评估，包含图像级和像素级标注。

Result: PatchCoreCL表现优异，性能接近任务专用模型，遗忘值低于1%。

Conclusion: 持续学习在医疗影像的适应性VAD中具有可行性和潜力。

Abstract: Visual Anomaly Detection (VAD) seeks to identify abnormal images and
precisely localize the corresponding anomalous regions, relying solely on
normal data during training. This approach has proven essential in domains such
as manufacturing and, more recently, in the medical field, where accurate and
explainable detection is critical. Despite its importance, the impact of
evolving input data distributions over time has received limited attention,
even though such changes can significantly degrade model performance. In
particular, given the dynamic and evolving nature of medical imaging data,
Continual Learning (CL) provides a natural and effective framework to
incrementally adapt models while preserving previously acquired knowledge. This
study explores for the first time the application of VAD models in a CL
scenario for the medical field. In this work, we utilize a CL version of the
well-established PatchCore model, called PatchCoreCL, and evaluate its
performance using BMAD, a real-world medical imaging dataset with both
image-level and pixel-level annotations. Our results demonstrate that
PatchCoreCL is an effective solution, achieving performance comparable to the
task-specific models, with a forgetting value less than a 1%, highlighting the
feasibility and potential of CL for adaptive VAD in medical imaging.

</details>


### [173] [FCR: Investigating Generative AI models for Forensic Craniofacial Reconstruction](https://arxiv.org/abs/2508.18031)
*Ravi Shankar Prasad,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出了一种基于生成模型的颅面重建框架，首次使用2D X射线图像作为输入，通过生成对抗网络（如CycleGANs、cGANs）生成更真实的颅骨和面部图像，并验证了其在法医学中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统颅面重建方法依赖专家知识且耗时，现有生成模型难以捕捉颅骨与面部的跨域特征，因此需要一种更高效、通用的解决方案。

Method: 使用多种生成模型（如CycleGANs、cGANs），通过微调生成器和判别器，从2D X射线图像生成颅骨和面部图像，并设计检索框架匹配生成的面部与真实面部数据库。

Result: 通过FID、IS和SSIM评分评估生成图像质量，实验结果表明该方法在法医学中具有潜在应用价值。

Conclusion: 提出的框架首次利用2D X射线图像进行颅面重建，生成效果良好，可作为法医学中的有效工具。

Abstract: Craniofacial reconstruction in forensics is one of the processes to identify
victims of crime and natural disasters. Identifying an individual from their
remains plays a crucial role when all other identification methods fail.
Traditional methods for this task, such as clay-based craniofacial
reconstruction, require expert domain knowledge and are a time-consuming
process. At the same time, other probabilistic generative models like the
statistical shape model or the Basel face model fail to capture the skull and
face cross-domain attributes. Looking at these limitations, we propose a
generic framework for craniofacial reconstruction from 2D X-ray images. Here,
we used various generative models (i.e., CycleGANs, cGANs, etc) and fine-tune
the generator and discriminator parts to generate more realistic images in two
distinct domains, which are the skull and face of an individual. This is the
first time where 2D X-rays are being used as a representation of the skull by
generative models for craniofacial reconstruction. We have evaluated the
quality of generated faces using FID, IS, and SSIM scores. Finally, we have
proposed a retrieval framework where the query is the generated face image and
the gallery is the database of real faces. By experimental results, we have
found that this can be an effective tool for forensic science.

</details>


### [174] [Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation](https://arxiv.org/abs/2508.18032)
*Yaqi Li,Peng Chen,Mingyang Han,Bu Pi,Haoxiang Shi,Runzhou Zhao,Yang Yao,Xuan Zhang,Jun Song*

Main category: cs.CV

TL;DR: 论文提出了一种名为Visual-CoG的新范式，通过分阶段奖励机制改进多属性和模糊提示的文本到图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有自回归模型在多属性和模糊提示的文本到图像生成中表现有限，且现有方法仅在生成阶段结束时提供奖励信号，导致难以优化中间阶段。

Method: 提出Visual-CoG范式，包含语义推理、过程细化和结果评估三个阶段，并在整个生成流程中提供阶段感知的奖励。

Result: 在GenEval、T2I-CompBench和VisCog-Bench上的评估显示，性能分别提升了15%、5%和19%。

Conclusion: Visual-CoG通过分阶段奖励机制显著提升了文本到图像生成的性能，尤其是在多属性和模糊提示任务中。

Abstract: Despite the promising progress of recent autoregressive models in
text-to-image (T2I) generation, their ability to handle multi-attribute and
ambiguous prompts remains limited. To address these limitations, existing works
have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and
employed reinforcement learning (RL) to improve reasoning capabilities.
However, most models provide reward signals only at the end of the generation
stage. This monolithic final-only guidance makes it difficult to identify which
stages contribute positively to the final outcome and may lead to suboptimal
policies. To tackle this issue, we propose a Visual-Chain of Guidance
(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process
refining, and outcome evaluation, with stage-aware rewards providing immediate
guidance throughout the image generation pipeline. We further construct a
visual cognition benchmark, VisCog-Bench, which comprises four subtasks to
evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on
GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,
5%, and 19%, respectively, demonstrating the superior performance of the
proposed Visual-CoG. We will release all the resources soon.

</details>


### [175] [ArgusCogito: Chain-of-Thought for Cross-Modal Synergy and Omnidirectional Reasoning in Camouflaged Object Segmentation](https://arxiv.org/abs/2508.18050)
*Jianwen Tan,Huiyao Zhang,Rui Xiong,Han Zhou,Hongfei Wang,Ye Li*

Main category: cs.CV

TL;DR: ArgusCogito是一个基于视觉语言模型的零样本、链式思维框架，通过跨模态协同和全方位推理解决伪装目标分割问题，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 伪装目标分割因目标与背景高度相似而极具挑战性，现有方法因特征表示浅、推理机制不足和跨模态整合弱而表现不佳。

Method: ArgusCogito包含三个阶段：猜想（全局推理与跨模态融合）、聚焦（注意力驱动扫描与精确定位）和雕刻（迭代生成高保真分割掩码）。

Result: 在四个COS基准和三个MIS基准测试中，ArgusCogito实现了最先进的性能。

Conclusion: ArgusCogito展示了卓越的效能、泛化能力和鲁棒性。

Abstract: Camouflaged Object Segmentation (COS) poses a significant challenge due to
the intrinsic high similarity between targets and backgrounds, demanding models
capable of profound holistic understanding beyond superficial cues. Prevailing
methods, often limited by shallow feature representation, inadequate reasoning
mechanisms, and weak cross-modal integration, struggle to achieve this depth of
cognition, resulting in prevalent issues like incomplete target separation and
imprecise segmentation. Inspired by the perceptual strategy of the Hundred-eyed
Giant-emphasizing holistic observation, omnidirectional focus, and intensive
scrutiny-we introduce ArgusCogito, a novel zero-shot, chain-of-thought
framework underpinned by cross-modal synergy and omnidirectional reasoning
within Vision-Language Models (VLMs). ArgusCogito orchestrates three
cognitively-inspired stages: (1) Conjecture: Constructs a strong cognitive
prior through global reasoning with cross-modal fusion (RGB, depth, semantic
maps), enabling holistic scene understanding and enhanced target-background
disambiguation. (2) Focus: Performs omnidirectional, attention-driven scanning
and focused reasoning, guided by semantic priors from Conjecture, enabling
precise target localization and region-of-interest refinement. (3) Sculpting:
Progressively sculpts high-fidelity segmentation masks by integrating
cross-modal information and iteratively generating dense positive/negative
point prompts within focused regions, emulating Argus' intensive scrutiny.
Extensive evaluations on four challenging COS benchmarks and three Medical
Image Segmentation (MIS) benchmarks demonstrate that ArgusCogito achieves
state-of-the-art (SOTA) performance, validating the framework's exceptional
efficacy, superior generalization capability, and robustness.

</details>


### [176] [Annotation-Free Open-Vocabulary Segmentation for Remote-Sensing Images](https://arxiv.org/abs/2508.18067)
*Kaiyu Li,Xiangyong Cao,Ruixun Liu,Shihong Wang,Zixuan Jiang,Zhi Wang,Deyu Meng*

Main category: cs.CV

TL;DR: SegEarth-OV是一个无需标注的开放词汇遥感图像分割框架，通过SimFeatUp和全局偏差缓解操作提升细节和语义保真度，并通过AlignEarth扩展到SAR图像。


<details>
  <summary>Details</summary>
Motivation: 解决遥感图像开放词汇分割中标注成本高和现有自然图像框架不适应的问题。

Method: 提出SimFeatUp恢复高分辨率细节，全局偏差缓解操作提升语义保真度，以及AlignEarth将知识从光学VLM迁移到SAR编码器。

Result: 在光学和SAR数据集上显著优于现有方法。

Conclusion: SegEarth-OV为无标注和开放世界的遥感观测提供了坚实基础。

Abstract: Semantic segmentation of remote sensing (RS) images is pivotal for
comprehensive Earth observation, but the demand for interpreting new object
categories, coupled with the high expense of manual annotation, poses
significant challenges. Although open-vocabulary semantic segmentation (OVSS)
offers a promising solution, existing frameworks designed for natural images
are insufficient for the unique complexities of RS data. They struggle with
vast scale variations and fine-grained details, and their adaptation often
relies on extensive, costly annotations. To address this critical gap, this
paper introduces SegEarth-OV, the first framework for annotation-free
open-vocabulary segmentation of RS images. Specifically, we propose SimFeatUp,
a universal upsampler that robustly restores high-resolution spatial details
from coarse features, correcting distorted target shapes without any
task-specific post-training. We also present a simple yet effective Global Bias
Alleviation operation to subtract the inherent global context from patch
features, significantly enhancing local semantic fidelity. These components
empower SegEarth-OV to effectively harness the rich semantics of pre-trained
VLMs, making OVSS possible in optical RS contexts. Furthermore, to extend the
framework's universality to other challenging RS modalities like SAR images,
where large-scale VLMs are unavailable and expensive to create, we introduce
AlignEarth, which is a distillation-based strategy and can efficiently transfer
semantic knowledge from an optical VLM encoder to an SAR encoder, bypassing the
need to build SAR foundation models from scratch and enabling universal OVSS
across diverse sensor types. Extensive experiments on both optical and SAR
datasets validate that SegEarth-OV can achieve dramatic improvements over the
SOTA methods, establishing a robust foundation for annotation-free and
open-world Earth observation.

</details>


### [177] [EventTracer: Fast Path Tracing-based Event Stream Rendering](https://arxiv.org/abs/2508.18071)
*Zhenyang Li,Xiaoyang Bai,Jinfan Lu,Pengfei Shen,Edmund Y. Lam,Yifan Peng*

Main category: cs.CV

TL;DR: EventTracer是一个基于路径追踪的渲染管道，用于高效且物理感知地模拟复杂3D场景中的高保真事件序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用无噪声的RGB帧，渲染成本高且时间分辨率低（100-300 FPS），无法匹配真实事件数据的频率。

Method: 通过低样本每像素（SPP）路径追踪加速渲染，并训练轻量级事件脉冲网络去噪RGB视频为真实事件序列。网络使用双极泄漏积分-发放（BiLIF）单元和双向地球移动距离（EMD）损失。

Result: EventTracer能以约4分钟/秒720p视频的速度运行，捕捉更好的场景细节，且与真实事件数据相似性更高。

Conclusion: EventTracer是低成本创建大规模事件-RGB数据集的有力工具，缩小了事件视觉的模拟-现实差距，适用于机器人、自动驾驶和VRAR等场景。

Abstract: Simulating event streams from 3D scenes has become a common practice in
event-based vision research, as it meets the demand for large-scale, high
temporal frequency data without setting up expensive hardware devices or
undertaking extensive data collections. Yet existing methods in this direction
typically work with noiseless RGB frames that are costly to render, and
therefore they can only achieve a temporal resolution equivalent to 100-300
FPS, far lower than that of real-world event data. In this work, we propose
EventTracer, a path tracing-based rendering pipeline that simulates
high-fidelity event sequences from complex 3D scenes in an efficient and
physics-aware manner. Specifically, we speed up the rendering process via low
sample-per-pixel (SPP) path tracing, and train a lightweight event spiking
network to denoise the resulting RGB videos into realistic event sequences. To
capture the physical properties of event streams, the network is equipped with
a bipolar leaky integrate-and-fired (BiLIF) spiking unit and trained with a
bidirectional earth mover distance (EMD) loss. Our EventTracer pipeline runs at
a speed of about 4 minutes per second of 720p video, and it inherits the merit
of accurate spatiotemporal modeling from its path tracing backbone. We show in
two downstream tasks that EventTracer captures better scene details and
demonstrates a greater similarity to real-world event data than other event
simulators, which establishes it as a promising tool for creating large-scale
event-RGB datasets at a low cost, narrowing the sim-to-real gap in event-based
vision, and boosting various application scenarios such as robotics, autonomous
driving, and VRAR.

</details>


### [178] [Few-shot Unknown Class Discovery of Hyperspectral Images with Prototype Learning and Clustering](https://arxiv.org/abs/2508.18075)
*Chun Liu,Chen Zhang,Zhuo Li,Zheng Li,Wei Yang*

Main category: cs.CV

TL;DR: 提出了一种原型学习和聚类方法，用于在少样本环境下发现高光谱图像中的未知类别，同时区分已知类别。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅能区分未知类别样本并拒绝它们，无法进一步识别或发现未知类别。

Method: 通过原型学习和聚类方法，推断未知类别的原型，并将未知类别样本聚类到不同类别。

Result: 在四个基准数据集上表现出色，优于现有方法。

Conclusion: 该方法在少样本开放集高光谱图像分类任务中具有竞争力。

Abstract: Open-set few-shot hyperspectral image (HSI) classification aims to classify
image pixels by using few labeled pixels per class, where the pixels to be
classified may be not all from the classes that have been seen. To address the
open-set HSI classification challenge, current methods focus mainly on
distinguishing the unknown class samples from the known class samples and
rejecting them to increase the accuracy of identifying known class samples.
They fails to further identify or discovery the unknow classes among the
samples. This paper proposes a prototype learning and clustering method for
discoverying unknown classes in HSIs under the few-shot environment. Using few
labeled samples, it strives to develop the ability of infering the prototypes
of unknown classes while distinguishing unknown classes from known classes.
Once the unknown class samples are rejected by the learned known class
classifier, the proposed method can further cluster the unknown class samples
into different classes according to their distance to the inferred unknown
class prototypes. Compared to existing state-of-the-art methods, extensive
experiments on four benchmark HSI datasets demonstrate that our proposed method
exhibits competitive performance in open-set few-shot HSI classification tasks.
All the codes are available at \href{https://github.com/KOBEN-ff/OpenFUCD-main}
{https://github.com/KOBEN-ff/OpenFUCD-main}

</details>


### [179] [Incorporating Pre-trained Diffusion Models in Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2508.18095)
*Zhicong Tang,Tiankai Hang,Shuyang Gu,Dong Chen,Baining Guo*

Main category: cs.CV

TL;DR: 本文通过三种重新参数化技术（IPMM、IPTM、IPFM）统一了基于分数的生成模型（SGMs）和薛定谔桥（SB）问题，显著加速和稳定了SB模型的训练。


<details>
  <summary>Details</summary>
Motivation: 旨在结合SGMs和SB问题的优势，提升生成模型的训练效率和性能。

Method: 提出三种重新参数化技术（IPMM、IPTM、IPFM）和基于预训练SGMs的初始化策略。

Result: 实验证明方法显著有效，提升了SB模型的训练效率和SGMs的性能。

Conclusion: 该研究为生成模型的未来研究提供了重要贡献和方向。

Abstract: This paper aims to unify Score-based Generative Models (SGMs), also known as
Diffusion models, and the Schr\"odinger Bridge (SB) problem through three
reparameterization techniques: Iterative Proportional Mean-Matching (IPMM),
Iterative Proportional Terminus-Matching (IPTM), and Iterative Proportional
Flow-Matching (IPFM). These techniques significantly accelerate and stabilize
the training of SB-based models. Furthermore, the paper introduces novel
initialization strategies that use pre-trained SGMs to effectively train
SB-based models. By using SGMs as initialization, we leverage the advantages of
both SB-based models and SGMs, ensuring efficient training of SB-based models
and further improving the performance of SGMs. Extensive experiments
demonstrate the significant effectiveness and improvements of the proposed
methods. We believe this work contributes to and paves the way for future
research on generative models.

</details>


### [180] [BirdRecorder's AI on Sky: Safeguarding birds of prey by detection and classification of tiny objects around wind turbines](https://arxiv.org/abs/2508.18136)
*Nico Klar,Nizam Gifary,Felix P. G. Ziegler,Frank Sehnke,Anton Kaifel,Eric Price,Aamir Ahmad*

Main category: cs.CV

TL;DR: BirdRecorder是一种基于AI的防碰撞系统，旨在通过实时图像处理保护濒危鸟类（如红鸢），以减少风力涡轮机与鸟类的冲突。


<details>
  <summary>Details</summary>
Motivation: 解决可再生能源（尤其是风能）扩展与野生动物保护之间的冲突，特别是保护濒危鸟类（如红鸢）免受风力涡轮机的伤害。

Method: 结合机器人技术、遥测技术和高性能AI算法，使用SSD进行检测，并通过专用硬件加速和跟踪算法实现实时图像处理。

Result: BirdRecorder在检测精度和效率上优于现有方法，并在实地测试中表现出色。

Conclusion: BirdRecorder通过技术手段实现了可再生能源扩展与野生动物保护的平衡，促进了技术与自然的可持续共存。

Abstract: The urgent need for renewable energy expansion, particularly wind power, is
hindered by conflicts with wildlife conservation. To address this, we developed
BirdRecorder, an advanced AI-based anti-collision system to protect endangered
birds, especially the red kite (Milvus milvus). Integrating robotics,
telemetry, and high-performance AI algorithms, BirdRecorder aims to detect,
track, and classify avian species within a range of 800 m to minimize
bird-turbine collisions.
  BirdRecorder integrates advanced AI methods with optimized hardware and
software architectures to enable real-time image processing. Leveraging Single
Shot Detector (SSD) for detection, combined with specialized hardware
acceleration and tracking algorithms, our system achieves high detection
precision while maintaining the speed necessary for real-time decision-making.
By combining these components, BirdRecorder outperforms existing approaches in
both accuracy and efficiency.
  In this paper, we summarize results on field tests and performance of the
BirdRecorder system. By bridging the gap between renewable energy expansion and
wildlife conservation, BirdRecorder contributes to a more sustainable
coexistence of technology and nature.

</details>


### [181] [Assessing the Noise Robustness of Class Activation Maps: A Framework for Reliable Model Interpretability](https://arxiv.org/abs/2508.18154)
*Syamantak Sarkar,Revoti P. Bora,Bhupender Kaushal,Sudhish N George,Kiran Raja*

Main category: cs.CV

TL;DR: 该论文评估了不同噪声对CAM方法的影响，提出了一个衡量CAM鲁棒性的新指标，并分析了其在不同模型和数据集中的表现。


<details>
  <summary>Details</summary>
Motivation: CAM方法在深度学习模型可视化中很重要，但其对不同噪声的鲁棒性尚未充分研究。

Method: 通过分析不同噪声类型对CAM解释的影响，评估了CAM的噪声敏感性和数据集特性对解释稳定性的影响。

Result: 研究发现不同CAM方法对噪声的敏感性差异显著，并提出了一个包含一致性和响应性的鲁棒性指标。

Conclusion: 提出的鲁棒性指标能有效评估CAM方法在不同噪声条件下的表现，为未来研究提供了实用工具。

Abstract: Class Activation Maps (CAMs) are one of the important methods for visualizing
regions used by deep learning models. Yet their robustness to different noise
remains underexplored. In this work, we evaluate and report the resilience of
various CAM methods for different noise perturbations across multiple
architectures and datasets. By analyzing the influence of different noise types
on CAM explanations, we assess the susceptibility to noise and the extent to
which dataset characteristics may impact explanation stability. The findings
highlight considerable variability in noise sensitivity for various CAMs. We
propose a robustness metric for CAMs that captures two key properties:
consistency and responsiveness. Consistency reflects the ability of CAMs to
remain stable under input perturbations that do not alter the predicted class,
while responsiveness measures the sensitivity of CAMs to changes in the
prediction caused by such perturbations. The metric is evaluated empirically
across models, different perturbations, and datasets along with complementary
statistical tests to exemplify the applicability of our proposed approach.

</details>


### [182] [SpotEdit: Evaluating Visually-Guided Image Editing Methods](https://arxiv.org/abs/2508.18159)
*Sara Ghazanfari,Wei-An Lin,Haitong Tian,Ersin Yumer*

Main category: cs.CV

TL;DR: SpotEdit是一个全面的基准测试，用于评估视觉引导图像编辑方法，揭示了不同生成模型之间的性能差异，并特别关注了幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于简单，无法代表真实世界的编辑挑战，因此需要更全面的基准测试。

Method: 通过设计SpotEdit基准，系统地评估扩散、自回归和混合生成模型在视觉引导图像编辑中的表现。

Result: 发现领先模型（如GPT-4o）常因幻觉问题错误执行编辑任务。

Conclusion: SpotEdit为视觉引导图像编辑提供了更全面的评估工具，揭示了模型在幻觉问题上的不足。

Abstract: Visually-guided image editing, where edits are conditioned on both visual
cues and textual prompts, has emerged as a powerful paradigm for fine-grained,
controllable content generation. Although recent generative models have shown
remarkable capabilities, existing evaluations remain simple and insufficiently
representative of real-world editing challenges. We present SpotEdit, a
comprehensive benchmark designed to systematically assess visually-guided image
editing methods across diverse diffusion, autoregressive, and hybrid generative
models, uncovering substantial performance disparities. To address a critical
yet underexplored challenge, our benchmark includes a dedicated component on
hallucination, highlighting how leading models, such as GPT-4o, often
hallucinate the existence of a visual cue and erroneously perform the editing
task. Our code and benchmark are publicly released at
https://github.com/SaraGhazanfari/SpotEdit.

</details>


### [183] [Scene-Aware Vectorized Memory Multi-Agent Framework with Cross-Modal Differentiated Quantization VLMs for Visually Impaired Assistance](https://arxiv.org/abs/2508.18177)
*Xiangxiang Wang,Xuanyu Wang,YiJia Luo,Yongbin Yu,Manping Fan,Jingtao Zhang,Liyong Ren*

Main category: cs.CV

TL;DR: 该研究提出了双技术创新框架，包括视觉语言模型的跨模态差异化量化框架和面向视障辅助的场景感知向量化记忆多智能体系统，显著降低了内存需求并保持了性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提升视觉语言模型的计算效率，并为视障用户提供全面的实时场景感知、文本识别和导航辅助。

Method: 采用差异化处理策略的模块化框架，结合场景分类、向量化记忆和多模态交互的多智能体架构。

Result: 量化后的19B参数模型性能仅下降2.05%，响应延迟为2.83-3.52秒，优于同类小模型。

Conclusion: 该研究在计算效率和辅助技术方面取得进展，为视障用户提供了高效的实时辅助。

Abstract: This study proposes the dual technological innovation framework, including a
cross-modal differ entiated quantization framework for vision-language models
(VLMs) and a scene-aware vectorized
  memory multi-agent system for visually impaired assistance. The modular
framework was developed
  implementing differentiated processing strategies, effectively reducing
memory requirements from
  38GB to 16GB while maintaining model performance. The multi-agent
architecture combines
  scene classification, vectorized memory, and multimodal interaction, enabling
persistent storage
  and efficient retrieval of scene memories. Through
perception-memory-reasoning workflows, the
  system provides environmental information beyond the current view using
historical memories.
  Experiments show the quantized 19B-parameter model only experiences a 2.05%
performance drop
  on MMBench and maintains 63.7 accuracy on OCR-VQA (original: 64.9),
outperforming smaller
  models with equivalent memory requirements like the Molmo-7B series. The
system maintains
  response latency between 2.83-3.52 seconds from scene analysis to initial
speech output, substantially
  faster than non-streaming methods. This research advances computational
efficiency and assistive
  technology, offering visually impaired users comprehensive real-time
assistance in scene perception,
  text recognition, and navigation.

</details>


### [184] [Emerging Semantic Segmentation from Positive and Negative Coarse Label Learning](https://arxiv.org/abs/2508.18186)
*Le Zhang,Fuping Wu,Arun Thirunavukarasu,Kevin Bronik,Thomas Nichols,Bartlomiej W. Papiez*

Main category: cs.CV

TL;DR: 提出了一种利用粗糙标注训练语义分割模型的方法，通过两个耦合的CNN学习真实标签分布，并在多种数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 精细标注耗时且昂贵，而粗糙标注更易获取，但包含噪声。本文旨在利用粗糙标注训练高质量的分割模型。

Method: 使用两个耦合的CNN从粗糙标注中学习真实标签分布，并引入互补标签学习以估计负标签分布。

Result: 在MNIST、Cityscapes和视网膜图像数据集上，方法优于现有技术，尤其在粗糙标注比例较低时表现突出。

Conclusion: 该方法有效利用粗糙标注训练分割模型，为减少标注成本提供了可行方案。

Abstract: Large annotated datasets are vital for training segmentation models, but
pixel-level labeling is time-consuming, error-prone, and often requires scarce
expert annotators, especially in medical imaging. In contrast, coarse
annotations are quicker, cheaper, and easier to produce, even by non-experts.
In this paper, we propose to use coarse drawings from both positive (target)
and negative (background) classes in the image, even with noisy pixels, to
train a convolutional neural network (CNN) for semantic segmentation. We
present a method for learning the true segmentation label distributions from
purely noisy coarse annotations using two coupled CNNs. The separation of the
two CNNs is achieved by high fidelity with the characters of the noisy training
annotations. We propose to add a complementary label learning that encourages
estimating negative label distribution. To illustrate the properties of our
method, we first use a toy segmentation dataset based on MNIST. We then present
the quantitative results of experiments using publicly available datasets:
Cityscapes dataset for multi-class segmentation, and retinal images for medical
applications. In all experiments, our method outperforms state-of-the-art
methods, particularly in the cases where the ratio of coarse annotations is
small compared to the given dense annotations.

</details>


### [185] [BRAIN: Bias-Mitigation Continual Learning Approach to Vision-Brain Understanding](https://arxiv.org/abs/2508.18187)
*Xuan-Bac Nguyen,Thanh-Dat Truong,Pawan Sinha,Khoa Luu*

Main category: cs.CV

TL;DR: 论文提出了一种新的Bias-Mitigation Continual Learning (BRAIN)方法，用于解决大脑信号随时间变化导致的视觉-大脑理解模型性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 人类大脑的记忆衰减导致视觉对象识别和细节保留能力下降，进而使记录的脑信号随时间变得微弱、不确定且缺乏视觉上下文。

Method: 提出BRAIN方法，采用持续学习框架，通过De-bias Contrastive Learning损失函数和Angular-based Forgetting Mitigation技术来减少偏差和防止灾难性遗忘。

Result: 实验表明，该方法在多个基准测试中达到了最先进的性能，优于现有方法。

Conclusion: BRAIN方法有效解决了脑信号不一致性对模型学习的影响，提升了视觉-大脑理解模型的性能。

Abstract: Memory decay makes it harder for the human brain to recognize visual objects
and retain details. Consequently, recorded brain signals become weaker,
uncertain, and contain poor visual context over time. This paper presents one
of the first vision-learning approaches to address this problem. First, we
statistically and experimentally demonstrate the existence of inconsistency in
brain signals and its impact on the Vision-Brain Understanding (VBU) model. Our
findings show that brain signal representations shift over recording sessions,
leading to compounding bias, which poses challenges for model learning and
degrades performance. Then, we propose a new Bias-Mitigation Continual Learning
(BRAIN) approach to address these limitations. In this approach, the model is
trained in a continual learning setup and mitigates the growing bias from each
learning step. A new loss function named De-bias Contrastive Learning is also
introduced to address the bias problem. In addition, to prevent catastrophic
forgetting, where the model loses knowledge from previous sessions, the new
Angular-based Forgetting Mitigation approach is introduced to preserve learned
knowledge in the model. Finally, the empirical experiments demonstrate that our
approach achieves State-of-the-Art (SOTA) performance across various
benchmarks, surpassing prior and non-continual learning methods.

</details>


### [186] [Explain and Monitor Deep Learning Models for Computer Vision using Obz AI](https://arxiv.org/abs/2508.18188)
*Neo Christopher Chung,Jakub Binda*

Main category: cs.CV

TL;DR: Obz AI是一个软件生态系统，旨在提升计算机视觉系统的可解释性和可观测性。


<details>
  <summary>Details</summary>
Motivation: 尽管深度学习在计算机视觉中表现出色，但其决策过程缺乏透明度，且现有可解释AI技术在实际部署中应用不足。

Method: 开发了Obz AI，提供从Python客户端库到全栈分析仪表板的集成解决方案，支持XAI技术、异常检测和实时监控。

Result: Obz AI使深度学习模型的决策机制可解释，促进了计算机视觉系统的可观测性和负责任部署。

Conclusion: Obz AI填补了计算机视觉系统中可解释性和可观测性的技术空白。

Abstract: Deep learning has transformed computer vision (CV), achieving outstanding
performance in classification, segmentation, and related tasks. Such AI-based
CV systems are becoming prevalent, with applications spanning from medical
imaging to surveillance. State of the art models such as convolutional neural
networks (CNNs) and vision transformers (ViTs) are often regarded as ``black
boxes,'' offering limited transparency into their decision-making processes.
Despite a recent advancement in explainable AI (XAI), explainability remains
underutilized in practical CV deployments. A primary obstacle is the absence of
integrated software solutions that connect XAI techniques with robust knowledge
management and monitoring frameworks. To close this gap, we have developed Obz
AI, a comprehensive software ecosystem designed to facilitate state-of-the-art
explainability and observability for vision AI systems. Obz AI provides a
seamless integration pipeline, from a Python client library to a full-stack
analytics dashboard. With Obz AI, a machine learning engineer can easily
incorporate advanced XAI methodologies, extract and analyze features for
outlier detection, and continuously monitor AI models in real time. By making
the decision-making mechanisms of deep models interpretable, Obz AI promotes
observability and responsible deployment of computer vision systems.

</details>


### [187] [Follow My Hold: Hand-Object Interaction Reconstruction through Geometric Guidance](https://arxiv.org/abs/2508.18213)
*Ayce Idil Aytekin,Helge Rhodin,Rishabh Dabral,Christian Theobalt*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的新框架，通过手-物体交互作为几何指导，从单目RGB图像重建手持物体的3D几何。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖大量后处理或产生低质量重建，无法直接生成高质量几何。

Method: 利用潜在扩散模型，结合优化循环设计，通过多模态几何线索（如法线、深度对齐等）指导扩散过程。

Result: 在遮挡情况下生成准确、鲁棒且连贯的重建，适用于野外场景。

Conclusion: 该方法通过优化设计和多模态指导，实现了高质量的手持物体3D重建。

Abstract: We propose a novel diffusion-based framework for reconstructing 3D geometry
of hand-held objects from monocular RGB images by leveraging hand-object
interaction as geometric guidance. Our method conditions a latent diffusion
model on an inpainted object appearance and uses inference-time guidance to
optimize the object reconstruction, while simultaneously ensuring plausible
hand-object interactions. Unlike prior methods that rely on extensive
post-processing or produce low-quality reconstructions, our approach directly
generates high-quality object geometry during the diffusion process by
introducing guidance with an optimization-in-the-loop design. Specifically, we
guide the diffusion model by applying supervision to the velocity field while
simultaneously optimizing the transformations of both the hand and the object
being reconstructed. This optimization is driven by multi-modal geometric cues,
including normal and depth alignment, silhouette consistency, and 2D keypoint
reprojection. We further incorporate signed distance field supervision and
enforce contact and non-intersection constraints to ensure physical
plausibility of hand-object interaction. Our method yields accurate, robust and
coherent reconstructions under occlusion while generalizing well to in-the-wild
scenarios.

</details>


### [188] [GM-Skip: Metric-Guided Transformer Block Skipping for Efficient Vision-Language Models](https://arxiv.org/abs/2508.18227)
*Lianming Huang,Haibo Hu,Qiao Li,Xin He,Nan Guan,Chun Jason Xue*

Main category: cs.CV

TL;DR: GM-Skip是一种灵活的Transformer块跳过框架，通过贪婪的度量引导策略加速视觉语言模型推理，同时保持输出质量。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer视觉语言模型计算成本高，难以部署在延迟敏感的应用中。

Method: GM-Skip采用贪婪的度量引导块选择策略和反向顺序删除机制，平衡稀疏性和性能。

Result: 在COCO数据集上，GM-Skip跳过40%以上的Transformer块，将分类准确率从19.1%提升到87.3%，并在实际部署中减少45.4%的延迟。

Conclusion: GM-Skip有效加速推理并保持性能，适用于实际应用。

Abstract: Transformer-based Vision-Language Models (VLMs) have achieved impressive
performance on tasks such as image captioning, object recognition, and visual
reasoning, but their high computational cost hinders deployment in
latency-sensitive applications like autonomous driving. We introduce GM-Skip, a
flexible and metric-adaptive framework for Transformer block skipping that
accelerates VLM inference while preserving output quality. GM-Skip features a
greedy, metric-guided block selection strategy that uses metric feedback (e.g.,
accuracy, CIDEr) to identify redundant layers, along with a reverse-order
deletion mechanism that preserves early foundational blocks to avoid
performance collapse. To support diverse deployment needs, it incorporates a
tunable trade-off between sparsity and performance via a score-sparsity balance
objective. Experiments across multiple tasks and datasets, including COCO and
CODA, show that GM-Skip consistently improves inference speed while maintaining
task performance. On the COCO dataset, GM-Skip improves single-object
classification accuracy on the Person category from 19.1 percent to 87.3
percent while skipping more than 40 percent of Transformer blocks. In
real-world deployment, it achieves up to 45.4 percent latency reduction on
single-object detection when integrated into an autonomous vehicle running
Autoware.Universe, validating the effectiveness of its skip configurations and
confirming its practical value in accelerating real-world inference.

</details>


### [189] [Sealing The Backdoor: Unlearning Adversarial Text Triggers In Diffusion Models Using Knowledge Distillation](https://arxiv.org/abs/2508.18235)
*Ashwath Vaithinathan Aravindan,Abha Jha,Matthew Salaway,Atharva Sandeep Bhide,Duygu Nur Yaldiz*

Main category: cs.CV

TL;DR: 论文提出了一种名为SKD-CAG的方法，通过选择性消除模型对对抗性文本触发器的学习关联，有效防御文本到图像扩散模型的后门攻击，同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 文本到图像扩散模型易受后门攻击，现有防御方法在生成模型中效果有限，需要一种既能消除后门影响又不损害生成质量的技术。

Method: 采用自知识蒸馏与交叉注意力引导（SKD-CAG），利用知识蒸馏修正模型对中毒提示的响应，并通过交叉注意力机制在注意力层面消除后门影响。

Result: 实验表明，SKD-CAG在像素后门和基于风格的攻击中分别达到100%和93%的移除准确率，且不损害模型鲁棒性或图像质量。

Conclusion: SKD-CAG是一种有效的生成模型后门防御方法，为生成模型的安全性提供了新思路。

Abstract: Text-to-image diffusion models have revolutionized generative AI, but their
vulnerability to backdoor attacks poses significant security risks. Adversaries
can inject imperceptible textual triggers into training data, causing models to
generate manipulated outputs. Although text-based backdoor defenses in
classification models are well-explored, generative models lack effective
mitigation techniques against. We address this by selectively erasing the
model's learned associations between adversarial text triggers and poisoned
outputs, while preserving overall generation quality. Our approach,
Self-Knowledge Distillation with Cross-Attention Guidance (SKD-CAG), uses
knowledge distillation to guide the model in correcting responses to poisoned
prompts while maintaining image quality by exploiting the fact that the
backdoored model still produces clean outputs in the absence of triggers. Using
the cross-attention mechanism, SKD-CAG neutralizes backdoor influences at the
attention level, ensuring the targeted removal of adversarial effects.
Extensive experiments show that our method outperforms existing approaches,
achieving removal accuracy 100\% for pixel backdoors and 93\% for style-based
attacks, without sacrificing robustness or image fidelity. Our findings
highlight targeted unlearning as a promising defense to secure generative
models. Code and model weights can be found at
https://github.com/Mystic-Slice/Sealing-The-Backdoor .

</details>


### [190] [Interpretable Evaluation of AI-Generated Content with Language-Grounded Sparse Encoders](https://arxiv.org/abs/2508.18236)
*Yiming Tang,Arash Lagzian,Srinivas Anumasa,Qiran Zou,Trang Nguyen,Ehsan Adeli,Ching-Yu Cheng,Yilun Du,Dianbo Liu*

Main category: cs.CV

TL;DR: LanSE是一种新型架构，通过识别可解释的视觉模式并用自然语言描述，为AI生成内容提供细粒度评估。


<details>
  <summary>Details</summary>
Motivation: 当前评估指标仅提供粗粒度评估，无法识别具体优缺点，限制了生成模型的理解和商业部署。

Method: LanSE结合大规模人类评估和多模态模型分析，检测可解释的视觉模式，并提供四个关键维度的细粒度评估框架。

Result: LanSE在自然图像中检测视觉模式的准确率超过93%，揭示了现有指标无法捕捉的模型差异。

Conclusion: LanSE为生成AI模型的用户提供了强大的工具，支持模型选择、质量控制和改进，增强公众对AI生成内容的信心和安全性。

Abstract: While the quality of AI-generated contents, such as synthetic images, has
become remarkably high, current evaluation metrics provide only coarse-grained
assessments, failing to identify specific strengths and weaknesses that
researchers and practitioners need for model selection and development, further
limiting the scientific understanding and commercial deployment of these
generative models. To address this, we introduce Language-Grounded Sparse
Encoders (LanSE), a novel architecture that creates interpretable evaluation
metrics by identifying interpretable visual patterns and automatically
describing them in natural language. Through large-scale human evaluation (more
than 11,000 annotations) and large multimodal model (LMM) based analysis, LanSE
demonstrates reliable capabilities to detect interpretable visual patterns in
synthetic images with more than 93\% accuracy in natural images. LanSE further
provides a fine-grained evaluation framework that quantifies four key
dimensions of generation quality, prompt match, visual realism, physical
plausibility, and content diversity. LanSE reveals nuanced model differences
invisible to existing metrics, for instance, FLUX's superior physical
plausibility and SDXL-medium's strong content diversity, while aligning with
human judgments. By bridging interpretability with practical evaluation needs,
LanSE offers all users of generative AI models a powerful tool for model
selection, quality control of synthetic content, and model improvement. These
capabilities directly address the need for public confidence and safety in
AI-generated content, both critical for the future of generative AI
applications.

</details>


### [191] [PriorFormer: A Transformer for Real-time Monocular 3D Human Pose Estimation with Versatile Geometric Priors](https://arxiv.org/abs/2508.18238)
*Mohamed Adjel,Vincent Bonnet*

Main category: cs.CV

TL;DR: 提出了一种轻量级Transformer模型，将2D关节位置序列映射到3D姿态，适用于单摄像头场景，支持校准和非校准设置。


<details>
  <summary>Details</summary>
Motivation: 解决在单摄像头场景下，从2D关节位置序列到3D姿态的高效、轻量级映射问题，并适应不同部署环境。

Method: 使用几何先验（如段长度和相机内参）作为输入，通过掩码机制处理缺失先验，训练时使用AMASS数据集生成的合成2D数据。

Result: 模型在GPU上运行时间为380μs，CPU上为1800μs，3D关节中心位置估计误差低至36mm，优于现有技术。

Conclusion: 该模型在性能和计算效率上均优于专家模型，适用于嵌入式平台和低功耗设备。

Abstract: This paper proposes a new lightweight Transformer-based lifter that maps
short sequences of human 2D joint positions to 3D poses using a single camera.
The proposed model takes as input geometric priors including segment lengths
and camera intrinsics and is designed to operate in both calibrated and
uncalibrated settings. To this end, a masking mechanism enables the model to
ignore missing priors during training and inference. This yields a single
versatile network that can adapt to different deployment scenarios, from fully
calibrated lab environments to in-the-wild monocular videos without
calibration. The model was trained using 3D keypoints from AMASS dataset with
corresponding 2D synthetic data generated by sampling random camera poses and
intrinsics. It was then compared to an expert model trained, only on complete
priors, and the validation was done by conducting an ablation study. Results
show that both, camera and segment length priors, improve performance and that
the versatile model outperforms the expert, even when all priors are available,
and maintains high accuracy when priors are missing. Overall the average 3D
joint center positions estimation accuracy was as low as 36mm improving state
of the art by half a centimeter and at a much lower computational cost. Indeed,
the proposed model runs in 380$\mu$s on GPU and 1800$\mu$s on CPU, making it
suitable for deployment on embedded platforms and low-power devices.

</details>


### [192] [GSVisLoc: Generalizable Visual Localization for Gaussian Splatting Scene Representations](https://arxiv.org/abs/2508.18242)
*Fadi Khatib,Dror Moran,Guy Trostianetsky,Yoni Kasten,Meirav Galun,Ronen Basri*

Main category: cs.CV

TL;DR: GSVisLoc是一种基于3D高斯溅射（3DGS）场景表示的视觉定位方法，通过匹配场景特征和图像特征来估计相机位姿。


<details>
  <summary>Details</summary>
Motivation: 旨在利用3DGS场景表示进行视觉定位，无需修改、重新训练或额外参考图像。

Method: 分三步：粗匹配、细匹配和位姿细化，利用3D高斯特征和图像特征进行匹配。

Result: 在室内外场景中表现优异，优于现有3DGS基线方法，且能泛化到新场景。

Conclusion: GSVisLoc是一种高效且通用的视觉定位方法，适用于3DGS场景表示。

Abstract: We introduce GSVisLoc, a visual localization method designed for 3D Gaussian
Splatting (3DGS) scene representations. Given a 3DGS model of a scene and a
query image, our goal is to estimate the camera's position and orientation. We
accomplish this by robustly matching scene features to image features. Scene
features are produced by downsampling and encoding the 3D Gaussians while image
features are obtained by encoding image patches. Our algorithm proceeds in
three steps, starting with coarse matching, then fine matching, and finally by
applying pose refinement for an accurate final estimate. Importantly, our
method leverages the explicit 3DGS scene representation for visual localization
without requiring modifications, retraining, or additional reference images. We
evaluate GSVisLoc on both indoor and outdoor scenes, demonstrating competitive
localization performance on standard benchmarks while outperforming existing
3DGS-based baselines. Moreover, our approach generalizes effectively to novel
scenes without additional training.

</details>


### [193] [MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs](https://arxiv.org/abs/2508.18264)
*Sixun Dong,Juhua Hu,Mian Zhang,Ming Yin,Yanjie Fu,Qi Qian*

Main category: cs.CV

TL;DR: 提出了一种基于覆盖准则的多模态信息融合方法MMTok，用于优化视觉语言模型中的视觉令牌选择，显著提升推理效率并保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅利用单模态信息（视觉或文本）进行令牌剪枝，忽略了多模态任务的固有特性，且缺乏通用准则。

Method: 将子集选择问题建模为最大覆盖问题，优化视觉令牌子集以同时覆盖文本令牌和原始视觉令牌，并利用VLM代理提升文本令牌质量。

Result: 在基准数据集上验证了多模态信息的互补性，MMTok在POPE数据集上实现1.87倍加速且保持98.7%性能，仅用4个视觉令牌仍保留87.7%性能。

Conclusion: 覆盖准则在令牌选择中有效，多模态信息融合显著优于单模态基线。

Abstract: Vision-Language Models (VLMs) demonstrate impressive performance in
understanding visual content with language instruction by converting visual
input to vision tokens. However, redundancy in vision tokens results in the
degenerated inference efficiency of VLMs. While many algorithms have been
proposed to reduce the number of vision tokens, most of them apply only
unimodal information (i.e., vision/text) for pruning and ignore the inherent
multimodal property of vision-language tasks. Moreover, it lacks a generic
criterion that can be applied to different modalities. To mitigate this
limitation, in this work, we propose to leverage both vision and text tokens to
select informative vision tokens by the criterion of coverage. We first
formulate the subset selection problem as a maximum coverage problem.
Afterward, a subset of vision tokens is optimized to cover the text tokens and
the original set of vision tokens, simultaneously. Finally, a VLM agent can be
adopted to further improve the quality of text tokens for guiding vision
pruning. The proposed method MMTok is extensively evaluated on benchmark
datasets with different VLMs. The comparison illustrates that vision and text
information are complementary, and combining multimodal information can surpass
the unimodal baseline with a clear margin. Moreover, under the maximum coverage
criterion on the POPE dataset, our method achieves a 1.87x speedup while
maintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,
with only four vision tokens, it still preserves 87.7% of the original
performance on LLaVA-1.5-7B. These results highlight the effectiveness of
coverage in token selection.

</details>


### [194] [InternVL3.5: Advancing Open-Source Multimodal Models in Versatility, Reasoning, and Efficiency](https://arxiv.org/abs/2508.18265)
*Weiyun Wang,Zhangwei Gao,Lixin Gu,Hengjun Pu,Long Cui,Xingguang Wei,Zhaoyang Liu,Linglin Jing,Shenglong Ye,Jie Shao,Zhaokai Wang,Zhe Chen,Hongjie Zhang,Ganlin Yang,Haomin Wang,Qi Wei,Jinhui Yin,Wenhao Li,Erfei Cui,Guanzhou Chen,Zichen Ding,Changyao Tian,Zhenyu Wu,Jingjing Xie,Zehao Li,Bowen Yang,Yuchen Duan,Xuehui Wang,Songze Li,Xiangyu Zhao,Haodong Duan,Nianchen Deng,Bin Fu,Yinan He,Yi Wang,Conghui He,Botian Shi,Junjun He,Yingtong Xiong,Han Lv,Lijun Wu,Wenqi Shao,Kaipeng Zhang,Huipeng Deng,Biqing Qi,Jiaye Ge,Qipeng Guo,Wenwei Zhang,Wanli Ouyang,Limin Wang,Min Dou,Xizhou Zhu,Tong Lu,Dahua Lin,Jifeng Dai,Bowen Zhou,Weijie Su,Kai Chen,Yu Qiao,Wenhai Wang,Gen Luo*

Main category: cs.CV

TL;DR: InternVL 3.5是一个开源多模态模型家族，通过Cascade RL框架和ViR技术显著提升了推理能力和效率。


<details>
  <summary>Details</summary>
Motivation: 提升多模态模型的通用性、推理能力和推理效率。

Method: 采用Cascade RL框架（离线RL和在线RL两阶段训练）和ViR技术动态调整视觉分辨率。

Result: 在推理任务上性能提升16%，推理速度加快4.05倍，支持GUI交互和代理能力。

Conclusion: InternVL 3.5在开源多模态模型中达到领先水平，缩小了与商业模型的差距。

Abstract: We introduce InternVL 3.5, a new family of open-source multimodal models that
significantly advances versatility, reasoning capability, and inference
efficiency along the InternVL series. A key innovation is the Cascade
Reinforcement Learning (Cascade RL) framework, which enhances reasoning through
a two-stage process: offline RL for stable convergence and online RL for
refined alignment. This coarse-to-fine training strategy leads to substantial
improvements on downstream reasoning tasks, e.g., MMMU and MathVista. To
optimize efficiency, we propose a Visual Resolution Router (ViR) that
dynamically adjusts the resolution of visual tokens without compromising
performance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)
strategy separates the vision encoder and language model across different GPUs,
effectively balancing computational load. These contributions collectively
enable InternVL3.5 to achieve up to a +16.0\% gain in overall reasoning
performance and a 4.05$\times$ inference speedup compared to its predecessor,
i.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as
GUI interaction and embodied agency. Notably, our largest model, i.e.,
InternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs
across general multimodal, reasoning, text, and agentic tasks -- narrowing the
performance gap with leading commercial models like GPT-5. All models and code
are publicly released.

</details>


### [195] [ObjFiller-3D: Consistent Multi-view 3D Inpainting via Video Diffusion Models](https://arxiv.org/abs/2508.18271)
*Haitang Feng,Jie Liu,Jie Tang,Gangshan Wu,Beiqi Chen,Jianhuang Lai,Guangcong Wang*

Main category: cs.CV

TL;DR: ObjFiller-3D是一种新颖的3D修复方法，通过利用视频编辑模型解决多视角2D修复中的不一致性问题，实现了高质量和一致的3D对象完成。


<details>
  <summary>Details</summary>
Motivation: 多视角2D修复在3D修复中存在不一致性问题，导致模糊纹理和视觉伪影，影响3D对象的准确性和真实性。

Method: ObjFiller-3D采用视频修复模型填补3D对象的掩码区域，并引入基于参考的3D修复方法以提升重建质量。

Result: 实验表明，ObjFiller-3D在PSNR和LPIPS指标上优于现有方法（PSNR 26.6 vs. 15.9，LPIPS 0.19 vs. 0.25）。

Conclusion: ObjFiller-3D在3D编辑应用中具有实际部署潜力，能够生成更忠实和精细的重建结果。

Abstract: 3D inpainting often relies on multi-view 2D image inpainting, where the
inherent inconsistencies across different inpainted views can result in blurred
textures, spatial discontinuities, and distracting visual artifacts. These
inconsistencies pose significant challenges when striving for accurate and
realistic 3D object completion, particularly in applications that demand high
fidelity and structural coherence. To overcome these limitations, we propose
ObjFiller-3D, a novel method designed for the completion and editing of
high-quality and consistent 3D objects. Instead of employing a conventional 2D
image inpainting model, our approach leverages a curated selection of
state-of-the-art video editing model to fill in the masked regions of 3D
objects. We analyze the representation gap between 3D and videos, and propose
an adaptation of a video inpainting model for 3D scene inpainting. In addition,
we introduce a reference-based 3D inpainting method to further enhance the
quality of reconstruction. Experiments across diverse datasets show that
compared to previous methods, ObjFiller-3D produces more faithful and
fine-grained reconstructions (PSNR of 26.6 vs. NeRFiller (15.9) and LPIPS of
0.19 vs. Instant3dit (0.25)). Moreover, it demonstrates strong potential for
practical deployment in real-world 3D editing applications. Project page:
https://objfiller3d.github.io/ Code:
https://github.com/objfiller3d/ObjFiller-3D .

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [196] [VGGSounder: Audio-Visual Evaluations for Foundation Models](https://arxiv.org/abs/2508.08237)
*Daniil Zverev,Thaddäus Wiedemer,Ameya Prabhu,Matthias Bethge,Wieland Brendel,A. Sophia Koepke*

Main category: cs.MM

TL;DR: VGGSounder是一个重新标注的多标签测试集，用于更准确地评估音频-视觉基础模型。


<details>
  <summary>Details</summary>
Motivation: VGGSound数据集在评估音频-视觉分类时存在标注不完整、类别重叠和模态不对齐等问题，导致评估失真。

Method: 通过重新标注VGGSound数据集，引入VGGSounder，提供详细的模态注释和新的模态混淆度量。

Result: VGGSounder能够更精确地分析模态特定性能，并揭示模型在多模态输入时的性能下降。

Conclusion: VGGSounder解决了VGGSound的局限性，为音频-视觉基础模型的评估提供了更可靠的基准。

Abstract: The emergence of audio-visual foundation models underscores the importance of
reliably assessing their multi-modal understanding. The VGGSound dataset is
commonly used as a benchmark for evaluation audio-visual classification.
However, our analysis identifies several limitations of VGGSound, including
incomplete labelling, partially overlapping classes, and misaligned modalities.
These lead to distorted evaluations of auditory and visual capabilities. To
address these limitations, we introduce VGGSounder, a comprehensively
re-annotated, multi-label test set that extends VGGSound and is specifically
designed to evaluate audio-visual foundation models. VGGSounder features
detailed modality annotations, enabling precise analyses of modality-specific
performance. Furthermore, we reveal model limitations by analysing performance
degradation when adding another input modality with our new modality confusion
metric.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [197] [Neural Proteomics Fields for Super-resolved Spatial Proteomics Prediction](https://arxiv.org/abs/2508.17389)
*Bokai Zhao,Weiyang Shi,Hanqing Chao,Zijiang Yang,Yiyang Zhang,Ming Song,Tianzi Jiang*

Main category: q-bio.QM

TL;DR: 本文提出了一种用于测序空间蛋白质组学（seq-SP）空间超分辨率任务的新型深度学习模型——神经蛋白质组学场（NPF），并建立了开源基准数据集Pseudo-Visium SP。


<details>
  <summary>Details</summary>
Motivation: 当前测序技术的空间分辨率较低，且蛋白质表达的跨组织变异性影响了现有分子数据预测方法的性能。

Method: NPF将seq-SP建模为连续空间中的蛋白质重建问题，通过为每个组织训练专用网络，包含学习组织特异性蛋白质空间分布的空间建模模块和提取组织形态特征的形态建模模块。

Result: 实验结果表明，NPF以较少的可学习参数实现了最先进的性能。

Conclusion: NPF具有推动空间蛋白质组学研究的潜力，代码和数据集已开源。

Abstract: Spatial proteomics maps protein distributions in tissues, providing
transformative insights for life sciences. However, current sequencing-based
technologies suffer from low spatial resolution, and substantial inter-tissue
variability in protein expression further compromises the performance of
existing molecular data prediction methods. In this work, we introduce the
novel task of spatial super-resolution for sequencing-based spatial proteomics
(seq-SP) and, to the best of our knowledge, propose the first deep learning
model for this task--Neural Proteomics Fields (NPF). NPF formulates seq-SP as a
protein reconstruction problem in continuous space by training a dedicated
network for each tissue. The model comprises a Spatial Modeling Module, which
learns tissue-specific protein spatial distributions, and a Morphology Modeling
Module, which extracts tissue-specific morphological features. Furthermore, to
facilitate rigorous evaluation, we establish an open-source benchmark dataset,
Pseudo-Visium SP, for this task. Experimental results demonstrate that NPF
achieves state-of-the-art performance with fewer learnable parameters,
underscoring its potential for advancing spatial proteomics research. Our code
and dataset are publicly available at https://github.com/Bokai-Zhao/NPF.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [198] [A Laplace diffusion-based transformer model for heart rate forecasting within daily activity context](https://arxiv.org/abs/2508.16655)
*Andrei Mateescu,Ioana Hadarau,Ionut Anghel,Tudor Cioara,Ovidiu Anchidin,Ancuta Nemes*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer和Laplace扩散技术的模型，用于基于患者活动数据建模心率波动，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 远程心率监测中，心率波动常受活动影响，但现有模型很少整合活动数据，导致评估困难。

Method: 采用Transformer模型和Laplace扩散技术，通过专用嵌入和注意力机制，将活动数据作为建模核心。

Result: 模型在真实数据集上验证，平均绝对误差降低43%，R2为0.97，预测与实际心率高度一致。

Conclusion: 该模型为远程患者监测提供了实用有效的工具，支持医疗决策。

Abstract: With the advent of wearable Internet of Things (IoT) devices, remote patient
monitoring (RPM) emerged as a promising solution for managing heart failure.
However, the heart rate can fluctuate significantly due to various factors, and
without correlating it to the patient's actual physical activity, it becomes
difficult to assess whether changes are significant. Although Artificial
Intelligence (AI) models may enhance the accuracy and contextual understanding
of remote heart rate monitoring, the integration of activity data is still
rarely addressed. In this paper, we propose a Transformer model combined with a
Laplace diffusion technique to model heart rate fluctuations driven by physical
activity of the patient. Unlike prior models that treat activity as secondary,
our approach conditions the entire modeling process on activity context using
specialized embeddings and attention mechanisms to prioritize activity specific
historical patents. The model captures both long-term patterns and
activity-specific heart rate dynamics by incorporating contextualized
embeddings and dedicated encoder. The Transformer model was validated on a
real-world dataset collected from 29 patients over a 4-month period.
Experimental results show that our model outperforms current state-of-the-art
methods, achieving a 43% reduction in mean absolute error compared to the
considered baseline models. Moreover, the coefficient of determination R2 is
0.97 indicating the model predicted heart rate is in strong agreement with
actual heart rate values. These findings suggest that the proposed model is a
practical and effective tool for supporting both healthcare providers and
remote patient monitoring systems.

</details>


### [199] [Hyperbolic Multimodal Representation Learning for Biological Taxonomies](https://arxiv.org/abs/2508.16744)
*ZeMing Gong,Chuanqi Tang,Xiaoliang Huo,Nicholas Pellegrino,Austin T. Wang,Graham W. Taylor,Angel X. Chang,Scott C. Lowe,Joakim Bruslund Haurum*

Main category: cs.LG

TL;DR: 论文研究了双曲网络是否能为生物多样性研究中的层次分类提供更好的嵌入空间，通过对比和新颖的堆叠蕴含目标将多模态输入嵌入共享双曲空间。


<details>
  <summary>Details</summary>
Motivation: 生物多样性研究中的分类需要将生物样本组织成基于多模态证据（如图像和遗传信息）的结构化层次结构，双曲网络可能为此提供更好的嵌入空间。

Method: 使用对比和堆叠蕴含目标将多模态输入嵌入共享双曲空间。

Result: 在BIOSCAN-1M数据集上，双曲嵌入与欧几里得基线性能相当，并在使用DNA条形码对未见物种分类时优于其他模型。

Conclusion: 双曲嵌入为生物多样性建模提供了结构感知基础，但在细粒度分类和开放世界泛化方面仍有挑战，适用于物种发现、生态监测和保护工作。

Abstract: Taxonomic classification in biodiversity research involves organizing
biological specimens into structured hierarchies based on evidence, which can
come from multiple modalities such as images and genetic information. We
investigate whether hyperbolic networks can provide a better embedding space
for such hierarchical models. Our method embeds multimodal inputs into a shared
hyperbolic space using contrastive and a novel stacked entailment-based
objective. Experiments on the BIOSCAN-1M dataset show that hyperbolic embedding
achieves competitive performance with Euclidean baselines, and outperforms all
other models on unseen species classification using DNA barcodes. However,
fine-grained classification and open-world generalization remain challenging.
Our framework offers a structure-aware foundation for biodiversity modelling,
with potential applications to species discovery, ecological monitoring, and
conservation efforts.

</details>


### [200] [UM3: Unsupervised Map to Map Matching](https://arxiv.org/abs/2508.16874)
*Chaolong Ying,Yinan Zhang,Lei Zhang,Jiazhuang Wang,Shujun Jia,Tianshu Yu*

Main category: cs.LG

TL;DR: 提出了一种无监督的图框架，用于解决地图匹配中的挑战，包括无训练数据需求、伪坐标引入和自适应平衡机制，实现了高精度和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 地图匹配任务因缺乏真实对应关系、稀疏节点特征和可扩展性需求而具有挑战性，需要一种无需训练数据且能处理大规模数据的方法。

Method: 采用无监督学习框架，引入伪坐标增强特征区分性，设计自适应平衡机制和几何一致损失函数，并开发基于分块的并行后处理流程。

Result: 在真实数据集上实现了最先进的匹配精度，尤其在噪声高和大规模场景中显著优于现有方法。

Conclusion: 该框架为地图对齐提供了可扩展且实用的解决方案，是一种鲁棒且高效的替代传统方法的选择。

Abstract: Map-to-map matching is a critical task for aligning spatial data across
heterogeneous sources, yet it remains challenging due to the lack of ground
truth correspondences, sparse node features, and scalability demands. In this
paper, we propose an unsupervised graph-based framework that addresses these
challenges through three key innovations. First, our method is an unsupervised
learning approach that requires no training data, which is crucial for
large-scale map data where obtaining labeled training samples is challenging.
Second, we introduce pseudo coordinates that capture the relative spatial
layout of nodes within each map, which enhances feature discriminability and
enables scale-invariant learning. Third, we design an mechanism to adaptively
balance feature and geometric similarity, as well as a geometric-consistent
loss function, ensuring robustness to noisy or incomplete coordinate data. At
the implementation level, to handle large-scale maps, we develop a tile-based
post-processing pipeline with overlapping regions and majority voting, which
enables parallel processing while preserving boundary coherence. Experiments on
real-world datasets demonstrate that our method achieves state-of-the-art
accuracy in matching tasks, surpassing existing methods by a large margin,
particularly in high-noise and large-scale scenarios. Our framework provides a
scalable and practical solution for map alignment, offering a robust and
efficient alternative to traditional approaches.

</details>


### [201] [Disentangling Polysemantic Neurons with a Null-Calibrated Polysemanticity Index and Causal Patch Interventions](https://arxiv.org/abs/2508.16950)
*Manan Gupta,Dhruv Kumar*

Main category: cs.LG

TL;DR: 提出了Polysemanticity Index (PSI)指标，用于量化神经元的多义性，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 神经网络中存在多义性神经元，增加了机制解释的复杂性，需要一种量化方法。

Method: 提出PSI指标，结合几何聚类质量、标签类别对齐和开放词汇语义区分度，并在ResNet-50上进行实验。

Result: PSI能有效识别多义性神经元，并发现深层神经元的多义性显著高于浅层。

Conclusion: PSI为发现和量化神经网络中的多义性单元提供了实用工具。

Abstract: Neural networks often contain polysemantic neurons that respond to multiple,
sometimes unrelated, features, complicating mechanistic interpretability. We
introduce the Polysemanticity Index (PSI), a null-calibrated metric that
quantifies when a neuron's top activations decompose into semantically distinct
clusters. PSI multiplies three independently calibrated components: geometric
cluster quality (S), alignment to labeled categories (Q), and open-vocabulary
semantic distinctness via CLIP (D). On a pretrained ResNet-50 evaluated with
Tiny-ImageNet images, PSI identifies neurons whose activation sets split into
coherent, nameable prototypes, and reveals strong depth trends: later layers
exhibit substantially higher PSI than earlier layers. We validate our approach
with robustness checks (varying hyperparameters, random seeds, and
cross-encoder text heads), breadth analyses (comparing class-only vs.
open-vocabulary concepts), and causal patch-swap interventions. In particular,
aligned patch replacements increase target-neuron activation significantly more
than non-aligned, random, shuffled-position, or ablate-elsewhere controls. PSI
thus offers a principled and practical lever for discovering, quantifying, and
studying polysemantic units in neural networks.

</details>


### [202] [SACA: Selective Attention-Based Clustering Algorithm](https://arxiv.org/abs/2508.17150)
*Meysam Shirdel Bilehsavar,Razieh Ghaedi,Samira Seyed Taheri,Xinqi Fan,Christian O'Reilly*

Main category: cs.LG

TL;DR: 提出了一种基于选择性注意力的新型密度聚类方法，减少对用户定义参数的依赖，简化参数调整过程。


<details>
  <summary>Details</summary>
Motivation: 传统密度聚类方法（如DBSCAN）依赖用户定义参数，优化困难，需要领域专业知识。

Method: 算法初始无需用户参数，必要时引入单一整数参数；通过阈值过滤稀疏点和离群点，形成初步聚类后重新整合。

Result: 实验表明该方法易于使用且性能稳健。

Conclusion: 该方法为密度聚类任务提供了有效替代方案。

Abstract: Clustering algorithms are widely used in various applications, with
density-based methods such as Density-Based Spatial Clustering of Applications
with Noise (DBSCAN) being particularly prominent. These algorithms identify
clusters in high-density regions while treating sparser areas as noise.
However, reliance on user-defined parameters often poses optimization
challenges that require domain expertise. This paper presents a novel
density-based clustering method inspired by the concept of selective attention,
which minimizes the need for user-defined parameters under standard conditions.
Initially, the algorithm operates without requiring user-defined parameters. If
parameter adjustment is needed, the method simplifies the process by
introducing a single integer parameter that is straightforward to tune. The
approach computes a threshold to filter out the most sparsely distributed
points and outliers, forms a preliminary cluster structure, and then
reintegrates the excluded points to finalize the results. Experimental
evaluations on diverse data sets highlight the accessibility and robust
performance of the method, providing an effective alternative for density-based
clustering tasks.

</details>


### [203] [Curvature Learning for Generalization of Hyperbolic Neural Networks](https://arxiv.org/abs/2508.17232)
*Xiaomeng Fan,Yuwei Wu,Zhi Gao,Mehrtash Harandi,Yunde Jia*

Main category: cs.LG

TL;DR: 本文通过推导HNNs的PAC-Bayesian泛化边界，提出了一种基于锐度感知的曲率学习方法，以平滑损失景观并提升HNNs的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 曲率在优化HNNs中起关键作用，但缺乏理论支持其对HNNs影响的研究。

Method: 提出锐度感知曲率学习方法，通过双层次优化最小化曲率锐度，并引入隐式微分算法高效求解。

Result: 实验表明，该方法在分类、长尾数据学习、噪声数据学习和少样本学习中均能提升HNNs性能。

Conclusion: 通过理论分析和实验验证，本文方法有效提升了HNNs的泛化能力。

Abstract: Hyperbolic neural networks (HNNs) have demonstrated notable efficacy in
representing real-world data with hierarchical structures via exploiting the
geometric properties of hyperbolic spaces characterized by negative curvatures.
Curvature plays a crucial role in optimizing HNNs. Inappropriate curvatures may
cause HNNs to converge to suboptimal parameters, degrading overall performance.
So far, the theoretical foundation of the effect of curvatures on HNNs has not
been developed. In this paper, we derive a PAC-Bayesian generalization bound of
HNNs, highlighting the role of curvatures in the generalization of HNNs via
their effect on the smoothness of the loss landscape. Driven by the derived
bound, we propose a sharpness-aware curvature learning method to smooth the
loss landscape, thereby improving the generalization of HNNs. In our method,
  we design a scope sharpness measure for curvatures, which is minimized
through a bi-level optimization process. Then, we introduce an implicit
differentiation algorithm that efficiently solves the bi-level optimization by
approximating gradients of curvatures. We present the approximation error and
convergence analyses of the proposed method, showing that the approximation
error is upper-bounded, and the proposed method can converge by bounding
gradients of HNNs. Experiments on four settings: classification, learning from
long-tailed data, learning from noisy data, and few-shot learning show that our
method can improve the performance of HNNs.

</details>


### [204] [ShaLa: Multimodal Shared Latent Space Modelling](https://arxiv.org/abs/2508.17376)
*Jiali Cui,Yan-Ying Chen,Yanxia Zhang,Matthew Klenk*

Main category: cs.LG

TL;DR: ShaLa提出了一种新的生成框架，通过学习多模态数据的共享潜在表示，解决了多模态VAE在联合变分后验设计和合成质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法过于关注模态特定细节，可能掩盖跨模态的高级语义概念；多模态VAE虽能捕获共享表示，但在表达性和合成质量上存在不足。

Method: ShaLa通过集成新型架构推理模型和第二阶段扩散先验，有效推断共享潜在表示并提升多模态合成质量。

Result: ShaLa在多个基准测试中表现优异，合成质量和一致性优于现有方法，并能扩展到更多模态。

Conclusion: ShaLa在多模态共享表示学习和合成任务中表现出色，解决了现有方法的局限性。

Abstract: This paper presents a novel generative framework for learning shared latent
representations across multimodal data. Many advanced multimodal methods focus
on capturing all combinations of modality-specific details across inputs, which
can inadvertently obscure the high-level semantic concepts that are shared
across modalities. Notably, Multimodal VAEs with low-dimensional latent
variables are designed to capture shared representations, enabling various
tasks such as joint multimodal synthesis and cross-modal inference. However,
multimodal VAEs often struggle to design expressive joint variational
posteriors and suffer from low-quality synthesis. In this work, ShaLa addresses
these challenges by integrating a novel architectural inference model and a
second-stage expressive diffusion prior, which not only facilitates effective
inference of shared latent representation but also significantly improves the
quality of downstream multimodal synthesis. We validate ShaLa extensively
across multiple benchmarks, demonstrating superior coherence and synthesis
quality compared to state-of-the-art multimodal VAEs. Furthermore, ShaLa scales
to many more modalities while prior multimodal VAEs have fallen short in
capturing the increasing complexity of the shared latent space.

</details>


### [205] [Robustness Feature Adapter for Efficient Adversarial Training](https://arxiv.org/abs/2508.17680)
*Quanwei Wu,Jun Guo,Wei Wang,Yi Wang*

Main category: cs.LG

TL;DR: 提出了一种基于适配器的方法，用于在特征空间中高效进行对抗训练，解决了计算开销大和鲁棒过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 对抗训练在大型骨干模型中计算开销大且存在鲁棒过拟合问题，需要更高效且可靠的解决方案。

Method: 采用基于适配器的方法，直接在特征空间中进行对抗训练，提升收敛质量。

Result: 该方法显著提高了计算效率，消除了鲁棒过拟合，并增强了对未见攻击的鲁棒性。

Conclusion: 适配器方法在不同骨干架构和大规模对抗训练中均表现出有效性，为构建更可信的基础模型提供了新思路。

Abstract: Adversarial training (AT) with projected gradient descent is the most popular
method to improve model robustness under adversarial attacks. However,
computational overheads become prohibitively large when AT is applied to large
backbone models. AT is also known to have the issue of robust overfitting. This
paper contributes to solving both problems simultaneously towards building more
trustworthy foundation models. In particular, we propose a new adapter-based
approach for efficient AT directly in the feature space. We show that the
proposed adapter-based approach can improve the inner-loop convergence quality
by eliminating robust overfitting. As a result, it significantly increases
computational efficiency and improves model accuracy by generalizing
adversarial robustness to unseen attacks. We demonstrate the effectiveness of
the new adapter-based approach in different backbone architectures and in AT at
scale.

</details>


### [206] [Learning to Detect Label Errors by Making Them: A Method for Segmentation and Object Detection Datasets](https://arxiv.org/abs/2508.17930)
*Sarina Penquitt,Tobias Riedlinger,Timo Heller,Markus Reischl,Matthias Rottmann*

Main category: cs.LG

TL;DR: 提出了一种统一的方法来检测目标检测、语义分割和实例分割数据集中的标签错误，通过注入标签错误并将其视为实例分割问题。


<details>
  <summary>Details</summary>
Motivation: 监督学习任务中标签错误的影响包括降低模型性能、基准结果偏差和整体准确性下降，现有方法多针对单一任务且非学习型。

Method: 通过注入不同类型的标签错误，将标签错误检测视为基于复合输入的实例分割问题。

Result: 在模拟标签错误和真实标签错误上验证了方法的性能，并发布了Cityscapes数据集中的459个真实标签错误。

Conclusion: 该方法在多任务、数据集和基础模型上表现优异，为标签错误检测提供了新的基准。

Abstract: Recently, detection of label errors and improvement of label quality in
datasets for supervised learning tasks has become an increasingly important
goal in both research and industry. The consequences of incorrectly annotated
data include reduced model performance, biased benchmark results, and lower
overall accuracy. Current state-of-the-art label error detection methods often
focus on a single computer vision task and, consequently, a specific type of
dataset, containing, for example, either bounding boxes or pixel-wise
annotations. Furthermore, previous methods are not learning-based. In this
work, we overcome this research gap. We present a unified method for detecting
label errors in object detection, semantic segmentation, and instance
segmentation datasets. In a nutshell, our approach - learning to detect label
errors by making them - works as follows: we inject different kinds of label
errors into the ground truth. Then, the detection of label errors, across all
mentioned primary tasks, is framed as an instance segmentation problem based on
a composite input. In our experiments, we compare the label error detection
performance of our method with various baselines and state-of-the-art
approaches of each task's domain on simulated label errors across multiple
tasks, datasets, and base models. This is complemented by a generalization
study on real-world label errors. Additionally, we release 459 real label
errors identified in the Cityscapes dataset and provide a benchmark for real
label error detection in Cityscapes.

</details>


### [207] [Generative Feature Imputing - A Technique for Error-resilient Semantic Communication](https://arxiv.org/abs/2508.17957)
*Jianhao Huang,Qunsong Zeng,Hongyang Du,Kaibin Huang*

Main category: cs.LG

TL;DR: 本文提出了一种名为生成特征填补的新框架，通过空间错误集中分组策略、生成特征填补方法和语义感知功率分配方案，解决了语义通信在数字系统中的鲁棒性问题。


<details>
  <summary>Details</summary>
Motivation: 语义通信（SemCom）在6G网络中具有高效通信潜力，但在数字系统中部署时面临传输错误导致语义内容失真的挑战。

Method: 1. 空间错误集中分组策略；2. 基于扩散模型的生成特征填补方法；3. 语义感知功率分配方案。

Result: 实验表明，该框架在块衰落条件下优于DJSCC和JPEG2000，实现了更高的语义准确性和更低的LPIPS分数。

Conclusion: 提出的生成特征填补框架有效提升了语义通信的鲁棒性和效率。

Abstract: Semantic communication (SemCom) has emerged as a promising paradigm for
achieving unprecedented communication efficiency in sixth-generation (6G)
networks by leveraging artificial intelligence (AI) to extract and transmit the
underlying meanings of source data. However, deploying SemCom over digital
systems presents new challenges, particularly in ensuring robustness against
transmission errors that may distort semantically critical content. To address
this issue, this paper proposes a novel framework, termed generative feature
imputing, which comprises three key techniques. First, we introduce a spatial
error concentration packetization strategy that spatially concentrates feature
distortions by encoding feature elements based on their channel mappings, a
property crucial for both the effectiveness and reduced complexity of the
subsequent techniques. Second, building on this strategy, we propose a
generative feature imputing method that utilizes a diffusion model to
efficiently reconstruct missing features caused by packet losses. Finally, we
develop a semantic-aware power allocation scheme that enables unequal error
protection by allocating transmission power according to the semantic
importance of each packet. Experimental results demonstrate that the proposed
framework outperforms conventional approaches, such as Deep Joint
Source-Channel Coding (DJSCC) and JPEG2000, under block fading conditions,
achieving higher semantic accuracy and lower Learned Perceptual Image Patch
Similarity (LPIPS) scores.

</details>


### [208] [Topology Aware Neural Interpolation of Scalar Fields](https://arxiv.org/abs/2508.17995)
*Mohamed Kissi,Keanu Sisouk,Joshua A. Levine,Julien Tierny*

Main category: cs.LG

TL;DR: 提出了一种基于神经网络的拓扑感知时间变化标量场插值方法，通过关键帧和非关键帧的持久图，利用拓扑损失提升插值质量。


<details>
  <summary>Details</summary>
Motivation: 解决时间变化标量场中非关键帧数据的缺失问题，通过插值方法填补缺失数据。

Method: 使用神经网络架构学习时间值与标量场的关系，并通过拓扑损失优化几何和拓扑重建。

Result: 实验表明，该方法在2D和3D时间变化数据集上的插值效果优于现有方法。

Conclusion: 该方法能够快速生成高质量的插值结果，并在数据和拓扑拟合方面表现优异。

Abstract: This paper presents a neural scheme for the topology-aware interpolation of
time-varying scalar fields. Given a time-varying sequence of persistence
diagrams, along with a sparse temporal sampling of the corresponding scalar
fields, denoted as keyframes, our interpolation approach aims at "inverting"
the non-keyframe diagrams to produce plausible estimations of the
corresponding, missing data. For this, we rely on a neural architecture which
learns the relation from a time value to the corresponding scalar field, based
on the keyframe examples, and reliably extends this relation to the
non-keyframe time steps. We show how augmenting this architecture with specific
topological losses exploiting the input diagrams both improves the geometrical
and topological reconstruction of the non-keyframe time steps. At query time,
given an input time value for which an interpolation is desired, our approach
instantaneously produces an output, via a single propagation of the time input
through the network. Experiments interpolating 2D and 3D time-varying datasets
show our approach superiority, both in terms of data and topological fitting,
with regard to reference interpolation schemes.

</details>


### [209] [AQ-PCDSys: An Adaptive Quantized Planetary Crater Detection System for Autonomous Space Exploration](https://arxiv.org/abs/2508.18025)
*Aditri Paul,Archan Paul*

Main category: cs.LG

TL;DR: AQ-PCDSys是一种自适应量化行星陨石坑检测系统，专为资源受限的行星探索任务设计，结合量化神经网络和多传感器融合，实现高效、准确的实时检测。


<details>
  <summary>Details</summary>
Motivation: 行星探索任务需要实时、准确的环境感知，但资源受限的计算硬件限制了深度学习模型的部署。

Method: 采用量化神经网络（QNN）和自适应多传感器融合（AMF）模块，结合多尺度检测头，优化模型大小和推理延迟。

Result: AQ-PCDSys在保持高精度的同时，显著优化了模型性能，适用于实时行星陨石坑检测。

Conclusion: AQ-PCDSys为下一代自主行星着陆、导航和科学探索提供了高效、可靠的解决方案。

Abstract: Autonomous planetary exploration missions are critically dependent on
real-time, accurate environmental perception for navigation and hazard
avoidance. However, deploying deep learning models on the resource-constrained
computational hardware of planetary exploration platforms remains a significant
challenge. This paper introduces the Adaptive Quantized Planetary Crater
Detection System (AQ-PCDSys), a novel framework specifically engineered for
real-time, onboard deployment in the computationally constrained environments
of space exploration missions. AQ-PCDSys synergistically integrates a Quantized
Neural Network (QNN) architecture, trained using Quantization-Aware Training
(QAT), with an Adaptive Multi-Sensor Fusion (AMF) module. The QNN architecture
significantly optimizes model size and inference latency suitable for real-time
onboard deployment in space exploration missions, while preserving high
accuracy. The AMF module intelligently fuses data from Optical Imagery (OI) and
Digital Elevation Models (DEMs) at the feature level, utilizing an Adaptive
Weighting Mechanism (AWM) to dynamically prioritize the most relevant and
reliable sensor modality based on planetary ambient conditions. This approach
enhances detection robustness across diverse planetary landscapes. Paired with
Multi-Scale Detection Heads specifically designed for robust and efficient
detection of craters across a wide range of sizes, AQ-PCDSys provides a
computationally efficient, reliable and accurate solution for planetary crater
detection, a critical capability for enabling the next generation of autonomous
planetary landing, navigation, and scientific exploration.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [210] [MDD: A Dataset for Text-and-Music Conditioned Duet Dance Generation](https://arxiv.org/abs/2508.16911)
*Prerit Gupta,Jason Alexander Fotso-Puepi,Zhengyuan Li,Jay Mehta,Aniket Bera*

Main category: cs.GR

TL;DR: MDD是一个多模态基准数据集，用于文本控制和音乐条件下的3D双人舞动作生成，包含高质量动作捕捉数据和详细文本描述。


<details>
  <summary>Details</summary>
Motivation: 现有数据集缺乏对双人舞动作、音乐和文本的整合，MDD填补了这一空白。

Method: MDD包含620分钟专业舞者的动作捕捉数据，同步音乐和10K+细粒度文本描述，支持两种新任务：文本到双人舞和文本到舞蹈伴奏。

Result: MDD是首个整合双人舞动作、音乐和文本的数据集，并提供了基线评估。

Conclusion: MDD为双人舞生成研究提供了重要资源，支持未来研究。

Abstract: We introduce Multimodal DuetDance (MDD), a diverse multimodal benchmark
dataset designed for text-controlled and music-conditioned 3D duet dance motion
generation. Our dataset comprises 620 minutes of high-quality motion capture
data performed by professional dancers, synchronized with music, and detailed
with over 10K fine-grained natural language descriptions. The annotations
capture a rich movement vocabulary, detailing spatial relationships, body
movements, and rhythm, making MDD the first dataset to seamlessly integrate
human motions, music, and text for duet dance generation. We introduce two
novel tasks supported by our dataset: (1) Text-to-Duet, where given music and a
textual prompt, both the leader and follower dance motion are generated (2)
Text-to-Dance Accompaniment, where given music, textual prompt, and the
leader's motion, the follower's motion is generated in a cohesive, text-aligned
manner. We include baseline evaluations on both tasks to support future
research.

</details>


### [211] [A Survey of Deep Learning-based Point Cloud Denoising](https://arxiv.org/abs/2508.17011)
*Jinxi Wang,Ben Fei,Dasith de Silva Edirimuni,Zheng Liu,Ying He,Xuequan Lu*

Main category: cs.GR

TL;DR: 本文综述了截至2025年8月的基于深度学习的点云去噪方法，从监督水平和建模角度分类，并分析了架构趋势和性能。


<details>
  <summary>Details</summary>
Motivation: 点云去噪在计算机图形学、自动驾驶等领域至关重要，但传统方法难以处理复杂噪声，深度学习显示出优势。

Method: 从监督水平（监督与无监督）和建模角度对文献进行分类，提出功能分类法，并建立统一基准进行评估。

Result: 分析了去噪质量、表面保真度、点分布和计算效率，总结了当前方法的性能。

Conclusion: 讨论了开放挑战和未来研究方向，强调了这一领域的快速发展。

Abstract: Accurate 3D geometry acquisition is essential for a wide range of
applications, such as computer graphics, autonomous driving, robotics, and
augmented reality. However, raw point clouds acquired in real-world
environments are often corrupted with noise due to various factors such as
sensor, lighting, material, environment etc, which reduces geometric fidelity
and degrades downstream performance. Point cloud denoising is a fundamental
problem, aiming to recover clean point sets while preserving underlying
structures. Classical optimization-based methods, guided by hand-crafted
filters or geometric priors, have been extensively studied but struggle to
handle diverse and complex noise patterns. Recent deep learning approaches
leverage neural network architectures to learn distinctive representations and
demonstrate strong outcomes, particularly on complex and large-scale point
clouds. Provided these significant advances, this survey provides a
comprehensive and up-to-date review of deep learning-based point cloud
denoising methods up to August 2025. We organize the literature from two
perspectives: (1) supervision level (supervised vs. unsupervised), and (2)
modeling perspective, proposing a functional taxonomy that unifies diverse
approaches by their denoising principles. We further analyze architectural
trends both structurally and chronologically, establish a unified benchmark
with consistent training settings, and evaluate methods in terms of denoising
quality, surface fidelity, point distribution, and computational efficiency.
Finally, we discuss open challenges and outline directions for future research
in this rapidly evolving field.

</details>


### [212] [DanceEditor: Towards Iterative Editable Music-driven Dance Generation with Open-Vocabulary Descriptions](https://arxiv.org/abs/2508.17342)
*Hengyuan Zhang,Zhe Li,Xingqun Qi,Mengze Li,Muyi Sun,Man Zhang,Sirui Han*

Main category: cs.GR

TL;DR: 论文提出了一种可编辑的舞蹈生成框架DanceEditor，并构建了大规模数据集DanceRemix，支持用户通过文本描述迭代编辑舞蹈动作。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然支持直接生成舞蹈，但缺乏对用户编辑需求的支持，且缺乏高质量的可编辑舞蹈数据集。

Method: 采用预测-编辑范式，结合音乐和文本提示，通过Cross-modality Editing Module（CEM）实现舞蹈动作的迭代编辑。

Result: 实验表明，DanceEditor在新数据集DanceRemix上优于现有方法。

Conclusion: DanceEditor能够生成与音乐和文本描述一致的可编辑舞蹈，为实际编舞场景提供了实用工具。

Abstract: Generating coherent and diverse human dances from music signals has gained
tremendous progress in animating virtual avatars. While existing methods
support direct dance synthesis, they fail to recognize that enabling users to
edit dance movements is far more practical in real-world choreography
scenarios. Moreover, the lack of high-quality dance datasets incorporating
iterative editing also limits addressing this challenge. To achieve this goal,
we first construct DanceRemix, a large-scale multi-turn editable dance dataset
comprising the prompt featuring over 25.3M dance frames and 84.5K pairs. In
addition, we propose a novel framework for iterative and editable dance
generation coherently aligned with given music signals, namely DanceEditor.
Considering the dance motion should be both musical rhythmic and enable
iterative editing by user descriptions, our framework is built upon a
prediction-then-editing paradigm unifying multi-modal conditions. At the
initial prediction stage, our framework improves the authority of generated
results by directly modeling dance movements from tailored, aligned music.
Moreover, at the subsequent iterative editing stages, we incorporate text
descriptions as conditioning information to draw the editable results through a
specifically designed Cross-modality Editing Module (CEM). Specifically, CEM
adaptively integrates the initial prediction with music and text prompts as
temporal motion cues to guide the synthesized sequences. Thereby, the results
display music harmonics while preserving fine-grained semantic alignment with
text descriptions. Extensive experiments demonstrate that our method
outperforms the state-of-the-art models on our newly collected DanceRemix
dataset. Code is available at https://lzvsdy.github.io/DanceEditor/.

</details>


### [213] [MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian Splatting](https://arxiv.org/abs/2508.17811)
*Hanzhi Chang,Ruijie Zhu,Wenjie Chang,Mulin Yu,Yanzhe Liang,Jiahao Lu,Zhuoyuan Li,Tianzhu Zhang*

Main category: cs.GR

TL;DR: MeshSplat是一个通过高斯散射实现稀疏视图表面重建的通用框架，利用2DGS连接新视图合成与几何先验，无需直接3D监督。


<details>
  <summary>Details</summary>
Motivation: 现有表面重建方法在输入视图极度稀疏时难以恢复准确几何，MeshSplat旨在解决这一问题。

Method: 通过前馈网络预测每视图像素对齐的2DGS，结合加权Chamfer距离损失和法线预测网络优化位置与方向。

Result: 实验表明MeshSplat在稀疏视图网格重建任务中达到最先进性能。

Conclusion: MeshSplat通过2DGS和几何先验的有效结合，成功提升了稀疏视图下的表面重建精度。

Abstract: Surface reconstruction has been widely studied in computer vision and
graphics. However, existing surface reconstruction works struggle to recover
accurate scene geometry when the input views are extremely sparse. To address
this issue, we propose MeshSplat, a generalizable sparse-view surface
reconstruction framework via Gaussian Splatting. Our key idea is to leverage
2DGS as a bridge, which connects novel view synthesis to learned geometric
priors and then transfers these priors to achieve surface reconstruction.
Specifically, we incorporate a feed-forward network to predict per-view
pixel-aligned 2DGS, which enables the network to synthesize novel view images
and thus eliminates the need for direct 3D ground-truth supervision. To improve
the accuracy of 2DGS position and orientation prediction, we propose a Weighted
Chamfer Distance Loss to regularize the depth maps, especially in overlapping
areas of input views, and also a normal prediction network to align the
orientation of 2DGS with normal vectors predicted by a monocular normal
estimator. Extensive experiments validate the effectiveness of our proposed
improvement, demonstrating that our method achieves state-of-the-art
performance in generalizable sparse-view mesh reconstruction tasks. Project
Page: https://hanzhichang.github.io/meshsplat_web

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [214] [BrainPath: Generating Subject-Specific Brain Aging Trajectories](https://arxiv.org/abs/2508.16667)
*Yifan Li,Javad Sohankar,Ji Luo,Jing Li,Yi Su*

Main category: q-bio.NC

TL;DR: BrainPath是一个3D生成框架，通过学习纵向脑老化动态，从单一基线扫描预测任意时间点的解剖学忠实MRI图像，优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 量化并预测个体脑老化轨迹对理解神经退行性疾病和老化异质性至关重要，但现有方法存在局限性。

Method: BrainPath整合了年龄校准损失、交换学习策略和年龄感知损失，以保留细微的生物有意义变化。

Result: 在ADNI和NACC数据集上，BrainPath在结构相似性、均方误差、峰值信噪比和MRI年龄差异准确性方面优于现有模型。

Conclusion: BrainPath为脑老化的精准建模提供了基础，支持神经退行性疾病和老化干预研究。

Abstract: Quantifying and forecasting individual brain aging trajectories is critical
for understanding neurodegenerative disease and the heterogeneity of aging, yet
current approaches remain limited. Most models predict chronological age, an
imperfect surrogate for biological aging, or generate synthetic MRIs that
enhance data diversity but fail to capture subject-specific trajectories. Here,
we present BrainPath, a 3D generative framework that learns longitudinal brain
aging dynamics during training and, at inference, predicts anatomically
faithful MRIs at arbitrary timepoints from a single baseline scan. BrainPath
integrates an age calibration loss, a swap learning strategy, and an age
perceptual loss to preserve subtle, biologically meaningful variations. Across
held-out ADNI and an independent NACC dataset, BrainPath outperforms
state-of-the-art reference models in structural similarity (SSIM), mean squared
error (MSE), peak signal-to-noise ratio (PSNR), and MRI age-difference
accuracy, while capturing realistic and temporally consistent aging patterns.
Beyond methodological innovation, BrainPath enables personalized mapping of
brain aging, synthetic follow-up scan prediction, and trajectory-based
analyses, providing a foundation for precision modeling of brain aging and
supporting research into neurodegeneration and aging interventions.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [215] [3D latent diffusion models for parameterizing and history matching multiscenario facies systems](https://arxiv.org/abs/2508.16621)
*Guido Di Federico,Louis J. Durlofsky*

Main category: physics.geo-ph

TL;DR: 论文提出了一种基于生成潜在扩散模型（LDM）的地质参数化方法，用于3D河道-堤坝-泥浆系统，显著减少了历史匹配中的变量数量，并保持了地质真实性。


<details>
  <summary>Details</summary>
Motivation: 地质参数化在历史匹配中非常有用，因为它可以减少需要校准的变量数量，并自动保持地质真实性。本文旨在开发一种基于LDM的参数化方法，以应对3D地质模型中的不确定性。

Method: 使用生成潜在扩散模型（LDM）对3D河道-堤坝-泥浆系统进行参数化，包括泥浆比例、河道方向和宽度等变量。训练中加入感知损失项以提高地质真实性。

Result: 新生成的地质模型在视觉和统计上与参考模型高度一致，流动响应分布也接近。该方法在历史匹配中成功减少了生产预测和地质参数的不确定性。

Conclusion: 基于LDM的参数化方法能够有效处理地质场景不确定性，生成的地质模型与真实模型一致，适用于历史匹配。

Abstract: Geological parameterization procedures entail the mapping of a
high-dimensional geomodel to a low-dimensional latent variable. These
parameterizations can be very useful for history matching because the number of
variables to be calibrated is greatly reduced, and the mapping can be
constructed such that geological realism is automatically preserved. In this
work, a parameterization method based on generative latent diffusion models
(LDMs) is developed for 3D channel-levee-mud systems. Geomodels with variable
scenario parameters, specifically mud fraction, channel orientation, and
channel width, are considered. A perceptual loss term is included during
training to improve geological realism. For any set of scenario parameters, an
(essentially) infinite number of realizations can be generated, so our LDM
parameterizes over a very wide model space. New realizations constructed using
the LDM procedure are shown to closely resemble reference geomodels, both
visually and in terms of one- and two-point spatial statistics. Flow response
distributions, for a specified set of injection and production wells, are also
shown to be in close agreement between the two sets of models. The
parameterization method is applied for ensemble-based history matching, with
model updates performed in the LDM latent space, for cases involving geological
scenario uncertainty. For three synthetic true models corresponding to
different geological scenarios, we observe clear uncertainty reduction in both
production forecasts and geological scenario parameters. The overall method is
additionally shown to provide posterior geomodels consistent with the synthetic
true model in each case.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [216] [Citizen Centered Climate Intelligence: Operationalizing Open Tree Data for Urban Cooling and Eco-Routing in Indian Cities](https://arxiv.org/abs/2508.17648)
*Kaushik Ravi,Andreas Brück*

Main category: cs.CY

TL;DR: 提出了一种基于公民参与的、可扩展的城市气候韧性框架，通过智能手机测量、卫星数据分析和生态路由引擎，实现数据驱动的城市规划。


<details>
  <summary>Details</summary>
Motivation: 解决城市气候韧性需求，强调公民参与和数据共享的重要性，以应对生态不平等和数据集中化问题。

Method: 框架包含三个模块：智能手机测量工具（AI增强）、卫星数据模型（计算冷却效果）、生态路由引擎（基于环境质量评分）。

Result: 在印度浦那应用，形成闭环反馈系统，公民生成数据并受益于个性化干预，提升环境公平性。

Conclusion: 该框架将开放数据转化为共享治理平台，为公民驱动的城市智能提供了可复制模型。

Abstract: Urban climate resilience requires more than high-resolution data; it demands
systems that embed data collection, interpretation, and action within the daily
lives of citizens. This chapter presents a scalable, citizen-centric framework
that reimagines environmental infrastructure through participatory sensing,
open analytics, and prescriptive urban planning tools. Applied in Pune, India,
the framework comprises three interlinked modules: (1) a smartphone-based
measurement toolkit enhanced by AI segmentation to extract tree height, canopy
diameter, and trunk girth; (2) a percentile-based model using satellite-derived
Land Surface Temperature to calculate localized cooling through two new
metrics, Cooling Efficacy and Ambient Heat Relief; and (3) an eco-routing
engine that guides mobility using a Static Environmental Quality score, based
on tree density, species diversity, and cumulative carbon sequestration.
Together, these modules form a closed feedback loop where citizens generate
actionable data and benefit from personalized, sustainable interventions. This
framework transforms open data from a passive repository into an active
platform for shared governance and environmental equity. In the face of growing
ecological inequality and data centralization, this chapter presents a
replicable model for citizen-driven urban intelligence, reframing planning as a
co-produced, climate-resilient, and radically local practice.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [217] [WebSight: A Vision-First Architecture for Robust Web Agents](https://arxiv.org/abs/2508.16987)
*Tanvir Bhathal,Asanshay Gupta*

Main category: cs.AI

TL;DR: WebSight是一种基于视觉的自主网络代理，通过纯视觉感知与网页环境交互，无需依赖HTML或DOM输入。其核心是WebSight-7B模型，在WebVoyager基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 减少对HTML或DOM输入的依赖，提升视觉导航的鲁棒性和效率。

Method: 采用多智能体架构，包括规划、推理、视觉动作和验证智能体，通过情景记忆机制协调。

Result: WebSight-7B在Showdown Clicks基准测试中达到58.84%的top-1准确率，WebSight在WebVoyager基准测试中成功率为68.0%，正确率97.14%。

Conclusion: WebSight和WebSight-7B为视觉网页导航设立了新的标准，具有可解释性、鲁棒性和高效性。

Abstract: We introduce WebSight, a vision-based autonomous web agent, designed to
interact with web environments purely through visual perception, eliminating
dependence on HTML or DOM-based inputs. Central to our approach we introduce
our new model, WebSight-7B, a fine-tuned vision-language model optimized for UI
element interaction, trained using LoRA on a web-focused subset of the
Wave-UI-25K dataset. WebSight integrates this model into a modular multi-agent
architecture, comprising planning, reasoning, vision-action, and verification
agents, coordinated through an episodic memory mechanism.
  WebSight-7B achieves a top-1 accuracy of 58.84% on the Showdown Clicks
benchmark, outperforming several larger generalist models while maintaining
lower latency. The full WebSight agent achieves a 68.0% success rate on the
WebVoyager benchmark, surpassing systems from labs such as OpenAI (61.0%) and
HCompany (Runner H, 67.0%). Among tasks completed, WebSight answers correctly
97.14% of the time, indicating high precision. Together, WebSight and
WebSight-7B establish a new standard for interpretable, robust, and efficient
visual web navigation.

</details>


### [218] [MaRVL-QA: A Benchmark for Mathematical Reasoning over Visual Landscapes](https://arxiv.org/abs/2508.17180)
*Nilay Pande,Sahiti Yerramilli,Jayant Sravan Tamarapalli,Rynaa Grover*

Main category: cs.AI

TL;DR: MaRVL-QA是一个新的基准测试，用于评估多模态大语言模型（MLLMs）在数学和空间推理方面的能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 推动MLLMs在数学和空间推理方面的能力，超越其现有的语义描述能力。

Method: 通过数学曲面图设计MaRVL-QA基准测试，包含拓扑计数和变换识别两个任务。

Result: 即使最先进的MLLMs在MaRVL-QA上也表现不佳，倾向于使用表面启发式方法而非深度推理。

Conclusion: MaRVL-QA为研究社区提供了一个挑战性工具，用于衡量进展并指导MLLMs的发展。

Abstract: A key frontier for Multimodal Large Language Models (MLLMs) is the ability to
perform deep mathematical and spatial reasoning directly from images, moving
beyond their established success in semantic description. Mathematical surface
plots provide a rigorous testbed for this capability, as they isolate the task
of reasoning from the semantic noise common in natural images. To measure
progress on this frontier, we introduce MaRVL-QA (Mathematical Reasoning over
Visual Landscapes), a new benchmark designed to quantitatively evaluate these
core reasoning skills. The benchmark comprises two novel tasks: Topological
Counting, identifying and enumerating features like local maxima; and
Transformation Recognition, recognizing applied geometric transformations.
Generated from a curated library of functions with rigorous ambiguity
filtering, our evaluation on MaRVL-QA reveals that even state-of-the-art MLLMs
struggle significantly, often resorting to superficial heuristics instead of
robust spatial reasoning. MaRVL-QA provides a challenging new tool for the
research community to measure progress, expose model limitations, and guide the
development of MLLMs with more profound reasoning abilities.

</details>


### [219] [SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models](https://arxiv.org/abs/2508.18179)
*Zhenwei Tang,Difan Jiao,Blair Yang,Ashton Anderson*

Main category: cs.AI

TL;DR: SEAM是一个评估视觉语言模型（VLMs）跨模态推理一致性的基准，通过语义等效的输入对比较文本符号和视觉空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在模态比较中常因任务差异和信息不对称而受限，SEAM旨在提供一个严格且语义等效的评估框架。

Method: SEAM利用四种领域的标准化文本和视觉符号系统，避免OCR依赖，对比21种VLMs的跨模态推理能力。

Result: 发现视觉模态普遍落后于语言模态，跨模态一致性较低，主要错误源于文本感知失败和视觉幻觉。

Conclusion: SEAM为改进模态无关推理提供了可控的评估环境，结果对视觉变换具有鲁棒性。

Abstract: Evaluating whether vision-language models (VLMs) reason consistently across
representations is challenging because modality comparisons are typically
confounded by task differences and asymmetric information. We introduce SEAM, a
benchmark that pairs semantically equivalent inputs across four domains that
have existing standardized textual and visual notations. By employing distinct
notation systems across modalities, in contrast to OCR-based image-text
pairing, SEAM provides a rigorous comparative assessment of the
textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21
contemporary models, we observe systematic modality imbalance: vision
frequently lags language in overall performance, despite the problems
containing semantically equivalent information, and cross-modal agreement is
relatively low. Our error analysis reveals two main drivers: textual perception
failures from tokenization in domain notation and visual perception failures
that induce hallucinations. We also show that our results are largely robust to
visual transformations. SEAM establishes a controlled, semantically equivalent
setting for measuring and improving modality-agnostic reasoning.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [220] [HunyuanVideo-Foley: Multimodal Diffusion with Representation Alignment for High-Fidelity Foley Audio Generation](https://arxiv.org/abs/2508.16930)
*Sizhe Shan,Qiulin Li,Yutao Cui,Miles Yang,Yuehai Wang,Qun Yang,Jin Zhou,Zhao Zhong*

Main category: eess.AS

TL;DR: HunyuanVideo-Foley是一个端到端的文本-视频-音频生成框架，通过创新的数据管道、表示对齐策略和多模态扩散变换器，解决了视频到音频生成中的关键挑战，实现了高保真音频与视觉动态的精确对齐。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成方法缺乏同步音频，影响了沉浸感。本文旨在解决视频到音频生成中的数据稀缺、模态不平衡和音频质量有限等问题。

Method: 提出HunyuanVideo-Foley框架，包含三个核心创新：1) 自动化标注的100k小时多模态数据集；2) 自监督音频特征表示对齐策略；3) 多模态扩散变换器，通过联合注意力和跨注意力实现音频-视频融合与文本语义注入。

Result: 综合评估表明，HunyuanVideo-Foley在音频保真度、视觉-语义对齐、时间对齐和分布匹配方面达到了新的最先进性能。

Conclusion: HunyuanVideo-Foley通过创新的多模态方法，显著提升了视频到音频生成的质量和同步性，为沉浸式体验提供了重要支持。

Abstract: Recent advances in video generation produce visually realistic content, yet
the absence of synchronized audio severely compromises immersion. To address
key challenges in video-to-audio generation, including multimodal data
scarcity, modality imbalance and limited audio quality in existing methods, we
propose HunyuanVideo-Foley, an end-to-end text-video-to-audio framework that
synthesizes high-fidelity audio precisely aligned with visual dynamics and
semantic context. Our approach incorporates three core innovations: (1) a
scalable data pipeline curating 100k-hour multimodal datasets through automated
annotation; (2) a representation alignment strategy using self-supervised audio
features to guide latent diffusion training, efficiently improving audio
quality and generation stability; (3) a novel multimodal diffusion transformer
resolving modal competition, containing dual-stream audio-video fusion through
joint attention, and textual semantic injection via cross-attention.
Comprehensive evaluations demonstrate that HunyuanVideo-Foley achieves new
state-of-the-art performance across audio fidelity, visual-semantic alignment,
temporal alignment and distribution matching. The demo page is available at:
https://szczesnys.github.io/hunyuanvideo-foley/.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [221] [Optimizing Grasping in Legged Robots: A Deep Learning Approach to Loco-Manipulation](https://arxiv.org/abs/2508.17466)
*Dilermando Almeida,Guilherme Lazzarini,Juliano Negri,Thiago H. Segreto,Ricardo V. Godoy,Marcelo Becker*

Main category: cs.RO

TL;DR: 提出了一种深度学习框架，通过模拟训练提升四足机器人抓取能力，减少对真实数据收集的依赖。


<details>
  <summary>Details</summary>
Motivation: 四足机器人结合机械臂在复杂环境中执行任务时，精确抓取仍面临挑战，需大量校准和预编程。

Method: 采用模拟到现实的方法，在Genesis环境中生成合成数据集，训练U-Net架构CNN模型处理多模态输入。

Result: 系统成功完成自主导航、感知、预测最优抓取位姿并执行精确抓取的全流程任务。

Conclusion: 模拟训练结合先进传感为物体处理提供了可扩展且有效的解决方案。

Abstract: Quadruped robots have emerged as highly efficient and versatile platforms,
excelling in navigating complex and unstructured terrains where traditional
wheeled robots might fail. Equipping these robots with manipulator arms unlocks
the advanced capability of loco-manipulation to perform complex physical
interaction tasks in areas ranging from industrial automation to
search-and-rescue missions. However, achieving precise and adaptable grasping
in such dynamic scenarios remains a significant challenge, often hindered by
the need for extensive real-world calibration and pre-programmed grasp
configurations. This paper introduces a deep learning framework designed to
enhance the grasping capabilities of quadrupeds equipped with arms, focusing on
improved precision and adaptability. Our approach centers on a sim-to-real
methodology that minimizes reliance on physical data collection. We developed a
pipeline within the Genesis simulation environment to generate a synthetic
dataset of grasp attempts on common objects. By simulating thousands of
interactions from various perspectives, we created pixel-wise annotated
grasp-quality maps to serve as the ground truth for our model. This dataset was
used to train a custom CNN with a U-Net-like architecture that processes
multi-modal input from an onboard RGB and depth cameras, including RGB images,
depth maps, segmentation masks, and surface normal maps. The trained model
outputs a grasp-quality heatmap to identify the optimal grasp point. We
validated the complete framework on a four-legged robot. The system
successfully executed a full loco-manipulation task: autonomously navigating to
a target object, perceiving it with its sensors, predicting the optimal grasp
pose using our model, and performing a precise grasp. This work proves that
leveraging simulated training with advanced sensing offers a scalable and
effective solution for object handling.

</details>


### [222] [GWM: Towards Scalable Gaussian World Models for Robotic Manipulation](https://arxiv.org/abs/2508.17600)
*Guanxing Lu,Baoxiong Jia,Puhao Li,Yixin Chen,Ziwei Wang,Yansong Tang,Siyuan Huang*

Main category: cs.RO

TL;DR: 提出了一种名为高斯世界模型（GWM）的新方法，用于机器人操作，通过高斯原语推断未来状态，结合扩散变换器和3D变分自编码器，提升视觉表示和策略训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有基于图像的世界模型缺乏对三维世界的几何信息理解，限制了机器人策略的训练效果。

Method: 使用高斯原语和扩散变换器（DiT）结合3D变分自编码器，通过高斯泼溅技术重建未来状态。

Result: GWM能精确预测未来场景，并显著提升策略训练效果，优于现有方法。

Conclusion: GWM展示了3D世界模型在数据扩展和机器人策略训练中的潜力。

Abstract: Training robot policies within a learned world model is trending due to the
inefficiency of real-world interactions. The established image-based world
models and policies have shown prior success, but lack robust geometric
information that requires consistent spatial and physical understanding of the
three-dimensional world, even pre-trained on internet-scale video sources. To
this end, we propose a novel branch of world model named Gaussian World Model
(GWM) for robotic manipulation, which reconstructs the future state by
inferring the propagation of Gaussian primitives under the effect of robot
actions. At its core is a latent Diffusion Transformer (DiT) combined with a 3D
variational autoencoder, enabling fine-grained scene-level future state
reconstruction with Gaussian Splatting. GWM can not only enhance the visual
representation for imitation learning agent by self-supervised future
prediction training, but can serve as a neural simulator that supports
model-based reinforcement learning. Both simulated and real-world experiments
depict that GWM can precisely predict future scenes conditioned on diverse
robot actions, and can be further utilized to train policies that outperform
the state-of-the-art by impressive margins, showcasing the initial data scaling
potential of 3D world model.

</details>


### [223] [SEBVS: Synthetic Event-based Visual Servoing for Robot Navigation and Manipulation](https://arxiv.org/abs/2508.17643)
*Krishna Vinod,Prithvi Jai Ramesh,Pavan Kumar B N,Bharatesh Chakravarthi*

Main category: cs.RO

TL;DR: 论文提出了一种开源ROS包v2e，用于Gazebo模拟中从RGB摄像头生成事件流，并研究了基于事件的机器人策略（ERP）在实时导航和操作中的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管事件相机在实时机器人感知中具有优势，但主流机器人模拟器中缺乏合成事件视觉的模拟设置，阻碍了事件驱动方法在机器人任务中的评估。

Method: 开发了v2e ROS包，用于从RGB摄像头生成事件流，并训练了基于Transformer的ERP策略，通过行为克隆与RGB策略进行对比。

Result: 实验结果表明，事件引导的策略在各种操作条件下均表现出竞争优势。

Conclusion: 事件驱动感知在实时机器人导航和操作中具有潜力，为事件相机在机器人策略学习中的更广泛应用奠定了基础。

Abstract: Event cameras offer microsecond latency, high dynamic range, and low power
consumption, making them ideal for real-time robotic perception under
challenging conditions such as motion blur, occlusion, and illumination
changes. However, despite their advantages, synthetic event-based vision
remains largely unexplored in mainstream robotics simulators. This lack of
simulation setup hinders the evaluation of event-driven approaches for robotic
manipulation and navigation tasks. This work presents an open-source,
user-friendly v2e robotics operating system (ROS) package for Gazebo simulation
that enables seamless event stream generation from RGB camera feeds. The
package is used to investigate event-based robotic policies (ERP) for real-time
navigation and manipulation. Two representative scenarios are evaluated: (1)
object following with a mobile robot and (2) object detection and grasping with
a robotic manipulator. Transformer-based ERPs are trained by behavior cloning
and compared to RGB-based counterparts under various operating conditions.
Experimental results show that event-guided policies consistently deliver
competitive advantages. The results highlight the potential of event-driven
perception to improve real-time robotic navigation and manipulation, providing
a foundation for broader integration of event cameras into robotic policy
learning. The GitHub repo for the dataset and code:
https://eventbasedvision.github.io/SEBVS/

</details>


### [224] [Egocentric Instruction-oriented Affordance Prediction via Large Multimodal Model](https://arxiv.org/abs/2508.17922)
*Bokai Ji,Jie Gu,Xiaokang Ma,Chu Tang,Jingmin Chen,Guangxia Li*

Main category: cs.RO

TL;DR: 论文提出任务/指令依赖的affordance概念，并构建了一个包含1.5万个对象-指令-affordance三元组的数据集，同时通过“搜索验证”流程利用大模型预测affordance。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了affordance的任务/指令依赖性，即同一对象在不同指令下可能有不同的操作区域和方向。

Method: 构建新数据集，并设计“搜索验证”流程，利用大模型迭代预测和验证affordance。

Result: 实验表明，该方法不仅实现了指令导向的affordance预测，还表现出广泛优异的性能。

Conclusion: 任务/指令依赖的affordance概念和“搜索验证”流程为智能机器人操作提供了新思路。

Abstract: Affordance is crucial for intelligent robots in the context of object
manipulation. In this paper, we argue that affordance should be
task-/instruction-dependent, which is overlooked by many previous works. That
is, different instructions can lead to different manipulation regions and
directions even for the same object. According to this observation, we present
a new dataset comprising fifteen thousand object-instruction-affordance
triplets. All scenes in the dataset are from an egocentric viewpoint, designed
to approximate the perspective of a human-like robot. Furthermore, we
investigate how to enable large multimodal models (LMMs) to serve as affordance
predictors by implementing a ``search against verifiers'' pipeline. An LMM is
asked to progressively predict affordances, with the output at each step being
verified by itself during the iterative process, imitating a reasoning process.
Experiments show that our method not only unlocks new instruction-oriented
affordance prediction capabilities, but also achieves outstanding performance
broadly.

</details>


### [225] [A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm](https://arxiv.org/abs/2508.17969)
*Alexandros Gkillas,Christos Anagnostopoulos,Nikos Piperigkos,Dimitris Tsiktsiris,Theofilos Christodoulou,Theofanis Siamatras,Dimitrios Triantafyllou,Christos Basdekis,Theoktisti Marinopoulou,Panagiotis Lepentsiotis,Elefterios Blitsis,Aggeliki Zacharaki,Nearchos Stylianidis,Leonidas Katelaris,Lamberto Salvan,Aris S. Lalos,Christos Laoudias,Antonios Lalas,Konstantinos Votis*

Main category: cs.RO

TL;DR: 论文提出了一种用于自动驾驶车辆内外监控的整体感知系统，通过AI驱动的自适应框架优化车辆感知和体验。


<details>
  <summary>Details</summary>
Motivation: 旨在通过内外监控系统提升自动驾驶车辆的性能和效率，优化驾驶员和乘客的体验。

Method: 内部监控采用多摄像头和大型语言模型进行行为识别和虚拟辅助；外部监控使用LiDAR进行语义分割和超分辨率处理。

Result: 实验验证表明，该架构在性能和效率上均有显著提升。

Conclusion: 提出的整体感知框架在自动驾驶车辆中表现出高效和实用性。

Abstract: This paper introduces a holistic perception system for internal and external
monitoring of autonomous vehicles, with the aim of demonstrating a novel
AI-leveraged self-adaptive framework of advanced vehicle technologies and
solutions that optimize perception and experience on-board. Internal monitoring
system relies on a multi-camera setup designed for predicting and identifying
driver and occupant behavior through facial recognition, exploiting in addition
a large language model as virtual assistant. Moreover, the in-cabin monitoring
system includes AI-empowered smart sensors that measure air-quality and perform
thermal comfort analysis for efficient on and off-boarding. On the other hand,
external monitoring system perceives the surrounding environment of vehicle,
through a LiDAR-based cost-efficient semantic segmentation approach, that
performs highly accurate and efficient super-resolution on low-quality raw 3D
point clouds. The holistic perception framework is developed in the context of
EU's Horizon Europe programm AutoTRUST, and has been integrated and deployed on
a real electric vehicle provided by ALKE. Experimental validation and
evaluation at the integration site of Joint Research Centre at Ispra, Italy,
highlights increased performance and efficiency of the modular blocks of the
proposed perception architecture.

</details>


### [226] [Scene-Agnostic Traversability Labeling and Estimation via a Multimodal Self-supervised Framework](https://arxiv.org/abs/2508.18249)
*Zipeng Fang,Yanbo Wang,Lei Zhao,Weidong Chen*

Main category: cs.RO

TL;DR: 提出了一种多模态自监督框架，用于通过整合足迹、LiDAR和相机数据来生成可通行性标签，并通过双流网络学习不同模态，显著提升了可通行性估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在捕捉不可通行区域特征方面表现不足，且多集中于单一模态，忽略了多模态数据的互补优势。

Method: 结合足迹、LiDAR和相机数据生成标签，训练双流网络，并引入稀疏LiDAR监督以减少伪标签噪声。

Result: 在多种环境中测试，自动标注方法达到约88% IoU，多模态网络比现有方法提升1.6-3.5% IoU。

Conclusion: 多模态自监督框架显著提升了可通行性估计的准确性和鲁棒性。

Abstract: Traversability estimation is critical for enabling robots to navigate across
diverse terrains and environments. While recent self-supervised learning
methods achieve promising results, they often fail to capture the
characteristics of non-traversable regions. Moreover, most prior works
concentrate on a single modality, overlooking the complementary strengths
offered by integrating heterogeneous sensory modalities for more robust
traversability estimation. To address these limitations, we propose a
multimodal self-supervised framework for traversability labeling and
estimation. First, our annotation pipeline integrates footprint, LiDAR, and
camera data as prompts for a vision foundation model, generating traversability
labels that account for both semantic and geometric cues. Then, leveraging
these labels, we train a dual-stream network that jointly learns from different
modalities in a decoupled manner, enhancing its capacity to recognize diverse
traversability patterns. In addition, we incorporate sparse LiDAR-based
supervision to mitigate the noise introduced by pseudo labels. Finally,
extensive experiments conducted across urban, off-road, and campus environments
demonstrate the effectiveness of our approach. The proposed automatic labeling
method consistently achieves around 88% IoU across diverse datasets. Compared
to existing self-supervised state-of-the-art methods, our multimodal
traversability estimation network yields consistently higher IoU, improving by
1.6-3.5% on all evaluated datasets.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [227] [Predicting User Grasp Intentions in Virtual Reality](https://arxiv.org/abs/2508.16582)
*Linghao Zeng*

Main category: cs.HC

TL;DR: 论文研究了在虚拟现实（VR）中预测用户意图的方法，比较了分类和回归模型在复杂抓取动作中的表现，发现回归模型（尤其是LSTM）表现更优。


<details>
  <summary>Details</summary>
Motivation: 预测VR中用户意图对提升沉浸感至关重要，尤其是在需要精确触觉反馈的复杂抓取任务中。

Method: 利用手部运动的时序数据，通过分类和回归方法（包括LSTM网络）对810次试验进行分析。

Result: 回归模型（LSTM）在预测抓取前两秒内的动作时表现更稳健，时间误差在0.25秒内，距离误差在5-20厘米之间。

Conclusion: 回归模型更适合处理VR中用户行为的动态复杂性，为未来实时预测和自适应触觉反馈提供了基础。

Abstract: Predicting user intentions in virtual reality (VR) is crucial for creating
immersive experiences, particularly in tasks involving complex grasping motions
where accurate haptic feedback is essential. In this work, we leverage
time-series data from hand movements to evaluate both classification and
regression approaches across 810 trials with varied object types, sizes, and
manipulations. Our findings reveal that classification models struggle to
generalize across users, leading to inconsistent performance. In contrast,
regression-based approaches, particularly those using Long Short Term Memory
(LSTM) networks, demonstrate more robust performance, with timing errors within
0.25 seconds and distance errors around 5-20 cm in the critical two-second
window before a grasp. Despite these improvements, predicting precise hand
postures remains challenging. Through a comprehensive analysis of user
variability and model interpretability, we explore why certain models fail and
how regression models better accommodate the dynamic and complex nature of user
behavior in VR. Our results underscore the potential of machine learning models
to enhance VR interactions, particularly through adaptive haptic feedback, and
lay the groundwork for future advancements in real-time prediction of user
actions in VR.

</details>


### [228] [Negative Shanshui: Real-time Interactive Ink Painting Synthesis](https://arxiv.org/abs/2508.16612)
*Aven-Le Zhou*

Main category: cs.HC

TL;DR: Negative Shanshui是一个实时交互式AI合成方法，通过重新诠释中国古典山水画来应对人类世的生态危机，结合了优化的Stable Diffusion模型和视线驱动的修复技术。


<details>
  <summary>Details</summary>
Motivation: 通过艺术与技术结合，引发人们对生态危机的关注和反思。

Method: 优化Stable Diffusion模型，结合视线驱动的修复和帧插值技术，实现动态变形动画和VR交互体验。

Result: 在艺术节中展示，观众通过共情、矛盾心理和批判性反思与作品互动。

Conclusion: Negative Shanshui展示了技术与艺术结合的潜力，成功引发公众对生态问题的思考。

Abstract: This paper presents Negative Shanshui, a real-time interactive AI synthesis
approach that reinterprets classical Chinese landscape ink painting, i.e.,
shanshui, to engage with ecological crises in the Anthropocene. Negative
Shanshui optimizes a fine-tuned Stable Diffusion model for real-time inferences
and integrates it with gaze-driven inpainting, frame interpolation; it enables
dynamic morphing animations in response to the viewer's gaze and presents as an
interactive virtual reality (VR) experience. The paper describes the complete
technical pipeline, covering the system framework, optimization strategies,
gaze-based interaction, and multimodal deployment in an art festival. Further
analysis of audience feedback collected during its public exhibition highlights
how participants variously engaged with the work through empathy, ambivalence,
and critical reflection.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [229] [Predicting brain tumour enhancement from non-contrast MR imaging with artificial intelligence](https://arxiv.org/abs/2508.16650)
*James K Ruffle,Samia Mohinta,Guilherme Pombo,Asthik Biswas,Alan Campbell,Indran Davagnanam,David Doig,Ahmed Hamman,Harpreet Hyare,Farrah Jabeen,Emma Lim,Dermot Mallon,Stephanie Owen,Sophie Wilkinson,Sebastian Brandner,Parashkev Nachev*

Main category: eess.IV

TL;DR: 开发深度学习模型，仅通过非对比MRI预测脑肿瘤增强，性能优于专家放射科医生。


<details>
  <summary>Details</summary>
Motivation: 减少对钆造影剂的依赖，适用于频繁随访、肾功能不全、过敏或儿科患者。

Method: 使用nnU-Net、SegResNet、SwinUNETR等模型，基于非对比T1、T2和T2/FLAIR加权图像训练。

Result: 最佳模型nnU-Net的平衡准确率为83%，敏感性91.5%，特异性74.4%，优于放射科医生。

Conclusion: 深度学习模型在非对比MRI中识别增强脑肿瘤具有临床意义，有望减少钆造影剂的使用。

Abstract: Brain tumour imaging assessment typically requires both pre- and
post-contrast MRI, but gadolinium administration is not always desirable, such
as in frequent follow-up, renal impairment, allergy, or paediatric patients. We
aimed to develop and validate a deep learning model capable of predicting brain
tumour contrast enhancement from non-contrast MRI sequences alone. We assembled
11089 brain MRI studies from 10 international datasets spanning adult and
paediatric populations with various neuro-oncological states, including glioma,
meningioma, metastases, and post-resection appearances. Deep learning models
(nnU-Net, SegResNet, SwinUNETR) were trained to predict and segment enhancing
tumour using only non-contrast T1-, T2-, and T2/FLAIR-weighted images.
Performance was evaluated on 1109 held-out test patients using patient-level
detection metrics and voxel-level segmentation accuracy. Model predictions were
compared against 11 expert radiologists who each reviewed 100 randomly selected
patients. The best-performing nnU-Net achieved 83% balanced accuracy, 91.5%
sensitivity, and 74.4% specificity in detecting enhancing tumour. Enhancement
volume predictions strongly correlated with ground truth (R2 0.859). The model
outperformed expert radiologists, who achieved 69.8% accuracy, 75.9%
sensitivity, and 64.7% specificity. 76.8% of test patients had Dice over 0.3
(acceptable detection), 67.5% had Dice over 0.5 (good detection), and 50.2% had
Dice over 0.7 (excellent detection). Deep learning can identify
contrast-enhancing brain tumours from non-contrast MRI with clinically relevant
performance. These models show promise as screening tools and may reduce
gadolinium dependence in neuro-oncology imaging. Future work should evaluate
clinical utility alongside radiology experts.

</details>


### [230] [Analysis of Transferability Estimation Metrics for Surgical Phase Recognition](https://arxiv.org/abs/2508.16730)
*Prabhant Singh,Yiping Li,Yasmina Al Khalil*

Main category: eess.IV

TL;DR: 论文提出了一种用于手术视频分析中预训练模型迁移性评估的方法（SITE），并比较了三种代表性指标（LogME、H-Score和TransRate）的性能。


<details>
  <summary>Details</summary>
Motivation: 在手术视频分析中，专家标注成本高昂，因此需要一种无需完整微调即可预测模型迁移性能的方法。

Method: 通过SITE方法，利用模型的嵌入或输出预测其在下游任务中的表现，并在两个数据集（RAMIE和AutoLaparo）上对三种指标进行基准测试。

Result: LogME（尤其是按子集最小分数聚合时）与微调准确性最接近；H-Score预测能力较弱；TransRate常与真实模型排名相反。

Conclusion: 研究强调了模型多样性的重要性，并提出了未来研究方向，包括领域特定指标和交互式基准工具。

Abstract: Fine-tuning pre-trained models has become a cornerstone of modern machine
learning, allowing practitioners to achieve high performance with limited
labeled data. In surgical video analysis, where expert annotations are
especially time-consuming and costly, identifying the most suitable pre-trained
model for a downstream task is both critical and challenging.
Source-independent transferability estimation (SITE) offers a solution by
predicting how well a model will fine-tune on target data using only its
embeddings or outputs, without requiring full retraining. In this work, we
formalize SITE for surgical phase recognition and provide the first
comprehensive benchmark of three representative metrics, LogME, H-Score, and
TransRate, on two diverse datasets (RAMIE and AutoLaparo). Our results show
that LogME, particularly when aggregated by the minimum per-subset score,
aligns most closely with fine-tuning accuracy; H-Score yields only weak
predictive power; and TransRate often inverses true model rankings. Ablation
studies show that when candidate models have similar performances,
transferability estimates lose discriminative power, emphasizing the importance
of maintaining model diversity or using additional validation. We conclude with
practical guidelines for model selection and outline future directions toward
domain-specific metrics, theoretical foundations, and interactive benchmarking
tools.

</details>


### [231] [Multimodal Medical Endoscopic Image Analysis via Progressive Disentangle-aware Contrastive Learning](https://arxiv.org/abs/2508.16882)
*Junhao Wu,Yun Li,Junhao Li,Jingliang Bian,Xiaomao Fan,Wenbin Lei,Ruxin Wang*

Main category: eess.IV

TL;DR: 提出了一种基于'对齐-解缠-融合'机制的多模态表示学习框架，用于提升喉咽部肿瘤分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统单模态成像方法难以捕捉喉咽部肿瘤的复杂特征，多模态方法有望提升分割性能。

Method: 采用多尺度分布对齐和渐进特征解缠策略，结合2D WLI和NBI图像对进行学习。

Result: 在多个数据集上表现优于现有方法，适用于多样化的临床场景。

Conclusion: 该方法通过多模态融合和特征解缠显著提升了肿瘤分割的准确性和鲁棒性。

Abstract: Accurate segmentation of laryngo-pharyngeal tumors is crucial for precise
diagnosis and effective treatment planning. However, traditional
single-modality imaging methods often fall short of capturing the complex
anatomical and pathological features of these tumors. In this study, we present
an innovative multi-modality representation learning framework based on the
`Align-Disentangle-Fusion' mechanism that seamlessly integrates 2D White Light
Imaging (WLI) and Narrow Band Imaging (NBI) pairs to enhance segmentation
performance. A cornerstone of our approach is multi-scale distribution
alignment, which mitigates modality discrepancies by aligning features across
multiple transformer layers. Furthermore, a progressive feature disentanglement
strategy is developed with the designed preliminary disentanglement and
disentangle-aware contrastive learning to effectively separate
modality-specific and shared features, enabling robust multimodal contrastive
learning and efficient semantic fusion. Comprehensive experiments on multiple
datasets demonstrate that our method consistently outperforms state-of-the-art
approaches, achieving superior accuracy across diverse real clinical scenarios.

</details>


### [232] [Generating Synthetic Contrast-Enhanced Chest CT Images from Non-Contrast Scans Using Slice-Consistent Brownian Bridge Diffusion Network](https://arxiv.org/abs/2508.16897)
*Pouya Shiri,Xin Yi,Neel P. Mistry,Samaneh Javadinia,Mohammad Chegini,Seok-Bum Ko,Amirali Baniasadi,Scott J. Adams*

Main category: eess.IV

TL;DR: 提出了一种基于桥扩散的方法，从非对比CT扫描合成高保真对比增强CTA图像，提升患者安全性和可及性。


<details>
  <summary>Details</summary>
Motivation: 对比剂在CT成像中可能导致肾毒性和过敏反应，因此无需对比剂生成高质量CTA图像具有重要临床意义。

Method: 采用Slice-Consistent Brownian Bridge Diffusion Model (SC-BBDM)，结合预处理流程（重采样、配准、分割掩模）和双数据集分析。

Result: 在Coltea-Lung数据集上验证，优于基线方法，能有效保留血管结构并提升对比度保真度。

Conclusion: 该方法在低内存预算下实现3D解剖完整性，为无对比剂CTA成像提供了可行解决方案。

Abstract: Contrast-enhanced computed tomography (CT) imaging is essential for
diagnosing and monitoring thoracic diseases, including aortic pathologies.
However, contrast agents pose risks such as nephrotoxicity and allergic-like
reactions. The ability to generate high-fidelity synthetic contrast-enhanced CT
angiography (CTA) images without contrast administration would be
transformative, enhancing patient safety and accessibility while reducing
healthcare costs. In this study, we propose the first bridge diffusion-based
solution for synthesizing contrast-enhanced CTA images from non-contrast CT
scans. Our approach builds on the Slice-Consistent Brownian Bridge Diffusion
Model (SC-BBDM), leveraging its ability to model complex mappings while
maintaining consistency across slices. Unlike conventional slice-wise synthesis
methods, our framework preserves full 3D anatomical integrity while operating
in a high-resolution 2D fashion, allowing seamless volumetric interpretation
under a low memory budget. To ensure robust spatial alignment, we implement a
comprehensive preprocessing pipeline that includes resampling, registration
using the Symmetric Normalization method, and a sophisticated dilated
segmentation mask to extract the aorta and surrounding structures. We create
two datasets from the Coltea-Lung dataset: one containing only the aorta and
another including both the aorta and heart, enabling a detailed analysis of
anatomical context. We compare our approach against baseline methods on both
datasets, demonstrating its effectiveness in preserving vascular structures
while enhancing contrast fidelity.

</details>


### [233] [Deep Learning Architectures for Medical Image Denoising: A Comparative Study of CNN-DAE, CADTra, and DCMIEDNet](https://arxiv.org/abs/2508.17223)
*Asadullah Bin Rahman,Masud Ibn Afjal,Md. Abdulla Al Mamun*

Main category: eess.IV

TL;DR: 该论文比较了三种深度学习架构（CNN-DAE、CADTra和DCMIEDNet）在MRI脑图像去噪中的表现，发现DCMIEDNet在低噪声水平下表现最佳，而CADTra在高噪声水平下更稳健。


<details>
  <summary>Details</summary>
Motivation: 医学成像易受噪声影响，降低诊断准确性，因此需要评估深度学习架构在去噪中的表现。

Method: 使用Figshare MRI脑数据集，在三种高斯噪声强度（σ=10、15、25）下系统评估三种模型。

Result: DCMIEDNet在低噪声水平下表现最佳，CADTra在高噪声水平下更稳健，所有深度学习模型均显著优于传统小波方法。

Conclusion: 研究为医学图像去噪提供了定量基准，并揭示了不同架构在不同噪声强度下的优势。

Abstract: Medical imaging modalities are inherently susceptible to noise contamination
that degrades diagnostic utility and clinical assessment accuracy. This paper
presents a comprehensive comparative evaluation of three state-of-the-art deep
learning architectures for MRI brain image denoising: CNN-DAE, CADTra, and
DCMIEDNet. We systematically evaluate these models across multiple Gaussian
noise intensities ($\sigma = 10, 15, 25$) using the Figshare MRI Brain Dataset.
Our experimental results demonstrate that DCMIEDNet achieves superior
performance at lower noise levels, with PSNR values of $32.921 \pm 2.350$ dB
and $30.943 \pm 2.339$ dB for $\sigma = 10$ and $15$ respectively. However,
CADTra exhibits greater robustness under severe noise conditions ($\sigma =
25$), achieving the highest PSNR of $27.671 \pm 2.091$ dB. All deep learning
approaches significantly outperform traditional wavelet-based methods, with
improvements ranging from 5-8 dB across tested conditions. This study
establishes quantitative benchmarks for medical image denoising and provides
insights into architecture-specific strengths for varying noise intensities.

</details>


### [234] [Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing](https://arxiv.org/abs/2508.17326)
*Tristan S. W. Stevens,Oisín Nolan,Ruud J. G. van Sloun*

Main category: eess.IV

TL;DR: 提出了一种基于语义引导的扩散去雾算法，用于提升超声心动图图像质量。


<details>
  <summary>Details</summary>
Motivation: 超声心动图在心脏成像中至关重要，但图像质量常因多路径混响导致的雾化而下降。

Method: 结合像素级噪声模型和语义分割，利用干净超声数据训练的生成先验引导扩散后验采样。

Result: 在挑战数据集上定量评估显示，对比度和保真度指标表现优异。

Conclusion: 该方法有效提升了超声心动图的图像质量，代码已开源。

Abstract: Echocardiography plays a central role in cardiac imaging, offering dynamic
views of the heart that are essential for diagnosis and monitoring. However,
image quality can be significantly degraded by haze arising from multipath
reverberations, particularly in difficult-to-image patients. In this work, we
propose a semantic-guided, diffusion-based dehazing algorithm developed for the
MICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method
integrates a pixel-wise noise model, derived from semantic segmentation of hazy
inputs into a diffusion posterior sampling framework guided by a generative
prior trained on clean ultrasound data. Quantitative evaluation on the
challenge dataset demonstrates strong performance across contrast and fidelity
metrics. Code for the submitted algorithm is available at
https://github.com/tristan-deep/semantic-diffusion-echo-dehazing.

</details>


### [235] [Towards Trustworthy Breast Tumor Segmentation in Ultrasound using Monte Carlo Dropout and Deep Ensembles for Epistemic Uncertainty Estimation](https://arxiv.org/abs/2508.17768)
*Toufiq Musah,Chinasa Kalaiwo,Maimoona Akram,Ubaida Napari Abdulai,Maruf Adewole,Farouk Dako,Adaobi Chiazor Emegoakor,Udunna C. Anazodo,Prince Ebenezer Adjei,Confidence Raymond*

Main category: eess.IV

TL;DR: 本文提出了一种改进的Residual Encoder U-Net用于乳腺超声图像分割，并量化不确定性，解决了数据集重复问题，并在跨域数据上验证了模型性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺超声图像分割对精确病变描绘和肿瘤特征分析至关重要，但存在固有伪影和数据集不一致的挑战。

Method: 使用改进的Residual Encoder U-Net，通过蒙特卡洛dropout和深度集成量化不确定性，并在去重后的数据集上评估性能。

Result: 在Breast-Lesion-USG数据集上达到最先进的分割精度，并提供校准的不确定性估计。跨域数据上性能下降，凸显领域偏移的挑战。

Conclusion: 集成不确定性建模对临床部署的可信度至关重要，跨域性能下降表明领域偏移问题仍需解决。

Abstract: Automated segmentation of BUS images is important for precise lesion
delineation and tumor characterization, but is challenged by inherent artifacts
and dataset inconsistencies. In this work, we evaluate the use of a modified
Residual Encoder U-Net for breast ultrasound segmentation, with a focus on
uncertainty quantification. We identify and correct for data duplication in the
BUSI dataset, and use a deduplicated subset for more reliable estimates of
generalization performance. Epistemic uncertainty is quantified using Monte
Carlo dropout, deep ensembles, and their combination. Models are benchmarked on
both in-distribution and out-of-distribution datasets to demonstrate how they
generalize to unseen cross-domain data. Our approach achieves state-of-the-art
segmentation accuracy on the Breast-Lesion-USG dataset with in-distribution
validation, and provides calibrated uncertainty estimates that effectively
signal regions of low model confidence. Performance declines and increased
uncertainty observed in out-of-distribution evaluation highlight the persistent
challenge of domain shift in medical imaging, and the importance of integrated
uncertainty modeling for trustworthy clinical deployment. \footnote{Code
available at: https://github.com/toufiqmusah/nn-uncertainty.git}

</details>


### [236] [TuningIQA: Fine-Grained Blind Image Quality Assessment for Livestreaming Camera Tuning](https://arxiv.org/abs/2508.17965)
*Xiangfei Sheng,Zhichao Duan,Xiaofeng Pan,Yipo Huang,Zhichao Yang,Pengfei Chen,Leida Li*

Main category: eess.IV

TL;DR: 提出了一种细粒度的盲图像质量评估方法TuningIQA，用于直播相机参数调优，并建立了FGLive-10K数据集。


<details>
  <summary>Details</summary>
Motivation: 现有盲图像质量评估模型仅提供粗粒度质量评分，无法满足直播相机参数调优的细粒度需求。

Method: 建立了FGLive-10K数据集，开发了TuningIQA方法，结合人类感知特征提取和基于图的相机参数融合。

Result: TuningIQA在分数回归和细粒度质量排序上显著优于现有方法，适用于直播相机调优。

Conclusion: TuningIQA为直播相机参数调优提供了有效的细粒度质量评估工具。

Abstract: Livestreaming has become increasingly prevalent in modern visual
communication, where automatic camera quality tuning is essential for
delivering superior user Quality of Experience (QoE). Such tuning requires
accurate blind image quality assessment (BIQA) to guide parameter optimization
decisions. Unfortunately, the existing BIQA models typically only predict an
overall coarse-grained quality score, which cannot provide fine-grained
perceptual guidance for precise camera parameter tuning. To bridge this gap, we
first establish FGLive-10K, a comprehensive fine-grained BIQA database
containing 10,185 high-resolution images captured under varying camera
parameter configurations across diverse livestreaming scenarios. The dataset
features 50,925 multi-attribute quality annotations and 19,234 fine-grained
pairwise preference annotations. Based on FGLive-10K, we further develop
TuningIQA, a fine-grained BIQA metric for livestreaming camera tuning, which
integrates human-aware feature extraction and graph-based camera parameter
fusion. Extensive experiments and comparisons demonstrate that TuningIQA
significantly outperforms state-of-the-art BIQA methods in both score
regression and fine-grained quality ranking, achieving superior performance
when deployed for livestreaming camera tuning.

</details>
