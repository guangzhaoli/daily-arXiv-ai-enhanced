<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 89]
- [cs.CR](#cs.CR) [Total: 3]
- [cs.AI](#cs.AI) [Total: 1]
- [cs.CL](#cs.CL) [Total: 3]
- [astro-ph.IM](#astro-ph.IM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 4]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.LG](#cs.LG) [Total: 3]
- [cs.RO](#cs.RO) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Real-Time Intuitive AI Drawing System for Collaboration: Enhancing Human Creativity through Formal and Contextual Intent Integration](https://arxiv.org/abs/2508.19254)
*Jookyung Song,Mookyoung Kang,Nojun Kwak*

Main category: cs.CV

TL;DR: 提出了一种实时生成绘图系统，结合形式意图和上下文意图，实现低延迟、多用户协作的视觉创作。


<details>
  <summary>Details</summary>
Motivation: 传统基于文本提示的生成系统主要关注高层上下文描述，而忽略了底层几何特征。本文旨在同时分析这两类意图，提升生成效果。

Method: 通过多阶段生成管道，结合轮廓保持的结构控制和风格-内容感知的图像合成，利用触摸屏界面和分布式推理架构实现。

Result: 系统支持低延迟的两阶段转换，允许多用户在共享画布上协作创作，无论艺术专长如何。

Conclusion: 该系统重新定义了人机交互为共同创作和相互增强的过程。

Abstract: This paper presents a real-time generative drawing system that interprets and
integrates both formal intent - the structural, compositional, and stylistic
attributes of a sketch - and contextual intent - the semantic and thematic
meaning inferred from its visual content - into a unified transformation
process. Unlike conventional text-prompt-based generative systems, which
primarily capture high-level contextual descriptions, our approach
simultaneously analyzes ground-level intuitive geometric features such as line
trajectories, proportions, and spatial arrangement, and high-level semantic
cues extracted via vision-language models. These dual intent signals are
jointly conditioned in a multi-stage generation pipeline that combines
contour-preserving structural control with style- and content-aware image
synthesis. Implemented with a touchscreen-based interface and distributed
inference architecture, the system achieves low-latency, two-stage
transformation while supporting multi-user collaboration on shared canvases.
The resulting platform enables participants, regardless of artistic expertise,
to engage in synchronous, co-authored visual creation, redefining human-AI
interaction as a process of co-creation and mutual enhancement.

</details>


### [2] [TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models](https://arxiv.org/abs/2508.19257)
*Chenghao Liu,Jiachen Zhang,Chengxuan Li,Zhimu Zhou,Shixin Wu,Songfang Huang,Huiling Duan*

Main category: cs.CV

TL;DR: TTF通过整合历史和当前视觉表示，提升VLA模型的推理质量，实验显示在多个任务中性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型逐帧处理视觉输入，忽略了时间信息，导致对视觉噪声敏感且忽视帧间连贯性。

Method: 提出Temporal Token Fusion (TTF)，结合灰度像素差异分析和语义相关性评估，选择性融合时间令牌。

Result: 在LIBERO、SimplerEnv和真实机器人任务中分别提升4.0、4.8%和8.7%的性能。

Conclusion: TTF模型无关且高效，揭示了注意力机制中Query矩阵重用的潜力。

Abstract: Vision-Language-Action (VLA) models process visual inputs independently at
each timestep, discarding valuable temporal information inherent in robotic
manipulation tasks. This frame-by-frame processing makes models vulnerable to
visual noise while ignoring the substantial coherence between consecutive
frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a
training-free approach that intelligently integrates historical and current
visual representations to enhance VLA inference quality. Our method employs
dual-dimension detection combining efficient grayscale pixel difference
analysis with attention-based semantic relevance assessment, enabling selective
temporal token fusion through hard fusion strategies and keyframe anchoring to
prevent error accumulation. Comprehensive experiments across LIBERO,
SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0
percentage points average on LIBERO (72.4\% vs 68.4\% baseline),
cross-environment validation on SimplerEnv (4.8\% relative improvement), and
8.7\% relative improvement on real robot tasks. Our approach proves
model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,
TTF reveals that selective Query matrix reuse in attention mechanisms enhances
rather than compromises performance, suggesting promising directions for direct
KQV matrix reuse strategies that achieve computational acceleration while
improving task success rates.

</details>


### [3] [Seeing Like a Designer Without One: A Study on Unsupervised Slide Quality Assessment via Designer Cue Augmentation](https://arxiv.org/abs/2508.19289)
*Tai Inui,Steven Oh,Magdeline Kuan*

Main category: cs.CV

TL;DR: 提出了一种结合视觉设计指标与CLIP-ViT嵌入的无监督幻灯片质量评估方法，效果优于现有视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 通过结合专家启发的视觉设计指标和多模态嵌入，更准确地评估幻灯片质量，提供可扩展的实时反馈。

Method: 使用七种视觉设计指标和CLIP-ViT嵌入，结合Isolation Forest异常评分，评估幻灯片质量。

Result: 在专业讲座幻灯片上训练，与人类视觉质量评分的Pearson相关性高达0.83，优于其他模型。

Conclusion: 结合低级设计线索和多模态嵌入能有效近似观众对幻灯片质量的感知，实现可扩展的实时评估。

Abstract: We present an unsupervised slide-quality assessment pipeline that combines
seven expert-inspired visual-design metrics (whitespace, colorfulness, edge
density, brightness contrast, text density, color harmony, layout balance) with
CLIP-ViT embeddings, using Isolation Forest-based anomaly scoring to evaluate
presentation slides. Trained on 12k professional lecture slides and evaluated
on six academic talks (115 slides), our method achieved Pearson correlations up
to 0.83 with human visual-quality ratings-1.79x to 3.23x stronger than scores
from leading vision-language models (ChatGPT o4-mini-high, ChatGPT o3, Claude
Sonnet 4, Gemini 2.5 Pro). We demonstrate convergent validity with visual
ratings, discriminant validity against speaker-delivery scores, and exploratory
alignment with overall impressions. Our results show that augmenting low-level
design cues with multimodal embeddings closely approximates audience
perceptions of slide quality, enabling scalable, objective feedback in real
time.

</details>


### [4] [Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation](https://arxiv.org/abs/2508.19290)
*Alexandros Gkillas,Ioulia Kapsali,Nikos Piperigkos,Aris S. Lalos*

Main category: cs.CV

TL;DR: 提出了一种针对2D范围视图LiDAR分割的高效对抗防御方法，通过可解释的净化网络实现强鲁棒性且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 现代LiDAR分割网络易受对抗攻击影响，现有防御方法多针对3D点云且计算量大，而2D范围视图的轻量级防御研究不足。

Method: 设计了基于数学优化问题的可解释净化网络，直接在2D范围视图域进行对抗防御。

Result: 在公开基准测试中表现优异，优于生成模型和对抗训练基线，并在实际自动驾驶场景中验证了有效性。

Conclusion: 该方法为2D范围视图LiDAR分割提供了高效且实用的对抗防御解决方案。

Abstract: LiDAR-based segmentation is essential for reliable perception in autonomous
vehicles, yet modern segmentation networks are highly susceptible to
adversarial attacks that can compromise safety. Most existing defenses are
designed for networks operating directly on raw 3D point clouds and rely on
large, computationally intensive generative models. However, many
state-of-the-art LiDAR segmentation pipelines operate on more efficient 2D
range view representations. Despite their widespread adoption, dedicated
lightweight adversarial defenses for this domain remain largely unexplored. We
introduce an efficient model-based purification framework tailored for
adversarial defense in 2D range-view LiDAR segmentation. We propose a direct
attack formulation in the range-view domain and develop an explainable
purification network based on a mathematical justified optimization problem,
achieving strong adversarial resilience with minimal computational overhead.
Our method achieves competitive performance on open benchmarks, consistently
outperforming generative and adversarial training baselines. More importantly,
real-world deployment on a demo vehicle demonstrates the framework's ability to
deliver accurate operation in practical autonomous driving scenarios.

</details>


### [5] [Object Detection with Multimodal Large Vision-Language Models: An In-depth Review](https://arxiv.org/abs/2508.19294)
*Ranjan Sapkota,Manoj Karkee*

Main category: cs.CV

TL;DR: 大型视觉语言模型（LVLMs）通过融合语言和视觉技术，提升了目标检测的适应性、上下文推理和泛化能力，未来可能超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 探讨LVLMs如何通过结合自然语言处理（NLP）和计算机视觉（CV）技术，革新目标检测领域。

Method: 通过三步研究流程，分析LVLMs的架构创新、训练范式和信息整合方法。

Result: LVLMs在多样场景中表现出色，未来有望超越传统目标检测方法。

Conclusion: LVLMs的进步将对目标检测和机器人应用产生深远影响。

Abstract: The fusion of language and vision in large vision-language models (LVLMs) has
revolutionized deep learning-based object detection by enhancing adaptability,
contextual reasoning, and generalization beyond traditional architectures. This
in-depth review presents a structured exploration of the state-of-the-art in
LVLMs, systematically organized through a three-step research review process.
First, we discuss the functioning of vision language models (VLMs) for object
detection, describing how these models harness natural language processing
(NLP) and computer vision (CV) techniques to revolutionize object detection and
localization. We then explain the architectural innovations, training
paradigms, and output flexibility of recent LVLMs for object detection,
highlighting how they achieve advanced contextual understanding for object
detection. The review thoroughly examines the approaches used in integration of
visual and textual information, demonstrating the progress made in object
detection using VLMs that facilitate more sophisticated object detection and
localization strategies. This review presents comprehensive visualizations
demonstrating LVLMs' effectiveness in diverse scenarios including localization
and segmentation, and then compares their real-time performance, adaptability,
and complexity to traditional deep learning systems. Based on the review, its
is expected that LVLMs will soon meet or surpass the performance of
conventional methods in object detection. The review also identifies a few
major limitations of the current LVLM modes, proposes solutions to address
those challenges, and presents a clear roadmap for the future advancement in
this field. We conclude, based on this study, that the recent advancement in
LVLMs have made and will continue to make a transformative impact on object
detection and robotic applications in the future.

</details>


### [6] [Large VLM-based Stylized Sports Captioning](https://arxiv.org/abs/2508.19295)
*Sauptik Dhar,Nicholas Buoncristiani,Joe Anakata,Haoyu Zhang,Michelle Munson*

Main category: cs.CV

TL;DR: 论文提出了一种两级微调的LVLM管道，用于从图像生成风格化的体育比赛描述，显著提升了F1和BERT分数，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有LLM/LVLM在生成专业体育比赛描述时缺乏领域特定的术语，无法满足生产级需求。

Method: 采用两级微调的LVLM管道，优化了生成风格化体育描述的能力。

Result: F1分数提升8-10%，BERT分数提升2-10%，运行时内存占用小且执行速度快。

Conclusion: 该方法在超级碗LIX中成功应用，证明了其在实时体育新闻中的实用性和高效性。

Abstract: The advent of large (visual) language models (LLM / LVLM) have led to a
deluge of automated human-like systems in several domains including social
media content generation, search and recommendation, healthcare prognosis, AI
assistants for cognitive tasks etc. Although these systems have been
successfully integrated in production; very little focus has been placed on
sports, particularly accurate identification and natural language description
of the game play. Most existing LLM/LVLMs can explain generic sports
activities, but lack sufficient domain-centric sports' jargon to create natural
(human-like) descriptions. This work highlights the limitations of existing
SoTA LLM/LVLMs for generating production-grade sports captions from images in a
desired stylized format, and proposes a two-level fine-tuned LVLM pipeline to
address that. The proposed pipeline yields an improvement > 8-10% in the F1,
and > 2-10% in BERT score compared to alternative approaches. In addition, it
has a small runtime memory footprint and fast execution time. During Super Bowl
LIX the pipeline proved its practical application for live professional sports
journalism; generating highly accurate and stylized captions at the rate of 6
images per 3-5 seconds for over 1000 images during the game play.

</details>


### [7] [DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models](https://arxiv.org/abs/2508.19298)
*Abu Sufian,Anirudha Ghosh,Debaditya Barman,Marco Leo,Cosimo Distante*

Main category: cs.CV

TL;DR: 论文通过DemoBias项目评估了大型视觉语言模型（LVLMs）在生物特征人脸识别任务中的 demographic biases，发现不同模型在不同人口群体中存在性能差异。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决LVLMs在生物特征人脸识别任务中存在的 demographic biases 问题，确保模型在不同人口群体中的公平性和可靠性。

Method: 方法包括对LLaVA、BLIP-2和PaliGemma三种预训练LVLMs进行微调，并在自建的人口平衡数据集上评估，使用BERTScores和Fairness Discrepancy Rate等指标量化性能差异。

Result: 实验结果显示PaliGemma和LLaVA在Hispanic/Latino、Caucasian和South Asian群体中表现差异较大，而BLIP-2表现相对一致。

Conclusion: 结论是LVLMs存在 demographic biases，需要进一步优化以提高公平性。

Abstract: Large Vision Language Models (LVLMs) have demonstrated remarkable
capabilities across various downstream tasks, including biometric face
recognition (FR) with description. However, demographic biases remain a
critical concern in FR, as these foundation models often fail to perform
equitably across diverse demographic groups, considering ethnicity/race,
gender, and age. Therefore, through our work DemoBias, we conduct an empirical
evaluation to investigate the extent of demographic biases in LVLMs for
biometric FR with textual token generation tasks. We fine-tuned and evaluated
three widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our own
generated demographic-balanced dataset. We utilize several evaluation metrics,
like group-specific BERTScores and the Fairness Discrepancy Rate, to quantify
and trace the performance disparities. The experimental results deliver
compelling insights into the fairness and reliability of LVLMs across diverse
demographic groups. Our empirical study uncovered demographic biases in LVLMs,
with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,
Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparably
consistent. Repository: https://github.com/Sufianlab/DemoBias.

</details>


### [8] [Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities](https://arxiv.org/abs/2508.19305)
*Chen Chu,Cyrus Shahabi*

Main category: cs.CV

TL;DR: Geo2Vec是一种新型的空间表示学习方法，通过自适应采样和符号距离场直接编码地理实体的几何特征，避免了现有方法的分解和高计算成本问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么仅适用于单一地理实体类型，要么需要分解实体并引入高计算成本，且缺乏几何对齐，导致细粒度特征模糊。

Method: Geo2Vec利用符号距离场（SDF）直接在原始空间中操作，自适应采样点并编码其符号距离，通过神经网络训练生成紧凑且几何感知的统一表示。

Result: Geo2Vec在形状和位置表示、拓扑和距离关系捕捉以及实际GeoAI应用中的效率方面均优于现有方法。

Conclusion: Geo2Vec提供了一种高效、统一且几何感知的地理实体表示方法，适用于多种GeoAI任务。

Abstract: Spatial representation learning is essential for GeoAI applications such as
urban analytics, enabling the encoding of shapes, locations, and spatial
relationships (topological and distance-based) of geo-entities like points,
polylines, and polygons. Existing methods either target a single geo-entity
type or, like Poly2Vec, decompose entities into simpler components to enable
Fourier transformation, introducing high computational cost. Moreover, since
the transformed space lacks geometric alignment, these methods rely on uniform,
non-adaptive sampling, which blurs fine-grained features like edges and
boundaries. To address these limitations, we introduce Geo2Vec, a novel method
inspired by signed distance fields (SDF) that operates directly in the original
space. Geo2Vec adaptively samples points and encodes their signed distances
(positive outside, negative inside), capturing geometry without decomposition.
A neural network trained to approximate the SDF produces compact,
geometry-aware, and unified representations for all geo-entity types.
Additionally, we propose a rotation-invariant positional encoding to model
high-frequency spatial variations and construct a structured and robust
embedding space for downstream GeoAI models. Empirical results show that
Geo2Vec consistently outperforms existing methods in representing shape and
location, capturing topological and distance relationships, and achieving
greater efficiency in real-world GeoAI applications. Code and Data can be found
at: https://github.com/chuchen2017/GeoNeuralRepresentation.

</details>


### [9] [Advancements in Crop Analysis through Deep Learning and Explainable AI](https://arxiv.org/abs/2508.19307)
*Hamza Khan*

Main category: cs.CV

TL;DR: 该研究提出了一种基于卷积神经网络（CNN）的自动化方法，用于分类五种稻米品种，并结合可解释人工智能（XAI）技术开发了稻叶病害诊断方法，展示了深度学习在农业中的潜力。


<details>
  <summary>Details</summary>
Motivation: 稻米是全球重要的主食，其质量和产量监测对消费者满意度和国家声誉至关重要。传统人工检测方法效率低且易出错，亟需自动化解决方案。

Method: 使用包含75000张图像的公开数据集，采用CNN进行稻米品种分类，并结合VGG16、ResNet50和MobileNetV2等深度学习模型及SHAP、LIME等XAI技术进行病害诊断。

Result: 模型在稻米品种分类中表现出高准确率，且XAI技术增强了模型的透明度和可靠性，为农业自动化提供了有力支持。

Conclusion: 深度学习结合XAI技术可有效支持农作物质量检测和病害诊断，对农民、消费者和农业经济具有重要价值。

Abstract: Rice is a staple food of global importance in terms of trade, nutrition, and
economic growth. Among Asian nations such as China, India, Pakistan, Thailand,
Vietnam and Indonesia are leading producers of both long and short grain
varieties, including basmati, jasmine, arborio, ipsala, and kainat saila. To
ensure consumer satisfaction and strengthen national reputations, monitoring
rice crops and grain quality is essential. Manual inspection, however, is
labour intensive, time consuming and error prone, highlighting the need for
automated solutions for quality control and yield improvement. This study
proposes an automated approach to classify five rice grain varieties using
Convolutional Neural Networks (CNN). A publicly available dataset of 75000
images was used for training and testing. Model evaluation employed accuracy,
recall, precision, F1-score, ROC curves, and confusion matrices. Results
demonstrated high classification accuracy with minimal misclassifications,
confirming the model effectiveness in distinguishing rice varieties. In
addition, an accurate diagnostic method for rice leaf diseases such as Brown
Spot, Blast, Bacterial Blight, and Tungro was developed. The framework combined
explainable artificial intelligence (XAI) with deep learning models including
CNN, VGG16, ResNet50, and MobileNetV2. Explainability techniques such as SHAP
(SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic
Explanations) revealed how specific grain and leaf features influenced
predictions, enhancing model transparency and reliability. The findings
demonstrate the strong potential of deep learning in agricultural applications,
paving the way for robust, interpretable systems that can support automated
crop quality inspection and disease diagnosis, ultimately benefiting farmers,
consumers, and the agricultural economy.

</details>


### [10] [Sistema de Reconocimiento Facial Federado en Conjuntos Abiertos basado en OpenMax](https://arxiv.org/abs/2508.19312)
*Ander Galván,Marivi Higuero,Jorge Sasiain,Eduardo Jacob*

Main category: cs.CV

TL;DR: 论文提出了一种基于联邦学习框架的面部识别系统，结合OpenMax算法，用于开放集场景中区分已知和未知个体。


<details>
  <summary>Details</summary>
Motivation: AI驱动的面部识别在隐私和身份管理方面存在挑战，尤其是在开放集中处理未知个体时。

Method: 设计并实现了一个结合OpenMax算法的联邦学习系统，通过交换平均激活向量和局部距离度量来区分已知和未知个体。

Result: 实验验证了该方法的有效性，展示了其在分布式环境中增强隐私和鲁棒性的潜力。

Conclusion: 该方法为隐私感知和鲁棒的面部识别提供了可行的解决方案。

Abstract: Facial recognition powered by Artificial Intelligence has achieved high
accuracy in specific scenarios and applications. Nevertheless, it faces
significant challenges regarding privacy and identity management, particularly
when unknown individuals appear in the operational context. This paper presents
the design, implementation, and evaluation of a facial recognition system
within a federated learning framework tailored to open-set scenarios. The
proposed approach integrates the OpenMax algorithm into federated learning,
leveraging the exchange of mean activation vectors and local distance measures
to reliably distinguish between known and unknown subjects. Experimental
results validate the effectiveness of the proposed solution, demonstrating its
potential for enhancing privacy-aware and robust facial recognition in
distributed environments.
  --
  El reconocimiento facial impulsado por Inteligencia Artificial ha demostrado
una alta precisi\'on en algunos escenarios y aplicaciones. Sin embargo,
presenta desaf\'ios relacionados con la privacidad y la identificaci\'on de
personas, especialmente considerando que pueden aparecer sujetos desconocidos
para el sistema que lo implementa. En este trabajo, se propone el dise\~no,
implementaci\'on y evaluaci\'on de un sistema de reconocimiento facial en un
escenario de aprendizaje federado, orientado a conjuntos abiertos.
Concretamente, se dise\~na una soluci\'on basada en el algoritmo OpenMax para
escenarios de aprendizaje federado. La propuesta emplea el intercambio de los
vectores de activaci\'on promedio y distancias locales para identificar de
manera eficaz tanto personas conocidas como desconocidas. Los experimentos
realizados demuestran la implementaci\'on efectiva de la soluci\'on propuesta.

</details>


### [11] [Automated classification of natural habitats using ground-level imagery](https://arxiv.org/abs/2508.19314)
*Mahdis Tourian,Sareh Rowlands,Remy Vandaele,Max Fancourt,Rebecca Mein,Hywel T. P. Williams*

Main category: cs.CV

TL;DR: 提出了一种基于地面照片的深度学习方法来分类陆地栖息地，展示了较高的分类准确性，并开发了配套的网页应用。


<details>
  <summary>Details</summary>
Motivation: 准确的栖息地分类对生物多样性保护、生态监测和土地利用规划至关重要，但现有方法依赖卫星图像和人工验证，难以规模化。

Method: 使用DeepLabV3-ResNet101模型对地面照片进行分类，通过预处理和重采样优化数据，采用五折交叉验证评估性能。

Result: 模型平均F1分数为0.61，部分类别（如BSSP和BS）超过0.90，混合类别表现较差。

Conclusion: 地面照片结合深度学习方法在生态监测中具有潜力，配套网页应用便于实际使用。

Abstract: Accurate classification of terrestrial habitats is critical for biodiversity
conservation, ecological monitoring, and land-use planning. Several habitat
classification schemes are in use, typically based on analysis of satellite
imagery with validation by field ecologists. Here we present a methodology for
classification of habitats based solely on ground-level imagery (photographs),
offering improved validation and the ability to classify habitats at scale (for
example using citizen-science imagery). In collaboration with Natural England,
a public sector organisation responsible for nature conservation in England,
this study develops a classification system that applies deep learning to
ground-level habitat photographs, categorising each image into one of 18
classes defined by the 'Living England' framework. Images were pre-processed
using resizing, normalisation, and augmentation; re-sampling was used to
balance classes in the training data and enhance model robustness. We developed
and fine-tuned a DeepLabV3-ResNet101 classifier to assign a habitat class label
to each photograph. Using five-fold cross-validation, the model demonstrated
strong overall performance across 18 habitat classes, with accuracy and
F1-scores varying between classes. Across all folds, the model achieved a mean
F1-score of 0.61, with visually distinct habitats such as Bare Soil, Silt and
Peat (BSSP) and Bare Sand (BS) reaching values above 0.90, and mixed or
ambiguous classes scoring lower. These findings demonstrate the potential of
this approach for ecological monitoring. Ground-level imagery is readily
obtained, and accurate computational methods for habitat classification based
on such data have many potential applications. To support use by practitioners,
we also provide a simple web application that classifies uploaded images using
our model.

</details>


### [12] [MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation](https://arxiv.org/abs/2508.19320)
*Ming Chen,Liyuan Cui,Wenyuan Zhang,Haoxian Zhang,Yan Zhou,Xiaohan Li,Xiaoqiang Liu,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文提出了一种自回归视频生成框架，支持多模态交互控制和低延迟流式推断，解决了现有方法的高延迟、高计算成本和有限可控性问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在实时交互数字人视频生成中存在高延迟、高计算成本和可控性有限的问题，亟需一种高效、低延迟且可控性强的解决方案。

Method: 通过微调大型语言模型（LLM），接受音频、姿态和文本等多模态条件编码，输出空间和语义一致的表示以指导扩散头的去噪过程。构建了大规模对话数据集（约20,000小时），并引入深度压缩自编码器（压缩比高达64倍）以减轻自回归模型的长时推理负担。

Result: 实验表明，该方法在双工对话、多语言人合成和交互世界模型中表现出低延迟、高效率和细粒度多模态可控性优势。

Conclusion: 该框架为实时交互数字人视频生成提供了一种高效、可控且低延迟的解决方案。

Abstract: Recently, interactive digital human video generation has attracted widespread
attention and achieved remarkable progress. However, building such a practical
system that can interact with diverse input signals in real time remains
challenging to existing methods, which often struggle with high latency, heavy
computational cost, and limited controllability. In this work, we introduce an
autoregressive video generation framework that enables interactive multimodal
control and low-latency extrapolation in a streaming manner. With minimal
modifications to a standard large language model (LLM), our framework accepts
multimodal condition encodings including audio, pose, and text, and outputs
spatially and semantically coherent representations to guide the denoising
process of a diffusion head. To support this, we construct a large-scale
dialogue dataset of approximately 20,000 hours from multiple sources, providing
rich conversational scenarios for training. We further introduce a deep
compression autoencoder with up to 64$\times$ reduction ratio, which
effectively alleviates the long-horizon inference burden of the autoregressive
model. Extensive experiments on duplex conversation, multilingual human
synthesis, and interactive world model highlight the advantages of our approach
in low latency, high efficiency, and fine-grained multimodal controllability.

</details>


### [13] [Deep Data Hiding for ICAO-Compliant Face Images: A Survey](https://arxiv.org/abs/2508.19324)
*Jefferson David Rodriguez Chivata,Davide Ghiani,Simone Maurizio La Cava,Marco Micheletto,Giulia Orrù,Federico Lama,Gian Luca Marcialis*

Main category: cs.CV

TL;DR: 本文探讨了数字水印和隐写术作为补充解决方案，用于在ICAO合规面部图像中嵌入防篡改信号，以实现持久验证。


<details>
  <summary>Details</summary>
Motivation: ICAO合规面部图像在身份验证中广泛应用，但其标准化也带来了如变形和深度伪造等风险，传统防护措施如PAD无法提供捕获后保护。

Method: 本文通过调查数字水印和隐写术技术，分析其在ICAO合规图像中的应用潜力与局限性。

Result: 研究提供了对现有技术的全面分析，强调了关键权衡，并为实际身份系统中的安全部署提供了指导。

Conclusion: 数字水印和隐写术是有效的补充解决方案，可在不违反ICAO合规性的情况下增强图像的安全性。

Abstract: ICAO-compliant facial images, initially designed for secure biometric
passports, are increasingly becoming central to identity verification in a wide
range of application contexts, including border control, digital travel
credentials, and financial services. While their standardization enables global
interoperability, it also facilitates practices such as morphing and deepfakes,
which can be exploited for harmful purposes like identity theft and illegal
sharing of identity documents. Traditional countermeasures like Presentation
Attack Detection (PAD) are limited to real-time capture and offer no
post-capture protection. This survey paper investigates digital watermarking
and steganography as complementary solutions that embed tamper-evident signals
directly into the image, enabling persistent verification without compromising
ICAO compliance. We provide the first comprehensive analysis of
state-of-the-art techniques to evaluate the potential and drawbacks of the
underlying approaches concerning the applications involving ICAO-compliant
images and their suitability under standard constraints. We highlight key
trade-offs, offering guidance for secure deployment in real-world identity
systems.

</details>


### [14] [PRISM: A Framework Harnessing Unsupervised Visual Representations and Textual Prompts for Explainable MACE Survival Prediction from Cardiac Cine MRI](https://arxiv.org/abs/2508.19325)
*Haoyang Su,Jin-Yi Xiang,Shaohao Rui,Yifan Gao,Xingyu Chen,Tingxuan Yin,Xiaosong Wang,Lian-Ming Wu*

Main category: cs.CV

TL;DR: PRISM是一种自监督框架，结合心脏磁共振成像和电子健康记录，通过医学提示进行精细风险预测，显著优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 准确预测主要不良心脏事件（MACE）是心血管预后的核心挑战。

Method: PRISM通过运动感知多视图蒸馏提取同步成像特征，并结合医学提示进行调制。

Result: 在四个独立临床队列中，PRISM表现优于传统模型和SOTA基线，并揭示了三种与MACE风险相关的成像特征。

Conclusion: PRISM为心脏风险提供了有价值的见解，并识别了高血压、糖尿病和吸烟等主要风险因素。

Abstract: Accurate prediction of major adverse cardiac events (MACE) remains a central
challenge in cardiovascular prognosis. We present PRISM (Prompt-guided
Representation Integration for Survival Modeling), a self-supervised framework
that integrates visual representations from non-contrast cardiac cine magnetic
resonance imaging with structured electronic health records (EHRs) for survival
analysis. PRISM extracts temporally synchronized imaging features through
motion-aware multi-view distillation and modulates them using medically
informed textual prompts to enable fine-grained risk prediction. Across four
independent clinical cohorts, PRISM consistently surpasses classical survival
prediction models and state-of-the-art (SOTA) deep learning baselines under
internal and external validation. Further clinical findings demonstrate that
the combined imaging and EHR representations derived from PRISM provide
valuable insights into cardiac risk across diverse cohorts. Three distinct
imaging signatures associated with elevated MACE risk are uncovered, including
lateral wall dyssynchrony, inferior wall hypersensitivity, and anterior
elevated focus during diastole. Prompt-guided attribution further identifies
hypertension, diabetes, and smoking as dominant contributors among clinical and
physiological EHR factors.

</details>


### [15] [EffNetViTLoRA: An Efficient Hybrid Deep Learning Approach for Alzheimer's Disease Diagnosis](https://arxiv.org/abs/2508.19349)
*Mahdieh Behjat Khatooni,Mohsen Soryani*

Main category: cs.CV

TL;DR: EffNetViTLoRA模型结合CNN和ViT，利用全ADNI MRI数据集进行阿尔茨海默病诊断，准确率达92.52%。


<details>
  <summary>Details</summary>
Motivation: 阿尔茨海默病（AD）的早期诊断至关重要，但轻度认知障碍（MCI）的诊断因症状细微而困难。现有研究多依赖有限数据，导致模型不够稳健。

Method: 提出EffNetViTLoRA模型，整合CNN和ViT以捕捉MRI图像的局部和全局特征，并采用LoRA技术优化预训练ViT的迁移学习。

Result: 模型在AD、MCI和CN三个类别上的分类准确率为92.52%，F1分数为92.76%。

Conclusion: EffNetViTLoRA通过全数据集训练和LoRA技术，显著提升了AD诊断的准确性和临床可靠性。

Abstract: Alzheimer's disease (AD) is one of the most prevalent neurodegenerative
disorders worldwide. As it progresses, it leads to the deterioration of
cognitive functions. Since AD is irreversible, early diagnosis is crucial for
managing its progression. Mild Cognitive Impairment (MCI) represents an
intermediate stage between Cognitively Normal (CN) individuals and those with
AD, and is considered a transitional phase from normal cognition to Alzheimer's
disease. Diagnosing MCI is particularly challenging due to the subtle
differences between adjacent diagnostic categories. In this study, we propose
EffNetViTLoRA, a generalized end-to-end model for AD diagnosis using the whole
Alzheimer's Disease Neuroimaging Initiative (ADNI) Magnetic Resonance Imaging
(MRI) dataset. Our model integrates a Convolutional Neural Network (CNN) with a
Vision Transformer (ViT) to capture both local and global features from MRI
images. Unlike previous studies that rely on limited subsets of data, our
approach is trained on the full T1-weighted MRI dataset from ADNI, resulting in
a more robust and unbiased model. This comprehensive methodology enhances the
model's clinical reliability. Furthermore, fine-tuning large pretrained models
often yields suboptimal results when source and target dataset domains differ.
To address this, we incorporate Low-Rank Adaptation (LoRA) to effectively adapt
the pretrained ViT model to our target domain. This method enables efficient
knowledge transfer and reduces the risk of overfitting. Our model achieves a
classification accuracy of 92.52% and an F1-score of 92.76% across three
diagnostic categories: AD, MCI, and CN for full ADNI dataset.

</details>


### [16] [Concurrent validity of computer-vision artificial intelligence player tracking software using broadcast footage](https://arxiv.org/abs/2508.19477)
*Zachary L. Crang,Rich D. Johnston,Katie L. Mills,Johsan Billingham,Sam Robertson,Michael H. Cole,Jonathon Weakley,Adam Hewitt and,Grant M. Duthie*

Main category: cs.CV

TL;DR: 研究评估了商业计算机视觉和AI球员追踪软件在广播画面中测量球员位置、速度和距离的准确性，并探讨了摄像头画面和分辨率的影响。


<details>
  <summary>Details</summary>
Motivation: 了解商业计算机视觉和AI球员追踪软件的准确性及其受摄像头画面和分辨率的影响。

Method: 使用2022年卡塔尔世界杯的一场比赛数据，比较了三家商业追踪提供商的数据与高精度多摄像头追踪系统（TRACAB Gen 5）的数据。

Result: 位置RMSE范围为1.68至16.39米，速度RMSE范围为0.34至2.38米/秒，总距离偏差范围为-1745米至1945米。

Conclusion: 计算机视觉和AI追踪软件在检测到球员时具有较好的精度，建议使用战术画面以提高准确性，720p和1080p分辨率均适用。

Abstract: This study aimed to: (1) understand whether commercially available
computer-vision and artificial intelligence (AI) player tracking software can
accurately measure player position, speed and distance using broadcast footage
and (2) determine the impact of camera feed and resolution on accuracy. Data
were obtained from one match at the 2022 Qatar Federation Internationale de
Football Association (FIFA) World Cup. Tactical, programme and camera 1 feeds
were used. Three commercial tracking providers that use computer-vision and AI
participated. Providers analysed instantaneous position (x, y coordinates) and
speed (m\,s^{-1}) of each player. Their data were compared with a
high-definition multi-camera tracking system (TRACAB Gen 5). Root mean square
error (RMSE) and mean bias were calculated. Position RMSE ranged from 1.68 to
16.39 m, while speed RMSE ranged from 0.34 to 2.38 m\,s^{-1}. Total match
distance mean bias ranged from -1745 m (-21.8%) to 1945 m (24.3%) across
providers. Computer-vision and AI player tracking software offer the ability to
track players with fair precision when players are detected by the software.
Providers should use a tactical feed when tracking position and speed, which
will maximise player detection, improving accuracy. Both 720p and 1080p
resolutions are suitable, assuming appropriate computer-vision and AI models
are implemented.

</details>


### [17] [JVLGS: Joint Vision-Language Gas Leak Segmentation](https://arxiv.org/abs/2508.19485)
*Xinlong Zhao,Qixiang Pang,Shan Du*

Main category: cs.CV

TL;DR: 提出了一种名为JVLGS的新框架，结合视觉和文本模态的优势，提升气体泄漏的检测和分割效果。


<details>
  <summary>Details</summary>
Motivation: 气体泄漏对人类健康和大气污染构成严重威胁，但现有检测方法效果有限，尤其是针对模糊和非刚性的气体云。

Method: 通过联合视觉和语言模态，增强气体泄漏的表征和分割，并引入后处理步骤减少误报。

Result: 在多种场景下的实验表明，JVLGS显著优于现有方法，并在监督学习和少样本学习设置下均表现优异。

Conclusion: JVLGS框架通过多模态融合和后处理，有效提升了气体泄漏检测的准确性和鲁棒性。

Abstract: Gas leaks pose serious threats to human health and contribute significantly
to atmospheric pollution, drawing increasing public concern. However, the lack
of effective detection methods hampers timely and accurate identification of
gas leaks. While some vision-based techniques leverage infrared videos for leak
detection, the blurry and non-rigid nature of gas clouds often limits their
effectiveness. To address these challenges, we propose a novel framework called
Joint Vision-Language Gas leak Segmentation (JVLGS), which integrates the
complementary strengths of visual and textual modalities to enhance gas leak
representation and segmentation. Recognizing that gas leaks are sporadic and
many video frames may contain no leak at all, our method incorporates a
post-processing step to reduce false positives caused by noise and non-target
objects, an issue that affects many existing approaches. Extensive experiments
conducted across diverse scenarios show that JVLGS significantly outperforms
state-of-the-art gas leak segmentation methods. We evaluate our model under
both supervised and few-shot learning settings, and it consistently achieves
strong performance in both, whereas competing methods tend to perform well in
only one setting or poorly in both. Code available at:
https://github.com/GeekEagle/JVLGS

</details>


### [18] [UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models](https://arxiv.org/abs/2508.19498)
*Yimu Wang,Weiming Zhuang,Chen Chen,Jiabo Huang,Jingtao Li,Lingjuan Lyu*

Main category: cs.CV

TL;DR: UNIFORM框架通过投票机制整合异构预训练模型的知识，提升无监督目标识别性能，并展示出卓越的可扩展性。


<details>
  <summary>Details</summary>
Motivation: 预训练模型的异构性导致知识整合困难，现有方法受限于数据分布和架构假设。

Method: 提出UNIFORM框架，通过logit和特征级别的投票机制整合多样化预训练模型的知识。

Result: 实验表明UNIFORM显著提升性能，并支持超过100个教师模型的扩展。

Conclusion: UNIFORM有效解决了异构模型知识整合的挑战，具有广泛的应用潜力。

Abstract: In the era of deep learning, the increasing number of pre-trained models
available online presents a wealth of knowledge. These models, developed with
diverse architectures and trained on varied datasets for different tasks,
provide unique interpretations of the real world. Their collective consensus is
likely universal and generalizable to unseen data. However, effectively
harnessing this collective knowledge poses a fundamental challenge due to the
heterogeneity of pre-trained models. Existing knowledge integration solutions
typically rely on strong assumptions about training data distributions and
network architectures, limiting them to learning only from specific types of
models and resulting in data and/or inductive biases. In this work, we
introduce a novel framework, namely UNIFORM, for knowledge transfer from a
diverse set of off-the-shelf models into one student model without such
constraints. Specifically, we propose a dedicated voting mechanism to capture
the consensus of knowledge both at the logit level -- incorporating teacher
models that are capable of predicting target classes of interest -- and at the
feature level, utilizing visual representations learned on arbitrary label
spaces. Extensive experiments demonstrate that UNIFORM effectively enhances
unsupervised object recognition performance compared to strong knowledge
transfer baselines. Notably, it exhibits remarkable scalability by benefiting
from over one hundred teachers, while existing methods saturate at a much
smaller scale.

</details>


### [19] [Sat2Flow: A Structure-Aware Diffusion Framework for Human Flow Generation from Satellite Imagery](https://arxiv.org/abs/2508.19499)
*Xiangxu Wang,Tianhong Zhao,Wei Tu,Bowen Zhang,Guanzhou Chen,Jinzhou Cao*

Main category: cs.CV

TL;DR: Sat2Flow是一种基于扩散的框架，仅使用卫星图像生成结构一致的OD流，解决了现有方法对辅助数据和空间拓扑敏感的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖成本高且覆盖有限的辅助特征，并对空间拓扑敏感，Sat2Flow旨在解决这些问题。

Method: Sat2Flow采用多核编码器捕捉区域交互，并通过排列感知扩散过程确保结构一致性。

Result: 实验表明，Sat2Flow在数值精度上优于基线方法，并在索引置换下保持分布和结构。

Conclusion: Sat2Flow为数据稀缺环境提供了可扩展的OD流生成方案，消除了对区域特定数据的依赖。

Abstract: Origin-Destination (OD) flow matrices are essential for urban mobility
analysis, underpinning applications in traffic forecasting, infrastructure
planning, and policy design. However, existing methods suffer from two critical
limitations: (1) reliance on auxiliary features (e.g., Points of Interest,
socioeconomic statistics) that are costly to collect and have limited spatial
coverage; and (2) sensitivity to spatial topology, where minor index reordering
of urban regions (e.g., census tract relabeling) disrupts structural coherence
in generated flows. To address these challenges, we propose Sat2Flow, a latent
structure-aware diffusion-based framework that generates structurally coherent
OD flows using solely satellite imagery as input. Our approach introduces a
multi-kernel encoder to capture diverse regional interactions and employs a
permutation-aware diffusion process that aligns latent representations across
different regional orderings. Through a joint contrastive training objective
that bridges satellite-derived features with OD patterns, combined with
equivariant diffusion training that enforces structural consistency, Sat2Flow
ensures topological robustness under arbitrary regional reindexing.
Experimental results on real-world urban datasets demonstrate that Sat2Flow
outperforms both physics-based and data-driven baselines in numerical accuracy
while preserving empirical distributions and spatial structures under index
permutations. Sat2Flow offers a globally scalable solution for OD flow
generation in data-scarce urban environments, eliminating region-specific
auxiliary data dependencies while maintaining structural invariance for robust
mobility modeling.

</details>


### [20] [Weed Detection in Challenging Field Conditions: A Semi-Supervised Framework for Overcoming Shadow Bias and Data Scarcity](https://arxiv.org/abs/2508.19511)
*Alzayat Saleh,Shunsuke Hatano,Mostafa Rahimi Azghadi*

Main category: cs.CV

TL;DR: 该研究提出了一种诊断驱动的半监督框架，用于解决农业中入侵杂草自动管理的两个主要问题：恶劣环境条件和数据标注成本高。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决深度学习模型在真实农田中因环境挑战和高标注成本导致的性能下降问题。

Method: 方法包括建立强监督基线（ResNet、YOLO、RF-DETR），并通过诊断发现“阴影偏差”，进而设计半监督管道利用未标记数据增强模型鲁棒性。

Result: 结果展示了分类和检测的高性能（F1分数达0.90，mAP50超过0.82），并通过伪标记提升召回率，减少杂草遗漏。

Conclusion: 结论是该框架为精准农业中的计算机视觉系统提供了开发、诊断和改进的实用方法。

Abstract: The automated management of invasive weeds is critical for sustainable
agriculture, yet the performance of deep learning models in real-world fields
is often compromised by two factors: challenging environmental conditions and
the high cost of data annotation. This study tackles both issues through a
diagnostic-driven, semi-supervised framework. Using a unique dataset of
approximately 975 labeled and 10,000 unlabeled images of Guinea Grass in
sugarcane, we first establish strong supervised baselines for classification
(ResNet) and detection (YOLO, RF-DETR), achieving F1 scores up to 0.90 and
mAP50 scores exceeding 0.82. Crucially, this foundational analysis, aided by
interpretability tools, uncovered a pervasive "shadow bias," where models
learned to misidentify shadows as vegetation. This diagnostic insight motivated
our primary contribution: a semi-supervised pipeline that leverages unlabeled
data to enhance model robustness. By training models on a more diverse set of
visual information through pseudo-labeling, this framework not only helps
mitigate the shadow bias but also provides a tangible boost in recall, a
critical metric for minimizing weed escapes in automated spraying systems. To
validate our methodology, we demonstrate its effectiveness in a low-data regime
on a public crop-weed benchmark. Our work provides a clear and field-tested
framework for developing, diagnosing, and improving robust computer vision
systems for the complex realities of precision agriculture.

</details>


### [21] [MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment](https://arxiv.org/abs/2508.19527)
*Zhiting Gao,Dan Song,Diqiong Jiang,Chao Xue,An-An Liu*

Main category: cs.CV

TL;DR: TAPO和MotionFLUX框架解决了文本驱动运动生成中的语义对齐和实时性问题，显著提升了生成速度和运动质量。


<details>
  <summary>Details</summary>
Motivation: 现有文本驱动方法在语义对齐和推理效率上存在不足，需要改进以实现更精确和高效的运动生成。

Method: 提出TAPO框架优化语义对齐，MotionFLUX框架通过确定性整流流匹配实现实时生成。

Result: 实验表明，该系统在语义一致性和运动质量上优于现有方法，同时加速了生成速度。

Conclusion: TAPO和MotionFLUX的结合为运动生成提供了高效且高质量的解决方案。

Abstract: Motion generation is essential for animating virtual characters and embodied
agents. While recent text-driven methods have made significant strides, they
often struggle with achieving precise alignment between linguistic descriptions
and motion semantics, as well as with the inefficiencies of slow, multi-step
inference. To address these issues, we introduce TMR++ Aligned Preference
Optimization (TAPO), an innovative framework that aligns subtle motion
variations with textual modifiers and incorporates iterative adjustments to
reinforce semantic grounding. To further enable real-time synthesis, we propose
MotionFLUX, a high-speed generation framework based on deterministic rectified
flow matching. Unlike traditional diffusion models, which require hundreds of
denoising steps, MotionFLUX constructs optimal transport paths between noise
distributions and motion spaces, facilitating real-time synthesis. The
linearized probability paths reduce the need for multi-step sampling typical of
sequential methods, significantly accelerating inference time without
sacrificing motion quality. Experimental results demonstrate that, together,
TAPO and MotionFLUX form a unified system that outperforms state-of-the-art
approaches in both semantic consistency and motion quality, while also
accelerating generation speed. The code and pretrained models will be released.

</details>


### [22] [CVBench: Evaluating Cross-Video Synergies for Complex Multimodal Understanding and Reasoning](https://arxiv.org/abs/2508.19542)
*Nannan Zhu,Yonghao Dong,Teng Wang,Xueqian Li,Shengjun Deng,Yijia Wang,Zheng Hong,Tiantian Geng,Guo Niu,Hanyan Huang,Xiongfei Yao,Shuaiwei Jiao*

Main category: cs.CV

TL;DR: CVBench是首个全面评估跨视频关系推理能力的基准测试，包含1000个问题-答案对，覆盖三个层次的任务。测试发现，当前多模态大语言模型在跨视频推理上存在显著性能差距。


<details>
  <summary>Details</summary>
Motivation: 现实应用（如多摄像头监控和跨视频程序学习）需要模型具备跨视频推理能力，但现有研究对此关注不足。

Method: 构建CVBench基准，包含三个层次的任务（对象关联、事件关联和复杂推理），并对10多个领先模型进行零样本或思维链提示评估。

Result: 顶级模型（如GPT-4o）在因果推理任务上仅达到60%准确率，远低于人类的91%。分析揭示了模型在跨视频上下文保留和实体消歧上的瓶颈。

Conclusion: CVBench为诊断和提升多视频推理能力提供了严格框架，并为下一代多模态大语言模型的设计提供了洞见。

Abstract: While multimodal large language models (MLLMs) exhibit strong performance on
single-video tasks (e.g., video question answering), their ability across
multiple videos remains critically underexplored. However, this capability is
essential for real-world applications, including multi-camera surveillance and
cross-video procedural learning. To bridge this gap, we present CVBench, the
first comprehensive benchmark designed to assess cross-video relational
reasoning rigorously. CVBench comprises 1,000 question-answer pairs spanning
three hierarchical tiers: cross-video object association (identifying shared
entities), cross-video event association (linking temporal or causal event
chains), and cross-video complex reasoning (integrating commonsense and domain
knowledge). Built from five domain-diverse video clusters (e.g., sports, life
records), the benchmark challenges models to synthesise information across
dynamic visual contexts. Extensive evaluation of 10+ leading MLLMs (including
GPT-4o, Gemini-2.0-flash, Qwen2.5-VL) under zero-shot or chain-of-thought
prompting paradigms. Key findings reveal stark performance gaps: even top
models, such as GPT-4o, achieve only 60% accuracy on causal reasoning tasks,
compared to the 91% accuracy of human performance. Crucially, our analysis
reveals fundamental bottlenecks inherent in current MLLM architectures, notably
deficient inter-video context retention and poor disambiguation of overlapping
entities. CVBench establishes a rigorous framework for diagnosing and advancing
multi-video reasoning, offering architectural insights for next-generation
MLLMs.The data and evaluation code are available at
https://github.com/Hokhim2/CVBench.

</details>


### [23] [WEBEYETRACK: Scalable Eye-Tracking for the Browser via On-Device Few-Shot Personalization](https://arxiv.org/abs/2508.19544)
*Eduardo Davalos,Yike Zhang,Namrata Srivastava,Yashvitha Thatigotla,Jorge A. Salas,Sara McFadden,Sun-Joo Cho,Amanda Goodwin,Ashwin TS,Gautam Biswas*

Main category: cs.CV

TL;DR: WebEyeTrack是一个轻量级框架，直接在浏览器中集成SOTA视线估计模型，解决了模型大小、推理时间和隐私问题，同时通过头部姿态估计和少量校准样本适应新用户。


<details>
  <summary>Details</summary>
Motivation: 现有AI视线估计方法在真实应用中与商业眼动追踪解决方案存在差距，且模型大小、推理时间和隐私问题常被忽视；基于摄像头的眼动追踪方法因头部运动导致精度不足。

Method: WebEyeTrack结合轻量级SOTA视线估计模型、基于模型的头部姿态估计和设备端少量样本学习（k < 9）。

Result: 在GazeCapture上实现2.32 cm的误差，iPhone 14上实时推理速度为2.4毫秒。

Conclusion: WebEyeTrack在性能和效率上达到SOTA水平，适用于实际应用。

Abstract: With advancements in AI, new gaze estimation methods are exceeding
state-of-the-art (SOTA) benchmarks, but their real-world application reveals a
gap with commercial eye-tracking solutions. Factors like model size, inference
time, and privacy often go unaddressed. Meanwhile, webcam-based eye-tracking
methods lack sufficient accuracy, in particular due to head movement. To tackle
these issues, we introduce We bEyeTrack, a framework that integrates
lightweight SOTA gaze estimation models directly in the browser. It
incorporates model-based head pose estimation and on-device few-shot learning
with as few as nine calibration samples (k < 9). WebEyeTrack adapts to new
users, achieving SOTA performance with an error margin of 2.32 cm on
GazeCapture and real-time inference speeds of 2.4 milliseconds on an iPhone 14.
Our open-source code is available at
https://github.com/RedForestAi/WebEyeTrack.

</details>


### [24] [MonoRelief V2: Leveraging Real Data for High-Fidelity Monocular Relief Recovery](https://arxiv.org/abs/2508.19555)
*Yu-Wei Zhang,Tongju Han,Lipeng Gao,Mingqiang Wei,Hui Liu,Changbao Li,Caiming Zhang*

Main category: cs.CV

TL;DR: MonoRelief V2是一个端到端模型，用于从单张图像中直接恢复2.5D浮雕，改进了前代模型，结合了真实数据以提高鲁棒性、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 解决在复杂材料和光照变化下从单张图像恢复2.5D浮雕的挑战，并克服大规模真实数据集获取的困难。

Method: 使用文本到图像生成模型生成伪真实图像，并通过深度和法线预测融合生成伪标签；构建小规模真实数据集，逐步训练模型。

Result: 在深度和法线预测方面表现出色，具有广泛的下游应用潜力。

Conclusion: MonoRelief V2通过结合伪真实和真实数据训练，实现了高性能的2.5D浮雕恢复。

Abstract: This paper presents MonoRelief V2, an end-to-end model designed for directly
recovering 2.5D reliefs from single images under complex material and
illumination variations. In contrast to its predecessor, MonoRelief V1 [1],
which was solely trained on synthetic data, MonoRelief V2 incorporates real
data to achieve improved robustness, accuracy and efficiency. To overcome the
challenge of acquiring large-scale real-world dataset, we generate
approximately 15,000 pseudo real images using a text-to-image generative model,
and derive corresponding depth pseudo-labels through fusion of depth and normal
predictions. Furthermore, we construct a small-scale real-world dataset (800
samples) via multi-view reconstruction and detail refinement. MonoRelief V2 is
then progressively trained on the pseudo-real and real-world datasets.
Comprehensive experiments demonstrate its state-of-the-art performance both in
depth and normal predictions, highlighting its strong potential for a range of
downstream applications. Code is at: https://github.com/glp1001/MonoreliefV2.

</details>


### [25] [FlowDet: Overcoming Perspective and Scale Challenges in Real-Time End-to-End Traffic Detection](https://arxiv.org/abs/2508.19565)
*Yuhang Zhao,Zixing Wang*

Main category: cs.CV

TL;DR: FlowDet是一种基于DETR架构的高效端到端目标检测器，通过几何可变形单元和尺度感知注意力模块优化性能，在Intersection-Flow-5k数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决端到端目标检测器在复杂场景（如交通监控）中计算成本高的问题。

Method: 提出FlowDet，采用解耦编码器优化策略，结合几何可变形单元（GDU）和尺度感知注意力模块（SAA）。

Result: 在Intersection-Flow-5k数据集上，FlowDet的AP(test)和AP50(test)分别提升1.5%和1.6%，同时减少63.2%的GFLOPs并提高16.2%的推理速度。

Conclusion: FlowDet为高效、高精度的目标检测器提供了新思路，适用于高要求的现实世界感知系统。

Abstract: End-to-end object detectors offer a promising NMS-free paradigm for real-time
applications, yet their high computational cost remains a significant barrier,
particularly for complex scenarios like intersection traffic monitoring. To
address this challenge, we propose FlowDet, a high-speed detector featuring a
decoupled encoder optimization strategy applied to the DETR architecture.
Specifically, FlowDet employs a novel Geometric Deformable Unit (GDU) for
traffic-aware geometric modeling and a Scale-Aware Attention (SAA) module to
maintain high representational power across extreme scale variations. To
rigorously evaluate the model's performance in environments with severe
occlusion and high object density, we collected the Intersection-Flow-5k
dataset, a new challenging scene for this task. Evaluated on
Intersection-Flow-5k, FlowDet establishes a new state-of-the-art. Compared to
the strong RT-DETR baseline, it improves AP(test) by 1.5% and AP50(test) by
1.6%, while simultaneously reducing GFLOPs by 63.2% and increasing inference
speed by 16.2%. Our work demonstrates a new path towards building highly
efficient and accurate detectors for demanding, real-world perception systems.
The Intersection-Flow-5k dataset is available at
https://github.com/AstronZh/Intersection-Flow-5K.

</details>


### [26] [DNP-Guided Contrastive Reconstruction with a Reverse Distillation Transformer for Medical Anomaly Detection](https://arxiv.org/abs/2508.19573)
*Luhu Li,Bowen Lin,Mukhtiar Khan,Shujun Fu*

Main category: cs.CV

TL;DR: 提出了一种结合可训练编码器和原型引导重建的统一框架，通过多样性感知对齐损失解决原型崩溃问题，显著提升了医学图像异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像异常检测面临标注有限和与自然图像的领域差距问题，现有方法依赖固定预训练编码器，限制了领域适应性。原型学习虽具解释性，但易出现原型崩溃。

Method: 结合可训练编码器与原型引导重建，引入多样性感知对齐损失，通过动量分支增强特征学习，轻量原型提取器挖掘正常原型以指导解码器。

Result: 在多个医学影像基准测试中表现优异，显著提升了表示质量和异常定位能力，优于现有方法。

Conclusion: 提出的框架有效解决了原型崩溃问题，增强了模型的解释性和性能。

Abstract: Anomaly detection in medical images is challenging due to limited annotations
and a domain gap compared to natural images. Existing reconstruction methods
often rely on frozen pre-trained encoders, which limits adaptation to
domain-specific features and reduces localization accuracy. Prototype-based
learning offers interpretability and clustering benefits but suffers from
prototype collapse, where few prototypes dominate training, harming diversity
and generalization. To address this, we propose a unified framework combining a
trainable encoder with prototype-guided reconstruction and a novel
Diversity-Aware Alignment Loss. The trainable encoder, enhanced by a momentum
branch, enables stable domain-adaptive feature learning. A lightweight
Prototype Extractor mines informative normal prototypes to guide the decoder
via attention for precise reconstruction. Our loss enforces balanced prototype
use through diversity constraints and per-prototype normalization, effectively
preventing collapse. Experiments on multiple medical imaging benchmarks show
significant improvements in representation quality and anomaly localization,
outperforming prior methods. Visualizations and prototype assignment analyses
further validate the effectiveness of our anti-collapse mechanism and enhanced
interpretability.

</details>


### [27] [Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation](https://arxiv.org/abs/2508.19574)
*Mingxi Fu,Fanglei Fu,Xitong Ling,Huaitian Yuan,Tian Guan,Yonghong He,Lianghui Zhu*

Main category: cs.CV

TL;DR: MPAMatch is a novel segmentation framework that uses multimodal prototype-guided supervision and dual contrastive learning to improve semantic boundary modeling in pathological images.


<details>
  <summary>Details</summary>
Motivation: Addressing challenges in pathological image segmentation, such as ambiguous semantic boundaries and high annotation costs, by leveraging multimodal supervision.

Method: Proposes MPAMatch, which includes dual contrastive learning between image/text prototypes and pixel labels, and replaces the ViT backbone with a pathology-pretrained model (Uni).

Result: MPAMatch outperforms state-of-the-art methods on multiple datasets (GLAS, EBHI-SEG-GLAND, EBHI-SEG-CANCER, KPI), demonstrating superior structural and semantic modeling.

Conclusion: MPAMatch effectively enhances segmentation performance by integrating multimodal supervision and pathology-specific feature extraction.

Abstract: Pathological image segmentation faces numerous challenges, particularly due
to ambiguous semantic boundaries and the high cost of pixel-level annotations.
Although recent semi-supervised methods based on consistency regularization
(e.g., UniMatch) have made notable progress, they mainly rely on
perturbation-based consistency within the image modality, making it difficult
to capture high-level semantic priors, especially in structurally complex
pathology images. To address these limitations, we propose MPAMatch - a novel
segmentation framework that performs pixel-level contrastive learning under a
multimodal prototype-guided supervision paradigm. The core innovation of
MPAMatch lies in the dual contrastive learning scheme between image prototypes
and pixel labels, and between text prototypes and pixel labels, providing
supervision at both structural and semantic levels. This coarse-to-fine
supervisory strategy not only enhances the discriminative capability on
unlabeled samples but also introduces the text prototype supervision into
segmentation for the first time, significantly improving semantic boundary
modeling. In addition, we reconstruct the classic segmentation architecture
(TransUNet) by replacing its ViT backbone with a pathology-pretrained
foundation model (Uni), enabling more effective extraction of
pathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,
EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-art
methods, validating its dual advantages in structural and semantic modeling.

</details>


### [28] [Interact-Custom: Customized Human Object Interaction Image Generation](https://arxiv.org/abs/2508.19575)
*Zhu Xu,Zhaowen Wang,Yuxin Peng,Yang Liu*

Main category: cs.CV

TL;DR: 论文提出了一种定制化人机交互图像生成任务（CHOI），通过两阶段模型Interact-Custom解决身份保留与交互控制的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注目标实体的外观保留，而忽略了目标实体之间的细粒度交互控制。

Method: 设计了两阶段模型Interact-Custom，首先生成描述交互行为的前景掩码，再在掩码指导下生成目标人机交互图像。

Result: 实验表明，该方法在定制的CHOI任务指标上表现有效。

Conclusion: Interact-Custom能够同时实现身份保留和交互语义控制，具有高内容可控性。

Abstract: Compositional Customized Image Generation aims to customize multiple target
concepts within generation content, which has gained attention for its wild
application.Existing approaches mainly concentrate on the target entity's
appearance preservation, while neglecting the fine-grained interaction control
among target entities.To enable the model of such interaction control
capability, we focus on human object interaction scenario and propose the task
of Customized Human Object Interaction Image Generation(CHOI), which
simultaneously requires identity preservation for target human object and the
interaction semantic control between them.Two primary challenges exist for
CHOI:(1)simultaneous identity preservation and interaction control demands
require the model to decompose the human object into self-contained identity
features and pose-oriented interaction features, while the current HOI image
datasets fail to provide ideal samples for such feature-decomposed
learning.(2)inappropriate spatial configuration between human and object may
lead to the lack of desired interaction semantics.To tackle it, we first
process a large-scale dataset, where each sample encompasses the same pair of
human object involving different interactive poses.Then we design a two-stage
model Interact-Custom, which firstly explicitly models the spatial
configuration by generating a foreground mask depicting the interaction
behavior, then under the guidance of this mask, we generate the target human
object interacting while preserving their identities features.Furthermore, if
the background image and the union location of where the target human object
should appear are provided by users, Interact-Custom also provides the optional
functionality to specify them, offering high content controllability. Extensive
experiments on our tailored metrics for CHOI task demonstrate the effectiveness
of our approach.

</details>


### [29] [High-Speed FHD Full-Color Video Computer-Generated Holography](https://arxiv.org/abs/2508.19579)
*Haomiao Zhang,Miao Cao,Xuan Yu,Hui Luo,Yanling Piao,Mengjie Qin,Zhangyuan Li,Ping Wang,Xin Yuan*

Main category: cs.CV

TL;DR: 提出了一种新型高速全息视频生成方案，结合SGDDM和HoloMamba技术，解决了高帧率与色彩保真度的矛盾，并显著提升了计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有计算机生成全息技术在高帧率显示和高效计算方面存在局限，导致色彩串扰和计算效率低下。

Method: 提出SGDDM优化相位分布以实现高保真全彩显示，并设计HoloMamba架构建模时空相关性以提升重建质量和计算效率。

Result: SGDDM实现了高帧率下的高保真全彩显示，HoloMamba以260 FPS生成1080p全彩全息视频，速度提升2.6倍。

Conclusion: 该方案有效解决了高帧率与色彩保真度的矛盾，并大幅提升了计算效率，为下一代全息显示技术提供了新思路。

Abstract: Computer-generated holography (CGH) is a promising technology for
next-generation displays. However, generating high-speed, high-quality
holographic video requires both high frame rate display and efficient
computation, but is constrained by two key limitations: ($i$) Learning-based
models often produce over-smoothed phases with narrow angular spectra, causing
severe color crosstalk in high frame rate full-color displays such as
depth-division multiplexing and thus resulting in a trade-off between frame
rate and color fidelity. ($ii$) Existing frame-by-frame optimization methods
typically optimize frames independently, neglecting spatial-temporal
correlations between consecutive frames and leading to computationally
inefficient solutions. To overcome these challenges, in this paper, we propose
a novel high-speed full-color video CGH generation scheme. First, we introduce
Spectrum-Guided Depth Division Multiplexing (SGDDM), which optimizes phase
distributions via frequency modulation, enabling high-fidelity full-color
display at high frame rates. Second, we present HoloMamba, a lightweight
asymmetric Mamba-Unet architecture that explicitly models spatial-temporal
correlations across video sequences to enhance reconstruction quality and
computational efficiency. Extensive simulated and real-world experiments
demonstrate that SGDDM achieves high-fidelity full-color display without
compromise in frame rate, while HoloMamba generates FHD (1080p) full-color
holographic video at over 260 FPS, more than 2.6$\times$ faster than the prior
state-of-the-art Divide-Conquer-and-Merge Strategy.

</details>


### [30] [Guiding Noisy Label Conditional Diffusion Models with Score-based Discriminator Correction](https://arxiv.org/abs/2508.19581)
*Dat Nguyen Cong,Hieu Tran Bao,Hoang Thanh-Tung*

Main category: cs.CV

TL;DR: 论文提出了一种名为Score-based Discriminator Correction (SBDC)的引导技术，用于对齐预训练的条件扩散模型中的噪声，通过判别器训练和对抗损失来提升生成能力和可控性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在大规模数据集上表现优异，但数据集中的人工标注错误可能影响其生成能力和可控性，目前对此研究不足。

Method: 提出SBDC技术，利用判别器训练和对抗损失来评估样本真实性，并限制引导在生成过程的早期阶段使用。

Result: 实验表明，该方法在不同噪声设置下优于现有技术，计算高效且几乎不增加推理时间。

Conclusion: SBDC是一种高效且无需重新训练扩散模型的方法，显著提升了生成模型的性能和可控性。

Abstract: Diffusion models have gained prominence as state-of-the-art techniques for
synthesizing images and videos, particularly due to their ability to scale
effectively with large datasets. Recent studies have uncovered that these
extensive datasets often contain mistakes from manual labeling processes.
However, the extent to which such errors compromise the generative capabilities
and controllability of diffusion models is not well studied. This paper
introduces Score-based Discriminator Correction (SBDC), a guidance technique
for aligning noisy pre-trained conditional diffusion models. The guidance is
built on discriminator training using adversarial loss, drawing on prior noise
detection techniques to assess the authenticity of each sample. We further show
that limiting the usage of our guidance to the early phase of the generation
process leads to better performance. Our method is computationally efficient,
only marginally increases inference time, and does not require retraining
diffusion models. Experiments on different noise settings demonstrate the
superiority of our method over previous state-of-the-art methods.

</details>


### [31] [Generalizing Monocular 3D Object Detection](https://arxiv.org/abs/2508.19593)
*Abhinav Kumar*

Main category: cs.CV

TL;DR: 该论文提出了一种单目3D物体检测（Mono3D）的通用化方法，通过改进NMS、深度等变网络、分割方法和数学分析，解决了遮挡、数据集多样性、大物体检测和相机参数变化等问题。


<details>
  <summary>Details</summary>
Motivation: 单目3D物体检测在自动驾驶、增强现实等领域有广泛应用，但现有模型在遮挡、数据集多样性、大物体检测和相机参数变化等场景下泛化能力不足。

Method: 1. 提出GrooMeD-NMS增强遮挡鲁棒性；2. 使用DEVIANT网络提升数据集泛化能力；3. 引入SeaBird方法解决大物体检测问题；4. 数学分析相机高度外推问题。

Result: 通过实验验证，提出的方法在遮挡、数据集多样性、大物体检测和相机参数变化等场景下显著提升了模型的泛化性能。

Conclusion: 该论文通过多角度改进，显著提升了单目3D物体检测模型的通用性，为实际应用提供了更可靠的解决方案。

Abstract: Monocular 3D object detection (Mono3D) is a fundamental computer vision task
that estimates an object's class, 3D position, dimensions, and orientation from
a single image. Its applications, including autonomous driving, augmented
reality, and robotics, critically rely on accurate 3D environmental
understanding. This thesis addresses the challenge of generalizing Mono3D
models to diverse scenarios, including occlusions, datasets, object sizes, and
camera parameters. To enhance occlusion robustness, we propose a mathematically
differentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, we
explore depth equivariant (DEVIANT) backbones. We address the issue of large
object detection, demonstrating that it's not solely a data imbalance or
receptive field problem but also a noise sensitivity issue. To mitigate this,
we introduce a segmentation-based approach in bird's-eye view with dice loss
(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3D
models to unseen camera heights and improve Mono3D generalization in such
out-of-distribution settings.

</details>


### [32] [Quantization Robustness to Input Degradations for Object Detection](https://arxiv.org/abs/2508.19600)
*Toghrul Karimov,Hassan Imani,Allan Kazakov*

Main category: cs.CV

TL;DR: 本文研究了后训练量化（PTQ）对YOLO模型在真实世界输入退化（如噪声、模糊和压缩伪影）下的鲁棒性影响，并提出了一种退化感知的校准策略。


<details>
  <summary>Details</summary>
Motivation: 研究量化对模型鲁棒性的影响，并提出改进策略以应对真实环境中的输入退化问题。

Method: 通过多种精度格式（FP32、FP16、Dynamic UINT8、Static INT8）评估YOLO模型，并引入退化感知校准策略。

Result: Static INT8在干净数据上表现良好，但退化感知校准未在大多数情况下显著提升鲁棒性，仅在大模型特定噪声条件下有效。

Conclusion: 研究揭示了增强PTQ鲁棒性的挑战，为在不可控环境中部署量化检测器提供了见解。

Abstract: Post-training quantization (PTQ) is crucial for deploying efficient object
detection models, like YOLO, on resource-constrained devices. However, the
impact of reduced precision on model robustness to real-world input
degradations such as noise, blur, and compression artifacts is a significant
concern. This paper presents a comprehensive empirical study evaluating the
robustness of YOLO models (nano to extra-large scales) across multiple
precision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8
(TensorRT). We introduce and evaluate a degradation-aware calibration strategy
for Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix
of clean and synthetically degraded images. Models were benchmarked on the COCO
dataset under seven distinct degradation conditions (including various types
and levels of noise, blur, low contrast, and JPEG compression) and a
mixed-degradation scenario. Results indicate that while Static INT8 TensorRT
engines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop
(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did
not yield consistent, broad improvements in robustness over standard clean-data
calibration across most models and degradations. A notable exception was
observed for larger model scales under specific noise conditions, suggesting
model capacity may influence the efficacy of this calibration approach. These
findings highlight the challenges in enhancing PTQ robustness and provide
insights for deploying quantized detectors in uncontrolled environments. All
code and evaluation tables are available at https://github.com/AllanK24/QRID.

</details>


### [33] [IELDG: Suppressing Domain-Specific Noise with Inverse Evolution Layers for Domain Generalized Semantic Segmentation](https://arxiv.org/abs/2508.19604)
*Qizhe Fan,Chaoyu Liu,Zhonghua Qiao,Xiaoqin Shen*

Main category: cs.CV

TL;DR: 论文提出了一种结合逆演化层（IELs）的扩散模型（IELDM）和数据增强框架，用于提升域广义语义分割（DGSS）的性能，并通过多尺度频率融合（MFF）模块增强跨域一致性。


<details>
  <summary>Details</summary>
Motivation: 解决扩散模型生成数据中的结构或语义缺陷问题，避免分割模型性能下降和错误累积。

Method: 引入IELs到生成过程和分割网络，设计IELDM和IELFormer，并加入MFF模块。

Result: 在基准数据集上表现出优于现有方法的泛化性能。

Conclusion: IELDM和IELFormer有效提升了DGSS的泛化能力，MFF模块进一步增强了跨尺度语义一致性。

Abstract: Domain Generalized Semantic Segmentation (DGSS) focuses on training a model
using labeled data from a source domain, with the goal of achieving robust
generalization to unseen target domains during inference. A common approach to
improve generalization is to augment the source domain with synthetic data
generated by diffusion models (DMs). However, the generated images often
contain structural or semantic defects due to training imperfections. Training
segmentation models with such flawed data can lead to performance degradation
and error accumulation. To address this issue, we propose to integrate inverse
evolution layers (IELs) into the generative process. IELs are designed to
highlight spatial discontinuities and semantic inconsistencies using
Laplacian-based priors, enabling more effective filtering of undesirable
generative patterns. Based on this mechanism, we introduce IELDM, an enhanced
diffusion-based data augmentation framework that can produce higher-quality
images. Furthermore, we observe that the defect-suppression capability of IELs
can also benefit the segmentation network by suppressing artifact propagation.
Based on this insight, we embed IELs into the decoder of the DGSS model and
propose IELFormer to strengthen generalization capability in cross-domain
scenarios. To further strengthen the model's semantic consistency across
scales, IELFormer incorporates a multi-scale frequency fusion (MFF) module,
which performs frequency-domain analysis to achieve structured integration of
multi-resolution features, thereby improving cross-scale coherence. Extensive
experiments on benchmark datasets demonstrate that our approach achieves
superior generalization performance compared to existing methods.

</details>


### [34] [Controllable Skin Synthesis via Lesion-Focused Vector Autoregression Model](https://arxiv.org/abs/2508.19626)
*Jiajun Sun,Zhen Yu,Siyuan Yan,Jason J. Ong,Zongyuan Ge,Lei Zhang*

Main category: cs.CV

TL;DR: LF-VAR模型通过量化病变评分和类型标签，实现了可控的皮肤图像合成，显著提升了生成图像的质量和临床相关性。


<details>
  <summary>Details</summary>
Motivation: 解决现有皮肤图像合成方法生成质量低、无法控制病变位置和类型的问题。

Method: 结合多尺度病变聚焦的VQVAE和VAR Transformer，利用病变测量和类型标签作为条件嵌入。

Result: 在七种病变类型中平均FID得分0.74，比之前最佳方法提升6.3%。

Conclusion: LF-VAR能高效生成高保真、临床相关的合成皮肤图像。

Abstract: Skin images from real-world clinical practice are often limited, resulting in
a shortage of training data for deep-learning models. While many studies have
explored skin image synthesis, existing methods often generate low-quality
images and lack control over the lesion's location and type. To address these
limitations, we present LF-VAR, a model leveraging quantified lesion
measurement scores and lesion type labels to guide the clinically relevant and
controllable synthesis of skin images. It enables controlled skin synthesis
with specific lesion characteristics based on language prompts. We train a
multiscale lesion-focused Vector Quantised Variational Auto-Encoder (VQVAE) to
encode images into discrete latent representations for structured tokenization.
Then, a Visual AutoRegressive (VAR) Transformer trained on tokenized
representations facilitates image synthesis. Lesion measurement from the lesion
region and types as conditional embeddings are integrated to enhance synthesis
fidelity. Our method achieves the best overall FID score (average 0.74) among
seven lesion types, improving upon the previous state-of-the-art (SOTA) by
6.3%. The study highlights our controllable skin synthesis model's
effectiveness in generating high-fidelity, clinically relevant synthetic skin
images. Our framework code is available at
https://github.com/echosun1996/LF-VAR.

</details>


### [35] [Divide, Weight, and Route: Difficulty-Aware Optimization with Dynamic Expert Fusion for Long-tailed Recognition](https://arxiv.org/abs/2508.19630)
*Xiaolei Wei,Yi Ouyang,Haibo Ye*

Main category: cs.CV

TL;DR: DQRoute是一个模块化框架，通过结合难度感知优化和动态专家协作，解决了长尾视觉识别中的类别不平衡和分类难度差异问题。


<details>
  <summary>Details</summary>
Motivation: 长尾视觉识别不仅面临类别不平衡问题，还因不同类别的分类难度差异而复杂化。简单的类别频率加权方法往往忽略了内在难以学习的类别。

Method: DQRoute首先基于预测不确定性和历史性能估计类别难度，并利用该信号指导自适应损失加权训练。架构上采用混合专家设计，每个专家专注于类别分布的不同区域。推理时，专家预测通过专家特定OOD检测器生成的置信度分数加权，实现输入自适应路由。

Result: 在标准长尾基准测试中，DQRoute显著提升了性能，尤其是在稀有和困难类别上。

Conclusion: DQRoute通过整合难度建模和分散式专家路由，展示了其在长尾视觉识别中的优势。

Abstract: Long-tailed visual recognition is challenging not only due to class imbalance
but also because of varying classification difficulty across categories. Simply
reweighting classes by frequency often overlooks those that are intrinsically
hard to learn. To address this, we propose \textbf{DQRoute}, a modular
framework that combines difficulty-aware optimization with dynamic expert
collaboration. DQRoute first estimates class-wise difficulty based on
prediction uncertainty and historical performance, and uses this signal to
guide training with adaptive loss weighting. On the architectural side, DQRoute
employs a mixture-of-experts design, where each expert specializes in a
different region of the class distribution. At inference time, expert
predictions are weighted by confidence scores derived from expert-specific OOD
detectors, enabling input-adaptive routing without the need for a centralized
router. All components are trained jointly in an end-to-end manner. Experiments
on standard long-tailed benchmarks demonstrate that DQRoute significantly
improves performance, particularly on rare and difficult classes, highlighting
the benefit of integrating difficulty modeling with decentralized expert
routing.

</details>


### [36] [Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception](https://arxiv.org/abs/2508.19638)
*Yang Li,Quan Yuan,Guiyang Luo,Xiaoyuan Fu,Rui Pan,Yujia Yang,Congzhang Shao,Yuewen Liu,Jinglin Li*

Main category: cs.CV

TL;DR: CoPLOT提出了一种基于点级优化的协作感知框架，通过点原生处理流程和语义感知的令牌重排序，提升了协作感知的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常使用2D鸟瞰图表示中间特征，忽略了关键的3D结构信息，影响了物体识别和定位的准确性。

Method: CoPLOT引入点级令牌作为中间表示，结合令牌重排序、序列建模和多智能体空间对齐，优化了协作感知过程。

Result: 在模拟和真实数据集上的实验表明，CoPLOT优于现有方法，且通信和计算开销更低。

Conclusion: CoPLOT通过点级优化令牌和语义感知处理，显著提升了协作感知的准确性和效率。

Abstract: Collaborative perception allows agents to enhance their perceptual
capabilities by exchanging intermediate features. Existing methods typically
organize these intermediate features as 2D bird's-eye-view (BEV)
representations, which discard critical fine-grained 3D structural cues
essential for accurate object recognition and localization. To this end, we
first introduce point-level tokens as intermediate representations for
collaborative perception. However, point-cloud data are inherently unordered,
massive, and position-sensitive, making it challenging to produce compact and
aligned point-level token sequences that preserve detailed structural
information. Therefore, we present CoPLOT, a novel Collaborative perception
framework that utilizes Point-Level Optimized Tokens. It incorporates a
point-native processing pipeline, including token reordering, sequence
modeling, and multi-agent spatial alignment. A semantic-aware token reordering
module generates adaptive 1D reorderings by leveraging scene-level and
token-level semantic information. A frequency-enhanced state space model
captures long-range sequence dependencies across both spatial and spectral
domains, improving the differentiation between foreground tokens and background
clutter. Lastly, a neighbor-to-ego alignment module applies a closed-loop
process, combining global agent-level correction with local token-level
refinement to mitigate localization noise. Extensive experiments on both
simulated and real-world datasets show that CoPLOT outperforms state-of-the-art
models, with even lower communication and computation overhead. Code will be
available at https://github.com/CheeryLeeyy/CoPLOT.

</details>


### [37] [UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks](https://arxiv.org/abs/2508.19647)
*Bikash Kumar Badatya,Vipul Baghel,Ravi Hegde*

Main category: cs.CV

TL;DR: 提出了一种轻量级、无监督的基于骨架的动作定位方法，利用时空图神经网络表示，无需标注数据即可实现高效动作分析。


<details>
  <summary>Details</summary>
Motivation: 细粒度动作定位在未修剪的运动视频中具有挑战性，现有方法依赖大量标注数据和复杂模型，计算成本高且适应性差。

Method: 使用基于注意力的时空图卷积网络（ASTGCN）预训练，通过动作动态度量（ADM）检测运动边界。

Result: 在DSV Diving数据集上达到82.66%的mAP和29.09 ms的定位延迟，性能与监督方法相当。

Conclusion: 该方法在未训练的真实场景中表现鲁棒，适用于轻量级实时动作分析系统。

Abstract: Fine-grained action localization in untrimmed sports videos presents a
significant challenge due to rapid and subtle motion transitions over short
durations. Existing supervised and weakly supervised solutions often rely on
extensive annotated datasets and high-capacity models, making them
computationally intensive and less adaptable to real-world scenarios. In this
work, we introduce a lightweight and unsupervised skeleton-based action
localization pipeline that leverages spatio-temporal graph neural
representations. Our approach pre-trains an Attention-based Spatio-Temporal
Graph Convolutional Network (ASTGCN) on a pose-sequence denoising task with
blockwise partitions, enabling it to learn intrinsic motion dynamics without
any manual labeling. At inference, we define a novel Action Dynamics Metric
(ADM), computed directly from low-dimensional ASTGCN embeddings, which detects
motion boundaries by identifying inflection points in its curvature profile.
Our method achieves a mean Average Precision (mAP) of 82.66% and average
localization latency of 29.09 ms on the DSV Diving dataset, matching
state-of-the-art supervised performance while maintaining computational
efficiency. Furthermore, it generalizes robustly to unseen, in-the-wild diving
footage without retraining, demonstrating its practical applicability for
lightweight, real-time action analysis systems in embedded or dynamic
environments.

</details>


### [38] [IDF: Iterative Dynamic Filtering Networks for Generalizable Image Denoising](https://arxiv.org/abs/2508.19649)
*Dongjin Kim,Jaekyun Ko,Muhammad Kashif Ali,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 提出了一种基于动态生成核的图像去噪方法，通过高效操作防止过拟合并提升对未知噪声的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法依赖特定噪声分布，泛化能力有限，且需要大量训练数据和计算资源。

Method: 利用特征提取模块、全局统计和局部相关性模块捕获噪声特征，通过核预测模块生成像素级变化的核进行迭代去噪。

Result: 紧凑模型（约0.04M）在多种噪声类型和级别上表现优异，即使仅用单级高斯噪声训练。

Conclusion: 迭代动态滤波在图像去噪中具有实际应用潜力。

Abstract: Image denoising is a fundamental challenge in computer vision, with
applications in photography and medical imaging. While deep learning-based
methods have shown remarkable success, their reliance on specific noise
distributions limits generalization to unseen noise types and levels. Existing
approaches attempt to address this with extensive training data and high
computational resources but they still suffer from overfitting. To address
these issues, we conduct image denoising by utilizing dynamically generated
kernels via efficient operations. This approach helps prevent overfitting and
improves resilience to unseen noise. Specifically, our method leverages a
Feature Extraction Module for robust noise-invariant features, Global
Statistics and Local Correlation Modules to capture comprehensive noise
characteristics and structural correlations. The Kernel Prediction Module then
employs these cues to produce pixel-wise varying kernels adapted to local
structures, which are then applied iteratively for denoising. This ensures both
efficiency and superior restoration quality. Despite being trained on
single-level Gaussian noise, our compact model (~ 0.04 M) excels across diverse
noise types and levels, demonstrating the promise of iterative dynamic
filtering for practical image denoising.

</details>


### [39] [Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models](https://arxiv.org/abs/2508.19650)
*Hou Xia,Zheren Fu,Fangcan Ling,Jiajun Li,Yi Tu,Zhendong Mao,Yongdong Zhang*

Main category: cs.CV

TL;DR: Video-LevelGauge是一个专门用于评估大型视频语言模型（LVLMs）位置偏见的基准测试，通过标准化探针和定制化上下文设置，揭示了27种LVLMs的显著位置偏见。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试通常评估整个视频序列的整体性能，而忽略了位置偏见等细微行为，这是LVLM性能中关键但未被充分探索的方面。

Method: 采用标准化探针和定制化上下文设置，结合统计测量和形态模式识别方法，评估438个手动策划的视频和1,297个问题。

Result: 许多领先的开源模型表现出显著的位置偏见（如头部或邻近内容偏好），而商业模型（如Gemini2.5-Pro）表现一致。

Conclusion: Video-LevelGauge为缓解偏见和指导模型改进提供了可行见解，强调了位置偏见评估的重要性。

Abstract: Large video language models (LVLMs) have made notable progress in video
understanding, spurring the development of corresponding evaluation benchmarks.
However, existing benchmarks generally assess overall performance across entire
video sequences, overlooking nuanced behaviors such as contextual positional
bias, a critical yet under-explored aspect of LVLM performance. We present
Video-LevelGauge, a dedicated benchmark designed to systematically assess
positional bias in LVLMs. We employ standardized probes and customized
contextual setups, allowing flexible control over context length, probe
position, and contextual types to simulate diverse real-world scenarios. In
addition, we introduce a comprehensive analysis method that combines
statistical measures with morphological pattern recognition to characterize
bias. Our benchmark comprises 438 manually curated videos spanning multiple
types, yielding 1,177 high-quality multiple-choice questions and 120 open-ended
questions, validated for their effectiveness in exposing positional bias. Based
on these, we evaluate 27 state-of-the-art LVLMs, including both commercial and
open-source models. Our findings reveal significant positional biases in many
leading open-source models, typically exhibiting head or neighbor-content
preferences. In contrast, commercial models such as Gemini2.5-Pro show
impressive, consistent performance across entire video sequences. Further
analyses on context length, context variation, and model scale provide
actionable insights for mitigating bias and guiding model enhancement.

</details>


### [40] [Scalable Object Detection in the Car Interior With Vision Foundation Models](https://arxiv.org/abs/2508.19651)
*Bálint Mészáros,Ahmet Firintepe,Sebastian Schmidt,Stephan Günnemann*

Main category: cs.CV

TL;DR: 论文提出了一种名为ODAL的框架，用于解决车载系统中AI任务的计算资源限制问题，通过分布式架构结合轻量级模型和微调技术，显著提升了检测和定位性能。


<details>
  <summary>Details</summary>
Motivation: 车载系统的计算资源有限，限制了直接在车内部署AI解决方案的能力，因此需要一种高效的分布式方法来处理车内场景理解任务。

Method: 采用分布式架构，将计算任务分配在车载系统和云端之间，结合轻量级模型LLaVA 1.5 7B，并通过微调提升性能。

Result: 微调后的ODAL-LLaVA模型在ODAL得分上达到89%，比基线性能提升71%，并比GPT-4o高出近20%，同时显著减少了幻觉现象。

Conclusion: ODAL框架通过分布式设计和轻量级模型微调，为车内场景理解任务提供了高效解决方案，具有潜在的新标准制定能力。

Abstract: AI tasks in the car interior like identifying and localizing externally
introduced objects is crucial for response quality of personal assistants.
However, computational resources of on-board systems remain highly constrained,
restricting the deployment of such solutions directly within the vehicle. To
address this limitation, we propose the novel Object Detection and Localization
(ODAL) framework for interior scene understanding. Our approach leverages
vision foundation models through a distributed architecture, splitting
computational tasks between on-board and cloud. This design overcomes the
resource constraints of running foundation models directly in the car. To
benchmark model performance, we introduce ODALbench, a new metric for
comprehensive assessment of detection and localization.Our analysis
demonstrates the framework's potential to establish new standards in this
domain. We compare the state-of-the-art GPT-4o vision foundation model with the
lightweight LLaVA 1.5 7B model and explore how fine-tuning enhances the
lightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA model
achieves an ODAL$_{score}$ of 89%, representing a 71% improvement over its
baseline performance and outperforming GPT-4o by nearly 20%. Furthermore, the
fine-tuned model maintains high detection accuracy while significantly reducing
hallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.

</details>


### [41] [Self-Rewarding Vision-Language Model via Reasoning Decomposition](https://arxiv.org/abs/2508.19652)
*Zongxia Li,Wenhao Yu,Chengsong Huang,Rui Liu,Zhenwen Liang,Fuxiao Liu,Jingxi Che,Dian Yu,Jordan Boyd-Graber,Haitao Mi,Dong Yu*

Main category: cs.CV

TL;DR: Vision-SR1是一种自奖励方法，通过强化学习改进视觉语言模型的视觉推理能力，减少幻觉和语言捷径。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）常因视觉幻觉和语言捷径问题而表现不佳，现有方法依赖外部监督，成本高且易导致分布偏移。

Method: Vision-SR1将VLM推理分为视觉感知和语言推理两阶段，通过自奖励机制验证视觉感知的自足性，并结合最终输出监督进行训练。

Result: 实验表明，Vision-SR1提升了视觉推理能力，减少了幻觉和语言捷径问题。

Conclusion: Vision-SR1提供了一种无需外部监督的有效方法，显著改善了视觉语言模型的性能。

Abstract: Vision-Language Models (VLMs) often suffer from visual hallucinations, saying
things that are not actually in the image, and language shortcuts, where they
skip the visual part and just rely on text priors. These issues arise because
most post-training methods for VLMs rely on simple verifiable answer matching
and supervise only final outputs, leaving intermediate visual reasoning without
explicit guidance. As a result, VLMs receive sparse visual signals and often
learn to prioritize language-based reasoning over visual perception. To
mitigate this, some existing methods add visual supervision using human
annotations or distilled labels from external large models. However, human
annotations are labor-intensive and costly, and because external signals cannot
adapt to the evolving policy, they cause distributional shifts that can lead to
reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method
that improves visual reasoning without relying on external visual supervisions
via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two
stages: visual perception and language reasoning. The model is first prompted
to produce self-contained visual perceptions that are sufficient to answer the
question without referring back the input image. To validate this
self-containment, the same VLM model is then re-prompted to perform language
reasoning using only the generated perception as input to compute reward. This
self-reward is combined with supervision on final outputs, providing a balanced
training signal that strengthens both visual perception and language reasoning.
Our experiments demonstrate that Vision-SR1 improves visual reasoning,
mitigates visual hallucinations, and reduces reliance on language shortcuts
across diverse vision-language tasks.

</details>


### [42] [Hardware-aware vs. Hardware-agnostic Energy Estimation for SNN in Space Applications](https://arxiv.org/abs/2508.19654)
*Matthias Höfflin,Jürgen Wassner*

Main category: cs.CV

TL;DR: SNNs与ANNs在能源效率上的比较显示，硬件感知分析揭示SNN仅在神经形态硬件和高输入稀疏性下显著节能。


<details>
  <summary>Details</summary>
Motivation: 质疑SNNs在数字实现中的能源效率声誉，并探索其在多输出回归任务中的表现。

Method: 使用LIF神经元的膜电位训练SNN，并与CNN在卫星数据集上比较MSE和能源消耗。

Result: SNN在神经形态硬件和高输入稀疏性下节能显著，硬件无关方法预测的节能优势不准确。

Conclusion: 需透明评估方法和明确假设披露，以确保神经网络能源效率的公平比较。

Abstract: Spiking Neural Networks (SNNs), inspired by biological intelligence, have
long been considered inherently energy-efficient, making them attractive for
resource-constrained domains such as space applications. However, recent
comparative studies with conventional Artificial Neural Networks (ANNs) have
begun to question this reputation, especially for digital implementations. This
work investigates SNNs for multi-output regression, specifically 3-D satellite
position estimation from monocular images, and compares hardware-aware and
hardware-agnostic energy estimation methods. The proposed SNN, trained using
the membrane potential of the Leaky Integrate-and-Fire (LIF) neuron in the
final layer, achieves comparable Mean Squared Error (MSE) to a reference
Convolutional Neural Network (CNN) on a photorealistic satellite dataset.
Energy analysis shows that while hardware-agnostic methods predict a consistent
50-60% energy advantage for SNNs over CNNs, hardware-aware analysis reveals
that significant energy savings are realized only on neuromorphic hardware and
with high input sparsity. The influence of dark pixel ratio on energy
consumption is quantified, emphasizing the impact of data characteristics and
hardware assumptions. These findings highlight the need for transparent
evaluation methods and explicit disclosure of underlying assumptions to ensure
fair comparisons of neural network energy efficiency.

</details>


### [43] [A Frequency-Aware Self-Supervised Learning for Ultra-Wide-Field Image Enhancement](https://arxiv.org/abs/2508.19664)
*Weicheng Liao,Zan Chen,Jianyang Xie,Yalin Zheng,Yuhui Ma,Yitian Zhao*

Main category: cs.CV

TL;DR: 提出了一种基于频率感知的自监督学习方法，用于超广角（UWF）视网膜图像增强，结合了频率解耦的去模糊和Retinex引导的照明补偿模块。


<details>
  <summary>Details</summary>
Motivation: UWF视网膜图像常因模糊和照明不均导致细节丢失，现有方法无法满足其独特需求，尤其是病理细节的保留。

Method: 采用频率解耦的去模糊模块（结合全局和局部视图）和Retinex引导的照明补偿模块（包含颜色保护单元）。

Result: 实验表明，该方法不仅提升了图像质量，还通过恢复细节和校正照明不均改善了疾病诊断性能。

Conclusion: 这是首个针对UWF图像增强的方法，为视网膜疾病管理提供了临床价值工具。

Abstract: Ultra-Wide-Field (UWF) retinal imaging has revolutionized retinal diagnostics
by providing a comprehensive view of the retina. However, it often suffers from
quality-degrading factors such as blurring and uneven illumination, which
obscure fine details and mask pathological information. While numerous retinal
image enhancement methods have been proposed for other fundus imageries, they
often fail to address the unique requirements in UWF, particularly the need to
preserve pathological details. In this paper, we propose a novel
frequency-aware self-supervised learning method for UWF image enhancement. It
incorporates frequency-decoupled image deblurring and Retinex-guided
illumination compensation modules. An asymmetric channel integration operation
is introduced in the former module, so as to combine global and local views by
leveraging high- and low-frequency information, ensuring the preservation of
fine and broader structural details. In addition, a color preservation unit is
proposed in the latter Retinex-based module, to provide multi-scale spatial and
frequency information, enabling accurate illumination estimation and
correction. Experimental results demonstrate that the proposed work not only
enhances visualization quality but also improves disease diagnosis performance
by restoring and correcting fine local details and uneven intensity. To the
best of our knowledge, this work is the first attempt for UWF image
enhancement, offering a robust and clinically valuable tool for improving
retinal disease management.

</details>


### [44] [SAT: Supervisor Regularization and Animation Augmentation for Two-process Monocular Texture 3D Human Reconstruction](https://arxiv.org/abs/2508.19688)
*Gangjian Zhang,Jian Shu,Nanjie Yao,Hao Wang*

Main category: cs.CV

TL;DR: SAT框架通过统一学习多种几何先验并引入监督特征正则化和在线动画增强模块，解决了单目纹理3D人体重建中的几何模糊性和数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 单目2D图像的几何模糊性和3D人体训练数据的稀缺性是当前3D人体重建领域的主要障碍。

Method: 提出SAT框架，包括监督特征正则化模块和在线动画增强模块，以统一学习几何先验并生成高质量3D虚拟形象。

Result: 在两个基准测试中，SAT框架优于现有方法。

Conclusion: SAT框架通过多视角网络和在线数据增强，显著提升了3D人体重建的质量和一致性。

Abstract: Monocular texture 3D human reconstruction aims to create a complete 3D
digital avatar from just a single front-view human RGB image. However, the
geometric ambiguity inherent in a single 2D image and the scarcity of 3D human
training data are the main obstacles limiting progress in this field. To
address these issues, current methods employ prior geometric estimation
networks to derive various human geometric forms, such as the SMPL model and
normal maps. However, they struggle to integrate these modalities effectively,
leading to view inconsistencies, such as facial distortions. To this end, we
propose a two-process 3D human reconstruction framework, SAT, which seamlessly
learns various prior geometries in a unified manner and reconstructs
high-quality textured 3D avatars as the final output. To further facilitate
geometry learning, we introduce a Supervisor Feature Regularization module. By
employing a multi-view network with the same structure to provide intermediate
features as training supervision, these varied geometric priors can be better
fused. To tackle data scarcity and further improve reconstruction quality, we
also propose an Online Animation Augmentation module. By building a
one-feed-forward animation network, we augment a massive number of samples from
the original 3D human data online for model training. Extensive experiments on
two benchmarks show the superiority of our approach compared to
state-of-the-art methods.

</details>


### [45] [Synthetic Image Detection via Spectral Gaps of QC-RBIM Nishimori Bethe-Hessian Operators](https://arxiv.org/abs/2508.19698)
*V. S. Usatyuk,D. A. Sapozhnikov,S. I. Egorov*

Main category: cs.CV

TL;DR: 提出了一种基于物理启发的无监督检测器，通过将合成图像识别视为稀疏加权图上的社区检测问题，实现了对GAN和扩散模型生成图像的高效检测。


<details>
  <summary>Details</summary>
Motivation: 随着深度生成模型（如GAN和扩散模型）生成的图像越来越逼真，传统的监督检测方法在面对新生成器或对抗后处理时效果下降，而无监督方法依赖的低级统计特征又过于脆弱。

Method: 使用预训练的CNN提取图像特征，并将其降维至32维，构建多边类型QC-LDPC图。通过将成对相似性转化为校准后的边耦合，形成随机键伊辛模型（RBIM），利用其Bethe-Hessian谱的特征间隙检测真实图像。

Result: 在未使用标记合成数据或重新训练特征提取器的情况下，检测器在二分类任务中达到了超过94%的准确率。

Conclusion: 该方法通过新颖的LDPC图构建和RBIM谱分析，提供了一种鲁棒的无监督合成图像检测方案，未来可扩展至视频流和多类异常检测。

Abstract: The rapid advance of deep generative models such as GANs and diffusion
networks now produces images that are virtually indistinguishable from genuine
photographs, undermining media forensics and biometric security. Supervised
detectors quickly lose effectiveness on unseen generators or after adversarial
post-processing, while existing unsupervised methods that rely on low-level
statistical cues remain fragile. We introduce a physics-inspired,
model-agnostic detector that treats synthetic-image identification as a
community-detection problem on a sparse weighted graph. Image features are
first extracted with pretrained CNNs and reduced to 32 dimensions, each feature
vector becomes a node of a Multi-Edge Type QC-LDPC graph. Pairwise similarities
are transformed into edge couplings calibrated at the Nishimori temperature,
producing a Random Bond Ising Model (RBIM) whose Bethe-Hessian spectrum
exhibits a characteristic gap when genuine community structure (real images) is
present. Synthetic images violate the Nishimori symmetry and therefore lack
such gaps. We validate the approach on binary tasks cat versus dog and male
versus female using real photos from Flickr-Faces-HQ and CelebA and synthetic
counterparts generated by GANs and diffusion models. Without any labeled
synthetic data or retraining of the feature extractor, the detector achieves
over 94% accuracy. Spectral analysis shows multiple well separated gaps for
real image sets and a collapsed spectrum for generated ones. Our contributions
are threefold: a novel LDPC graph construction that embeds deep image features,
an analytical link between Nishimori temperature RBIM and the Bethe-Hessian
spectrum providing a Bayes optimal detection criterion; and a practical,
unsupervised synthetic image detector robust to new generative architectures.
Future work will extend the framework to video streams and multi-class anomaly
detection.

</details>


### [46] [LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation](https://arxiv.org/abs/2508.19699)
*Yupeng Zhang,Dezhi Zheng,Ping Lu,Han Zhang,Lei Wang,Liping xiang,Cheng Luo,Kaijun Deng,Xiaowen Fu,Linlin Shen,Jinbao Wang*

Main category: cs.CV

TL;DR: LabelGS通过增强3D高斯表示的对象标签，实现了高效的3D场景分割，显著提升了训练速度和性能。


<details>
  <summary>Details</summary>
Motivation: 3D高斯溅射（3DGS）缺乏3D分割能力，限制了其在需要场景理解的任务中的应用。

Method: LabelGS引入了跨视图一致的语义掩码、遮挡分析模型、主高斯标记模型和高斯投影过滤器，优化了3DGS表示。

Result: LabelGS在3D场景分割任务中优于现有方法，训练速度提升了22倍。

Conclusion: LabelGS通过改进3DGS表示，实现了高效的3D场景分割，具有广泛的应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representation
for 3D scenes, offering both high-fidelity reconstruction and efficient
rendering. However, 3DGS lacks 3D segmentation ability, which limits its
applicability in tasks that require scene understanding. The identification and
isolating of specific object components is crucial. To address this limitation,
we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augments
the Gaussian representation with object label.LabelGS introduces cross-view
consistent semantic masks for 3D Gaussians and employs a novel Occlusion
Analysis Model to avoid overfitting occlusion during optimization, Main
Gaussian Labeling model to lift 2D semantic prior to 3D Gaussian and Gaussian
Projection Filter to avoid Gaussian label conflict. Our approach achieves
effective decoupling of Gaussian representations and refines the 3DGS
optimization process through a random region sampling strategy, significantly
improving efficiency. Extensive experiments demonstrate that LabelGS
outperforms previous state-of-the-art methods, including Feature-3DGS, in the
3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedup
in training compared to Feature-3DGS, at a resolution of 1440X1080. Our code
will be at https://github.com/garrisonz/LabelGS.

</details>


### [47] [FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation](https://arxiv.org/abs/2508.19705)
*Qiang Hu,Ying Zhou,Gepeng Ji,Nick Barnes,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: FreeVPS通过结合IPS模型的空间上下文和SAM2的时间建模能力，解决了视频息肉分割中的时空建模与领域泛化问题，并通过两个无训练模块提升了分割稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有视频息肉分割（VPS）方法在时空建模和领域泛化之间难以平衡，限制了其在真实临床场景中的应用。

Method: 将VPS任务重新定义为跟踪-检测范式，结合IPS模型的空间上下文和SAM2的时间建模能力，并通过两个无训练模块（内部关联过滤和跨关联细化）减少错误积累。

Result: FreeVPS在领域内和领域外场景中均实现了最先进的性能，并在长未修剪的结肠镜视频中展示了强大的跟踪能力。

Conclusion: FreeVPS通过无训练模块稳定了SAM2，提升了分割稳定性，展示了其在临床分析中的可靠潜力。

Abstract: Existing video polyp segmentation (VPS) paradigms usually struggle to balance
between spatiotemporal modeling and domain generalization, limiting their
applicability in real clinical scenarios. To embrace this challenge, we recast
the VPS task as a track-by-detect paradigm that leverages the spatial contexts
captured by the image polyp segmentation (IPS) model while integrating the
temporal modeling capabilities of segment anything model 2 (SAM2). However,
during long-term polyp tracking in colonoscopy videos, SAM2 suffers from error
accumulation, resulting in a snowball effect that compromises segmentation
stability. We mitigate this issue by repurposing SAM2 as a video polyp
segmenter with two training-free modules. In particular, the intra-association
filtering module eliminates spatial inaccuracies originating from the detecting
stage, reducing false positives. The inter-association refinement module
adaptively updates the memory bank to prevent error propagation over time,
enhancing temporal coherence. Both modules work synergistically to stabilize
SAM2, achieving cutting-edge performance in both in-domain and out-of-domain
scenarios. Furthermore, we demonstrate the robust tracking capabilities of
FreeVPS in long-untrimmed colonoscopy videos, underscoring its potential
reliable clinical analysis.

</details>


### [48] [Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning](https://arxiv.org/abs/2508.19730)
*Stelios Mylonas,Symeon Papadopoulos*

Main category: cs.CV

TL;DR: 提出了一种基于面部基础模型的鲁棒视频深度伪造检测框架，通过自监督学习和集成数据集微调，结合三元组损失和属性监督，显著提升了泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术日益逼真且易获取，威胁媒体真实性和信息完整性。现有检测模型在训练分布外泛化能力不足，尤其在真实场景中表现不佳。

Method: 基于自监督面部基础模型FSFM，通过集成多种深度伪造数据集微调，引入三元组损失变体和属性监督方案，增强模型判别力。

Result: 在多样化评估基准上表现出色，尤其在真实场景中具有强泛化能力。

Conclusion: 该方法通过结合面部基础模型和多任务学习，显著提升了深度伪造检测的鲁棒性和泛化能力。

Abstract: The increasing realism and accessibility of deepfakes have raised critical
concerns about media authenticity and information integrity. Despite recent
advances, deepfake detection models often struggle to generalize beyond their
training distributions, particularly when applied to media content found in the
wild. In this work, we present a robust video deepfake detection framework with
strong generalization that takes advantage of the rich facial representations
learned by face foundation models. Our method is built on top of FSFM, a
self-supervised model trained on real face data, and is further fine-tuned
using an ensemble of deepfake datasets spanning both face-swapping and
face-reenactment manipulations. To enhance discriminative power, we incorporate
triplet loss variants during training, guiding the model to produce more
separable embeddings between real and fake samples. Additionally, we explore
attribution-based supervision schemes, where deepfakes are categorized by
manipulation type or source dataset, to assess their impact on generalization.
Extensive experiments across diverse evaluation benchmarks demonstrate the
effectiveness of our approach, especially in challenging real-world scenarios.

</details>


### [49] [POEv2: a flexible and robust framework for generic line segment detection and wireframe line segment detection](https://arxiv.org/abs/2508.19742)
*Chenguang Liu,Chisheng Wang,Yuhua Cai,Chuanhua Zhu,Qingquan Li*

Main category: cs.CV

TL;DR: 提出了一种名为POEv2的鲁棒框架，适用于通用线段检测和线框线段检测，结合高效边缘检测器在三个公开数据集上达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有线段检测器分为通用和线框两类，各自设计目标不同导致性能不理想，需要一种能同时满足两种任务的框架。

Method: 改进Pixel Orientation Estimation (POE)方法，提出POEv2，从边缘强度图中检测线段，可与任何边缘检测器结合。

Result: 结合高效边缘检测器后，POEv2在三个公开数据集上达到最先进性能。

Conclusion: POEv2是一种鲁棒且通用的线段检测框架，适用于多种任务。

Abstract: Line segment detection in images has been studied for several decades.
Existing line segment detectors can be roughly divided into two categories:
generic line segment detectors and wireframe line segment detectors. Generic
line segment detectors aim to detect all meaningful line segments in images and
traditional approaches usually fall into this category. Recent deep learning
based approaches are mostly wireframe line segment detectors. They detect only
line segments that are geometrically meaningful and have large spatial support.
Due to the difference in the aim of design, the performance of generic line
segment detectors for the task of wireframe line segment detection won't be
satisfactory, and vice versa. In this work, we propose a robust framework that
can be used for both generic line segment detection and wireframe line segment
detection. The proposed method is an improved version of the Pixel Orientation
Estimation (POE) method. It is thus named as POEv2. POEv2 detects line segments
from edge strength maps, and can be combined with any edge detector. We show in
our experiments that by combining the proposed POEv2 with an efficient edge
detector, it achieves state-of-the-art performance on three publicly available
datasets.

</details>


### [50] [SPLF-SAM: Self-Prompting Segment Anything Model for Light Field Salient Object Detection](https://arxiv.org/abs/2508.19746)
*Qiyao Xu,Qiming Wu,Xiaowei Li*

Main category: cs.CV

TL;DR: SPLF-SAM模型通过UMFEB和MAFA模块解决了LF SOD任务中提示信息提取和频域信息分析的不足，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型在LF SOD任务中忽视提示信息提取和频域信息分析，导致小目标被噪声淹没。

Method: 提出SPLF-SAM模型，包含UMFEB（统一多尺度特征嵌入块）和MAFA（多尺度自适应滤波适配器）。

Result: 实验表明，SPLF-SAM在十种SOTA方法中表现最优。

Conclusion: SPLF-SAM通过结合提示信息和频域分析，显著提升了LF SOD任务的性能。

Abstract: Segment Anything Model (SAM) has demonstrated remarkable capabilities in
solving light field salient object detection (LF SOD). However, most existing
models tend to neglect the extraction of prompt information under this task.
Meanwhile, traditional models ignore the analysis of frequency-domain
information, which leads to small objects being overwhelmed by noise. In this
paper, we put forward a novel model called self-prompting light field segment
anything model (SPLF-SAM), equipped with unified multi-scale feature embedding
block (UMFEB) and a multi-scale adaptive filtering adapter (MAFA). UMFEB is
capable of identifying multiple objects of varying sizes, while MAFA, by
learning frequency features, effectively prevents small objects from being
overwhelmed by noise. Extensive experiments have demonstrated the superiority
of our method over ten state-of-the-art (SOTA) LF SOD methods. Our code will be
available at https://github.com/XucherCH/splfsam.

</details>


### [51] [FastAvatar: Towards Unified Fast High-Fidelity 3D Avatar Reconstruction with Large Gaussian Reconstruction Transformers](https://arxiv.org/abs/2508.19754)
*Yue Wu,Yufan Wu,Wen Li,Yuxi Lu,Kairui Feng,Xuanhong Chen*

Main category: cs.CV

TL;DR: FastAvatar是一个快速3D头像重建框架，通过统一模型高效利用多种日常记录数据，在几秒内重建高质量3D高斯模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有3D头像重建方法的高时间复杂性、数据质量敏感性和低数据利用率问题。

Method: 采用大型高斯重建Transformer，结合多帧线索、多粒度引导编码和增量高斯聚合技术。

Result: 实验表明FastAvatar在质量和速度上优于现有方法。

Conclusion: FastAvatar提供了一种高质量、快速且可调的头像建模范式。

Abstract: Despite significant progress in 3D avatar reconstruction, it still faces
challenges such as high time complexity, sensitivity to data quality, and low
data utilization. We propose FastAvatar, a feedforward 3D avatar framework
capable of flexibly leveraging diverse daily recordings (e.g., a single image,
multi-view observations, or monocular video) to reconstruct a high-quality 3D
Gaussian Splatting (3DGS) model within seconds, using only a single unified
model. FastAvatar's core is a Large Gaussian Reconstruction Transformer
featuring three key designs: First, a variant VGGT-style transformer
architecture aggregating multi-frame cues while injecting initial 3D prompt to
predict an aggregatable canonical 3DGS representation; Second, multi-granular
guidance encoding (camera pose, FLAME expression, head pose) mitigating
animation-induced misalignment for variable-length inputs; Third, incremental
Gaussian aggregation via landmark tracking and sliced fusion losses.
Integrating these features, FastAvatar enables incremental reconstruction,
i.e., improving quality with more observations, unlike prior work wasting input
data. This yields a quality-speed-tunable paradigm for highly usable avatar
modeling. Extensive experiments show that FastAvatar has higher quality and
highly competitive speed compared to existing methods.

</details>


### [52] [BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions](https://arxiv.org/abs/2508.19762)
*Ahmed Emam,Mohamed Elbassiouny,Julius Miller,Patrick Donworth,Sabine Seidel,Ribana Roscher*

Main category: cs.CV

TL;DR: BuzzSet是一个用于自动化传粉昆虫监测的大规模高分辨率图像数据集，包含7856张手动验证和标注的图像，为生态计算机视觉和小物体检测提供了基准。


<details>
  <summary>Details</summary>
Motivation: 传粉昆虫种群数量下降，需要可扩展的自动化监测方法。

Method: 使用YOLOv12模型生成初始标注，并通过人工验证和开源标注工具进行优化，图像预处理为256×256的图块。采用RF-DETR基于变压器的目标检测器提供基线。

Result: 模型在蜜蜂和大黄蜂类别上分别达到0.94和0.92的F1分数，mAP@0.50为0.559。未识别类别的表现较差。

Conclusion: BuzzSet为小物体检测、标签噪声下的类别分离和生态计算机视觉提供了有价值的基准。

Abstract: Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
increasing anthropogenic and environmental stressors. To support scalable,
automated pollinator monitoring, we introduce BuzzSet, a new large-scale
dataset of high-resolution pollinator images collected in real agricultural
field conditions. BuzzSet contains 7856 manually verified and labeled images,
with over 8000 annotated instances across three classes: honeybees, bumblebees,
and unidentified insects. Initial annotations were generated using a YOLOv12
model trained on external data and refined via human verification using
open-source labeling tools. All images were preprocessed into 256~$\times$~256
tiles to improve the detection of small insects. We provide strong baselines
using the RF-DETR transformer-based object detector. The model achieves high
F1-scores of 0.94 and 0.92 for honeybee and bumblebee classes, respectively,
with confusion matrix results showing minimal misclassification between these
categories. The unidentified class remains more challenging due to label
ambiguity and lower sample frequency, yet still contributes useful insights for
robustness evaluation. Overall detection quality is strong, with a best
mAP@0.50 of 0.559. BuzzSet offers a valuable benchmark for small object
detection, class separation under label noise, and ecological computer vision.

</details>


### [53] [AIM: Adaptive Intra-Network Modulation for Balanced Multimodal Learning](https://arxiv.org/abs/2508.19769)
*Shu Shen,C. L. Philip Chen,Tong Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为AIM的方法，通过自适应调整网络内参数和深度的优化状态，解决了多模态学习中的不平衡问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在平衡多模态学习时，通常会抑制主导模态的学习，从而影响整体性能。本文发现这是由于网络内部的优化偏差导致的。

Method: 提出了AIM方法，通过将主导模态的未优化参数解耦到辅助块中，并鼓励依赖这些性能下降的块与较弱模态联合训练，同时自适应调整不同深度的调制强度。

Result: 实验表明，AIM在多个基准测试中优于现有方法，并在不同主干网络、融合策略和优化器上表现出强泛化能力。

Conclusion: AIM首次实现了在不抑制任何模态的情况下平衡多模态学习，为多模态学习中的不平衡问题提供了有效解决方案。

Abstract: Multimodal learning has significantly enhanced machine learning performance
but still faces numerous challenges and limitations. Imbalanced multimodal
learning is one of the problems extensively studied in recent works and is
typically mitigated by modulating the learning of each modality. However, we
find that these methods typically hinder the dominant modality's learning to
promote weaker modalities, which affects overall multimodal performance. We
analyze the cause of this issue and highlight a commonly overlooked problem:
optimization bias within networks. To address this, we propose Adaptive
Intra-Network Modulation (AIM) to improve balanced modality learning. AIM
accounts for differences in optimization state across parameters and depths
within the network during modulation, achieving balanced multimodal learning
without hindering either dominant or weak modalities for the first time.
Specifically, AIM decouples the dominant modality's under-optimized parameters
into Auxiliary Blocks and encourages reliance on these performance-degraded
blocks for joint training with weaker modalities. This approach effectively
prevents suppression of weaker modalities while enabling targeted optimization
of under-optimized parameters to improve the dominant modality. Additionally,
AIM assesses modality imbalance level across network depths and adaptively
adjusts modulation strength at each depth. Experimental results demonstrate
that AIM outperforms state-of-the-art imbalanced modality learning methods
across multiple benchmarks and exhibits strong generalizability across
different backbones, fusion strategies, and optimizers.

</details>


### [54] [The Return of Structural Handwritten Mathematical Expression Recognition](https://arxiv.org/abs/2508.19773)
*Jakob Seitz,Tobias Lengfeld,Radu Timofte*

Main category: cs.CV

TL;DR: 本文提出了一种结构化的手写数学表达式识别方法，通过自动标注和模块化系统，实现了符号与轨迹的显式对齐，提升了错误分析和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现代编码器-解码器架构虽然擅长生成LaTeX，但缺乏符号与轨迹的显式对齐，限制了错误分析和交互式应用的需求。

Method: 1. 使用神经网络自动标注LaTeX方程到原始轨迹，生成符号分割、分类和空间关系注释；2. 模块化结构识别系统独立优化分割、分类和关系预测。

Result: 在CROHME-2023基准测试中表现优异，生成完整的图结构，直接关联手写轨迹与预测符号。

Conclusion: 该方法通过结构化解码和自动标注，显著提升了手写数学表达式识别的透明度和可解释性。

Abstract: Handwritten Mathematical Expression Recognition is foundational for
educational technologies, enabling applications like digital note-taking and
automated grading. While modern encoder-decoder architectures with large
language models excel at LaTeX generation, they lack explicit symbol-to-trace
alignment, a critical limitation for error analysis, interpretability, and
spatially aware interactive applications requiring selective content updates.
This paper introduces a structural recognition approach with two innovations: 1
an automatic annotation system that uses a neural network to map LaTeX
equations to raw traces, automatically generating annotations for symbol
segmentation, classification, and spatial relations, and 2 a modular structural
recognition system that independently optimizes segmentation, classification,
and relation prediction. By leveraging a dataset enriched with structural
annotations from our auto-labeling system, the proposed recognition system
combines graph-based trace sorting, a hybrid convolutional-recurrent network,
and transformer-based correction to achieve competitive performance on the
CROHME-2023 benchmark. Crucially, our structural recognition system generates a
complete graph structure that directly links handwritten traces to predicted
symbols, enabling transparent error analysis and interpretable outputs.

</details>


### [55] [MAPo : Motion-Aware Partitioning of Deformable 3D Gaussian Splatting for High-Fidelity Dynamic Scene Reconstruction](https://arxiv.org/abs/2508.19786)
*Han Jiao,Jiakai Sun,Yexing Xu,Lei Zhao,Wei Xing,Huaizhong Lin*

Main category: cs.CV

TL;DR: MAPo框架通过动态分区策略和跨帧一致性损失，解决了3D高斯泼溅在动态场景重建中的模糊和细节丢失问题，提升了渲染质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于变形的3D高斯泼溅方法在动态场景重建中因单一模型限制导致模糊和细节丢失，需要更精细的运动建模方法。

Method: 提出动态分数分区策略，区分高动态和低动态3D高斯，对高动态部分递归分区并复制变形网络，同时引入跨帧一致性损失。

Result: MAPo在复杂或快速运动区域实现了优于基线的渲染质量，同时保持计算成本。

Conclusion: MAPo通过分区和一致性损失有效提升了动态场景重建的渲染质量，适用于复杂运动场景。

Abstract: 3D Gaussian Splatting, known for enabling high-quality static scene
reconstruction with fast rendering, is increasingly being applied to dynamic
scene reconstruction. A common strategy involves learning a deformation field
to model the temporal changes of a canonical set of 3D Gaussians. However,
these deformation-based methods often produce blurred renderings and lose fine
motion details in highly dynamic regions due to the inherent limitations of a
single, unified model in representing diverse motion patterns. To address these
challenges, we introduce Motion-Aware Partitioning of Deformable 3D Gaussian
Splatting (MAPo), a novel framework for high-fidelity dynamic scene
reconstruction. Its core is a dynamic score-based partitioning strategy that
distinguishes between high- and low-dynamic 3D Gaussians. For high-dynamic 3D
Gaussians, we recursively partition them temporally and duplicate their
deformation networks for each new temporal segment, enabling specialized
modeling to capture intricate motion details. Concurrently, low-dynamic 3DGs
are treated as static to reduce computational costs. However, this temporal
partitioning strategy for high-dynamic 3DGs can introduce visual
discontinuities across frames at the partition boundaries. To address this, we
introduce a cross-frame consistency loss, which not only ensures visual
continuity but also further enhances rendering quality. Extensive experiments
demonstrate that MAPo achieves superior rendering quality compared to baselines
while maintaining comparable computational costs, particularly in regions with
complex or rapid motions.

</details>


### [56] [StableIntrinsic: Detail-preserving One-step Diffusion Model for Multi-view Material Estimation](https://arxiv.org/abs/2508.19789)
*Xiuchao Wu,Pengfei Zhu,Jiangjing Lyu,Xinguo Liu,Jie Guo,Yanwen Guo,Weiwei Xu,Chengfei Lyu*

Main category: cs.CV

TL;DR: StableIntrinsic提出了一种一步扩散模型，用于多视角材料估计，解决了传统扩散模型的多步去噪耗时和高方差问题。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多步去噪过程中耗时且结果方差高，不适合确定性材料估计任务。

Method: 采用一步扩散模型，结合像素空间损失函数和细节注入网络（DIN）提升细节保留。

Result: 在PSNR和MSE指标上显著优于现有方法，分别提升9.9%和降低44.4%（金属）、60.0%（粗糙度）。

Conclusion: StableIntrinsic在材料估计任务中实现了高效、低方差的高质量结果。

Abstract: Recovering material information from images has been extensively studied in
computer graphics and vision. Recent works in material estimation leverage
diffusion model showing promising results. However, these diffusion-based
methods adopt a multi-step denoising strategy, which is time-consuming for each
estimation. Such stochastic inference also conflicts with the deterministic
material estimation task, leading to a high variance estimated results. In this
paper, we introduce StableIntrinsic, a one-step diffusion model for multi-view
material estimation that can produce high-quality material parameters with low
variance. To address the overly-smoothing problem in one-step diffusion,
StableIntrinsic applies losses in pixel space, with each loss designed based on
the properties of the material. Additionally, StableIntrinsic introduces a
Detail Injection Network (DIN) to eliminate the detail loss caused by VAE
encoding, while further enhancing the sharpness of material prediction results.
The experimental results indicate that our method surpasses the current
state-of-the-art techniques by achieving a $9.9\%$ improvement in the Peak
Signal-to-Noise Ratio (PSNR) of albedo, and by reducing the Mean Square Error
(MSE) for metallic and roughness by $44.4\%$ and $60.0\%$, respectively.

</details>


### [57] [Not Every Gift Comes in Gold Paper or with a Red Ribbon: Exploring Color Perception in Text-to-Image Models](https://arxiv.org/abs/2508.19791)
*Shay Shomer Chai,Wenxuan Peng,Bharath Hariharan,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文研究了文本到图像生成中多对象颜色属性的语义对齐问题，并提出了一种新的图像编辑技术来改善这一问题。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法在处理复杂多对象提示时难以准确捕捉语义，尤其是颜色属性。现有方法依赖粗粒度指标或人工评估，难以大规模应用。

Method: 通过案例研究分析颜色属性，提出了一种专用的图像编辑技术，以解决多对象语义对齐问题。

Result: 实验表明，该方法显著提升了多颜色提示下的图像生成性能，优于现有推理技术和编辑方法。

Conclusion: 本文提出的技术有效解决了多对象颜色属性的语义对齐问题，为文本到图像生成提供了更可靠的解决方案。

Abstract: Text-to-image generation has recently seen remarkable success, granting users
with the ability to create high-quality images through the use of text.
However, contemporary methods face challenges in capturing the precise
semantics conveyed by complex multi-object prompts. Consequently, many works
have sought to mitigate such semantic misalignments, typically via
inference-time schemes that modify the attention layers of the denoising
networks. However, prior work has mostly utilized coarse metrics, such as the
cosine similarity between text and image CLIP embeddings, or human evaluations,
which are challenging to conduct on a larger-scale. In this work, we perform a
case study on colors -- a fundamental attribute commonly associated with
objects in text prompts, which offer a rich test bed for rigorous evaluation.
Our analysis reveals that pretrained models struggle to generate images that
faithfully reflect multiple color attributes-far more so than with single-color
prompts-and that neither inference-time techniques nor existing editing methods
reliably resolve these semantic misalignments. Accordingly, we introduce a
dedicated image editing technique, mitigating the issue of multi-object
semantic alignment for prompts containing multiple colors. We demonstrate that
our approach significantly boosts performance over a wide range of metrics,
considering images generated by various text-to-image diffusion-based
techniques.

</details>


### [58] [FusionSort: Enhanced Cluttered Waste Segmentation with Advanced Decoding and Comprehensive Modality Optimization](https://arxiv.org/abs/2508.19798)
*Muhammad Ali,Omar Ali AlSuwaidi*

Main category: cs.CV

TL;DR: 提出了一种基于Encoder-Decoder结构的增强神经网络架构，用于提高非生物降解废物分类的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 废物管理中非生物降解材料的自动分类因复杂性和多样性而面临挑战，需要更高效的解决方案。

Method: 结合了Comprehensive Attention Block、Mamba架构的注意力机制和Data Fusion Block，通过PCA降维处理多通道图像数据。

Result: 在RGB、高光谱、多光谱及其组合数据上测试，性能显著优于现有方法。

Conclusion: 该模型在废物分类任务中表现出色，为自动化废物管理提供了有效工具。

Abstract: In the realm of waste management, automating the sorting process for
non-biodegradable materials presents considerable challenges due to the
complexity and variability of waste streams. To address these challenges, we
introduce an enhanced neural architecture that builds upon an existing
Encoder-Decoder structure to improve the accuracy and efficiency of waste
sorting systems. Our model integrates several key innovations: a Comprehensive
Attention Block within the decoder, which refines feature representations by
combining convolutional and upsampling operations. In parallel, we utilize
attention through the Mamba architecture, providing an additional performance
boost. We also introduce a Data Fusion Block that fuses images with more than
three channels. To achieve this, we apply PCA transformation to reduce the
dimensionality while retaining the maximum variance and essential information
across three dimensions, which are then used for further processing. We
evaluated the model on RGB, hyperspectral, multispectral, and a combination of
RGB and hyperspectral data. The results demonstrate that our approach
outperforms existing methods by a significant margin.

</details>


### [59] [A bag of tricks for real-time Mitotic Figure detection](https://arxiv.org/abs/2508.19804)
*Christian Marzahl,Brian Napora*

Main category: cs.CV

TL;DR: 本文提出了一种用于组织病理学图像中有丝分裂象（MF）检测的鲁棒实时方法，通过多领域训练数据、平衡采样和增强技术，结合RTMDet单阶段目标检测器，实现了高推理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于扫描仪、染色协议、组织类型和伪影的多样性，MF检测具有挑战性，需要一种能够适应不同领域的鲁棒方法。

Method: 基于RTMDet单阶段目标检测器，采用多领域训练数据、平衡采样、增强技术和硬负样本挖掘来提升性能。

Result: 在多个MF数据集上，模型F1分数为0.78至0.84；在MIDOG 2025挑战赛测试集上F1为0.81，优于更大模型。

Conclusion: 该方法在准确性和速度之间取得了平衡，适合临床实际应用。

Abstract: Mitotic figure (MF) detection in histopathology images is challenging due to
large variations in slide scanners, staining protocols, tissue types, and the
presence of artifacts. This paper presents a collection of training techniques
- a bag of tricks - that enable robust, real-time MF detection across diverse
domains. We build on the efficient RTMDet single stage object detector to
achieve high inference speed suitable for clinical deployment. Our method
addresses scanner variability and tumor heterogeneity via extensive
multi-domain training data, balanced sampling, and careful augmentation.
Additionally, we employ targeted, hard negative mining on necrotic and debris
tissue to reduce false positives. In a grouped 5-fold cross-validation across
multiple MF datasets, our model achieves an F1 score between 0.78 and 0.84. On
the preliminary test set of the MItosis DOmain Generalization (MIDOG) 2025
challenge, our single-stage RTMDet-S based approach reaches an F1 of 0.81,
outperforming larger models and demonstrating adaptability to new, unfamiliar
domains. The proposed solution offers a practical trade-off between accuracy
and speed, making it attractive for real-world clinical adoption.

</details>


### [60] [Context-aware Sparse Spatiotemporal Learning for Event-based Vision](https://arxiv.org/abs/2508.19806)
*Shenqi Wang,Guangzhi Tang*

Main category: cs.CV

TL;DR: CSSL框架通过动态调节神经元激活，高效处理事件数据，适用于资源受限的边缘应用。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法未能充分利用事件数据的稀疏性，而神经形态计算在复杂任务中性能不足。

Method: 提出CSSL框架，通过上下文感知阈值动态调节神经元激活，无需显式稀疏约束。

Result: 在事件目标检测和光流估计任务中，CSSL性能优于或接近现有方法，同时保持高神经元稀疏性。

Conclusion: CSSL为神经形态处理提供了高效的事件视觉解决方案。

Abstract: Event-based camera has emerged as a promising paradigm for robot perception,
offering advantages with high temporal resolution, high dynamic range, and
robustness to motion blur. However, existing deep learning-based event
processing methods often fail to fully leverage the sparse nature of event
data, complicating their integration into resource-constrained edge
applications. While neuromorphic computing provides an energy-efficient
alternative, spiking neural networks struggle to match of performance of
state-of-the-art models in complex event-based vision tasks, like object
detection and optical flow. Moreover, achieving high activation sparsity in
neural networks is still difficult and often demands careful manual tuning of
sparsity-inducing loss terms. Here, we propose Context-aware Sparse
Spatiotemporal Learning (CSSL), a novel framework that introduces context-aware
thresholding to dynamically regulate neuron activations based on the input
distribution, naturally reducing activation density without explicit sparsity
constraints. Applied to event-based object detection and optical flow
estimation, CSSL achieves comparable or superior performance to
state-of-the-art methods while maintaining extremely high neuronal sparsity.
Our experimental results highlight CSSL's crucial role in enabling efficient
event-based vision for neuromorphic processing.

</details>


### [61] [AutoQ-VIS: Improving Unsupervised Video Instance Segmentation via Automatic Quality Assessment](https://arxiv.org/abs/2508.19808)
*Kaixuan Lu,Mehmet Onurcan Kaya,Dim P. Papadopoulos*

Main category: cs.CV

TL;DR: AutoQ-VIS是一种无监督视频实例分割框架，通过质量引导的自训练方法，解决了合成数据与真实视频之间的领域差距问题，并在YouTubeVIS-2019数据集上取得了52.6 AP50的优异性能。


<details>
  <summary>Details</summary>
Motivation: 视频实例分割（VIS）需要像素级掩码和时间一致性标签，标注成本高。现有无监督方法依赖合成数据，但存在合成与真实视频的领域差距问题。

Method: AutoQ-VIS通过质量引导的自训练方法，构建了一个伪标签生成与自动质量评估的闭环系统，逐步从合成数据适应到真实视频。

Result: 在YouTubeVIS-2019验证集上达到52.6 AP50，比之前的SOTA方法VideoCutLER提升了4.4%，且无需人工标注。

Conclusion: AutoQ-VIS证明了质量感知自训练在无监督VIS中的可行性，为减少标注依赖提供了新思路。

Abstract: Video Instance Segmentation (VIS) faces significant annotation challenges due
to its dual requirements of pixel-level masks and temporal consistency labels.
While recent unsupervised methods like VideoCutLER eliminate optical flow
dependencies through synthetic data, they remain constrained by the
synthetic-to-real domain gap. We present AutoQ-VIS, a novel unsupervised
framework that bridges this gap through quality-guided self-training. Our
approach establishes a closed-loop system between pseudo-label generation and
automatic quality assessment, enabling progressive adaptation from synthetic to
real videos. Experiments demonstrate state-of-the-art performance with 52.6
$\text{AP}_{50}$ on YouTubeVIS-2019 val set, surpassing the previous
state-of-the-art VideoCutLER by 4.4$\%$, while requiring no human annotations.
This demonstrates the viability of quality-aware self-training for unsupervised
VIS. The source code of our method is available at
https://github.com/wcbup/AutoQ-VIS.

</details>


### [62] [ERSR: An Ellipse-constrained pseudo-label refinement and symmetric regularization framework for semi-supervised fetal head segmentation in ultrasound images](https://arxiv.org/abs/2508.19815)
*Linkuan Zhou,Zhexin Chen,Yufei Shen,Junlin Xu,Ping Xuan,Yixin Zhu,Yuqi Fang,Cong Cong,Leyi Wei,Ran Su,Jia Zhou,Qiangguo Jin*

Main category: cs.CV

TL;DR: 提出了一种名为ERSR的半监督框架，用于胎儿头部超声图像分割，通过双评分自适应过滤策略、椭圆约束伪标签精炼和对称性多一致性正则化，显著提升了分割性能。


<details>
  <summary>Details</summary>
Motivation: 胎儿头部超声图像分割因图像质量差和标注数据不足而具有挑战性，现有半监督方法难以生成可靠的伪标签和一致性约束。

Method: ERSR框架包含双评分自适应过滤策略（评估和过滤教师输出）、椭圆约束伪标签精炼（通过最小二乘椭圆拟合优化伪标签）和对称性多一致性正则化（多级一致性约束）。

Result: 在HC18和PSFH数据集上，使用10%和20%标注数据时，Dice分数分别达到92.05%/95.36%和91.68%/93.70%。

Conclusion: ERSR框架通过创新的策略和正则化方法，显著提升了半监督胎儿头部超声图像分割的性能，达到最先进水平。

Abstract: Automated segmentation of the fetal head in ultrasound images is critical for
prenatal monitoring. However, achieving robust segmentation remains challenging
due to the poor quality of ultrasound images and the lack of annotated data.
Semi-supervised methods alleviate the lack of annotated data but struggle with
the unique characteristics of fetal head ultrasound images, making it
challenging to generate reliable pseudo-labels and enforce effective
consistency regularization constraints. To address this issue, we propose a
novel semi-supervised framework, ERSR, for fetal head ultrasound segmentation.
Our framework consists of the dual-scoring adaptive filtering strategy, the
ellipse-constrained pseudo-label refinement, and the symmetry-based multiple
consistency regularization. The dual-scoring adaptive filtering strategy uses
boundary consistency and contour regularity criteria to evaluate and filter
teacher outputs. The ellipse-constrained pseudo-label refinement refines these
filtered outputs by fitting least-squares ellipses, which strengthens pixels
near the center of the fitted ellipse and suppresses noise simultaneously. The
symmetry-based multiple consistency regularization enforces multi-level
consistency across perturbed images, symmetric regions, and between original
predictions and pseudo-labels, enabling the model to capture robust and stable
shape representations. Our method achieves state-of-the-art performance on two
benchmarks. On the HC18 dataset, it reaches Dice scores of 92.05% and 95.36%
with 10% and 20% labeled data, respectively. On the PSFH dataset, the scores
are 91.68% and 93.70% under the same settings.

</details>


### [63] [Gradient Rectification for Robust Calibration under Distribution Shift](https://arxiv.org/abs/2508.19830)
*Yilin Zhang,Cai Xu,You Wu,Ziyu Guan,Wei Zhao*

Main category: cs.CV

TL;DR: 提出了一种无需目标域信息的新型校准框架，通过低频滤波和梯度修正机制，显著提升了分布偏移下的校准性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在安全关键应用中常因预测过度自信而不可靠，现有方法依赖目标域信息，实用性受限。

Method: 从频域视角分析分布偏移，提出低频滤波策略和梯度修正机制，以优化校准性能。

Result: 在合成和真实数据集（如CIFAR-10/100-C和WILDS）上验证了方法的有效性。

Conclusion: 该方法在分布偏移下显著提升校准性能，同时保持强内分布性能。

Abstract: Deep neural networks often produce overconfident predictions, undermining
their reliability in safety-critical applications. This miscalibration is
further exacerbated under distribution shift, where test data deviates from the
training distribution due to environmental or acquisition changes. While
existing approaches improve calibration through training-time regularization or
post-hoc adjustment, their reliance on access to or simulation of target
domains limits their practicality in real-world scenarios. In this paper, we
propose a novel calibration framework that operates without access to target
domain information. From a frequency-domain perspective, we identify that
distribution shifts often distort high-frequency visual cues exploited by deep
models, and introduce a low-frequency filtering strategy to encourage reliance
on domain-invariant features. However, such information loss may degrade
In-Distribution (ID) calibration performance. Therefore, we further propose a
gradient-based rectification mechanism that enforces ID calibration as a hard
constraint during optimization. Experiments on synthetic and real-world shifted
datasets, including CIFAR-10/100-C and WILDS, demonstrate that our method
significantly improves calibration under distribution shift while maintaining
strong in-distribution performance.

</details>


### [64] [Image Quality Assessment for Machines: Paradigm, Large-scale Database, and Models](https://arxiv.org/abs/2508.19850)
*Xiaoqi Wang,Yun Zhang,Weisi Lin*

Main category: cs.CV

TL;DR: 提出了一种机器中心的图像质量评估（MIQA）框架，用于量化图像退化对机器视觉系统（MVS）性能的影响，并构建了一个包含250万样本的数据库（MIQD-2.5M）。


<details>
  <summary>Details</summary>
Motivation: 机器视觉系统在恶劣视觉条件下性能下降，现有的人类视觉系统（HVS）评估方法不适用于MVS。

Method: 建立了MIQA范式，包括端到端评估流程，并提出了区域感知的MIQA（RA-MIQA）模型进行细粒度空间退化分析。

Result: RA-MIQA在多个维度上表现优异，例如在图像分类任务中一致性（SRCC）提升13.56%，准确性提升13.37%。

Conclusion: HVS评估方法不适用于MVS，而MIQA模型在背景退化、准确性估计和细微失真方面仍有挑战。该研究为机器中心的图像处理和优化奠定了基础。

Abstract: Machine vision systems (MVS) are intrinsically vulnerable to performance
degradation under adverse visual conditions. To address this, we propose a
machine-centric image quality assessment (MIQA) framework that quantifies the
impact of image degradations on MVS performance. We establish an MIQA paradigm
encompassing the end-to-end assessment workflow. To support this, we construct
a machine-centric image quality database (MIQD-2.5M), comprising 2.5 million
samples that capture distinctive degradation responses in both consistency and
accuracy metrics, spanning 75 vision models, 250 degradation types, and three
representative vision tasks. We further propose a region-aware MIQA (RA-MIQA)
model to evaluate MVS visual quality through fine-grained spatial degradation
analysis. Extensive experiments benchmark the proposed RA-MIQA against seven
human visual system (HVS)-based IQA metrics and five retrained classical
backbones. Results demonstrate RA-MIQA's superior performance in multiple
dimensions, e.g., achieving SRCC gains of 13.56% on consistency and 13.37% on
accuracy for image classification, while also revealing task-specific
degradation sensitivities. Critically, HVS-based metrics prove inadequate for
MVS quality prediction, while even specialized MIQA models struggle with
background degradations, accuracy-oriented estimation, and subtle distortions.
This study can advance MVS reliability and establish foundations for
machine-centric image processing and optimization. The model and code are
available at: https://github.com/XiaoqiWang/MIQA.

</details>


### [65] [Ego-centric Predictive Model Conditioned on Hand Trajectories](https://arxiv.org/abs/2508.19852)
*Binjie Zhang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 提出了一种统一的两阶段预测框架，联合建模自我中心场景中的动作和视觉未来，以手部轨迹为条件。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法联合建模动作预测和视觉场景影响，导致结果不准确或不一致。

Method: 两阶段框架：第一阶段处理多模态输入并预测手部轨迹；第二阶段通过因果交叉注意力融合多模态线索，引导LDM生成未来视频。

Result: 在Ego4D、BridgeData和RLBench上优于现有基线方法。

Conclusion: 该框架首次统一处理自我中心人类活动理解和机器人操作任务，提供动作和视觉后果的显式预测。

Abstract: In egocentric scenarios, anticipating both the next action and its visual
outcome is essential for understanding human-object interactions and for
enabling robotic planning. However, existing paradigms fall short of jointly
modeling these aspects. Vision-Language-Action (VLA) models focus on action
prediction but lack explicit modeling of how actions influence the visual
scene, while video prediction models generate future frames without
conditioning on specific actions, often resulting in implausible or
contextually inconsistent outcomes. To bridge this gap, we propose a unified
two-stage predictive framework that jointly models action and visual future in
egocentric scenarios, conditioned on hand trajectories. In the first stage, we
perform consecutive state modeling to process heterogeneous inputs (visual
observations, language, and action history) and explicitly predict future hand
trajectories. In the second stage, we introduce causal cross-attention to fuse
multi-modal cues, leveraging inferred action signals to guide an image-based
Latent Diffusion Model (LDM) for frame-by-frame future video generation. Our
approach is the first unified model designed to handle both egocentric human
activity understanding and robotic manipulation tasks, providing explicit
predictions of both upcoming actions and their visual consequences. Extensive
experiments on Ego4D, BridgeData, and RLBench demonstrate that our method
outperforms state-of-the-art baselines in both action prediction and future
video synthesis.

</details>


### [66] [Multimodal Conditional MeshGAN for Personalized Aneurysm Growth Prediction](https://arxiv.org/abs/2508.19862)
*Long Chen,Ashiv Patel,Mengyun Qiao,Mohammad Yousuf Salmasi,Salah A. Hammouche,Vasilis Stavrinides,Jasleen Nagi,Soodeh Kalaie,Xiao Yun Xu,Wenjia Bai,Declan P. O'Regan*

Main category: cs.CV

TL;DR: MCMeshGAN是一种多模态条件网格生成对抗网络，用于预测主动脉瘤的3D生长，结合局部和全局网络分支，克服了传统方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 主动脉瘤进展的个性化准确预测对及时干预至关重要，但现有方法难以同时建模局部变形和全局解剖变化。

Method: 提出MCMeshGAN，结合局部KNN卷积网络（KCN）和全局图卷积网络（GCN），并引入临床属性编码分支。

Result: 在几何精度和临床直径估计上优于现有方法，并在新数据集TAAMesh上验证。

Conclusion: MCMeshGAN为临床可部署的个性化3D疾病轨迹建模提供了重要进展。

Abstract: Personalized, accurate prediction of aortic aneurysm progression is essential
for timely intervention but remains challenging due to the need to model both
subtle local deformations and global anatomical changes within complex 3D
geometries. We propose MCMeshGAN, the first multimodal conditional mesh-to-mesh
generative adversarial network for 3D aneurysm growth prediction. MCMeshGAN
introduces a dual-branch architecture combining a novel local KNN-based
convolutional network (KCN) to preserve fine-grained geometric details and a
global graph convolutional network (GCN) to capture long-range structural
context, overcoming the over-smoothing limitations of deep GCNs. A dedicated
condition branch encodes clinical attributes (age, sex) and the target time
interval to generate anatomically plausible, temporally controlled predictions,
enabling retrospective and prospective modeling. We curated TAAMesh, a new
longitudinal thoracic aortic aneurysm mesh dataset consisting of 590 multimodal
records (CT scans, 3D meshes, and clinical data) from 208 patients. Extensive
experiments demonstrate that MCMeshGAN consistently outperforms
state-of-the-art baselines in both geometric accuracy and clinically important
diameter estimation. This framework offers a robust step toward clinically
deployable, personalized 3D disease trajectory modeling. The source code for
MCMeshGAN and the baseline methods is publicly available at
https://github.com/ImperialCollegeLondon/MCMeshGAN.

</details>


### [67] [Self-supervised structured object representation learning](https://arxiv.org/abs/2508.19864)
*Oussama Hadjerci,Antoine Letienne,Mohamed Abbas Hedjazi,Adel Hafiane*

Main category: cs.CV

TL;DR: 提出了一种基于ProtoScale模块的自监督学习方法，通过结合语义分组、实例级别分离和层次结构，逐步构建结构化视觉表示，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自监督学习方法在全局图像理解上表现良好，但在捕捉场景结构化表示方面存在局限。

Method: 提出ProtoScale模块，结合语义分组、实例级别分离和层次结构，保留完整场景上下文以提升密集预测任务性能。

Result: 在COCO和UA-DETRAC数据集上的实验表明，该方法学习到的对象中心表示优于现有方法，尤其在有限标注数据和较少微调轮次下。

Conclusion: 该方法通过结构化表示学习，显著提升了对象检测任务的性能，为自监督学习提供了新思路。

Abstract: Self-supervised learning (SSL) has emerged as a powerful technique for
learning visual representations. While recent SSL approaches achieve strong
results in global image understanding, they are limited in capturing the
structured representation in scenes. In this work, we propose a self-supervised
approach that progressively builds structured visual representations by
combining semantic grouping, instance level separation, and hierarchical
structuring. Our approach, based on a novel ProtoScale module, captures visual
elements across multiple spatial scales. Unlike common strategies like DINO
that rely on random cropping and global embeddings, we preserve full scene
context across augmented views to improve performance in dense prediction
tasks. We validate our method on downstream object detection tasks using a
combined subset of multiple datasets (COCO and UA-DETRAC). Experimental results
show that our method learns object centric representations that enhance
supervised object detection and outperform the state-of-the-art methods, even
when trained with limited annotated data and fewer fine-tuning epochs.

</details>


### [68] [TrajFusionNet: Pedestrian Crossing Intention Prediction via Fusion of Sequential and Visual Trajectory Representations](https://arxiv.org/abs/2508.19866)
*François G. Landry,Moulay A. Akhloufi*

Main category: cs.CV

TL;DR: TrajFusionNet是一种基于Transformer的模型，结合未来行人轨迹和车辆速度预测来预测行人过马路意图，性能优越且推理时间短。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的出现，预测行人过马路意图成为重要研究方向，以提高道路安全性。

Method: 提出TrajFusionNet，包含序列注意力模块（SAM）和视觉注意力模块（VAM），结合行人轨迹和车辆速度的序列与视觉信息。

Result: 在三个常用数据集上达到最优性能，且推理时间最短。

Conclusion: TrajFusionNet通过轻量级模态和多模态融合，实现了高效且准确的行人过马路意图预测。

Abstract: With the introduction of vehicles with autonomous capabilities on public
roads, predicting pedestrian crossing intention has emerged as an active area
of research. The task of predicting pedestrian crossing intention involves
determining whether pedestrians in the scene are likely to cross the road or
not. In this work, we propose TrajFusionNet, a novel transformer-based model
that combines future pedestrian trajectory and vehicle speed predictions as
priors for predicting crossing intention. TrajFusionNet comprises two branches:
a Sequence Attention Module (SAM) and a Visual Attention Module (VAM). The SAM
branch learns from a sequential representation of the observed and predicted
pedestrian trajectory and vehicle speed. Complementarily, the VAM branch
enables learning from a visual representation of the predicted pedestrian
trajectory by overlaying predicted pedestrian bounding boxes onto scene images.
By utilizing a small number of lightweight modalities, TrajFusionNet achieves
the lowest total inference time (including model runtime and data
preprocessing) among current state-of-the-art approaches. In terms of
performance, it achieves state-of-the-art results across the three most
commonly used datasets for pedestrian crossing intention prediction.

</details>


### [69] [Sky Background Building of Multi-objective Fiber spectra Based on Mutual Information Network](https://arxiv.org/abs/2508.19875)
*Hui Zhang,Jianghui Cai,Haifeng Yang,Ali Luo,Yuqing Yang,Xiao Kong,Zhichao Ding,Lichan Zhou,Qin Han*

Main category: cs.CV

TL;DR: 提出了一种基于互信息的天空背景估计模型（SMI），通过利用所有光纤的光谱来估计天空背景，解决了传统方法依赖天空光纤光谱的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前天空背景扣除主要依赖天空光纤光谱构建超级天空，缺乏对目标周围环境的建模。

Method: SMI包含两个主要网络：第一个网络通过波长校准模块提取光谱中的天空特征；第二个网络采用增量训练方法最大化不同光谱表示之间的互信息以捕获共同成分，并最小化相邻光谱表示之间的互信息以获得个体成分。

Result: 在LAMOST光谱上的实验表明，SMI在观测期间能获得更好的目标天空背景，尤其在蓝端效果显著。

Conclusion: SMI方法通过互信息和增量训练有效解决了天空背景估计问题，提升了光谱处理的准确性。

Abstract: Sky background subtraction is a critical step in Multi-objective Fiber
spectra process. However, current subtraction relies mainly on sky fiber
spectra to build Super Sky. These average spectra are lacking in the modeling
of the environment surrounding the objects. To address this issue, a sky
background estimation model: Sky background building based on Mutual
Information (SMI) is proposed. SMI based on mutual information and incremental
training approach. It utilizes spectra from all fibers in the plate to estimate
the sky background. SMI contains two main networks, the first network applies a
wavelength calibration module to extract sky features from spectra, and can
effectively solve the feature shift problem according to the corresponding
emission position. The second network employs an incremental training approach
to maximize mutual information between representations of different spectra to
capturing the common component. Then, it minimizes the mutual information
between adjoining spectra representations to obtain individual components. This
network yields an individual sky background at each location of the object. To
verify the effectiveness of the method in this paper, we conducted experiments
on the spectra of LAMOST. Results show that SMI can obtain a better object sky
background during the observation, especially in the blue end.

</details>


### [70] [Multispectral LiDAR data for extracting tree points in urban and suburban areas](https://arxiv.org/abs/2508.19881)
*Narges Takhtkeshha,Gabriele Mazzacca,Fabio Remondino,Juha Hyyppä,Gottfried Mandlburger*

Main category: cs.CV

TL;DR: 该研究探讨了利用多光谱LiDAR和深度学习模型提取树木点云的方法，评估了三种模型，发现SPT在时间和准确性上表现最佳，结合pNDVI可显著降低错误率。


<details>
  <summary>Details</summary>
Motivation: 监测城市树木动态对绿化政策和减少电力基础设施风险至关重要，但复杂环境和树木多样性带来挑战。

Method: 使用多光谱LiDAR和深度学习模型（SPT、PTv3、PTv1）提取树木点云，并评估其性能。

Result: SPT表现最佳（mIoU 85.28%），结合pNDVI可降低错误率10.61个百分点。

Conclusion: 多光谱LiDAR和深度学习模型在树木提取和库存管理中具有潜力。

Abstract: Monitoring urban tree dynamics is vital for supporting greening policies and
reducing risks to electrical infrastructure. Airborne laser scanning has
advanced large-scale tree management, but challenges remain due to complex
urban environments and tree variability. Multispectral (MS) light detection and
ranging (LiDAR) improves this by capturing both 3D spatial and spectral data,
enabling detailed mapping. This study explores tree point extraction using
MS-LiDAR and deep learning (DL) models. Three state-of-the-art models are
evaluated: Superpoint Transformer (SPT), Point Transformer V3 (PTv3), and Point
Transformer V1 (PTv1). Results show the notable time efficiency and accuracy of
SPT, with a mean intersection over union (mIoU) of 85.28%. The highest
detection accuracy is achieved by incorporating pseudo normalized difference
vegetation index (pNDVI) with spatial data, reducing error rate by 10.61
percentage points (pp) compared to using spatial information alone. These
findings highlight the potential of MS-LiDAR and DL to improve tree extraction
and further tree inventories.

</details>


### [71] [PersonaAnimator: Personalized Motion Transfer from Unconstrained Videos](https://arxiv.org/abs/2508.19895)
*Ziyun Qian,Runyu Xiao,Shuyuan Tu,Wei Xue,Dingkang Yang,Mingcheng Li,Dongliang Kou,Minghao Han,Zizhi Chen,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新任务：视频到视频的运动个性化，并提出了PersonaAnimator框架，直接从无约束视频中学习个性化运动模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动生成中存在三个主要问题：缺乏风格特征学习、依赖运动捕捉数据、生成的运动可能违反物理规律。

Method: 提出了PersonaAnimator框架，结合PersonaVid数据集和物理感知的运动风格正则化机制。

Result: 实验表明，PersonaAnimator在运动个性化任务中优于现有方法。

Conclusion: PersonaAnimator为视频到视频运动个性化任务设定了新基准。

Abstract: Recent advances in motion generation show remarkable progress. However,
several limitations remain: (1) Existing pose-guided character motion transfer
methods merely replicate motion without learning its style characteristics,
resulting in inexpressive characters. (2) Motion style transfer methods rely
heavily on motion capture data, which is difficult to obtain. (3) Generated
motions sometimes violate physical laws. To address these challenges, this
paper pioneers a new task: Video-to-Video Motion Personalization. We propose a
novel framework, PersonaAnimator, which learns personalized motion patterns
directly from unconstrained videos. This enables personalized motion transfer.
To support this task, we introduce PersonaVid, the first video-based
personalized motion dataset. It contains 20 motion content categories and 120
motion style categories. We further propose a Physics-aware Motion Style
Regularization mechanism to enforce physical plausibility in the generated
motions. Extensive experiments show that PersonaAnimator outperforms
state-of-the-art motion transfer methods and sets a new benchmark for the
Video-to-Video Motion Personalization task.

</details>


### [72] [Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities](https://arxiv.org/abs/2508.19905)
*Imad Ali Shah,Jiarong Li,Roshan George,Tim Brophy,Enda Ward,Martin Glavin,Edward Jones,Brian Deegan*

Main category: cs.CV

TL;DR: 本文综述了高光谱成像（HSI）在自动驾驶和高级驾驶辅助系统中的应用，分析了其技术优势和商业准备度，并指出了当前数据集和技术的局限性。


<details>
  <summary>Details</summary>
Motivation: 探讨HSI在自动驾驶领域的潜力，评估其技术成熟度和商业可行性，为未来研究提供方向。

Method: 通过定性综述和定量分析216款商用HSI和多光谱相机，评估其性能指标（如帧率、空间分辨率等）和AEC-Q100标准合规性。

Result: 研究发现仅有四款相机满足性能阈值，且无一符合AEC-Q100标准；当前HSI数据集在规模和环境多样性上存在不足。

Conclusion: HSI在自动驾驶领域的研究潜力与商业准备度存在显著差距，需进一步优化技术和数据集以实现实际应用。

Abstract: Hyperspectral imaging (HSI) offers a transformative sensing modality for
Advanced Driver Assistance Systems (ADAS) and autonomous driving (AD)
applications, enabling material-level scene understanding through fine spectral
resolution beyond the capabilities of traditional RGB imaging. This paper
presents the first comprehensive review of HSI for automotive applications,
examining the strengths, limitations, and suitability of current HSI
technologies in the context of ADAS/AD. In addition to this qualitative review,
we analyze 216 commercially available HSI and multispectral imaging cameras,
benchmarking them against key automotive criteria: frame rate, spatial
resolution, spectral dimensionality, and compliance with AEC-Q100 temperature
standards. Our analysis reveals a significant gap between HSI's demonstrated
research potential and its commercial readiness. Only four cameras meet the
defined performance thresholds, and none comply with AEC-Q100 requirements. In
addition, the paper reviews recent HSI datasets and applications, including
semantic segmentation for road surface classification, pedestrian separability,
and adverse weather perception. Our review shows that current HSI datasets are
limited in terms of scale, spectral consistency, the number of spectral
channels, and environmental diversity, posing challenges for the development of
perception algorithms and the adequate validation of HSI's true potential in
ADAS/AD applications. This review paper establishes the current state of HSI in
automotive contexts as of 2025 and outlines key research directions toward
practical integration of spectral imaging in ADAS and autonomous systems.

</details>


### [73] [Streamlining the Development of Active Learning Methods in Real-World Object Detection](https://arxiv.org/abs/2508.19906)
*Moussa Kassem Sbeyti,Nadja Klein,Michelle Karg,Christian Wirth,Sahin Albayrak*

Main category: cs.CV

TL;DR: 论文提出了一种基于对象相似性的度量方法（OSS），用于高效评估和选择主动学习方法，无需训练检测器，适用于自动驾驶数据集。


<details>
  <summary>Details</summary>
Motivation: 主动学习在现实世界目标检测中面临计算成本和可靠性问题，需要一种无需训练检测器的评估方法。

Method: 提出对象级集合相似性（OSS）度量，通过对象级特征量化训练集与目标域的相似性，并选择代表性验证集。

Result: 在三个自动驾驶数据集（KITTI、BDD100K、CODA）上验证了OSS的有效性，展示了其计算效率和可靠性。

Conclusion: OSS为主动学习在目标检测中的实际应用提供了高效且可靠的框架，适用于计算效率和评估可靠性要求高的场景。

Abstract: Active learning (AL) for real-world object detection faces computational and
reliability challenges that limit practical deployment. Developing new AL
methods requires training multiple detectors across iterations to compare
against existing approaches. This creates high costs for autonomous driving
datasets where the training of one detector requires up to 282 GPU hours.
Additionally, AL method rankings vary substantially across validation sets,
compromising reliability in safety-critical transportation systems. We
introduce object-based set similarity ($\mathrm{OSS}$), a metric that addresses
these challenges. $\mathrm{OSS}$ (1) quantifies AL method effectiveness without
requiring detector training by measuring similarity between training sets and
target domains using object-level features. This enables the elimination of
ineffective AL methods before training. Furthermore, $\mathrm{OSS}$ (2) enables
the selection of representative validation sets for robust evaluation. We
validate our similarity-based approach on three autonomous driving datasets
(KITTI, BDD100K, CODA) using uncertainty-based AL methods as a case study with
two detector architectures (EfficientDet, YOLOv3). This work is the first to
unify AL training and evaluation strategies in object detection based on object
similarity. $\mathrm{OSS}$ is detector-agnostic, requires only labeled object
crops, and integrates with existing AL pipelines. This provides a practical
framework for deploying AL in real-world applications where computational
efficiency and evaluation reliability are critical. Code is available at
https://mos-ks.github.io/publications/.

</details>


### [74] [Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation](https://arxiv.org/abs/2508.19909)
*Lechun You,Zhonghua Wu,Weide Liu,Xulei Yang,Jun Cheng,Wei Zhou,Bharadwaj Veeravalli,Guosheng Lin*

Main category: cs.CV

TL;DR: 提出了一种利用2D基础模型生成的掩码来增强稀疏3D标注的新方法，通过几何对应和一致性正则化提升3D弱监督分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅关注3D领域，未充分利用2D与3D数据的互补性，且对伪标签的利用不足。

Method: 结合2D基础模型的掩码，通过几何对应传播到3D空间，并利用置信度和不确定性正则化筛选可靠伪标签。

Result: 显著增加了可用标注数量，提升了3D弱监督分割的性能。

Conclusion: 该方法有效弥补了3D标注不足与2D基础模型能力之间的差距，为3D分割提供了新思路。

Abstract: Current methods for 3D semantic segmentation propose training models with
limited annotations to address the difficulty of annotating large, irregular,
and unordered 3D point cloud data. They usually focus on the 3D domain only,
without leveraging the complementary nature of 2D and 3D data. Besides, some
methods extend original labels or generate pseudo labels to guide the training,
but they often fail to fully use these labels or address the noise within them.
Meanwhile, the emergence of comprehensive and adaptable foundation models has
offered effective solutions for segmenting 2D data. Leveraging this
advancement, we present a novel approach that maximizes the utility of sparsely
available 3D annotations by incorporating segmentation masks generated by 2D
foundation models. We further propagate the 2D segmentation masks into the 3D
space by establishing geometric correspondences between 3D scenes and 2D views.
We extend the highly sparse annotations to encompass the areas delineated by 3D
masks, thereby substantially augmenting the pool of available labels.
Furthermore, we apply confidence- and uncertainty-based consistency
regularization on augmentations of the 3D point cloud and select the reliable
pseudo labels, which are further spread on the 3D masks to generate more
labels. This innovative strategy bridges the gap between limited 3D annotations
and the powerful capabilities of 2D foundation models, ultimately improving the
performance of 3D weakly supervised segmentation.

</details>


### [75] [WaveHiT-SR: Hierarchical Wavelet Network for Efficient Image Super-Resolution](https://arxiv.org/abs/2508.19927)
*Fayaz Ali,Muhammad Zawish,Steven Davy,Radu Timofte*

Main category: cs.CV

TL;DR: 提出了一种基于小波变换的分层Transformer框架（WaveHiT-SR），通过自适应分层窗口和多频带分解，显著提升了图像超分辨率任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在图像超分辨率任务中因窗口自注意力的二次计算复杂度限制了感受野，无法有效建模长距离依赖关系。

Method: 结合小波变换与分层Transformer，利用自适应分层窗口和多频带分解，逐步重建高分辨率图像。

Result: 实验表明，WaveHiT-SR在性能、参数数量、计算复杂度和速度上均优于现有方法（如SwinIR-Light、SwinIR-NG和SRFormer-Light）。

Conclusion: WaveHiT-SR通过分层处理和多频带分解，在保持高性能的同时降低了计算复杂度，为图像超分辨率任务提供了高效解决方案。

Abstract: Transformers have demonstrated promising performance in computer vision
tasks, including image super-resolution (SR). The quadratic computational
complexity of window self-attention mechanisms in many transformer-based SR
methods forces the use of small, fixed windows, limiting the receptive field.
In this paper, we propose a new approach by embedding the wavelet transform
within a hierarchical transformer framework, called (WaveHiT-SR). First, using
adaptive hierarchical windows instead of static small windows allows to capture
features across different levels and greatly improve the ability to model
long-range dependencies. Secondly, the proposed model utilizes wavelet
transforms to decompose images into multiple frequency subbands, allowing the
network to focus on both global and local features while preserving structural
details. By progressively reconstructing high-resolution images through
hierarchical processing, the network reduces computational complexity without
sacrificing performance. The multi-level decomposition strategy enables the
network to capture fine-grained information in lowfrequency components while
enhancing high-frequency textures. Through extensive experimentation, we
confirm the effectiveness and efficiency of our WaveHiT-SR. Our refined
versions of SwinIR-Light, SwinIR-NG, and SRFormer-Light deliver cutting-edge SR
results, achieving higher efficiency with fewer parameters, lower FLOPs, and
faster speeds.

</details>


### [76] [KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts](https://arxiv.org/abs/2508.19944)
*Taebaek Hwang,Minseo Kim,Gisang Lee,Seonuk Kim,Hyunjun Eun*

Main category: cs.CV

TL;DR: KRETA是一个针对韩语的文本丰富视觉问答（VQA）基准，旨在填补低资源语言在视觉文本理解和推理能力评估上的空白。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏针对低资源语言（如韩语）的全面视觉问答基准，阻碍了模型的评估和比较。

Method: 提出KRETA基准，包含15个领域和26种图像类型，并开发了一个半自动化的VQA生成流程，采用逐步图像分解和七项指标评估协议。

Result: KRETA支持对视觉文本理解和推理能力的深入评估，并提供了高质量的数据集。

Conclusion: KRETA不仅适用于韩语，其可扩展的流程也有助于其他语言的类似基准开发，推动多语言VLM研究。

Abstract: Understanding and reasoning over text within visual contexts poses a
significant challenge for Vision-Language Models (VLMs), given the complexity
and diversity of real-world scenarios. To address this challenge, text-rich
Visual Question Answering (VQA) datasets and benchmarks have emerged for
high-resource languages like English. However, a critical gap persists for
low-resource languages such as Korean, where the lack of comprehensive
benchmarks hinders robust model evaluation and comparison. To bridge this gap,
we introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich
VQA Attuned to diverse visual contexts. KRETA facilitates an in-depth
evaluation of both visual text understanding and reasoning capabilities, while
also supporting a multifaceted assessment across 15 domains and 26 image types.
Additionally, we introduce a semi-automated VQA generation pipeline
specifically optimized for text-rich settings, leveraging refined stepwise
image decomposition and a rigorous seven-metric evaluation protocol to ensure
data quality. While KRETA is tailored for Korean, we hope our adaptable and
extensible pipeline will facilitate the development of similar benchmarks in
other languages, thereby accelerating multilingual VLM research. The code and
dataset for KRETA are available at https://github.com/tabtoyou/KRETA.

</details>


### [77] [Reimagining Image Segmentation using Active Contour: From Chan Vese Algorithm into a Proposal Novel Functional Loss Framework](https://arxiv.org/abs/2508.19946)
*Gianluca Guzzetta*

Main category: cs.CV

TL;DR: 本文对Chan-Vese算法进行了全面研究，提出了一种基于功能分割损失的新方法，并在MATLAB和PyTorch中实现。


<details>
  <summary>Details</summary>
Motivation: 研究Chan-Vese算法在图像分割中的应用，并改进其功能分割损失。

Method: 采用离散化方案和水平集函数，结合PyTorch的模块化损失函数，提出新的功能分割损失方法。

Result: 与传统损失函数相比，新方法在计算机视觉分割数据集上表现更优。

Conclusion: 提出的功能分割损失方法有效提升了Chan-Vese算法的性能，代码已开源。

Abstract: In this paper, we present a comprehensive study and analysis of the Chan-Vese
algorithm for image segmentation. We employ a discretized scheme derived from
the empirical study of the Chan-Vese model's functional energy and its partial
differential equation based on its level set function. We provide a proof of
the results and an implementation using MATLAB. Leveraging modern computer
vision methodologies, we propose a functional segmentation loss based on active
contours, utilizing pytorch.nn.ModuleLoss and a level set based on the
Chan-Vese algorithm. We compare our results with common computer vision
segmentation datasets and evaluate the performance of classical loss functions
against our proposed method. All code and materials used are available at
https://github.com/gguzzy/chan_vese_functional_loss.

</details>


### [78] [Assessing the Geolocation Capabilities, Limitations and Societal Risks of Generative Vision-Language Models](https://arxiv.org/abs/2508.19967)
*Oliver Grainge,Sania Waheed,Jack Stilgoe,Michael Milford,Shoaib Ehsan*

Main category: cs.CV

TL;DR: 论文分析了视觉语言模型（VLMs）在图像地理定位任务中的能力、局限性和潜在隐私风险，评估了25种先进模型在多样环境数据集上的表现。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补对生成式VLMs地理定位能力的系统性评估空白，揭示其潜在隐私风险，尤其是在社交媒体图像中的应用。

Method: 方法包括对25种最先进的VLMs在四个不同环境下的基准图像数据集上进行全面评估。

Result: 结果显示，当前VLMs在普通街景图像上表现不佳，但在类似社交媒体内容的图像上准确率高达61%，引发隐私担忧。

Conclusion: 结论指出VLMs的地理定位能力存在显著隐私风险，需引起重视并采取应对措施。

Abstract: Geo-localization is the task of identifying the location of an image using
visual cues alone. It has beneficial applications, such as improving disaster
response, enhancing navigation, and geography education. Recently,
Vision-Language Models (VLMs) are increasingly demonstrating capabilities as
accurate image geo-locators. This brings significant privacy risks, including
those related to stalking and surveillance, considering the widespread uses of
AI models and sharing of photos on social media. The precision of these models
is likely to improve in the future. Despite these risks, there is little work
on systematically evaluating the geolocation precision of Generative VLMs,
their limits and potential for unintended inferences. To bridge this gap, we
conduct a comprehensive assessment of the geolocation capabilities of 25
state-of-the-art VLMs on four benchmark image datasets captured in diverse
environments. Our results offer insight into the internal reasoning of VLMs and
highlight their strengths, limitations, and potential societal risks. Our
findings indicate that current VLMs perform poorly on generic street-level
images yet achieve notably high accuracy (61\%) on images resembling social
media content, raising significant and urgent privacy concerns.

</details>


### [79] [GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity](https://arxiv.org/abs/2508.19972)
*Seongheon Park,Yixuan Li*

Main category: cs.CV

TL;DR: GLSim是一种新的无需训练的对象幻觉检测框架，结合全局和局部嵌入相似性信号，显著提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅采用全局或局部视角的局限性，提高对象幻觉检测的可靠性。

Method: 利用图像和文本模态的全局和局部嵌入相似性信号，设计无需训练的检测框架GLSim。

Result: GLSim在多样场景中表现优异，显著超越现有基线方法。

Conclusion: GLSim通过结合全局和局部信号，实现了更准确和可靠的对象幻觉检测。

Abstract: Object hallucination in large vision-language models presents a significant
challenge to their safe deployment in real-world applications. Recent works
have proposed object-level hallucination scores to estimate the likelihood of
object hallucination; however, these methods typically adopt either a global or
local perspective in isolation, which may limit detection reliability. In this
paper, we introduce GLSim, a novel training-free object hallucination detection
framework that leverages complementary global and local embedding similarity
signals between image and text modalities, enabling more accurate and reliable
hallucination detection in diverse scenarios. We comprehensively benchmark
existing object hallucination detection methods and demonstrate that GLSim
achieves superior detection performance, outperforming competitive baselines by
a significant margin.

</details>


### [80] [GS: Generative Segmentation via Label Diffusion](https://arxiv.org/abs/2508.20020)
*Yuhao Chen,Shubin Chen,Liang Lin,Guangrun Wang*

Main category: cs.CV

TL;DR: GS框架将分割任务转化为生成任务，通过标签扩散直接生成分割掩码，显著提升了语言驱动分割的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法将分割视为判别问题，而现有扩散模型方法仍以图像为中心，未能直接生成分割掩码。

Method: 提出GS框架，通过标签扩散直接生成分割掩码，结合输入图像和语言描述进行端到端训练。

Result: 在Panoptic Narrative Grounding基准测试中，GS显著优于现有方法，达到新SOTA。

Conclusion: GS通过生成式分割范式，实现了对空间和语义保真度的显式控制，为语言驱动分割提供了新思路。

Abstract: Language-driven image segmentation is a fundamental task in vision-language
understanding, requiring models to segment regions of an image corresponding to
natural language expressions. Traditional methods approach this as a
discriminative problem, assigning each pixel to foreground or background based
on semantic alignment. Recently, diffusion models have been introduced to this
domain, but existing approaches remain image-centric: they either (i) use image
diffusion models as visual feature extractors, (ii) synthesize segmentation
data via image generation to train discriminative models, or (iii) perform
diffusion inversion to extract attention cues from pre-trained image diffusion
models-thereby treating segmentation as an auxiliary process. In this paper, we
propose GS (Generative Segmentation), a novel framework that formulates
segmentation itself as a generative task via label diffusion. Instead of
generating images conditioned on label maps and text, GS reverses the
generative process: it directly generates segmentation masks from noise,
conditioned on both the input image and the accompanying language description.
This paradigm makes label generation the primary modeling target, enabling
end-to-end training with explicit control over spatial and semantic fidelity.
To demonstrate the effectiveness of our approach, we evaluate GS on Panoptic
Narrative Grounding (PNG), a representative and challenging benchmark for
multimodal segmentation that requires panoptic-level reasoning guided by
narrative captions. Experimental results show that GS significantly outperforms
existing discriminative and diffusion-based methods, setting a new
state-of-the-art for language-driven segmentation.

</details>


### [81] [Segmentation Assisted Incremental Test Time Adaptation in an Open World](https://arxiv.org/abs/2508.20029)
*Manogna Sreenivas,Soma Biswas*

Main category: cs.CV

TL;DR: 论文提出了一种名为SegAssist的框架，用于视觉语言模型在测试时的增量适应，能够同时处理协变量和标签偏移，并通过主动标注技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 动态环境中，模型常遇到陌生对象和分布偏移，传统测试时适应方法无法处理新类别和新领域的持续出现。

Method: 提出SegAssist模块，利用视觉语言模型的分割能力进行主动样本选择，无需额外训练。

Result: 在多个基准数据集上的实验表明，SegAssist能有效提升模型在持续适应新数据时的性能。

Conclusion: SegAssist为视觉语言模型在现实场景中的增量适应提供了有效解决方案。

Abstract: In dynamic environments, unfamiliar objects and distribution shifts are often
encountered, which challenge the generalization abilities of the deployed
trained models. This work addresses Incremental Test Time Adaptation of Vision
Language Models, tackling scenarios where unseen classes and unseen domains
continuously appear during testing. Unlike traditional Test Time Adaptation
approaches, where the test stream comes only from a predefined set of classes,
our framework allows models to adapt simultaneously to both covariate and label
shifts, actively incorporating new classes as they emerge. Towards this goal,
we establish a new benchmark for ITTA, integrating single image TTA methods for
VLMs with active labeling techniques that query an oracle for samples
potentially representing unseen classes during test time. We propose a
segmentation assisted active labeling module, termed SegAssist, which is
training free and repurposes the segmentation capabilities of VLMs to refine
active sample selection, prioritizing samples likely to belong to unseen
classes. Extensive experiments on several benchmark datasets demonstrate the
potential of SegAssist to enhance the performance of VLMs in real world
scenarios, where continuous adaptation to emerging data is essential.
Project-page:https://manogna-s.github.io/segassist/

</details>


### [82] [OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations](https://arxiv.org/abs/2508.20063)
*Peng-Hao Hsu,Ke Zhang,Fu-En Wang,Tao Tu,Ming-Feng Li,Yu-Lun Liu,Albert Y. C. Chen,Min Sun,Cheng-Hao Kuo*

Main category: cs.CV

TL;DR: OpenM3D是一种无需人工标注的新型开放词汇多视角室内3D目标检测器，通过联合训练类无关3D定位损失和体素语义对齐损失，实现了高效且高精度的检测。


<details>
  <summary>Details</summary>
Motivation: 探索基于图像的开放词汇3D目标检测方法，弥补现有研究主要集中在点云方法上的不足。

Method: 采用单阶段检测器，结合2D诱导的体素特征，通过3D伪框生成和多样CLIP特征采样实现训练。

Result: 在ScanNet200和ARKitScenes基准测试中，OpenM3D在精度和速度（0.3秒/场景）上均优于现有方法。

Conclusion: OpenM3D展示了高效且高精度的开放词汇3D目标检测能力，为基于图像的方法提供了新思路。

Abstract: Open-vocabulary (OV) 3D object detection is an emerging field, yet its
exploration through image-based methods remains limited compared to 3D point
cloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-view
indoor 3D object detector trained without human annotations. In particular,
OpenM3D is a single-stage detector adapting the 2D-induced voxel features from
the ImGeoNet model. To support OV, it is jointly trained with a class-agnostic
3D localization loss requiring high-quality 3D pseudo boxes and a
voxel-semantic alignment loss requiring diverse pre-trained CLIP features. We
follow the training setting of OV-3DET where posed RGB-D images are given but
no human annotations of 3D boxes or classes are available. We propose a 3D
Pseudo Box Generation method using a graph embedding technique that combines 2D
segments into coherent 3D structures. Our pseudo-boxes achieve higher precision
and recall than other methods, including the method proposed in OV-3DET. We
further sample diverse CLIP features from 2D segments associated with each
coherent 3D structure to align with the corresponding voxel feature. The key to
training a highly accurate single-stage detector requires both losses to be
learned toward high-quality targets. At inference, OpenM3D, a highly efficient
detector, requires only multi-view images for input and demonstrates superior
accuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoor
benchmarks compared to existing methods. We outperform a strong two-stage
method that leverages our class-agnostic detector with a ViT CLIP-based OV
classifier and a baseline incorporating multi-view depth estimator on both
accuracy and speed.

</details>


### [83] [Patch Progression Masked Autoencoder with Fusion CNN Network for Classifying Evolution Between Two Pairs of 2D OCT Slices](https://arxiv.org/abs/2508.20064)
*Philippe Zhang,Weili Jiang,Yihao Li,Jing Zhang,Sarah Matta,Yubo Tan,Hui Lin,Haoshen Wang,Jiangtian Pan,Hui Xu,Laurent Borderie,Alexandre Le Guilcher,Béatrice Cochener,Chubin Ou,Gwenolé Quellec,Mathieu Lamard*

Main category: cs.CV

TL;DR: 论文研究了通过OCT扫描监测AMD进展的方法，提出融合CNN和自编码器模型，在MARIO挑战中表现优异。


<details>
  <summary>Details</summary>
Motivation: AMD影响视力，抗VEGF治疗有效但需及时监测。通过OCT跟踪进展可优化治疗方案。

Method: 任务1用融合CNN和模型集成分类OCT切片演变；任务2用自编码器预测未来进展。

Result: 在MARIO挑战中两项任务均进入前十。

Conclusion: 提出的方法能有效监测AMD进展，但因组织关系未参与评奖。

Abstract: Age-related Macular Degeneration (AMD) is a prevalent eye condition affecting
visual acuity. Anti-vascular endothelial growth factor (anti-VEGF) treatments
have been effective in slowing the progression of neovascular AMD, with better
outcomes achieved through timely diagnosis and consistent monitoring. Tracking
the progression of neovascular activity in OCT scans of patients with exudative
AMD allows for the development of more personalized and effective treatment
plans. This was the focus of the Monitoring Age-related Macular Degeneration
Progression in Optical Coherence Tomography (MARIO) challenge, in which we
participated. In Task 1, which involved classifying the evolution between two
pairs of 2D slices from consecutive OCT acquisitions, we employed a fusion CNN
network with model ensembling to further enhance the model's performance. For
Task 2, which focused on predicting progression over the next three months
based on current exam data, we proposed the Patch Progression Masked
Autoencoder that generates an OCT for the next exam and then classifies the
evolution between the current OCT and the one generated using our solution from
Task 1. The results we achieved allowed us to place in the Top 10 for both
tasks. Some team members are part of the same organization as the challenge
organizers; therefore, we are not eligible to compete for the prize.

</details>


### [84] [PAUL: Uncertainty-Guided Partition and Augmentation for Robust Cross-View Geo-Localization under Noisy Correspondence](https://arxiv.org/abs/2508.20066)
*Zheng Li,Yanming Guo,WenZhe Liu,Xueyi Zhang,Zhaoyun Ding,Long Xu,Mingrui Lao*

Main category: cs.CV

TL;DR: 论文提出了一种名为PAUL的新框架，用于解决跨视角地理定位中的噪声对应问题，通过不确定性学习和数据增强来提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常假设训练图像对完美对齐，而实际中因GPS漂移等因素导致噪声对应，这一问题在研究中未得到充分关注。

Method: PAUL框架通过不确定性感知的协同增强和证据协同训练，对数据进行分区和增强，以抑制噪声对的影响。

Result: PAUL在不同噪声比例下均优于其他竞争方法，验证了其组件的有效性。

Conclusion: PAUL为跨视角地理定位中的噪声对应问题提供了有效的解决方案，填补了理想化基准与实际应用之间的差距。

Abstract: Cross-view geo-localization is a critical task for UAV navigation, event
detection, and aerial surveying, as it enables matching between drone-captured
and satellite imagery. Most existing approaches embed multi-modal data into a
joint feature space to maximize the similarity of paired images. However, these
methods typically assume perfect alignment of image pairs during training,
which rarely holds true in real-world scenarios. In practice, factors such as
urban canyon effects, electromagnetic interference, and adverse weather
frequently induce GPS drift, resulting in systematic alignment shifts where
only partial correspondences exist between pairs. Despite its prevalence, this
source of noisy correspondence has received limited attention in current
research. In this paper, we formally introduce and address the Noisy
Correspondence on Cross-View Geo-Localization (NC-CVGL) problem, aiming to
bridge the gap between idealized benchmarks and practical applications. To this
end, we propose PAUL (Partition and Augmentation by Uncertainty Learning), a
novel framework that partitions and augments training data based on estimated
data uncertainty through uncertainty-aware co-augmentation and evidential
co-training. Specifically, PAUL selectively augments regions with high
correspondence confidence and utilizes uncertainty estimation to refine feature
learning, effectively suppressing noise from misaligned pairs. Distinct from
traditional filtering or label correction, PAUL leverages both data uncertainty
and loss discrepancy for targeted partitioning and augmentation, thus providing
robust supervision for noisy samples. Comprehensive experiments validate the
effectiveness of individual components in PAUL,which consistently achieves
superior performance over other competitive noisy-correspondence-driven methods
in various noise ratios.

</details>


### [85] [Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies](https://arxiv.org/abs/2508.20072)
*Zhixuan Liang,Yizhuo Li,Tianshuo Yang,Chengyue Wu,Sitong Mao,Liuao Pei,Xiaokang Yang,Jiangmiao Pang,Yao Mu,Ping Luo*

Main category: cs.CV

TL;DR: Discrete Diffusion VLA是一种单变压器策略，通过离散扩散建模离散动作块，与VLM主干兼容，支持并行解码和自适应解码顺序，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有VLA解码器要么采用固定顺序自回归生成动作，要么依赖连续扩散或流匹配头，导致训练复杂且难以统一扩展。

Method: 提出Discrete Diffusion VLA，利用离散扩散建模动作块，采用与VLM主干相同的交叉熵目标训练，支持自适应解码和二次掩码修正。

Result: 在LIBERO、SimplerEnv Fractal和SimplerEnv Bridge上分别达到96.3%、71.2%和49.3%的性能，优于自回归和连续扩散基线。

Conclusion: 离散扩散动作解码器支持精确动作建模和一致训练，为VLA扩展到更大模型和数据集奠定了基础。

Abstract: Vision-Language-Action (VLA) models adapt large vision-language backbones to
map images and instructions to robot actions. However, prevailing VLA decoders
either generate actions autoregressively in a fixed left-to-right order or
attach continuous diffusion or flow matching heads outside the backbone,
demanding specialized training and iterative sampling that hinder a unified,
scalable architecture. We present Discrete Diffusion VLA, a single-transformer
policy that models discretized action chunks with discrete diffusion and is
trained with the same cross-entropy objective as the VLM backbone. The design
retains diffusion's progressive refinement paradigm while remaining natively
compatible with the discrete token interface of VLMs. Our method achieves an
adaptive decoding order that resolves easy action elements before harder ones
and uses secondary remasking to revisit uncertain predictions across refinement
rounds, which improves consistency and enables robust error correction. This
unified decoder preserves pretrained vision language priors, supports parallel
decoding, breaks the autoregressive bottleneck, and reduces the number of
function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,
71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv
Bridge, improving over both autoregressive and continuous diffusion baselines.
These findings indicate that discrete-diffusion action decoder supports precise
action modeling and consistent training, laying groundwork for scaling VLA to
larger models and datasets.

</details>


### [86] [Seam360GS: Seamless 360° Gaussian Splatting from Real-World Omnidirectional Images](https://arxiv.org/abs/2508.20080)
*Changha Shin,Woong Oh Cho,Seon Joo Kim*

Main category: cs.CV

TL;DR: 提出了一种新的双鱼眼相机校准框架，结合3D高斯溅射技术，优化360度图像渲染效果。


<details>
  <summary>Details</summary>
Motivation: 解决消费级双鱼眼系统因镜头分离和角度失真导致的不完美全景图问题。

Method: 将双鱼眼相机模型融入3D高斯溅射流程，联合优化3D高斯参数和校准变量。

Result: 在真实数据集上验证，该方法能从不完美图像生成无缝渲染，优于现有360度渲染模型。

Conclusion: 该框架有效提升了双鱼眼相机的360度图像合成质量。

Abstract: 360-degree visual content is widely shared on platforms such as YouTube and
plays a central role in virtual reality, robotics, and autonomous navigation.
However, consumer-grade dual-fisheye systems consistently yield imperfect
panoramas due to inherent lens separation and angular distortions. In this
work, we introduce a novel calibration framework that incorporates a
dual-fisheye camera model into the 3D Gaussian splatting pipeline. Our approach
not only simulates the realistic visual artifacts produced by dual-fisheye
cameras but also enables the synthesis of seamlessly rendered 360-degree
images. By jointly optimizing 3D Gaussian parameters alongside calibration
variables that emulate lens gaps and angular distortions, our framework
transforms imperfect omnidirectional inputs into flawless novel view synthesis.
Extensive evaluations on real-world datasets confirm that our method produces
seamless renderings-even from imperfect images-and outperforms existing
360-degree rendering models.

</details>


### [87] [AudioStory: Generating Long-Form Narrative Audio with Large Language Models](https://arxiv.org/abs/2508.20088)
*Yuxin Guo,Teng Wang,Yuying Ge,Shijie Ma,Yixiao Ge,Wei Zou,Ying Shan*

Main category: cs.CV

TL;DR: AudioStory是一个结合大语言模型（LLM）和文本到音频（TTA）系统的框架，用于生成长篇叙事音频，解决了现有方法在时间连贯性和组合推理上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有TTA生成方法在短音频合成上表现优异，但在长篇叙事音频中缺乏时间连贯性和组合推理能力。

Method: AudioStory通过LLM分解复杂叙事查询为有序子任务，并采用解耦桥接机制和端到端训练，实现语义对齐和跨事件连贯性。

Result: 实验表明，AudioStory在单音频生成和叙事音频生成上均优于现有TTA基线，指令跟随能力和音频保真度更高。

Conclusion: AudioStory为生成长篇叙事音频提供了一种高效统一的解决方案，并在多个领域验证了其优越性。

Abstract: Recent advances in text-to-audio (TTA) generation excel at synthesizing short
audio clips but struggle with long-form narrative audio, which requires
temporal coherence and compositional reasoning. To address this gap, we propose
AudioStory, a unified framework that integrates large language models (LLMs)
with TTA systems to generate structured, long-form audio narratives. AudioStory
possesses strong instruction-following reasoning generation capabilities. It
employs LLMs to decompose complex narrative queries into temporally ordered
sub-tasks with contextual cues, enabling coherent scene transitions and
emotional tone consistency. AudioStory has two appealing features: (1)
Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser
collaboration into two specialized components, i.e., a bridging query for
intra-event semantic alignment and a residual query for cross-event coherence
preservation. (2) End-to-end training: By unifying instruction comprehension
and audio generation within a single end-to-end framework, AudioStory
eliminates the need for modular training pipelines while enhancing synergy
between components. Furthermore, we establish a benchmark AudioStory-10K,
encompassing diverse domains such as animated soundscapes and natural sound
narratives. Extensive experiments show the superiority of AudioStory on both
single-audio generation and narrative audio generation, surpassing prior TTA
baselines in both instruction-following ability and audio fidelity. Our code is
available at https://github.com/TencentARC/AudioStory

</details>


### [88] [Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors](https://arxiv.org/abs/2508.20089)
*Ross J Gardiner,Guillaume Mougeot,Sareh Rowlands,Benno I Simmons,Flemming Helsing,Toke Thomas Høye*

Main category: cs.CV

TL;DR: 提出了一种轻量级分类方法，结合专家标记的野外数据和BioCLIP2基础模型的知识蒸馏，用于蛾类图像分类，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于野外图像与精心整理的图像之间存在领域差异，准确识别蛾类物种具有挑战性，这对理解昆虫数量下降至关重要。

Method: 结合有限的专家标记野外数据和BioCLIP2基础模型的知识蒸馏，采用ConvNeXt-tiny架构进行分类。

Result: 在101种丹麦蛾类的实验中，BioCLIP2表现优于其他方法，蒸馏后的轻量级模型在显著降低计算成本的同时保持了相当的准确性。

Conclusion: 该方法为高效昆虫监测系统的开发和解决细粒度分类中的领域差距提供了实用指南。

Abstract: Labelling images of Lepidoptera (moths) from automated camera systems is
vital for understanding insect declines. However, accurate species
identification is challenging due to domain shifts between curated images and
noisy field imagery. We propose a lightweight classification approach,
combining limited expert-labelled field data with knowledge distillation from
the high-performance BioCLIP2 foundation model into a ConvNeXt-tiny
architecture. Experiments on 101 Danish moth species from AMI camera systems
demonstrate that BioCLIP2 substantially outperforms other methods and that our
distilled lightweight model achieves comparable accuracy with significantly
reduced computational cost. These insights offer practical guidelines for the
development of efficient insect monitoring systems and bridging domain gaps for
fine-grained classification.

</details>


### [89] [CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning](https://arxiv.org/abs/2508.20096)
*Zeyi Sun,Yuhang Cao,Jianze Liang,Qiushi Sun,Ziyu Liu,Zhixiong Zhang,Yuhang Zang,Xiaoyi Dong,Kai Chen,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: CODA是一种新型可训练的组合框架，结合通用规划器和专业执行器，通过两阶段训练提升GUI自主代理在科学计算领域的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有GUI自主代理在科学计算领域中长期规划和精确执行之间的权衡问题，克服静态组合框架无法适应经验的局限性。

Method: 采用两阶段训练：1) 专业化阶段，通过GRPO方法训练专家规划器；2) 泛化阶段，整合成功轨迹进行监督微调。

Result: 在ScienceBoard基准测试的四个应用中，CODA显著优于基线模型，达到开源模型的新最佳性能。

Conclusion: CODA通过可训练的组合框架，成功实现了科学计算领域中GUI代理的长期规划和精确执行，并具备跨领域泛化能力。

Abstract: Autonomous agents for Graphical User Interfaces (GUIs) face significant
challenges in specialized domains such as scientific computing, where both
long-horizon planning and precise execution are required. Existing approaches
suffer from a trade-off: generalist agents excel at planning but perform poorly
in execution, while specialized agents demonstrate the opposite weakness.
Recent compositional frameworks attempt to bridge this gap by combining a
planner and an actor, but they are typically static and non-trainable, which
prevents adaptation from experience. This is a critical limitation given the
scarcity of high-quality data in scientific domains. To address these
limitations, we introduce CODA, a novel and trainable compositional framework
that integrates a generalist planner (Cerebrum) with a specialist executor
(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,
Specialization, we apply a decoupled GRPO approach to train an expert planner
for each scientific application individually, bootstrapping from a small set of
task trajectories. In the second stage, Generalization, we aggregate all
successful trajectories from the specialized experts to build a consolidated
dataset, which is then used for supervised fine-tuning of the final planner.
This equips CODA with both robust execution and cross-domain generalization.
Evaluated on four challenging applications from the ScienceBoard benchmark,
CODA significantly outperforms baselines and establishes a new state of the art
among open-source models.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [90] [A Technical Review on Comparison and Estimation of Steganographic Tools](https://arxiv.org/abs/2508.19323)
*Ms. Preeti P. Bhatt,Rakesh R. Savant*

Main category: cs.CR

TL;DR: 该论文综述了图像隐写术的分类，并比较了不同图像格式下的多种隐写工具，通过实验评估了六种常用工具的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是比较不同图像隐写工具的性能，以找出最有效的工具。

Method: 选择了六种常用工具，使用相同的输入进行测试，嵌入特定文本到宿主图像中，分析工具在图像特征（如大小、尺寸、像素值和直方图差异）上的表现。

Result: 实验结果显示，六种工具性能相近，但某些工具在效率上表现更优。

Conclusion: 结论是不同工具在性能上差异不大，但效率因图像特征而异。

Abstract: Steganography is technique of hiding a data under cover media using different
steganography tools. Image steganography is hiding of data
(Text/Image/Audio/Video) under a cover as Image. This review paper presents
classification of image steganography and the comparison of various Image
steganography tools using different image formats. Analyzing numerous tools on
the basis of Image features and extracting the best one. Some of the tools
available in the market were selected based on the frequent use; these tools
were tested using the same input on all of them. Specific text was embedded
within all host images for each of the six Steganography tools selected. The
results of the experiment reveal that all the six tools were relatively
performing at the same level, though some software performs better than others
through efficiency. And it was based on the image features like size,
dimensions, and pixel value and histogram differentiation.

</details>


### [91] [Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents](https://arxiv.org/abs/2508.19493)
*Zhixin Lin,Jungang Li,Shidong Pan,Yibo Shi,Yue Yao,Dongliang Xu*

Main category: cs.CR

TL;DR: 论文提出了一个大规模基准测试（7,138个场景）来评估智能手机代理的隐私意识，发现主流代理的隐私意识普遍不足，性能低于60%。封闭源代理表现优于开源代理，Gemini 2.0-flash表现最佳（RA 67%）。


<details>
  <summary>Details</summary>
Motivation: 智能手机代理在自动化任务中表现优异，但会访问大量敏感用户信息，目前缺乏对其隐私意识的全面评估。

Method: 构建包含7,138个场景的基准测试，标注隐私类型、敏感度级别和位置，并评估七种主流智能手机代理。

Result: 几乎所有代理的隐私意识表现不佳（RA <60%），封闭源代理优于开源代理，Gemini 2.0-flash表现最佳（RA 67%）。隐私检测能力与场景敏感度级别高度相关。

Conclusion: 研究揭示了智能手机代理在隐私意识方面的不足，呼吁研究社区重新思考效用与隐私之间的不平衡权衡。

Abstract: Smartphones bring significant convenience to users but also enable devices to
extensively record various types of personal information. Existing smartphone
agents powered by Multimodal Large Language Models (MLLMs) have achieved
remarkable performance in automating different tasks. However, as the cost,
these agents are granted substantial access to sensitive users' personal
information during this operation. To gain a thorough understanding of the
privacy awareness of these agents, we present the first large-scale benchmark
encompassing 7,138 scenarios to the best of our knowledge. In addition, for
privacy context in scenarios, we annotate its type (e.g., Account Credentials),
sensitivity level, and location. We then carefully benchmark seven available
mainstream smartphone agents. Our results demonstrate that almost all
benchmarked agents show unsatisfying privacy awareness (RA), with performance
remaining below 60% even with explicit hints. Overall, closed-source agents
show better privacy ability than open-source ones, and Gemini 2.0-flash
achieves the best, achieving an RA of 67%. We also find that the agents'
privacy detection capability is highly related to scenario sensitivity level,
i.e., the scenario with a higher sensitivity level is typically more
identifiable. We hope the findings enlighten the research community to rethink
the unbalanced utility-privacy tradeoff about smartphone agents. Our code and
benchmark are available at https://zhixin-l.github.io/SAPA-Bench.

</details>


### [92] [Addressing Deepfake Issue in Selfie banking through camera based authentication](https://arxiv.org/abs/2508.19714)
*Subhrojyoti Mukherjee,Manoranjan Mohanty*

Main category: cs.CR

TL;DR: 论文探讨了利用已有的法医识别系统检测深度伪造图像，以应对自拍银行中的虚假图像威胁。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术使伪造身份更加逼真，欺诈者利用其绕过在线银行的面部识别系统，威胁自拍银行安全。

Method: 采用已有的法医识别系统（原用于图片相机定位）进行深度伪造检测。

Result: 未明确提及具体结果，但提出了利用现有系统的新应用方向。

Conclusion: 已有法医识别系统可扩展用于深度伪造检测，为自拍银行安全提供潜在解决方案。

Abstract: Fake images in selfie banking are increasingly becoming a threat. Previously,
it was just Photoshop, but now deep learning technologies enable us to create
highly realistic fake identities, which fraudsters exploit to bypass biometric
systems such as facial recognition in online banking. This paper explores the
use of an already established forensic recognition system, previously used for
picture camera localization, in deepfake detection.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [93] [SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control](https://arxiv.org/abs/2508.20018)
*Quanfeng Lu,Zhantao Ma,Shuai Zhong,Jin Wang,Dahai Yu,Michael K. Ng,Ping Luo*

Main category: cs.AI

TL;DR: SWIRL是一种分阶段的多智能体强化学习方法，通过将多智能体任务分解为单智能体任务序列，提升训练效率和协调性。


<details>
  <summary>Details</summary>
Motivation: 现有单智能体方法在移动GUI代理中存在结构限制，而多智能体强化学习（MARL）效率低且与当前大视觉语言模型（LVLM）架构不兼容。

Method: SWIRL将MARL重新表述为单智能体强化学习任务序列，每次更新一个智能体，其他智能体固定，确保稳定训练和高效协调。

Result: 在移动GUI控制任务中，SWIRL表现优异，同时在多智能体数学推理中也展现出强大能力。

Conclusion: SWIRL是一种高效、鲁棒的多智能体系统通用框架，适用于GUI任务及其他多智能体场景。

Abstract: The rapid advancement of large vision language models (LVLMs) and agent
systems has heightened interest in mobile GUI agents that can reliably
translate natural language into interface operations. Existing single-agent
approaches, however, remain limited by structural constraints. Although
multi-agent systems naturally decouple different competencies, recent progress
in multi-agent reinforcement learning (MARL) has often been hindered by
inefficiency and remains incompatible with current LVLM architectures. To
address these challenges, we introduce SWIRL, a staged workflow for interleaved
reinforcement learning designed for multi-agent systems. SWIRL reformulates
MARL into a sequence of single-agent reinforcement learning tasks, updating one
agent at a time while keeping the others fixed. This formulation enables stable
training and promotes efficient coordination across agents. Theoretically, we
provide a stepwise safety bound, a cross-round monotonic improvement theorem,
and convergence guarantees on return, ensuring robust and principled
optimization. In application to mobile GUI control, SWIRL instantiates a
Navigator that converts language and screen context into structured plans, and
an Interactor that grounds these plans into executable atomic actions.
Extensive experiments demonstrate superior performance on both high-level and
low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong
capability in multi-agent mathematical reasoning, underscoring its potential as
a general framework for developing efficient and robust multi-agent systems.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [94] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)
*Gueter Josmy Faure,Min-Hung Chen,Jia-Fong Yeh,Ying Cheng,Hung-Ting Su,Yung-Hao Tang,Shang-Hong Lai,Winston H. Hsu*

Main category: cs.CL

TL;DR: MovieCORE是一个新的视频问答数据集，专注于电影内容的深层认知理解，通过多LLM代理生成高质量问题-答案对，并提出评估方案和增强模块ACE。


<details>
  <summary>Details</summary>
Motivation: 现有数据集多关注表面理解，缺乏对电影内容的深层认知理解，因此需要更复杂的问题和评估方法。

Method: 采用多LLM代理的头脑风暴方法生成问题-答案对，开发认知测试评估数据集质量，并提出ACE模块增强模型推理能力。

Result: ACE模块将模型推理能力提升25%，数据集和评估方案为电影理解提供了新基准。

Conclusion: MovieCORE推动了AI对电影内容的理解，揭示了当前VQA模型在复杂问题上的局限性，并提供了改进方向。

Abstract: This paper introduces MovieCORE, a novel video question answering (VQA)
dataset designed to probe deeper cognitive understanding of movie content.
Unlike existing datasets that focus on surface-level comprehension, MovieCORE
emphasizes questions that engage System-2 thinking while remaining specific to
the video material. We present an innovative agentic brainstorming approach,
utilizing multiple large language models (LLMs) as thought agents to generate
and refine high-quality question-answer pairs. To evaluate dataset quality, we
develop a set of cognitive tests assessing depth, thought-provocation
potential, and syntactic complexity. We also propose a comprehensive evaluation
scheme for assessing VQA model performance on deeper cognitive tasks. To
address the limitations of existing video-language models (VLMs), we introduce
an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves
model reasoning capabilities post-training by up to 25%. Our work contributes
to advancing movie understanding in AI systems and provides valuable insights
into the capabilities and limitations of current VQA models when faced with
more challenging, nuanced questions about cinematic content. Our project page,
dataset and code can be found at
https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [95] [Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement](https://arxiv.org/abs/2508.19887)
*Mohammed Rakibul Hasan,Rafi Majid,Ahanaf Tahmid*

Main category: cs.CL

TL;DR: Bangla-Bayanno是一个高质量、开放式的孟加拉语视觉问答数据集，旨在推动低资源多模态学习研究。


<details>
  <summary>Details</summary>
Motivation: 解决现有数据集在特定领域或格式上的局限性，以及多语言翻译质量问题。

Method: 采用多语言LLM辅助的翻译优化流程，确保数据质量。

Result: 数据集包含52,650个问答对和4,750+图像，涵盖三类问题类型。

Conclusion: Bangla-Bayanno为孟加拉语提供了最全面的开源VQA基准，促进包容性AI发展。

Abstract: In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question
Answering (VQA) Dataset in Bangla, a widely used, low-resource language in
multimodal AI research. The majority of existing datasets are either manually
annotated with an emphasis on a specific domain, query type, or answer type or
are constrained by niche answer formats. In order to mitigate human-induced
errors and guarantee lucidity, we implemented a multilingual LLM-assisted
translation refinement pipeline. This dataset overcomes the issues of
low-quality translations from multilingual sources. The dataset comprises
52,650 question-answer pairs across 4750+ images. Questions are classified into
three distinct answer types: nominal (short descriptive), quantitative
(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive
open-source, high-quality VQA benchmark in Bangla, aiming to advance research
in low-resource multimodal learning and facilitate the development of more
inclusive AI systems.

</details>


### [96] [11Plus-Bench: Demystifying Multimodal LLM Spatial Reasoning with Cognitive-Inspired Analysis](https://arxiv.org/abs/2508.20068)
*Chengzu Li,Wenshan Wu,Huanyu Zhang,Qingtao Li,Zeyu Gao,Yan Xia,José Hernández-Orallo,Ivan Vulić,Furu Wei*

Main category: cs.CL

TL;DR: 本文提出了一个系统评估框架11Plus-Bench，用于评估多模态大语言模型（MLLMs）的空间推理能力，并与人类表现对比。


<details>
  <summary>Details</summary>
Motivation: 探索MLLMs在空间推理和感知方面的能力，填补现有研究中对这一领域评估的不足。

Method: 通过11Plus-Bench基准测试，结合细粒度专家注释，对14种MLLMs进行实验，并与人类表现对比。

Result: MLLMs显示出初步的空间认知能力，但与人类相比仍有较大差距；其认知模式与人类相似，但实例级表现仍较随机。

Conclusion: 当前MLLMs在空间推理方面具备潜力但仍有局限，研究结果为模型设计提供了可操作的改进方向。

Abstract: For human cognitive process, spatial reasoning and perception are closely
entangled, yet the nature of this interplay remains underexplored in the
evaluation of multimodal large language models (MLLMs). While recent MLLM
advancements show impressive performance on reasoning, their capacity for
human-like spatial cognition remains an open question. In this work, we
introduce a systematic evaluation framework to assess the spatial reasoning
abilities of state-of-the-art MLLMs relative to human performance. Central to
our work is 11Plus-Bench, a high-quality benchmark derived from realistic
standardized spatial aptitude tests. 11Plus-Bench also features fine-grained
expert annotations of both perceptual complexity and reasoning process,
enabling detailed instance-level analysis of model behavior. Through extensive
experiments across 14 MLLMs and human evaluation, we find that current MLLMs
exhibit early signs of spatial cognition. Despite a large performance gap
compared to humans, MLLMs' cognitive profiles resemble those of humans in that
cognitive effort correlates strongly with reasoning-related complexity.
However, instance-level performance in MLLMs remains largely random, whereas
human correctness is highly predictable and shaped by abstract pattern
complexity. These findings highlight both emerging capabilities and limitations
in current MLLMs' spatial reasoning capabilities and provide actionable
insights for advancing model design.

</details>


<div id='astro-ph.IM'></div>

# astro-ph.IM [[Back]](#toc)

### [97] [Modeling spectral filtering effects on color-matching functions: Implications for observer variability](https://arxiv.org/abs/2508.19291)
*Luvin Munish Ragoo,Ivar Farup,Casper F. Andersen,Graham Finlayson*

Main category: astro-ph.IM

TL;DR: 研究探讨了光谱滤波对颜色匹配函数（CMFs）的影响及其在观察者变异性建模中的意义，提出了一种高效表征变异性方法。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解光谱滤波如何影响CMFs，并探索观察者变异性建模的简化方法。

Method: 通过颜色匹配实验和计算模型，估计滤波透射率和转换矩阵，比较不同CMF数据集。

Result: 发现滤波特性与测量结果一致，并识别出能转换不同CMF数据集的“黄色”滤波。

Conclusion: 该方法通过单一滤波高效表征观察者变异性，减少实验开销，同时保持准确性。

Abstract: This study investigates the impact of spectral filtering on color-matching
functions (CMFs) and its implications for observer variability modeling. We
conducted color matching experiments with a single observer, both with and
without a spectral filter in front of a bipartite field. Using a novel
computational approach, we estimated the filter transmittance and
transformation matrix necessary to convert unfiltered CMFs to filtered CMFs.
Statistical analysis revealed good agreement between estimated and measured
filter characteristics, particularly in central wavelength regions. Applying
this methodology to compare between Stiles and Burch 1955 (SB1955) mean
observer CMFs and our previously published "ICVIO" mean observer CMFs, we
identified a "yellow" (short-wavelength suppressing) filter that effectively
transforms between these datasets. This finding aligns with our hypothesis that
observed differences between the CMF sets are attributable to age-related lens
yellowing (average observer age: 49 years in ICVIO versus 30 years in SB1955).
Our approach enables efficient representation of observer variability through a
single filter rather than three separate functions, offering potentially
reduced experimental overhead while maintaining accuracy in characterizing
individual color vision differences.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [98] [CellINR: Implicitly Overcoming Photo-induced Artifacts in 4D Live Fluorescence Microscopy](https://arxiv.org/abs/2508.19300)
*Cunmin Zhao,Ziyuan Luo,Guoye Guan,Zelin Li,Yiming Ma,Zhongying Zhao,Renjie Wan*

Main category: eess.IV

TL;DR: CellINR框架通过隐式神经表示和盲卷积策略，有效减少4D活细胞荧光显微镜中的光漂白和光毒性伪影，提升图像连续性和细节恢复。


<details>
  <summary>Details</summary>
Motivation: 解决高强度光照导致的光漂白和光毒性问题，避免伪影并恢复图像细节。

Method: 采用隐式神经表示和盲卷积策略，结合结构增强技术，将3D空间坐标映射到高频域。

Result: CellINR在伪影去除和结构连续性恢复上显著优于现有技术，并首次提供了配对的4D活细胞成像数据集。

Conclusion: CellINR为后续定量分析和生物研究提供了可靠基础，代码和数据集将公开。

Abstract: 4D live fluorescence microscopy is often compromised by prolonged high
intensity illumination which induces photobleaching and phototoxic effects that
generate photo-induced artifacts and severely impair image continuity and
detail recovery. To address this challenge, we propose the CellINR framework, a
case-specific optimization approach based on implicit neural representation.
The method employs blind convolution and structure amplification strategies to
map 3D spatial coordinates into the high frequency domain, enabling precise
modeling and high-accuracy reconstruction of cellular structures while
effectively distinguishing true signals from artifacts. Experimental results
demonstrate that CellINR significantly outperforms existing techniques in
artifact removal and restoration of structural continuity, and for the first
time, a paired 4D live cell imaging dataset is provided for evaluating
reconstruction performance, thereby offering a solid foundation for subsequent
quantitative analyses and biological research. The code and dataset will be
public.

</details>


### [99] [2D Ultrasound Elasticity Imaging of Abdominal Aortic Aneurysms Using Deep Neural Networks](https://arxiv.org/abs/2508.19303)
*Utsav Ratna Tuladhar,Richard Simon,Doran Mix,Michael Richards*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的弹性成像框架，用于通过2D超声评估腹主动脉瘤（AAA）的血管壁弹性模量分布，以改进破裂风险评估。


<details>
  <summary>Details</summary>
Motivation: 当前AAA风险评估主要依赖最大直径，但忽略了血管壁材料特性对破裂风险的关键影响。

Method: 利用有限元模拟生成位移场数据集，训练U-Net模型以从位移场推断弹性模量分布，并在数字模拟、物理模型和临床数据上验证。

Result: 模型在模拟数据上NMSE为0.73%，在物理模型中预测模量比与预期一致，且计算速度优于迭代方法。

Conclusion: 深度学习方法能快速、非侵入地评估AAA组织刚度，有望改进破裂风险评估。

Abstract: Abdominal aortic aneurysms (AAA) pose a significant clinical risk due to
their potential for rupture, which is often asymptomatic but can be fatal.
Although maximum diameter is commonly used for risk assessment, diameter alone
is insufficient as it does not capture the properties of the underlying
material of the vessel wall, which play a critical role in determining the risk
of rupture. To overcome this limitation, we propose a deep learning-based
framework for elasticity imaging of AAAs with 2D ultrasound. Leveraging finite
element simulations, we generate a diverse dataset of displacement fields with
their corresponding modulus distributions. We train a model with U-Net
architecture and normalized mean squared error (NMSE) to infer the spatial
modulus distribution from the axial and lateral components of the displacement
fields. This model is evaluated across three experimental domains: digital
phantom data from 3D COMSOL simulations, physical phantom experiments using
biomechanically distinct vessel models, and clinical ultrasound exams from AAA
patients. Our simulated results demonstrate that the proposed deep learning
model is able to reconstruct modulus distributions, achieving an NMSE score of
0.73\%. Similarly, in phantom data, the predicted modular ratio closely matches
the expected values, affirming the model's ability to generalize to phantom
data. We compare our approach with an iterative method which shows comparable
performance but higher computation time. In contrast, the deep learning method
can provide quick and effective estimates of tissue stiffness from ultrasound
images, which could help assess the risk of AAA rupture without invasive
procedures.

</details>


### [100] [MedVQA-TREE: A Multimodal Reasoning and Retrieval Framework for Sarcopenia Prediction](https://arxiv.org/abs/2508.19319)
*Pardis Moradbeiki,Nasser Ghadiri,Sayed Jalal Zahabi,Uffe Kock Wiil,Kristoffer Kittelmann Brockhattingen,Ali Ebrahimi*

Main category: eess.IV

TL;DR: MedVQA-TREE是一个多模态框架，通过结合视觉理解和知识检索，显著提高了超声诊断肌肉减少症的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前超声诊断肌肉减少症面临成像线索细微、标记数据有限和缺乏临床背景的挑战。

Method: 框架包括分层图像解释模块、门控特征级融合机制和多跳多查询检索策略，结合视觉特征与临床知识。

Result: 在两个公共数据集和自定义数据集上达到99%的诊断准确率，优于现有方法10%以上。

Conclusion: 结合结构化视觉理解和知识检索对AI辅助诊断肌肉减少症非常有效。

Abstract: Accurate sarcopenia diagnosis via ultrasound remains challenging due to
subtle imaging cues, limited labeled data, and the absence of clinical context
in most models. We propose MedVQA-TREE, a multimodal framework that integrates
a hierarchical image interpretation module, a gated feature-level fusion
mechanism, and a novel multi-hop, multi-query retrieval strategy. The vision
module includes anatomical classification, region segmentation, and graph-based
spatial reasoning to capture coarse, mid-level, and fine-grained structures. A
gated fusion mechanism selectively integrates visual features with textual
queries, while clinical knowledge is retrieved through a UMLS-guided pipeline
accessing PubMed and a sarcopenia-specific external knowledge base. MedVQA-TREE
was trained and evaluated on two public MedVQA datasets (VQA-RAD and PathVQA)
and a custom sarcopenia ultrasound dataset. The model achieved up to 99%
diagnostic accuracy and outperformed previous state-of-the-art methods by over
10%. These results underscore the benefit of combining structured visual
understanding with guided knowledge retrieval for effective AI-assisted
diagnosis in sarcopenia.

</details>


### [101] [AT-CXR: Uncertainty-Aware Agentic Triage for Chest X-rays](https://arxiv.org/abs/2508.19322)
*Xueyang Li,Mingze Jiang,Gelei Xu,Jun Xia,Mengzhao Jia,Danny Chen,Yiyu Shi*

Main category: eess.IV

TL;DR: AT-CXR是一个不确定性感知的AI代理，用于胸部X光片的自动分诊，通过评估置信度和分布拟合来决定是否自动决策或建议人工干预。


<details>
  <summary>Details</summary>
Motivation: 当前自主医学影像分诊系统在真实约束下的决策能力尚未充分探索，AT-CXR旨在填补这一空白。

Method: AT-CXR采用两种路由器设计：确定性规则路由器和LLM决策路由器，评估其在NIH ChestX-ray14数据集上的性能。

Result: 两种路由器在五折评估中均优于零样本视觉语言模型和监督分类器，具有更高的准确性和选择性预测性能，且延迟更低。

Conclusion: AT-CXR提供了互补的操作点，适用于不同优先级的临床部署，代码已开源。

Abstract: Agentic AI is advancing rapidly, yet truly autonomous medical-imaging triage,
where a system decides when to stop, escalate, or defer under real constraints,
remains relatively underexplored. To address this gap, we introduce AT-CXR, an
uncertainty-aware agent for chest X-rays. The system estimates per-case
confidence and distributional fit, then follows a stepwise policy to issue an
automated decision or abstain with a suggested label for human intervention. We
evaluate two router designs that share the same inputs and actions: a
deterministic rule-based router and an LLM-decided router. Across five-fold
evaluation on a balanced subset of NIH ChestX-ray14 dataset, both variants
outperform strong zero-shot vision-language models and state-of-the-art
supervised classifiers, achieving higher full-coverage accuracy and superior
selective-prediction performance, evidenced by a lower area under the
risk-coverage curve (AURC) and a lower error rate at high coverage, while
operating with lower latency that meets practical clinical constraints. The two
routers provide complementary operating points, enabling deployments to
prioritize maximal throughput or maximal accuracy. Our code is available at
https://github.com/XLIAaron/uncertainty-aware-cxr-agent.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [102] [Saccade crossing avoidance as a visual search strategy](https://arxiv.org/abs/2508.18404)
*Alex Szorkovszky,Rujeena Mathema,Pedro Lencastre,Pedro Lind,Anis Yazidi*

Main category: q-bio.NC

TL;DR: 研究发现了一种新的记忆依赖性效应，称为自我交叉避免，即眼跳倾向于避免与早期扫描路径交叉，尤其是在小幅度眼跳时。


<details>
  <summary>Details</summary>
Motivation: 量化视觉搜索中较长路径历史对眼跳方向的影响，并探索个体间的差异。

Method: 使用运动生态学中的步骤选择框架，分析45秒观看“Where's Waldo”的数据，并与无记忆模型生成的合成数据对比。

Result: 自我交叉避免效应最强时涉及约7秒的扫描路径历史，效应大小与已知的历史依赖性形式相当。

Conclusion: 自我交叉避免是一种局部定向策略，有助于补充抑制返回，促进视觉场景的探索。

Abstract: Although visual search appears largely random, several oculomotor biases
exist such that the likelihoods of saccade directions and lengths depend on the
previous scan path. Compared to the most recent fixations, the impact of the
longer path history is more difficult to quantify. Using the step-selection
framework commonly used in movement ecology, and analyzing data from 45-second
viewings of "Where's Waldo"?, we report a new memory-dependent effect that also
varies significantly between individuals, which we term self-crossing
avoidance. This is a tendency for saccades to avoid crossing those earlier in
the scan path, and is most evident when both have small amplitudes. We show
this by comparing real data to synthetic data generated from a memoryless
approximation of the spatial statistics (i.e. a Markovian nonparametric model
with a matching distribution of saccade lengths over time). Maximum likelihood
fitting indicates that this effect is strongest when including the last
$\approx 7$ seconds of a scan path. The effect size is comparable to well-known
forms of history dependence such as inhibition of return. A parametric
probabilistic model including a self-crossing penalty term was able to
reproduce joint statistics of saccade lengths and self-crossings. We also
quantified individual strategic differences, and their consistency over the six
images viewed per participant, using mixed-effect regressions. Participants
with a higher tendency to avoid crossings displayed smaller saccade lengths and
shorter fixation durations on average, but did not display more horizontal,
vertical, forward or reverse saccades. Together, these results indicate that
the avoidance of crossings is a local orienting strategy that facilitates and
complements inhibition of return, and hence exploration of visual scenes.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [103] [Fast Texture Transfer for XR Avatars via Barycentric UV Conversion](https://arxiv.org/abs/2508.19518)
*Hail Song,Seokhwan Yang,Woontack Woo*

Main category: cs.GR

TL;DR: 提出了一种快速高效的方法，将面部纹理转移到基于SMPL-X的全身虚拟形象上，相比传统方法速度提升7000倍且质量更高。


<details>
  <summary>Details</summary>
Motivation: 传统基于仿射变换的方法速度慢且容易产生视觉伪影，需要一种更高效的解决方案。

Method: 采用重心UV转换技术，预计算整个UV映射到单一变换矩阵中，实现单次操作完成纹理转移。

Result: 速度提升7000倍，显著改善纹理质量，消除边界伪影。

Conclusion: 该方法为沉浸式XR应用中的个性化提供了实用解决方案，代码已开源。

Abstract: We present a fast and efficient method for transferring facial textures onto
SMPL-X-based full-body avatars. Unlike conventional affine-transform methods
that are slow and prone to visual artifacts, our method utilizes a barycentric
UV conversion technique. Our approach precomputes the entire UV mapping into a
single transformation matrix, enabling texture transfer in a single operation.
This results in a speedup of over 7000x compared to the baseline, while also
significantly improving the final texture quality by eliminating boundary
artifacts. Through quantitative and qualitative evaluations, we demonstrate
that our method offers a practical solution for personalization in immersive XR
applications. The code is available online.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [104] [Efficient Multi-Source Knowledge Transfer by Model Merging](https://arxiv.org/abs/2508.19353)
*Marcin Osial,Bartosz Wójcik,Bartosz Zieliński,Sebastian Cygert*

Main category: cs.LG

TL;DR: 提出了一种基于SVD的多源迁移学习框架，通过分解和聚合源模型的秩一组件，提升效率和精度。


<details>
  <summary>Details</summary>
Motivation: 现有迁移学习方法无法有效利用多源模型的知识，且缺乏精细的知识提取和高效聚合能力。

Method: 使用SVD分解源模型为秩一组件，选择最显著组件聚合，并通过微调主奇异值适应目标任务。

Result: 框架高效、鲁棒，能应对输入和参数空间的扰动，计算扩展性好。

Conclusion: 该方法显著提升了多源迁移学习的效率和适应性。

Abstract: While transfer learning is an advantageous strategy, it overlooks the
opportunity to leverage knowledge from numerous available models online.
Addressing this multi-source transfer learning problem is a promising path to
boost adaptability and cut re-training costs. However, existing approaches are
inherently coarse-grained, lacking the necessary precision for granular
knowledge extraction and the aggregation efficiency required to fuse knowledge
from either a large number of source models or those with high parameter
counts. We address these limitations by leveraging Singular Value Decomposition
(SVD) to first decompose each source model into its elementary, rank-one
components. A subsequent aggregation stage then selects only the most salient
components from all sources, thereby overcoming the previous efficiency and
precision limitations. To best preserve and leverage the synthesized knowledge
base, our method adapts to the target task by fine-tuning only the principal
singular values of the merged matrix. In essence, this process only
recalibrates the importance of top SVD components. The proposed framework
allows for efficient transfer learning, is robust to perturbations both at the
input level and in the parameter space (e.g., noisy or pruned sources), and
scales well computationally.

</details>


### [105] [Fine-Tuning Vision-Language Models for Neutrino Event Analysis in High-Energy Physics Experiments](https://arxiv.org/abs/2508.19376)
*Dikshant Sagar,Kaiwen Yu,Alejandro Yankelevich,Jianming Bian,Pierre Baldi*

Main category: cs.LG

TL;DR: 本文探讨了基于LLaMA 3.2的视觉语言模型（VLM）在高能物理实验中用于中微子相互作用分类的性能，发现其优于传统CNN方法。


<details>
  <summary>Details</summary>
Motivation: 探索视觉语言模型在多模态推理中的潜力，特别是在高能物理实验中的中微子相互作用分类任务。

Method: 使用基于LLaMA 3.2的VLM对像素化探测器图像进行分类，并与传统CNN方法进行性能对比。

Result: VLM在分类准确率、精确率、召回率和AUC-ROC等指标上匹配或超越CNN，并能更好地整合辅助文本或语义信息。

Conclusion: VLMs为高能物理中的事件分类提供了有前景的多模态方法，推动了实验性中微子物理的发展。

Abstract: Recent progress in large language models (LLMs) has shown strong potential
for multimodal reasoning beyond natural language. In this work, we explore the
use of a fine-tuned Vision-Language Model (VLM), based on LLaMA 3.2, for
classifying neutrino interactions from pixelated detector images in high-energy
physics (HEP) experiments. We benchmark its performance against an established
CNN baseline used in experiments like NOvA and DUNE, evaluating metrics such as
classification accuracy, precision, recall, and AUC-ROC. Our results show that
the VLM not only matches or exceeds CNN performance but also enables richer
reasoning and better integration of auxiliary textual or semantic context.
These findings suggest that VLMs offer a promising general-purpose backbone for
event classification in HEP, paving the way for multimodal approaches in
experimental neutrino physics.

</details>


### [106] [NM-Hebb: Coupling Local Hebbian Plasticity with Metric Learning for More Accurate and Interpretable CNNs](https://arxiv.org/abs/2508.19896)
*Davorin Miličević,Ratko Grbić*

Main category: cs.LG

TL;DR: NM-Hebb是一种结合神经启发局部可塑性和距离感知监督的两阶段训练框架，旨在解决CNN中的过拟合和可解释性问题。


<details>
  <summary>Details</summary>
Motivation: 传统CNN依赖全局梯度优化，易导致过拟合、冗余过滤器和可解释性差。NM-Hebb通过生物启发机制改进这些问题。

Method: 第一阶段结合交叉熵目标、Hebbian正则化和可学习神经调节器；第二阶段通过度量学习损失微调。

Result: 在多个数据集和骨干网络上，NM-Hebb显著提升准确率和NMI，并生成更结构化特征。

Conclusion: NM-Hebb不仅提高CNN准确性，还增强可解释性，适用于资源受限和安全关键场景。

Abstract: Deep Convolutional Neural Networks (CNNs) achieve high accuracy but often
rely on purely global, gradient-based optimisation, which can lead to
overfitting, redundant filters, and reduced interpretability. To address these
limitations, we propose NM-Hebb, a two-phase training framework that integrates
neuro-inspired local plasticity with distance-aware supervision. Phase 1
extends standard supervised training by jointly optimising a cross-entropy
objective with two biologically inspired mechanisms: (i) a Hebbian regulariser
that aligns the spatial mean of activations with the mean of the corresponding
convolutional filter weights, encouraging structured, reusable primitives; and
(ii) a learnable neuromodulator that gates an elastic-weight-style
consolidation loss, preserving beneficial parameters without freezing the
network. Phase 2 fine-tunes the backbone with a pairwise metric-learning loss,
explicitly compressing intra-class distances and enlarging inter-class margins
in the embedding space. Evaluated on CIFAR-10, CIFAR-100, and TinyImageNet
across five backbones (ResNet-18, VGG-11, MobileNet-v2, EfficientNet-V2,
DenseNet-121), NM-Hebb achieves consistent gains over baseline and other
methods: Top-1 accuracy improves by +2.0-10.0 pp (CIFAR-10), +2.0-9.0 pp
(CIFAR-100), and up to +4.3-8.9 pp (TinyImageNet), with Normalised Mutual
Information (NMI) increased by up to +0.15. Qualitative visualisations and
filter-level analyses further confirm that NM-Hebb produces more structured and
selective features, yielding tighter and more interpretable class clusters.
Overall, coupling local Hebbian plasticity with metric-based fine-tuning yields
CNNs that are not only more accurate but also more interpretable, offering
practical benefits for resource-constrained and safety-critical AI deployments.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [107] [DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View](https://arxiv.org/abs/2508.19508)
*Tian Qiu,Alan Zoubi,Yiyuan Lin,Ruiming Du,Lailiang Cheng,Yu Jiang*

Main category: cs.RO

TL;DR: DATR框架通过两阶段方法从稀疏视图中重建苹果树3D模型，优于现有方法，并显著提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在稀疏和遮挡视图下表现不佳，限制了数字孪生在农业中的应用。

Method: 采用两阶段框架：第一阶段生成树掩码，第二阶段结合扩散模型和大重建模型进行3D重建。

Result: DATR在真实和合成数据集上均优于现有方法，吞吐量提升约360倍。

Conclusion: DATR展示了可扩展农业数字孪生系统的潜力。

Abstract: Digital twin applications offered transformative potential by enabling
real-time monitoring and robotic simulation through accurate virtual replicas
of physical assets. The key to these systems is 3D reconstruction with high
geometrical fidelity. However, existing methods struggled under field
conditions, especially with sparse and occluded views. This study developed a
two-stage framework (DATR) for the reconstruction of apple trees from sparse
views. The first stage leverages onboard sensors and foundation models to
semi-automatically generate tree masks from complex field images. Tree masks
are used to filter out background information in multi-modal data for the
single-image-to-3D reconstruction at the second stage. This stage consists of a
diffusion model and a large reconstruction model for respective multi view and
implicit neural field generation. The training of the diffusion model and LRM
was achieved by using realistic synthetic apple trees generated by a Real2Sim
data generator. The framework was evaluated on both field and synthetic
datasets. The field dataset includes six apple trees with field-measured ground
truth, while the synthetic dataset featured structurally diverse trees.
Evaluation results showed that our DATR framework outperformed existing 3D
reconstruction methods across both datasets and achieved domain-trait
estimation comparable to industrial-grade stationary laser scanners while
improving the throughput by $\sim$360 times, demonstrating strong potential for
scalable agricultural digital twin systems.

</details>


### [108] [Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots](https://arxiv.org/abs/2508.19788)
*Sena Ishii,Akash Chikhalikar,Ankit A. Ravankar,Jose Victorio Salazar Luces,Yasuhisa Hirata*

Main category: cs.RO

TL;DR: 提出了一种新颖的框架，用于估计日常室内场景中的事故易发区域，旨在提高服务机器人在以人为中心环境中的实时风险意识。


<details>
  <summary>Details</summary>
Motivation: 随着机器人融入日常生活，特别是在家庭中，预测和应对环境危险的能力对于确保用户安全、信任和有效的人机交互至关重要。

Method: 通过基于语义图的传播算法建模对象级风险和上下文。每个对象表示为具有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称传播到低风险对象。

Result: 在人类标注的风险区域数据集上验证，实现了75%的二元风险检测准确率，系统与人类感知高度一致，尤其是在涉及尖锐或不稳定物体的场景中。

Conclusion: 该框架展示了上下文感知风险推理在增强机器人场景理解和主动安全行为方面的潜力，可作为未来系统的基础，用于做出上下文驱动的安全决策、提供实时警报或自主协助用户避免或减轻家庭环境中的危险。

Abstract: We present a novel framework for estimating accident-prone regions in
everyday indoor scenes, aimed at improving real-time risk awareness in service
robots operating in human-centric environments. As robots become integrated
into daily life, particularly in homes, the ability to anticipate and respond
to environmental hazards is crucial for ensuring user safety, trust, and
effective human-robot interaction. Our approach models object-level risk and
context through a semantic graph-based propagation algorithm. Each object is
represented as a node with an associated risk score, and risk propagates
asymmetrically from high-risk to low-risk objects based on spatial proximity
and accident relationship. This enables the robot to infer potential hazards
even when they are not explicitly visible or labeled. Designed for
interpretability and lightweight onboard deployment, our method is validated on
a dataset with human-annotated risk regions, achieving a binary risk detection
accuracy of 75%. The system demonstrates strong alignment with human
perception, particularly in scenes involving sharp or unstable objects. These
results underline the potential of context-aware risk reasoning to enhance
robotic scene understanding and proactive safety behaviors in shared
human-robot spaces. This framework could serve as a foundation for future
systems that make context-driven safety decisions, provide real-time alerts, or
autonomously assist users in avoiding or mitigating hazards within home
environments.

</details>
