<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 77]
- [physics.optics](#physics.optics) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [eess.IV](#eess.IV) [Total: 5]
- [cs.RO](#cs.RO) [Total: 4]
- [cs.HC](#cs.HC) [Total: 1]
- [cs.LG](#cs.LG) [Total: 8]
- [cs.AI](#cs.AI) [Total: 4]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.CR](#cs.CR) [Total: 1]
- [q-bio.QM](#q-bio.QM) [Total: 2]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration](https://arxiv.org/abs/2510.01339)
*Alessio Spagnoletti,Andrés Almansa,Marcelo Pereyra*

Main category: cs.CV

TL;DR: LVTINO是首个基于视频一致性模型(VCMs)的零样本高分辨率视频恢复方法，通过条件机制实现高效的时间一致性重建，在视频逆问题上显著优于逐帧应用图像LDM的方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于图像扩散模型的零样本图像逆求解器取得了显著进展，但直接应用于高分辨率视频恢复时会出现时间不一致问题，因为需要同时恢复精细空间细节和捕捉微妙的时间依赖关系。

Method: 利用视频一致性模型(VCMs)作为先验，提出条件机制绕过自动微分需求，仅需少量神经网络函数评估即可实现高质量视频重建，确保强测量一致性和平滑的时间过渡。

Result: 在多样化视频逆问题上的大量实验表明，LVTINO在重建保真度和计算效率方面均优于当前最先进的逐帧图像LDM方法，实现了显著的感知质量提升。

Conclusion: LVTINO为高分辨率视频恢复建立了新的基准，证明了基于VCMs的先验编码在实现时间一致性视频重建方面的有效性。

Abstract: Computational imaging methods increasingly rely on powerful generative
diffusion models to tackle challenging image restoration tasks. In particular,
state-of-the-art zero-shot image inverse solvers leverage distilled
text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy
and perceptual quality with high computational efficiency. However, extending
these advances to high-definition video restoration remains a significant
challenge, due to the need to recover fine spatial detail while capturing
subtle temporal dependencies. Consequently, methods that naively apply
image-based LDM priors on a frame-by-frame basis often result in temporally
inconsistent reconstructions. We address this challenge by leveraging recent
advances in Video Consistency Models (VCMs), which distill video latent
diffusion models into fast generators that explicitly capture temporal
causality. Building on this foundation, we propose LVTINO, the first zero-shot
or plug-and-play inverse solver for high definition video restoration with
priors encoded by VCMs. Our conditioning mechanism bypasses the need for
automatic differentiation and achieves state-of-the-art video reconstruction
quality with only a few neural function evaluations, while ensuring strong
measurement consistency and smooth temporal transitions across frames.
Extensive experiments on a diverse set of video inverse problems show
significant perceptual improvements over current state-of-the-art methods that
apply image LDMs frame by frame, establishing a new benchmark in both
reconstruction fidelity and computational efficiency.

</details>


### [2] [Image Generation Based on Image Style Extraction](https://arxiv.org/abs/2510.01347)
*Shuochen Chang*

Main category: cs.CV

TL;DR: 提出了一种基于风格提取的三阶段训练图像生成方法，通过风格编码器和投影层将风格表示与文本表示对齐，实现细粒度文本引导的风格化图像生成


<details>
  <summary>Details</summary>
Motivation: 解决文本到图像生成模型中细粒度风格难以用自然语言精确描述和控制的问题，以及风格参考图像的引导信息难以与传统文本条件直接对齐的问题

Method: 使用风格编码器和风格投影层提取单张风格参考图像的细粒度风格表示，在不改变下游生成模型结构框架的情况下注入风格表示，实现三阶段训练

Result: 构建了包含图像、风格标签和文本描述三元组的Style30k-captions数据集，用于训练风格编码器和投影层

Conclusion: 该方法能够最大化预训练生成模型的生成能力，实现细粒度控制的风格化图像生成

Abstract: Image generation based on text-to-image generation models is a task with
practical application scenarios that fine-grained styles cannot be precisely
described and controlled in natural language, while the guidance information of
stylized reference images is difficult to be directly aligned with the textual
conditions of traditional textual guidance generation. This study focuses on
how to maximize the generative capability of the pretrained generative model,
by obtaining fine-grained stylistic representations from a single given
stylistic reference image, and injecting the stylistic representations into the
generative body without changing the structural framework of the downstream
generative model, so as to achieve fine-grained controlled stylized image
generation. In this study, we propose a three-stage training style
extraction-based image generation method, which uses a style encoder and a
style projection layer to align the style representations with the textual
representations to realize fine-grained textual cue-based style guide
generation. In addition, this study constructs the Style30k-captions dataset,
whose samples contain a triad of images, style labels, and text descriptions,
to train the style encoder and style projection layer in this experiment.

</details>


### [3] [EvoStruggle: A Dataset Capturing the Evolution of Struggle across Activities and Skill Levels](https://arxiv.org/abs/2510.01362)
*Shijia Feng,Michael Wray,Walterio Mayol-Cuevas*

Main category: cs.CV

TL;DR: 本文提出了一个用于技能学习过程中挣扎行为检测的数据集EvoStruggle，包含61.68小时视频记录和5,385个标注的时间段，通过时序动作定位模型验证了挣扎行为在不同任务间的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有操纵数据集未关注挣扎行为随时间演变的过程，而理解技能学习过程中挣扎行为的演变对于优化人类学习和开发辅助系统至关重要。

Method: 收集了76名参与者在18个任务（分为4类活动）的数据集，每个任务重复5次以捕捉技能演变。将挣扎检测问题定义为时序动作定位任务，使用TAL模型进行检测。

Result: 模型在跨任务泛化时达到34.56%的平均mAP，跨活动泛化时达到19.24%的平均mAP，表明挣扎行为在不同技能任务间具有可迁移性。

Conclusion: 挣扎行为是跨各种技能任务的可迁移概念，但挣扎检测仍有改进空间，该数据集为研究技能学习过程中的挣扎演变提供了重要资源。

Abstract: The ability to determine when a person struggles during skill acquisition is
crucial for both optimizing human learning and enabling the development of
effective assistive systems. As skills develop, the type and frequency of
struggles tend to change, and understanding this evolution is key to
determining the user's current stage of learning. However, existing
manipulation datasets have not focused on how struggle evolves over time. In
this work, we collect a dataset for struggle determination, featuring 61.68
hours of video recordings, 2,793 videos, and 5,385 annotated temporal struggle
segments collected from 76 participants. The dataset includes 18 tasks grouped
into four diverse activities -- tying knots, origami, tangram puzzles, and
shuffling cards, representing different task variations. In addition,
participants repeated the same task five times to capture their evolution of
skill. We define the struggle determination problem as a temporal action
localization task, focusing on identifying and precisely localizing struggle
segments with start and end times. Experimental results show that Temporal
Action Localization models can successfully learn to detect struggle cues, even
when evaluated on unseen tasks or activities. The models attain an overall
average mAP of 34.56% when generalizing across tasks and 19.24% across
activities, indicating that struggle is a transferable concept across various
skill-based tasks while still posing challenges for further improvement in
struggle detection. Our dataset is available at
https://github.com/FELIXFENG2019/EvoStruggle.

</details>


### [4] [SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs](https://arxiv.org/abs/2510.01370)
*Abu Bucker Siddik,Diane Oyen,Alexander Most,Michal Kucer,Ayan Biswas*

Main category: cs.CV

TL;DR: SPUS是一种紧凑高效的基于残差U-Net架构的PDE求解基础模型，相比基于Transformer的现有方法具有更少的参数和计算开销，在多种下游PDE任务上实现了最先进的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的PDE基础模型存在计算复杂和参数过多的问题，需要开发更轻量高效的架构来统一求解各类偏微分方程。

Method: 采用轻量级残差U-Net架构，使用自回归预训练策略模拟数值求解器行为，在流体动力学PDE数据集上进行预训练。

Result: 在6个未见过的下游PDE任务上，SPUS实现了最先进的泛化性能，同时需要显著更少的参数和微调数据。

Conclusion: SPUS展示了作为参数高效的基础模型解决多样化PDE系统的潜力，残差U-Net架构在此领域具有重要应用价值。

Abstract: We introduce Small PDE U-Net Solver (SPUS), a compact and efficient
foundation model (FM) designed as a unified neural operator for solving a wide
range of partial differential equations (PDEs). Unlike existing
state-of-the-art PDE FMs-primarily based on large complex transformer
architectures with high computational and parameter overhead-SPUS leverages a
lightweight residual U-Net-based architecture that has been largely
underexplored as a foundation model architecture in this domain. To enable
effective learning in this minimalist framework, we utilize a simple yet
powerful auto-regressive pretraining strategy which closely replicates the
behavior of numerical solvers to learn the underlying physics. SPUS is
pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6
challenging unseen downstream PDEs spanning various physical systems.
Experimental results demonstrate that SPUS using residual U-Net based
architecture achieves state-of-the-art generalization on these downstream tasks
while requiring significantly fewer parameters and minimal fine-tuning data,
highlighting its potential as a highly parameter-efficient FM for solving
diverse PDE systems.

</details>


### [5] [DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation](https://arxiv.org/abs/2510.01399)
*Shubhankar Borse,Farzad Farhadzadeh,Munawar Hayat,Fatih Porikli*

Main category: cs.CV

TL;DR: DisCo是一个基于强化学习的框架，通过多样性约束直接优化多人生成中的身份多样性，解决了文本到图像模型在多人生成中面部重复、身份合并和计数错误的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像模型在生成多个人物时容易出现面部重复、身份合并和计数错误的问题，这限制了模型在复杂场景中的应用。

Method: DisCo使用组相对策略优化（GRPO）对流匹配模型进行微调，采用组合奖励函数，包括惩罚图像内面部相似性、减少跨样本身份重复、确保准确的人物计数，并通过人类偏好评分保持视觉保真度。单阶段课程学习稳定了训练过程，无需额外标注。

Result: 在DiverseHumans测试集上，DisCo实现了98.6%的唯一面部准确率和近乎完美的全局身份分布，超越了开源和专有方法（如Gemini、GPT-Image），同时保持了竞争力的感知质量。

Conclusion: DisCo作为一个可扩展、无需标注的解决方案，解决了生成模型中长期存在的身份危机问题，并为组合多人生成设定了新的基准。

Abstract: State-of-the-art text-to-image models excel at realism but collapse on
multi-human prompts - duplicating faces, merging identities, and miscounting
individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the
first RL-based framework to directly optimize identity diversity in multi-human
generation. DisCo fine-tunes flow-matching models via Group-Relative Policy
Optimization (GRPO) with a compositional reward that (i) penalizes intra-image
facial similarity, (ii) discourages cross-sample identity repetition, (iii)
enforces accurate person counts, and (iv) preserves visual fidelity through
human preference scores. A single-stage curriculum stabilizes training as
complexity scales, requiring no extra annotations. On the DiverseHumans
Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global
Identity Spread - surpassing both open-source and proprietary methods (e.g.,
Gemini, GPT-Image) while maintaining competitive perceptual quality. Our
results establish DisCo as a scalable, annotation-free solution that resolves
the long-standing identity crisis in generative models and sets a new benchmark
for compositional multi-human generation.

</details>


### [6] [GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings](https://arxiv.org/abs/2510.01448)
*Angel Daruna,Nicholas Meegan,Han-Pang Chiu,Supun Samarasekera,Rakesh Kumar*

Main category: cs.CV

TL;DR: 提出了一种新颖的地理表示方法，将世界建模为地理嵌入的层次结构，并通过融合外观特征和语义分割图形成鲁棒的视觉表示，在视觉地理定位任务中取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 解决全球视觉地理定位问题，即仅使用图像视觉内容确定地球任意位置的地理位置。现有的地理表示学习方法仍有改进空间。

Method: 将地理定位问题表述为对齐查询图像的视觉表示与学习的地理表示。采用层次化的地理嵌入模型，并引入有效融合外观特征和语义分割图的方法。

Result: 在五个基准数据集上的25个指标中，22个指标超越了之前的最先进方法和大型视觉语言模型，创造了新的最佳性能。

Conclusion: 消融研究表明，性能提升主要归功于地理表示和视觉表示的有效结合。

Abstract: Worldwide visual geo-localization seeks to determine the geographic location
of an image anywhere on Earth using only its visual content. Learned
representations of geography for visual geo-localization remain an active
research topic despite much progress. We formulate geo-localization as aligning
the visual representation of the query image with a learned geographic
representation. Our novel geographic representation explicitly models the world
as a hierarchy of geographic embeddings. Additionally, we introduce an approach
to efficiently fuse the appearance features of the query image with its
semantic segmentation map, forming a robust visual representation. Our main
experiments demonstrate improved all-time bests in 22 out of 25 metrics
measured across five benchmark datasets compared to prior state-of-the-art
(SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional
ablation studies support the claim that these gains are primarily driven by the
combination of geographic and visual representations.

</details>


### [7] [Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories](https://arxiv.org/abs/2510.01454)
*Nilay Naharas,Dang Nguyen,Nesihan Bulut,Mohammadhossein Bateni,Vahab Mirrokni,Baharan Mirzasoleiman*

Main category: cs.CV

TL;DR: XMAS是首个针对大型视觉语言模型（LVLM）的数据高效指令调优方法，通过聚类跨模态注意力矩阵来消除训练数据冗余，能在保留模型性能的同时大幅减少50-85%的训练数据。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法在LVLM指令调优中表现不佳，甚至无法超越随机选择，需要开发专门针对LVLM的高效数据选择方法。

Method: 基于跨模态注意力矩阵相似性理论，通过聚类注意力矩阵的top奇异值轨迹，从代理LVLM微调中获取代表性样本，实现平衡子集采样。

Result: 在LLaVA-665k数据集上减少50%数据，Vision-Flan数据集上减少85%数据，完全保留LLaVA-1.5-7B在10个下游基准上的性能，训练速度提升1.2倍。

Conclusion: XMAS是首个在LVLM指令调优中有效的数据选择方法，相比最佳基线多减少30%数据，证明了基于注意力矩阵聚类的数据选择策略的有效性。

Abstract: Data-efficient learning aims to eliminate redundancy in large training
datasets by training models on smaller subsets of the most informative
examples. While data selection has been extensively explored for vision models
and large language models (LLMs), it remains underexplored for Large
Vision-Language Models (LVLMs). Notably, none of existing methods can
outperform random selection at different subset sizes. In this work, we propose
the first principled method for data-efficient instruction tuning of LVLMs. We
prove that examples with similar cross-modal attention matrices during
instruction tuning have similar gradients. Thus, they influence model
parameters in a similar manner and convey the same information to the model
during training. Building on this insight, we propose XMAS, which clusters
examples based on the trajectories of the top singular values of their
attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a
balanced subset from these clusters, XMAS effectively removes redundancy in
large-scale LVLM training data. Extensive experiments show that XMAS can
discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while
fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and
speeding up its training by 1.2x. This is 30% more data reduction compared to
the best baseline for LLaVA-665k. The project's website can be found at
https://bigml-cs-ucla.github.io/XMAS-project-page/.

</details>


### [8] [Purrception: Variational Flow Matching for Vector-Quantized Image Generation](https://arxiv.org/abs/2510.01478)
*Răzvan-Andrei Matişan,Vincent Tao Hu,Grigory Bartosh,Björn Ommer,Cees G. M. Snoek,Max Welling,Jan-Willem van de Meent,Mohammad Mahdi Derakhshani,Floor Eijkelboom*

Main category: cs.CV

TL;DR: Purrception是一种结合连续传输动力学和离散分类监督的向量量化图像生成方法，通过变分流匹配在连续嵌入空间中学习速度场，同时在码本索引上学习分类后验。


<details>
  <summary>Details</summary>
Motivation: 旨在将连续方法的几何感知能力与离散分类方法的监督优势相结合，为图像生成提供不确定性量化和温度可控的生成能力。

Method: 基于变分流匹配框架，在向量量化潜空间中进行适配：在连续嵌入空间计算速度场，同时在码本索引上学习分类后验分布。

Result: 在ImageNet-1k 256x256生成任务上，训练收敛速度比连续和离散流匹配基线更快，同时达到与最先进模型竞争的FID分数。

Conclusion: 变分流匹配能够有效桥接连续传输和离散监督，提高图像生成的训练效率，证明了该方法在结合连续几何感知和离散分类监督方面的有效性。

Abstract: We introduce Purrception, a variational flow matching approach for
vector-quantized image generation that provides explicit categorical
supervision while maintaining continuous transport dynamics. Our method adapts
Variational Flow Matching to vector-quantized latents by learning categorical
posteriors over codebook indices while computing velocity fields in the
continuous embedding space. This combines the geometric awareness of continuous
methods with the discrete supervision of categorical approaches, enabling
uncertainty quantification over plausible codes and temperature-controlled
generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training
converges faster than both continuous flow matching and discrete flow matching
baselines while achieving competitive FID scores with state-of-the-art models.
This demonstrates that Variational Flow Matching can effectively bridge
continuous transport and discrete supervision for improved training efficiency
in image generation.

</details>


### [9] [AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging](https://arxiv.org/abs/2510.01498)
*Yuxuan Ou,Ning Bi,Jiazhen Pan,Jiancheng Yang,Boliang Yu,Usama Zidan,Regent Lee,Vicente Grau*

Main category: cs.CV

TL;DR: 提出一个统一的深度学习框架，通过条件扩散模型和多任务学习，从非对比CT扫描中生成合成对比增强CT图像，同时分割主动脉腔和血栓，避免了多阶段流程中的错误累积问题。


<details>
  <summary>Details</summary>
Motivation: 传统对比增强CT检查需要碘对比剂，存在肾毒性、患者过敏和环境危害等风险。现有方法采用多阶段流程（先生成图像再分割）导致错误累积，且无法利用共享的语义和解剖结构。

Method: 集成条件扩散模型与多任务学习的端到端联合优化框架，共享编码器和解码器参数，采用半监督训练策略处理缺失分割标签的临床数据，无需初始预测。

Result: 在264名患者队列中评估，图像合成PSNR达25.61 dB（优于单任务CDM的23.80 dB），分割性能提升：腔Dice分数0.89（原0.87），血栓Dice分数0.53（原0.48），临床测量更准确。

Conclusion: 该方法在减少对比剂使用的同时，通过联合优化图像合成和分割任务，显著提升了腹部主动脉瘤评估的准确性和临床实用性。

Abstract: While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic
aneurysms (AAA), the required iodinated contrast agents pose significant risks,
including nephrotoxicity, patient allergies, and environmental harm. To reduce
contrast agent use, recent deep learning methods have focused on generating
synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a
multi-stage pipeline that first generates images and then performs
segmentation, which leads to error accumulation and fails to leverage shared
semantic and anatomical structures. To address this, we propose a unified deep
learning framework that generates synthetic CECT images from NCCT scans while
simultaneously segmenting the aortic lumen and thrombus. Our approach
integrates conditional diffusion models (CDM) with multi-task learning,
enabling end-to-end joint optimization of image synthesis and anatomical
segmentation. Unlike previous multitask diffusion models, our approach requires
no initial predictions (e.g., a coarse segmentation mask), shares both encoder
and decoder parameters across tasks, and employs a semi-supervised training
strategy to learn from scans with missing segmentation labels, a common
constraint in real-world clinical data. We evaluated our method on a cohort of
264 patients, where it consistently outperformed state-of-the-art single-task
and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61
dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation,
it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus
Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to
more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm
from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to
nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.

</details>


### [10] [From Videos to Indexed Knowledge Graphs -- Framework to Marry Methods for Multimodal Content Analysis and Understanding](https://arxiv.org/abs/2510.01513)
*Basem Rizk,Joel Walsh,Mark Core,Benjamin Nye*

Main category: cs.CV

TL;DR: 提出了一个用于多模态内容分析的高效原型框架，能够将视频转换为时间半结构化数据，并进一步转换为可查询的知识图谱表示


<details>
  <summary>Details</summary>
Motivation: 多模态内容分析复杂且计算成本高，现有预训练模型难以有效融合用于视频等复杂数据，需要简化工程实现

Method: 设计候选流水线方案，结合预训练模型将视频转换为时间半结构化数据格式，再转换为帧级索引知识图谱

Result: 创建了可查询的知识图谱表示，支持持续学习，能够通过交互方式动态融入新的领域知识

Conclusion: 该框架为多模态内容分析提供了高效原型工具，解决了现有方法融合困难的问题

Abstract: Analysis of multi-modal content can be tricky, computationally expensive, and
require a significant amount of engineering efforts. Lots of work with
pre-trained models on static data is out there, yet fusing these opensource
models and methods with complex data such as videos is relatively challenging.
In this paper, we present a framework that enables efficiently prototyping
pipelines for multi-modal content analysis. We craft a candidate recipe for a
pipeline, marrying a set of pre-trained models, to convert videos into a
temporal semi-structured data format. We translate this structure further to a
frame-level indexed knowledge graph representation that is query-able and
supports continual learning, enabling the dynamic incorporation of new
domain-specific knowledge through an interactive medium.

</details>


### [11] [WALT: Web Agents that Learn Tools](https://arxiv.org/abs/2510.01524)
*Viraj Prabhu,Yutong Dai,Matthew Fernandez,Jing Gu,Krithika Ramakrishnan,Yanqi Luo,Silvio Savarese,Caiming Xiong,Junnan Li,Zeyuan Chen,Ran Xu*

Main category: cs.CV

TL;DR: WALT框架通过将网站功能逆向工程为可调用工具，使网络代理能够像人类一样使用高级操作（如搜索、过滤、排序），减少对逐步UI交互的依赖，提高自动化任务的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前网络代理方法依赖逐步UI交互和大量LLM推理，在动态布局和长任务中表现脆弱。人类则利用网站提供的高级功能进行操作，因此需要开发能够类似利用网站内置功能的代理框架。

Method: WALT框架逆向工程网站潜在功能，将其转化为可重用的可调用工具，包括发现（搜索、过滤、排序）、通信（发布、评论、点赞）和内容管理（创建、编辑、删除）等操作。

Result: 在VisualWebArena和WebArena基准测试中，WALT实现了更高的成功率，使用更少的步骤和更少的LLM依赖推理。

Conclusion: WALT通过将计算负担从脆弱的逐步推理转移到可靠的工具调用，建立了一个鲁棒且可推广的浏览器自动化范式。

Abstract: Web agents promise to automate complex browser tasks, but current methods
remain brittle -- relying on step-by-step UI interactions and heavy LLM
reasoning that break under dynamic layouts and long horizons. Humans, by
contrast, exploit website-provided functionality through high-level operations
like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools),
a framework that reverse-engineers latent website functionality into reusable
invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust
implementations of automations already designed into websites -- spanning
discovery (search, filter, sort), communication (post, comment, upvote), and
content management (create, edit, delete). Tools abstract away low-level
execution: instead of reasoning about how to click and type, agents simply call
search(query) or create(listing). This shifts the computational burden from
fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena
and WebArena, WALT achieves higher success with fewer steps and less
LLM-dependent reasoning, establishing a robust and generalizable paradigm for
browser automation.

</details>


### [12] [MATCH: Multi-faceted Adaptive Topo-Consistency for Semi-Supervised Histopathology Segmentation](https://arxiv.org/abs/2510.01532)
*Meilong Xu,Xiaoling Hu,Shahira Abousamra,Chen Li,Chao Chen*

Main category: cs.CV

TL;DR: 提出了一种半监督分割框架，通过拓扑一致性机制和特征匹配策略，在组织病理学图像中实现更鲁棒和准确的分割。


<details>
  <summary>Details</summary>
Motivation: 解决半监督分割中从无标签数据捕获有意义语义结构的挑战，特别是在组织病理学图像分析中，对象密集分布且难以区分生物结构与噪声伪影。

Method: 利用随机dropout和时间训练快照获得多个扰动预测，通过拓扑一致性机制和结合空间重叠与全局结构对齐的新型匹配策略，最小化预测间的差异。

Result: 大量实验表明，该方法有效减少拓扑错误，产生更鲁棒和准确的分割结果，对可靠的下游分析至关重要。

Conclusion: 所提出的框架通过拓扑一致性约束和特征匹配，在半监督分割任务中取得了显著改进，特别是在组织病理学图像分析领域。

Abstract: In semi-supervised segmentation, capturing meaningful semantic structures
from unlabeled data is essential. This is particularly challenging in
histopathology image analysis, where objects are densely distributed. To
address this issue, we propose a semi-supervised segmentation framework
designed to robustly identify and preserve relevant topological features. Our
method leverages multiple perturbed predictions obtained through stochastic
dropouts and temporal training snapshots, enforcing topological consistency
across these varied outputs. This consistency mechanism helps distinguish
biologically meaningful structures from transient and noisy artifacts. A key
challenge in this process is to accurately match the corresponding topological
features across the predictions in the absence of ground truth. To overcome
this, we introduce a novel matching strategy that integrates spatial overlap
with global structural alignment, minimizing discrepancies among predictions.
Extensive experiments demonstrate that our approach effectively reduces
topological errors, resulting in more robust and accurate segmentations
essential for reliable downstream analysis. Code is available at
\href{https://github.com/Melon-Xu/MATCH}{https://github.com/Melon-Xu/MATCH}.

</details>


### [13] [Towards Better Optimization For Listwise Preference in Diffusion Models](https://arxiv.org/abs/2510.01540)
*Jiamu Bai,Xin Yu,Meilong Xu,Weitao Lu,Xin Pan,Kiwan Maeng,Daniel Kifer,Jian Wang,Yu Wang*

Main category: cs.CV

TL;DR: Diffusion-LPO：一种用于扩散模型的列表偏好优化框架，通过将用户反馈聚合成图像排名列表，在Plackett-Luce模型下推导出DPO目标的列表扩展，从而在视觉质量和偏好对齐方面优于成对DPO基线。


<details>
  <summary>Details</summary>
Motivation: 当前基于成对偏好的DPO方法在处理人类图像偏好时存在局限，人类反馈往往包含隐式的排名信息，这些信息比成对比较能传达更精确的人类偏好。

Method: 提出Diffusion-LPO框架，给定一个标题，将用户反馈聚合成图像排名列表，在Plackett-Luce模型下推导出DPO目标的列表扩展，通过鼓励每个样本优于其所有排名较低的替代品来确保整个排名的一致性。

Result: 在文本到图像生成、图像编辑和个性化偏好对齐等任务中，Diffusion-LPO在视觉质量和偏好对齐方面一致优于成对DPO基线。

Conclusion: Diffusion-LPO是一种简单有效的列表偏好优化方法，能够更好地利用人类反馈中的排名信息来对齐扩散模型与人类偏好。

Abstract: Reinforcement learning from human feedback (RLHF) has proven effectiveness
for aligning text-to-image (T2I) diffusion models with human preferences.
Although Direct Preference Optimization (DPO) is widely adopted for its
computational efficiency and avoidance of explicit reward modeling, its
applications to diffusion models have primarily relied on pairwise preferences.
The precise optimization of listwise preferences remains largely unaddressed.
In practice, human feedback on image preferences often contains implicit ranked
information, which conveys more precise human preferences than pairwise
comparisons. In this work, we propose Diffusion-LPO, a simple and effective
framework for Listwise Preference Optimization in diffusion models with
listwise data. Given a caption, we aggregate user feedback into a ranked list
of images and derive a listwise extension of the DPO objective under the
Plackett-Luce model. Diffusion-LPO enforces consistency across the entire
ranking by encouraging each sample to be preferred over all of its lower-ranked
alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO
across various tasks, including text-to-image generation, image editing, and
personalized preference alignment. Diffusion-LPO consistently outperforms
pairwise DPO baselines on visual quality and preference alignment.

</details>


### [14] [Growing Visual Generative Capacity for Pre-Trained MLLMs](https://arxiv.org/abs/2510.01546)
*Hanyu Wang,Jiaming Han,Ziyan Yang,Qi Zhao,Shanchuan Lin,Xiangyu Yue,Abhinav Shrivastava,Zhenheng Yang,Hao Chen*

Main category: cs.CV

TL;DR: Bridge是一个纯自回归的统一多模态大语言模型，通过Mixture-of-Transformers架构和语义到像素的离散表示，在单一框架内实现图像理解和生成，在保持语义对齐的同时提升像素级保真度。


<details>
  <summary>Details</summary>
Motivation: 现有统一MLLMs面临挑战：混合方法打破自回归范式，纯自回归方法在语义对齐和像素保真度之间存在权衡。需要构建既能保持自回归特性又能实现高质量生成的方法。

Method: 采用Mixture-of-Transformers架构，在预训练视觉理解模型基础上增强生成能力；提出语义到像素离散表示，结合紧凑语义标记和细粒度像素标记，仅增加7.9%序列长度。

Result: 在多样化多模态基准测试中，Bridge在理解和生成任务上均取得竞争性或更优结果，同时需要更少训练数据和更短训练时间。

Conclusion: Bridge证明了纯自回归方法可以实现高质量的多模态理解和生成，为构建统一MLLMs提供了有效解决方案。

Abstract: Multimodal large language models (MLLMs) extend the success of language
models to visual understanding, and recent efforts have sought to build unified
MLLMs that support both understanding and generation. However, constructing
such models remains challenging: hybrid approaches combine continuous
embeddings with diffusion or flow-based objectives, producing high-quality
images but breaking the autoregressive paradigm, while pure autoregressive
approaches unify text and image prediction over discrete visual tokens but
often face trade-offs between semantic alignment and pixel-level fidelity. In
this work, we present Bridge, a pure autoregressive unified MLLM that augments
pre-trained visual understanding models with generative ability through a
Mixture-of-Transformers architecture, enabling both image understanding and
generation within a single next-token prediction framework. To further improve
visual generation fidelity, we propose a semantic-to-pixel discrete
representation that integrates compact semantic tokens with fine-grained pixel
tokens, achieving strong language alignment and precise description of visual
details with only a 7.9% increase in sequence length. Extensive experiments
across diverse multimodal benchmarks demonstrate that Bridge achieves
competitive or superior results in both understanding and generation
benchmarks, while requiring less training data and reduced training time
compared to prior unified MLLMs.

</details>


### [15] [Robust Classification of Oral Cancer with Limited Training Data](https://arxiv.org/abs/2510.01547)
*Akshay Bhagwan Sonawane,Lena D. Swamikannan,Lakshman Tamil*

Main category: cs.CV

TL;DR: 提出结合CNN和贝叶斯深度学习的混合模型，用于小数据集的口腔癌分类，通过不确定性量化提高可靠性


<details>
  <summary>Details</summary>
Motivation: 解决传统深度学习模型在医疗资源匮乏地区过度依赖大数据集、过度自信和泛化能力差的问题，特别是在口腔癌早期诊断中数据稀缺的挑战

Method: 使用卷积神经网络结合贝叶斯深度学习，采用变分推理进行不确定性量化，基于智能手机拍摄的彩色图像进行训练

Result: 在相似分布测试集上达到94%准确率，在真实世界多样化数据集上达到88%准确率（传统CNN为72.94%），模型对正确分类样本显示低不确定性，对错误分类样本显示高不确定性

Conclusion: 贝叶斯推理在数据稀缺环境下能有效提高口腔癌早期诊断的模型可靠性和泛化能力

Abstract: Oral cancer ranks among the most prevalent cancers globally, with a
particularly high mortality rate in regions lacking adequate healthcare access.
Early diagnosis is crucial for reducing mortality; however, challenges persist
due to limited oral health programs, inadequate infrastructure, and a shortage
of healthcare practitioners. Conventional deep learning models, while
promising, often rely on point estimates, leading to overconfidence and reduced
reliability. Critically, these models require large datasets to mitigate
overfitting and ensure generalizability, an unrealistic demand in settings with
limited training data. To address these issues, we propose a hybrid model that
combines a convolutional neural network (CNN) with Bayesian deep learning for
oral cancer classification using small training sets. This approach employs
variational inference to enhance reliability through uncertainty
quantification. The model was trained on photographic color images captured by
smartphones and evaluated on three distinct test datasets. The proposed method
achieved 94% accuracy on a test dataset with a distribution similar to that of
the training data, comparable to traditional CNN performance. Notably, for
real-world photographic image data, despite limitations and variations
differing from the training dataset, the proposed model demonstrated superior
generalizability, achieving 88% accuracy on diverse datasets compared to 72.94%
for traditional CNNs, even with a smaller dataset. Confidence analysis revealed
that the model exhibits low uncertainty (high confidence) for correctly
classified samples and high uncertainty (low confidence) for misclassified
samples. These results underscore the effectiveness of Bayesian inference in
data-scarce environments in enhancing early oral cancer diagnosis by improving
model reliability and generalizability.

</details>


### [16] [Consistent Assistant Domains Transformer for Source-free Domain Adaptation](https://arxiv.org/abs/2510.01559)
*Renrong Shao,Wei Zhang,Kangyang Luo,Qin Li,and Jun Wang*

Main category: cs.CV

TL;DR: 提出CADTrans方法解决无源域适应中硬样本和域偏差问题，通过构建辅助域和一致性策略获得不变特征表示


<details>
  <summary>Details</summary>
Motivation: 现有SFDA方法难以获取确定性不变特征，容易受到硬样本和域偏差影响，需要更有效的特征表示方法

Method: 使用辅助域模块获取多样化表示，通过多一致性策略获得不变特征，构建CMK-MMD策略对齐硬样本和易样本

Result: 在Office-31、Office-Home、VISDA-C和DomainNet-126等基准测试中取得显著性能提升

Conclusion: CADTrans通过构建域一致性不变特征表示，有效解决了SFDA中的硬样本和域偏差问题

Abstract: Source-free domain adaptation (SFDA) aims to address the challenge of
adapting to a target domain without accessing the source domain directly.
However, due to the inaccessibility of source domain data, deterministic
invariable features cannot be obtained. Current mainstream methods primarily
focus on evaluating invariant features in the target domain that closely
resemble those in the source domain, subsequently aligning the target domain
with the source domain. However, these methods are susceptible to hard samples
and influenced by domain bias. In this paper, we propose a Consistent Assistant
Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue
by constructing invariable feature representations of domain consistency.
Concretely, we develop an assistant domain module for CADTrans to obtain
diversified representations from the intermediate aggregated global attentions,
which addresses the limitation of existing methods in adequately representing
diversity. Based on assistant and target domains, invariable feature
representations are obtained by multiple consistent strategies, which can be
used to distinguish easy and hard samples. Finally, to align the hard samples
to the corresponding easy samples, we construct a conditional multi-kernel max
mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same
category and those of different categories. Extensive experiments are conducted
on various benchmarks such as Office-31, Office-Home, VISDA-C, and
DomainNet-126, proving the significant performance improvements achieved by our
proposed approaches. Code is available at
https://github.com/RoryShao/CADTrans.git.

</details>


### [17] [Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations](https://arxiv.org/abs/2510.01576)
*Ricardo Gonzalez Penuela,Felipe Arias-Russi,Victor Capriles*

Main category: cs.CV

TL;DR: 该论文开发了一个系统，利用BLV用户的历史视觉问题来指导多模态大语言模型生成更相关的图像描述，而不是默认的全面描述。


<details>
  <summary>Details</summary>
Motivation: 现有的MLLM应用在为BLV用户提供视觉解释时，往往提供冗长且不相关的描述，用户需要筛选大量无关细节才能获得所需信息。

Method: 系统从VizWiz-LF数据集中识别相似的过往视觉上下文，利用相关的问题来指导MLLM生成更符合BLV用户需求的描述。

Result: 评估显示，上下文感知描述在76.1%的情况下能够预测并回答用户问题，并在54.4%的比较中被优先选择。

Conclusion: 利用BLV用户的历史视觉问题可以显著提高MLLM生成的图像描述的相关性和实用性。

Abstract: Multimodal large language models (MLLMs) have been integrated into visual
interpretation applications to support Blind and Low Vision (BLV) users because
of their accuracy and ability to provide rich, human-like interpretations.
However, these applications often default to comprehensive, lengthy
descriptions regardless of context. This leads to inefficient exchanges, as
users must go through irrelevant details rather than receiving the specific
information they are likely to seek. To deliver more contextually-relevant
information, we developed a system that draws on historical BLV users
questions. When given an image, our system identifies similar past visual
contexts from the VizWiz-LF dataset and uses the associated questions to guide
the MLLM generate descriptions more relevant to BLV users. An evaluation with
three human labelers who revised 92 context-aware and context-free descriptions
showed that context-aware descriptions anticipated and answered users'
questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of
comparisons (50 out of 92). Our paper reviews, and data analysis are publicly
available in a Github repository at
https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .

</details>


### [18] [ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models](https://arxiv.org/abs/2510.01582)
*Krishna Teja Chitty-Venkata,Murali Emani*

Main category: cs.CV

TL;DR: 开发了ImageNet-Think数据集，这是一个多模态推理数据集，旨在帮助开发具有显式推理能力的视觉语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于训练和评估多模态推理模型的数据集，需要为开发更强大的视觉语言模型提供结构化推理数据支持。

Method: 基于ImageNet21k的25万张图像，使用GLM-4.1V-9B-Thinking和Kimi-VL-A3B-Thinking-2506两个先进VLMs生成结构化思维标记和对应答案，每张图像配有两对思维-答案序列。

Result: 创建了包含逐步推理过程和最终描述性答案的数据集，为多模态推理研究提供了重要资源。

Conclusion: 该数据集将公开发布，有助于推动多模态推理模型的发展，促进对多模态推理机制的深入理解。

Abstract: We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the
development of Vision Language Models (VLMs) with explicit reasoning
capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset,
providing structured thinking tokens and corresponding answers. Our synthetic
dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and
Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of
thinking-answer sequences, creating a resource for training and evaluating
multimodal reasoning models. We capture the step-by-step reasoning process of
VLMs and the final descriptive answers. Our goal with this dataset is to enable
the development of more robust VLMs while contributing to the broader
understanding of multimodal reasoning mechanisms. The dataset and evaluation
benchmarks will be publicly available to aid research in reasoning/thinking
multimodal VLMs.

</details>


### [19] [NPN: Non-Linear Projections of the Null-Space for Imaging Inverse Problems](https://arxiv.org/abs/2510.01608)
*Roman Jacome,Romario Gualdrón-Hurtado,Leon Suarez,Henry Arguello*

Main category: cs.CV

TL;DR: 提出了一种新的正则化方法NPN，通过神经网络在感知矩阵的零空间上施加低维投影约束，提高成像逆问题的重建质量


<details>
  <summary>Details</summary>
Motivation: 传统正则化方法忽略了感知矩阵零空间的特定结构，无法有效利用与感知过程正交的信息

Method: NPN方法通过神经网络学习感知矩阵零空间的低维投影，将解约束在该投影空间中，可与现有重建框架兼容

Result: 在压缩感知、去模糊、超分辨率、CT和MRI等多种成像逆问题中，NPN正则化均能显著提升重建保真度

Conclusion: NPN提供了一种可解释且灵活的正则化方法，能有效利用零空间结构信息，与现有重建方法互补

Abstract: Imaging inverse problems aims to recover high-dimensional signals from
undersampled, noisy measurements, a fundamentally ill-posed task with infinite
solutions in the null-space of the sensing operator. To resolve this ambiguity,
prior information is typically incorporated through handcrafted regularizers or
learned models that constrain the solution space. However, these priors
typically ignore the task-specific structure of that null-space. In this work,
we propose \textit{Non-Linear Projections of the Null-Space} (NPN), a novel
class of regularization that, instead of enforcing structural constraints in
the image domain, promotes solutions that lie in a low-dimensional projection
of the sensing matrix's null-space with a neural network. Our approach has two
key advantages: (1) Interpretability: by focusing on the structure of the
null-space, we design sensing-matrix-specific priors that capture information
orthogonal to the signal components that are fundamentally blind to the sensing
process. (2) Flexibility: NPN is adaptable to various inverse problems,
compatible with existing reconstruction frameworks, and complementary to
conventional image-domain priors. We provide theoretical guarantees on
convergence and reconstruction accuracy when used within plug-and-play methods.
Empirical results across diverse sensing matrices demonstrate that NPN priors
consistently enhance reconstruction fidelity in various imaging inverse
problems, such as compressive sensing, deblurring, super-resolution, computed
tomography, and magnetic resonance imaging, with plug-and-play methods,
unrolling networks, deep image prior, and diffusion models.

</details>


### [20] [Automated Genomic Interpretation via Concept Bottleneck Models for Medical Robotics](https://arxiv.org/abs/2510.01618)
*Zijun Li,Jinchang Zhang,Ming Zhang,Guoyu Lu*

Main category: cs.CV

TL;DR: 提出了一种自动基因组解释模块，将原始DNA序列转化为可解释的决策，适用于医疗自动化和机器人系统。


<details>
  <summary>Details</summary>
Motivation: 弥合可解释基因组建模与自动决策之间的差距，为基因组医学中的机器人和临床自动化建立可靠基础。

Method: 结合混沌游戏表示(CGR)和概念瓶颈模型(CBM)，通过生物学有意义的概念进行预测，并加入概念保真度监督、先验一致性对齐、KL分布匹配和不确定性校准。

Result: 在HIV亚型分类任务上达到最先进性能，具有优越的概念预测保真度和更好的成本效益权衡。

Conclusion: 该工作为基因组医学的自动化和机器人应用提供了可靠的基础框架。

Abstract: We propose an automated genomic interpretation module that transforms raw DNA
sequences into actionable, interpretable decisions suitable for integration
into medical automation and robotic systems. Our framework combines Chaos Game
Representation (CGR) with a Concept Bottleneck Model (CBM), enforcing
predictions to flow through biologically meaningful concepts such as GC
content, CpG density, and k mer motifs. To enhance reliability, we incorporate
concept fidelity supervision, prior consistency alignment, KL distribution
matching, and uncertainty calibration. Beyond accurate classification of HIV
subtypes across both in-house and LANL datasets, our module delivers
interpretable evidence that can be directly validated against biological
priors. A cost aware recommendation layer further translates predictive outputs
into decision policies that balance accuracy, calibration, and clinical
utility, reducing unnecessary retests and improving efficiency. Extensive
experiments demonstrate that the proposed system achieves state of the art
classification performance, superior concept prediction fidelity, and more
favorable cost benefit trade-offs compared to existing baselines. By bridging
the gap between interpretable genomic modeling and automated decision-making,
this work establishes a reliable foundation for robotic and clinical automation
in genomic medicine.

</details>


### [21] [VLA-R1: Enhancing Reasoning in Vision-Language-Action Models](https://arxiv.org/abs/2510.01623)
*Angen Ye,Zeyu Zhang,Boyuan Wang,Xiaofeng Wang,Dapeng Zhang,Zheng Zhu*

Main category: cs.CV

TL;DR: VLA-R1是一个增强推理能力的视觉-语言-动作模型，通过强化学习验证奖励和相对策略优化来系统优化推理和执行能力，在多个评估平台上表现出优越的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLA模型缺乏显式的逐步推理能力，通常直接输出最终动作而不考虑可操作性约束和几何关系，后训练流程也很少强化推理质量。

Method: 提出VLA-R1模型，集成RLVR（验证奖励强化学习）和GRPO（组相对策略优化）方法，设计了基于RLVR的后训练策略，包含区域对齐、轨迹一致性和输出格式化的可验证奖励，并构建了VLA-CoT-13K高质量数据集。

Result: 在领域内、领域外、仿真和真实机器人平台上的广泛评估表明，VLA-R1相比之前的VLA方法实现了优越的泛化能力和真实世界性能。

Conclusion: VLA-R1通过系统化的推理增强方法有效解决了当前VLA模型的局限性，在多个评估场景下表现出色，计划发布模型、代码和数据集。

Abstract: Vision-Language-Action (VLA) models aim to unify perception, language
understanding, and action generation, offering strong cross-task and
cross-scene generalization with broad impact on embodied AI. However, current
VLA models often lack explicit step-by-step reasoning, instead emitting final
actions without considering affordance constraints or geometric relations.
Their post-training pipelines also rarely reinforce reasoning quality, relying
primarily on supervised fine-tuning with weak reward design. To address these
challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates
Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative
Policy Optimization (GRPO) to systematically optimize both reasoning and
execution. Specifically, we design an RLVR-based post-training strategy with
verifiable rewards for region alignment, trajectory consistency, and output
formatting, thereby strengthening reasoning robustness and execution accuracy.
Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides
chain-of-thought supervision explicitly aligned with affordance and trajectory
annotations. Furthermore, extensive evaluations on in-domain, out-of-domain,
simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior
generalization and real-world performance compared to prior VLA methods. We
plan to release the model, code, and dataset following the publication of this
work. Code: https://github.com/GigaAI-research/VLA-R1. Website:
https://gigaai-research.github.io/VLA-R1.

</details>


### [22] [Joint Deblurring and 3D Reconstruction for Macrophotography](https://arxiv.org/abs/2510.01640)
*Yifan Zhao,Liangchen Li,Yuqi Zhou,Kai Wang,Yan Liang,Juyong Zhang*

Main category: cs.CV

TL;DR: 提出了一种针对微距摄影的联合去模糊和3D重建方法，通过多视图模糊图像联合优化物体的清晰3D模型和每个像素的离焦模糊核


<details>
  <summary>Details</summary>
Motivation: 微距摄影存在严重的离焦模糊问题，传统去模糊方法需要大量图像和标注，且目前没有针对微距摄影的多视图3D重建方法

Method: 使用可微分渲染方法自监督优化3D模型和离焦模糊核，从多视图模糊图像出发进行联合优化

Result: 实验表明，仅需少量多视图图像即可实现高质量图像去模糊和高保真3D外观恢复

Conclusion: 该方法成功解决了微距摄影中的离焦模糊问题，实现了高质量的联合去模糊和3D重建

Abstract: Macro lens has the advantages of high resolution and large magnification, and
3D modeling of small and detailed objects can provide richer information.
However, defocus blur in macrophotography is a long-standing problem that
heavily hinders the clear imaging of the captured objects and high-quality 3D
reconstruction of them. Traditional image deblurring methods require a large
number of images and annotations, and there is currently no multi-view 3D
reconstruction method for macrophotography. In this work, we propose a joint
deblurring and 3D reconstruction method for macrophotography. Starting from
multi-view blurry images captured, we jointly optimize the clear 3D model of
the object and the defocus blur kernel of each pixel. The entire framework
adopts a differentiable rendering method to self-supervise the optimization of
the 3D model and the defocus blur kernel. Extensive experiments show that from
a small number of multi-view images, our proposed method can not only achieve
high-quality image deblurring but also recover high-fidelity 3D appearance.

</details>


### [23] [FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring](https://arxiv.org/abs/2510.01641)
*Xiaoyang Liu,Zhengyan Zhou,Zihang Xu,Jiezhang Cao,Zheng Chen,Yulun Zhang*

Main category: cs.CV

TL;DR: FideDiff是一个新颖的单步扩散模型，用于高保真度图像去模糊。它通过重新定义运动模糊为扩散过程，训练一致性模型将模糊图像对齐到清晰图像，实现了快速且高质量的去模糊效果。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的图像去模糊方法存在推理时间过长和保真度不足的问题，限制了其在真实世界应用中的潜力。需要开发既能保持扩散模型强大生成能力，又能实现快速推理的高保真度去模糊方法。

Method: 将运动模糊重新定义为扩散过程，每个时间步代表不同程度的模糊图像；训练一致性模型将所有时间步对齐到同一清晰图像；通过重建具有匹配模糊轨迹的训练数据学习时间一致性；集成Kernel ControlNet进行模糊核估计，并引入自适应时间步预测。

Result: 在完整参考指标上实现了优越性能，超越了先前的基于扩散的方法，并与其他最先进模型的性能相匹配。

Conclusion: FideDiff为将预训练扩散模型应用于高保真度图像恢复任务提供了新方向，为在真实世界工业应用中进一步推进扩散模型建立了稳健的基线。

Abstract: Recent advancements in image motion deblurring, driven by CNNs and
transformers, have made significant progress. Large-scale pre-trained diffusion
models, which are rich in true-world modeling, have shown great promise for
high-quality image restoration tasks such as deblurring, demonstrating stronger
generative capabilities than CNN and transformer-based methods. However,
challenges such as unbearable inference time and compromised fidelity still
limit the full potential of the diffusion models. To address this, we introduce
FideDiff, a novel single-step diffusion model designed for high-fidelity
deblurring. We reformulate motion deblurring as a diffusion-like process where
each timestep represents a progressively blurred image, and we train a
consistency model that aligns all timesteps to the same clean image. By
reconstructing training data with matched blur trajectories, the model learns
temporal consistency, enabling accurate one-step deblurring. We further enhance
model performance by integrating Kernel ControlNet for blur kernel estimation
and introducing adaptive timestep prediction. Our model achieves superior
performance on full-reference metrics, surpassing previous diffusion-based
methods and matching the performance of other state-of-the-art models. FideDiff
offers a new direction for applying pre-trained diffusion models to
high-fidelity image restoration tasks, establishing a robust baseline for
further advancing diffusion models in real-world industrial applications. Our
dataset and code will be available at https://github.com/xyLiu339/FideDiff.

</details>


### [24] [LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition](https://arxiv.org/abs/2510.01651)
*Rixin Zhou,Peiqiang Qiu,Qian Zhang,Chuntao Li,Xi Yang*

Main category: cs.CV

TL;DR: 该论文提出了一个青铜器铭文识别系统，通过构建大规模数据集和两阶段检测识别流程，结合LadderMoE技术解决多域变异性、视觉退化和长尾分布等挑战。


<details>
  <summary>Details</summary>
Motivation: 青铜器铭文是早期中国文字的重要阶段，但自动识别面临视觉退化严重、多域变异性（照片、拓片、摹本）和极长尾字符分布等困难。

Method: 构建包含22454张全页图像和198598个标注字符的大规模数据集；开发两阶段检测识别流程，先定位铭文再转录字符；采用LadderMoE技术增强预训练CLIP编码器，实现动态专家专业化。

Result: 在单字符和全页识别任务上显著优于现有场景文本识别基线，在头部、中部和尾部类别以及所有采集模态上都取得了优越的准确率。

Conclusion: 为青铜器铭文识别和下游考古分析建立了坚实基础，证明了方法的有效性和鲁棒性。

Abstract: Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial
stage of early Chinese writing and provide indispensable evidence for
archaeological and historical studies. However, automatic BI recognition
remains difficult due to severe visual degradation, multi-domain variability
across photographs, rubbings, and tracings, and an extremely long-tailed
character distribution. To address these challenges, we curate a large-scale BI
dataset comprising 22454 full-page images and 198598 annotated characters
spanning 6658 unique categories, enabling robust cross-domain evaluation.
Building on this resource, we develop a two-stage detection-recognition
pipeline that first localizes inscriptions and then transcribes individual
characters. To handle heterogeneous domains and rare classes, we equip the
pipeline with LadderMoE, which augments a pretrained CLIP encoder with
ladder-style MoE adapters, enabling dynamic expert specialization and stronger
robustness. Comprehensive experiments on single-character and full-page
recognition tasks demonstrate that our method substantially outperforms
state-of-the-art scene text recognition baselines, achieving superior accuracy
across head, mid, and tail categories as well as all acquisition modalities.
These results establish a strong foundation for bronze inscription recognition
and downstream archaeological analysis.

</details>


### [25] [VirDA: Reusing Backbone for Unsupervised Domain Adaptation with Visual Reprogramming](https://arxiv.org/abs/2510.01660)
*Duy Nguyen,Dat Nguyen*

Main category: cs.CV

TL;DR: VirDA提出了一种通过视觉重编程进行域适应的参数高效方法，仅在骨干网络前添加可训练的视觉提示层，而不需要修改骨干网络参数，实现了跨域重用和显著减少参数数量。


<details>
  <summary>Details</summary>
Motivation: 现有的UDA方法为每个新的源-目标对微调整个骨干网络，导致参数数量和存储需求线性增长，且无法重用训练好的骨干参数。

Method: VirDA在骨干网络前添加域特定的视觉重编程层，生成视觉提示作为纹理偏置，通过优化域内和域间分布差异的目标函数来训练这些提示层，保持骨干网络参数不变。

Result: 在Office-31数据集上达到92.8%的平均准确率，仅需1.5M可训练参数，优于参数高效的PDA基线（+1.6%准确率，参数减少54%），并接近全骨干微调方法的性能但参数显著减少。

Conclusion: VirDA提供了一种参数高效的域适应方法，通过视觉重编程实现跨域重用骨干网络，在保持竞争力的准确率同时大幅减少参数需求。

Abstract: Existing UDA pipelines fine-tune already well-trained backbone parameters for
every new source-and-target pair, resulting in the number of training
parameters and storage memory growing linearly with each new pair, and also
preventing the reuse of these well-trained backbone parameters.
  Inspired by recent implications that existing backbones have textural biases,
we propose making use of domain-specific textural bias for domain adaptation
via visual reprogramming, namely VirDA.Instead of fine-tuning the full
backbone, VirDA prepends a domain-specific visual reprogramming layer to the
backbone. This layer produces visual prompts that act as an added textural bias
to the input image, adapting its ``style'' to a target domain. To optimize
these visual reprogramming layers, we use multiple objective functions that
optimize the intra- and inter-domain distribution differences when
domain-adapting visual prompts are applied. This process does not require
modifying the backbone parameters, allowing the same backbone to be reused
across different domains.
  We evaluate VirDA on Office-31 and obtain 92.8% mean accuracy with only 1.5M
trainable parameters. VirDA surpasses PDA, the state-of-the-art
parameter-efficient UDA baseline, by +1.6% accuracy while using just 46% of its
parameters. Compared with full-backbone fine-tuning, VirDA outperforms CDTrans
and FixBi by +0.2% and +1.4%, respectively, while requiring only 1.7% and 2.8%
of their trainable parameters. Relative to the strongest current methods
(PMTrans and TVT), VirDA uses ~1.7% of their parameters and trades off only
2.2% and 1.1% accuracy, respectively.

</details>


### [26] [Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery](https://arxiv.org/abs/2510.01662)
*Minh Tran,Maksim Siniukov,Zhangyu Jin,Mohammad Soleymani*

Main category: cs.CV

TL;DR: 本文提出了一种无监督的离散面部编码（DFE）方法，通过残差向量量化变分自编码器从3D网格序列中学习紧凑且可解释的面部表情字典，替代传统的手动标注FACS系统。


<details>
  <summary>Details</summary>
Motivation: 现有面部表情分析系统如FACS存在覆盖范围有限和手动标注成本高的问题，需要一种更有效、可扩展的替代方案。

Method: 使用3D可变形模型提取身份不变的表情特征，然后通过RVQ-VAE进行编码，生成来自共享码本的离散标记序列，每个标记捕获特定的可重用面部变形模式。

Result: DFE在三个心理学任务（压力检测、人格预测和抑郁检测）中优于FACS和强基线模型，能够捕捉更精确的面部行为并覆盖更广泛的面部表现。

Conclusion: DFE作为FACS的可扩展有效替代方案，在心理学和情感计算应用中具有重要潜力。

Abstract: Facial expression analysis is central to understanding human behavior, yet
existing coding systems such as the Facial Action Coding System (FACS) are
constrained by limited coverage and costly manual annotation. In this work, we
introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven
alternative of compact and interpretable dictionary of facial expressions from
3D mesh sequences learned through a Residual Vector Quantized Variational
Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant
expression features from images using a 3D Morphable Model (3DMM), effectively
disentangling factors such as head pose and facial geometry. We then encode
these features using an RVQ-VAE, producing a sequence of discrete tokens from a
shared codebook, where each token captures a specific, reusable facial
deformation pattern that contributes to the overall expression. Through
extensive experiments, we demonstrate that Discrete Facial Encoding captures
more precise facial behaviors than FACS and other facial encoding alternatives.
We evaluate the utility of our representation across three high-level
psychological tasks: stress detection, personality prediction, and depression
detection. Using a simple Bag-of-Words model built on top of the learned
tokens, our system consistently outperforms both FACS-based pipelines and
strong image and video representation learning models such as Masked
Autoencoders. Further analysis reveals that our representation covers a wider
variety of facial displays, highlighting its potential as a scalable and
effective alternative to FACS for psychological and affective computing
applications.

</details>


### [27] [Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale](https://arxiv.org/abs/2510.01665)
*Yongbo Chen,Yanhao Zhang,Shaifali Parashar,Liang Zhao,Shoudong Huang*

Main category: cs.CV

TL;DR: Con-NRSfM是一种用于共形变形下非刚性结构从运动的新方法，通过图优化框架和自监督学习实现精确的3D重建，超越了现有方法的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的NRSfM方法依赖严格的假设（如局部平面表面或局部线性变形），无法恢复共形尺度，且深度和共形尺度约束耦合，限制了重建精度。

Method: 使用2D选定的图像变形进行逐点重建，通过图优化框架优化；采用并行可分离迭代优化策略处理问题敏感性；结合自监督学习的编码器-解码器网络生成带纹理的密集3D点云。

Result: 在合成和真实数据集上的仿真和实验结果表明，该方法在重建精度和鲁棒性方面优于现有方法。

Conclusion: Con-NRSfM方法消除了现有方法的约束，能够准确计算局部共形尺度，实现更精确的深度估计，为单目视觉可变形SLAM提供了有效的解决方案。

Abstract: Non-rigid structure-from-motion (NRSfM), a promising technique for addressing
the mapping challenges in monocular visual deformable simultaneous localization
and mapping (SLAM), has attracted growing attention. We introduce a novel
method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing
isometric deformations as a subset. Our approach performs point-wise
reconstruction using 2D selected image warps optimized through a graph-based
framework. Unlike existing methods that rely on strict assumptions, such as
locally planar surfaces or locally linear deformations, and fail to recover the
conformal scale, our method eliminates these constraints and accurately
computes the local conformal scale. Additionally, our framework decouples
constraints on depth and conformal scale, which are inseparable in other
approaches, enabling more precise depth estimation. To address the sensitivity
of the formulated problem, we employ a parallel separable iterative
optimization strategy. Furthermore, a self-supervised learning framework,
utilizing an encoder-decoder network, is incorporated to generate dense 3D
point clouds with texture. Simulation and experimental results using both
synthetic and real datasets demonstrate that our method surpasses existing
approaches in terms of reconstruction accuracy and robustness. The code for the
proposed method will be made publicly available on the project website:
https://sites.google.com/view/con-nrsfm.

</details>


### [28] [UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction](https://arxiv.org/abs/2510.01669)
*Jin Cao,Hongrui Wu,Ziyong Feng,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: UniVerse是一个统一框架，通过将鲁棒重建分解为修复和重建两个子任务，利用视频扩散模型从不一致的多视角图像中重建3D场景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在稀疏观测下难以同时处理图像不一致性和进行3D重建的问题，传统方法严重依赖密集观测来优化模型参数。

Method: 先将不一致图像转换为初始视频，然后使用专门设计的视频扩散模型将其修复为一致图像，最后从修复后的图像重建3D场景。

Result: 在合成和真实数据集上的广泛实验表明，该方法在鲁棒重建方面具有强大的泛化能力和优越性能。

Conclusion: UniVerse框架通过解耦修复和重建任务，简化了优化过程，并能控制重建3D场景的风格。

Abstract: This paper tackles the challenge of robust reconstruction, i.e., the task of
reconstructing a 3D scene from a set of inconsistent multi-view images. Some
recent works have attempted to simultaneously remove image inconsistencies and
perform reconstruction by integrating image degradation modeling into neural 3D
scene representations.However, these methods rely heavily on dense observations
for robustly optimizing model parameters.To address this issue, we propose to
decouple robust reconstruction into two subtasks: restoration and
reconstruction, which naturally simplifies the optimization process.To this
end, we introduce UniVerse, a unified framework for robust reconstruction based
on a video diffusion model. Specifically, UniVerse first converts inconsistent
images into initial videos, then uses a specially designed video diffusion
model to restore them into consistent images, and finally reconstructs the 3D
scenes from these restored images.Compared with case-by-case per-view
degradation modeling, the diffusion model learns a general scene prior from
large-scale data, making it applicable to diverse image
inconsistencies.Extensive experiments on both synthetic and real-world datasets
demonstrate the strong generalization capability and superior performance of
our method in robust reconstruction. Moreover, UniVerse can control the style
of the reconstructed 3D scene. Project page:
https://jin-cao-tma.github.io/UniVerse.github.io/

</details>


### [29] [An Efficient Deep Template Matching and In-Plane Pose Estimation Method via Template-Aware Dynamic Convolution](https://arxiv.org/abs/2510.01678)
*Ke Jia,Ji Zhou,Hanxin Li,Zhigan Zhou,Haojie Chu,Xiaojie Li*

Main category: cs.CV

TL;DR: 提出了一种轻量级端到端模板匹配框架PoseMatch-TDCM，将模板匹配重新定义为联合定位和几何回归问题，通过模板感知动态卷积模块实现高效复合变换下的精确姿态估计。


<details>
  <summary>Details</summary>
Motivation: 传统方法通过穷举角度和尺度效率低下，而深度学习方法大多只估计相似度得分而不显式建模几何姿态，无法满足工业应用需求。

Method: 采用联合定位和几何回归框架，输出中心坐标、旋转角度和独立水平/垂直尺度；设计模板感知动态卷积模块(TDCM)在推理时动态注入模板特征；使用旋转-剪切增强策略和结构感知伪标签实现无几何标注训练；集成深度可分离卷积和像素洗牌实现高效匹配。

Result: 3.07M参数模型在复合变换下实现高精度和14ms推理速度，在小模板和多目标场景中表现出强鲁棒性。

Conclusion: 该方法适用于实时工业应用部署，在复合变换下具有高精度和强鲁棒性。

Abstract: In industrial inspection and component alignment tasks, template matching
requires efficient estimation of a target's position and geometric state
(rotation and scaling) under complex backgrounds to support precise downstream
operations. Traditional methods rely on exhaustive enumeration of angles and
scales, leading to low efficiency under compound transformations. Meanwhile,
most deep learning-based approaches only estimate similarity scores without
explicitly modeling geometric pose, making them inadequate for real-world
deployment. To overcome these limitations, we propose a lightweight end-to-end
framework that reformulates template matching as joint localization and
geometric regression, outputting the center coordinates, rotation angle, and
independent horizontal and vertical scales. A Template-Aware Dynamic
Convolution Module (TDCM) dynamically injects template features at inference to
guide generalizable matching. The compact network integrates depthwise
separable convolutions and pixel shuffle for efficient matching. To enable
geometric-annotation-free training, we introduce a rotation-shear-based
augmentation strategy with structure-aware pseudo labels. A lightweight
refinement module further improves angle and scale precision via local
optimization. Experiments show our 3.07M model achieves high precision and 14ms
inference under compound transformations. It also demonstrates strong
robustness in small-template and multi-object scenarios, making it highly
suitable for deployment in real-time industrial applications. The code is
available at:https://github.com/ZhouJ6610/PoseMatch-TDCM.

</details>


### [30] [Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning](https://arxiv.org/abs/2510.01681)
*Xuchen Li,Xuzhao Li,Jiahui Gao,Renjie Pi,Shiyu Hu,Wentao Zhang*

Main category: cs.CV

TL;DR: 提出了首个自适应像素推理框架，通过动态确定像素级操作来提升视觉语言模型在细粒度视觉任务中的性能，同时显著减少不必要的视觉操作。


<details>
  <summary>Details</summary>
Motivation: 当前视觉语言模型在处理需要精确理解细粒度视觉元素的任务时存在困难，主要由于图像编码过程中的信息丢失或对关键区域关注不足。现有方法引入像素级信息但容易过度使用，导致效率低下和无关视觉细节干扰。

Method: 提出自适应像素推理框架：1）应用操作感知的监督微调建立文本推理和视觉操作的基础能力；2）设计基于模型自身响应反馈的rollout引导强化学习框架，使模型能根据查询难度动态决定何时调用像素操作。

Result: 在广泛的多模态推理基准测试中，模型在HR-Bench 4K上达到73.4%的准确率，同时仅使用20.1%的工具使用率，相比先前方法在提高准确率的同时将工具使用率降低了66.5%。

Conclusion: 该框架有效解决了视觉语言模型中细粒度视觉理解的问题，通过自适应像素推理机制实现了性能提升与效率优化的平衡。

Abstract: Vision-Language Models (VLMs) excel at many multimodal tasks, yet they
frequently struggle with tasks requiring precise understanding and handling of
fine-grained visual elements. This is mainly due to information loss during
image encoding or insufficient attention to critical regions. Recent work has
shown promise by incorporating pixel-level visual information into the
reasoning process, enabling VLMs to access high-resolution visual details
during their thought process. However, this pixel-level information is often
overused, leading to inefficiency and distraction from irrelevant visual
details. To address these challenges, we propose the first framework for
adaptive pixel reasoning that dynamically determines necessary pixel-level
operations based on the input query. Specifically, we first apply
operation-aware supervised fine-tuning to establish baseline competence in
textual reasoning and visual operations, then design a novel rollout-guided
reinforcement learning framework relying on feedback of the model's own
responses, which enables the VLM to determine when pixel operations should be
invoked based on query difficulty. Experiments on extensive multimodal
reasoning benchmarks show that our model achieves superior performance while
significantly reducing unnecessary visual operations. Impressively, our model
achieves 73.4\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of
only 20.1\%, improving accuracy and simultaneously reducing tool usage by
66.5\% compared to the previous methods.

</details>


### [31] [Uncovering Overconfident Failures in CXR Models via Augmentation-Sensitivity Risk Scoring](https://arxiv.org/abs/2510.01683)
*Han-Jay Shu,Wei-Ning Chiu,Shun-Ting Chang,Meng-Ping Huang,Takeshi Tohyama,Ahram Han,Po-Chih Kuo*

Main category: cs.CV

TL;DR: 提出了基于增强敏感性的风险评分框架（ASRS），用于识别胸片诊断中易出错的病例，通过测量图像旋转后的嵌入变化来评估模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在胸片诊断中表现出色，但在不同患者亚组中存在准确性不均的问题，现有错误检测方法难以处理分布内的细微错误。

Method: ASRS框架应用临床可行的旋转（±15°/±30°），使用RAD-DINO编码器测量嵌入变化，通过敏感性评分将样本分为稳定性四分位数。

Result: 高敏感性病例的召回率显著降低（-0.2至-0.3），尽管AUC和置信度较高，表明该方法能有效识别易错病例。

Conclusion: ASRS提供了一种无需标签的选择性预测方法，可用于临床医生审查，提高医疗AI的公平性和安全性。

Abstract: Deep learning models achieve strong performance in chest radiograph (CXR)
interpretation, yet fairness and reliability concerns persist. Models often
show uneven accuracy across patient subgroups, leading to hidden failures not
reflected in aggregate metrics. Existing error detection approaches -- based on
confidence calibration or out-of-distribution (OOD) detection -- struggle with
subtle within-distribution errors, while image- and representation-level
consistency-based methods remain underexplored in medical imaging. We propose
an augmentation-sensitivity risk scoring (ASRS) framework to identify
error-prone CXR cases. ASRS applies clinically plausible rotations ($\pm
15^\circ$/$\pm 30^\circ$) and measures embedding shifts with the RAD-DINO
encoder. Sensitivity scores stratify samples into stability quartiles, where
highly sensitive cases show substantially lower recall ($-0.2$ to $-0.3$)
despite high AUROC and confidence. ASRS provides a label-free means for
selective prediction and clinician review, improving fairness and safety in
medical AI.

</details>


### [32] [FreeViS: Training-free Video Stylization with Inconsistent References](https://arxiv.org/abs/2510.01686)
*Jiacong Xu,Yiqun Mei,Ke Zhang,Vishal M. Patel*

Main category: cs.CV

TL;DR: FreeViS是一个无需训练的视频风格化框架，通过整合多个风格化参考到预训练的I2V模型，实现高质量、时间一致性的视频风格化


<details>
  <summary>Details</summary>
Motivation: 现有视频风格化方法存在时间一致性差、风格丰富度不足的问题，而训练专用模型需要配对视频数据且计算成本高

Method: 整合多个风格化参考到预训练I2V模型，采用高频补偿约束内容布局和运动，结合基于光流的运动线索保留低显著性区域的风格纹理

Result: FreeViS在风格化保真度和时间一致性方面优于现有基线方法，获得强烈的人类偏好

Conclusion: 该训练无关的流程为高质量、时间一致的视频风格化提供了实用且经济的解决方案

Abstract: Video stylization plays a key role in content creation, but it remains a
challenging problem. Na\"ively applying image stylization frame-by-frame hurts
temporal consistency and reduces style richness. Alternatively, training a
dedicated video stylization model typically requires paired video data and is
computationally expensive. In this paper, we propose FreeViS, a training-free
video stylization framework that generates stylized videos with rich style
details and strong temporal coherence. Our method integrates multiple stylized
references to a pretrained image-to-video (I2V) model, effectively mitigating
the propagation errors observed in prior works, without introducing flickers
and stutters. In addition, it leverages high-frequency compensation to
constrain the content layout and motion, together with flow-based motion cues
to preserve style textures in low-saliency regions. Through extensive
evaluations, FreeViS delivers higher stylization fidelity and superior temporal
consistency, outperforming recent baselines and achieving strong human
preference. Our training-free pipeline offers a practical and economic solution
for high-quality, temporally coherent video stylization. The code and videos
can be accessed via https://xujiacong.github.io/FreeViS/

</details>


### [33] [MedQ-Bench: Evaluating and Exploring Medical Image Quality Assessment Abilities in MLLMs](https://arxiv.org/abs/2510.01691)
*Jiyao Liu,Jinjie Wei,Wanying Qu,Chenglong Ma,Junzhi Ning,Yunheng Li,Ying Chen,Xinzhe Luo,Pengcheng Chen,Xin Gao,Ming Hu,Huihui Xu,Xin Wang,Shujian Gao,Dingkang Yang,Zhongying Deng,Jin Ye,Lihao Liu,Junjun He,Ningsheng Xu*

Main category: cs.CV

TL;DR: MedQ-Bench是一个用于医学图像质量评估的基准测试，通过多模态大语言模型建立感知-推理范式，包含两个互补任务：MedQ-Perception（感知低级视觉属性）和MedQ-Reasoning（无参考和比较推理任务）。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像质量评估方法受限于标量评分指标，无法反映专家评估中的人类推理过程，需要建立更符合人类推理的评估范式。

Method: 构建涵盖5种成像模态、40多个质量属性的综合基准，包含2600个感知查询和708个推理评估，采用多维度评判协议评估模型输出，并与放射科医生进行人机对齐验证。

Result: 评估14个最先进MLLMs发现，模型表现出初步但不稳定的感知和推理能力，准确度不足以可靠临床使用。

Conclusion: MLLMs在医学图像质量评估方面具有潜力但需要针对性优化，MedQ-Bench将推动该领域的进一步探索。

Abstract: Medical Image Quality Assessment (IQA) serves as the first-mile safety gate
for clinical AI, yet existing approaches remain constrained by scalar,
score-based metrics and fail to reflect the descriptive, human-like reasoning
process central to expert evaluation. To address this gap, we introduce
MedQ-Bench, a comprehensive benchmark that establishes a perception-reasoning
paradigm for language-based evaluation of medical image quality with
Multi-modal Large Language Models (MLLMs). MedQ-Bench defines two complementary
tasks: (1) MedQ-Perception, which probes low-level perceptual capability via
human-curated questions on fundamental visual attributes; and (2)
MedQ-Reasoning, encompassing both no-reference and comparison reasoning tasks,
aligning model evaluation with human-like reasoning on image quality. The
benchmark spans five imaging modalities and over forty quality attributes,
totaling 2,600 perceptual queries and 708 reasoning assessments, covering
diverse image sources including authentic clinical acquisitions, images with
simulated degradations via physics-based reconstructions, and AI-generated
images. To evaluate reasoning ability, we propose a multi-dimensional judging
protocol that assesses model outputs along four complementary axes. We further
conduct rigorous human-AI alignment validation by comparing LLM-based judgement
with radiologists. Our evaluation of 14 state-of-the-art MLLMs demonstrates
that models exhibit preliminary but unstable perceptual and reasoning skills,
with insufficient accuracy for reliable clinical use. These findings highlight
the need for targeted optimization of MLLMs in medical IQA. We hope that
MedQ-Bench will catalyze further exploration and unlock the untapped potential
of MLLMs for medical image quality evaluation.

</details>


### [34] [Holistic Order Prediction in Natural Scenes](https://arxiv.org/abs/2510.01704)
*Pierre Musacchio,Hyunmin Lee,Jaesik Park*

Main category: cs.CV

TL;DR: InstaFormer是一个能够通过单次前向传播从RGB图像中预测完整遮挡和深度顺序的网络，解决了现有方法需要昂贵输入格式和高推理成本的问题。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型在理解实例级几何关系方面存在挑战，现有方法依赖昂贵的输入格式（如类别标签、二值分割掩码）和二次方的前向传播推理成本。

Method: InstaFormer通过对象查询和潜在掩码描述符之间的交互来实现整体顺序预测，这些描述符在语义上表示相同对象但携带互补信息。

Result: 该方法在综合基准测试和消融实验中表现出有效性，能够仅从RGB图像中恢复场景中所有实例的完整遮挡和深度顺序。

Conclusion: InstaFormer提供了一种高效的解决方案，减少了输入要求和推理成本，同时保持了准确的实例级几何关系预测能力。

Abstract: Even in controlled settings, understanding instance-wise geometries is a
challenging task for a wide range of visual models. Although specialized
systems exist, modern arts rely on expensive input formats (category labels,
binary segmentation masks) and inference costs (a quadratic amount of forward
passes). We mitigate these limitations by proposing InstaFormer, a network
capable of holistic order prediction. That is, solely given an input RGB image,
InstaFormer returns the full occlusion and depth orderings for all the
instances in the scene in a single forward pass. At its core, InstaFormer
relies on interactions between object queries and latent mask descriptors that
semantically represent the same objects while carrying complementary
information. We comprehensively benchmark and ablate our approach to highlight
its effectiveness. Our code and models are open-source and available at this
URL: https://github.com/SNU-VGILab/InstaOrder.

</details>


### [35] [PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning](https://arxiv.org/abs/2510.01715)
*Raahul Krishna Durairaju,K. Saruladha*

Main category: cs.CV

TL;DR: PyramidStyler是一个基于Transformer的神经风格迁移框架，通过金字塔位置编码和强化学习优化，实现了高效高质量的风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有的CNN和Transformer模型在处理复杂风格和高分辨率输入时效率低下，无法有效扩展。

Method: 提出PyramidStyler框架，包含金字塔位置编码(PPE)来捕捉多尺度特征，并结合强化学习动态优化风格化过程。

Result: 在Microsoft COCO和WikiArt数据集上训练4000轮后，内容损失降低62.6%至2.07，风格损失降低57.4%至0.86，推理时间1.39秒。使用RL后进一步改善为内容损失2.03，风格损失0.75，推理时间1.40秒。

Conclusion: 该方法实现了实时高质量的艺术渲染，在媒体和设计领域具有广泛应用前景。

Abstract: Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based
algorithm, enabling AI-driven artistic image synthesis. However, existing CNN
and transformer-based models struggle to scale efficiently to complex styles
and high-resolution inputs. We introduce PyramidStyler, a transformer framework
with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding
that captures both local details and global context while reducing
computational load. We further incorporate reinforcement learning to
dynamically optimize stylization, accelerating convergence. Trained on
Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to
2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s
inference--and yields further improvements (content 2.03; style 0.75) with
minimal speed penalty (1.40 s) when using RL. These results demonstrate
real-time, high-quality artistic rendering, with broad applications in media
and design.

</details>


### [36] [LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction](https://arxiv.org/abs/2510.01767)
*Sheng-Hsiang Hung,Ting-Yu Yen,Wei-Fang Sun,Simon See,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: LoBE-GS是一个负载均衡的高效3D高斯泼溅框架，通过深度感知分区和优化策略解决了大规模场景重建中的负载不平衡问题，将预处理时间从数小时缩短到分钟级，训练速度提升2倍。


<details>
  <summary>Details</summary>
Motivation: 现有分治方法在大规模无边界场景（如城市街区）中存在两个瓶颈：分区负载严重不平衡，粗到细流水线效率低下，经常需要重新加载整个模型并产生高开销。

Method: 提出深度感知分区方法减少预处理时间；采用基于优化的策略平衡各区块的可见高斯分布（计算负载的强代理）；引入可见性裁剪和选择性致密化两种轻量级技术降低训练成本。

Result: 在大规模城市和户外数据集上的评估显示，LoBE-GS比现有最优基线实现端到端训练时间提升2倍，同时保持重建质量，并能扩展到传统3DGS无法处理的场景。

Conclusion: LoBE-GS通过重新设计大规模3DGS流水线，有效解决了负载不平衡和效率问题，为大规模场景重建提供了高效的解决方案。

Abstract: 3D Gaussian Splatting (3DGS) has established itself as an efficient
representation for real-time, high-fidelity 3D scene reconstruction. However,
scaling 3DGS to large and unbounded scenes such as city blocks remains
difficult. Existing divide-and-conquer methods alleviate memory pressure by
partitioning the scene into blocks, but introduce new bottlenecks: (i)
partitions suffer from severe load imbalance since uniform or heuristic splits
do not reflect actual computational demands, and (ii) coarse-to-fine pipelines
fail to exploit the coarse stage efficiently, often reloading the entire model
and incurring high overhead. In this work, we introduce LoBE-GS, a novel
Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers
the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning
method that reduces preprocessing from hours to minutes, an optimization-based
strategy that balances visible Gaussians -- a strong proxy for computational
load -- across blocks, and two lightweight techniques, visibility cropping and
selective densification, to further reduce training cost. Evaluations on
large-scale urban and outdoor datasets show that LoBE-GS consistently achieves
up to $2\times$ faster end-to-end training time than state-of-the-art
baselines, while maintaining reconstruction quality and enabling scalability to
scenes infeasible with vanilla 3DGS.

</details>


### [37] [Pack and Force Your Memory: Long-form and Consistent Video Generation](https://arxiv.org/abs/2510.01784)
*Xiaofei Wu,Guozhen Zhang,Zhiyong Xu,Yuan Zhou,Qinglin Lu,Xuming He*

Main category: cs.CV

TL;DR: 提出MemoryPack和Direct Forcing两种方法，解决长视频生成中的长期依赖建模和错误累积问题


<details>
  <summary>Details</summary>
Motivation: 长视频生成面临两个关键挑战：需要捕捉长期依赖关系，同时防止自回归解码中固有的错误累积

Method: 1. MemoryPack：可学习的上下文检索机制，利用文本和图像信息作为全局指导，联合建模短期和长期依赖关系；2. Direct Forcing：高效的单步近似策略，改善训练-推理对齐以减少推理过程中的错误传播

Result: 实现了分钟级的时间一致性，计算效率高且保持线性复杂度，显著提升了长视频生成的上下文一致性和可靠性

Conclusion: MemoryPack和Direct Forcing的组合显著增强了自回归视频模型的实用性和可靠性，推动了长视频生成的实际应用

Abstract: Long-form video generation presents a dual challenge: models must capture
long-range dependencies while preventing the error accumulation inherent in
autoregressive decoding. To address these challenges, we make two
contributions. First, for dynamic context modeling, we propose MemoryPack, a
learnable context-retrieval mechanism that leverages both textual and image
information as global guidance to jointly model short- and long-term
dependencies, achieving minute-level temporal consistency. This design scales
gracefully with video length, preserves computational efficiency, and maintains
linear complexity. Second, to mitigate error accumulation, we introduce Direct
Forcing, an efficient single-step approximating strategy that improves
training-inference alignment and thereby curtails error propagation during
inference. Together, MemoryPack and Direct Forcing substantially enhance the
context consistency and reliability of long-form video generation, advancing
the practical usability of autoregressive video models.

</details>


### [38] [Calibrating the Full Predictive Class Distribution of 3D Object Detectors for Autonomous Driving](https://arxiv.org/abs/2510.01829)
*Cornelius Schröder,Marius-Raphael Schlüter,Markus Lienkamp*

Main category: cs.CV

TL;DR: 该论文提出了一种针对3D物体检测器分类任务置信度校准的方法，通过引入两种正则化损失项来改善主导预测和完整预测向量的校准效果。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，精确的物体检测和不确定性估计对于自感知和安全操作至关重要。当前需要关注完整预测置信度分布在校准方面的不足，特别是主导和次要类别预测的校准问题。

Method: 提出了两种辅助正则化损失项：一种校准主导预测，另一种校准完整预测向量。评估了CenterPoint、PillarNet和DSVT-Pillar等检测器的后处理和训练时方法。

Result: 研究发现，结合完整类别预测校准损失项和等渗回归方法，能够为CenterPoint和PillarNet提供最佳的主导和次要类别预测校准效果。但DSVT-Pillar无法使用相同方法同时校准主导和次要预测。

Conclusion: 该方法有效提升了3D物体检测器的置信度校准性能，但不同检测器架构需要针对性的校准策略。

Abstract: In autonomous systems, precise object detection and uncertainty estimation
are critical for self-aware and safe operation. This work addresses confidence
calibration for the classification task of 3D object detectors. We argue that
it is necessary to regard the calibration of the full predictive confidence
distribution over all classes and deduce a metric which captures the
calibration of dominant and secondary class predictions. We propose two
auxiliary regularizing loss terms which introduce either calibration of the
dominant prediction or the full prediction vector as a training goal. We
evaluate a range of post-hoc and train-time methods for CenterPoint, PillarNet
and DSVT-Pillar and find that combining our loss term, which regularizes for
calibration of the full class prediction, and isotonic regression lead to the
best calibration of CenterPoint and PillarNet with respect to both dominant and
secondary class predictions. We further find that DSVT-Pillar can not be
jointly calibrated for dominant and secondary predictions using the same
method.

</details>


### [39] [Leveraging Prior Knowledge of Diffusion Model for Person Search](https://arxiv.org/abs/2510.01841)
*Giyeol Kim,Sooyoung Yang,Jihyong Oh,Myungjoo Kang,Chanho Eom*

Main category: cs.CV

TL;DR: DiffPS是一个利用预训练扩散模型进行人员搜索的新框架，通过三个专门模块解决现有方法的问题，在两个基准数据集上达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 现有人员搜索方法使用ImageNet预训练骨干网络可能无法有效捕捉复杂空间上下文和细粒度身份线索，且共享骨干网络特征会导致检测和重识别两个子任务优化目标冲突。

Method: 提出DiffPS框架，利用预训练扩散模型，包含三个模块：扩散引导区域提议网络（DGRPN）用于改进人员定位，多尺度频率细化网络（MSFRN）减轻形状偏差，语义自适应特征聚合网络（SFAN）利用文本对齐的扩散特征。

Result: 在CUHK-SYSU和PRW数据集上达到了新的最先进水平。

Conclusion: 扩散先验知识可以有效解决人员搜索中检测和重识别任务的优化冲突问题，提出的DiffPS框架显著提升了性能。

Abstract: Person search aims to jointly perform person detection and re-identification
by localizing and identifying a query person within a gallery of uncropped
scene images. Existing methods predominantly utilize ImageNet pre-trained
backbones, which may be suboptimal for capturing the complex spatial context
and fine-grained identity cues necessary for person search. Moreover, they rely
on a shared backbone feature for both person detection and re-identification,
leading to suboptimal features due to conflicting optimization objectives. In
this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a
novel framework that leverages a pre-trained diffusion model while eliminating
the optimization conflict between two sub-tasks. We analyze key properties of
diffusion priors and propose three specialized modules: (i) Diffusion-Guided
Region Proposal Network (DGRPN) for enhanced person localization, (ii)
Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and
(iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage
text-aligned diffusion features. DiffPS sets a new state-of-the-art on
CUHK-SYSU and PRW.

</details>


### [40] [Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2510.01912)
*Yi Ai,Yuanhao Cai,Yulun Zhang,Xiaokang Yang*

Main category: cs.CV

TL;DR: FMU是首个将流匹配生成先验嵌入深度展开框架的HSI重建方法，通过平均速度损失增强流一致性，显著提升重建质量。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像成本高且重建困难，现有压缩感知系统如CASSI在严重退化情况下难以准确恢复精细光谱细节。

Method: 提出流匹配引导的展开网络(FMU)，将流匹配的生成先验嵌入深度展开框架，并引入平均速度损失来增强流的全局一致性。

Result: 在模拟和真实数据集上的大量实验表明，FMU在重建质量上显著优于现有方法。

Conclusion: FMU成功结合了基于优化方法的可解释性和流匹配的生成能力，为HSI重建提供了更鲁棒准确的解决方案。

Abstract: Hyperspectral imaging (HSI) provides rich spatial-spectral information but
remains costly to acquire due to hardware limitations and the difficulty of
reconstructing three-dimensional data from compressed measurements. Although
compressive sensing systems such as CASSI improve efficiency, accurate
reconstruction is still challenged by severe degradation and loss of fine
spectral details. We propose the Flow-Matching-guided Unfolding network (FMU),
which, to our knowledge, is the first to integrate flow matching into HSI
reconstruction by embedding its generative prior within a deep unfolding
framework. To further strengthen the learned dynamics, we introduce a mean
velocity loss that enforces global consistency of the flow, leading to a more
robust and accurate reconstruction. This hybrid design leverages the
interpretability of optimization-based methods and the generative capacity of
flow matching. Extensive experiments on both simulated and real datasets show
that FMU significantly outperforms existing approaches in reconstruction
quality. Code and models will be available at https://github.com/YiAi03/FMU.

</details>


### [41] [Automated Defect Detection for Mass-Produced Electronic Components Based on YOLO Object Detection Models](https://arxiv.org/abs/2510.01914)
*Wei-Lung Mao,Chun-Chi Wang,Po-Heng Chou,Yen-Ting Liu*

Main category: cs.CV

TL;DR: 提出基于深度学习的DIP封装自动缺陷检测系统，使用ConSinGAN解决缺陷样本不足问题，YOLOv7结合ConSinGAN在精度和检测时间上表现最优


<details>
  <summary>Details</summary>
Motivation: 传统工业元件缺陷检测耗时耗力，给质检人员带来负担且难以管理产品质量

Method: 使用数字相机光学和深度学习模型，采用ConSinGAN生成训练数据，比较四种YOLO模型（v3、v4、v7、v9）的性能

Result: YOLOv7结合ConSinGAN达到95.50%的准确率和285ms检测时间，远优于阈值方法

Conclusion: 该自动缺陷检测系统可轻松应用于多种缺陷类型或缺陷数据不足的情况

Abstract: Since the defect detection of conventional industry components is
time-consuming and labor-intensive, it leads to a significant burden on quality
inspection personnel and makes it difficult to manage product quality. In this
paper, we propose an automated defect detection system for the dual in-line
package (DIP) that is widely used in industry, using digital camera optics and
a deep learning (DL)-based model. The two most common defect categories of DIP
are examined: (1) surface defects, and (2) pin-leg defects. However, the lack
of defective component images leads to a challenge for detection tasks. To
solve this problem, the ConSinGAN is used to generate a suitable-sized dataset
for training and testing. Four varieties of the YOLO model are investigated
(v3, v4, v7, and v9), both in isolation and with the ConSinGAN augmentation.
The proposed YOLOv7 with ConSinGAN is superior to the other YOLO versions in
accuracy of 95.50\%, detection time of 285 ms, and is far superior to
threshold-based approaches. In addition, the supervisory control and data
acquisition (SCADA) system is developed, and the associated sensor architecture
is described. The proposed automated defect detection can be easily established
with numerous types of defects or insufficient defect data.

</details>


### [42] [Foundation Visual Encoders Are Secretly Few-Shot Anomaly Detectors](https://arxiv.org/abs/2510.01934)
*Guangyao Zhai,Yue Zhou,Xinyan Deng,Lars Heckler,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: Few-shot anomaly detection method called FoundAD that uses foundation visual encoders to detect anomalies by learning a nonlinear projection operator onto the natural image manifold, achieving competitive performance with fewer parameters.


<details>
  <summary>Details</summary>
Motivation: Limited samples in few-shot anomaly detection make accurate differentiation between normal and abnormal features challenging, especially under category-agnostic conditions. Foundation visual encoders pre-trained on large-scale data can help learn general distribution of normal images.

Method: FoundAD learns a nonlinear projection operator onto the natural image manifold. The method utilizes the correlation between anomaly amount in an image and the difference in learnt embeddings from foundation encoders to characterize and identify out-of-distribution regions.

Result: Extensive experiments show that FoundAD supports multi-class detection and achieves competitive performance while using substantially fewer parameters than prior methods. Evaluations with multiple foundation encoders including DINOv3 demonstrate effectiveness.

Conclusion: The approach broadens the perspective on foundation features and advances the field of few-shot anomaly detection by providing an effective tool for anomaly characterization and identification.

Abstract: Few-shot anomaly detection streamlines and simplifies industrial safety
inspection. However, limited samples make accurate differentiation between
normal and abnormal features challenging, and even more so under
category-agnostic conditions. Large-scale pre-training of foundation visual
encoders has advanced many fields, as the enormous quantity of data helps to
learn the general distribution of normal images. We observe that the anomaly
amount in an image directly correlates with the difference in the learnt
embeddings and utilize this to design a few-shot anomaly detector termed
FoundAD. This is done by learning a nonlinear projection operator onto the
natural image manifold. The simple operator acts as an effective tool for
anomaly detection to characterize and identify out-of-distribution regions in
an image. Extensive experiments show that our approach supports multi-class
detection and achieves competitive performance while using substantially fewer
parameters than prior methods. Backed up by evaluations with multiple
foundation encoders, including fresh DINOv3, we believe this idea broadens the
perspective on foundation features and advances the field of few-shot anomaly
detection.

</details>


### [43] [ClustViT: Clustering-based Token Merging for Semantic Segmentation](https://arxiv.org/abs/2510.01948)
*Fabio Montello,Ronja Güldenring,Lazaros Nalpantidis*

Main category: cs.CV

TL;DR: ClustViT提出了一种基于Vision Transformer的语义分割方法，通过可训练的聚类模块合并相似token，并使用再生模块恢复细节，显著降低计算复杂度。


<details>
  <summary>Details</summary>
Motivation: Vision Transformer在机器人系统中的实际应用受到其二次注意力复杂度的限制，现有token合并方法适用于分类任务但不适合密集预测任务如语义分割。

Method: 在ViT骨干网络基础上，引入可训练的聚类模块根据分割掩码的伪聚类指导合并相似token，然后使用再生模块恢复细节信息供下游头使用。

Result: 在三个不同数据集上实现了高达2.18倍的GFLOPs减少和1.64倍的推理加速，同时保持可比较的分割精度。

Conclusion: ClustViT为Vision Transformer在密集预测任务中的高效应用提供了可行方案，平衡了计算效率和分割精度。

Abstract: Vision Transformers can achieve high accuracy and strong generalization
across various contexts, but their practical applicability on real-world
robotic systems is limited due to their quadratic attention complexity. Recent
works have focused on dynamically merging tokens according to the image
complexity. Token merging works well for classification but is less suited to
dense prediction. We propose ClustViT, where we expand upon the Vision
Transformer (ViT) backbone and address semantic segmentation. Within our
architecture, a trainable Cluster module merges similar tokens along the
network guided by pseudo-clusters from segmentation masks. Subsequently, a
Regenerator module restores fine details for downstream heads. Our approach
achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different
datasets, with comparable segmentation accuracy. Our code and models will be
made publicly available.

</details>


### [44] [Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs](https://arxiv.org/abs/2510.01954)
*Yongyi Su,Haojie Zhang,Shijie Li,Nanqing Liu,Jingyi Liao,Junyi Pan,Yuan Liu,Xiaofen Xing,Chong Sun,Chen Li,Nancy F. Chen,Shuicheng Yan,Xulei Yang,Xun Xu*

Main category: cs.CV

TL;DR: PaDT是一种新的多模态大语言模型范式，通过将视觉补丁嵌入作为可解码令牌，使模型能够直接生成文本和视觉输出，解决了现有方法依赖间接表示的限制。


<details>
  <summary>Details</summary>
Motivation: 现有多模态大语言模型在视觉任务中依赖间接表示（如将坐标生成为文本），限制了性能并阻碍了密集预测任务如分割的实现。

Method: 提出Patch-as-Decodable Token (PaDT)范式，使用视觉参考令牌(VRTs)从查询图像的视觉补丁嵌入中派生，与LLM的文本输出令牌交错排列。通过轻量级解码器将LLM输出转换为检测、分割和定位预测，并采用动态扩展嵌入表和改进的训练策略。

Result: 在四个视觉感知和理解任务上的实证研究表明，PaDT始终达到最先进的性能，甚至优于显著更大的MLLM模型。

Conclusion: PaDT提供了一种统一的方法，使多模态大语言模型能够直接生成多样化的视觉输出，为密集视觉预测任务提供了有效的解决方案。

Abstract: Multimodal large language models (MLLMs) have advanced rapidly in recent
years. However, existing approaches for vision tasks often rely on indirect
representations, such as generating coordinates as text for detection, which
limits performance and prevents dense prediction tasks like segmentation. To
overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a
unified paradigm that enables MLLMs to directly generate both textual and
diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs),
derived from visual patch embeddings of query images and interleaved seamlessly
with LLM's output textual tokens. A lightweight decoder then transforms LLM's
outputs into detection, segmentation, and grounding predictions. Unlike prior
methods, PaDT processes VRTs independently at each forward pass and dynamically
expands the embedding table, thus improving localization and differentiation
among similar objects. We further tailor a training strategy for PaDT by
randomly selecting VRTs for supervised fine-tuning and introducing a robust
per-token cross-entropy loss. Our empirical studies across four visual
perception and understanding tasks suggest PaDT consistently achieving
state-of-the-art performance, even compared with significantly larger MLLM
models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.

</details>


### [45] [TriAlignXA: An Explainable Trilemma Alignment Framework for Trustworthy Agri-product Grading](https://arxiv.org/abs/2510.01990)
*Jianfei Xie,Ziyang Li*

Main category: cs.CV

TL;DR: 本文提出TriAlignXA可解释AI框架，通过'信任金字塔'模型和'三角信任指数'解决生鲜电商中的信任赤字问题，在农产品分级任务中实现比基线模型更高的准确率。


<details>
  <summary>Details</summary>
Motivation: 在线生鲜电商存在'信任赤字'，因为数字交易无法提供直接的产品质量感官感知。传统绝对分级标准在生物特性、时效性和经济可行性之间存在'不可能三角'限制。

Method: 构建'信任金字塔'模型，提出'三角信任指数'(TTI)，设计TriAlignXA可解释AI框架，包含三个引擎：生物自适应引擎、时效优化引擎和经济优化引擎，以及'预映射机制'将流程数据编码到QR码中。

Result: 在分级任务实验中准确率显著高于基线模型，实证证据和理论分析验证了框架在解决'不可能三角'方面的平衡能力。

Conclusion: 该研究为构建可信赖的在线农产品生态系统提供了从理论到实践的全面支持，建立了从算法决策到消费者信任的关键路径。

Abstract: The 'trust deficit' in online fruit and vegetable e-commerce stems from the
inability of digital transactions to provide direct sensory perception of
product quality. This paper constructs a 'Trust Pyramid' model through
'dual-source verification' of consumer trust. Experiments confirm that quality
is the cornerstone of trust. The study reveals an 'impossible triangle' in
agricultural product grading, comprising biological characteristics,
timeliness, and economic viability, highlighting the limitations of traditional
absolute grading standards. To quantitatively assess this trade-off, we propose
the 'Triangular Trust Index' (TTI). We redefine the role of algorithms from
'decision-makers' to 'providers of transparent decision-making bases',
designing the explainable AI framework--TriAlignXA. This framework supports
trustworthy online transactions within agricultural constraints through
multi-objective optimization. Its core relies on three engines: the
Bio-Adaptive Engine for granular quality description; the Timeliness
Optimization Engine for processing efficiency; and the Economic Optimization
Engine for cost control. Additionally, the "Pre-Mapping Mechanism" encodes
process data into QR codes, transparently conveying quality information.
Experiments on grading tasks demonstrate significantly higher accuracy than
baseline models. Empirical evidence and theoretical analysis verify the
framework's balancing capability in addressing the "impossible triangle". This
research provides comprehensive support--from theory to practice--for building
a trustworthy online produce ecosystem, establishing a critical pathway from
algorithmic decision-making to consumer trust.

</details>


### [46] [4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing](https://arxiv.org/abs/2510.01991)
*Lei Liu,Can Wang,Zhenghao Chen,Dong Xu*

Main category: cs.CV

TL;DR: 提出4DGS-Craft框架，解决4D高斯溅射编辑中的视图、时间和非编辑区域一致性问题，通过4D感知的InstructPix2Pix模型、多视图网格模块和基于LLM的用户意图理解实现一致且交互式的4D场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有4D高斯溅射编辑方法在视图一致性、时间一致性和非编辑区域保持方面存在挑战，且难以处理复杂文本指令，需要更一致和可控的编辑框架。

Method: 1. 引入4D感知的InstructPix2Pix模型，整合4D VGGT几何特征；2. 使用多视图网格模块迭代优化多视图输入图像；3. 提出高斯选择机制保护非编辑区域；4. 设计基于LLM的用户意图理解模块，将复杂指令分解为原子操作序列。

Result: 相比现有方法，4DGS-Craft能够实现更一致和可控的4D场景编辑，有效处理复杂用户指令，保持编辑区域和非编辑区域的一致性。

Conclusion: 4DGS-Craft通过整合几何特征、多视图优化和智能意图理解，为4D高斯溅射编辑提供了有效的解决方案，显著提升了编辑的一致性和用户交互体验。

Abstract: Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges
with view, temporal, and non-editing region consistency, as well as with
handling complex text instructions. To address these issues, we propose
4DGS-Craft, a consistent and interactive 4DGS editing framework. We first
introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal
consistency. This model incorporates 4D VGGT geometry features extracted from
the initial scene, enabling it to capture underlying 4D geometric structures
during editing. We further enhance this model with a multi-view grid module
that enforces consistency by iteratively refining multi-view input images while
jointly optimizing the underlying 4D scene. Furthermore, we preserve the
consistency of non-edited regions through a novel Gaussian selection mechanism,
which identifies and optimizes only the Gaussians within the edited regions.
Beyond consistency, facilitating user interaction is also crucial for effective
4DGS editing. Therefore, we design an LLM-based module for user intent
understanding. This module employs a user instruction template to define atomic
editing operations and leverages an LLM for reasoning. As a result, our
framework can interpret user intent and decompose complex instructions into a
logical sequence of atomic operations, enabling it to handle intricate user
commands and further enhance editing performance. Compared to related works,
our approach enables more consistent and controllable 4D scene editing. Our
code will be made available upon acceptance.

</details>


### [47] [Pure-Pass: Fine-Grained, Adaptive Masking for Dynamic Token-Mixing Routing in Lightweight Image Super-Resolution](https://arxiv.org/abs/2510.01997)
*Junyu Wu,Jie Tang,Jie Liu,Gangshan Wu*

Main category: cs.CV

TL;DR: 提出了Pure-Pass（PP）像素级掩码机制，通过识别纯像素并免除其昂贵计算，在保持自适应灵活性的同时实现细粒度、空间灵活的掩码，集成到ATD-light模型中在计算量相似的情况下优于CAMixer-ATD-light。


<details>
  <summary>Details</summary>
Motivation: 现有轻量级超分辨率方法如CAMixer存在适应性差、掩码粒度粗、空间灵活性不足等局限性，需要更精细的像素级处理机制来提升效率。

Method: 使用固定颜色中心点对像素进行分类，提出Pure-Pass像素级掩码机制，识别纯像素并免除其昂贵计算，集成到ATD-light模型中。

Result: PP-ATD-light在保持最小开销的同时实现了优越的超分辨率性能，在相似计算量下在重建质量和参数效率方面均优于CAMixer-ATD-light。

Conclusion: Pure-Pass机制通过像素级精细掩码有效提升了超分辨率模型的效率和性能，为轻量级图像重建提供了新的解决方案。

Abstract: Image Super-Resolution (SR) aims to reconstruct high-resolution images from
low-resolution counterparts, but the computational complexity of deep
learning-based methods often hinders practical deployment. CAMixer is the
pioneering work to integrate the advantages of existing lightweight SR methods
and proposes a content-aware mixer to route token mixers of varied complexities
according to the difficulty of content recovery. However, several limitations
remain, such as poor adaptability, coarse-grained masking and spatial
inflexibility, among others. We propose Pure-Pass (PP), a pixel-level masking
mechanism that identifies pure pixels and exempts them from expensive
computations. PP utilizes fixed color center points to classify pixels into
distinct categories, enabling fine-grained, spatially flexible masking while
maintaining adaptive flexibility. Integrated into the state-of-the-art
ATD-light model, PP-ATD-light achieves superior SR performance with minimal
overhead, outperforming CAMixer-ATD-light in reconstruction quality and
parameter efficiency when saving a similar amount of computation.

</details>


### [48] [Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework](https://arxiv.org/abs/2510.02001)
*Nanaka Hosokawa,Ryo Takahashi,Tomoya Kitano,Yukihiro Iida,Chisako Muramatsu,Tatsuro Hayashi,Yuta Seino,Xiangrong Zhou,Takeshi Hara,Akitoshi Katsumata,Hiroshi Fujita*

Main category: cs.CV

TL;DR: 使用GPT-4o多模态能力自动生成牙科全景X光片中颌骨囊肿的发现，通过自校正循环与结构化输出(SLSO)框架提高准确性


<details>
  <summary>Details</summary>
Motivation: 提高牙科影像中颌骨囊肿自动识别的准确性和结构化输出质量，减少AI幻觉现象

Method: 构建SLSO框架，包含10步流程：图像输入分析、结构化数据生成、牙号提取与一致性检查、不一致时迭代再生、发现生成与重构验证，并与传统CoT方法比较

Result: SLSO框架在牙号识别准确率提升66.9%，牙齿移动识别提升33.3%，牙根吸收识别提升28.6%，成功案例最多经过5次再生获得一致结构化输出

Conclusion: SLSO框架能有效强化阴性发现描述、抑制幻觉、提高牙号识别准确性，但对跨多牙的广泛病变识别有限，需进一步优化以实现实用化

Abstract: In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to
automatically generate jaw cyst findings on dental panoramic radiographs. To
improve accuracy, we constructed a Self-correction Loop with Structured Output
(SLSO) framework and verified its effectiveness. A 10-step process was
implemented for 22 cases of jaw cysts, including image input and analysis,
structured data generation, tooth number extraction and consistency checking,
iterative regeneration when inconsistencies were detected, and finding
generation with subsequent restructuring and consistency verification. A
comparative experiment was conducted using the conventional Chain-of-Thought
(CoT) method across seven evaluation items: transparency, internal structure,
borders, root resorption, tooth movement, relationships with other structures,
and tooth number. The results showed that the proposed SLSO framework improved
output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates
for tooth number, tooth movement, and root resorption, respectively. In the
successful cases, a consistently structured output was achieved after up to
five regenerations. Although statistical significance was not reached because
of the small size of the dataset, the overall SLSO framework enforced negative
finding descriptions, suppressed hallucinations, and improved tooth number
identification accuracy. However, the accurate identification of extensive
lesions spanning multiple teeth is limited. Nevertheless, further refinement is
required to enhance overall performance and move toward a practical finding
generation system.

</details>


### [49] [LiLa-Net: Lightweight Latent LiDAR Autoencoder for 3D Point Cloud Reconstruction](https://arxiv.org/abs/2510.02028)
*Mario Resino,Borja Pérez,Jaime Godoy,Abdulla Al-Kaff,Fernando García*

Main category: cs.CV

TL;DR: LiLa-Net是一种3D自编码器架构，仅使用LiDAR点云从真实交通环境中编码高效特征。通过减少编码器层数和简化跳跃连接，在保持性能的同时实现高效重构。


<details>
  <summary>Details</summary>
Motivation: 开发一种仅依赖LiDAR点云的3D自编码器，用于真实交通环境中的特征编码，旨在在不过度消耗资源的情况下提升性能。

Method: 采用跳跃连接概念，减少编码器层数并简化跳跃连接结构，构建高效的潜在空间表示。使用配备Velodyne LiDAR的半自动驾驶车辆收集数据。

Result: 模型在跳跃连接携带的信息与潜在编码之间实现了有效平衡，提高了重构质量而不影响性能，并展现出强大的泛化能力。

Conclusion: LiLa-Net架构在资源使用和性能之间取得了良好平衡，能够准确重构原始点云，并对非交通环境对象也表现出良好的重构能力。

Abstract: This work proposed a 3D autoencoder architecture, named LiLa-Net, which
encodes efficient features from real traffic environments, employing only the
LiDAR's point clouds. For this purpose, we have real semi-autonomous vehicle,
equipped with Velodyne LiDAR. The system leverage skip connections concept to
improve the performance without using extensive resources as the
state-of-the-art architectures. Key changes include reducing the number of
encoder layers and simplifying the skip connections, while still producing an
efficient and representative latent space which allows to accurately
reconstruct the original point cloud. Furthermore, an effective balance has
been achieved between the information carried by the skip connections and the
latent encoding, leading to improved reconstruction quality without
compromising performance. Finally, the model demonstrates strong generalization
capabilities, successfully reconstructing objects unrelated to the original
traffic environment.

</details>


### [50] [kabr-tools: Automated Framework for Multi-Species Behavioral Monitoring](https://arxiv.org/abs/2510.02030)
*Jenna Kline,Maksim Kholiavchenko,Samuel Stevens,Nina van Tiel,Alison Zhong,Namrata Banerji,Alec Sheets,Sowbaranika Balasubramaniam,Isla Duporge,Matthew Thompson,Elizabeth Campolongo,Jackson Miliko,Neil Rosser,Tanya Berger-Wolf,Charles V. Stewart,Daniel I. Rubenstein*

Main category: cs.CV

TL;DR: 提出kabr-tools开源工具包，通过无人机视频和机器学习实现野生动物多物种行为自动监测，显著提升行为数据采集效率和精度


<details>
  <summary>Details</summary>
Motivation: 传统野外观察方法范围有限、耗时费力，难以评估跨景观的行为响应，需要可扩展的方法来量化复杂多维行为模式

Method: 集成无人机视频与机器学习系统，利用目标检测、跟踪和行为分类技术提取行为、社会和空间指标，包括时间预算、行为转换、社会互动等

Result: 相比地面方法，无人机观测减少15%的可见性损失，捕获更多行为转换且精度更高；验证了斑马群规模与警惕性关系等行为模式

Conclusion: kabr-tools实现了大规模自动行为监测，为生态系统研究、保护和生态监测提供了强大工具

Abstract: A comprehensive understanding of animal behavior ecology depends on scalable
approaches to quantify and interpret complex, multidimensional behavioral
patterns. Traditional field observations are often limited in scope,
time-consuming, and labor-intensive, hindering the assessment of behavioral
responses across landscapes. To address this, we present kabr-tools (Kenyan
Animal Behavior Recognition Tools), an open-source package for automated
multi-species behavioral monitoring. This framework integrates drone-based
video with machine learning systems to extract behavioral, social, and spatial
metrics from wildlife footage. Our pipeline leverages object detection,
tracking, and behavioral classification systems to generate key metrics,
including time budgets, behavioral transitions, social interactions, habitat
associations, and group composition dynamics. Compared to ground-based methods,
drone-based observations significantly improved behavioral granularity,
reducing visibility loss by 15% and capturing more transitions with higher
accuracy and continuity. We validate kabr-tools through three case studies,
analyzing 969 behavioral sequences, surpassing the capacity of traditional
methods for data capture and annotation. We found that, like Plains zebras,
vigilance in Grevy's zebras decreases with herd size, but, unlike Plains
zebras, habitat has a negligible impact. Plains and Grevy's zebras exhibit
strong behavioral inertia, with rare transitions to alert behaviors and
observed spatial segregation between Grevy's zebras, Plains zebras, and
giraffes in mixed-species herds. By enabling automated behavioral monitoring at
scale, kabr-tools offers a powerful tool for ecosystem-wide studies, advancing
conservation, biodiversity research, and ecological monitoring.

</details>


### [51] [GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing](https://arxiv.org/abs/2510.02034)
*Mengtian Li,Yunshu Bai,Yimin Chu,Yijun Shen,Zhongmei Li,Weifeng Ge,Zhifeng Xie,Chaofeng Chen*

Main category: cs.CV

TL;DR: GaussianMorphing是一个用于多视角图像语义感知3D形状和纹理变形的新框架，通过网格引导的3D高斯泼溅实现高保真几何和外观建模，无需预定义同胚映射或标注数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖点云或需要预定义同胚映射来处理无纹理数据，存在局限性。本文旨在克服这些限制，实现高质量的3D形状和纹理变形。

Method: 采用网格引导的3D高斯泼溅技术，核心是统一变形策略：将3D高斯锚定到重建的网格面片上，通过拓扑感知约束确保几何一致变换和纹理保真度；利用网格拓扑作为几何先验建立无监督语义对应，通过物理合理的点轨迹保持结构完整性。

Result: 在TexMorph基准测试中，GaussianMorphing显著优于现有2D/3D方法，颜色一致性误差(ΔE)降低22.2%，EI指标降低26.2%。

Conclusion: 该框架在保持局部细节和全局语义一致性的同时，实现了高质量的3D形状和纹理变形，为多视角图像处理提供了有效的解决方案。

Abstract: We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape
and texture morphing from multi-view images. Previous approaches usually rely
on point clouds or require pre-defined homeomorphic mappings for untextured
data. Our method overcomes these limitations by leveraging mesh-guided 3D
Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling.
The core of our framework is a unified deformation strategy that anchors
3DGaussians to reconstructed mesh patches, ensuring geometrically consistent
transformations while preserving texture fidelity through topology-aware
constraints. In parallel, our framework establishes unsupervised semantic
correspondence by using the mesh topology as a geometric prior and maintains
structural integrity via physically plausible point trajectories. This
integrated approach preserves both local detail and global semantic coherence
throughout the morphing process with out requiring labeled data. On our
proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior
2D/3D methods, reducing color consistency error ($\Delta E$) by 22.2% and EI by
26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/

</details>


### [52] [Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers](https://arxiv.org/abs/2510.02043)
*Sahil Bhandary Karnoor,Romit Roy Choudhury*

Main category: cs.CV

TL;DR: 提出InPose方法，通过扩散模型和逆问题求解实现零样本泛化的人体姿态估计


<details>
  <summary>Details</summary>
Motivation: 现有基于条件扩散模型的方法在不同用户间泛化能力差，主要因为位置测量受用户体型影响大

Method: 使用预训练扩散模型仅基于旋转测量，通过似然项引导模型生成符合测量位置的姿态序列

Result: 提出的InPose方法能够零样本泛化，为任意用户生成符合稀疏体表测量的高概率姿态序列

Conclusion: 将姿态估计建模为逆问题，结合扩散模型先验和测量似然，实现了更好的跨用户泛化性能

Abstract: Pose estimation refers to tracking a human's full body posture, including
their head, torso, arms, and legs. The problem is challenging in practical
settings where the number of body sensors are limited. Past work has shown
promising results using conditional diffusion models, where the pose prediction
is conditioned on both <location, rotation> measurements from the sensors.
Unfortunately, nearly all these approaches generalize poorly across users,
primarly because location measurements are highly influenced by the body size
of the user. In this paper, we formulate pose estimation as an inverse problem
and design an algorithm capable of zero-shot generalization. Our idea utilizes
a pre-trained diffusion model and conditions it on rotational measurements
alone; the priors from this model are then guided by a likelihood term, derived
from the measured locations. Thus, given any user, our proposed InPose method
generatively estimates the highly likely sequence of poses that best explains
the sparse on-body measurements.

</details>


### [53] [VGDM: Vision-Guided Diffusion Model for Brain Tumor Detection and Segmentation](https://arxiv.org/abs/2510.02086)
*Arman Behnam*

Main category: cs.CV

TL;DR: VGDM是一种基于Transformer和扩散模型的脑肿瘤检测与分割框架，通过结合全局上下文推理和迭代去噪技术，在MRI脑肿瘤分割任务中超越了传统U-Net基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统卷积架构如U-Net在捕捉长距离依赖关系方面能力有限，限制了在复杂肿瘤结构上的性能表现。扩散模型在医学图像生成和分割边界细化方面展现出强大潜力。

Method: 在扩散过程核心嵌入视觉Transformer，利用Transformer主干网络建模整个MRI体积的空间关系，同时通过扩散细化减轻体素级误差并恢复细粒度肿瘤细节。

Result: 在MRI脑肿瘤数据集上的实验验证显示，在Dice相似系数和Hausdorff距离指标上均获得持续提升。

Conclusion: 这种混合设计为神经肿瘤学提供了改进鲁棒性和可扩展性的途径，展示了Transformer引导的扩散模型在推进肿瘤分割技术前沿方面的潜力。

Abstract: Accurate detection and segmentation of brain tumors from magnetic resonance
imaging (MRI) are essential for diagnosis, treatment planning, and clinical
monitoring. While convolutional architectures such as U-Net have long been the
backbone of medical image segmentation, their limited capacity to capture
long-range dependencies constrains performance on complex tumor structures.
Recent advances in diffusion models have demonstrated strong potential for
generating high-fidelity medical images and refining segmentation boundaries.
  In this work, we propose VGDM: Vision-Guided Diffusion Model for Brain Tumor
Detection and Segmentation framework, a transformer-driven diffusion framework
for brain tumor detection and segmentation. By embedding a vision transformer
at the core of the diffusion process, the model leverages global contextual
reasoning together with iterative denoising to enhance both volumetric accuracy
and boundary precision. The transformer backbone enables more effective
modeling of spatial relationships across entire MRI volumes, while diffusion
refinement mitigates voxel-level errors and recovers fine-grained tumor
details.
  This hybrid design provides a pathway toward improved robustness and
scalability in neuro-oncology, moving beyond conventional U-Net baselines.
Experimental validation on MRI brain tumor datasets demonstrates consistent
gains in Dice similarity and Hausdorff distance, underscoring the potential of
transformer-guided diffusion models to advance the state of the art in tumor
segmentation.

</details>


### [54] [Mapping Historic Urban Footprints in France: Balancing Quality, Scalability and AI Techniques](https://arxiv.org/abs/2510.02097)
*Walid Rabehi,Marion Le Texier,Rémi Lemoy*

Main category: cs.CV

TL;DR: 开发了一个可扩展的深度学习管道，从历史地图中提取法国1925-1950年间的城市足迹数据，填补了国家尺度数字化城市数据的空白。


<details>
  <summary>Details</summary>
Motivation: 1970年代之前法国历史城市扩张的定量分析受到缺乏全国性数字化城市足迹数据的限制。

Method: 采用双通道U-Net方法处理历史地图的高辐射度和风格复杂性，第一通道生成初步地图识别混淆区域，第二通道使用精炼数据集和第一模型输出最小化辐射噪声。

Result: 处理了覆盖法国全境的941个高分辨率图块，最终镶嵌图总体准确率达到73%，有效捕捉了多样化的城市模式。

Conclusion: 成功创建了首个开放获取的国家尺度城市足迹数据集，为长期城市化动态研究提供了支持。

Abstract: Quantitative analysis of historical urban sprawl in France before the 1970s
is hindered by the lack of nationwide digital urban footprint data. This study
bridges this gap by developing a scalable deep learning pipeline to extract
urban areas from the Scan Histo historical map series (1925-1950), which
produces the first open-access, national-scale urban footprint dataset for this
pivotal period. Our key innovation is a dual-pass U-Net approach designed to
handle the high radiometric and stylistic complexity of historical maps. The
first pass, trained on an initial dataset, generates a preliminary map that
identifies areas of confusion, such as text and roads, to guide targeted data
augmentation. The second pass uses a refined dataset and the binarized output
of the first model to minimize radiometric noise, which significantly reduces
false positives. Deployed on a high-performance computing cluster, our method
processes 941 high-resolution tiles covering the entirety of metropolitan
France. The final mosaic achieves an overall accuracy of 73%, effectively
capturing diverse urban patterns while overcoming common artifacts like labels
and contour lines. We openly release the code, training datasets, and the
resulting nationwide urban raster to support future research in long-term
urbanization dynamics.

</details>


### [55] [When Tracking Fails: Analyzing Failure Modes of SAM2 for Point-Based Tracking in Surgical Videos](https://arxiv.org/abs/2510.02100)
*Woowon Jang,Jiwon Im,Juseung Choi,Niki Rashidian,Wesley De Neve,Utku Ozbulak*

Main category: cs.CV

TL;DR: 本文系统分析了腹腔镜胆囊切除术视频中点跟踪方法的失败模式，发现点跟踪在手术工具上表现良好，但在解剖目标上表现不佳。


<details>
  <summary>Details</summary>
Motivation: SAM2等视频对象分割模型在手术视频中提供零样本跟踪能力，但点跟踪在复杂手术环境中的可靠性和失败情况尚未被充分理解。

Method: 聚焦胆囊、抓钳和L型电钩三个手术目标，比较点跟踪与分割掩码初始化的性能，通过定性分析揭示影响跟踪结果的关键因素。

Result: 点跟踪在手术工具上具有竞争力，但在解剖目标上表现一致较差，组织相似性和模糊边界导致失败。

Conclusion: 提供了选择定位跟踪点的可操作建议，以改善手术视频分析中的性能。

Abstract: Video object segmentation (VOS) models such as SAM2 offer promising zero-shot
tracking capabilities for surgical videos using minimal user input. Among the
available input types, point-based tracking offers an efficient and low-cost
alternative, yet its reliability and failure cases in complex surgical
environments are not well understood. In this work, we systematically analyze
the failure modes of point-based tracking in laparoscopic cholecystectomy
videos. Focusing on three surgical targets, the gallbladder, grasper, and
L-hook electrocautery, we compare the performance of point-based tracking with
segmentation mask initialization. Our results show that point-based tracking is
competitive for surgical tools but consistently underperforms for anatomical
targets, where tissue similarity and ambiguous boundaries lead to failure.
Through qualitative analysis, we reveal key factors influencing tracking
outcomes and provide several actionable recommendations for selecting and
placing tracking points to improve performance in surgical video analysis.

</details>


### [56] [FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation](https://arxiv.org/abs/2510.02114)
*Ding-Ruei Shen*

Main category: cs.CV

TL;DR: 提出了FFREEDG新任务和FRIEREN框架，解决联邦学习中无标签客户端数据下的语义分割域适应问题


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法要么不切实际地假设客户端有标签数据，要么未能充分利用现代视觉基础模型的能力，需要解决无标签客户端数据下的域适应挑战

Method: 使用视觉-语言解码器结合CLIP文本嵌入来改善语义消歧，采用弱到强一致性学习策略在伪标签上进行鲁棒的本地训练

Result: 在合成到真实和清晰到恶劣天气基准测试中，该框架有效解决了新任务，性能与现有域泛化和适应方法相当

Conclusion: FRIEREN框架为FFREEDG任务建立了强基线，展示了在无标签客户端数据下利用视觉基础模型进行联邦语义分割的有效性

Abstract: Federeated Learning (FL) offers a privacy-preserving solution for Semantic
Segmentation (SS) tasks to adapt to new domains, but faces significant
challenges from these domain shifts, particularly when client data is
unlabeled. However, most existing FL methods unrealistically assume access to
labeled data on remote clients or fail to leverage the power of modern Vision
Foundation Models (VFMs). Here, we propose a novel and challenging task,
FFREEDG, in which a model is pretrained on a server's labeled source dataset
and subsequently trained across clients using only their unlabeled data,
without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a
framework that leverages the knowledge of a VFM by integrating vision and
language modalities. Our approach employs a Vision-Language decoder guided by
CLIP-based text embeddings to improve semantic disambiguation and uses a
weak-to-strong consistency learning strategy for robust local training on
pseudo-labels. Our experiments on synthetic-to-real and
clear-to-adverse-weather benchmarks demonstrate that our framework effectively
tackles this new task, achieving competitive performance against established
domain generalization and adaptation methods and setting a strong baseline for
future research.

</details>


### [57] [Unlocking Vision-Language Models for Video Anomaly Detection via Fine-Grained Prompting](https://arxiv.org/abs/2510.02155)
*Shu Zou,Xinyu Tian,Lukas Wesemann,Fabian Waschkowski,Zhaoyuan Yang,Jing Zhang*

Main category: cs.CV

TL;DR: ASK-Hint是一个结构化提示框架，通过利用动作中心知识来提升冻结视觉语言模型在视频异常检测中的性能，实现了最先进的AUC表现和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有提示方法过于抽象，忽略了细粒度的人-物交互和动作语义，而这些对于复杂监控视频中的异常检测至关重要。

Method: 提出结构化提示框架，将提示组织成语义连贯的组别（如暴力、财产犯罪、公共安全），并制定细粒度引导问题，使模型预测与判别性视觉线索对齐。

Result: 在UCF-Crime和XD-Violence数据集上的实验表明，ASK-Hint持续提升AUC，相比微调和免训练方法都达到了最先进性能，并展现出强大的跨数据集和VLM骨干网络泛化能力。

Conclusion: 研究强调了提示粒度的重要性，确立了ASK-Hint作为可解释视频异常检测的免训练、可泛化解决方案。

Abstract: Prompting has emerged as a practical way to adapt frozen vision-language
models (VLMs) for video anomaly detection (VAD). Yet, existing prompts are
often overly abstract, overlooking the fine-grained human-object interactions
or action semantics that define complex anomalies in surveillance videos. We
propose ASK-Hint, a structured prompting framework that leverages
action-centric knowledge to elicit more accurate and interpretable reasoning
from frozen VLMs. Our approach organizes prompts into semantically coherent
groups (e.g. violence, property crimes, public safety) and formulates
fine-grained guiding questions that align model predictions with discriminative
visual cues. Extensive experiments on UCF-Crime and XD-Violence show that
ASK-Hint consistently improves AUC over prior baselines, achieving
state-of-the-art performance compared to both fine-tuned and training-free
methods. Beyond accuracy, our framework provides interpretable reasoning traces
towards anomaly and demonstrates strong generalization across datasets and VLM
backbones. These results highlight the critical role of prompt granularity and
establish ASK-Hint as a new training-free and generalizable solution for
explainable video anomaly detection.

</details>


### [58] [GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation](https://arxiv.org/abs/2510.02186)
*Weijia Dou,Xu Zhang,Yi Bin,Jian Liu,Bo Peng,Guoqing Wang,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: GeoPurify方法通过利用2D视觉语言模型的潜在几何信息和小型学生亲和力网络，在仅使用1.5%训练数据的情况下实现了最先进的3D语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决2D视觉语言模型特征直接投影到3D时产生的噪声和碎片化预测问题，同时避免传统方法需要大量标注3D数据和复杂训练流程的限制。

Method: 使用小型学生亲和力网络从3D自监督教师模型中提取几何先验来净化2D VLM生成的3D点特征，并在推理时采用几何引导池化模块确保语义和结构一致性。

Result: 在主要3D基准测试中达到或超越最先进性能，仅使用约1.5%的训练数据。

Conclusion: GeoPurify有效缓解了2D语义与3D几何结构之间的权衡，实现了卓越的数据效率。

Abstract: Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to
3D semantic segmentation expose a persistent trade-off. Directly projecting 2D
features into 3D yields noisy and fragmented predictions, whereas enforcing
geometric coherence necessitates costly training pipelines and large-scale
annotated 3D data. We argue that this limitation stems from the dominant
segmentation-and-matching paradigm, which fails to reconcile 2D semantics with
3D geometric structure. The geometric cues are not eliminated during the
2D-to-3D transfer but remain latent within the noisy and view-aggregated
features. To exploit this property, we propose GeoPurify that applies a small
Student Affinity Network to purify 2D VLM-generated 3D point features using
geometric priors distilled from a 3D self-supervised teacher model. During
inference, we devise a Geometry-Guided Pooling module to further denoise the
point cloud and ensure the semantic and structural consistency. Benefiting from
latent geometric information and the learned affinity network, GeoPurify
effectively mitigates the trade-off and achieves superior data efficiency.
Extensive experiments on major 3D benchmarks demonstrate that GeoPurify
achieves or surpasses state-of-the-art performance while utilizing only about
1.5% of the training data. Our codes and checkpoints are available at
[https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).

</details>


### [59] [Cross-Breed Pig Identification Using Auricular Vein Pattern Recognition: A Machine Learning Approach for Small-Scale Farming Applications](https://arxiv.org/abs/2510.02197)
*Emmanuel Nsengiyumvaa,Leonard Niyitegekaa,Eric Umuhoza*

Main category: cs.CV

TL;DR: 提出了一种基于猪耳静脉模式的无创生物识别方法，使用智能手机采集图像，通过计算机视觉和机器学习实现98.12%的识别准确率，为小规模农户提供低成本解决方案。


<details>
  <summary>Details</summary>
Motivation: 解决传统猪只识别方法（耳标、芯片）不可靠、成本高且主要针对纯种猪的问题，为小规模农户提供实用替代方案。

Method: 收集800张混种猪耳图像，开发多阶段计算机视觉流程增强静脉可见性，提取结构空间特征生成生物特征签名，使用支持向量机等机器学习模型进行分类。

Result: SVM模型达到98.12%的识别准确率，从图像处理到分类平均耗时8.3秒，证明实时农场部署可行性。

Conclusion: 耳静脉生物识别技术能够替代易损的物理标识，为农民提供经济高效、无压力的动物识别方法，有望将精准农业优势扩展到资源有限的农业社区。

Abstract: Accurate livestock identification is a cornerstone of modern farming: it
supports health monitoring, breeding programs, and productivity tracking.
However, common pig identification methods, such as ear tags and microchips,
are often unreliable, costly, target pure breeds, and thus impractical for
small-scale farmers. To address this gap, we propose a noninvasive biometric
identification approach that leverages uniqueness of the auricular vein
patterns. To this end, we have collected 800 ear images from 20 mixed-breed
pigs (Landrace cross Pietrain and Duroc cross Pietrain), captured using a
standard smartphone and simple back lighting. A multistage computer vision
pipeline was developed to enhance vein visibility, extract structural and
spatial features, and generate biometric signatures. These features were then
classified using machine learning models. Support Vector Machines (SVM)
achieved the highest accuracy: correctly identifying pigs with 98.12% precision
across mixed-breed populations. The entire process from image processing to
classification was completed in an average of 8.3 seconds, demonstrating
feasibility for real-time farm deployment. We believe that by replacing fragile
physical identifiers with permanent biological markers, this system provides
farmers with a cost-effective and stress-free method of animal identification.
More broadly, the findings confirm the practicality of auricular vein
biometrics for digitizing livestock management, reinforcing its potential to
extend the benefits of precision farming to resource-constrained agricultural
communities.

</details>


### [60] [MMDEW: Multipurpose Multiclass Density Estimation in the Wild](https://arxiv.org/abs/2510.02213)
*Villanelle O'Reilly,Jonathan Cox,Georgios Leontidis,Marc Hanheide,Petra Bosilj,James Brown*

Main category: cs.CV

TL;DR: 提出了一种基于Twins金字塔视觉Transformer的多类别密度估计框架，通过专门的计数头和类别聚焦模块，在密集遮挡场景中实现优于现有方法的多类别人群计数


<details>
  <summary>Details</summary>
Motivation: 解决密集遮挡场景中离散检测方法失效的问题，实现多类别对象的精确计数，特别是在人群密集和生物多样性监测等应用场景

Method: 使用Twins金字塔视觉Transformer作为主干网络，结合多尺度解码的多类别计数头，添加基于分割的类别聚焦模块来抑制类别间干扰

Result: 在VisDrone和iSAID基准测试中相比现有方法MAE降低33%、43%和64%，在密集场景中优于YOLOv11，成功应用于生物多样性监测数据集

Conclusion: 该方法通过区域损失机制拓展了多类别人群计数的应用领域，为保护工作和生态研究提供了可扩展的解决方案

Abstract: Density map estimation can be used to estimate object counts in dense and
occluded scenes where discrete counting-by-detection methods fail. We propose a
multicategory counting framework that leverages a Twins pyramid
vision-transformer backbone and a specialised multi-class counting head built
on a state-of-the-art multiscale decoding approach. A two-task design adds a
segmentation-based Category Focus Module, suppressing inter-category cross-talk
at training time. Training and evaluation on the VisDrone and iSAID benchmarks
demonstrates superior performance versus prior multicategory crowd-counting
approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11
underscores the necessity of crowd counting methods in dense scenes. The
method's regional loss opens up multi-class crowd counting to new domains,
demonstrated through the application to a biodiversity monitoring dataset,
highlighting its capacity to inform conservation efforts and enable scalable
ecological insights.

</details>


### [61] [TempoControl: Temporal Attention Guidance for Text-to-Video Models](https://arxiv.org/abs/2510.02226)
*Shira Schiber,Ofir Lindenbaum,Idan Schwartz*

Main category: cs.CV

TL;DR: TempoControl是一种无需重新训练或额外监督的方法，通过优化跨注意力图来实现视频生成中的时间对齐控制。


<details>
  <summary>Details</summary>
Motivation: 现有生成视频模型缺乏细粒度时间控制，无法让用户指定特定视觉元素在生成序列中的出现时间。

Method: 利用文本到视频扩散模型中的跨注意力图，通过相关性、能量和熵三个互补原则来引导概念的时间安排。

Result: 该方法能够精确控制时间安排，同时确保高质量和多样性的视频生成，适用于多种应用场景。

Conclusion: TempoControl在保持视频质量的同时实现了精确的时间控制，为视频生成提供了更灵活的时间对齐能力。

Abstract: Recent advances in generative video models have enabled the creation of
high-quality videos based on natural language prompts. However, these models
frequently lack fine-grained temporal control, meaning they do not allow users
to specify when particular visual elements should appear within a generated
sequence. In this work, we introduce TempoControl, a method that allows for
temporal alignment of visual concepts during inference, without requiring
retraining or additional supervision. TempoControl utilizes cross-attention
maps, a key component of text-to-video diffusion models, to guide the timing of
concepts through a novel optimization approach. Our method steers attention
using three complementary principles: aligning its temporal shape with a
control signal (via correlation), amplifying it where visibility is needed (via
energy), and maintaining spatial focus (via entropy). TempoControl allows
precise control over timing while ensuring high video quality and diversity. We
demonstrate its effectiveness across various video generation applications,
including temporal reordering for single and multiple objects, as well as
action and audio-aligned generation.

</details>


### [62] [RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning](https://arxiv.org/abs/2510.02240)
*Sicheng Feng,Kaiwen Tuo,Song Wang,Lingdong Kong,Jianke Zhu,Huan Wang*

Main category: cs.CV

TL;DR: 提出RewardMap框架解决多模态大语言模型在细粒度视觉推理任务中的稀疏奖励和优化不稳定问题，通过密集奖励设计和多阶段强化学习提升性能


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在结构化信息丰富的空间推理任务（如交通地图）中表现不佳，标准RL方法因稀疏奖励和优化不稳定而受阻

Method: 构建ReasonMap-Plus数据集提供密集奖励信号，提出RewardMap多阶段RL框架，包含难度感知奖励设计和从简单感知到复杂推理的多阶段训练策略

Result: 在ReasonMap和ReasonMap-Plus上实验显示各组件均带来性能提升，组合使用效果最佳，在6个基准测试上平均提升3.47%

Conclusion: RewardMap框架有效解决了MLLMs在细粒度视觉推理中的训练挑战，显著提升了视觉理解和推理能力

Abstract: Fine-grained visual reasoning remains a core challenge for multimodal large
language models (MLLMs). The recently introduced ReasonMap highlights this gap
by showing that even advanced MLLMs struggle with spatial reasoning in
structured and information-rich settings such as transit maps, a task of clear
practical and scientific importance. However, standard reinforcement learning
(RL) on such tasks is impeded by sparse rewards and unstable optimization. To
address this, we first construct ReasonMap-Plus, an extended dataset that
introduces dense reward signals through Visual Question Answering (VQA) tasks,
enabling effective cold-start training of fine-grained visual understanding
skills. Next, we propose RewardMap, a multi-stage RL framework designed to
improve both visual understanding and reasoning capabilities of MLLMs.
RewardMap incorporates two key designs. First, we introduce a difficulty-aware
reward design that incorporates detail rewards, directly tackling the sparse
rewards while providing richer supervision. Second, we propose a multi-stage RL
scheme that bootstraps training from simple perception to complex reasoning
tasks, offering a more effective cold-start strategy than conventional
Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus
demonstrate that each component of RewardMap contributes to consistent
performance gains, while their combination yields the best results. Moreover,
models trained with RewardMap achieve an average improvement of 3.47% across 6
benchmarks spanning spatial reasoning, fine-grained visual reasoning, and
general tasks beyond transit maps, underscoring enhanced visual understanding
and reasoning capabilities.

</details>


### [63] [DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing](https://arxiv.org/abs/2510.02253)
*Zihan Zhou,Shilin Lu,Shuli Leng,Shaocong Zhang,Zhuming Lian,Xinlei Yu,Adams Wai-Kin Kong*

Main category: cs.CV

TL;DR: DragFlow是首个利用FLUX强大先验进行基于拖拽的图像编辑的框架，通过区域级编辑范式、个性化适配器和MLLM解决任务歧义，在DragBench-DR和ReD Bench上超越现有基线。


<details>
  <summary>Details</summary>
Motivation: 基于拖拽的图像编辑长期存在目标区域失真问题，因为早期基础模型（如Stable Diffusion）的先验不足。随着DiT和流匹配技术的发展（如SD3.5、FLUX），生成先验显著增强，但拖拽编辑尚未受益于这些更强先验。

Method: 1. 引入区域级编辑范式，使用仿射变换提供更一致的特征监督；2. 集成预训练开放域个性化适配器（如IP-Adapter）增强主体一致性；3. 通过梯度掩码硬约束保持背景保真度；4. 利用多模态大语言模型（MLLM）解决任务歧义。

Result: 在DragBench-DR和ReD Bench上的大量实验表明，DragFlow超越了基于点和基于区域的基线方法，在基于拖拽的图像编辑中达到了新的最先进水平。

Conclusion: DragFlow有效利用了FLUX的丰富先验，通过区域级编辑和个性化适配器等技术，显著提升了拖拽编辑的质量和一致性，为基于拖拽的图像编辑设立了新的标杆。

Abstract: Drag-based image editing has long suffered from distortions in the target
region, largely because the priors of earlier base models, Stable Diffusion,
are insufficient to project optimized latents back onto the natural image
manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow
matching (e.g., SD3.5, FLUX), generative priors have become significantly
stronger, enabling advances across diverse editing tasks. However, drag-based
editing has yet to benefit from these stronger priors. This work proposes the
first framework to effectively harness FLUX's rich prior for drag-based
editing, dubbed DragFlow, achieving substantial gains over baselines. We first
show that directly applying point-based drag editing to DiTs performs poorly:
unlike the highly compressed features of UNets, DiT features are insufficiently
structured to provide reliable guidance for point-wise motion supervision. To
overcome this limitation, DragFlow introduces a region-based editing paradigm,
where affine transformations enable richer and more consistent feature
supervision. Additionally, we integrate pretrained open-domain personalization
adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving
background fidelity through gradient mask-based hard constraints. Multimodal
large language models (MLLMs) are further employed to resolve task ambiguities.
For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench)
featuring region-level dragging instructions. Extensive experiments on
DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and
region-based baselines, setting a new state-of-the-art in drag-based image
editing. Code and datasets will be publicly available upon publication.

</details>


### [64] [From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding](https://arxiv.org/abs/2510.02262)
*Guangyu Sun,Archit Singhal,Burak Uzkent,Mubarak Shah,Chen Chen,Garin Kessler*

Main category: cs.CV

TL;DR: 论文提出F2C方法，通过从关键帧选择扩展到关键片段选择来改善视频理解，在固定计算预算下保持时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型存在"大海捞针"问题，大量视觉标记耗尽模型上下文窗口。现有解决方案通过选择稀疏帧集来减少标记数量，但这种逐帧选择丢弃了基本的时间动态，导致对运动和事件连续性的推理效果不佳。

Method: 提出F2C方法，将选择从孤立的关键帧扩展到关键片段（短的、时间连贯的片段）。为了在固定计算预算下适应片段的更大标记占用，提出自适应分辨率策略，动态平衡空间分辨率和片段长度，确保每个视频的标记数量恒定。

Result: 在三个长视频基准测试上的实验表明，F2C在Video-MME、LongVideoBench和MLVU基准上分别比均匀采样高出8.1%、5.6%和10.3%。

Conclusion: 结果强调了在帧选择中保持时间连贯性的重要性，并为将视频LLM扩展到现实世界视频理解应用提供了实用途径。

Abstract: Video Large Language Models (VLMs) have achieved remarkable results on a
variety of vision language tasks, yet their practical use is limited by the
"needle in a haystack" problem: the massive number of visual tokens produced
from raw video frames exhausts the model's context window. Existing solutions
alleviate this issue by selecting a sparse set of frames, thereby reducing
token count, but such frame-wise selection discards essential temporal
dynamics, leading to suboptimal reasoning about motion and event continuity. In
this work we systematically explore the impact of temporal information and
demonstrate that extending selection from isolated key frames to key clips,
which are short, temporally coherent segments, improves video understanding. To
maintain a fixed computational budget while accommodating the larger token
footprint of clips, we propose an adaptive resolution strategy that dynamically
balances spatial resolution and clip length, ensuring a constant token count
per video. Experiments on three long-form video benchmarks demonstrate that our
training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and
10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These
results highlight the importance of preserving temporal coherence in frame
selection and provide a practical pathway for scaling Video LLMs to real world
video understanding applications. Project webpage is available at
https://guangyusun.com/f2c .

</details>


### [65] [Paving the Way Towards Kinematic Assessment Using Monocular Video: A Preclinical Benchmark of State-of-the-Art Deep-Learning-Based 3D Human Pose Estimators Against Inertial Sensors in Daily Living Activities](https://arxiv.org/abs/2510.02264)
*Mario Medrano-Paredes,Carmen Fernández-González,Francisco-Javier Díaz-Pernas,Hichem Saoudi,Javier González-Alonso,Mario Martínez-Zarzuela*

Main category: cs.CV

TL;DR: 该研究比较了基于单目视频的3D人体姿态估计模型与惯性测量单元（IMU）在临床相关日常活动中的性能，发现MotionAGFormer表现最佳，揭示了两种技术在成本、可及性和精度之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习和可穿戴传感器的发展，在非专业实验室环境下捕捉和分析人体运动成为可能。准确评估真实世界条件下的人体运动对于远程医疗、运动科学和康复至关重要。

Method: 利用VIDIMU数据集，比较了四种先进的深度学习框架（MotionAGFormer、MotionBERT、MMPose 2D-to-3D姿态提升和NVIDIA BodyTrack）与基于IMU数据的OpenSim逆运动学方法，评估了13种临床相关日常活动的关节角度。

Result: MotionAGFormer表现最优，总体RMSE为9.27°±4.80°，MAE为7.86°±4.18°，Pearson相关系数为0.86±0.15，决定系数R²为0.67±0.28。两种技术都适用于实验室外的运动学评估。

Conclusion: 研究明确了现成视频模型在健康成年人中提供临床有前景的运动学数据的领域，以及它们落后于IMU估计的领域，为研究人员和临床医生开发稳健、成本效益高且用户友好的远程医疗解决方案提供了宝贵指南。

Abstract: Advances in machine learning and wearable sensors offer new opportunities for
capturing and analyzing human movement outside specialized laboratories.
Accurate assessment of human movement under real-world conditions is essential
for telemedicine, sports science, and rehabilitation. This preclinical
benchmark compares monocular video-based 3D human pose estimation models with
inertial measurement units (IMUs), leveraging the VIDIMU dataset containing a
total of 13 clinically relevant daily activities which were captured using both
commodity video cameras and five IMUs. During this initial study only healthy
subjects were recorded, so results cannot be generalized to pathological
cohorts. Joint angles derived from state-of-the-art deep learning frameworks
(MotionAGFormer, MotionBERT, MMPose 2D-to-3D pose lifting, and NVIDIA
BodyTrack) were evaluated against joint angles computed from IMU data using
OpenSim inverse kinematics following the Human3.6M dataset format with 17
keypoints. Among them, MotionAGFormer demonstrated superior performance,
achieving the lowest overall RMSE ($9.27\deg \pm 4.80\deg$) and MAE ($7.86\deg
\pm 4.18\deg$), as well as the highest Pearson correlation ($0.86 \pm 0.15$)
and the highest coefficient of determination $R^{2}$ ($0.67 \pm 0.28$). The
results reveal that both technologies are viable for out-of-the-lab kinematic
assessment. However, they also highlight key trade-offs between video- and
sensor-based approaches including costs, accessibility, and precision. This
study clarifies where off-the-shelf video models already provide clinically
promising kinematics in healthy adults and where they lag behind IMU-based
estimates while establishing valuable guidelines for researchers and clinicians
seeking to develop robust, cost-effective, and user-friendly solutions for
telehealth and remote patient monitoring.

</details>


### [66] [NeuroSwift: A Lightweight Cross-Subject Framework for fMRI Visual Reconstruction of Complex Scenes](https://arxiv.org/abs/2510.02266)
*Shiyi Zhang,Dong Liang,Yihang Zhou*

Main category: cs.CV

TL;DR: NeuroSwift是一种新的fMRI视觉重建方法，通过集成AutoKL和CLIP适配器解决跨被试重建的挑战，只需少量参数微调即可实现最先进性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨被试视觉刺激重建的挑战，包括被试间神经表征差异和大脑对复杂视觉输入的抽象语义编码问题。

Method: 提出NeuroSwift方法，集成AutoKL（处理低级特征）和CLIP（处理语义）适配器，通过扩散模型实现。在单个被试上预训练后，仅对新被试的17%参数（全连接层）进行微调。

Result: 在轻量级GPU（三块RTX 4090）上仅需1小时训练即可达到最先进性能，优于现有方法。

Conclusion: NeuroSwift提供了一种高效且准确的跨被试视觉重建解决方案，显著降低了计算需求并提升了重建质量。

Abstract: Reconstructing visual information from brain activity via computer vision
technology provides an intuitive understanding of visual neural mechanisms.
Despite progress in decoding fMRI data with generative models, achieving
accurate cross-subject reconstruction of visual stimuli remains challenging and
computationally demanding. This difficulty arises from inter-subject
variability in neural representations and the brain's abstract encoding of core
semantic features in complex visual inputs. To address these challenges, we
propose NeuroSwift, which integrates complementary adapters via diffusion:
AutoKL for low-level features and CLIP for semantics. NeuroSwift's CLIP Adapter
is trained on Stable Diffusion generated images paired with COCO captions to
emulate higher visual cortex encoding. For cross-subject generalization, we
pretrain on one subject and then fine-tune only 17 percent of parameters (fully
connected layers) for new subjects, while freezing other components. This
enables state-of-the-art performance with only one hour of training per subject
on lightweight GPUs (three RTX 4090), and it outperforms existing methods.

</details>


### [67] [microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification](https://arxiv.org/abs/2510.02270)
*Sathira Silva,Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: microCLIP是一个自训练框架，通过联合优化CLIP的视觉和文本表示来提升细粒度图像分类性能，在13个基准测试上平均提升2.90%准确率


<details>
  <summary>Details</summary>
Motivation: CLIP模型在细粒度图像分类任务中表现受限，因为它依赖粗粒度的全局特征而忽略了空间精度。现有方法通过大语言模型描述与CLIP [CLS] token对齐，但缺乏空间精确性

Method: 提出Saliency-Oriented Attention Pooling (SOAP)和TokenFusion模块构建细粒度token；使用双头LLM分类器（冻结和可学习）提供稳定文本先验；采用动态知识聚合结合固定先验和演化logits迭代优化伪标签

Result: 在13个细粒度基准测试上平均获得2.90%的准确率提升，仅需轻量级适应

Conclusion: microCLIP成功挖掘了CLIP中潜在的细粒度信号，通过轻量级适应显著提升了细粒度分类性能

Abstract: Unsupervised adaptation of CLIP-based vision-language models (VLMs) for
fine-grained image classification requires sensitivity to microscopic local
cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse
global features restricts its performance on fine-grained classification tasks.
Prior efforts inject fine-grained knowledge by aligning large language model
(LLM) descriptions with the CLIP $\texttt{[CLS]}$ token; however, this approach
overlooks spatial precision. We propose $\textbf{microCLIP}$, a self-training
framework that jointly refines CLIP's visual and textual representations using
fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP)
within a lightweight TokenFusion module, which builds a saliency-guided
$\texttt{[FG]}$ token from patch embeddings and fuses it with the global
$\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we
introduce a two-headed LLM-derived classifier: a frozen classifier that, via
multi-view alignment, provides a stable text-based prior for pseudo-labeling,
and a learnable classifier initialized from LLM descriptions and fine-tuned
with TokenFusion. We further develop Dynamic Knowledge Aggregation, which
convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to
iteratively refine pseudo-labels. Together, these components uncover latent
fine-grained signals in CLIP, yielding a consistent $2.90\%$ average accuracy
gain across 13 fine-grained benchmarks while requiring only light adaptation.
Our code is available at https://github.com/sathiiii/microCLIP.

</details>


### [68] [VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL](https://arxiv.org/abs/2510.02282)
*Kyoungjun Park,Yifan Yang,Juheon Yi,Shicheng Zheng,Yifei Shen,Dongqi Han,Caihua Shan,Muhammad Muaz,Lili Qiu*

Main category: cs.CV

TL;DR: VidGuard-R1是首个基于多模态大语言模型(MLLM)的视频真实性检测器，采用组相对策略优化(GRPO)进行微调，能够同时提供高精度分类和可解释性推理。


<details>
  <summary>Details</summary>
Motivation: 随着AI生成视频技术的快速发展，需要有效的检测工具来应对错误信息和声誉损害等社会风险，同时检测模型需要提供可解释的推理以确保监管机构和终端用户的透明度。

Method: 通过构建包含14万真实和AI生成视频的挑战性数据集，使用Qwen-VL模型进行GRPO微调，并采用针对时间伪影和生成复杂度的两个专用奖励模型。

Result: VidGuard-R1在现有基准测试中实现了最先进的零样本性能，额外训练后准确率超过95%，案例研究显示其预测具有精确且可解释的推理依据。

Conclusion: 该研究成功开发了首个能够同时提供高精度检测和可解释推理的视频真实性检测器，为应对AI生成视频带来的社会风险提供了有效解决方案。

Abstract: With the rapid advancement of AI-generated videos, there is an urgent need
for effective detection tools to mitigate societal risks such as misinformation
and reputational harm. In addition to accurate classification, it is essential
that detection models provide interpretable explanations to ensure transparency
for regulators and end users. To address these challenges, we introduce
VidGuard-R1, the first video authenticity detector that fine-tunes a
multi-modal large language model (MLLM) using group relative policy
optimization (GRPO). Our model delivers both highly accurate judgments and
insightful reasoning. We curate a challenging dataset of 140k real and
AI-generated videos produced by state-of-the-art generation models, carefully
designing the generation process to maximize discrimination difficulty. We then
fine-tune Qwen-VL using GRPO with two specialized reward models that target
temporal artifacts and generation complexity. Extensive experiments demonstrate
that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing
benchmarks, with additional training pushing accuracy above 95%. Case studies
further show that VidGuard-R1 produces precise and interpretable rationales
behind its predictions. The code is publicly available at
https://VidGuard-R1.github.io.

</details>


### [69] [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://arxiv.org/abs/2510.02283)
*Justin Cui,Jie Wu,Ming Li,Tao Yang,Xiaojie Li,Rui Wang,Andrew Bai,Yuanhao Ban,Cho-Jui Hsieh*

Main category: cs.CV

TL;DR: 提出了一种简单有效的方法来缓解长视频生成中的质量退化问题，通过利用教师模型的丰富知识为自生成长视频中的学生模型提供指导，无需长视频教师监督或重新训练长视频数据集。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像和视频生成方面取得了革命性进展，但基于Transformer架构的计算成本过高，特别是在生成长视频时。现有自回归方法在超越训练范围时会出现质量显著下降的问题。

Method: 利用教师模型的知识为从自生成长视频中采样的片段提供指导，保持时间一致性，将视频长度扩展到教师能力的20倍，避免过度曝光和错误累积问题。

Result: 方法能够生成长达4分15秒的视频，相当于基础模型位置嵌入支持的最大跨度的99.9%，比基线模型长50倍以上。在标准基准测试和改进基准测试中，在保真度和一致性方面显著优于基线方法。

Conclusion: 该方法有效解决了长视频生成中的质量退化问题，实现了高质量的长视频生成，同时避免了重新计算重叠帧等先前方法的局限性。

Abstract: Diffusion models have revolutionized image and video generation, achieving
unprecedented visual quality. However, their reliance on transformer
architectures incurs prohibitively high computational costs, particularly when
extending generation to long videos. Recent work has explored autoregressive
formulations for long video generation, typically by distilling from
short-horizon bidirectional teachers. Nevertheless, given that teacher models
cannot synthesize long videos, the extrapolation of student models beyond their
training horizon often leads to pronounced quality degradation, arising from
the compounding of errors within the continuous latent space. In this paper, we
propose a simple yet effective approach to mitigate quality degradation in
long-horizon video generation without requiring supervision from long-video
teachers or retraining on long video datasets. Our approach centers on
exploiting the rich knowledge of teacher models to provide guidance for the
student model through sampled segments drawn from self-generated long videos.
Our method maintains temporal consistency while scaling video length by up to
20x beyond teacher's capability, avoiding common issues such as over-exposure
and error-accumulation without recomputing overlapping frames like previous
methods. When scaling up the computation, our method shows the capability of
generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the
maximum span supported by our base model's position embedding and more than 50x
longer than that of our baseline model. Experiments on standard benchmarks and
our proposed improved benchmark demonstrate that our approach substantially
outperforms baseline methods in both fidelity and consistency. Our long-horizon
videos demo can be found at https://self-forcing-plus-plus.github.io/

</details>


### [70] [Learning to Generate Object Interactions with Physics-Guided Video Diffusion](https://arxiv.org/abs/2510.02284)
*David Romero,Ariana Bermudez,Hao Li,Fabio Pizzati,Ivan Laptev*

Main category: cs.CV

TL;DR: KineMask是一个物理引导的视频生成方法，通过两阶段训练策略实现逼真的刚体控制和物体交互，显著提升了视频生成中物体交互的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成模型在物体交互的物理合理性方面存在不足，缺乏物理基础的控制机制，限制了其在机器人学和决策模拟等领域的应用潜力。

Method: 提出KineMask方法，采用两阶段训练策略：首先使用物体掩码进行未来运动监督，然后逐步移除监督；结合低层级运动控制（物体速度）和高层级文本条件（场景描述）来训练视频扩散模型。

Result: 在合成场景和真实场景的实验表明，KineMask在物体交互质量上显著优于同类规模的现有模型，能够生成更加物理合理的视频内容。

Conclusion: KineMask通过物理引导的视频生成方法有效解决了物体交互的物理合理性问题，低层级和高层级条件的结合在视频扩散模型中发挥了互补作用，为物理模拟和决策支持提供了有力工具。

Abstract: Recent models for video generation have achieved remarkable progress and are
now deployed in film, social media production, and advertising. Beyond their
creative potential, such models also hold promise as world simulators for
robotics and embodied decision making. Despite strong advances, however,
current approaches still struggle to generate physically plausible object
interactions and lack physics-grounded control mechanisms. To address this
limitation, we introduce KineMask, an approach for physics-guided video
generation that enables realistic rigid body control, interactions, and
effects. Given a single image and a specified object velocity, our method
generates videos with inferred motions and future object interactions. We
propose a two-stage training strategy that gradually removes future motion
supervision via object masks. Using this strategy we train video diffusion
models (VDMs) on synthetic scenes of simple interactions and demonstrate
significant improvements of object interactions in real scenes. Furthermore,
KineMask integrates low-level motion control with high-level textual
conditioning via predictive scene descriptions, leading to effective support
for synthesis of complex dynamical phenomena. Extensive experiments show that
KineMask achieves strong improvements over recent models of comparable size.
Ablation studies further highlight the complementary roles of low- and
high-level conditioning in VDMs. Our code, model, and data will be made
publicly available.

</details>


### [71] [MultiModal Action Conditioned Video Generation](https://arxiv.org/abs/2510.02287)
*Yichen Li,Antonio Torralba*

Main category: cs.CV

TL;DR: 当前视频模型缺乏精细控制能力，无法作为世界模型。本文引入多模态感官信息来实现精细动作控制，并提出特征学习范式和正则化方案来提高模拟精度。


<details>
  <summary>Details</summary>
Motivation: 通用家用机器人需要实时精细运动控制来处理精细任务和紧急情况，但现有视频模型缺乏这种能力。

Method: 引入本体感觉、动觉、力触觉和肌肉激活等多模态感官信息，开发特征学习范式对齐这些模态，同时提出正则化方案增强动作轨迹特征的因果关系。

Result: 实验表明，多模态感官信息的加入提高了模拟精度并减少了时间漂移，消融研究和下游应用验证了方法的有效性。

Conclusion: 多模态感官信息能够有效实现精细交互控制，为机器人精细动作模拟提供了实用解决方案。

Abstract: Current video models fail as world model as they lack fine-graiend control.
General-purpose household robots require real-time fine motor control to handle
delicate tasks and urgent situations. In this work, we introduce fine-grained
multimodal actions to capture such precise control. We consider senses of
proprioception, kinesthesia, force haptics, and muscle activation. Such
multimodal senses naturally enables fine-grained interactions that are
difficult to simulate with text-conditioned generative models. To effectively
simulate fine-grained multisensory actions, we develop a feature learning
paradigm that aligns these modalities while preserving the unique information
each modality provides. We further propose a regularization scheme to enhance
causality of the action trajectory features in representing intricate
interaction dynamics. Experiments show that incorporating multimodal senses
improves simulation accuracy and reduces temporal drift. Extensive ablation
studies and downstream applications demonstrate the effectiveness and
practicality of our work.

</details>


### [72] [VideoNSA: Native Sparse Attention Scales Video Understanding](https://arxiv.org/abs/2510.02295)
*Enxin Song,Wenhao Chai,Shusheng Yang,Ethan Armand,Xiaojun Shan,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: VideoNSA通过将Native Sparse Attention适配到视频语言模型中，解决了长视频理解中的上下文长度限制问题，在216K视频指令数据集上训练Qwen2.5-VL，实现了128K token的可靠扩展和性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有视频理解模型受限于上下文长度，容易错过关键过渡帧，难以在长时间尺度上保持连贯性。

Method: 采用硬件感知的混合注意力方法：文本使用密集注意力，视频使用Native Sparse Attention；在216K视频指令数据集上对Qwen2.5-VL进行端到端训练。

Result: 相比token压缩和无训练稀疏基线，VideoNSA在长视频理解、时序推理和空间基准测试上表现更优；可靠扩展到128K tokens；发现了最优的全局-局部注意力分配模式。

Conclusion: VideoNSA证明了稀疏注意力在视频语言模型中的有效性，能够诱导动态注意力汇聚点，为长视频理解提供了可行解决方案。

Abstract: Video understanding in multimodal language models remains limited by context
length: models often miss key transition frames and struggle to maintain
coherence across long time scales. To address this, we adapt Native Sparse
Attention (NSA) to video-language models. Our method, VideoNSA, adapts
Qwen2.5-VL through end-to-end training on a 216K video instruction dataset. We
employ a hardware-aware hybrid approach to attention, preserving dense
attention for text, while employing NSA for video. Compared to
token-compression and training-free sparse baselines, VideoNSA achieves
improved performance on long-video understanding, temporal reasoning, and
spatial benchmarks. Further ablation analysis reveals four key findings: (1)
reliable scaling to 128K tokens; (2) an optimal global-local attention
allocation at a fixed budget; (3) task-dependent branch usage patterns; and (4)
the learnable combined sparse attention help induce dynamic attention sinks.

</details>


### [73] [NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation](https://arxiv.org/abs/2510.02307)
*Ruozhen He,Moayed Haji-Ali,Ziyan Yang,Vicente Ordonez*

Main category: cs.CV

TL;DR: NoiseShift是一种无需训练的方法，通过重新校准噪声调度器来解决扩散模型在不同分辨率下的泛化问题，显著提升低分辨率图像生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前高分辨率文本到图像生成器无法为不需要高分辨率图像的用户提供经济高效的替代方案，因为扩散模型在固定分辨率训练后往往无法很好地泛化到低分辨率生成。

Method: 提出NoiseShift方法，根据分辨率大小重新校准去噪器的噪声水平，无需修改模型架构或采样计划，与现有模型兼容。

Result: 在LAION-COCO数据集上，NoiseShift将SD3.5的FID提升15.89%，SD3提升8.56%，Flux-Dev提升2.44%；在CelebA数据集上，分别提升10.36%、5.19%和3.02%。

Conclusion: NoiseShift能有效缓解分辨率相关的伪影，提升低分辨率图像生成质量，为扩散模型提供了更好的分辨率泛化能力。

Abstract: Text-to-image diffusion models trained on a fixed set of resolutions often
fail to generalize, even when asked to generate images at lower resolutions
than those seen during training. High-resolution text-to-image generators are
currently unable to easily offer an out-of-the-box budget-efficient alternative
to their users who might not need high-resolution images. We identify a key
technical insight in diffusion models that when addressed can help tackle this
limitation: Noise schedulers have unequal perceptual effects across
resolutions. The same level of noise removes disproportionately more signal
from lower-resolution images than from high-resolution images, leading to a
train-test mismatch. We propose NoiseShift, a training-free method that
recalibrates the noise level of the denoiser conditioned on resolution size.
NoiseShift requires no changes to model architecture or sampling schedule and
is compatible with existing models. When applied to Stable Diffusion 3, Stable
Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly
improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and
Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by
10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results
demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent
artifacts and enhancing the quality of low-resolution image generation.

</details>


### [74] [Inferring Dynamic Physical Properties from Video Foundation Models](https://arxiv.org/abs/2510.02311)
*Guanqi Zhan,Xianzheng Ma,Weidi Xie,Andrew Zisserman*

Main category: cs.CV

TL;DR: 该论文研究从视频中预测动态物理属性的任务，包括弹性、粘度和动态摩擦，提出了新的数据集和三种推理方法，并比较了不同模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究如何从视频中推断需要时间信息才能判断的动态物理属性，如弹性、粘度和动态摩擦，填补现有研究的空白。

Method: 收集了三个物理属性的新视频数据集；探索了三种推理方法：基于传统计算机视觉的oracle方法、使用预训练视频生成和自监督模型的简单读取机制、以及多模态大语言模型的提示策略。

Result: 生成式或自监督预训练的视频基础模型性能相似，但落后于oracle方法；多模态大语言模型目前性能较差，但通过合适的提示可以改善。

Conclusion: 视频基础模型在动态物理属性预测任务上表现良好，但仍有改进空间；多模态大语言模型需要更好的提示策略来提升性能。

Abstract: We study the task of predicting dynamic physical properties from videos. More
specifically, we consider physical properties that require temporal information
to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid,
and dynamic friction of an object sliding on a surface. To this end, we make
the following contributions: (i) We collect a new video dataset for each
physical property, consisting of synthetic training and testing splits, as well
as a real split for real world evaluation. (ii) We explore three ways to infer
the physical property from videos: (a) an oracle method where we supply the
visual cues that intrinsically reflect the property using classical computer
vision techniques; (b) a simple read out mechanism using a visual prompt and
trainable prompt vector for cross-attention on pre-trained video generative and
self-supervised models; and (c) prompt strategies for Multi-modal Large
Language Models (MLLMs). (iii) We show that video foundation models trained in
a generative or self-supervised manner achieve a similar performance, though
behind that of the oracle, and MLLMs are currently inferior to the other
models, though their performance can be improved through suitable prompting.

</details>


### [75] [Clink! Chop! Thud! -- Learning Object Sounds from Real-World Interactions](https://arxiv.org/abs/2510.02313)
*Mengyu Yang,Yiming Chen,Haozheng Pei,Siddhant Agarwal,Arun Balajee Vasudevan,James Hays*

Main category: cs.CV

TL;DR: 提出声音对象检测任务，通过多模态对象感知框架从第一人称视角视频中学习，利用分割掩码和槽注意力视觉编码器来识别日常物体交互中的声音来源。


<details>
  <summary>Details</summary>
Motivation: 日常物体交互会产生独特的声音，需要模型能够将这些声音与直接参与的物体联系起来，评估模型理解物体与声音对应关系的能力。

Method: 开发自动管道计算参与物体的分割掩码，使用槽注意力视觉编码器强化对象先验，从第一人称视角视频中学习多模态对象感知框架。

Result: 在新任务和现有多模态动作理解任务上实现了最先进的性能。

Conclusion: 提出的方法能够有效识别物体交互中的声音来源，在声音对象检测任务中表现出色。

Abstract: Can a model distinguish between the sound of a spoon hitting a hardwood floor
versus a carpeted one? Everyday object interactions produce sounds unique to
the objects involved. We introduce the sounding object detection task to
evaluate a model's ability to link these sounds to the objects directly
involved. Inspired by human perception, our multimodal object-aware framework
learns from in-the-wild egocentric videos. To encourage an object-centric
approach, we first develop an automatic pipeline to compute segmentation masks
of the objects involved to guide the model's focus during training towards the
most informative regions of the interaction. A slot attention visual encoder is
used to further enforce an object prior. We demonstrate state of the art
performance on our new task along with existing multimodal action understanding
tasks.

</details>


### [76] [StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions](https://arxiv.org/abs/2510.02314)
*Bo-Hsu Ke,You-Zhe Xie,Yu-Lun Liu,Wei-Chen Chiu*

Main category: cs.CV

TL;DR: 本文提出了一种针对3D高斯泼溅（3DGS）的密度引导毒化攻击方法，通过在低密度区域注入高斯点来嵌入视角相关的幻觉对象，同时引入自适应噪声策略破坏多视图一致性。


<details>
  <summary>Details</summary>
Motivation: 随着3D场景表示方法（如NeRF和3DGS）在新视角合成中的广泛应用，解决这些方法的脆弱性变得至关重要。本文旨在分析3DGS对图像级毒化攻击的鲁棒性。

Method: 提出密度引导毒化方法：1）使用核密度估计（KDE）识别低密度区域；2）在这些区域战略性地注入高斯点来嵌入视角相关的幻觉对象；3）引入自适应噪声策略破坏多视图一致性。

Result: 大量实验表明，该方法在毒化攻击效果上优于现有最先进技术，能够从毒化视角清晰显示幻觉对象，同时对无辜视角影响最小。

Conclusion: 本文不仅提出了有效的3DGS毒化攻击方法，还建立了基于KDE的系统评估协议，为未来研究提供了客观的基准测试框架。

Abstract: 3D scene representation methods like Neural Radiance Fields (NeRF) and 3D
Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As
these methods become prevalent, addressing their vulnerabilities becomes
critical. We analyze 3DGS robustness against image-level poisoning attacks and
propose a novel density-guided poisoning method. Our method strategically
injects Gaussian points into low-density regions identified via Kernel Density
Estimation (KDE), embedding viewpoint-dependent illusory objects clearly
visible from poisoned views while minimally affecting innocent views.
Additionally, we introduce an adaptive noise strategy to disrupt multi-view
consistency, further enhancing attack effectiveness. We propose a KDE-based
evaluation protocol to assess attack difficulty systematically, enabling
objective benchmarking for future research. Extensive experiments demonstrate
our method's superior performance compared to state-of-the-art techniques.
Project page: https://hentci.github.io/stealthattack/

</details>


### [77] [Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity](https://arxiv.org/abs/2510.02315)
*Eric Tillmann Bill,Enis Simsar,Thomas Hofmann*

Main category: cs.CV

TL;DR: 提出了首个理论框架和优化目标，通过随机最优控制来指导文本到图像模型的多主体生成，解决现有模型在多主体提示下属性泄漏、身份纠缠和主体遗漏的问题。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型在单实体提示上表现优秀，但在多主体描述中经常出现属性泄漏、身份纠缠和主体遗漏等问题，需要系统性的解决方案。

Method: 通过随机最优控制视角看待流匹配，提出了两种架构无关的算法：测试时控制器和伴随匹配微调方法，统一了先前的注意力启发式方法。

Result: 在Stable Diffusion 3.5、FLUX和Stable Diffusion XL等模型上，两种算法均能持续改善多主体对齐效果，同时保持基础模型风格。测试时控制在普通GPU上高效运行，微调控制器在有限提示上训练后能泛化到未见提示。

Conclusion: FOCUS方法在多个模型上实现了最先进的多主体保真度，为多主体文本到图像生成提供了有效的理论框架和实用解决方案。

Abstract: Text-to-image (T2I) models excel on single-entity prompts but struggle with
multi-subject descriptions, often showing attribute leakage, identity
entanglement, and subject omissions. We introduce the first theoretical
framework with a principled, optimizable objective for steering sampling
dynamics toward multi-subject fidelity. Viewing flow matching (FM) through
stochastic optimal control (SOC), we formulate subject disentanglement as
control over a trained FM sampler. This yields two architecture-agnostic
algorithms: (i) a training-free test-time controller that perturbs the base
velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight
fine-tuning rule that regresses a control network to a backward adjoint signal
while preserving base-model capabilities. The same formulation unifies prior
attention heuristics, extends to diffusion models via a flow-diffusion
correspondence, and provides the first fine-tuning route explicitly designed
for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and
Stable Diffusion XL, both algorithms consistently improve multi-subject
alignment while maintaining base-model style. Test-time control runs
efficiently on commodity GPUs, and fine-tuned controllers trained on limited
prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal
Control for Unentangled Subjects), which achieves state-of-the-art
multi-subject fidelity across models.

</details>


<div id='physics.optics'></div>

# physics.optics [[Back]](#toc)

### [78] [Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models](https://arxiv.org/abs/2510.01749)
*Valentin Delchevalerie,Nicolas Roy,Arnaud Bougaham,Alexandre Mayer,Benoît Frénay,Michaël Lobet*

Main category: physics.optics

TL;DR: 首次提出基于扩散模型的光子能带图生成方法，用于解决传统计算方法在光子晶体优化设计中的计算成本问题


<details>
  <summary>Details</summary>
Motivation: 传统光子能带图计算方法需要求解大量麦克斯韦方程，计算成本高昂，特别是在逆向设计优化循环中，限制了光子器件的设计效率

Method: 结合Transformer编码器提取输入结构的上下文嵌入，与潜在扩散模型耦合生成对应的光子能带图

Result: 开发出能够处理任意三维结构的光子能带图生成方法，为光子学领域的代理建模提供新策略

Conclusion: Transformer和扩散模型能够有效捕捉光子学中复杂的干涉和散射现象，为光子晶体设计开辟了新的高效计算途径

Abstract: Photonic crystals enable fine control over light propagation at the
nanoscale, and thus play a central role in the development of photonic and
quantum technologies. Photonic band diagrams (BDs) are a key tool to
investigate light propagation into such inhomogeneous structured materials.
However, computing BDs requires solving Maxwell's equations across many
configurations, making it numerically expensive, especially when embedded in
optimization loops for inverse design techniques, for example. To address this
challenge, we introduce the first approach for BD generation based on diffusion
models, with the capacity to later generalize and scale to arbitrary three
dimensional structures. Our method couples a transformer encoder, which
extracts contextual embeddings from the input structure, with a latent
diffusion model to generate the corresponding BD. In addition, we provide
insights into why transformers and diffusion models are well suited to capture
the complex interference and scattering phenomena inherent to photonics, paving
the way for new surrogate modeling strategies in this domain.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [79] [JaneEye: A 12-nm 2K-FPS 18.9-$μ$J/Frame Event-based Eye Tracking Accelerator](https://arxiv.org/abs/2510.01213)
*Tao Han,Ang Li,Qinyu Chen,Chang Gao*

Main category: eess.SP

TL;DR: JaneEye是一种基于事件相机的高效能眼动追踪硬件加速器，专为XR可穿戴设备设计，通过创新的轻量级神经网络架构和硬件优化，实现了高精度、低延迟和低功耗的眼动追踪。


<details>
  <summary>Details</summary>
Motivation: 传统基于帧的眼动追踪系统在XR应用中存在精度不足、延迟高和能耗大的问题，而事件相机具有超高时间分辨率和低功耗特性，为改进眼动追踪技术提供了新机遇。

Method: 提出超轻量级神经网络架构，包含新型ConvJANET层（仅保留遗忘门的简化ConvLSTM），将计算复杂度减半；采用自定义线性激活函数近似和定点量化；通过软硬件协同设计实现ASIC芯片优化。

Result: 在3ET+数据集上达到2.45像素误差精度，仅使用17.6K参数，支持1250Hz事件帧率；12nm ASIC芯片在400MHz频率下实现0.5ms端到端延迟（相当于2000FPS），能效为18.9μJ/帧。

Conclusion: JaneEye为下一代XR可穿戴设备设立了低功耗高性能眼动追踪的新基准，证明了事件相机和硬件加速器在解决XR眼动追踪挑战中的巨大潜力。

Abstract: Eye tracking has become a key technology for gaze-based interactions in
Extended Reality (XR). However, conventional frame-based eye-tracking systems
often fall short of XR's stringent requirements for high accuracy, low latency,
and energy efficiency. Event cameras present a compelling alternative, offering
ultra-high temporal resolution and low power consumption. In this paper, we
present JaneEye, an energy-efficient event-based eye-tracking hardware
accelerator designed specifically for wearable devices, leveraging sparse,
high-temporal-resolution event data. We introduce an ultra-lightweight neural
network architecture featuring a novel ConvJANET layer, which simplifies the
traditional ConvLSTM by retaining only the forget gate, thereby halving
computational complexity without sacrificing temporal modeling capability. Our
proposed model achieves high accuracy with a pixel error of 2.45 on the 3ET+
dataset, using only 17.6K parameters, with up to 1250 Hz event frame rate. To
further enhance hardware efficiency, we employ custom linear approximations of
activation functions (hardsigmoid and hardtanh) and fixed-point quantization.
Through software-hardware co-design, our 12-nm ASIC implementation operates at
400 MHz, delivering an end-to-end latency of 0.5 ms (equivalent to 2000 Frames
Per Second (FPS)) at an energy efficiency of 18.9 $\mu$J/frame. JaneEye sets a
new benchmark in low-power, high-performance eye-tracking solutions suitable
for integration into next-generation XR wearables.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [80] [An Efficient Quality Metric for Video Frame Interpolation Based on Motion-Field Divergence](https://arxiv.org/abs/2510.01361)
*Conall Daly,Darren Ramsook,Anil Kokaram*

Main category: eess.IV

TL;DR: 提出了一种基于运动发散加权的视频帧插值质量评估指标PSNR_DIV，在保持高效率的同时显著提升了与人类感知的相关性


<details>
  <summary>Details</summary>
Motivation: 现有视频帧插值质量评估指标（如PSNR、SSIM、LPIPS）忽略时间一致性，而专门设计的指标如FloLPIPS计算效率低下，限制了实际应用

Method: 通过运动发散加权增强PSNR，该技术源自胶片修复领域用于检测时间不一致性，突出运动场中的奇异点并用于加权图像误差

Result: 在BVI-VFI数据集上评估显示，PSNR_DIV相比FloLPIPS提升0.09皮尔逊相关系数，速度快2.5倍，内存使用减少4倍，性能在不同内容类别和运动估计器下保持稳定

Conclusion: PSNR_DIV的高效性和准确性使其能够快速评估质量，并可作为神经网络训练视频帧插值任务的实用损失函数

Abstract: Video frame interpolation is a fundamental tool for temporal video
enhancement, but existing quality metrics struggle to evaluate the perceptual
impact of interpolation artefacts effectively. Metrics like PSNR, SSIM and
LPIPS ignore temporal coherence. State-of-the-art quality metrics tailored
towards video frame interpolation, like FloLPIPS, have been developed but
suffer from computational inefficiency that limits their practical application.
We present $\text{PSNR}_{\text{DIV}}$, a novel full-reference quality metric
that enhances PSNR through motion divergence weighting, a technique adapted
from archival film restoration where it was developed to detect temporal
inconsistencies. Our approach highlights singularities in motion fields which
is then used to weight image errors. Evaluation on the BVI-VFI dataset (180
sequences across multiple frame rates, resolutions and interpolation methods)
shows $\text{PSNR}_{\text{DIV}}$ achieves statistically significant
improvements: +0.09 Pearson Linear Correlation Coefficient over FloLPIPS, while
being 2.5$\times$ faster and using 4$\times$ less memory. Performance remains
consistent across all content categories and are robust to the motion estimator
used. The efficiency and accuracy of $\text{PSNR}_{\text{DIV}}$ enables fast
quality evaluation and practical use as a loss function for training neural
networks for video frame interpolation tasks. An implementation of our metric
is available at www.github.com/conalld/psnr-div.

</details>


### [81] [Median2Median: Zero-shot Suppression of Structured Noise in Images](https://arxiv.org/abs/2510.01666)
*Jianxu Wang,Ge Wang*

Main category: eess.IV

TL;DR: 提出了Median2Median (M2M)框架，一种针对结构化噪声的零样本去噪方法，通过生成伪独立子图像对来突破传统方法对独立同分布噪声的限制


<details>
  <summary>Details</summary>
Motivation: 现实世界图像常被具有强各向异性相关性的结构化噪声污染，现有数据驱动方法需要大量高质量标签且泛化性有限，而零样本方法仅适用于独立同分布噪声，无法处理结构化噪声

Method: M2M引入新颖的采样策略，通过方向插值和广义中值滤波生成伪独立子图像对，采用随机分配策略扩大有效采样空间并消除系统偏差，适用于Noise2Noise训练

Result: 在真实模拟研究中，M2M在独立同分布噪声下与最先进的零样本方法性能相当，在相关噪声下始终优于这些方法

Conclusion: M2M为结构化噪声抑制提供了高效、无需数据的解决方案，标志着在突破严格独立同分布假设方面迈出了第一步

Abstract: Image denoising is a fundamental problem in computer vision and medical
imaging. However, real-world images are often degraded by structured noise with
strong anisotropic correlations that existing methods struggle to remove. Most
data-driven approaches rely on large datasets with high-quality labels and
still suffer from limited generalizability, whereas existing zero-shot methods
avoid this limitation but remain effective only for independent and identically
distributed (i.i.d.) noise. To address this gap, we propose Median2Median
(M2M), a zero-shot denoising framework designed for structured noise. M2M
introduces a novel sampling strategy that generates pseudo-independent
sub-image pairs from a single noisy input. This strategy leverages directional
interpolation and generalized median filtering to adaptively exclude values
distorted by structured artifacts. To further enlarge the effective sampling
space and eliminate systematic bias, a randomized assignment strategy is
employed, ensuring that the sampled sub-image pairs are suitable for
Noise2Noise training. In our realistic simulation studies, M2M performs on par
with state-of-the-art zero-shot methods under i.i.d. noise, while consistently
outperforming them under correlated noise. These findings establish M2M as an
efficient, data-free solution for structured noise suppression and mark the
first step toward effective zero-shot denoising beyond the strict i.i.d.
assumption.

</details>


### [82] [GFSR-Net: Guided Focus via Segment-Wise Relevance Network for Interpretable Deep Learning in Medical Imaging](https://arxiv.org/abs/2510.01919)
*Jhonatan Contreras,Thomas Bocklitz*

Main category: eess.IV

TL;DR: GFSR-Net是一种通过少量人工标注来提升医学图像分析可解释性的方法，能够使模型关注与诊断相关的区域，减少对无关模式的依赖。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中取得了显著成功，但在临床实践中由于缺乏可解释性而受到限制。模型可能基于与疾病无关的图像区域或现实中不存在的视觉线索进行预测，这会降低信任度并增加误诊风险。

Method: GFSR-Net使用少量人工标注来近似人类在图像中的关注区域，无需精确边界或详尽标记。在训练过程中，模型学习将其关注点与这些区域对齐，逐步强调具有诊断意义的特征。

Result: 实验表明GFSR-Net在保持可比或更优准确率的同时，生成的显著性图谱更好地反映了人类预期，减少了对无关模式的依赖，提升了自动化诊断工具的可信度。

Conclusion: GFSR-Net通过引导模型关注相关区域，有效提升了医学图像分析的可解释性和可靠性，适用于胸部X光、视网膜扫描和皮肤图像等多种医学图像类型。

Abstract: Deep learning has achieved remarkable success in medical image analysis,
however its adoption in clinical practice is limited by a lack of
interpretability. These models often make correct predictions without
explaining their reasoning. They may also rely on image regions unrelated to
the disease or visual cues, such as annotations, that are not present in
real-world conditions. This can reduce trust and increase the risk of
misleading diagnoses. We introduce the Guided Focus via Segment-Wise Relevance
Network (GFSR-Net), an approach designed to improve interpretability and
reliability in medical imaging. GFSR-Net uses a small number of human
annotations to approximate where a person would focus within an image
intuitively, without requiring precise boundaries or exhaustive markings,
making the process fast and practical. During training, the model learns to
align its focus with these areas, progressively emphasizing features that carry
diagnostic meaning. This guidance works across different types of natural and
medical images, including chest X-rays, retinal scans, and dermatological
images. Our experiments demonstrate that GFSR achieves comparable or superior
accuracy while producing saliency maps that better reflect human expectations.
This reduces the reliance on irrelevant patterns and increases confidence in
automated diagnostic tools.

</details>


### [83] [SpurBreast: A Curated Dataset for Investigating Spurious Correlations in Real-world Breast MRI Classification](https://arxiv.org/abs/2510.02109)
*Jong Bum Won,Wesley De Neve,Joris Vankerschaver,Utku Ozbulak*

Main category: eess.IV

TL;DR: SpurBreast是一个精心策划的乳腺MRI数据集，专门设计用于研究深度神经网络在医学影像中学习虚假相关性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有医学影像数据集缺乏系统性研究虚假相关性的能力，因为模型可能学习非临床特征而非有意义的医学模式，这阻碍了DNN在真实世界的部署。

Method: 通过分析100多个涉及患者、设备和成像协议的特征，识别出两个主要的虚假信号：磁场强度（全局特征）和图像方向（局部特征），并创建包含和不包含虚假相关性的对比数据集。

Result: 研究表明DNN确实会利用这些非临床信号，在验证集上获得高准确率但在无偏测试数据上泛化失败。

Conclusion: SpurBreast数据集为研究人员提供了系统研究临床相关/无关特征、不确定性估计、对抗鲁棒性和泛化策略的基准平台。

Abstract: Deep neural networks (DNNs) have demonstrated remarkable success in medical
imaging, yet their real-world deployment remains challenging due to spurious
correlations, where models can learn non-clinical features instead of
meaningful medical patterns. Existing medical imaging datasets are not designed
to systematically study this issue, largely due to restrictive licensing and
limited supplementary patient data. To address this gap, we introduce
SpurBreast, a curated breast MRI dataset that intentionally incorporates
spurious correlations to evaluate their impact on model performance. Analyzing
over 100 features involving patient, device, and imaging protocol, we identify
two dominant spurious signals: magnetic field strength (a global feature
influencing the entire image) and image orientation (a local feature affecting
spatial alignment). Through controlled dataset splits, we demonstrate that DNNs
can exploit these non-clinical signals, achieving high validation accuracy
while failing to generalize to unbiased test data. Alongside these two datasets
containing spurious correlations, we also provide benchmark datasets without
spurious correlations, allowing researchers to systematically investigate
clinically relevant and irrelevant features, uncertainty estimation,
adversarial robustness, and generalization strategies. Models and datasets are
available at https://github.com/utkuozbulak/spurbreast.

</details>


### [84] [Measurement-Guided Consistency Model Sampling for Inverse Problems](https://arxiv.org/abs/2510.02208)
*Amirreza Tanevardi,Pooria Abbas Rad Moghadam,Sajjad Amini*

Main category: eess.IV

TL;DR: 提出了一种针对逆问题重建的改进一致性采样方法，通过测量一致性机制引导采样随机性，在保持效率的同时提高重建质量


<details>
  <summary>Details</summary>
Motivation: 扩散模型在逆成像问题中表现强大但采样速度慢，一致性模型虽能快速生成但直接应用于逆问题的研究不足

Method: 修改一致性采样方法，将采样随机性与测量算子相关的测量一致性机制结合，强制保持对测量数据的保真度

Result: 在Fashion-MNIST和LSUN Bedroom数据集上，相比基线一致性采样，在感知和像素级指标上均有提升

Conclusion: 该方法能够在仅几步采样的情况下获得竞争性或更优的重建结果，为高效逆问题求解提供了新思路

Abstract: Diffusion models have become powerful generative priors for solving inverse
imaging problems, but their reliance on slow multi-step sampling limits
practical deployment. Consistency models address this bottleneck by enabling
high-quality generation in a single or only a few steps, yet their direct
adaptation to inverse problems is underexplored. In this paper, we present a
modified consistency sampling approach tailored for inverse problem
reconstruction: the sampler's stochasticity is guided by a
measurement-consistency mechanism tied to the measurement operator, which
enforces fidelity to the acquired measurements while retaining the efficiency
of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom
datasets demonstrate consistent improvements in perceptual and pixel-level
metrics, including Fr\'echet Inception Distance, Kernel Inception Distance,
peak signal-to-noise ratio, and structural similarity index measure, compared
to baseline consistency sampling, yielding competitive or superior
reconstructions with only a handful of steps.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [85] [VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation](https://arxiv.org/abs/2510.01388)
*Arthur Zhang,Xiangyun Meng,Luca Calliari,Dong-Ki Kim,Shayegan Omidshafiei,Joydeep Biswas,Ali Agha,Amirreza Shaban*

Main category: cs.RO

TL;DR: VENTURA是一个视觉语言导航系统，通过微调预训练的扩散模型进行路径规划，生成视觉路径掩码来指导机器人导航，在真实世界评估中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有视觉语言模型在机器人导航任务中难以迁移的问题，因为动作空间差异和预训练目标不匹配导致直接应用困难。

Method: 使用预训练的扩散模型生成路径掩码作为视觉规划，然后通过轻量级的行为克隆策略将视觉规划转换为可执行轨迹。训练数据来自自监督跟踪模型和VLM增强的标注，无需人工像素级标注。

Result: 在真实世界评估中，VENTURA在物体到达、避障和地形偏好任务上优于最先进的基础模型基线，成功率提高33%，碰撞减少54%，并能泛化到未见过的任务组合。

Conclusion: VENTURA展示了通过视觉规划方法有效解决机器人导航任务的能力，具有涌现的组合能力，为机器人适应开放世界环境提供了有效解决方案。

Abstract: Robots must adapt to diverse human instructions and operate safely in
unstructured, open-world environments. Recent Vision-Language models (VLMs)
offer strong priors for grounding language and perception, but remain difficult
to steer for navigation due to differences in action spaces and pretraining
objectives that hamper transferability to robotics tasks. Towards addressing
this, we introduce VENTURA, a vision-language navigation system that finetunes
internet-pretrained image diffusion models for path planning. Instead of
directly predicting low-level actions, VENTURA generates a path mask (i.e. a
visual plan) in image space that captures fine-grained, context-aware
navigation behaviors. A lightweight behavior-cloning policy grounds these
visual plans into executable trajectories, yielding an interface that follows
natural language instructions to generate diverse robot behaviors. To scale
training, we supervise on path masks derived from self-supervised tracking
models paired with VLM-augmented captions, avoiding manual pixel-level
annotation or highly engineered data collection setups. In extensive real-world
evaluations, VENTURA outperforms state-of-the-art foundation model baselines on
object reaching, obstacle avoidance, and terrain preference tasks, improving
success rates by 33% and reducing collisions by 54% across both seen and unseen
scenarios. Notably, we find that VENTURA generalizes to unseen combinations of
distinct tasks, revealing emergent compositional capabilities. Videos, code,
and additional materials: https://venturapath.github.io

</details>


### [86] [ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations](https://arxiv.org/abs/2510.01607)
*Qiyuan Zeng,Chengmeng Li,Jude St. John,Zhongyi Zhou,Junjie Wen,Guorui Feng,Yichen Zhu,Yi Xu*

Main category: cs.RO

TL;DR: ActiveUMI是一个便携式数据收集系统，通过VR遥操作将人类演示转移到机器人，实现复杂的双手操作任务学习。


<details>
  <summary>Details</summary>
Motivation: 解决从真实世界人类演示中收集高质量机器人学习数据的问题，特别是复杂双手操作任务，同时确保系统的便携性和数据质量。

Method: 结合便携式VR遥操作套件和传感器控制器，通过精确姿态对齐桥接人类-机器人运动学，采用沉浸式3D模型渲染、自包含可穿戴计算机和高效校准方法。关键创新是捕捉主动自我中心感知，记录操作者的头部运动以学习视觉注意与操作之间的联系。

Result: 在六个挑战性双手任务上评估，仅使用ActiveUMI数据训练的策略在分布内任务上达到70%平均成功率，对新物体和新环境保持56%的成功率，表现出强泛化能力。

Conclusion: 便携式数据收集系统结合学习的主动感知，为创建可泛化和高能力的真实世界机器人策略提供了有效且可扩展的途径。

Abstract: We present ActiveUMI, a framework for a data collection system that transfers
in-the-wild human demonstrations to robots capable of complex bimanual
manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized
controllers that mirror the robot's end-effectors, bridging human-robot
kinematics via precise pose alignment. To ensure mobility and data quality, we
introduce several key techniques, including immersive 3D model rendering, a
self-contained wearable computer, and efficient calibration methods.
ActiveUMI's defining feature is its capture of active, egocentric perception.
By recording an operator's deliberate head movements via a head-mounted
display, our system learns the crucial link between visual attention and
manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies
trained exclusively on ActiveUMI data achieve an average success rate of 70\%
on in-distribution tasks and demonstrate strong generalization, retaining a
56\% success rate when tested on novel objects and in new environments. Our
results demonstrate that portable data collection systems, when coupled with
learned active perception, provide an effective and scalable pathway toward
creating generalizable and highly capable real-world robot policies.

</details>


### [87] [DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis](https://arxiv.org/abs/2510.02178)
*Jialin Gao,Donghao Zhou,Mingjian Liang,Lihao Liu,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.RO

TL;DR: DisCo-Layout是一个用于3D室内布局合成的创新框架，通过解耦和协调物理与语义细化来解决传统方法的泛化问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法因固定数据集而泛化能力差，而现有的LLM和VLM方法虽然语义丰富但缺乏灵活细化能力，导致布局不理想。

Method: 提出DisCo-Layout框架：语义细化工具(SRT)修正抽象对象关系，物理细化工具(PRT)通过网格匹配算法解决具体空间问题；多智能体框架协调工具使用，包括规划器、设计器和评估器。

Result: 实验证明DisCo-Layout达到最先进性能，能生成真实、连贯且可泛化的3D室内布局。

Conclusion: DisCo-Layout通过解耦和协调物理与语义细化，有效解决了3D室内布局合成的泛化和细化问题。

Abstract: 3D indoor layout synthesis is crucial for creating virtual environments.
Traditional methods struggle with generalization due to fixed datasets. While
recent LLM and VLM-based approaches offer improved semantic richness, they
often lack robust and flexible refinement, resulting in suboptimal layouts. We
develop DisCo-Layout, a novel framework that disentangles and coordinates
physical and semantic refinement. For independent refinement, our Semantic
Refinement Tool (SRT) corrects abstract object relationships, while the
Physical Refinement Tool (PRT) resolves concrete spatial issues via a
grid-matching algorithm. For collaborative refinement, a multi-agent framework
intelligently orchestrates these tools, featuring a planner for placement
rules, a designer for initial layouts, and an evaluator for assessment.
Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating
realistic, coherent, and generalizable 3D indoor layouts. Our code will be
publicly available.

</details>


### [88] [Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning](https://arxiv.org/abs/2510.02268)
*Tianchong Jiang,Jingtian Ji,Xiangshan Tan,Jiading Fang,Anand Bhattad,Vitor Guizilini,Matthew R. Walter*

Main category: cs.RO

TL;DR: 论文研究通过显式地将策略与相机外参关联来实现视角不变模仿学习，发现这种方法能显著提升行为克隆策略在跨视角下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习策略在视角变化时性能下降，因为它们可能依赖静态背景中的视觉线索来推断相机姿态，这种捷径在相机位置或工作空间几何变化时会失效。

Method: 使用Plucker嵌入对每像素光线进行编码，将策略显式地条件化于相机外参，并在RoboSuite和ManiSkill中设计了六个操纵任务来评估策略在视角变化下的鲁棒性。

Result: 实验表明，未条件化外参的策略在固定场景中依赖背景线索推断相机姿态，但在视角变化时性能崩溃；而条件化外参的策略恢复了性能，实现了仅使用RGB的鲁棒控制。

Conclusion: 显式条件化相机外参能有效提升模仿学习策略的视角不变性，避免对静态背景线索的依赖，从而实现更鲁棒的跨视角泛化。

Abstract: We study view-invariant imitation learning by explicitly conditioning
policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we
show that conditioning on extrinsics significantly improves generalization
across viewpoints for standard behavior cloning policies, including ACT,
Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic
viewpoint shifts, we introduce six manipulation tasks in RoboSuite and
ManiSkill that pair "fixed" and "randomized" scene variants, decoupling
background cues from camera pose. Our analysis reveals that policies without
extrinsics often infer camera pose using visual cues from static backgrounds in
fixed scenes; this shortcut collapses when workspace geometry or camera
placement shifts. Conditioning on extrinsics restores performance and yields
robust RGB-only control without depth. We release the tasks, demonstrations,
and code at https://ripl.github.io/know_your_camera/ .

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [89] [Development and Evaluation of an AI-Driven Telemedicine System for Prenatal Healthcare](https://arxiv.org/abs/2510.01194)
*Juan Barrientos,Michaelle Pérez,Douglas González,Favio Reyna,Julio Fajardo,Andrea Lara*

Main category: cs.HC

TL;DR: 开发了一个人机协同AI系统，帮助非专业医护人员在资源匮乏地区使用低成本超声设备获取诊断相关的胎儿图像。


<details>
  <summary>Details</summary>
Motivation: 解决低收入和中等收入国家农村地区产科超声检查资源有限的问题，扩大产前影像服务的可及性。

Method: 采用人机协同AI系统，结合分类模型和基于网络的专家异步审核平台，通过盲扫协议识别关键帧。

Result: 系统在非专家操作的盲扫视频中能有效识别标准胎儿平面，现场评估显示良好可用性和低认知负荷。

Conclusion: 该系统有望在服务不足地区扩大产前影像服务的覆盖范围，提高产科超声检查的可及性。

Abstract: Access to obstetric ultrasound is often limited in low-resource settings,
particularly in rural areas of low- and middle-income countries. This work
proposes a human-in-the-loop artificial intelligence (AI) system designed to
assist midwives in acquiring diagnostically relevant fetal images using blind
sweep protocols. The system incorporates a classification model along with a
web-based platform for asynchronous specialist reviews. By identifying key
frames in blind sweep studies, the AI system allows specialists to concentrate
on interpretation rather than having to review entire videos. To evaluate its
performance, blind sweep videos captured by a small group of soft-trained
midwives using a low-cost Point-of-Care Ultrasound (POCUS) device were
analyzed. The system demonstrated promising results in identifying standard
fetal planes from sweeps made by non-experts. A field evaluation indicated good
usability and a low cognitive workload, suggesting that it has the potential to
expand access to prenatal imaging in underserved regions.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [90] [From 2D to 3D, Deep Learning-based Shape Reconstruction in Magnetic Resonance Imaging: A Review](https://arxiv.org/abs/2510.01296)
*Emma McMillian,Abhirup Banerjee,Alfonso Bueno-Orovio*

Main category: cs.LG

TL;DR: 这篇综述论文系统回顾了基于深度学习的3D MRI重建方法，重点分析了四种主要方法：点云、网格、形状感知和体积模型，并讨论了临床应用、数据集和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习在医学影像中的广泛应用，从2D MRI重建3D形状在疾病诊断、治疗规划和计算建模中变得越来越重要，需要系统梳理当前方法现状和发展方向。

Method: 采用文献综述方法，对四种主要3D重建方法（点云、网格、形状感知、体积模型）进行系统性分析，包括技术基础、局限性、临床应用和评估指标。

Result: 提供了从心脏到神经到肺部成像的全面方法概览，分析了模型在病变解剖中的临床适用性，以及训练测试数据的影响，并识别了当前方法的局限性。

Conclusion: 该综述为研究人员提供了当前3D重建方法的结构化概览，指出了包括多模态集成和跨模态框架在内的新兴研究方向，旨在推动深度学习向更鲁棒、可泛化和临床影响力的解决方案发展。

Abstract: Deep learning-based 3-dimensional (3D) shape reconstruction from
2-dimensional (2D) magnetic resonance imaging (MRI) has become increasingly
important in medical disease diagnosis, treatment planning, and computational
modeling. This review surveys the methodological landscape of 3D MRI
reconstruction, focusing on 4 primary approaches: point cloud, mesh-based,
shape-aware, and volumetric models. For each category, we analyze the current
state-of-the-art techniques, their methodological foundation, limitations, and
applications across anatomical structures. We provide an extensive overview
ranging from cardiac to neurological to lung imaging. We also focus on the
clinical applicability of models to diseased anatomy, and the influence of
their training and testing data. We examine publicly available datasets,
computational demands, and evaluation metrics. Finally, we highlight the
emerging research directions including multimodal integration and
cross-modality frameworks. This review aims to provide researchers with a
structured overview of current 3D reconstruction methodologies to identify
opportunities for advancing deep learning towards more robust, generalizable,
and clinically impactful solutions.

</details>


### [91] [Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction](https://arxiv.org/abs/2510.01407)
*Ethan G. Rogers,Cheng Wang*

Main category: cs.LG

TL;DR: 提出了一种基于低秩表示和向量量化的神经压缩重建框架，显著降低解码器计算复杂度，消除解码瓶颈，同时保持高质量图像重建。


<details>
  <summary>Details</summary>
Motivation: 传统神经压缩方法虽然压缩率高，但基于卷积的解码器计算复杂且成本高，阻碍了实际应用，需要解决解码器瓶颈问题。

Method: 在带有向量量化的自编码器中引入低秩表示，通过对学习到的图像潜在表示执行一系列计算高效的低秩操作来实现高效重建。

Result: 该方法大幅降低了神经压缩/重建解码阶段的计算开销，基本消除了解码器计算瓶颈，同时保持了高质量图像输出。

Conclusion: 提出的低秩表示框架有效解决了神经压缩中的解码器瓶颈问题，为实际应用提供了可行的解决方案。

Abstract: Image compression and reconstruction are crucial for various digital
applications. While contemporary neural compression methods achieve impressive
compression rates, the adoption of such technology has been largely hindered by
the complexity and large computational costs of the convolution-based decoders
during data reconstruction. To address the decoder bottleneck in neural
compression, we develop a new compression-reconstruction framework based on
incorporating low-rank representation in an autoencoder with vector
quantization. We demonstrated that performing a series of computationally
efficient low-rank operations on the learned latent representation of images
can efficiently reconstruct the data with high quality. Our approach
dramatically reduces the computational overhead in the decoding phase of neural
compression/reconstruction, essentially eliminating the decoder compute
bottleneck while maintaining high fidelity of image outputs.

</details>


### [92] [Beyond Simple Fusion: Adaptive Gated Fusion for Robust Multimodal Sentiment Analysis](https://arxiv.org/abs/2510.01677)
*Han Wu,Yanming Sun,Yunhe Yang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出了一种自适应门控融合网络（AGFN），通过双门融合机制基于信息熵和模态重要性自适应调整特征权重，有效解决多模态情感分析中模态质量差异问题，显著提升情感预测性能。


<details>
  <summary>Details</summary>
Motivation: 多模态情感分析中，简单的融合技术往往无法处理模态质量差异（如噪声、缺失或语义冲突），导致在识别细微情感变化时性能不佳。

Method: AGFN网络采用双门融合机制，基于信息熵和模态重要性自适应调整特征权重，在单模态编码和跨模态交互后优先选择信息量大的模态特征，减少噪声模态的影响。

Result: 在CMU-MOSI和CMU-MOSEI数据集上的实验表明，AGFN在准确率上显著优于强基线模型，能够有效识别细微情感，并具有更强的鲁棒性。可视化分析显示AGFN通过降低特征位置与预测误差的相关性，学习更广泛的特征分布，从而提升泛化能力。

Conclusion: AGFN通过自适应门控融合机制有效解决了多模态情感分析中的模态质量差异问题，创造了更鲁棒的多模态特征表示，在情感识别任务中表现出色。

Abstract: Multimodal sentiment analysis (MSA) leverages information fusion from diverse
modalities (e.g., text, audio, visual) to enhance sentiment prediction.
However, simple fusion techniques often fail to account for variations in
modality quality, such as those that are noisy, missing, or semantically
conflicting. This oversight leads to suboptimal performance, especially in
discerning subtle emotional nuances. To mitigate this limitation, we introduce
a simple yet efficient \textbf{A}daptive \textbf{G}ated \textbf{F}usion
\textbf{N}etwork that adaptively adjusts feature weights via a dual gate fusion
mechanism based on information entropy and modality importance. This mechanism
mitigates the influence of noisy modalities and prioritizes informative cues
following unimodal encoding and cross-modal interaction. Experiments on
CMU-MOSI and CMU-MOSEI show that AGFN significantly outperforms strong
baselines in accuracy, effectively discerning subtle emotions with robust
performance. Visualization analysis of feature representations demonstrates
that AGFN enhances generalization by learning from a broader feature
distribution, achieved by reducing the correlation between feature location and
prediction error, thereby decreasing reliance on specific locations and
creating more robust multimodal feature representations.

</details>


### [93] [Unsupervised Dynamic Feature Selection for Robust Latent Spaces in Vision Tasks](https://arxiv.org/abs/2510.01758)
*Bruno Corcuera,Carlos Eiras-Franco,Brais Cancela*

Main category: cs.LG

TL;DR: 提出了一种无监督动态特征选择（DFS）方法，用于增强视觉任务中的潜在表示，通过去除图像中的误导性或冗余信息来提高模型性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 视觉任务中的潜在表示经常受到噪声或不相关特征的影响，这会降低模型的性能和泛化能力。现有方法通常依赖标注数据，限制了其广泛应用。

Method: 提出无监督动态特征选择（DFS）方法，为每个实例识别并移除图像中的误导性或冗余信息，确保只有最相关的特征贡献到潜在空间。该方法不依赖标注数据，具有广泛适用性。

Result: 在图像数据集上的实验表明，配备无监督DFS的模型在各种任务（包括聚类和图像生成）中实现了显著的泛化性能提升，同时计算成本增加极小。

Conclusion: 无监督DFS方法能有效增强潜在表示，提高模型性能和泛化能力，且具有广泛适用性和计算效率。

Abstract: Latent representations are critical for the performance and robustness of
machine learning models, as they encode the essential features of data in a
compact and informative manner. However, in vision tasks, these representations
are often affected by noisy or irrelevant features, which can degrade the
model's performance and generalization capabilities. This paper presents a
novel approach for enhancing latent representations using unsupervised Dynamic
Feature Selection (DFS). For each instance, the proposed method identifies and
removes misleading or redundant information in images, ensuring that only the
most relevant features contribute to the latent space. By leveraging an
unsupervised framework, our approach avoids reliance on labeled data, making it
broadly applicable across various domains and datasets. Experiments conducted
on image datasets demonstrate that models equipped with unsupervised DFS
achieve significant improvements in generalization performance across various
tasks, including clustering and image generation, while incurring a minimal
increase in the computational cost.

</details>


### [94] [$\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models](https://arxiv.org/abs/2510.01982)
*Yujie Zhou,Pengyang Ling,Jiazi Bu,Yibin Wang,Yuhang Zang,Jiaqi Wang,Li Niu,Guangtao Zhai*

Main category: cs.LG

TL;DR: 提出了一种名为G²RPO的新框架，通过粒度化优势集成和奇异随机采样策略，解决了流模型强化学习中奖励信号稀疏和窄化的问题，实现了更精确全面的采样方向评估。


<details>
  <summary>Details</summary>
Motivation: 现有的在线强化学习方法在探索高价值样本方面有效，但由于稀疏和狭窄的奖励信号，导致偏好对齐效果不佳。

Method: 采用奇异随机采样策略进行逐步随机探索，同时引入多粒度优势集成模块，在多个扩散尺度上聚合优势计算，提供更全面的采样方向评估。

Result: 在各种奖励模型上的实验表明，G²RPO显著优于现有的基于流的GRPO基线方法。

Conclusion: G²RPO框架在流模型的强化学习中表现出卓越的有效性和鲁棒性，为生成模型与人类偏好的对齐提供了更精确的解决方案。

Abstract: The integration of online reinforcement learning (RL) into diffusion and flow
models has recently emerged as a promising approach for aligning generative
models with human preferences. Stochastic sampling via Stochastic Differential
Equations (SDE) is employed during the denoising process to generate diverse
denoising directions for RL exploration. While existing methods effectively
explore potential high-value samples, they suffer from sub-optimal preference
alignment due to sparse and narrow reward signals. To address these challenges,
we propose a novel Granular-GRPO ($\text{G}^2$RPO ) framework that achieves
precise and comprehensive reward assessments of sampling directions in
reinforcement learning of flow models. Specifically, a Singular Stochastic
Sampling strategy is introduced to support step-wise stochastic exploration
while enforcing a high correlation between the reward and the injected noise,
thereby facilitating a faithful reward for each SDE perturbation. Concurrently,
to eliminate the bias inherent in fixed-granularity denoising, we introduce a
Multi-Granularity Advantage Integration module that aggregates advantages
computed at multiple diffusion scales, producing a more comprehensive and
robust evaluation of the sampling directions. Experiments conducted on various
reward models, including both in-domain and out-of-domain evaluations,
demonstrate that our $\text{G}^2$RPO significantly outperforms existing
flow-based GRPO baselines,highlighting its effectiveness and robustness.

</details>


### [95] [Test-Time Anchoring for Discrete Diffusion Posterior Sampling](https://arxiv.org/abs/2510.02291)
*Litu Rout,Andreas Lugmayr,Yasamin Jafarian,Srivatsan Varadharajan,Constantine Caramanis,Sanjay Shakkottai,Ira Kemelmacher-Shlizerman*

Main category: cs.LG

TL;DR: 本文提出了锚定后验采样（APS）方法，用于预训练离散扩散基础模型的后验采样，解决了现有方法在梯度引导、连续松弛和维度诅咒等方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 虽然扩散模型在生成建模方面取得了显著成功，但大多数进展依赖于连续高斯扩散。离散扩散为分类数据（如文本和图像）提供了统一的建模框架，具有更快的推理速度、更精细的控制和无训练贝叶斯推理的优势。然而，现有的离散扩散后验采样方法面临严重挑战。

Method: 提出了锚定后验采样（APS）方法，基于两个关键创新：在离散嵌入空间中使用量化期望进行类梯度引导，以及使用锚定重掩码进行自适应解码。

Result: 该方法在标准基准测试的线性和非线性逆问题上，在离散扩散采样器中达到了最先进的性能。还展示了在无训练风格化和文本引导编辑方面的优势。

Conclusion: APS方法成功克服了现有离散扩散后验采样方法的局限性，为预训练离散扩散基础模型的后验采样提供了一种有效的解决方案。

Abstract: We study the problem of posterior sampling using pretrained discrete
diffusion foundation models, aiming to recover images from noisy measurements
without retraining task-specific models. While diffusion models have achieved
remarkable success in generative modeling, most advances rely on continuous
Gaussian diffusion. In contrast, discrete diffusion offers a unified framework
for jointly modeling categorical data such as text and images. Beyond
unification, discrete diffusion provides faster inference, finer control, and
principled training-free Bayesian inference, making it particularly well-suited
for posterior sampling. However, existing approaches to discrete diffusion
posterior sampling face severe challenges: derivative-free guidance yields
sparse signals, continuous relaxations limit applicability, and split Gibbs
samplers suffer from the curse of dimensionality. To overcome these
limitations, we introduce Anchored Posterior Sampling (APS) for masked
diffusion foundation models, built on two key innovations -- quantized
expectation for gradient-like guidance in discrete embedding space, and
anchored remasking for adaptive decoding. Our approach achieves
state-of-the-art performance among discrete diffusion samplers across linear
and nonlinear inverse problems on the standard benchmarks. We further
demonstrate the benefits of our approach in training-free stylization and
text-guided editing.

</details>


### [96] [Continual Personalization for Diffusion Models](https://arxiv.org/abs/2510.02296)
*Yu-Chien Liao,Jr-Jen Chen,Chi-Pin Huang,Ci-Siang Lin,Meng-Lin Wu,Yu-Chiang Frank Wang*

Main category: cs.LG

TL;DR: CNS（概念神经元选择）是一种用于扩散模型增量学习的新方法，通过识别与目标概念相关的神经元并进行微调，实现多概念个性化同时避免灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在增量设置下更新具有实际应用价值但计算成本高，需要解决灾难性遗忘问题并保持零样本文本到图像生成能力。

Method: CNS独特识别扩散模型中与目标概念相关的神经元，以增量方式微调概念神经元并联合保留先前概念的知识，实现无融合操作。

Result: 在真实数据集上的评估显示，CNS以最少的参数调整实现了最先进的性能，在单概念和多概念个性化任务中均优于先前方法。

Conclusion: CNS是一种简单有效的持续学习策略，能够减少内存存储和处理时间，为扩散模型的持续个性化提供了实用解决方案。

Abstract: Updating diffusion models in an incremental setting would be practical in
real-world applications yet computationally challenging. We present a novel
learning strategy of Concept Neuron Selection (CNS), a simple yet effective
approach to perform personalization in a continual learning scheme. CNS
uniquely identifies neurons in diffusion models that are closely related to the
target concepts. In order to mitigate catastrophic forgetting problems while
preserving zero-shot text-to-image generation ability, CNS finetunes concept
neurons in an incremental manner and jointly preserves knowledge learned of
previous concepts. Evaluation of real-world datasets demonstrates that CNS
achieves state-of-the-art performance with minimal parameter adjustments,
outperforming previous methods in both single and multi-concept personalization
works. CNS also achieves fusion-free operation, reducing memory storage and
processing time for continual personalization.

</details>


### [97] [Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models](https://arxiv.org/abs/2510.02300)
*Runqian Wang,Yilun Du*

Main category: cs.LG

TL;DR: EqM是一种基于平衡动力学的生成建模框架，通过学习隐式能量景观的平衡梯度，取代传统扩散/流模型中的非平衡时间条件动态，实现优化驱动的推理。


<details>
  <summary>Details</summary>
Motivation: 传统扩散和流模型依赖非平衡的时间条件动态，EqM旨在通过平衡动力学视角简化生成过程，提供更灵活和高效的生成框架。

Method: EqM学习隐式能量景观的平衡梯度，在推理时采用基于优化的采样过程，通过梯度下降在学习的景观上生成样本，支持可调步长、自适应优化器和自适应计算。

Result: EqM在ImageNet 256×256上实现了1.90的FID，超越了传统扩散/流模型的生成性能，并能自然处理部分噪声图像去噪、OOD检测和图像合成等任务。

Conclusion: EqM通过统一平衡景观取代时间条件速度，为流模型和能量基模型提供了更紧密的桥梁，简化了优化驱动的推理过程。

Abstract: We introduce Equilibrium Matching (EqM), a generative modeling framework
built from an equilibrium dynamics perspective. EqM discards the
non-equilibrium, time-conditional dynamics in traditional diffusion and
flow-based generative models and instead learns the equilibrium gradient of an
implicit energy landscape. Through this approach, we can adopt an
optimization-based sampling process at inference time, where samples are
obtained by gradient descent on the learned landscape with adjustable step
sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation
performance of diffusion/flow models empirically, achieving an FID of 1.90 on
ImageNet 256$\times$256. EqM is also theoretically justified to learn and
sample from the data manifold. Beyond generation, EqM is a flexible framework
that naturally handles tasks including partially noised image denoising, OOD
detection, and image composition. By replacing time-conditional velocities with
a unified equilibrium landscape, EqM offers a tighter bridge between flow and
energy-based models and a simple route to optimization-driven inference.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [98] [On the Role of Domain Experts in Creating Effective Tutoring Systems](https://arxiv.org/abs/2510.01432)
*Sarath Sreedharan,Kelsey Sikes,Nathaniel Blanchard,Lisa Mason,Nikhil Krishnaswamy,Jill Zarestky*

Main category: cs.AI

TL;DR: 本文探讨了领域专家提供的精心策划知识在创建有效辅导系统中的作用，提出了两种利用专家知识开发新型教育系统的方法。


<details>
  <summary>Details</summary>
Motivation: AI教育社区经常忽视领域专家提供的精心策划知识在创建有效辅导系统中的重要作用，本文旨在强调这一主题的重要性。

Method: 1. 使用可解释AI技术结合专家指定的问题解决规则自动生成课程；2. 利用专家指定的学习课程开发自适应辅导系统。

Result: 通过传粉者识别辅导系统的案例研究，证明了这些方法能够提供更好的学习体验，并使用更高效的算法创建系统。

Conclusion: 精心策划的专家知识对于开发有效的AI教育系统具有重要价值，特别是在创建自动课程生成和自适应辅导系统方面。

Abstract: The role that highly curated knowledge, provided by domain experts, could
play in creating effective tutoring systems is often overlooked within the AI
for education community. In this paper, we highlight this topic by discussing
two ways such highly curated expert knowledge could help in creating novel
educational systems. First, we will look at how one could use explainable AI
(XAI) techniques to automatically create lessons. Most existing XAI methods are
primarily aimed at debugging AI systems. However, we will discuss how one could
use expert specified rules about solving specific problems along with novel XAI
techniques to automatically generate lessons that could be provided to
learners. Secondly, we will see how an expert specified curriculum for learning
a target concept can help develop adaptive tutoring systems, that can not only
provide a better learning experience, but could also allow us to use more
efficient algorithms to create these systems. Finally, we will highlight the
importance of such methods using a case study of creating a tutoring system for
pollinator identification, where such knowledge could easily be elicited from
experts.

</details>


### [99] [VaPR -- Vision-language Preference alignment for Reasoning](https://arxiv.org/abs/2510.01700)
*Rohan Wadhawan,Fabrice Y Harel-Canada,Zi-Yi Dou,Suhaila Shakiah,Robinson Piramuthu,Nanyun Peng*

Main category: cs.AI

TL;DR: 提出了VaPR框架，通过LLM引导的硬负样本生成来解决合成偏好数据中的风格和长度偏差问题，显著提升了LVLM在多任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有偏好微调方法忽视了合成偏好标注中普遍存在的风格和长度偏差噪声，影响了模型对齐效果。

Method: 基于LLM引导的响应编辑框架生成硬负样本，创建VaPR数据集（30K样本），对LLaVA、Qwen2VL等LVLM进行微调。

Result: 在10个基准测试上平均提升6.5%（LLaVA）、4.0%（Qwen2VL）、1.5%（Qwen2.5VL），推理任务表现显著改善，并减少了二元问题中的"是"倾向。

Conclusion: VaPR框架有效解决了合成偏好数据的偏差问题，具有良好的可扩展性和泛化性，开源LLM编辑器也能达到接近GPT-4o的性能。

Abstract: Preference finetuning methods like Direct Preference Optimization (DPO) with
AI-generated feedback have shown promise in aligning Large Vision-Language
Models (LVLMs) with human preferences. However, existing techniques overlook
the prevalence of noise in synthetic preference annotations in the form of
stylistic and length biases. To this end, we introduce a hard-negative response
generation framework based on LLM-guided response editing, that produces
rejected responses with targeted errors, maintaining stylistic and length
similarity to the accepted ones. Using this framework, we develop the VaPR
dataset, comprising 30K high-quality samples, to finetune three LVLM families:
LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver
significant performance improvements across ten benchmarks, achieving average
gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable
improvements on reasoning tasks. A scaling analysis shows that performance
consistently improves with data size, with LLaVA models benefiting even at
smaller scales. Moreover, VaPR reduces the tendency to answer "Yes" in binary
questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we
show that the framework generalizes to open-source LLMs as editors, with models
trained on VaPR-OS achieving ~99% of the performance of models trained on
\name, which is synthesized using GPT-4o. Our data, models, and code can be
found on the project page https://vap-r.github.io

</details>


### [100] [The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models](https://arxiv.org/abs/2510.02230)
*Phuc Minh Nguyen,Chinh D. La,Duy M. H. Nguyen,Nitesh V. Chawla,Binh T. Nguyen,Khoa D. Doan*

Main category: cs.AI

TL;DR: RLVR方法虽然旨在提升大语言模型的推理能力，但反而会缩小推理边界。本文揭示了RLVR中的负干扰和赢家通吃现象，并提出了一种针对低概率问题的数据筛选算法来改善Pass@k性能。


<details>
  <summary>Details</summary>
Motivation: 研究RLVR方法在提升大语言模型推理能力时出现的推理边界缩小问题，探究其失败原因并提出改进方案。

Method: 通过理论分析和实证研究，揭示了RLVR中的负干扰现象和赢家通吃现象，并提出了专注于低概率问题的数据筛选算法。

Result: 研究表明RLVR会抑制初始低概率问题的解决能力，导致Pass@k性能下降。提出的数据筛选算法显著改善了Pass@k性能。

Conclusion: RLVR存在固有缺陷，但通过针对性的数据筛选策略可以有效缓解推理边界缩小问题，提升模型推理能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key
method for improving Large Language Models' reasoning capabilities, yet recent
evidence suggests it may paradoxically shrink the reasoning boundary rather
than expand it. This paper investigates the shrinkage issue of RLVR by
analyzing its learning dynamics and reveals two critical phenomena that explain
this failure. First, we expose negative interference in RLVR, where learning to
solve certain training problems actively reduces the likelihood of correct
solutions for others, leading to the decline of Pass@$k$ performance, or the
probability of generating a correct solution within $k$ attempts. Second, we
uncover the winner-take-all phenomenon: RLVR disproportionately reinforces
problems with high likelihood, correct solutions, under the base model, while
suppressing other initially low-likelihood ones. Through extensive theoretical
and empirical analysis on multiple mathematical reasoning benchmarks, we show
that this effect arises from the inherent on-policy sampling in standard RL
objectives, causing the model to converge toward narrow solution strategies.
Based on these insights, we propose a simple yet effective data curation
algorithm that focuses RLVR learning on low-likelihood problems, achieving
notable improvement in Pass@$k$ performance. Our code is available at
https://github.com/mail-research/SELF-llm-interference.

</details>


### [101] [The Unreasonable Effectiveness of Scaling Agents for Computer Use](https://arxiv.org/abs/2510.02250)
*Gonzalo Gonzalez-Pumariega,Vincent Tu,Chih-Lun Lee,Jiachen Yang,Ang Li,Xin Eric Wang*

Main category: cs.AI

TL;DR: Behavior Best-of-N (bBoN)方法通过生成多个智能体轨迹并使用行为叙事进行选择，显著提高了计算机使用代理在复杂任务上的成功率，在OSWorld上达到69.9%的新SOTA，接近人类水平的72%。


<details>
  <summary>Details</summary>
Motivation: 计算机使用代理在自动化日常数字任务方面具有潜力，但其不可靠性和高方差阻碍了它们在长视野复杂任务中的应用。

Method: bBoN方法通过生成多个智能体轨迹（rollouts），并使用描述智能体轨迹的行为叙事来进行轨迹选择，实现广泛的探索和原则性的轨迹选择。

Result: 在OSWorld上达到69.9%的新SOTA，显著优于先前方法，接近人类水平的72%。在WindowsAgentArena和AndroidWorld上也展示了强大的泛化能力。

Conclusion: 有效扩展计算机使用代理需要结构化的轨迹理解和选择，bBoN提供了一个实用的框架来实现这一目标。

Abstract: Computer-use agents (CUAs) hold promise for automating everyday digital
tasks, but their unreliability and high variance hinder their application to
long-horizon, complex tasks. We introduce Behavior Best-of-N (bBoN), a method
that scales over agents by generating multiple rollouts and selecting among
them using behavior narratives that describe the agents' rollouts. It enables
both wide exploration and principled trajectory selection, substantially
improving robustness and success rates. On OSWorld, our bBoN scaling method
establishes a new state of the art (SoTA) at 69.9%, significantly outperforming
prior methods and approaching human-level performance at 72%, with
comprehensive ablations validating key design choices. We further demonstrate
strong generalization results to different operating systems on
WindowsAgentArena and AndroidWorld. Crucially, our results highlight the
unreasonable effectiveness of scaling CUAs, when you do it right: effective
scaling requires structured trajectory understanding and selection, and bBoN
provides a practical framework to achieve this.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [102] [Model Merging to Maintain Language-Only Performance in Developmentally Plausible Multimodal Models](https://arxiv.org/abs/2510.01845)
*Ece Takmaz,Lisa Bylinina,Jakub Dotlacil*

Main category: cs.CL

TL;DR: 本文针对多模态模型在语言任务中表现不佳的问题，提出通过模型融合方法将多模态模型与纯语言模型结合，以在保持多模态性能的同时提升语言能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型参数量大且依赖海量数据，与儿童语言习得的数据量存在差距。多模态模型在纯语言任务中表现不佳，需要解决这一性能下降问题。

Method: 使用发育合理的数据集构建低资源环境下的语言模型和多模态模型，并通过加权线性插值进行模型融合，将多模态模型与纯语言模型的参数结合。

Result: 多模态模型在语法相关的语言基准测试中表现较差，模型融合方法能在一定程度上缓解这一问题，同时保持多模态性能。

Conclusion: 模型融合是解决多模态模型语言能力不足的有效方法，但仅能部分解决问题，需要进一步研究来平衡多模态和语言性能。

Abstract: State-of-the-art vision-and-language models consist of many parameters and
learn from enormous datasets, surpassing the amounts of linguistic data that
children are exposed to as they acquire a language. This paper presents our
approach to the multimodal track of the BabyLM challenge addressing this
discrepancy. We develop language-only and multimodal models in low-resource
settings using developmentally plausible datasets, with our multimodal models
outperforming previous BabyLM baselines. One finding in the multimodal language
model literature is that these models tend to underperform in
\textit{language-only} tasks. Therefore, we focus on maintaining language-only
abilities in multimodal models. To this end, we experiment with \textit{model
merging}, where we fuse the parameters of multimodal models with those of
language-only models using weighted linear interpolation. Our results
corroborate the findings that multimodal models underperform in language-only
benchmarks that focus on grammar, and model merging with text-only models can
help alleviate this problem to some extent, while maintaining multimodal
performance.

</details>


### [103] [From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens](https://arxiv.org/abs/2510.02292)
*Hala Sheta,Eric Huang,Shuyu Wu,Ilia Alenabi,Jiajun Hong,Ryker Lin,Ruoxi Ning,Daniel Wei,Jialin Yang,Jiawei Zhou,Ziqiao Ma,Freda Shi*

Main category: cs.CL

TL;DR: VLM-Lens是一个用于系统化基准测试、分析和解释视觉语言模型(VLMs)的工具包，支持从开源VLMs的前向传播过程中提取任意层的中间输出。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型分析工具缺乏统一的接口，难以跨不同模型进行系统化比较和分析。

Method: 提供基于YAML配置的统一接口，抽象模型特定复杂性，支持16种最先进的基座VLM及其30多个变体，核心逻辑可扩展新模型。

Result: 通过两个简单分析实验，揭示了不同VLM在隐藏表示上的系统性差异，验证了工具的有效性。

Conclusion: VLM-Lens作为开源项目发布，旨在加速社区对VLM的理解和改进工作。

Abstract: We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking,
analysis, and interpretation of vision-language models (VLMs) by supporting the
extraction of intermediate outputs from any layer during the forward pass of
open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that
abstracts away model-specific complexities and supports user-friendly operation
across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and
their over 30 variants, and is extensible to accommodate new models without
changing the core logic.
  The toolkit integrates easily with various interpretability and analysis
methods. We demonstrate its usage with two simple analytical experiments,
revealing systematic differences in the hidden representations of VLMs across
layers and target concepts. VLM-Lens is released as an open-sourced project to
accelerate community efforts in understanding and improving VLMs.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [104] [Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation](https://arxiv.org/abs/2510.01284)
*Chetwin Low,Weimin Wang,Calder Katyal*

Main category: cs.MM

TL;DR: Ovi是一个统一的音频-视频生成范式，通过双模态DiT模块的块状跨模态融合，实现自然同步，无需复杂的多阶段架构或后处理对齐。


<details>
  <summary>Details</summary>
Motivation: 传统的音频-视频生成方法依赖复杂的多阶段架构或声音和视觉的顺序合成，Ovi旨在通过单一生成过程统一建模两种模态，简化流程并提高同步质量。

Method: 使用块状跨模态融合的双DiT模块，通过初始化与预训练视频模型架构相同的音频塔，并在大量原始音频数据上训练，实现细粒度多模态融合。融合通过联合训练视频和音频塔，利用缩放RoPE嵌入进行时间交换和双向跨注意力进行语义交换。

Result: Ovi能够生成具有自然语音和准确、上下文匹配音效的电影级视频片段，支持富有表现力的电影叙事。

Conclusion: Ovi提供了一个统一的音频-视频生成框架，通过跨模态融合实现了高质量的同步生成，简化了传统多阶段方法，并展示了在电影级内容生成方面的潜力。

Abstract: Audio-video generation has often relied on complex multi-stage architectures
or sequential synthesis of sound and visuals. We introduce Ovi, a unified
paradigm for audio-video generation that models the two modalities as a single
generative process. By using blockwise cross-modal fusion of twin-DiT modules,
Ovi achieves natural synchronization and removes the need for separate
pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion
modeling, we initialize an audio tower with an architecture identical to that
of a strong pretrained video model. Trained from scratch on hundreds of
thousands of hours of raw audio, the audio tower learns to generate realistic
sound effects, as well as speech that conveys rich speaker identity and
emotion. Fusion is obtained by jointly training the identical video and audio
towers via blockwise exchange of timing (via scaled-RoPE embeddings) and
semantics (through bidirectional cross-attention) on a vast video corpus. Our
model enables cinematic storytelling with natural speech and accurate,
context-matched sound effects, producing movie-grade video clips. All the
demos, code and model weights are published at https://aaxwaz.github.io/Ovi

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [105] [ZK-WAGON: Imperceptible Watermark for Image Generation Models using ZK-SNARKs](https://arxiv.org/abs/2510.01967)
*Aadarsh Anantha Ramakrishnan,Shubham Agarwal,Selvanayagam S,Kunwar Singh*

Main category: cs.CR

TL;DR: ZK-WAGON是一种基于ZK-SNARKs的新型图像生成模型水印系统，通过选择性层电路转换和LSB隐写技术实现安全、可验证的来源证明。


<details>
  <summary>Details</summary>
Motivation: 随着图像生成模型能力的增强，合成媒体的真实性、所有权和滥用问题日益严重。传统水印方法存在图像质量下降、易移除或需要模型内部信息等问题，无法满足安全可扩展部署的需求。

Method: 提出ZK-WAGON系统，使用ZK-SNARKs技术实现来源验证而不暴露模型权重或生成提示。采用选择性层ZK电路创建(SL-ZKCC)方法将关键层转换为电路，显著减少证明生成时间。通过LSB隐写技术将ZK-SNARK证明嵌入生成图像。

Result: 该系统在GAN和扩散模型上得到验证，提供了一个安全、模型无关的可信AI图像生成流水线。

Conclusion: ZK-WAGON为解决合成媒体认证问题提供了创新的解决方案，实现了不损害图像质量且难以移除的安全水印机制。

Abstract: As image generation models grow increasingly powerful and accessible,
concerns around authenticity, ownership, and misuse of synthetic media have
become critical. The ability to generate lifelike images indistinguishable from
real ones introduces risks such as misinformation, deepfakes, and intellectual
property violations. Traditional watermarking methods either degrade image
quality, are easily removed, or require access to confidential model internals
- making them unsuitable for secure and scalable deployment. We are the first
to introduce ZK-WAGON, a novel system for watermarking image generation models
using the Zero-Knowledge Succinct Non Interactive Argument of Knowledge
(ZK-SNARKs). Our approach enables verifiable proof of origin without exposing
model weights, generation prompts, or any sensitive internal information. We
propose Selective Layer ZK-Circuit Creation (SL-ZKCC), a method to selectively
convert key layers of an image generation model into a circuit, reducing proof
generation time significantly. Generated ZK-SNARK proofs are imperceptibly
embedded into a generated image via Least Significant Bit (LSB) steganography.
We demonstrate this system on both GAN and Diffusion models, providing a
secure, model-agnostic pipeline for trustworthy AI image generation.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [106] [MorphGen: Controllable and Morphologically Plausible Generative Cell-Imaging](https://arxiv.org/abs/2510.01298)
*Berker Demirel,Marco Fumero,Theofanis Karaletsos,Francesco Locatello*

Main category: q-bio.QM

TL;DR: MorphGen是一个基于扩散模型的荧光显微镜图像生成模型，能够跨多种细胞类型和扰动进行可控生成，通过匹配OpenPhenom的表型嵌入来保持生物一致性，并联合生成完整的荧光通道以保留细胞器细节。


<details>
  <summary>Details</summary>
Motivation: 加速基于高内涵图像的药物发现和基因编辑实验，通过计算机模拟细胞对干预的响应，需要能够生成生物一致且保留细胞器细节的显微镜图像。

Method: 使用扩散模型框架，结合对齐损失将模型表示与OpenPhenom生物基础模型的表型嵌入匹配，联合生成完整的荧光通道而非压缩为RGB图像。

Result: MorphGen在FID分数上比之前的SOTA模型MorphoDiff降低了35%以上，能够生成生物一致的真实图像，并通过CellProfiler特征验证了生物一致性。

Conclusion: MorphGen在保持生物一致性和细胞器细节方面优于现有方法，为药物发现和基因编辑研究提供了更可靠的图像生成工具。

Abstract: Simulating in silico cellular responses to interventions is a promising
direction to accelerate high-content image-based assays, critical for advancing
drug discovery and gene editing. To support this, we introduce MorphGen, a
state-of-the-art diffusion-based generative model for fluorescent microscopy
that enables controllable generation across multiple cell types and
perturbations. To capture biologically meaningful patterns consistent with
known cellular morphologies, MorphGen is trained with an alignment loss to
match its representations to the phenotypic embeddings of OpenPhenom, a
state-of-the-art biological foundation model. Unlike prior approaches that
compress multichannel stains into RGB images -- thus sacrificing
organelle-specific detail -- MorphGen generates the complete set of fluorescent
channels jointly, preserving per-organelle structures and enabling a
fine-grained morphological analysis that is essential for biological
interpretation. We demonstrate biological consistency with real images via
CellProfiler features, and MorphGen attains an FID score over $35\%$ lower than
the prior state-of-the-art MorphoDiff, which only generates RGB images for a
single cell type. Code is available at https://github.com/czi-ai/MorphGen.

</details>


### [107] [A Multicentric Dataset for Training and Benchmarking Breast Cancer Segmentation in H&E Slides](https://arxiv.org/abs/2510.02037)
*Carlijn Lems,Leslie Tessier,John-Melle Bokhorst,Mart van Rijthoven,Witali Aswolinskiy,Matteo Pozzi,Natalie Klubickova,Suzanne Dintzis,Michela Campora,Maschenka Balkenhol,Peter Bult,Joey Spronck,Thomas Detone,Mattia Barbareschi,Enrico Munari,Giuseppe Bogina,Jelle Wesseling,Esther H. Lips,Francesco Ciompi,Frédérique Meeuwsen,Jeroen van der Laak*

Main category: q-bio.QM

TL;DR: BEETLE数据集是一个用于乳腺癌H&E染色全切片图像多类语义分割的新数据集，旨在解决现有数据集形态多样性不足的问题，支持大规模AI生物标志物分析。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺癌分割数据集缺乏形态多样性，无法支持模型泛化性和跨异质患者群体的稳健生物标志物验证。

Method: 收集587个活检和切除样本，来自三个临床中心和两个公共数据集，使用七种扫描仪数字化，涵盖所有分子亚型和组织学分级，采用多样化标注策略对四个类别进行标注。

Result: 创建了一个包含多样化形态特征（如导管原位癌和分散性小叶肿瘤细胞）的高质量多中心数据集，并提供了标准化的外部评估集。

Conclusion: BEETLE数据集的高多样性和与自动化生物标志物量化领域的相关性确保了其高重用潜力，为乳腺癌分割模型提供了标准化基准测试平台。

Abstract: Automated semantic segmentation of whole-slide images (WSIs) stained with
hematoxylin and eosin (H&E) is essential for large-scale artificial
intelligence-based biomarker analysis in breast cancer. However, existing
public datasets for breast cancer segmentation lack the morphological diversity
needed to support model generalizability and robust biomarker validation across
heterogeneous patient cohorts. We introduce BrEast cancEr hisTopathoLogy
sEgmentation (BEETLE), a dataset for multiclass semantic segmentation of
H&E-stained breast cancer WSIs. It consists of 587 biopsies and resections from
three collaborating clinical centers and two public datasets, digitized using
seven scanners, and covers all molecular subtypes and histological grades.
Using diverse annotation strategies, we collected annotations across four
classes - invasive epithelium, non-invasive epithelium, necrosis, and other -
with particular focus on morphologies underrepresented in existing datasets,
such as ductal carcinoma in situ and dispersed lobular tumor cells. The
dataset's diversity and relevance to the rapidly growing field of automated
biomarker quantification in breast cancer ensure its high potential for reuse.
Finally, we provide a well-curated, multicentric external evaluation set to
enable standardized benchmarking of breast cancer segmentation models.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [108] [Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning](https://arxiv.org/abs/2510.01502)
*Kathy Garcia,Leyla Isik*

Main category: q-bio.NC

TL;DR: 研究发现预训练视频模型在社交感知方面存在差距，语言模型比视频模型更接近人类对社交视频相似性的判断。通过基于人类行为数据的微调，可以显著提升视频模型与人类社交感知的对齐程度。


<details>
  <summary>Details</summary>
Motivation: 人类能够直观感知视觉场景中的复杂社交信号，但现有AI模型是否编码了相同的相似性结构尚不清楚。研究旨在探索现代视频和语言模型是否能捕捉人类对社交视频的相似性感知，以及如何通过人类行为数据将这种结构注入模型。

Method: 构建包含49,000多个三元组相似性判断的新基准数据集，使用250个3秒社交互动视频片段。提出基于低秩适应的混合三元组-RSA目标函数，对TimeSformer视频模型进行微调，使其成对距离与人类相似性对齐。

Result: 微调后的视频模型在保留视频上显著提高了与人类感知的对齐度（解释方差和三元组准确率）。方差分解显示微调增加了与语言嵌入的共享方差，并解释了语言模型未捕捉的独特方差。线性探测表明人类相似性微调增强了社交情感属性的编码。

Conclusion: 预训练视频模型在社交识别方面存在差距，行为引导的微调能够将视频表征塑造成更接近人类社交感知的形式。

Abstract: Humans intuitively perceive complex social signals in visual scenes, yet it
remains unclear whether state-of-the-art AI models encode the same similarity
structure. We study (Q1) whether modern video and language models capture
human-perceived similarity in social videos, and (Q2) how to instill this
structure into models using human behavioral data. To address this, we
introduce a new benchmark of over 49,000 odd-one-out similarity judgments on
250 three-second video clips of social interactions, and discover a modality
gap: despite the task being visual, caption-based language embeddings align
better with human similarity than any pretrained video model. We close this gap
by fine-tuning a TimeSformer video model on these human judgments with our
novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning
pairwise distances to human similarity. This fine-tuning protocol yields
significantly improved alignment with human perceptions on held-out videos in
terms of both explained variance and odd-one-out triplet accuracy. Variance
partitioning shows that the fine-tuned video model increases shared variance
with language embeddings and explains additional unique variance not captured
by the language model. Finally, we test transfer via linear probes and find
that human-similarity fine-tuning strengthens the encoding of social-affective
attributes (intimacy, valence, dominance, communication) relative to the
pretrained baseline. Overall, our findings highlight a gap in pretrained video
models' social recognition and demonstrate that behavior-guided fine-tuning
shapes video representations toward human social perception.

</details>


### [109] [Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion](https://arxiv.org/abs/2510.02182)
*Yule Wang,Joseph Yu,Chengrui Li,Weihan Li,Anqi Wu*

Main category: q-bio.NC

TL;DR: MIG-Vis方法利用扩散模型可视化并验证神经潜在子空间中的视觉语义属性，揭示了高级视觉皮层中结构化的语义表征


<details>
  <summary>Details</summary>
Motivation: 解决神经群体如何编码物体中心视觉信息以及特征特异性信息如何在高级视觉区域神经群体中分布和组织的问题

Method: 使用变分自编码器推断神经群体的组间解耦神经潜在子空间，然后提出互信息引导的扩散合成过程可视化每个潜在组编码的视觉语义特征

Result: 在两只猕猴的颞下皮层多会话神经发放数据集上验证，结果显示方法识别出具有明确语义选择性的神经潜在组，包括物体姿态、类别间转换和类内内容

Conclusion: 研究提供了高级视觉皮层中结构化语义表征的直接、可解释证据，推进了对编码原理的理解

Abstract: Understanding how neural populations in higher visual areas encode
object-centered visual information remains a central challenge in computational
neuroscience. Prior works have investigated representational alignment between
artificial neural networks and the visual cortex. Nevertheless, these findings
are indirect and offer limited insights to the structure of neural populations
themselves. Similarly, decoding-based methods have quantified semantic features
from neural populations but have not uncovered their underlying organizations.
This leaves open a scientific question: "how feature-specific visual
information is distributed across neural populations in higher visual areas,
and whether it is organized into structured, semantically meaningful
subspaces." To tackle this problem, we present MIG-Vis, a method that leverages
the generative power of diffusion models to visualize and validate the
visual-semantic attributes encoded in neural latent subspaces. Our method first
uses a variational autoencoder to infer a group-wise disentangled neural latent
subspace from neural populations. Subsequently, we propose a mutual information
(MI)-guided diffusion synthesis procedure to visualize the specific
visual-semantic features encoded by each latent group. We validate MIG-Vis on
multi-session neural spiking datasets from the inferior temporal (IT) cortex of
two macaques. The synthesized results demonstrate that our method identifies
neural latent groups with clear semantic selectivity to diverse visual
features, including object pose, inter-category transformations, and
intra-class content. These findings provide direct, interpretable evidence of
structured semantic representation in the higher visual cortex and advance our
understanding of its encoding principles.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [110] [MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics](https://arxiv.org/abs/2510.01619)
*Changmin Lee,Jihyun Lee,Tae-Kyun Kim*

Main category: cs.GR

TL;DR: MPMAvatar是一个基于材料点方法的3D人体化身框架，能够从多视角视频创建支持高度真实动态和渲染的虚拟化身，在动态建模精度、渲染质量和鲁棒性方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于物理模拟的3D人体化身建模方法在处理宽松服装的动态时存在精度不足和鲁棒性差的问题，特别是在处理新颖动画输入时表现不佳。

Method: 采用材料点方法模拟器，结合各向异性本构模型和新型碰撞处理算法来精确建模服装的复杂变形和与身体的接触；同时使用3D高斯泼溅技术和准阴影渲染技术进行高质量渲染。

Result: MPMAvatar在动态建模精度、渲染精度、鲁棒性和效率方面显著优于现有最先进的物理化身方法，并能以零样本方式泛化到未见过的交互场景。

Conclusion: 该框架成功解决了宽松服装动态建模的挑战，为创建物理上合理且视觉逼真的3D人体化身提供了有效解决方案，在泛化能力方面超越了传统学习方法。

Abstract: While there has been significant progress in the field of 3D avatar creation
from visual observations, modeling physically plausible dynamics of humans with
loose garments remains a challenging problem. Although a few existing works
address this problem by leveraging physical simulation, they suffer from
limited accuracy or robustness to novel animation inputs. In this work, we
present MPMAvatar, a framework for creating 3D human avatars from multi-view
videos that supports highly realistic, robust animation, as well as
photorealistic rendering from free viewpoints. For accurate and robust dynamics
modeling, our key idea is to use a Material Point Method-based simulator, which
we carefully tailor to model garments with complex deformations and contact
with the underlying body by incorporating an anisotropic constitutive model and
a novel collision handling algorithm. We combine this dynamics modeling scheme
with our canonical avatar that can be rendered using 3D Gaussian Splatting with
quasi-shadowing, enabling high-fidelity rendering for physically realistic
animations. In our experiments, we demonstrate that MPMAvatar significantly
outperforms the existing state-of-the-art physics-based avatar in terms of (1)
dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and
efficiency. Additionally, we present a novel application in which our avatar
generalizes to unseen interactions in a zero-shot manner-which was not
achievable with previous learning-based methods due to their limited simulation
generalizability. Our project page is at:
https://KAISTChangmin.github.io/MPMAvatar/

</details>


### [111] [ROI-GS: Interest-based Local Quality 3D Gaussian Splatting](https://arxiv.org/abs/2510.01978)
*Quoc-Anh Bui,Gilles Rougeron,Géraldine Morin,Simone Gasparini*

Main category: cs.GR

TL;DR: ROI-GS是一个面向感兴趣区域的高效3D高斯重建框架，通过对象感知的资源分配策略，在保持实时性能的同时显著提升局部细节质量并减小模型规模


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯重建方法在场景中均匀分配资源，导致对感兴趣对象的细节重建不足且模型规模过大

Method: 提出对象引导的相机选择、针对性对象训练以及高保真对象重建与全局场景的无缝集成

Result: 局部质量提升高达2.96 dB PSNR，模型规模减少约17%，单对象场景训练速度更快

Conclusion: ROI-GS在提升感兴趣对象重建质量的同时有效控制了模型复杂度，优于现有方法

Abstract: We tackle the challenge of efficiently reconstructing 3D scenes with high
detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods
allocate resources uniformly across the scene, limiting fine detail to Regions
Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an
object-aware framework that enhances local details through object-guided camera
selection, targeted Object training, and seamless integration of high-fidelity
object of interest reconstructions into the global scene. Our method
prioritizes higher resolution details on chosen objects while maintaining
real-time performance. Experiments show that ROI-GS significantly improves
local quality (up to 2.96 dB PSNR), while reducing overall model size by
$\approx 17\%$ of baseline and achieving faster training for a scene with a
single object of interest, outperforming existing methods.

</details>


### [112] [Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects](https://arxiv.org/abs/2510.02069)
*Georgios Kouros,Minye Wu,Tinne Tuytelaars*

Main category: cs.GR

TL;DR: 提出了一种基于2D高斯泼溅和微表面BRDF的可重光照框架，能够更准确地重建和重光照光泽物体，实现高质量的几何和材质重建。


<details>
  <summary>Details</summary>
Motivation: 现有神经渲染方法通常依赖简化的BRDF模型或耦合漫反射和镜面反射的参数化方法，限制了材质恢复的准确性和重光照的保真度。光泽物体的形状、材质属性和光照难以分离是一个长期挑战。

Method: 将微表面BRDF与镜面光泽参数化集成到2D高斯泼溅中，采用延迟着色。利用基于扩散的表面法线和漫反射颜色先验指导早期优化，通过从粗到细的环境贴图优化加速收敛并保持高动态范围镜面反射。

Result: 在复杂光泽场景上的广泛实验表明，该方法实现了高质量的几何和材质重建，与现有高斯泼溅方法相比，在新颖光照下提供了更真实和一致的重光照效果。

Conclusion: 该方法通过物理一致的材质分解和有效的优化策略，成功解决了光泽物体重建和重光照的挑战，显著提升了重光照的真实感和一致性。

Abstract: Accurate reconstruction and relighting of glossy objects remain a
longstanding challenge, as object shape, material properties, and illumination
are inherently difficult to disentangle. Existing neural rendering approaches
often rely on simplified BRDF models or parameterizations that couple diffuse
and specular components, which restricts faithful material recovery and limits
relighting fidelity. We propose a relightable framework that integrates a
microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian
Splatting with deferred shading. This formulation enables more physically
consistent material decomposition, while diffusion-based priors for surface
normals and diffuse color guide early-stage optimization and mitigate
ambiguity. A coarse-to-fine optimization of the environment map accelerates
convergence and preserves high-dynamic-range specular reflections. Extensive
experiments on complex, glossy scenes demonstrate that our method achieves
high-quality geometry and material reconstruction, delivering substantially
more realistic and consistent relighting under novel illumination compared to
existing Gaussian splatting methods.

</details>
