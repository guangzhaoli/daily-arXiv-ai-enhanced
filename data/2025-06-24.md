<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 175]
- [cs.DB](#cs.DB) [Total: 1]
- [cs.LG](#cs.LG) [Total: 9]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.CY](#cs.CY) [Total: 3]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.SD](#cs.SD) [Total: 1]
- [cs.CL](#cs.CL) [Total: 1]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [cs.IR](#cs.IR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [cs.DL](#cs.DL) [Total: 1]
- [cs.AI](#cs.AI) [Total: 2]
- [cs.GR](#cs.GR) [Total: 8]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: 该论文通过定量电路级分析揭示了扩散模型在图像生成过程中的计算路径和机制原理，发现合成数据与自然数据处理的算法差异，并识别了八种功能不同的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在图像生成中的计算机制，以理解其行为并为生成模型的控制提供定量基础。

Method: 通过系统干预实验，分析了2,000张合成图像和2,000张CelebA面部图像，测量计算复杂性和注意力模式。

Result: 发现真实面部处理需要更高计算复杂度（1.084±0.008），并识别了八种功能不同的注意力机制（如边缘检测、纹理分析等）。干预实验显示性能下降25.6%至128.3%。

Conclusion: 研究为生成模型行为的算法理解和控制提供了定量基础，支持通过机制干预策略优化模型性能。

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [2] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/abs/2506.17290)
*Yuqi Li,Junhao Dong,Zeyu Dong,Chuanguang Yang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出了一种名为SRKD的知识蒸馏框架，将大型教师模型的知识迁移到轻量级学生模型，提升点云分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分割中大规模Transformer模型的计算复杂性和部署限制问题。

Method: 通过亲和矩阵关系对齐模块和跨样本小批量构建策略，结合KL散度对齐语义分布和真实监督。

Result: 在显著降低模型复杂度的同时，实现了最先进的性能。

Conclusion: SRKD框架在现实部署场景中表现出高效性和有效性。

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [3] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: MISO是一种基于视觉的机器学习模型，用于生成阿拉斯加高分辨率土壤地图，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 阿拉斯加细粒度土壤地图对研究永久冻土分布和气候变化影响至关重要，但传统方法效果有限。

Method: MISO结合地理空间基础模型、隐式神经表示和对比学习，进行连续空间预测和多模态对齐。

Result: MISO在空间交叉验证中表现优于随机森林，泛化能力更强，召回率更高。

Conclusion: MISO展示了先进机器学习方法在土壤地图绘制中的潜力，为未来采样和规划提供指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [4] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: 提出了一种基于时间感知的计算机视觉框架，用于预测非订阅制零工平台的用户流失，通过雷达图序列建模用户行为，显著提升了性能指标。


<details>
  <summary>Details</summary>
Motivation: 非订阅制零工平台的用户流失预测因缺乏显式标签和动态行为而具有挑战性，现有方法难以捕捉关键的时间线索。

Method: 将用户行为模式建模为雷达图图像序列，结合预训练的CNN编码器和双向LSTM，捕捉空间和时间模式。

Result: 在真实数据集上，F1分数提升17.7，精确度提升29.4，AUC提升16.1，同时提高了可解释性。

Conclusion: 该框架的模块化设计、可解释性工具和高效部署特性，使其适用于动态零工经济平台的大规模流失建模。

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [5] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: 提出了一种隐私保护的多模态跌倒检测系统P2MFDS，结合毫米波雷达和3D振动传感，在浴室环境中显著提高了检测准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 老龄化加剧导致跌倒风险增加，浴室是跌倒高发区，现有单模态系统在复杂环境中准确性不足。

Method: 开发了传感器评估框架，融合毫米波雷达和3D振动传感，构建了双流网络P2MFDS，结合CNN-BiLSTM-Attention和多尺度CNN-SEBlock-Self-Attention分支。

Result: P2MFDS在准确率和召回率上显著优于现有方法。

Conclusion: P2MFDS为浴室环境中的老年人提供了一种隐私保护的高效跌倒检测解决方案。

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [6] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: 论文提出了一种以任务为中心、数据质量为基础的五层框架，用于下一代自动驾驶车辆（AVs），旨在通过数据质量与任务需求的映射提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AV领域的研究和实践过于关注模型/算法，而忽视了数据质量（DQ）的重要性，导致在动态环境和异构数据流中功能、效率和可信度的不足。

Method: 提出一个五层框架（数据层、DQ层、任务层、应用层和目标层），通过案例研究（nuScenes数据集）验证冗余数据对YOLOv8目标检测任务性能的影响。

Result: 研究表明，部分去除多源图像数据的冗余可以提高任务性能，同时揭示了图像和LiDAR多模态数据中的冗余DQ问题。

Conclusion: 该框架为AV领域提供了解决DQ、任务编排和性能导向系统开发的新方向，有望推动构建更自适应、可解释和鲁棒的AV系统。

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [7] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: 提出了一种基于分组的高效反馈门网络方法，用于单幅高光谱图像超分辨率重建，通过反馈和门操作提升空间分辨率。


<details>
  <summary>Details</summary>
Motivation: 现有单幅高光谱图像超分辨率方法未能充分利用波段间和空间-光谱信息的相干性，导致性能受限。

Method: 设计了高效反馈门网络，包括大核卷积和光谱交互的反馈与门操作，以及空间-光谱强化门模块（SSRGM）和三维SSRGM。

Result: 在三个高光谱数据集上验证了所提网络在光谱保真度和空间内容重建方面的优越性能。

Conclusion: 该方法通过分组和门操作有效提升了高光谱图像的超分辨率重建性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [8] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: 提出了一种结合旋转感知目标检测模型和视觉语言解析器的混合框架，用于高效提取2D工程图纸中的关键信息。


<details>
  <summary>Details</summary>
Motivation: 手动提取2D工程图纸信息效率低且不可靠，现有OCR模型因复杂布局和工程符号表现不佳。

Method: 使用YOLOv11-OBB定位注释并提取定向边界框（OBB）补丁，再通过微调的轻量级视觉语言模型（VLM）解析为结构化输出。

Result: Donut模型表现优于Florence-2，精度达88.5%，召回率99.2%，F1分数93.5%。

Conclusion: 该框架在现代化2D图纸解析中具有实际应用价值，支持下游制造任务。

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [9] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/abs/2506.17403)
*Zhiyi Shi,Junsik Kim,Helen Y. Yang,Yonghyun Song,Hyun-Jic Oh,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 提出了一种名为STPT的自监督学习方法，用于解决胚胎发育视频中的长视频和时序不对齐问题，显著提高了IVF胚胎存活预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于标记的妊娠结果数据有限，且现有自监督学习方法无法直接应用于胚胎发育视频，需要一种新的方法来处理长视频和时序不对齐的挑战。

Method: STPT方法分为空间和时间两个阶段，分别训练一个编码器并冻结另一个，以减少内存需求，并通过视频内对齐和时序建模解决不对齐问题。

Result: 在23,027个时间推移视频（3,286个标记）上，STPT达到了0.635的最高AUC（95% CI: 0.632-0.638），优于基线方法。

Conclusion: STPT方法在有限计算资源下，有效解决了胚胎发育视频中的挑战，显著提升了预测性能。

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [10] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/abs/2506.17412)
*Zijun Sun,Solveig Thrun,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Mamba RNN和状态空间模型的乳腺癌风险预测方法，结合不对称模块，显著提升了预测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球主要死因之一，现有模型多依赖单次筛查数据，难以捕捉乳腺组织的动态变化趋势。

Method: 采用Vision Mamba RNN（VMRNN）结合状态空间模型和LSTM机制，并引入不对称模块（SAD和LAT）检测双侧差异。

Result: 在预测癌症发病方面表现优异，尤其对高密度乳腺和长期（4-5年）预测效果显著。

Conclusion: 该方法能有效捕捉乳腺组织的动态变化，为个性化筛查策略提供支持。

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [11] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: 提出了一种结合CNN-Transformer和点几何推理的稀疏视图CBCT重建方法，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CBCT重建因采样不足导致图像质量差，需要一种能同时捕捉局部细节和全局上下文的方法。

Method: 采用TransUNet结合CNN和Transformer，并引入邻居感知的Point Transformer来增强体积一致性。

Result: 在LUNA16数据集上，Trans-CBCT比基线提升1.17 dB PSNR和0.0163 SSIM；Trans²-CBCT进一步提升0.63 dB PSNR和0.0117 SSIM。

Conclusion: 结合CNN-Transformer特征和点几何推理的方法在稀疏视图CBCT重建中表现出色。

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [12] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/abs/2506.17439)
*Nisar Ahmed,Gulshan Saleem,Hafiz Muhammad Shahzad Asif,Muhammad Usman Younus,Kalsoom Safdar*

Main category: cs.CV

TL;DR: 论文提出了一种基于瞬态能量谱分析和CNN-Bi-GRU混合深度学习模型的RF设备识别方法，取得了高精度分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和5G网络的快速发展，复杂电磁环境中辐射设备的准确识别和分类成为关键挑战。

Method: 利用广义线性啁啾变换提取RF设备的瞬态特征，并采用CNN-Bi-GRU混合模型进行分类。

Result: 实验结果显示，该方法在10折交叉验证中达到了99.33%的精确率、99.53%的召回率和99.17%的分类准确率。

Conclusion: CNN-Bi-GRU模型在RF设备识别中表现出色，适用于复杂无线环境中的设备分类。

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [13] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/abs/2506.17455)
*Taufikur Rahman Fuad,Sabbir Ahmed,Shahriar Ivan*

Main category: cs.CV

TL;DR: AQUA20数据集为水下视觉识别提供了8,171张图像，涵盖20种海洋物种，评估了13种深度学习模型，其中ConvNeXt表现最佳。


<details>
  <summary>Details</summary>
Motivation: 水下环境的复杂失真（如浑浊、低光照和遮挡）严重影响了视觉系统的性能，需要新的数据集和模型来解决这一问题。

Method: 使用AQUA20数据集评估了13种深度学习模型，包括轻量级CNN和基于Transformer的架构，并通过GRAD-CAM和LIME进行可解释性分析。

Result: ConvNeXt表现最佳，Top-3准确率达98.82%，Top-1准确率为90.69%，F1-score为88.92%。

Conclusion: AQUA20数据集为水下物种识别提供了重要资源，未来研究仍有改进空间。

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [14] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/abs/2506.17457)
*Dong Xiao,Guangyao Chen,Peixi Peng,Yangru Huang,Yifan Zhao,Yongxing Dai,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出了一种实时异常检测方法，结合事件相机和RGB相机数据，通过异步图神经网络和CNN实现高精度和快速响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视响应时间，而自动驾驶场景中时间敏感性至关重要。

Method: 提出多模态异步混合网络，结合事件相机的高时间分辨率和RGB相机的空间特征。

Result: 在基准数据集上表现优于现有方法，实现毫秒级实时性能。

Conclusion: 该方法在精度和响应时间上均优于现有技术，适用于自动驾驶的实时异常检测。

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [15] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/abs/2506.17469)
*Thomas Plante St-Cyr,François Duhaime,Jean-Sébastien Dubé,Simon Grenier*

Main category: cs.CV

TL;DR: 传统颗粒分布分析耗时且成本高，光学粒度分析可缓解这一问题。本文提供了蒙特利尔地区321种土壤样本的高分辨率图像数据集，用于训练卷积神经网络。


<details>
  <summary>Details</summary>
Motivation: 传统颗粒分布分析方法效率低且成本高，希望通过光学粒度分析改进这一流程。

Method: 使用标准化拍摄方法，采集12,714张土壤样本图像，分辨率45 MP，最小尺度39.4微米/像素，包括湿润和干燥状态。

Result: 提供了高质量数据集，适用于训练卷积神经网络进行粒度分析。

Conclusion: 光学粒度分析结合高分辨率图像数据集，为改进传统颗粒分布分析方法提供了可行方案。

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [16] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/abs/2506.17500)
*Julio Silva-Rodríguez,Fereshteh Shakeri,Houda Bahig,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 该论文探讨了医学图像分析中视觉语言模型（VLMs）在现实不平衡数据条件下的适应性，并提出了一种无需训练的方法来提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设数据分布平衡且需要验证集，这在医学领域不现实，因此需要研究更真实的适应场景。

Method: 提出了一种无需训练的方法，通过自适应混合视觉和文本监督来适应不平衡数据。

Result: 实验表明现有方法在现实条件下性能下降，而提出的方法在多种任务中表现稳健。

Conclusion: 论文提出的方法为医学图像分析中的不平衡数据适应提供了高效且鲁棒的解决方案。

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [17] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/abs/2506.17503)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法SCA-T，用于在医学视觉语言模型（VLMs）的迁移学习中提供可信度保证，解决了传统方法在适应性阶段破坏交换性假设的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管医学视觉语言模型在数据高效的图像分类中表现出色，但其可靠性尚未充分研究。本文旨在通过分体共形预测（SCP）框架提供可信度保证。

Method: 提出了转导分体共形适应（SCA-T），一种新的迁移学习流程，通过无监督转导适应在标定数据和测试数据上联合优化。

Result: 实验表明，SCA-T在效率和条件覆盖方面优于传统SCP，同时保持了相同的经验保证。

Conclusion: SCA-T为医学VLMs的迁移学习提供了一种更可靠的解决方案，适用于多种图像模态和任务。

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [18] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/abs/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: 提出了一种基于手腕传感器的个性化高尔夫挥杆分析框架，解决了现有方法的局限性，实现了高精度的全身运动分析和挥杆阶段识别。


<details>
  <summary>Details</summary>
Motivation: 高尔夫挥杆分析受限于孤立指标、专业运动员数据不足和缺乏丰富的运动表示，需要一种更全面、数据驱动的方法。

Method: 通过公开视频构建专业挥杆数据集，利用生物准确的人体网格恢复重建3D运动，训练神经网络从手腕数据推断运动和挥杆阶段。

Result: 系统能准确估计全身运动和挥杆事件，提供可解释的运动特征，并支持异常动作的早期检测。

Conclusion: 该框架为研究、教练和伤病预防提供了可扩展的高保真运动分析，挑战了传统假设并揭示了新的生物标志物。

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [19] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/abs/2506.17545)
*Zhihao Yuan,Shuyi Jiang,Chun-Mei Feng,Yaolun Zhang,Shuguang Cui,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1是一个视频驱动的框架，通过强化学习和两阶段定位流程，无需3D实例监督即可理解3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知的LLM是黑盒模型，依赖预训练的3D检测器，缺乏透明性。

Method: 结合强化学习驱动的推理和两阶段定位（时间定位和图像定位），利用SAM2生成像素级掩码并投影回3D。

Result: 在多个数据集上超越现有开放词汇基线，提供透明、逐步的推理过程。

Conclusion: 基于强化学习和RGB-D视频的方法为可信赖的3D场景理解提供了高效且注释少的途径。

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [20] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: 论文提出了一个合成数据集SynDaCaTE，用于评估胶囊网络是否真正学习到部分-整体层次结构，并展示了现有模型的瓶颈和自注意力机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有胶囊网络在监督任务中难以验证是否真正学习到部分-整体层次结构，因此需要一个专门的数据集来评估。

Method: 使用合成数据集SynDaCaTE，分析现有胶囊模型的瓶颈，并测试自注意力机制在部分-整体推理中的效果。

Result: 发现现有胶囊模型存在瓶颈，同时证明自注意力机制在部分-整体推理中非常有效。

Conclusion: SynDaCaTE数据集有助于评估胶囊网络，并启发未来设计更有效的计算机视觉归纳偏置。

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [21] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: VLA-OS是一个统一的视觉-语言-动作架构系列，通过系统实验比较不同任务规划范式和表示方法，发现视觉基础规划表示优于语言规划表示，且分层VLA范式在性能上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在架构、规划范式和训练数据上差异显著，难以明确性能提升的来源和改进方向，因此需要系统研究不同规划范式和表示的影响。

Method: 提出VLA-OS架构系列，设计多维度控制实验（对象类别、视觉模态、环境、末端执行器），比较不同规划范式和表示方法。

Result: 视觉基础规划表示优于语言规划表示；分层VLA范式在任务性能、泛化能力等方面表现最佳，但训练和推理速度较慢。

Conclusion: 分层VLA范式是VLA模型的优选方案，但需权衡速度与性能。

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [22] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Main category: cs.CV

TL;DR: FedMRG是一个利用联邦学习（FL）开发隐私保护的、多中心LLM驱动医学报告生成（MRG）模型的框架，解决了通信效率和多模态数据异质性的挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像-报告数据分散且隐私保护严格，阻碍了LLM驱动MRG模型的发展。FedMRG旨在通过FL实现多中心协作，同时保护隐私。

Method: 采用低秩分解减少通信开销，结合客户端感知对比学习和双适配器机制处理数据异质性。

Result: FedMRG在FL-MRG基准测试中表现出良好的泛化性和适应性，能生成临床准确的报告并保持通信效率。

Conclusion: FedMRG为多中心协作开发LLM驱动MRG模型提供了可行方案，平衡了隐私保护和模型性能。

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [23] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN通过架构级解决方案解决LVLMs的幻觉问题，提出DG-DPU模块，通过跨层循环推理增强模型稳定性。


<details>
  <summary>Details</summary>
Motivation: LVLMs易产生视觉无依据的幻觉输出，现有方法资源消耗大或需任务特定配置。

Method: 提出Dual-Gated Depth Propagation Unit (DG-DPU)模块，跨层共享并循环优化隐藏状态，自适应传播信息。

Result: 仅微调DG-DPU模块，HalluRNN在多个基准测试中表现优异且稳健。

Conclusion: HalluRNN通过架构创新有效减少幻觉问题，资源高效且性能强大。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [24] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: DRAMA-X是一个用于评估自动驾驶决策中意图预测、风险评估等任务的细粒度基准，基于DRAMA数据集构建。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对安全关键场景下多类意图预测的评估，DRAMA-X填补了这一空白。

Method: 通过自动化标注流程构建DRAMA-X基准，并提出轻量级框架SGG-Intent，利用VLM和LLM进行推理。

Result: 实验表明，基于场景图的推理能提升意图预测和风险评估性能，尤其是显式建模上下文线索时。

Conclusion: DRAMA-X为自动驾驶决策提供了结构化评估工具，SGG-Intent框架展示了场景图推理的有效性。

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [25] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/abs/2506.17592)
*Younghun Kim,Minsuk Jang,Myung-Joon Kwon,Wonjun Lee,Changick Kim*

Main category: cs.CV

TL;DR: SELFI框架通过动态调节身份特征的使用，提高了深度伪造检测的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究身份特征在深度伪造检测中的作用，解决现有方法中身份特征使用不一致的问题。

Method: 提出SELFI框架，包括FAIA模块提取身份嵌入和IAFM模块选择性融合身份与视觉特征。

Result: 在四个基准测试中，SELFI平均AUC提升3.1%，在DFDC数据集上提升6%。

Conclusion: 身份特征应被显式建模和自适应控制，SELFI框架有效提升了检测性能。

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [26] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/abs/2506.17596)
*Wei Huang,Yinxuan Xu,Yintao Zhou,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于面部表情和行为步态的多模态体外诊断方法，用于帕金森病的早期检测，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的不可治愈性、快速进展和严重残疾对患者及其家庭造成巨大负担，老龄化社会对早期检测的需求增加。现有体外诊断方法存在数据不足、设备要求高和单模态风险等问题。

Method: 采用轻量级深度学习模型进行特征提取和融合，结合面部表情和行为步态的多模态数据，以提高诊断准确性并便于移动设备部署。

Result: 建立了最大的多模态PD数据集，并通过实验验证了所提方法的有效性。

Conclusion: 提出的多模态诊断方法在提高PD早期检测准确性和实用性方面具有潜力。

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [27] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/abs/2506.17597)
*Pengyu Kan,Craig Jones,Kenichi Oishi*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的可解释且稳健的脑MRI年龄预测模型，通过多视图和体积信息融合，实现了线性复杂度，并在多个数据集上验证了其高准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够应对人口统计学和技术差异的脑MRI年龄预测模型，同时具备可解释性。

Method: 采用Transformer架构，结合自监督预训练和多视图MRI数据处理，引入Stem架构降低计算复杂度，并在多个数据集上进行训练和验证。

Result: 在ADNI2 & 3和OASIS3测试集上MAE为3.65年，在AIBL数据集上MAE为3.54年，且脑年龄差与认知能力显著相关。

Conclusion: 模型在多视图和体积信息融合下实现了高精度、泛化性和可解释性，并与神经退行性疾病相关。

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [28] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 论文提出了一种浅层特征增强器，通过特征上采样实现高分辨率特征生成，显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现代多模态大语言模型通过高分辨率图像特征提升了细粒度视觉理解任务性能，但计算成本高昂。

Method: 提出浅层特征增强器，通过特征上采样生成高分辨率特征。

Result: 实验表明，该方法在训练和推理时间上大幅减少，计算成本降低1.5倍FLOPs。

Conclusion: 浅层特征增强器在保持性能的同时显著降低了计算开销。

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [29] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/abs/2506.17612)
*Yunlong Lin,Zixu Lin,Kunjie Lin,Jinbin Bai,Panwang Pan,Chenxin Li,Haoyu Chen,Zhongdao Wang,Xinghao Ding,Wenbo Li,Shuicheng Yan*

Main category: cs.CV

TL;DR: JarvisArt是一个基于多模态大语言模型的智能修图代理，通过两阶段训练和专用协议，实现了用户友好的交互和高质量的修图效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI修图工具缺乏可调性和泛化能力，无法满足个性化需求，JarvisArt旨在填补这一空白。

Method: 采用Chain-of-Thought监督微调和GRPO-R优化训练，结合Agent-to-Lightroom协议与Lightroom集成。

Result: 在MMArt-Bench上，JarvisArt在内容保真度上比GPT-4o提升60%，同时保持指令跟随能力。

Conclusion: JarvisArt为智能修图开辟了新途径，具备强大的泛化和精细控制能力。

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [30] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: CLiViS是一个无需训练的新框架，结合LLMs和VLMs的优势，通过动态认知地图实现视觉推理。


<details>
  <summary>Details</summary>
Motivation: 解决EVR中复杂指令和长时视频的时空动态性挑战，弥补现有方法在视觉细节和推理能力上的不足。

Method: 利用LLMs进行高级任务规划，协调VLM驱动的开放世界视觉感知，通过动态认知地图迭代更新场景上下文。

Result: 在多个基准测试中表现优异，尤其在处理长时视觉依赖时效果显著。

Conclusion: CLiViS通过结合LLMs和VLMs的优势，有效提升了视觉推理能力，尤其在复杂动态环境中表现突出。

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [31] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/abs/2506.17632)
*Hangcheng Liu,Xu Kuang,Xingshuo Han,Xingwan Wu,Haoran Ou,Shangwei Guo,Xingyi Huang,Tao Xiang,Tianwei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PatchHunter的优化自由对抗补丁攻击方法，针对立体深度估计（SDE）模型，解决了现有攻击方法在现实场景中适用性差的问题。


<details>
  <summary>Details</summary>
Motivation: 现有SDE模型的对抗攻击方法通常局限于不现实的设置（如静态场景中的数字扰动），缺乏现实适用性。因此，需要设计一种物理可实现、场景自适应且可迁移的攻击方法。

Method: 论文提出了一个统一的攻击框架，覆盖立体匹配的四个核心阶段，并发现基于优化的补丁迁移性差。基于此，提出了PatchHunter，通过强化学习驱动的搜索生成视觉模式补丁。

Result: PatchHunter在KITTI数据集、CARLA模拟器和真实车辆部署中均表现出色，攻击成功率高（如D1-all > 0.4），且迁移性优于基于优化的方法。

Conclusion: PatchHunter是一种高效且可迁移的对抗攻击方法，适用于现实场景中的SDE模型，即使在低光等挑战性条件下仍能保持高攻击成功率。

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [32] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: AMCN是一种新型网络，通过自适应多提示对比学习解决少样本OOD检测问题，利用CLIP连接文本与图像，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统OOD检测方法需要大量IID样本，限制了实际应用。少样本OOD检测更具挑战性，且现有方法忽略了类间多样性。

Method: 提出AMCN网络，通过自适应提示（可学习ID提示、固定OOD提示和自适应OOD提示）和类间阈值生成自适应边界，并设计提示引导的ID-OOD分离模块。

Result: 实验表明AMCN优于其他最先进方法。

Conclusion: AMCN通过自适应多提示对比学习有效解决了少样本OOD检测问题，为实际应用提供了新思路。

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [33] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/abs/2506.17645)
*Shih-Wen Liu,Hsuan-Yu Fan,Wei-Ta Chu,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: PathGenIC框架通过上下文学习和自适应反馈，在医学报告生成任务中取得最优结果。


<details>
  <summary>Details</summary>
Motivation: 自动化生成医学报告需要有效的视觉表示和领域知识，PathGenIC模拟专家实践，提升生成质量。

Method: 采用多模态上下文学习机制，动态检索相似WSI-报告对，并加入自适应反馈。

Result: 在HistGen基准测试中，BLEU、METEOR和ROUGE-L指标显著提升，且对不同报告长度和疾病类别具有鲁棒性。

Conclusion: PathGenIC为AI驱动的病理报告生成提供了解决方案，为多模态临床应用奠定了基础。

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [34] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/abs/2506.17664)
*Shuaiye Lu,Linjiang Zhou,Xiaochuan Shi*

Main category: cs.CV

TL;DR: 提出了一种无需训练的方法MDSAM，通过动态调整图像标记的注意力来减少大型视觉语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在解码时对图像标记的敏感性导致幻觉现象，需要一种无需额外训练的方法来改善。

Method: MDSAM通过记忆注意力模式并在解码时动态调整，增强对相关图像标记的关注。

Result: 在多个基准测试中，MDSAM能持续减少幻觉并提高可靠性，且兼容多种LVLM架构。

Conclusion: MDSAM是一种无需训练的高效方法，能显著减少幻觉现象，适用于多种模型架构。

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [35] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/abs/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: 本文提出了一种基于Transformer的检测头CSDN，通过引入门控机制替代传统的自注意力和交叉注意力层，以更高效地利用CNN骨干网络的特征，提升目标检测的准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在目标检测中受限于有限的感受野，难以捕捉全局上下文信息。本文认为特征的有效利用与特征提取过程同等重要，因此重新评估了DETR启发式头部网络架构，发现其自注意力机制存在信息冗余问题。

Method: 提出Context-Gated Scale-Adaptive Detection Network (CSDN)，采用门控机制动态选择和组合多注意力模式的特征维度和尺度信息，增强全局上下文建模能力。

Result: CSDN可直接替换多种基于CNN的检测器的原生头部，仅需少量微调即可显著提升检测精度，避免了大规模重新训练的需求。

Conclusion: CSDN通过门控机制优化特征利用，显著提升了目标检测的全局上下文建模能力和对不同尺寸、结构目标的适应性。

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [36] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/abs/2506.17685)
*Amirshayan Nasirimajd,Chiara Plizzari,Simone Alberto Peirone,Marco Ciccone,Giuseppe Averta,Barbara Caputo*

Main category: cs.CV

TL;DR: 提出SeqDG方法，通过视觉-文本序列重构和跨域序列混合训练，提升第一人称视角动作识别的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决第一人称视角动作识别模型在未见环境中性能下降的问题。

Method: 结合视觉-文本序列重构（SeqRec）和跨域序列混合训练（SeqMix）。

Result: 在EPIC-KITCHENS-100和EGTEA数据集上分别提升2.4%和0.6%的性能。

Conclusion: SeqDG通过序列建模有效提升了动作识别的跨域泛化能力。

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [37] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/abs/2506.17694)
*Gnana Praveen Rajasekhar,Jahangir Alam*

Main category: cs.CV

TL;DR: 提出了一种基于对比学习和掩码数据建模的自监督学习框架，用于音频-视觉说话人验证，减少了计算成本和对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量标注数据和独立模态架构，计算成本高且难以扩展。

Method: 采用对比学习和掩码数据建模的自监督学习框架，使用共享视觉Transformer骨干网络处理音频和视觉输入。

Result: 实验表明，该方法在无标注数据情况下表现优异，同时降低了计算成本。

Conclusion: 提出的统一框架高效且鲁棒，适用于多模态输入，解决了传统方法的局限性。

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [38] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney提出了一种两阶段框架，利用视频扩散模型生成动态场景的长期视频，解决了现有方法在3D感知和动态对象运动方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法基于2D扩散模型，缺乏3D感知且无法捕捉动态对象运动，导致生成视频存在失真和静态场景限制。

Method: DreamJourney分为两阶段：第一阶段通过3D点云和视频扩散模型生成跨视角一致视频；第二阶段利用多模态大语言模型描述对象运动并生成动态视频。

Result: 实验表明，DreamJourney在定量和定性上均优于现有方法。

Conclusion: DreamJourney成功实现了动态场景的长期视频生成，解决了现有技术的局限性。

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [39] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room是一个通过自然语言指令交互式生成和编辑3D房间网格的框架，结合视觉编程和扩散模型提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了实现对房间属性的精确控制，将复杂任务分解为多个简单步骤，并通过统一框架支持这些任务。

Method: 使用视觉编程（VP）和大型语言模型（LLM）生成Python式程序，结合扩散模型生成全景图像，并通过双向LSTM优化训练目标。

Result: 展示了框架在生成和编辑3D房间网格方面的灵活性，并在定量和定性上优于现有模型。

Conclusion: Programmable-Room通过分解任务和结合先进技术，实现了高质量的3D房间生成和编辑。

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [40] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/abs/2506.17712)
*Xinyu Xiong,Wuteng Cao,Zihuang Wu,Lei Zhang,Chong Gao,Guanbin Li,Qiyuan Qin*

Main category: cs.CV

TL;DR: 提出了一种名为PDC-Net的新方法，用于从MRI中准确分割盆腔放射损伤（PRI），通过多方向聚合模块和记忆引导上下文模块提升分割效果。


<details>
  <summary>Details</summary>
Motivation: 盆腔放射损伤（PRI）的准确分割对预后评估和个性化治疗计划至关重要，但自动化分割因器官形态复杂和上下文混淆而具有挑战性。

Method: 提出PDC-Net，包含多方向聚合模块（MDA）和记忆引导上下文模块（MGC），并通过自适应融合解码器（AFD）动态选择特征。

Result: 在首个大规模盆腔放射损伤数据集上验证，PDC-Net优于现有方法。

Conclusion: PDC-Net通过分而治之的策略有效解决了PRI分割中的挑战，为临床提供了更精确的工具。

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


### [41] [YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception](https://arxiv.org/abs/2506.17733)
*Mengqi Lei,Siqi Li,Yihong Wu,Han Hu,You Zhou,Xinhu Zheng,Guiguang Ding,Shaoyi Du,Zongze Wu,Yue Gao*

Main category: cs.CV

TL;DR: YOLOv13提出了一种基于超图的自适应相关性增强机制（HyperACE）和全流程聚合与分配范式（FullPAD），显著提升了复杂场景下的目标检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有YOLO系列模型在局部信息聚合和成对相关性建模方面存在局限，无法捕捉全局多对多高阶相关性，限制了复杂场景下的检测性能。

Method: 提出HyperACE机制和FullPAD范式，结合深度可分离卷积，实现高效全局特征融合与轻量化设计。

Result: 在MS COCO基准测试中，YOLOv13-N的mAP比YOLO11-N提升3.0%，比YOLOv12-N提升1.5%，且参数量和计算量更少。

Conclusion: YOLOv13通过创新机制和轻量化设计，实现了高性能和高效的目标检测。

Abstract: The YOLO series models reign supreme in real-time object detection due to
their superior accuracy and computational efficiency. However, both the
convolutional architectures of YOLO11 and earlier versions and the area-based
self-attention mechanism introduced in YOLOv12 are limited to local information
aggregation and pairwise correlation modeling, lacking the capability to
capture global multi-to-multi high-order correlations, which limits detection
performance in complex scenarios. In this paper, we propose YOLOv13, an
accurate and lightweight object detector. To address the above-mentioned
challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement
(HyperACE) mechanism that adaptively exploits latent high-order correlations
and overcomes the limitation of previous methods that are restricted to
pairwise correlation modeling based on hypergraph computation, achieving
efficient global cross-location and cross-scale feature fusion and enhancement.
Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)
paradigm based on HyperACE, which effectively achieves fine-grained information
flow and representation synergy within the entire network by distributing
correlation-enhanced features to the full pipeline. Finally, we propose to
leverage depthwise separable convolutions to replace vanilla large-kernel
convolutions, and design a series of blocks that significantly reduce
parameters and computational complexity without sacrificing performance. We
conduct extensive experiments on the widely used MS COCO benchmark, and the
experimental results demonstrate that our method achieves state-of-the-art
performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N
improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and
models of our YOLOv13 model are available at:
https://github.com/iMoonLab/yolov13.

</details>


### [42] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/abs/2506.17746)
*Sourabh Vasant Gothe,Ayon Chattopadhyay,Gunturi Venkata Sai Phani Kiran,Pratik,Vibhav Agarwal,Jayesh Rajkumar Vachhani,Sourav Ghosh,Parameswaranath VM,Barath Raj KR*

Main category: cs.CV

TL;DR: PhysID利用生成模型从单视角图像创建物理交互动态，简化3D建模和物理属性校准，实现实时渲染和个性化交互。


<details>
  <summary>Details</summary>
Motivation: 提升移动用户体验，通过交互式和AR/VR应用，解决静态图像转交互体验的挑战。

Method: 利用大型生成模型生成3D网格和预测物理属性，结合设备端物理引擎实现实时渲染。

Result: 实验验证了多模态大语言模型的零样本能力和3D重建模型性能，展示了端到端框架的有效性。

Conclusion: PhysID在移动端交互动态领域取得突破，支持实时、非确定性交互和高效内存使用。

Abstract: Transforming static images into interactive experiences remains a challenging
task in computer vision. Tackling this challenge holds the potential to elevate
mobile user experiences, notably through interactive and AR/VR applications.
Current approaches aim to achieve this either using pre-recorded video
responses or requiring multi-view images as input. In this paper, we present
PhysID, that streamlines the creation of physics-based interactive dynamics
from a single-view image by leveraging large generative models for 3D mesh
generation and physical property prediction. This significantly reduces the
expertise required for engineering-intensive tasks like 3D modeling and
intrinsic property calibration, enabling the process to be scaled with minimal
manual intervention. We integrate an on-device physics-based engine for
physically plausible real-time rendering with user interactions. PhysID
represents a leap forward in mobile-based interactive dynamics, offering
real-time, non-deterministic interactions and user-personalization with
efficient on-device memory consumption. Experiments evaluate the zero-shot
capabilities of various Multimodal Large Language Models (MLLMs) on diverse
tasks and the performance of 3D reconstruction models. These results
demonstrate the cohesive functioning of all modules within the end-to-end
framework, contributing to its effectiveness.

</details>


### [43] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/abs/2506.17759)
*Fadi Abdeladhim Zidi,Djamel Eddine Boukhari,Abdellah Zakaria Sellam,Abdelkrim Ouafi,Cosimo Distante,Salah Eddine Bekhouche,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: LoLA-SpecViT是一种轻量级光谱视觉Transformer，通过参数高效架构解决了高光谱图像分类中的挑战，显著提升了分类精度和适应性。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类面临高维度、频带冗余和标注样本不足的问题，现有Transformer模型在标签稀缺条件下的扩展性和适应性有限。

Method: 结合3D卷积光谱前端和局部窗口自注意力，引入低秩适应（LoRA）和循环学习率调度器，减少可训练参数并提升适应性。

Result: 在三个基准数据集上，LoLA-SpecViT以更少参数实现了高达99.91%的准确率，并在低标签条件下表现出更强的鲁棒性。

Conclusion: LoLA-SpecViT为高光谱图像分类提供了可扩展且通用的解决方案，适用于农业、环境监测和遥感分析等实际应用。

Abstract: Hyperspectral image classification remains a challenging task due to the high
dimensionality of spectral data, significant inter-band redundancy, and the
limited availability of annotated samples. While recent transformer-based
models have improved the global modeling of spectral-spatial dependencies,
their scalability and adaptability under label-scarce conditions remain
limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation
Local Attention Spectral Vision Transformer), a lightweight spectral vision
transformer that addresses these limitations through a parameter-efficient
architecture tailored to the unique characteristics of hyperspectral imagery.
Our model combines a 3D convolutional spectral front-end with local
window-based self-attention, enhancing both spectral feature extraction and
spatial consistency while reducing computational complexity. To further improve
adaptability, we integrate low-rank adaptation (LoRA) into attention and
projection layers, enabling fine-tuning with over 80\% fewer trainable
parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation
strength during training, improving convergence and generalisation. Extensive
experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and
Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art
baselines, achieving up to 99.91\% accuracy with substantially fewer parameters
and enhanced robustness under low-label regimes. The proposed framework
provides a scalable and generalizable solution for real-world HSI applications
in agriculture, environmental monitoring, and remote sensing analytics. Our
code is available in the following
\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [44] [Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert](https://arxiv.org/abs/2506.17787)
*Gelei Xu,Yuying Duan,Zheyuan Liu,Xueyang Li,Meng Jiang,Michael Lemmon,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: FairMoE框架通过动态路由数据到最适合的专家模块，在保持公平性的同时提高了皮肤疾病诊断的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有偏置缓解方法在消除敏感属性与诊断预测相关性时，常因丢失临床相关诊断线索而降低性能。

Method: 提出FairMoE框架，采用分层混合专家模块作为群体特定学习器，动态路由数据。

Result: 实验表明，FairMoE在保持公平性指标的同时显著提高了准确性。

Conclusion: FairMoE是一种有效的替代方法，能够在公平性和性能之间取得平衡。

Abstract: AI-based systems have achieved high accuracy in skin disease diagnostics but
often exhibit biases across demographic groups, leading to inequitable
healthcare outcomes and diminished patient trust. Most existing bias mitigation
methods attempt to eliminate the correlation between sensitive attributes and
diagnostic prediction, but those methods often degrade performance due to the
lost of clinically relevant diagnostic cues. In this work, we propose an
alternative approach that incorporates sensitive attributes to achieve
fairness. We introduce FairMoE, a framework that employs layer-wise
mixture-of-experts modules to serve as group-specific learners. Unlike
traditional methods that rigidly assign data based on group labels, FairMoE
dynamically routes data to the most suitable expert, making it particularly
effective for handling cases near group boundaries. Experimental results show
that, unlike previous fairness approaches that reduce performance, FairMoE
achieves substantial accuracy improvements while preserving comparable fairness
metrics.

</details>


### [45] [Time-Contrastive Pretraining for In-Context Image and Video Segmentation](https://arxiv.org/abs/2506.17837)
*Assefa Wahd,Jacob Jaremko,Abhilash Hareendranathan*

Main category: cs.CV

TL;DR: 论文提出了一种名为Temporal的自监督学习方法，通过将上下文学习（ICL）重新定义为视频对象分割（VOS）任务，解决了传统网格方法在视觉应用中的灵活性不足问题。


<details>
  <summary>Details</summary>
Motivation: 主流ICL方法依赖网格策略，限制了上下文图像的数量和分辨率，无法满足视觉应用的需求。

Method: 提出Temporal方法，通过时间对比自监督目标预训练提示检索器，并将ICL任务重新定义为VOS任务。

Result: 在MICCAI FLARE 2022数据集上，图像分割Dice分数提升10.64%（90.95%），视频分割提升14.88%（92.45%）。

Conclusion: Temporal方法通过自监督学习和VOS任务重构，显著提升了视觉ICL的性能和灵活性。

Abstract: In-context learning (ICL) enables generalization to new tasks with minimal
labeled data. However, mainstream ICL approaches rely on a gridding strategy,
which lacks the flexibility required for vision applications. We introduce
Temporal, a time-contrastive self-supervised objective that pretrains a prompt
retriever for visual ICL, and formulate ICL as a video object segmentation
(VOS) task. Temporal addresses key limitations of grid-based methods that
restrict the number and resolution of context images. By reframing ICL as a VOS
problem, our approach supports a variable number of context images while
preserving their full resolution. To address the challenge of selecting optimal
context sets for queries, we pretrain a prompt retriever on videos via
self-supervised learning, where adjacent frames serve as positives and distant
frames as negatives. For image segmentation, the prompt retriever selects
relevant sequences that, when combined with the query, form coherent videos for
VOS processing. For video segmentation, it identifies keyframes, predicts their
masks using our ICL pipeline, and propagates them throughout the sequence. When
evaluated on MICCAI FLARE 2022, our method achieves substantial improvements
over baselines: 90.95% Dice score for image segmentation (10.64% improvement)
and 92.45% Dice for video segmentation (14.88% improvement).

</details>


### [46] [Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling](https://arxiv.org/abs/2506.17838)
*Kazuki Naganuma,Shunsuke Ono*

Main category: cs.CV

TL;DR: 提出了一种基于卷积稀疏表示（CSR）的前景-背景分离（FBS）方法，能够处理低帧率和多种噪声的视频。


<details>
  <summary>Details</summary>
Motivation: 现有FBS方法无法准确分离低质量视频中的前景和背景，因为它们仅捕获特定或一般特征，且未明确建模多种噪声。

Method: 提出了一种基于CSR的前景模型，将FBS建模为多凸优化问题，结合CSR、通用特征函数和噪声表征函数。

Result: 实验表明，该方法在红外和显微镜视频中优于现有方法。

Conclusion: 该方法能有效分离前景和背景，并处理多种噪声和低帧率问题。

Abstract: This paper proposes a foreground-background separation (FBS) method with a
novel foreground model based on convolutional sparse representation (CSR). In
order to analyze the dynamic and static components of videos acquired under
undesirable conditions, such as hardware, environmental, and power limitations,
it is essential to establish an FBS method that can handle videos with low
frame rates and various types of noise. Existing FBS methods have two
limitations that prevent us from accurately separating foreground and
background components from such degraded videos. First, they only capture
either data-specific or general features of the components. Second, they do not
include explicit models for various types of noise to remove them in the FBS
process. To this end, we propose a robust FBS method with a CSR-based
foreground model. This model can adaptively capture specific spatial structures
scattered in imaging data. Then, we formulate FBS as a constrained multiconvex
optimization problem that incorporates CSR, functions that capture general
features, and explicit noise characterization functions for multiple types of
noise. Thanks to these functions, our method captures both data-specific and
general features to accurately separate the components from various types of
noise even under low frame rates. To obtain a solution of the optimization
problem, we develop an algorithm that alternately solves its two convex
subproblems by newly established algorithms. Experiments demonstrate the
superiority of our method over existing methods using two types of degraded
videos: infrared and microscope videos.

</details>


### [47] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/abs/2506.17858)
*Yingcheng Liu,Peiqi Wang,Sebastian Diaz,Esra Abaci Turk,Benjamin Billot,Patricia Ellen Grant,Polina Golland*

Main category: cs.CV

TL;DR: 提出了一种基于SMPL的3D胎儿统计身体模型，用于改进胎儿MRI分析中的运动和形状捕捉。


<details>
  <summary>Details</summary>
Motivation: 现有方法（关键点或体积分割）在胎儿MRI分析中各有局限，无法同时兼顾运动分析和形状细节。

Method: 构建3D胎儿统计身体模型，迭代估计图像空间中的姿势和规范姿势空间中的形状。

Result: 模型在未见过的胎儿形状上表现良好，表面对齐误差为3.2 mm。

Conclusion: 该模型首次实现了3D胎儿统计身体建模，为产前诊断中的运动和形状分析提供了新工具。

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [48] [Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation](https://arxiv.org/abs/2506.17869)
*Xiaodong Guo,Zi'ang Lin,Luwen Hu,Zhihong Deng,Tong Liu,Wujie Zhou*

Main category: cs.CV

TL;DR: CM-SSM是一种高效的RGB-热语义分割架构，通过跨模态状态空间建模（SSM）方法，显著降低了计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 多源数据处理（如基于Transformer的方法）在资源受限系统中计算开销大，限制了性能提升。

Method: 提出CM-SS2D模块建立RGB与热模态间的SSM，以及CM-SSA模块整合全局关联与局部空间特征。

Result: 在CART和PST900数据集上表现优异，计算复杂度线性于图像分辨率。

Conclusion: CM-SSM在减少参数和计算成本的同时，实现了最先进的性能，并具有泛化能力。

Abstract: The integration of RGB and thermal data can significantly improve semantic
segmentation performance in wild environments for field robots. Nevertheless,
multi-source data processing (e.g. Transformer-based approaches) imposes
significant computational overhead, presenting challenges for
resource-constrained systems. To resolve this critical limitation, we
introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture
leveraging a cross-modal state space modeling (SSM) approach. Our framework
comprises two key components. First, we introduced a cross-modal
2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal
modalities, which constructs cross-modal visual sequences and derives hidden
state representations of one modality from the other. Second, we developed a
cross-modal state space association (CM-SSA) module that effectively integrates
global associations from CM-SS2D with local spatial features extracted through
convolutional operations. In contrast with Transformer-based approaches, CM-SSM
achieves linear computational complexity with respect to image resolution.
Experimental results show that CM-SSM achieves state-of-the-art performance on
the CART dataset with fewer parameters and lower computational cost. Further
experiments on the PST900 dataset demonstrate its generalizability. Codes are
available at https://github.com/xiaodonguo/CMSSM.

</details>


### [49] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgVidLM是首个专为精细手术视频理解设计的视频语言模型，通过SVU-31K数据集和StageFocus机制显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频语言模型缺乏针对手术视频的精细理解能力，无法分析手术过程中的特定细节。

Method: 提出SurgVidLM模型，构建SVU-31K数据集，引入StageFocus机制和Multi-frequency Fusion Attention技术。

Result: SurgVidLM在全面和精细视频理解任务中显著优于现有模型。

Conclusion: SurgVidLM填补了手术视频精细理解的空白，展示了其在复杂手术场景中的优越性。

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [50] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为StainPIDR的染色归一化方法，通过解耦图像的结构特征和颜色特征，并利用交叉注意力机制重新染色，有效解决了病理图像颜色差异问题。


<details>
  <summary>Details</summary>
Motivation: 病理图像的颜色差异会影响计算机辅助诊断系统的性能，因此需要一种方法来消除这种差异。

Method: 通过解耦图像的结构特征和向量量化的颜色特征，利用交叉注意力机制重新染色结构特征，并设计模板图像选择算法优化目标颜色。

Result: 实验验证了StainPIDR和模板选择算法的有效性，表明该方法在染色归一化任务中表现良好。

Conclusion: StainPIDR能够有效解决病理图像的颜色差异问题，代码将公开。

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [51] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: 提出了一种结合SAR-光学特征融合和深度学习的云注意力重建框架，用于生成无云光学图像。


<details>
  <summary>Details</summary>
Motivation: 云污染严重影响光学卫星图像的可用性，阻碍了环境监测、灾害响应和土地利用分析等关键应用。

Method: 采用注意力驱动的特征融合机制，将SAR的结构信息与光学数据的光谱特征对齐，并结合云感知模型更新策略，通过自适应损失加权优先处理云遮挡区域。

Result: 实验结果显示，该方法在PSNR（31.01 dB）、SSIM（0.918）和MAE（0.017）上优于现有方法。

Conclusion: 该框架能够生成高保真、空间和光谱一致的无云光学图像，具有显著的应用价值。

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [52] [Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation](https://arxiv.org/abs/2506.17891)
*Jiahao Lu,Jiacheng Deng*

Main category: cs.CV

TL;DR: Relation3D通过增强点云实例分割中的关系建模，提出自适应超点聚合模块和对比学习引导的超点细化模块，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法主要关注场景特征与查询特征的外部关系，缺乏对场景特征内部及查询特征之间关系的有效建模。

Method: 提出自适应超点聚合模块和对比学习引导的超点细化模块，增强超点特征表示；引入关系感知的自注意力机制，结合位置和几何关系。

Result: 在ScanNetV2、ScanNet++、ScanNet200和S3DIS数据集上表现出优越性能。

Conclusion: Relation3D通过改进关系建模，显著提升了点云实例分割的效果。

Abstract: 3D instance segmentation aims to predict a set of object instances in a
scene, representing them as binary foreground masks with corresponding semantic
labels. Currently, transformer-based methods are gaining increasing attention
due to their elegant pipelines and superior predictions. However, these methods
primarily focus on modeling the external relationships between scene features
and query features through mask attention. They lack effective modeling of the
internal relationships among scene features as well as between query features.
In light of these disadvantages, we propose \textbf{Relation3D: Enhancing
Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we
introduce an adaptive superpoint aggregation module and a contrastive
learning-guided superpoint refinement module to better represent superpoint
features (scene features) and leverage contrastive learning to guide the
updates of these features. Furthermore, our relation-aware self-attention
mechanism enhances the capabilities of modeling relationships between queries
by incorporating positional and geometric relationships into the self-attention
mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and
S3DIS datasets demonstrate the superior performance of Relation3D.

</details>


### [53] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: 本文构建了首个真实工业场景下的传送带裂纹检测数据集（BeltCrack14ks和BeltCrack9kd），并提出了一种基于时空频三域特征分层融合的基线方法，验证了数据集的有效性和方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 传送带裂纹对工业安全和效率至关重要，但现有数据集多为路面裂纹或合成数据，缺乏真实工业场景的数据。本文旨在填补这一空白。

Method: 通过真实工厂场景构建数据集，并提出一种基于时空频三域特征分层融合的基线方法。

Result: 实验证明数据集的有效性，且基线方法优于其他类似检测方法。

Conclusion: 本文的数据集和方法为传送带裂纹检测领域提供了重要资源和技术支持。

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [54] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: EgoWorld是一个新颖的两阶段框架，通过外中心观察重建自我中心视图，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D线索、多视图同步设置和不切实际的假设，限制了外中心到自我中心视图的转换效果。

Method: EgoWorld通过估计外中心深度图重建点云，将其重投影到自我中心视角，并应用扩散修复生成密集、语义一致的图像。

Result: 在H2O和TACO数据集上，EgoWorld实现了最先进的性能，并展示了对新对象、动作、场景和主题的鲁棒泛化能力。

Conclusion: EgoWorld不仅在标记数据上表现优异，还在未标记的真实世界示例中显示出潜力。

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [55] [PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs](https://arxiv.org/abs/2506.17901)
*Yixuan Wu,Yang Zhang,Jian Wu,Philip Torr,Jindong Gu*

Main category: cs.CV

TL;DR: MMGrounded-PostAlign框架通过多模态对齐增强MLLMs的视觉理解能力，减少幻觉。


<details>
  <summary>Details</summary>
Motivation: MLLMs在视觉语言任务中过度依赖虚假相关性，需提升视觉信息利用能力。

Method: 引入多模态基础模块（视觉和文本基础）和负拒绝机制，选择性调整推理策略。

Result: 在多个基准测试中显著提升了细粒度视觉理解和幻觉抑制能力。

Conclusion: MMGrounded-PostAlign有效解决了MLLMs的视觉依赖问题，提升了模型性能。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such
as image captioning and visual question answering. However, they often suffer
from over-reliance on spurious correlations, primarily due to linguistic priors
that distract the model from leveraging actual visual information. To address
these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment
framework designed to enhance the visual understanding capabilities and
mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal
grounding module for both visual grounding, which identifies the referred
object in the image, and textual grounding, which generates the rationale for
the final answer, ensuring that outputs are anchored in both visual and textual
evidence. To mitigate the hallucinations, we introduce a negative rejection
mechanism in the visual grounding module to distinguish grounded entities from
non-existent objects influenced by linguistic biases. On the textual grounding
side, we propose a selective reasoning mechanism that adjusts the model's
reasoning strategy based on query complexity. Extensive evaluations are
conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench
showing significant improvements in fine-grained visual understanding and
hallucination suppression.

</details>


### [56] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: 提出了一种名为CEDO的新框架，通过三种机制（MHO、GMS、DLR）从因果和效应角度全面缓解医学视觉问答模型中的语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉问答模型存在语言偏差问题，即问题类型与答案类别之间建立了虚假关联。

Method: CEDO框架包含三种机制：MHO（模态驱动异质优化）、GMS（梯度引导模态协同）和DLR（分布适应损失重缩放），分别从不同角度缓解语言偏差。

Result: 在多个传统和偏差敏感基准测试中，CEDO表现出优于现有方法的鲁棒性。

Conclusion: CEDO通过综合优化机制有效缓解了医学视觉问答中的语言偏差问题，提升了模型的鲁棒性。

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [57] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: 提出一种基于3D立体视觉的交互系统管道，用于复杂环境中的场景理解和任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D相机在复杂环境中不可靠，需要更稳健的解决方案。

Method: 融合多个3D相机进行全场景重建，支持事件识别、目标跟踪和通知等功能。

Result: 初步实验展示了系统的可行性，并提出了反馈机制以优化决策。

Conclusion: 提出了未来将管道投入生产的路线图。

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [58] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/abs/2506.17912)
*Chuhao Jin,Haosen Li,Bingzi Zhang,Che Liu,Xiting Wang,Ruihua Song,Wenbing Huang,Ying Qin,Fuzheng Zhang,Di Zhang*

Main category: cs.CV

TL;DR: PlanMoGPT通过渐进式规划和流增强的细粒度运动标记化，解决了LLM在文本到运动生成中的性能瓶颈，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在文本到运动生成中表现不佳，主要由于运动标记化的粒度问题导致局部依赖和全局语义对齐的冲突。

Method: 提出PlanMoGPT框架，结合渐进式规划和流增强的细粒度标记化，通过分层生成运动标记并优化细节保留。

Result: 在长序列生成中FID分数提升63.8%，运动多样性提高49.9%，优于现有方法。

Conclusion: PlanMoGPT成功解决了多样性-质量的权衡问题，为文本到运动生成设定了新标准。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [59] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出了一种新的无监督域自适应方法，结合ResNet和FPN架构，通过新型损失函数组合提升模型在目标域的性能和训练速度。


<details>
  <summary>Details</summary>
Motivation: 现有对抗域自适应方法在多模态分布对齐上效果不佳，需改进以应对自然图像中的尺度、噪声和风格变化。

Method: 结合ResNet和FPN架构，设计新型损失函数组合，优化多模态分布对齐和训练效率。

Result: 在Office-Home、Office-31和VisDA-2017数据集上优于现有CNN方法，DomainNet上表现相当。

Conclusion: 该方法在多模态分布对齐和训练效率上表现优异，适用于自然图像的域自适应任务。

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [60] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Main category: cs.CV

TL;DR: 提出了一种新的医学视觉问答方法，通过视觉定位和可验证奖励机制提高答案可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答方法在答案可靠性和可解释性方面存在不足，影响临床决策的信任度。

Method: 提出Thinking with Visual Grounding (ThinkVG)数据集，分解答案生成过程为中间推理步骤，并引入可验证奖励机制进行强化学习。

Result: 方法仅用八分之一训练数据即达到可比性能，显著提升效率和效果。

Conclusion: 该方法通过视觉定位和可验证奖励机制，显著提高了医学视觉问答的可靠性和可解释性。

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [61] [SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models](https://arxiv.org/abs/2506.17944)
*Fei Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）增强的遥感变化检测方法（SegChange-R1），通过整合文本描述信息提升检测能力，并设计了基于线性注意力的空间变换模块（BEV）解决模态不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 遥感变化检测在多个领域有广泛应用，但现有方法在检测能力和模态对齐方面存在不足。

Method: 结合LLM增强推理，整合文本信息；设计BEV模块统一不同时间视角的特征。

Result: 在四个常用数据集上表现优于现有方法，并构建了首个无人机视角的建筑变化检测数据集（DVCD）。

Conclusion: SegChange-R1通过LLM增强和BEV模块显著提升了变化检测的性能和效率。

Abstract: Remote sensing change detection is widely used in a variety of fields such as
urban planning, terrain and geomorphology analysis, and environmental
monitoring, mainly by analyzing the significant change differences of features
(e.g., building changes) in the same spatial region at different time phases.
In this paper, we propose a large language model (LLM) augmented inference
approach (SegChange-R1), which enhances the detection capability by integrating
textual descriptive information and aims at guiding the model to segment the
more interested change regions, thus accelerating the convergence speed.
Moreover, we design a spatial transformation module (BEV) based on linear
attention, which solves the problem of modal misalignment in change detection
by unifying features from different temporal perspectives onto the BEV space.
In addition, we construct the first dataset for building change detection from
UAV viewpoints (DVCD ), and our experiments on four widely-used change
detection datasets show a significant improvement over existing methods. The
code and pre-trained models are available in
https://github.com/Yu-Zhouz/SegChange-R1.

</details>


### [62] [Classification of Tents in Street Bazaars Using CNN](https://arxiv.org/abs/2506.17946)
*Azamat Ibragimov,Ruslan Isaev,Remudin Reshid Mekuria,Gulnaz Gimaletdinova,Dim Shaiakhmetov*

Main category: cs.CV

TL;DR: 论文提出了一种改进的深度学习模型，用于分类街头集市中的帐篷，比较了自定义CNN和EfficientNetB0的性能，结果显示EfficientNetB0表现更优。


<details>
  <summary>Details</summary>
Motivation: 街头集市是许多地区的重要经济中心，但其非结构化特性使得帐篷等基础设施的自动分类具有挑战性。手动分类效率低下，因此需要自动化解决方案。

Method: 研究使用了自定义CNN和EfficientNetB0模型，训练数据包括126张原始照片及其增强生成的图像，并通过多种性能指标（如准确率、精确率、召回率等）进行评估。

Result: 自定义CNN的准确率为92.8%，而EfficientNetB0达到98.4%，表明迁移学习在集市图像分类中效果显著。

Conclusion: 使用预训练模型（如EfficientNetB0）可以显著提高分类准确率和泛化能力。

Abstract: This research paper proposes an improved deep learning model for classifying
tents in street bazaars, comparing a custom Convolutional Neural Network (CNN)
with EfficientNetB0. This is a critical task for market organization with a
tent classification, but manual methods in the past have been inefficient.
Street bazaars represent a vital economic hub in many regions, yet their
unstructured nature poses significant challenges for the automated
classification of market infrastructure, such as tents. In Kyrgyzstan, more
than a quarter of the country's GDP is derived from bazaars. While CNNs have
been widely applied to object recognition, their application to bazaar-specific
tasks remains underexplored. Here, we build upon our original approach by
training on an extended set of 126 original photographs that were augmented to
generate additional images. This dataset is publicly available for download on
Kaggle. A variety of performance metrics, such as accuracy, precision, recall,
F1 score, and mean average precision (mAP), were used to assess the models
comparatively, providing a more extensive analysis of classification
performance.
  The results show that the CNN custom model achieved 92.8% accuracy, and
EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of
transfer learning in the bazaar image classification. Also, when analyzing the
confusion matrix, the analysis reveals the weaknesses and strengths of each
model. These findings suggest that using a pre-trained model such as
EfficientNetB0 significantly improves classification accuracy and
generalization.

</details>


### [63] [Mobile Image Analysis Application for Mantoux Skin Test](https://arxiv.org/abs/2506.17954)
*Liong Gele,Tan Chye Cheah*

Main category: cs.CV

TL;DR: 开发了一款移动应用，利用ARCore和机器学习算法（如DeepLabv3）改进Mantoux皮肤测试（TST），提高LTBI诊断的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统TST方法存在随访率低、患者不适和主观解释问题，导致误诊和治疗延迟。

Method: 应用结合图像处理技术（ARCore）和机器学习算法（DeepLabv3），使用缩放贴纸作为参考对象，通过边缘检测算法提高测量精度。

Result: 与标准临床实践相比，应用显著提高了诊断的准确性和可靠性。

Conclusion: 该应用自动化并标准化TST评估，提升了结核病诊断的可及性和效率，未来将优化算法和扩展功能。

Abstract: This paper presents a newly developed mobile application designed to diagnose
Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).
Traditional TST methods often suffer from low follow-up return rates, patient
discomfort, and subjective manual interpretation, particularly with the
ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,
previous developed mobile applications that used 3D reconstruction, this app
utilizes scaling stickers as reference objects for induration measurement. This
mobile application integrates advanced image processing technologies, including
ARCore, and machine learning algorithms such as DeepLabv3 for robust image
segmentation and precise measurement of skin indurations indicative of LTBI.
The system employs an edge detection algorithm to enhance accuracy. The
application was evaluated against standard clinical practices, demonstrating
significant improvements in accuracy and reliability. This innovation is
crucial for effective tuberculosis management, especially in resource-limited
regions. By automating and standardizing TST evaluations, the application
enhances the accessibility and efficiency of TB di-agnostics. Future work will
focus on refining machine learning models, optimizing measurement algorithms,
expanding functionalities to include comprehensive patient data management, and
enhancing ARCore's performance across various lighting conditions and
operational settings.

</details>


### [64] [ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty](https://arxiv.org/abs/2506.17958)
*Xiangyuan Peng,Miao Tang,Huawei Sun,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 提出了一种利用4D雷达运动状态和跨模态不确定性的LiDAR检测框架，以解决多模态融合中的错位问题，并在VoD数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: LiDAR和4D雷达在自动驾驶中各有优势，但多模态融合中的错位问题常被忽视。本文旨在通过结合4D雷达的运动信息和跨模态不确定性来提升LiDAR检测性能。

Method: 使用动态运动感知编码模块提取4D雷达的运动信息，并通过估计边界框的实例级不确定性来减少跨模态错位，优化LiDAR预测。

Result: 在VoD数据集上，整个区域的mAP达到74.89%，驾驶走廊内达到88.70%，实时推理速度为30.02 FPS。

Conclusion: 该方法有效解决了多模态融合中的错位问题，显著提升了LiDAR检测性能，同时保持了实时性。

Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While
LiDAR provides rich spatial information, 4D radar offers velocity measurement
and remains robust under adverse conditions. As a result, increasing studies
have focused on the 4D radar-LiDAR fusion method to enhance the perception.
However, the misalignment between different modalities is often overlooked. To
address this challenge and leverage the strengths of both modalities, we
propose a LiDAR detection framework enhanced by 4D radar motion status and
cross-modal uncertainty. The object movement information from 4D radar is first
captured using a Dynamic Motion-Aware Encoding module during feature extraction
to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties
of bounding boxes are estimated to mitigate the cross-modal misalignment and
refine the final LiDAR predictions. Extensive experiments on the View-of-Delft
(VoD) dataset highlight the effectiveness of our method, achieving
state-of-the-art performance with the mAP of 74.89% in the entire area and
88.70% within the driving corridor while maintaining a real-time inference
speed of 30.02 FPS.

</details>


### [65] [BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP](https://arxiv.org/abs/2506.17969)
*Chenyue Song,Chen Hui,Wei Zhang,Haiqi Zhu,Shaohui Liu,Hong Huang,Feng Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的自底向上图像质量评估方法BPCLIP，通过多尺度交叉注意力模块和图像质量形容词增强语义与感知的联系，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常线性融合多尺度特征，未能充分捕捉失真对语义内容的影响。

Method: 利用CLIP模型，设计自底向上的多尺度交叉注意力模块，结合40个图像质量形容词生成图像内在质量表示。

Result: 在多数全参考和无参考IQA基准测试中取得优异结果，并表现出更强的鲁棒性。

Conclusion: BPCLIP方法通过结合CLIP和自底向上特征提取，有效提升了图像质量评估的性能和鲁棒性。

Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of
images based on human subjective perception. Existing methods generally combine
multiscale features to achieve high performance, but most rely on
straightforward linear fusion of these features, which may not adequately
capture the impact of distortions on semantic content. To address this, we
propose a bottom-up image quality assessment approach based on the Contrastive
Language-Image Pre-training (CLIP, a recently proposed model that aligns images
and text in a shared feature space), named BPCLIP, which progressively extracts
the impact of low-level distortions on high-level semantics. Specifically, we
utilize an encoder to extract multiscale features from the input image and
introduce a bottom-up multiscale cross attention module designed to capture the
relationships between shallow and deep features. In addition, by incorporating
40 image quality adjectives across six distinct dimensions, we enable the
pre-trained CLIP text encoder to generate representations of the intrinsic
quality of the image, thereby strengthening the connection between image
quality perception and human language. Our method achieves superior results on
most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while
demonstrating greater robustness.

</details>


### [66] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/abs/2506.17975)
*Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 论文提出了一种通过最大化多样性来保护隐私的合成数据生成框架，性能接近真实数据。


<details>
  <summary>Details</summary>
Motivation: 合成数据在医学影像中具有隐私保护潜力，但现有方法在合法性和性能上存在不足。

Method: 提出了一种通用的扩散模型训练框架，生成非个人化的合成数据集。

Result: 合成数据性能接近真实数据（差距1%以内），显著优于不保障隐私的现有方法。

Conclusion: 通过多样性最大化实现隐私保护，为合成数据的合法共享提供了新思路。

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it
nearly indistinguishable from real data, offering great promise for
privacy-preserving data sharing in medical imaging. However, fully synthetic
datasets still suffer from significant limitations: First and foremost, the
legal aspect of sharing synthetic data is often neglected and data regulations,
such as the GDPR, are largley ignored. Secondly, synthetic models fall short of
matching the performance of real data, even for in-domain downstream
applications. Recent methods for image generation have focused on maximising
image diversity instead of fidelity solely to improve the mode coverage and
therefore the downstream performance of synthetic data. In this work, we shift
perspective and highlight how maximizing diversity can also be interpreted as
protecting natural persons from being singled out, which leads to predicate
singling-out (PSO) secure synthetic datasets. Specifically, we propose a
generalisable framework for training diffusion models on personal data which
leads to unpersonal synthetic datasets achieving performance within one
percentage point of real-data models while significantly outperforming
state-of-the-art methods that do not ensure privacy. Our code is available at
https://github.com/MischaD/Trichotomy.

</details>


### [67] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: 提出了一种快速可靠的神经逆运动学框架，用于从3D关键点实时捕捉人体运动。


<details>
  <summary>Details</summary>
Motivation: 无标记运动捕捉虽然灵活且成本低，但通常计算需求高且推理速度慢，限制了实时应用。

Method: 详细描述了网络架构、训练方法和推理过程，并通过消融研究支持关键设计决策。

Result: 框架在定性和定量上均进行了评估。

Conclusion: 该框架能够高效地实现实时人体运动捕捉。

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [68] [OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model](https://arxiv.org/abs/2506.18006)
*Shuaiyu Chen,Fu Wang,Peng Ren,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: OSDMamba是一种基于Mamba的新型架构，用于解决油污检测中的小区域检测和类别不平衡问题，通过选择性扫描机制和非对称解码器提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于CNN的方法在油污检测中因感受野有限和全局上下文信息捕捉不足而表现不佳，且样本不平衡问题影响准确性。

Method: 提出OSDMamba架构，利用Mamba的选择性扫描机制扩展感受野，并结合非对称解码器和深度监督增强多尺度特征融合。

Result: 在两个公开数据集上，OSDMamba的性能分别提升了8.9%和11.8%，达到最先进水平。

Conclusion: OSDMamba通过结合Mamba的优势和创新的解码器设计，显著提升了油污检测的准确性和对小区域的敏感性。

Abstract: Semantic segmentation is commonly used for Oil Spill Detection (OSD) in
remote sensing images. However, the limited availability of labelled oil spill
samples and class imbalance present significant challenges that can reduce
detection accuracy. Furthermore, most existing methods, which rely on
convolutional neural networks (CNNs), struggle to detect small oil spill areas
due to their limited receptive fields and inability to effectively capture
global contextual information. This study explores the potential of State-Space
Models (SSMs), particularly Mamba, to overcome these limitations, building on
their recent success in vision applications. We propose OSDMamba, the first
Mamba-based architecture specifically designed for oil spill detection.
OSDMamba leverages Mamba's selective scanning mechanism to effectively expand
the model's receptive field while preserving critical details. Moreover, we
designed an asymmetric decoder incorporating ConvSSM and deep supervision to
strengthen multi-scale feature fusion, thereby enhancing the model's
sensitivity to minority class samples. Experimental results show that the
proposed OSDMamba achieves state-of-the-art performance, yielding improvements
of 8.9% and 11.8% in OSD across two publicly available datasets.

</details>


### [69] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/abs/2506.18021)
*Chi Xie,Shuang Liang,Jie Li,Feng Zhu,Rui Zhao,Yichen Wei,Shengjie Zhao*

Main category: cs.CV

TL;DR: 该论文提出了一种评估和增强人-物交互（HOI）检测模型在分布偏移下鲁棒性的方法，包括创建新基准、分析现有模型不足，并提出两种改进策略。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测研究集中在理想图像和自然分布上，忽略了实际场景中的分布偏移问题，限制了模型的实用性。

Method: 提出自动化方法创建鲁棒性评估基准，分析40多个现有模型，并提出跨域数据增强和特征融合策略。

Result: 实验表明，所提方法显著提高了多种模型的鲁棒性，并在标准基准上也有提升。

Conclusion: 论文通过新基准和改进策略解决了HOI检测的鲁棒性问题，为实际应用提供了支持。

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [70] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2是PP-DocBee的升级版，通过改进数据质量、视觉特征融合和推理方法，提升了中文商业文档的理解性能，并显著降低了推理延迟。


<details>
  <summary>Details</summary>
Motivation: 解决PP-DocBee在多模态文档理解中的局限性，提升模型性能和效率。

Method: 采用数据质量优化策略、改进视觉特征融合方法，并优化推理流程。

Result: 在内部基准测试中性能提升11.4%，推理延迟降低73%。

Conclusion: PP-DocBee2通过技术创新显著提升了多模态文档理解的性能，为实际应用提供了高效解决方案。

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [71] [MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2506.18028)
*Junjian Li,Hulin Kuang,Jin Liu,Hailin Yue,Mengshen He,Jianxin Wang*

Main category: cs.CV

TL;DR: 提出了一种名为MiCo的新型多实例学习框架，通过上下文感知聚类增强WSI分析中的跨区域组织相关性。


<details>
  <summary>Details</summary>
Motivation: 解决WSI中组织分布分散和跨区域空间交互难以建模的问题。

Method: 使用聚类提取形态模式，通过Cluster Route模块动态链接相同组织类型的实例，Cluster Reducer模块整合冗余锚点。

Result: 在九个大型公共癌症数据集上表现优于现有方法。

Conclusion: MiCo有效增强了WSI分析中的组织相关性和语义关联。

Abstract: Multiple instance learning (MIL) has shown significant promise in
histopathology whole slide image (WSI) analysis for cancer diagnosis and
prognosis. However, the inherent spatial heterogeneity of WSIs presents
critical challenges, as morphologically similar tissue types are often
dispersed across distant anatomical regions. Conventional MIL methods struggle
to model these scattered tissue distributions and capture cross-regional
spatial interactions effectively. To address these limitations, we propose a
novel Multiple instance learning framework with Context-Aware Clustering
(MiCo), designed to enhance cross-regional intra-tissue correlations and
strengthen inter-tissue semantic associations in WSIs. MiCo begins by
clustering instances to distill discriminative morphological patterns, with
cluster centroids serving as semantic anchors. To enhance cross-regional
intra-tissue correlations, MiCo employs a Cluster Route module, which
dynamically links instances of the same tissue type across distant regions via
feature similarity. These semantic anchors act as contextual hubs, propagating
semantic relationships to refine instance-level representations. To eliminate
semantic fragmentation and strengthen inter-tissue semantic associations, MiCo
integrates a Cluster Reducer module, which consolidates redundant anchors while
enhancing information exchange between distinct semantic groups. Extensive
experiments on two challenging tasks across nine large-scale public cancer
datasets demonstrate the effectiveness of MiCo, showcasing its superiority over
state-of-the-art methods. The code is available at
https://github.com/junjianli106/MiCo.

</details>


### [72] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种结合预训练冻结LLM层的混合结构（LLM4Seg），用于医学图像分割任务，显著提升了性能且参数增加极少。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLM的语义理解能力是否可以迁移到视觉任务中，尤其是医学图像分割。

Method: 设计了一个简单的混合结构，将冻结的预训练LLM层集成到CNN编码器-解码器分割框架中。

Result: 在多种模态（超声、皮肤镜、息肉镜、CT）上验证了性能提升，且对不同LLM（如LLaMA和DeepSeek）均有效。

Conclusion: LLM的语义感知能力可以增强分割任务，提供更好的全局理解和局部建模能力。

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [73] [CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images](https://arxiv.org/abs/2506.18042)
*Dongdong Meng,Sheng Li,Hao Wu,Suqing Tian,Wenjun Ma,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: CmFNet是一种新型的3D弱监督跨模态医学图像分割方法，通过整合多模态信息和混合监督学习策略，显著提升了分割性能，并在实验中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 高质量的密集标注成本高且耗时，弱监督学习虽然效率更高，但稀疏标注会导致性能下降和过拟合。

Method: CmFNet包含三个主要组件：模态特定特征学习网络、跨模态特征学习网络和混合监督学习策略，通过多模态信息整合和多种监督方式提升性能。

Result: 在临床数据集和公开数据集上的实验表明，CmFNet优于现有的弱监督方法，甚至在某些情况下优于全监督方法。

Conclusion: CmFNet有效解决了稀疏标注带来的问题，为临床治疗提供了有力支持，适用于多种医学专家。

Abstract: Accurate automatic medical image segmentation relies on high-quality, dense
annotations, which are costly and time-consuming. Weakly supervised learning
provides a more efficient alternative by leveraging sparse and coarse
annotations instead of dense, precise ones. However, segmentation performance
degradation and overfitting caused by sparse annotations remain key challenges.
To address these issues, we propose CmFNet, a novel 3D weakly supervised
cross-modal medical image segmentation approach. CmFNet consists of three main
components: a modality-specific feature learning network, a cross-modal feature
learning network, and a hybrid-supervised learning strategy. Specifically, the
modality-specific feature learning network and the cross-modal feature learning
network effectively integrate complementary information from multi-modal
images, enhancing shared features across modalities to improve segmentation
performance. Additionally, the hybrid-supervised learning strategy guides
segmentation through scribble supervision, intra-modal regularization, and
inter-modal consistency, modeling spatial and contextual relationships while
promoting feature alignment. Our approach effectively mitigates overfitting,
delivering robust segmentation results. It excels in segmenting both
challenging small tumor regions and common anatomical structures. Extensive
experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset
(including CT and MR imaging) and the publicly available CT Whole Abdominal
Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly
supervised methods. In addition, our approach also outperforms fully supervised
methods when full annotation is used. Our approach can facilitate clinical
therapy and benefit various specialists, including physicists, radiologists,
pathologists, and oncologists.

</details>


### [74] [CLGRPO: Reasoning Ability Enhancement for Small VLMs](https://arxiv.org/abs/2506.18048)
*Fanyi Wang,Binzhi Dong,Haotian Hu,Jinjin Xu,Zhiwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种增量训练策略（Incremental Training Strategy），通过自监督的链式思维（COT）数据构建系统和分阶段训练，显著提升了小型视觉语言模型（SVLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 小型视觉语言模型（SVLMs）因参数规模小（≤2B）而成本低，但推理能力受限。本文旨在通过后训练优化提升其推理能力。

Method: 1. 构建自监督COT数据系统；2. 分四阶段训练：SFT注入知识、GRPO对齐格式、GRPO增强推理、CLGRPO约束空间。

Result: 在EMOSet-118K数据集上，1B SVLM的准确率提升2.77，召回率提升0.69，性能接近8B模型。

Conclusion: 增量训练策略有效提升了SVLMs的推理能力，使其在低成本下达到接近大模型的性能。

Abstract: Small Vision Language Models (SVLMs) generally refer to models with parameter
sizes less than or equal to 2B. Their low cost and power consumption
characteristics confer high commercial value. However, their reasoning
abilities are limited by the number of parameters. To address this issue, this
paper proposes a post-training optimization paradigm called the Incremental
Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we
constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,
which leverages multiple LVLMs with 7B parameters or more to transform original
data into COT data in a self-supervised manner. Our proposed Incremental
Training Strategy consists of four stages. Stage 1 injects domain knowledge by
performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT
data. Stage 2 aligns the COT data format by conducting a small amount of Group
Relative Policy Optimization (GRPO) training constrained only by format rewards
on the COT data. Stage 3 enhances reasoning ability by applying GRPO training
on the COT data with constraints on both format and accuracy rewards. The
resulting model shows significant improvement compared to the baseline. Stage 4
addresses the limited capacity of the SVLMs and the weak ability to capture
complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture
space of the training process. We conducted extensive comparative and ablation
experiments on the abstract semantic recognition dataset EMOSet-118K.
Experimental results demonstrate that our method significantly improves the
reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the
original data, accuracy increased by 2.77 and recall by 0.69, achieving
performance comparable to that of 8B models.

</details>


### [75] [Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes](https://arxiv.org/abs/2506.18060)
*Olivia Zumsteg,Nico Graf,Aaron Haeusler,Norbert Kirchgessner,Nicola Storni,Lukas Roth,Andreas Hund*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的2D图像体积估计方法，用于小麦穗的非破坏性体积估计，通过结合DINOv2和LSTM网络，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于2D图像在田间条件下存在深度信息丢失、投影失真和遮挡等问题，传统方法难以准确估计小麦穗的体积。

Method: 采用自监督Vision Transformer（DINOv2）与单向LSTM网络结合的迁移学习管道，通过深度监督学习增强模型的泛化能力。

Result: 在六视角室内图像上，模型MAPE为6.46%，优于传统方法（9.36%和13.98%）；在单图像田间数据上，MAPE为10.82%。

Conclusion: 深度学习方法在复杂几何形状（如小麦穗）的体积估计中表现优于传统几何方法，且通过域适应可提升田间数据的准确性。

Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB
images presents inherent challenges due to the loss of depth information,
projection distortions, and occlusions under field conditions. In this work, we
explore multiple approaches for non-destructive volume estimation of wheat
spikes, using RGB image sequences and structured-light 3D scans as ground truth
references. Due to the complex geometry of the spikes, we propose a neural
network approach for volume estimation in 2D images, employing a transfer
learning pipeline that combines DINOv2, a self-supervised Vision Transformer,
with a unidirectional Long Short-Term Memory (LSTM) network. By using deep
supervision, the model is able to learn more robust intermediate
representations, which enhances its generalisation ability across varying
evaluation sequences. We benchmark our model against two conventional
baselines: a 2D area-based projection and a geometric reconstruction using
axis-aligned cross-sections. Our deep supervised model achieves a mean absolute
percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the
area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on
field-based single-image data enables domain adaptation, yielding a MAPE of
10.82%. We demonstrate that object shape significantly impacts volume
prediction accuracy, with irregular geometries such as wheat spikes posing
greater challenges for geometric methods compared to our deep learning
approach.

</details>


### [76] [Training-free Test-time Improvement for Explainable Medical Image Classification](https://arxiv.org/abs/2506.18070)
*Hangzhou He,Jiachen Tang,Lei Zhu,Kaiwen Li,Yanye Lu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的概念混淆识别策略，通过少量新数据和图像级标签提升跨域性能。


<details>
  <summary>Details</summary>
Motivation: 解决概念瓶颈模型在部署到新环境时因概念级偏移和缺乏专家标注概念标签而面临的挑战。

Method: 利用少量新数据（如每类4张图像）和图像级标签，通过屏蔽误激活的混淆概念和增强未充分激活的判别概念来优化性能。

Result: 在皮肤和白细胞图像上验证了方法的有效性。

Conclusion: 该方法在不牺牲源域准确性的情况下，提升了跨域性能，且无需额外训练。

Abstract: Deep learning-based medical image classification techniques are rapidly
advancing in medical image analysis, making it crucial to develop accurate and
trustworthy models that can be efficiently deployed across diverse clinical
scenarios. Concept Bottleneck Models (CBMs), which first predict a set of
explainable concepts from images and then perform classification based on these
concepts, are increasingly being adopted for explainable medical image
classification. However, the inherent explainability of CBMs introduces new
challenges when deploying trained models to new environments. Variations in
imaging protocols and staining methods may induce concept-level shifts, such as
alterations in color distribution and scale. Furthermore, since CBM training
requires explicit concept annotations, fine-tuning models solely with
image-level labels could compromise concept prediction accuracy and
faithfulness - a critical limitation given the high cost of acquiring
expert-annotated concept labels in medical domains. To address these
challenges, we propose a training-free confusion concept identification
strategy. By leveraging minimal new data (e.g., 4 images per class) with only
image-level labels, our approach enhances out-of-domain performance without
sacrificing source domain accuracy through two key operations: masking
misactivated confounding concepts and amplifying under-activated discriminative
concepts. The efficacy of our method is validated on both skin and white blood
cell images. Our code is available at:
https://github.com/riverback/TF-TTI-XMed.

</details>


### [77] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: MUPA是一种多路径协作方法，通过结合视频定位、问答、答案反思和聚合，显著提升了Grounded VideoQA的定位准确性，同时保持了答案的准确性。


<details>
  <summary>Details</summary>
Motivation: 现代多模态模型依赖语言先验和虚假相关性，导致预测定位不准确。MUPA旨在解决这一问题。

Method: MUPA采用多路径协作设计，包括视频定位、问答、答案反思和聚合，通过三种不同的推理路径和反思代理实现一致的结果。

Result: MUPA在2B参数下优于7B规模的竞争对手，7B参数时在NExT-GQA和DeVE-QA上分别达到30.3%和47.4%的Acc@GQA。

Conclusion: MUPA通过多路径协作设计，显著提升了Grounded VideoQA的定位准确性和答案一致性，为可信的视频-语言理解提供了有效方法。

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [78] [TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving](https://arxiv.org/abs/2506.18084)
*Wenzhuo Liu,Yicheng Qiao,Zhen Wang,Qiannan Guo,Zilong Chen,Meihua Zhou,Xinran Li,Letian Wang,Zhiwei Li,Huaping Liu,Wenshuo Wang*

Main category: cs.CV

TL;DR: TEM^3-Learning 是一种高效的多模态多任务学习框架，通过两阶段架构优化驾驶辅助任务，实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有多任务学习方法受限于单模态和低效架构，无法满足驾驶辅助场景的全面理解和实时需求。

Method: 采用 MTS-Mamba 子网络提取时空特征，结合 MGMI 模块自适应整合多模态特征。

Result: 在 AIDE 数据集上实现 SOTA 精度，模型轻量（<6M 参数），推理速度达 142.32 FPS。

Conclusion: TEM^3-Learning 通过高效架构和多模态整合，显著提升驾驶辅助任务的性能。

Abstract: Multi-task learning (MTL) can advance assistive driving by exploring
inter-task correlations through shared representations. However, existing
methods face two critical limitations: single-modality constraints limiting
comprehensive scene understanding and inefficient architectures impeding
real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient
Multimodal Multi-task Learning), a novel framework that jointly optimizes
driver emotion recognition, driver behavior recognition, traffic context
recognition, and vehicle behavior recognition through a two-stage architecture.
The first component, the mamba-based multi-view temporal-spatial feature
extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal
scanning mechanism and global-local spatial attention to efficiently extract
low-cost temporal-spatial features from multi-view sequential images. The
second component, the MTL-based gated multimodal feature integrator (MGMI),
employs task-specific multi-gating modules to adaptively highlight the most
relevant modality features for each task, effectively alleviating the negative
transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model
achieves state-of-the-art accuracy across all four tasks, maintaining a
lightweight architecture with fewer than 6 million parameters and delivering an
impressive 142.32 FPS inference speed. Rigorous ablation studies further
validate the effectiveness of the proposed framework and the independent
contributions of each module. The code is available on
https://github.com/Wenzhuo-Liu/TEM3-Learning.

</details>


### [79] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: ShareGPT-4o-Image数据集和Janus-4o模型旨在开源多模态生成能力，提升文本到图像和文本加图像到图像的生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前领先的多模态生成模型（如GPT-4o-Image）是专有的，限制了开放研究。

Method: 使用GPT-4o生成的数据集训练Janus-4o模型，支持文本到图像和文本加图像到图像的生成。

Result: Janus-4o在文本到图像生成上超越前代模型，并首次实现文本加图像到图像的生成，仅需少量数据和短时间训练。

Conclusion: 开源ShareGPT-4o-Image数据集和Janus-4o模型，推动多模态生成技术的开放研究。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [80] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: SAG-VICReg改进VICReg，通过新训练技术提升泛化能力和全局语义捕捉，实验表现优于现有SSL方法。


<details>
  <summary>Details</summary>
Motivation: VICReg可能因依赖训练数据而泛化不足，需改进以提升对未见数据的表示能力。

Method: 提出SAG-VICReg，结合新训练技术增强全局语义捕捉和泛化能力。

Result: SAG-VICReg在泛化和全局语义指标上优于现有方法，同时保持局部指标竞争力。

Conclusion: SAG-VICReg有效解决泛化问题，并提出无标签的全局评估新指标。

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [81] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/abs/2506.18134)
*Quan Zhou,Gan Luo,Qiang Hu,Qingyong Zhang,Jinhua Zhang,Yinjiao Tian,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种对抗性扩散框架，用于合成高价值的假阳性样本，以改进结肠息肉检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型受限于数据的规模和多样性，且生成模型在增强多样性时忽略了假阳性的问题。

Method: 设计了区域噪声匹配策略和检测器引导的对抗性扩散攻击模块（DADA），用于合成多样化的背景和误导检测器的高价值假阳性样本。

Result: 在公开和内部数据集上验证了方法的优越性，合成数据使检测器的F1分数分别提高了至少2.6%和2.7%。

Conclusion: 该方法首次将对抗性扩散应用于病灶检测，为假阳性合成提供了新范式，提升了结肠癌筛查的可靠性。

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing
models are limited by the scale and diversity of available data. While
generative models show promise for data augmentation, current methods mainly
focus on enhancing polyp diversity, often overlooking the critical issue of
false positives. In this paper, we address this gap by proposing an adversarial
diffusion framework to synthesize high-value false positives. The extensive
variability of negative backgrounds presents a significant challenge in false
positive synthesis. To overcome this, we introduce two key innovations: First,
we design a regional noise matching strategy to construct a negative synthesis
space using polyp detection datasets. This strategy trains a negative-centric
diffusion model by masking polyp regions, ensuring the model focuses
exclusively on learning diverse background patterns. Second, we introduce the
Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs
the negative synthesis process to disrupt a pre-trained detector's decision,
guiding the negative-centric diffusion model to generate high-value,
detector-confusing false positives instead of low-value, ordinary backgrounds.
Our approach is the first to apply adversarial diffusion to lesion detection,
establishing a new paradigm for targeted false positive synthesis and paving
the way for more reliable clinical applications in colorectal cancer screening.
Extensive results on public and in-house datasets verify the superiority of our
method over the current state-of-the-arts, with our synthesized data improving
the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the
baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [82] [See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis](https://arxiv.org/abs/2506.18140)
*Ruinan Jin,Gexin Huang,Xinwei Shen,Qiong Zhang,Yan Shuo Tan,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 论文探讨了医学影像诊断中比较推理的重要性，提出了一种结合参考图像和临床提示的视觉语言模型（VLM）方法，显著提升了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学视觉语言模型缺乏比较推理机制，而通用视觉语言模型虽具备多图像比较能力，但缺乏医学领域知识。本研究旨在填补这一空白。

Method: 通过提供查询图像和匹配的参考图像，结合临床提示，对通用视觉语言模型进行监督微调（SFT），以增强诊断能力。

Result: 实验表明，该方法在多个医学视觉问答（VQA）任务中显著优于单图像基线模型。

Conclusion: 研究证明了比较分析在医学诊断中的临床价值，并为视觉语言模型利用参考图像提供了新策略。

Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that
mimic normal anatomy and exhibit significant inter-patient variability.
Clinicians routinely employ comparative reasoning-using reference images from
healthy controls or previous patient examinations-to discern subtle yet
diagnostically critical abnormalities. However, existing medical
vision-language models (VLMs) focus primarily on single-image or single-series
analyses and lack explicit mechanisms for comparative reasoning. Conversely,
general-purpose VLMs demonstrate strong multi-image comparative reasoning
capabilities but lack essential medical-domain knowledge to identify nuanced
clinical differences. This work aims to bridge this gap by exploring
clinically-inspired comparative analysis within VLMs, leveraging reference
images to enhance diagnostic accuracy. Through extensive empirical analysis, we
show that providing general-purpose VLMs with query and normative matched
reference images, accompanied by clinically-informed comparative prompts,
significantly improves diagnostic outcomes compared to single-image baselines,
especially after supervised finetuning (SFT). Our contributions highlight the
clinical relevance of comparative analysis introduce novel strategies for
leveraging reference images in VLMs, empirically demonstrate enhanced
performance across multiple medical visual question answering (VQA) tasks, and
provide theoretical insights into the efficacy of comparative image analysis in
medical diagnosis.

</details>


### [83] [Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry](https://arxiv.org/abs/2506.18157)
*Christian Sax,Jochen Kriegseis*

Main category: cs.CV

TL;DR: 提出了一种基于后处理的相位分离方法，用于散焦粒子追踪测速中的分散两相流，通过单相机设置实现两相粒子的3D定位。


<details>
  <summary>Details</summary>
Motivation: 传统方法在波长、尺寸或集合相关性上不适用时，需要一种更灵活的方法来区分两相粒子。

Method: 利用卷积神经网络（如Faster R-CNN和YOLOv4）训练模型，通过生成对抗网络生成标记数据。

Result: 在合成和真实数据集上实现了95-100%的高检测精度和分类准确率。

Conclusion: 证实了CNN在分散两相DPTV中稳健相位分离的可行性。

Abstract: This work investigates the feasibility of a post-processing-based approach
for phase separation in defocusing particle tracking velocimetry for dispersed
two-phase flows. The method enables the simultaneous 3D localization
determination of both tracer particles and particles of the dispersed phase,
using a single-camera setup. The distinction between phases is based on pattern
differences in defocused particle images, which arise from distinct light
scattering behaviors of tracer particles and bubbles or droplets. Convolutional
neural networks, including Faster R-CNN and YOLOv4 variants, are trained to
detect and classify particle images based on these pattern features. To
generate large, labeled training datasets, a generative adversarial network
based framework is introduced, allowing the generation of auto-labeled data
that more closely reflects experiment-specific visual appearance. Evaluation
across six datasets, comprising synthetic two-phase and real single- and
two-phase flows, demonstrates high detection precision and classification
accuracy (95-100%), even under domain shifts. The results confirm the viability
of using CNNs for robust phase separation in disperse two-phase DPTV,
particularly in scenarios where traditional wavelength-, size-, or ensemble
correlation-based methods are impractical.

</details>


### [84] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/abs/2506.18164)
*Varun Belagali,Pierre Marza,Srikar Yellapragada,Zilinghan Li,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Stergios Christodoulidis,Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: CDG-MAE是一种基于MAE的自监督方法，通过扩散模型生成多样合成视图，显著提升密集对应学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集对应学习中手动标注繁琐且不可扩展的问题，以及现有自监督方法训练数据不足的挑战。

Method: 利用图像条件扩散模型生成多样合成视图，并采用多锚点策略调节预训练任务难度。

Result: CDG-MAE显著优于仅依赖图像的MAE方法，并大幅缩小与基于视频方法的性能差距。

Conclusion: 通过合成视图和多锚点策略，CDG-MAE为密集对应学习提供了高效的自监督解决方案。

Abstract: Learning dense correspondences, critical for application such as video label
propagation, is hindered by tedious and unscalable manual annotation.
Self-supervised methods address this by using a cross-view pretext task, often
modeled with a masked autoencoder, where a masked target view is reconstructed
from an anchor view. However, acquiring effective training data remains a
challenge - collecting diverse video datasets is difficult and costly, while
simple image crops lack necessary pose variations. This paper introduces
CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic
views generated from static images via an image-conditioned diffusion model.
These generated views exhibit substantial changes in pose and perspective,
providing a rich training signal that overcomes the limitations of video and
crop-based anchors. We present a quantitative method to evaluate local and
global consistency of generated images, discussing their use for cross-view
self-supervised pretraining. Furthermore, we enhance the standard single-anchor
MAE setting to a multi-anchor strategy to effectively modulate the difficulty
of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods
reliant only on images and substantially narrows the performance gap to
video-based approaches.

</details>


### [85] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Main category: cs.CV

TL;DR: 提出了一种基于时空交叉注意力的甲状腺超声时间序列分类模型（STACT-Time），用于提高甲状腺结节恶性预测的准确性，减少不必要的活检。


<details>
  <summary>Details</summary>
Motivation: 现有的甲状腺结节评估方法（如FNA活检）存在不必要的良性结节活检问题，且传统系统（如TI-RADS）受限于观察者间的变异性。深度学习模型未能充分利用超声动态视频的时空信息。

Method: 提出STACT-Time模型，结合超声动态视频的成像特征和预训练模型生成的掩码特征，通过自注意力和交叉注意力机制捕捉时空上下文。

Result: 模型在交叉验证中达到0.91的精确度和0.89的F1分数，优于现有方法。

Conclusion: 该模型能减少良性结节的不必要活检，同时保持高恶性检测灵敏度，有望改善临床决策和患者预后。

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [86] [DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data](https://arxiv.org/abs/2506.18173)
*Sabbir Ahmed,Md. Bakhtiar Hasan,Tasnim Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出了一种少样本学习框架DExNet，用于植物病害分类，通过结合多个预训练CNN架构的观察，显著减少训练数据需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在有限样本下植物病害分类性能不足的问题。

Method: 使用预训练的CNN架构提取特征，通过特征融合块和Bi-LSTM分类器进行分类。

Result: 在PlantVillage数据集上，5-shot、10-shot和15-shot分类准确率分别为89.06%、92.46%和94.07%，80-shot分类准确率达98.09%。

Conclusion: DExNet在少样本条件下优于现有方法，显著减少数据需求，适用于多种场景。

Abstract: While deep learning-based architectures have been widely used for correctly
detecting and classifying plant diseases, they require large-scale datasets to
learn generalized features and achieve state-of-the-art performance. This poses
a challenge for such models to obtain satisfactory performance in classifying
leaf diseases with limited samples. This work proposes a few-shot learning
framework, Domain-adapted Expert Network (DExNet), for plant disease
classification that compensates for the lack of sufficient training data by
combining observations of a number of expert critics. It starts with extracting
the feature embeddings as 'observations' from nine 'critics' that are
state-of-the-art pre-trained CNN-based architectures. These critics are 'domain
adapted' using a publicly available leaf disease dataset having no overlapping
classes with the specific downstream task of interest. The observations are
then passed to the 'Feature Fusion Block' and finally to a classifier network
consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10
classes of tomato leaf images from the PlantVillage dataset, achieving
promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,
10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%
has been achieved in 80-shot classification, which is only 1.2% less than
state-of-the-art, allowing a 94.5% reduction in the training data requirement.
The proposed pipeline also outperforms existing works on leaf disease
classification with limited data in both laboratory and real-life conditions in
single-domain, mixed-domain, and cross-domain scenarios.

</details>


### [87] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Main category: cs.CV

TL;DR: FMF-SLAM是一种高效的多模态融合SLAM方法，利用FFT提升算法效率，适用于噪声、光照变化和黑暗环境。


<details>
  <summary>Details</summary>
Motivation: 传统基于光流的视觉SLAM方法计算资源需求高，难以应对噪声、光照变化和黑暗环境的挑战。

Method: 提出基于傅里叶的自注意力和交叉注意力机制，从RGB和深度信号中提取特征，并结合多尺度知识蒸馏增强多模态特征交互。

Result: 在TUM、TartanAir和真实数据集上验证了FMF-SLAM的实时性能，展示了在噪声、光照变化和黑暗条件下的先进性能。

Conclusion: FMF-SLAM通过多模态融合和FFT优化，显著提升了视觉SLAM在复杂环境中的效率和性能。

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [88] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/abs/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: DINO-enhanced NeRF models underperform baseline NeRF in few-shot 3D reconstruction, challenging the assumption that pre-trained vision features improve performance.


<details>
  <summary>Details</summary>
Motivation: To evaluate the effectiveness of integrating pre-trained DINO features into NeRF for few-shot 3D scene reconstruction, especially in extreme scenarios.

Method: Systematic comparison of baseline NeRF, frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.

Result: All DINO variants performed worse than baseline NeRF (PSNR ~12.9-13.0 vs. 14.71), indicating potential harmful biases from pre-trained features.

Conclusion: Pre-trained vision features may not benefit few-shot 3D reconstruction; simpler architectures focusing on geometric consistency could be more effective.

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

</details>


### [89] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的膝关节对齐测量方法，通过自动定位膝关节解剖标志，提高了测量的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统膝关节对齐测量方法耗时且依赖长腿X光片，本研究旨在开发一种自动化、高精度的替代方案。

Method: 采用hourglass网络和注意力门结构，自动定位100多个膝关节解剖标志，并测量术前和术后的膝关节对齐。

Result: 测量结果与临床真实值相比平均绝对差异约1度，术前ICC为0.97，术后为0.86，表现出高准确性和可靠性。

Conclusion: 该方法能够高精度自动化膝关节对齐评估，为临床工作流程提供了数字化支持。

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [90] [Shape from Polarization of Thermal Emission and Reflection](https://arxiv.org/abs/2506.18217)
*Kazuma Kitazawa,Tsuyoshi Takatani*

Main category: cs.CV

TL;DR: 利用长波红外（LWIR）光谱中的偏振技术（SfP）估计透明物体形状，通过改进偏振模型和结合学习与物理建模方法，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 透明物体的形状估计因复杂的光传输而困难，现有LWIR SfP方法因偏振模型不足（尤其是忽略反射）导致误差较大。

Method: 提出一个明确考虑发射和反射的偏振模型，结合基于模型和学习的方法（使用神经网络），并建模LWIR偏振成像过程以减少系统误差。

Result: 实现了高精度的表面法线估计，适用于多种材料（包括可见光透明的材料），并创建了首个真实世界LWIR SfP基准数据集ThermoPol。

Conclusion: 该方法在透明物体形状估计中表现出高准确性和广泛适用性，为相关领域提供了新工具和基准。

Abstract: Shape estimation for transparent objects is challenging due to their complex
light transport. To circumvent these difficulties, we leverage the Shape from
Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where
most materials are opaque and emissive. While a few prior studies have explored
LWIR SfP, these attempts suffered from significant errors due to inadequate
polarimetric modeling, particularly the neglect of reflection. Addressing this
gap, we formulated a polarization model that explicitly accounts for the
combined effects of emission and reflection. Based on this model, we estimated
surface normals using not only a direct model-based method but also a
learning-based approach employing a neural network trained on a
physically-grounded synthetic dataset. Furthermore, we modeled the LWIR
polarimetric imaging process, accounting for inherent systematic errors to
ensure accurate polarimetry. We implemented a prototype system and created
ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through
comprehensive experiments, we demonstrated the high accuracy and broad
applicability of our method across various materials, including those
transparent in the visible spectrum.

</details>


### [91] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: 提出了一种轻量级、边缘设备可部署的视网膜疾病分类器，通过跨架构知识蒸馏方法，将高性能ViT教师模型压缩为CNN学生模型，适用于资源有限环境。


<details>
  <summary>Details</summary>
Motivation: 解决低资源环境下缺乏可靠视网膜疾病诊断设备的问题，提供一种可扩展的AI驱动解决方案。

Method: 使用I-JEPA自监督学习预训练ViT教师模型，通过PCA投影器、GL投影器和多视图鲁棒训练方法压缩为CNN学生模型。

Result: 学生模型参数比教师模型少97.4%，分类准确率达89%，保留了教师模型93%的诊断性能。

Conclusion: 该方法成功实现了ViT模型的压缩并保持准确性，为资源有限地区提供了可扩展的视网膜疾病分类解决方案。

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [92] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: 提出了一种无需训练的自适应动态稀疏注意力方法（ADSA），用于优化图像生成中的上下文计算，显著减少内存和计算开销。


<details>
  <summary>Details</summary>
Motivation: 解决自回归图像生成模型中因长上下文导致的KV缓存内存开销和计算延迟问题。

Method: 通过动态识别关键历史令牌，优化注意力计算，并引入动态KV缓存更新机制。

Result: 实验表明ADSA在生成质量和资源效率上均表现优越，GPU内存消耗减少约50%。

Conclusion: ADSA是一种高效且无需训练的方法，显著提升了图像生成的资源效率。

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [93] [Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning](https://arxiv.org/abs/2506.18234)
*Yue Li,Meng Tian,Dechang Zhu,Jiangtong Zhu,Zhenyu Lin,Zhiwei Xiong,Xinhai Zhao*

Main category: cs.CV

TL;DR: Drive-R1是一个专为自动驾驶设计的小规模视觉语言模型，通过结合场景推理和运动规划，解决了现有模型依赖历史信息和推理与规划不对齐的问题。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在自动驾驶中过于依赖历史信息，且推理过程与规划结果不一致，Drive-R1旨在解决这些问题。

Method: Drive-R1通过监督微调和强化学习框架训练，鼓励模型从视觉输入逐步推理到规划决策。

Result: 在nuScenes和DriveLM-nuScenes基准测试中，Drive-R1表现优于现有最先进模型。

Conclusion: Drive-R1为自动驾驶中的推理与规划提供了新方向，具有未来研究和应用的方法学价值。

Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving
beyond perception and cognition tasks toward motion planning. However, we
identify two critical challenges in this direction: (1) VLMs tend to learn
shortcuts by relying heavily on history input information, achieving seemingly
strong planning results without genuinely understanding the visual inputs; and
(2) the chain-ofthought (COT) reasoning processes are always misaligned with
the motion planning outcomes, and how to effectively leverage the complex
reasoning capability to enhance planning remains largely underexplored. In this
paper, we start from a small-scale domain-specific VLM and propose Drive-R1
designed to bridges the scenario reasoning and motion planning for AD. Drive-R1
first undergoes the supervised finetuning on a elaborate dataset containing
both long and short COT data. Drive-R1 is encouraged to reason step-by-step
from visual input to final planning decisions. Subsequently, Drive-R1 is
trained within a reinforcement learning framework that incentivizes the
discovery of reasoning paths that are more informative for planning, guided by
rewards based on predicted trajectories and meta actions. Experimental
evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that
Drive-R1 achieves superior performance compared to existing state-of-the-art
VLMs. We believe that Drive-R1 presents a promising direction for bridging
reasoning and planning in AD, offering methodological insights for future
research and applications.

</details>


### [94] [Referring Expression Instance Retrieval and A Strong End-to-End Baseline](https://arxiv.org/abs/2506.18246)
*Xiangzhao Hao,Kuan Zhu,Hongyu Guo,Haiyun Guo,Ming Tang,JinQiao Wang*

Main category: cs.CV

TL;DR: 提出新任务REIR，结合实例级检索与定位，并构建基准REIRCOCO和基线方法CLARE。


<details>
  <summary>Details</summary>
Motivation: 解决TIR缺乏精度和REC缺乏扩展性的问题，满足实际场景需求。

Method: 采用双流架构和MORE模块，结合目标检测、REC预训练和CLIA进行端到端优化。

Result: CLARE在REIR上表现优异，且能泛化到TIR和REC任务。

Conclusion: REIR任务和CLARE方法有效且通用，为视觉语言任务提供新方向。

Abstract: Natural language querying of visual content underpins many vision-language
tasks, typically categorized by text granularity and visual search scope.
Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions,
while Referring Expression Comprehension (REC) localizes objects using
fine-grained expressions within a single image. However, real-world scenarios
often require both instance-level retrieval and localization across large
galleries -- tasks where TIR lacks precision and REC lacks scalability. To
address this gap, we propose a new task: Referring Expression Instance
Retrieval (REIR), which jointly supports instance-level retrieval and
localization. We introduce REIRCOCO, a large-scale benchmark constructed by
prompting vision-language models to generate fine-grained expressions for
MSCOCO and RefCOCO instances. We also present a baseline method, CLARE,
featuring a dual-stream architecture with a Mix of Relation Experts (MORE)
module for capturing inter-instance relationships. CLARE integrates object
detection and REC pretraining with Contrastive Language-Instance Alignment
(CLIA) for end-to-end optimization. Experiments show that CLARE achieves
state-of-the-art performance on REIR and generalizes well to TIR and REC,
highlighting its effectiveness and versatility.

</details>


### [95] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于Mean Teacher的语义结构感知攻击框架，通过特征蒸馏增强对抗扰动的语义一致性和迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有生成对抗攻击方法未充分利用生成模型的语义信息，限制了扰动与对象显著区域的对齐。

Method: 利用Mean Teacher作为时间平滑特征参考，通过特征蒸馏指导学生和教师模型的早期层激活语义一致性。

Result: 实验表明，该方法在多种模型、领域和任务中均优于现有生成攻击方法。

Conclusion: 通过锚定生成器的语义显著区域，显著提升了对抗扰动的迁移性。

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [96] [Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain](https://arxiv.org/abs/2506.18261)
*Rui Su,Dong Xu,Luping Zhou,Wanli Ouyang*

Main category: cs.CV

TL;DR: 提出了一种两阶段方法，利用多分辨率信息生成高质量帧级伪标签，提升弱监督时序动作定位性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督时序动作定位任务中仅有视频级标注可用，需解决帧级标注不足的问题。

Method: 第一阶段通过ILG模块生成初始伪标签，第二阶段通过PTLR框架迭代优化伪标签，利用多分辨率信息交换提升性能。

Result: 通过多分辨率信息交换，提升了时序动作定位的性能。

Conclusion: 两阶段方法有效利用多分辨率信息，生成高质量伪标签，显著提升弱监督时序动作定位效果。

Abstract: Weakly supervised temporal action localization is a challenging task as only
the video-level annotation is available during the training process. To address
this problem, we propose a two-stage approach to fully exploit multi-resolution
information in the temporal domain and generate high quality frame-level pseudo
labels based on both appearance and motion streams. Specifically, in the first
stage, we generate reliable initial frame-level pseudo labels, and in the
second stage, we iteratively refine the pseudo labels and use a set of selected
frames with highly confident pseudo labels to train neural networks and better
predict action class scores at each frame. We fully exploit temporal
information at multiple scales to improve temporal action localization
performance. Specifically, in order to obtain reliable initial frame-level
pseudo labels, in the first stage, we propose an Initial Label Generation (ILG)
module, which leverages temporal multi-resolution consistency to generate high
quality class activation sequences (CASs), which consist of a number of
sequences with each sequence measuring how likely each video frame belongs to
one specific action class. In the second stage, we propose a Progressive
Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks
called Network-OTS and Network-RTS, which are respectively used to generate
CASs for the original temporal scale and the reduced temporal scales, are used
as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo
labels in turn. By this way, the multi-resolution information in the temporal
domain is exchanged at the pseudo label level, and our work can help improve
each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels
from another stream (i.e., the RTS/OTS stream).

</details>


### [97] [YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos](https://arxiv.org/abs/2506.18266)
*Haoming Chen,Lichen Yuan,TianFang Sun,Jingyu Gong,Xin Tan,Zhizhong Zhang,Yuan Xie*

Main category: cs.CV

TL;DR: 论文提出了一种仅使用室内互联网数据进行3D语义占用预测的自监督方法，无需相机参数。


<details>
  <summary>Details</summary>
Motivation: 解决复杂室内环境中数据采集困难和隐私问题，利用互联网数据实现3D空间准确训练。

Method: 收集YouTube房屋游览视频数据集YouTube-Occ，通过自监督模型利用2D先验知识，结合视觉基础模型和超像素分组技术。

Result: 在NYUv2和OccScanNet基准测试中实现了零样本状态最优性能。

Conclusion: 证明了仅依赖互联网数据和自监督学习即可实现高效的3D室内感知。

Abstract: 3D semantic occupancy prediction in the past was considered to require
precise geometric relationships in order to enable effective training. However,
in complex indoor environments, the large-scale and widespread collection of
data, along with the necessity for fine-grained annotations, becomes
impractical due to the complexity of data acquisition setups and privacy
concerns. In this paper, we demonstrate that 3D spatially-accurate training can
be achieved using only indoor Internet data, without the need for any
pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we
collect a web dataset, YouTube-Occ, which comprises house tour videos from
YouTube, providing abundant real house scenes for 3D representation learning.
Upon on this web dataset, we establish a fully self-supervised model to
leverage accessible 2D prior knowledge for reaching powerful 3D indoor
perception. Specifically, we harness the advantages of the prosperous vision
foundation models, distilling the 2D region-level knowledge into the occupancy
network by grouping the similar pixels into superpixels. Experimental results
show that our method achieves state-of-the-art zero-shot performance on two
popular benchmarks (NYUv2 and OccScanNet

</details>


### [98] [ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments](https://arxiv.org/abs/2506.18268)
*Yu Liu,Yangtao Meng,Xianfei Pan,Jie Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: ThermalLoc是一种新型端到端深度学习方法，用于热图像重定位，结合EfficientNet和Transformer提取特征，并通过MLP网络进行绝对姿态回归，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统视觉重定位方法不适用于热图像，而深度学习在热相机重定位方面的研究较少，因此提出ThermalLoc填补这一空白。

Method: ThermalLoc整合EfficientNet和Transformer提取热图像的局部和全局特征，并使用两个MLP网络进行绝对姿态回归。

Result: 在公开数据集和自有数据集上，ThermalLoc在准确性和鲁棒性上均优于AtLoc、MapNet、PoseNet和RobustLoc等现有方法。

Conclusion: ThermalLoc为热图像重定位提供了一种高效且鲁棒的解决方案，填补了该领域的研究空白。

Abstract: Thermal cameras capture environmental data through heat emission, a
fundamentally different mechanism compared to visible light cameras, which rely
on pinhole imaging. As a result, traditional visual relocalization methods
designed for visible light images are not directly applicable to thermal
images. Despite significant advancements in deep learning for camera
relocalization, approaches specifically tailored for thermal camera-based
relocalization remain underexplored. To address this gap, we introduce
ThermalLoc, a novel end-to-end deep learning method for thermal image
relocalization. ThermalLoc effectively extracts both local and global features
from thermal images by integrating EfficientNet with Transformers, and performs
absolute pose regression using two MLP networks. We evaluated ThermalLoc on
both the publicly available thermal-odometry dataset and our own dataset. The
results demonstrate that ThermalLoc outperforms existing representative methods
employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,
and RobustLoc, achieving superior accuracy and robustness.

</details>


### [99] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/abs/2506.18270)
*Qinrong Cai,Yu Guan,Zhibo Chen,Dong Liang,Qiuyun Fan,Qiegen Liu*

Main category: cs.CV

TL;DR: 提出了一种基于自适应掩码的扩散模型（AMDM），通过自适应调整k空间数据的频率分布，有效分离高低频成分，提升MRI重建质量。


<details>
  <summary>Details</summary>
Motivation: 传统MRI重建方法未考虑k空间不同频率区域的重要性，导致重建效果受限。

Method: 利用k空间频率分布生成自适应掩码，指导闭环扩散过程，分离高低频成分。

Result: 实验证明该方法能学习特定频率信息，显著提高MRI重建质量。

Conclusion: AMDM为未来基于掩码优化k空间数据提供了灵活框架。

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training, and has demonstrated exceptional
performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction
is a critical task in medical imaging that seeks to recover high-quality images
from under-sampled k-space data. However, previous MRI reconstruction
strategies usually optimized the entire image domain or k-space, without
considering the importance of different frequency regions in the k-space This
work introduces a diffusion model based on adaptive masks (AMDM), which
utilizes the adaptive adjustment of frequency distribution based on k-space
data to develop a hybrid masks mechanism that adapts to different k-space
inputs. This enables the effective separation of high-frequency and
low-frequency components, producing diverse frequency-specific representations.
Additionally, the k-space frequency distribution informs the generation of
adaptive masks, which, in turn, guide a closed-loop diffusion process.
Experimental results verified the ability of this method to learn specific
frequency information and thereby improved the quality of MRI reconstruction,
providing a flexible framework for optimizing k-space data using masks in the
future.

</details>


### [100] [ReFrame: Rectification Framework for Image Explaining Architectures](https://arxiv.org/abs/2506.18272)
*Debjyoti Das Adhikary,Aritra Hazra,Partha Pratim Chakrabarti*

Main category: cs.CV

TL;DR: 提出了一种新方法，通过可解释框架改进图像解释的完整性和一致性，显著提升了图像描述、视觉问答和基于提示的AI模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像解释方法常存在对象幻觉或遗漏，导致解释不完整或不一致。

Method: 提出可解释框架，可集成到多种图像解释框架中，纠正错误或缺失的对象。

Result: 在图像描述、视觉问答和基于提示的AI模型中，完整性和一致性均有显著提升。

Conclusion: 该方法有效解决了图像解释中的不完整和不一致问题，性能优于现有技术。

Abstract: Image explanation has been one of the key research interests in the Deep
Learning field. Throughout the years, several approaches have been adopted to
explain an input image fed by the user. From detecting an object in a given
image to explaining it in human understandable sentence, to having a
conversation describing the image, this problem has seen an immense change
throughout the years, However, the existing works have been often found to (a)
hallucinate objects that do not exist in the image and/or (b) lack identifying
the complete set of objects present in the image. In this paper, we propose a
novel approach to mitigate these drawbacks of inconsistency and incompleteness
of the objects recognized during the image explanation. To enable this, we
propose an interpretable framework that can be plugged atop diverse image
explaining frameworks including Image Captioning, Visual Question Answering
(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation
capabilities by rectifying the incorrect or missing objects. We further measure
the efficacy of the rectified explanations generated through our proposed
approaches leveraging object based precision metrics, and showcase the
improvements in the inconsistency and completeness of image explanations.
Quantitatively, the proposed framework is able to improve the explanations over
the baseline architectures of Image Captioning (improving the completeness by
81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%
and 37.10% in completeness and inconsistency respectively) and Prompt-based AI
model (0.01% and 5.2% for completeness and inconsistency respectively)
surpassing the current state-of-the-art by a substantial margin.

</details>


### [101] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Main category: cs.CV

TL;DR: 该论文探讨了在开放世界临床环境中应用开放集识别（OSR）技术于内窥镜图像分类，评估了多种深度学习架构的性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集分类框架在开放世界临床环境中存在局限性，无法处理未知条件，影响模型可靠性。

Method: 在Kvasir数据集上评估了ResNet-50、Swin Transformer和混合ResNet-Transformer模型的OSR能力，并采用OpenMax作为基线方法。

Result: 研究为医学图像分析中的OSR性能提供了基准，并展示了模型在临床现实环境中的行为。

Conclusion: OSR技术对于AI系统在内窥镜中的安全部署至关重要。

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [102] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出了一种通过选择重要邻居来预测主要人物轨迹的架构，使用重要性估计器和Gumbel Softmax解决梯度阻塞问题，实验显示方法高效且准确。


<details>
  <summary>Details</summary>
Motivation: 预测主要人物的轨迹时，周围人物的选择对准确性至关重要，但现有方法可能因非可微操作导致梯度阻塞。

Method: 提出重要性估计器模块选择重要邻居，并使用Gumbel Softmax进行训练以避免梯度阻塞。

Result: 在JRDB数据集上实验，方法在保持预测准确性的同时提高了效率。

Conclusion: 该方法通过有效选择邻居和解决梯度问题，实现了高效且准确的轨迹预测。

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [103] [Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture](https://arxiv.org/abs/2506.18292)
*Ziyue Guo,Xin Yang,Yutao Shen,Yang Zhu,Lixi Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: 提出了一种基于多视角成像的油菜群体三维重建点云补全模型（RP-PCN），通过虚拟-现实集成方法和遮挡点检测算法生成完整点云，显著提高了油菜群体冠层结构的描述精度和产量预测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有三维传感技术在植物和冠层重建中存在严重遮挡和复杂结构问题，难以准确描述冠层结构，影响光合作用和产量评估。

Method: 开发了虚拟-现实集成（VRI）模拟方法和遮挡点检测算法，设计RP-PCN网络，包含多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），并引入动态图卷积特征提取器（DGCFE）捕捉生长周期中的结构变化。

Result: RP-PCN在油菜不同生长阶段的CD值分别为3.35 cm、3.46 cm、4.32 cm和4.51 cm，MRDG和DGCFE模块分别降低CD值10%和23%，SEI指标将产量预测准确性提高11.2%。

Conclusion: RP-PCN模型能有效补全油菜群体点云，显著提升冠层结构分析和产量预测精度，并可推广至其他作物。

Abstract: Quantitative descriptions of complete canopy architecture are crucial for
evaluating crop photosynthesis and yield to guide ideotype design. Although
three-dimensional (3D) sensing technologies have been developed for plant and
canopy reconstruction, severe occlusion and complex architectures hinder
accurate canopy descriptions. In this study, we propose a point cloud
completion model for 3D reconstruction of rapeseed populations from seeding to
silique stages using multi-view imaging. A complete point cloud generation
framework was developed with the virtual-real integration (VRI) simulation
method and occlusion point detection algorithm to annotate the training dataset
by distinguishing surface from occluded points. The rapeseed population point
cloud completion network (RP-PCN) was designed with a multi-resolution dynamic
graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict
occluded points based on input surface point clouds. A dynamic graph
convolutional feature extractor (DGCFE) was introduced to capture structural
variations across the growth period. The effectiveness of point cloud
completion was validated by predicting yield using architectural indicators
from complete point clouds of rapeseed population. The results demonstrated
that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,
and 4.51 cm at the seedling, bolting, flowering, and silique stages,
respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE
modules, reducing CD values by 10% and 23%, respectively. The silique
efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%
compared to incomplete point clouds. The RP-PCN pipeline proposed in this study
has the potential to be extended to other crops, significantly enhancing the
analysis of population canopy architectures in field environments.

</details>


### [104] [Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion](https://arxiv.org/abs/2506.18321)
*Zeeshan Ramzan,Nisar Ahmed,Qurat-ul-Ain Akram,Shahzad Asif,Muhammad Shahbaz,Rabin Chakrabortty,Ahmed F. Elaksher*

Main category: cs.CV

TL;DR: 该研究通过结合遥感数据和高级建模技术，提高了灌溉农业区域的作物分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用遥感技术获取灌溉农业区域的作物覆盖信息，以提供准确的作物面积和类型数据。

Method: 研究分两阶段进行：第一阶段通过实地调查识别和地理编码六种目标作物；第二阶段获取Landsat 8-9影像，进行预处理和图像融合，提取植被指数，并利用传统分类器、集成学习和人工神经网络进行分类建模。

Result: 研究构建了一个包含50,835个数据点的数据集，并展示了结合遥感数据和高级建模技术的有效性。

Conclusion: 该研究表明，结合遥感数据和先进建模技术可以显著提高灌溉农业区域的作物分类准确性。

Abstract: Remote sensing offers a highly effective method for obtaining accurate
information on total cropped area and crop types. The study focuses on crop
cover identification for irrigated regions of Central Punjab. Data collection
was executed in two stages: the first involved identifying and geocoding six
target crops through field surveys conducted in January and February 2023. The
second stage involved acquiring Landsat 8-9 imagery for each geocoded field to
construct a labelled dataset. The satellite imagery underwent extensive
pre-processing, including radiometric calibration for reflectance values,
atmospheric correction, and georeferencing verification to ensure consistency
within a common coordinate system. Subsequently, image fusion techniques were
applied to combine Landsat 8 and 9 spectral bands, creating a composite image
with enhanced spectral information, followed by contrast enhancement. During
data acquisition, farmers were interviewed, and fields were meticulously mapped
using GPS instruments, resulting in a comprehensive dataset of 50,835 data
points. This dataset facilitated the extraction of vegetation indices such as
NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were
utilized for classification modeling using conventional classifiers, ensemble
learning, and artificial neural networks. A feature selection approach was also
incorporated to identify the optimal feature set for classification learning.
This study demonstrates the effectiveness of combining remote sensing data and
advanced modeling techniques to improve crop classification accuracy in
irrigated agricultural regions.

</details>


### [105] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: 论文提出了SpuriVerse基准，用于研究多模态大视觉语言模型中的虚假相关性，发现即使最先进的模型也表现不佳，但通过微调可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究多模态大视觉语言模型（LVLMs）中虚假相关性的影响，尤其是在未经显式任务监督的预训练模型中。

Method: 通过GPT-4o的错误分析构建SpuriVerse基准，包含124种虚假相关性类型，并通过LVLM-人类标注和合成反事实评估进行筛选。

Result: 15种LVLMs在SpuriVerse上表现不佳（最高37.1%准确率），但微调后性能提升至78.40%。

Conclusion: 模型可以通过学习避免虚假相关性来提升性能，表明多样化的训练数据有助于泛化。

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [106] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Main category: cs.CV

TL;DR: LucentVisionNet是一种新型的零样本学习框架，用于低光图像增强，无需配对训练数据，通过多尺度空间注意力和深度曲线估计网络实现精细增强。


<details>
  <summary>Details</summary>
Motivation: 解决传统和深度学习方法在低光图像增强中的局限性，特别是在缺乏配对数据的情况下。

Method: 结合多尺度空间注意力和深度曲线估计网络，采用循环增强策略和复合损失函数（包含六个定制组件）。

Result: 在多个基准数据集上优于现有的监督、无监督和零样本方法，具有高视觉质量、结构一致性和计算效率。

Conclusion: LucentVisionNet适用于移动摄影、监控和自主导航等实际应用。

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [107] [NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation](https://arxiv.org/abs/2506.18325)
*Yu Xie,Chengjie Zeng,Lingyun Zhang,Yanwei Fu*

Main category: cs.CV

TL;DR: 本文提出PromptSan方法，通过两种变体（PromptSan-Modify和PromptSan-Suffix）来净化有害提示，从而减少文本到图像模型生成有害内容的风险。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像（T2I）模型的快速发展，其生成有害内容（如色情、暴力、歧视）的风险增加，这违背了技术的伦理目标并阻碍其可持续发展。

Method: PromptSan包括两种方法：1）PromptSan-Modify通过迭代识别和替换有害标记来修改输入提示；2）PromptSan-Suffix训练优化的后缀标记序列以中和有害意图。

Result: 实验表明，PromptSan在减少有害内容生成方面表现优异，同时平衡了安全性和可用性。

Conclusion: PromptSan是一种无需修改模型架构或降低生成能力的新方法，能有效减少T2I模型生成有害内容的风险。

Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable
Diffusion, has enhanced their capability to synthesize images from textual
prompts. However, this progress also raises significant risks of misuse,
including the generation of harmful content (e.g., pornography, violence,
discrimination), which contradicts the ethical goals of T2I technology and
hinders its sustainable development. Inspired by "jailbreak" attacks in large
language models, which bypass restrictions through subtle prompt modifications,
this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a
novel approach to detoxify harmful prompts without altering model architecture
or degrading generation capability. PromptSan includes two variants:
PromptSan-Modify, which iteratively identifies and replaces harmful tokens in
input prompts using text NSFW classifiers during inference, and
PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize
harmful intent while passing both text and image NSFW classifier checks.
Extensive experiments demonstrate that PromptSan achieves state-of-the-art
performance in reducing harmful content generation across multiple metrics,
effectively balancing safety and usability.

</details>


### [108] [Geometry-Aware Preference Learning for 3D Texture Generation](https://arxiv.org/abs/2506.18331)
*AmirHossein Zamani,Tianhao Xie,Amir G. Aghdam,Tiberiu Popa,Eugene Belilovsky*

Main category: cs.CV

TL;DR: 提出了一种端到端的可微分偏好学习框架，通过几何感知的奖励函数优化3D生成模型，以更好地满足人类主观偏好和任务需求。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型生成的内容可能不符合人类主观偏好或任务需求，且依赖2D文本到图像模型，缺乏对3D结构的理解。

Method: 提出了一种端到端的可微分偏好学习框架，通过几何感知的奖励函数反向传播人类偏好。

Result: 通过四种几何感知奖励函数验证了框架的有效性，提供了更可控和可解释的3D内容生成路径。

Conclusion: 该框架为高质量3D内容生成提供了更符合人类偏好的解决方案。

Abstract: Recent advances in 3D generative models have achieved impressive results but
3D contents generated by these models may not align with subjective human
preferences or task-specific criteria. Moreover, a core challenge in the 3D
texture generation domain remains: most existing approaches rely on repeated
calls to 2D text-to-image generative models, which lack an inherent
understanding of the 3D structure of the input 3D mesh object. To address this,
we propose an end-to-end differentiable preference learning framework that
back-propagates human preferences, represented by differentiable reward
functions, through the entire 3D generative pipeline, making the process
inherently geometry-aware. We demonstrate the effectiveness of our framework
using four proposed novel geometry-aware reward functions, offering a more
controllable and interpretable pathway for high-quality 3D content creation
from natural language.

</details>


### [109] [Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention](https://arxiv.org/abs/2506.18335)
*Saad Wazir,Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种新的医学图像分割架构，通过多尺度局部和全局上下文信息捕获及新型解码器设计，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer和CNN的方法在染色和形态变化下特征提取受限，且端到端方法在样本有限的数据集上表现不佳。

Method: 设计了能捕获多尺度信息的架构和新型解码器，有效整合编码器特征，强调重要通道和区域，并重建空间维度。

Result: 在四个数据集上均优于现有SOTA方法，性能提升显著（如MoNuSeg提升2.76%）。

Conclusion: 该方法兼容多种编码器，通过改进特征传递和解码器效率，显著提升了医学图像分割的准确性。

Abstract: Segmenting biomarkers in medical images is crucial for various biotech
applications. Despite advances, Transformer and CNN based methods often
struggle with variations in staining and morphology, limiting feature
extraction. In medical image segmentation, where datasets often have limited
sample availability, recent state-of-the-art (SOTA) methods achieve higher
accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to
underperform. This is due to challenges in effectively transferring rich
multiscale features from encoders to decoders, as well as limitations in
decoder efficiency. To address these issues, we propose an architecture that
captures multi-scale local and global contextual information and a novel
decoder design, which effectively integrates features from the encoder,
emphasizes important channels and regions, and reconstructs spatial dimensions
to enhance segmentation accuracy. Our method, compatible with various encoders,
outperforms SOTA methods, as demonstrated by experiments on four datasets and
ablation studies. Specifically, our method achieves absolute performance gains
of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on
TNBC datasets compared to existing SOTA methods. Code:
https://github.com/saadwazir/MCADS-Decoder

</details>


### [110] [BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement](https://arxiv.org/abs/2506.18346)
*Tongshun Zhang,Pingping Liu,Mengen Cai,Zijian Zhang,Yubing Lu,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: BSMamba是一种新型视觉Mamba架构，通过亮度Mamba和语义Mamba组件，解决了低光图像增强中亮度恢复和语义一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在提升亮度的同时难以保持语义一致性和细节，且现有视觉Mamba方法因固定扫描规则限制了远距离token的交互。

Method: BSMamba包含亮度Mamba和语义Mamba，分别基于亮度和语义相似性建模token交互，突破了传统token序列的限制。

Result: 实验表明BSMamba在低光图像增强任务中达到了最先进的性能，同时保持了语义一致性。

Conclusion: BSMamba通过亮度引导的选择性注意力和语义优先交互，显著提升了低光图像增强的效果。

Abstract: Current low-light image enhancement (LLIE) methods face significant
limitations in simultaneously improving brightness while preserving semantic
consistency, fine details, and computational efficiency. With the emergence of
state-space models, particularly Mamba, image restoration has achieved
remarkable performance, yet existing visual Mamba approaches flatten 2D images
into 1D token sequences using fixed scanning rules, critically limiting
interactions between distant tokens with causal relationships and constraining
their ability to capture meaningful long-range dependencies. To address these
fundamental limitations, we propose BSMamba, a novel visual Mamba architecture
comprising two specially designed components: Brightness Mamba and Semantic
Mamba. The Brightness Mamba revolutionizes token interaction patterns by
prioritizing connections between distant tokens with similar brightness levels,
effectively addressing the challenge of brightness restoration in LLIE tasks
through brightness-guided selective attention. Complementing this, the Semantic
Mamba establishes priority interactions between tokens sharing similar semantic
meanings, allowing the model to maintain contextual consistency by connecting
semantically related regions across the image, thus preserving the hierarchical
nature of image semantics during enhancement. By intelligently modeling tokens
based on brightness and semantic similarity rather than arbitrary scanning
patterns, BSMamba transcends the constraints of conventional token sequencing
while adhering to the principles of causal modeling. Extensive experiments
demonstrate that BSMamba achieves state-of-the-art performance in LLIE while
preserving semantic consistency.

</details>


### [111] [Spatial frequency information fusion network for few-shot learning](https://arxiv.org/abs/2506.18364)
*Wenqing Zhao,Guojia Xie,Han Pan,Biao Yang,Weichuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合频域和空间域信息的SFIFNet方法，以提升小样本学习中的分类性能。


<details>
  <summary>Details</summary>
Motivation: 小样本学习中，传统方法常忽略频域信息，导致特征利用不充分，影响分类性能。

Method: 通过创新的数据预处理方法，将频域信息与空间域信息结合，提升图像特征表示的准确性。

Result: 实验结果表明，该方法有效提升了分类性能。

Conclusion: 结合频域和空间域信息的方法在小样本学习中具有显著优势。

Abstract: The objective of Few-shot learning is to fully leverage the limited data
resources for exploring the latent correlations within the data by applying
algorithms and training a model with outstanding performance that can
adequately meet the demands of practical applications. In practical
applications, the number of images in each category is usually less than that
in traditional deep learning, which can lead to over-fitting and poor
generalization performance. Currently, many Few-shot classification models pay
more attention to spatial domain information while neglecting frequency domain
information, which contains more feature information. Ignoring frequency domain
information will prevent the model from fully exploiting feature information,
which would effect the classification performance. Based on conventional data
augmentation, this paper proposes an SFIFNet with innovative data
preprocessing. The key of this method is enhancing the accuracy of image
feature representation by integrating frequency domain information with spatial
domain information. The experimental results demonstrate the effectiveness of
this method in enhancing classification performance.

</details>


### [112] [Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection](https://arxiv.org/abs/2506.18368)
*Anja Delić,Matej Grcić,Siniša Šegvić*

Main category: cs.CV

TL;DR: SeeKer是一种检测人体骨骼序列异常的方法，通过自回归因子化建模关键点分布，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗监控、工作场所安全等应用中，异常行为常通过异常姿势体现，需高效检测方法。

Method: 采用自回归因子化建模关键点条件分布，联合分布为条件高斯因果预测，异常分数为加权对数条件概率和。

Result: 在UBnormal、MSAD-HR数据集上超越所有先前方法，在ShanghaiTech数据集上表现竞争性。

Conclusion: SeeKer通过简单而有效的关键点级密度建模，实现了优异的异常检测性能。

Abstract: Detecting anomalous human behaviour is an important visual task in
safety-critical applications such as healthcare monitoring, workplace safety,
or public surveillance. In these contexts, abnormalities are often reflected
with unusual human poses. Thus, we propose SeeKer, a method for detecting
anomalies in sequences of human skeletons. Our method formulates the skeleton
sequence density through autoregressive factorization at the keypoint level.
The corresponding conditional distributions represent probable keypoint
locations given prior skeletal motion. We formulate the joint distribution of
the considered skeleton as causal prediction of conditional Gaussians across
its constituent keypoints. A skeleton is flagged as anomalous if its keypoint
locations surprise our model (i.e. receive a low density). In practice, our
anomaly score is a weighted sum of per-keypoint log-conditionals, where the
weights account for the confidence of the underlying keypoint detector. Despite
its conceptual simplicity, SeeKer surpasses all previous methods on the
UBnormal and MSAD-HR datasets while delivering competitive performance on the
ShanghaiTech dataset.

</details>


### [113] [RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models](https://arxiv.org/abs/2506.18369)
*Yeongtak Oh,Jisoo Mok,Dohyun Chung,Juhyeon Shin,Sangha Park,Johan Barthelemy,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的后训练框架，显著提升了多模态大语言模型在个性化图像描述任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的后训练方法在复杂场景（如多概念图像描述）中表现不佳，且高质量标注数据获取成本高。

Method: 采用强化学习（RL）进行后训练，替代传统的监督微调（SFT）。

Result: 模型在视觉识别和个性化生成能力上显著提升，尤其在多概念图像描述任务中优于现有基线。

Conclusion: RL后训练框架有效解决了数据依赖问题，为MLLM个性化任务提供了新思路。

Abstract: Recent multi-modal large language models (MLLMs) often struggle to generate
personalized image captions, even when trained on high-quality captions. In
this work, we observe that such limitations persist in existing
post-training-based MLLM personalization methods. Specifically, despite being
post-tuned with large-scale caption data through supervised fine-tuning (SFT),
these models frequently fail to produce faithful descriptions in real-world
scenarios, such as multi-concept image captioning. However, acquiring
large-scale, high-quality captions for such complex settings is both costly and
difficult. To address the data-centric nature of SFT, we propose a
reinforcement learning (RL)-based post-training framework. To the best of our
knowledge, this is the first RL-based approach to post-train MLLMs for
personalized image captioning. Our method significantly enhances both visual
recognition and personalized generation capabilities of MLLMs, and consistently
outperforms existing SFT-based baselines, especially in the challenging
multi-concept image captioning task.

</details>


### [114] [OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding](https://arxiv.org/abs/2506.18372)
*Hieu Nguyen,Phuc-Tan Nguyen,Thien-Phuc Tran,Minh-Quang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: OpenEvents V1是一个大规模基准数据集，专注于事件为中心的视觉语言理解，包含20万篇新闻文章和40万张相关图片，支持事件感知的图像描述生成和基于叙事文本查询的图像检索任务。


<details>
  <summary>Details</summary>
Motivation: 传统图像描述和检索数据集侧重于表面描述，缺乏对上下文和时间背景的深入理解。OpenEvents V1旨在填补这一空白，推动多模态模型对复杂现实事件的深度推理能力。

Method: 数据集包含来自CNN和The Guardian的新闻文章及相关图片，涵盖多样领域和时间段。提供了基线结果和标准化评估协议，支持事件感知的图像描述生成和叙事文本查询的图像检索任务。

Result: OpenEvents V1为开发能够深度推理复杂现实事件的多模态模型奠定了坚实基础。

Conclusion: OpenEvents V1通过其大规模和任务设计，为事件为中心的视觉语言理解提供了重要资源，并推动了多模态模型的发展。

Abstract: We introduce OpenEvents V1, a large-scale benchmark dataset aimed at
advancing event-centric vision-language understanding. Unlike conventional
image captioning and retrieval datasets that emphasize surface-level
descriptions, OpenEvents V1 focuses on contextual and temporal grounding
through two primary tasks: (1) generating rich, event-aware image captions and
(2) retrieving event-relevant images based on narrative-style textual queries.
The dataset contains over 200,000 news articles and 400,000 associated images
sourced from CNN and The Guardian, spanning diverse domains and time periods.
We provide extensive baseline results and standardized evaluation protocols for
both tasks. OpenEvents V1 establishes a robust foundation for developing
multimodal models capable of deep reasoning over complex real-world events. The
dataset is available at https://ltnghia.github.io/eventa/openevents-v1

</details>


### [115] [InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2506.18385)
*Nianchen Deng,Lixin Gu,Shenglong Ye,Yinan He,Zhe Chen,Songze Li,Haomin Wang,Xingguang Wei,Tianshuo Yang,Min Dou,Tong He,Wenqi Shao,Kaipeng Zhang,Yi Wang,Botian Shi,Yanting Zhang,Jifeng Dai,Yu Qiao,Hongjie Zhang,Wenhai Wang*

Main category: cs.CV

TL;DR: 提出了InternSpatial数据集和InternSpatial-Bench基准，用于提升视觉语言模型的空间推理能力，实验显示显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有资源在规模、视觉多样性和指令表达性上有限，需更大规模的数据集和基准。

Method: 构建包含1200万QA对的数据集，支持19种指令格式，并设计新的旋转角度预测任务。

Result: 模型在InternSpatial-Bench和VSI-Bench上分别提升12.1%和10.7%。

Conclusion: 这些资源将支持空间推理能力强的视觉语言模型在机器人等领域的应用。

Abstract: Recent benchmarks and datasets have been proposed to improve spatial
reasoning in vision-language models (VLMs), yet existing open resources remain
limited in scale, visual diversity, and instruction expressiveness. In this
work, we introduce InternSpatial, the largest open-source dataset for spatial
reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation
benchmark designed to assess spatial understanding under diverse instruction
formats. InternSpatial comprises 12 million QA pairs spanning both single-view
and multi-view settings, drawn from diverse visual environments and supporting
19 instruction formats that reflect varied query styles. For evaluation, we
propose InternSpatial-Bench for single-view tasks and expand multi-view
reasoning by introducing a novel rotation angle prediction task that has not
been explored in prior work. Experimental results show that models trained on
InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on
VSI-Bench, while maintaining strong performance on general-purpose benchmarks.
We hope these resources will support the development of spatially capable VLMs
in practical applications such as robotics and embodied AI.

</details>


### [116] [Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection](https://arxiv.org/abs/2506.18397)
*Ángel F. García-Fernández,Giorgio Battistelli*

Main category: cs.CV

TL;DR: 本文提出了一种基于广义协方差交集（GCI）融合规则的分布式泊松多伯努利（PMB）滤波器，用于分布式多目标滤波。通过近似方法解决了GCI融合的复杂性，实验结果表明其优于其他分布式多目标滤波器。


<details>
  <summary>Details</summary>
Motivation: 解决分布式多目标滤波中GCI融合的复杂性问题，提出一种高效的近似方法。

Method: 通过将PMB密度的幂近似为未归一化的PMB密度，实现GCI融合规则的简化，并保留PMBM形式。

Result: 实验证明该方法优于其他分布式多目标滤波器。

Conclusion: 提出的近似方法有效且高效，适用于分布式多目标滤波场景。

Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter
based on the generalised covariance intersection (GCI) fusion rule for
distributed multi-object filtering. Since the exact GCI fusion of two PMB
densities is intractable, we derive a principled approximation. Specifically,
we approximate the power of a PMB density as an unnormalised PMB density, which
corresponds to an upper bound of the PMB density. Then, the GCI fusion rule
corresponds to the normalised product of two unnormalised PMB densities. We
show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be
expressed in closed form. Future prediction and update steps in each filter
preserve the PMBM form, which can be projected back to a PMB density before the
next fusion step. Experimental results show the benefits of this approach
compared to other distributed multi-object filters.

</details>


### [117] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Main category: cs.CV

TL;DR: 提出了一种基于条件变分自编码器的可解释性风险建模方法，用于皮肤病变的连续评估和分类。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤的高死亡率需要早期且可解释的诊断工具，现有深度学习模型多为二分类，缺乏临床洞察。

Method: 使用条件变分自编码器学习结构化潜在空间，结合SVM进行分类。

Result: 模型在区分良性痣和黑色素瘤上表现优异，潜在空间支持可视化和几何解释。

Conclusion: 该方法结合预测性能和临床适用性，提升AI辅助诊断的可信度和早期检测能力。

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [118] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/abs/2506.18434)
*Filippo Ruffini,Elena Mulero Ayllon,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该论文提出了一个结构化基准，用于评估和比较卷积神经网络和基础模型在COVID-19患者预后预测中的迁移能力，并探索了多种微调策略。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像预后预测中具有潜力，但实际应用仍面临挑战，特别是在数据稀缺和类别不平衡的情况下。

Method: 通过大规模比较分析，评估了多种预训练模型和微调策略（如Full Fine-Tuning、Linear Probing、LoRA等）在不同学习范式下的表现。

Result: 研究提供了关于不同微调策略在数据稀缺和类别不平衡条件下的优势和局限性的详细见解。

Conclusion: 该基准研究旨在为临床预后预测中稳健、高效且可泛化的AI解决方案的实际部署提供指导。

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [119] [Frequency-Domain Fusion Transformer for Image Inpainting](https://arxiv.org/abs/2506.18437)
*Sijin He,Guangfeng Lin,Tao Li,Yajun Chen*

Main category: cs.CV

TL;DR: 提出了一种结合频域融合的Transformer图像修复方法，通过小波变换和Gabor滤波增强多尺度结构建模和细节保留，同时设计了基于快速傅里叶变换的可学习频域滤波器。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂纹理和大面积遮挡，而Transformer方法虽具有全局建模能力，但自注意力的低通特性导致高频细节丢失且计算成本高。

Method: 结合小波变换和Gabor滤波的注意力机制，设计基于快速傅里叶变换的可学习频域滤波器，采用四层编码器-解码器结构，并通过新型损失策略平衡全局语义和细节。

Result: 实验表明，该方法能有效保留更多高频信息，提升图像修复质量。

Conclusion: 通过频域融合和新型注意力机制，解决了Transformer在图像修复中的高频细节丢失问题，同时降低了计算成本。

Abstract: Image inpainting plays a vital role in restoring missing image regions and
supporting high-level vision tasks, but traditional methods struggle with
complex textures and large occlusions. Although Transformer-based approaches
have demonstrated strong global modeling capabilities, they often fail to
preserve high-frequency details due to the low-pass nature of self-attention
and suffer from high computational costs. To address these challenges, this
paper proposes a Transformer-based image inpainting method incorporating
frequency-domain fusion. Specifically, an attention mechanism combining wavelet
transform and Gabor filtering is introduced to enhance multi-scale structural
modeling and detail preservation. Additionally, a learnable frequency-domain
filter based on the fast Fourier transform is designed to replace the
feedforward network, enabling adaptive noise suppression and detail retention.
The model adopts a four-level encoder-decoder structure and is guided by a
novel loss strategy to balance global semantics and fine details. Experimental
results demonstrate that the proposed method effectively improves the quality
of image inpainting by preserving more high-frequency information.

</details>


### [120] [CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing](https://arxiv.org/abs/2506.18438)
*Dinh-Khoi Vo,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: CPAM是一种零样本框架，用于复杂非刚性真实图像编辑，通过自适应模块和掩码引导技术保持背景和对象细节。


<details>
  <summary>Details</summary>
Motivation: 现有方法在保持纹理和身份、处理复杂非刚性对象及特定区域编辑方面存在不足。

Method: 提出保护适应模块和局部提取模块，调整自注意力和交叉注意力机制，结合掩码引导策略。

Result: 在IMBA基准测试中表现优于现有技术，受到人类评分者青睐。

Conclusion: CPAM在复杂图像编辑任务中有效且高效，优于现有方法。

Abstract: Editing natural images using textual descriptions in text-to-image diffusion
models remains a significant challenge, particularly in achieving consistent
generation and handling complex, non-rigid objects. Existing methods often
struggle to preserve textures and identity, require extensive fine-tuning, and
exhibit limitations in editing specific spatial regions or objects while
retaining background details. This paper proposes Context-Preserving Adaptive
Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid
real image editing. Specifically, we propose a preservation adaptation module
that adjusts self-attention mechanisms to preserve and independently control
the object and background effectively. This ensures that the objects' shapes,
textures, and identities are maintained while keeping the background
undistorted during the editing process using the mask guidance technique.
Additionally, we develop a localized extraction module to mitigate the
interference with the non-desired modified regions during conditioning in
cross-attention mechanisms. We also introduce various mask-guidance strategies
to facilitate diverse image manipulation tasks in a simple manner. Extensive
experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a
robust benchmark dataset specifically designed for real image editing,
demonstrate that our proposed method is the preferred choice among human
raters, outperforming existing state-of-the-art editing techniques.

</details>


### [121] [DIP: Unsupervised Dense In-Context Post-training of Visual Representations](https://arxiv.org/abs/2506.18463)
*Sophia Sirko-Galouchenko,Spyros Gidaris,Antonin Vobecky,Andrei Bursuc,Nicolas Thome*

Main category: cs.CV

TL;DR: DIP是一种无监督的后训练方法，通过模拟下游任务场景的伪任务来增强预训练视觉编码器的密集图像表示，适用于上下文场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖复杂的自蒸馏架构，而DIP旨在通过更简单、无监督的方式提升密集表示的性能。

Method: 利用预训练扩散模型和视觉编码器自动生成上下文任务，基于元学习原理进行训练。

Result: DIP在多种下游任务中表现优异，计算高效（单A100 GPU耗时<9小时），优于现有方法。

Conclusion: DIP提供了一种实用且高效的解决方案，显著提升了密集图像表示的质量。

Abstract: We introduce DIP, a novel unsupervised post-training method designed to
enhance dense image representations in large-scale pretrained vision encoders
for in-context scene understanding. Unlike prior approaches that rely on
complex self-distillation architectures, our method trains the vision encoder
using pseudo-tasks that explicitly simulate downstream in-context scenarios,
inspired by meta-learning principles. To enable post-training on unlabeled
data, we propose an automatic mechanism for generating in-context tasks that
combines a pretrained diffusion model and the vision encoder itself. DIP is
simple, unsupervised, and computationally efficient, requiring less than 9
hours on a single A100 GPU. By learning dense representations through pseudo
in-context tasks, it achieves strong performance across a wide variety of
downstream real-world in-context scene understanding tasks. It outperforms both
the initial vision encoder and prior methods, offering a practical and
effective solution for improving dense representations. Code available here:
https://github.com/sirkosophia/DIP

</details>


### [122] [AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction](https://arxiv.org/abs/2506.18472)
*Gengyuan Zhang,Tanveer Hannan,Hermine Kleiner,Beste Aydemir,Xinyu Xie,Jian Lan,Thomas Seidl,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出了一个名为AViLA的异步视频-语言代理，用于处理流数据中的查询-证据异步问题，并通过实验验证其优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能代理需要处理动态数据流和用户临时查询，但查询与支持证据通常异步到达，这要求代理具备强大的推理能力和时间感知能力。

Method: 提出了AViLA代理，包含三个关键模块：全面记忆保留、证据识别和证据触发，以处理流数据中的异步查询。

Result: 实验表明，现有模型在时间感知上表现不佳，而AViLA显著提高了准确性和时间感知能力。

Conclusion: AViLA为解决查询-证据异步问题提供了有效方案，并展示了其在流数据交互中的优势。

Abstract: An ideal vision-language agent serves as a bridge between the human users and
their surrounding physical world in real-world applications like autonomous
driving and embodied agents, and proactively provides accurate and timely
responses given user intents. An intriguing challenge arises when agents
interact with the world as a dynamic data stream and ad-hoc queries from users:
supporting knowledge for queries, namely evidence, usually appears
asynchronously with the arrival time of queries, and agents need to ground
their responses in historical data, present observations, and even future
streams. We frame this challenge as Query-Evidence Asynchrony, where user
queries and their supporting evidence typically arrive asynchronously in the
streaming setting. This setting requires not only strong reasoning capabilities
but also the ability to retain past observations and respond to queries with
temporal awareness. In this paper, we introduce a diagnostic benchmark that
evaluates Multimodal Large Language Models (MLLMs) on their ability to handle
interaction with streaming data. Further, we present AViLA, Asynchronous
Video-Language Agent for streaming data interaction that can handle ad-hoc
queries and give time-aware responses. For this purpose, AViLA consists of
three key modules: comprehensive memory retention, evidence identification, and
evidence-grounded trigger, that are designed to maintain a general-purpose
memory and respond readily and timely to queries. Our experiments show that
existing models often fail to respond at appropriate times, while AViLA
significantly improves both accuracy and temporal awareness. Our code and
dataset will be publicly available.

</details>


### [123] [Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding](https://arxiv.org/abs/2506.18476)
*Yaokun Zhong,Siyu Jiang,Jian Zhu,Jian-Fang Hu*

Main category: cs.CV

TL;DR: SSVPG任务旨在用有限标注定位视频段落中的句子。现有方法忽略查询上下文扰动的重要性，本文提出CCL框架，结合一致性正则化和伪标签，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法关注师生一致性学习和视频级对比损失，但忽略了扰动查询上下文生成强监督信号的重要性。

Method: 提出CCL框架，结合师生学习和伪标签训练，利用强增强样本和预测一致性提升模型性能。

Result: 实验表明CCL大幅优于现有方法。

Conclusion: CCL通过统一一致性正则化和伪标签范式，显著提升了半监督视频段落定位性能。

Abstract: Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple
sentences in a paragraph from an untrimmed video with limited temporal
annotations. Existing methods focus on teacher-student consistency learning and
video-level contrastive loss, but they overlook the importance of perturbing
query contexts to generate strong supervisory signals. In this work, we propose
a novel Context Consistency Learning (CCL) framework that unifies the paradigms
of consistency regularization and pseudo-labeling to enhance semi-supervised
learning. Specifically, we first conduct teacher-student learning where the
student model takes as inputs strongly-augmented samples with sentences removed
and is enforced to learn from the adequately strong supervisory signals from
the teacher model. Afterward, we conduct model retraining based on the
generated pseudo labels, where the mutual agreement between the original and
augmented views' predictions is utilized as the label confidence. Extensive
experiments show that CCL outperforms existing methods by a large margin.

</details>


### [124] [GANs vs. Diffusion Models for virtual staining with the HER2match dataset](https://arxiv.org/abs/2506.18484)
*Pascal Klöckner,José Teixeira,Diana Montezuma,Jaime S. Cardoso,Hugo M. Horlings,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的H&E-HER2染色配对数据集HER2match，并比较了GANs和DMs在H&E-HER2染色转换任务中的表现，发现GANs总体优于DMs，仅BBDM表现接近。


<details>
  <summary>Details</summary>
Motivation: 解决H&E-HER2染色转换任务中缺乏公开数据集和模型性能不明确的问题。

Method: 引入HER2match数据集，比较多种GANs和DMs，并提出新的BBDM模型。

Result: GANs总体表现优于DMs，数据对齐显著提升视觉效果。

Conclusion: 研究提供了高质量数据集和模型比较，为相关研究提供了重要参考。

Abstract: Virtual staining is a promising technique that uses deep generative models to
recreate histological stains, providing a faster and more cost-effective
alternative to traditional tissue chemical staining. Specifically for H&E-HER2
staining transfer, despite a rising trend in publications, the lack of
sufficient public datasets has hindered progress in the topic. Additionally, it
is currently unclear which model frameworks perform best for this particular
task. In this paper, we introduce the HER2match dataset, the first publicly
available dataset with the same breast cancer tissue sections stained with both
H&E and HER2. Furthermore, we compare the performance of several Generative
Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel
Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate
that, overall, GANs perform better than DMs, with only the BBDM achieving
comparable results. Furthermore, we emphasize the importance of data alignment,
as all models trained on HER2match produced vastly improved visuals compared to
the widely used consecutive-slide BCI dataset. This research provides a new
high-quality dataset ([available upon publication acceptance]), improving both
model training and evaluation. In addition, our comparison of frameworks offers
valuable guidance for researchers working on the topic.

</details>


### [125] [ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation](https://arxiv.org/abs/2506.18493)
*Trong-Vu Hoang,Quang-Binh Nguyen,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ShowFlow是一个用于定制化图像生成的框架，包括ShowFlow-S（单概念生成）和ShowFlow-M（多概念生成），通过创新适配器和注意力机制解决身份保持和提示对齐问题。


<details>
  <summary>Details</summary>
Motivation: 在可控图像合成中，单概念生成需要平衡身份保持和提示对齐，而多概念生成则容易因缺乏额外条件导致身份丢失和概念遗漏。

Method: ShowFlow-S使用KronA-WED适配器和解耦学习方法；ShowFlow-M复用ShowFlow-S的模型，引入SAMA和布局一致性策略。

Result: 实验和用户研究表明，ShowFlow在广告和虚拟试衣等实际应用中表现优异。

Conclusion: ShowFlow通过创新方法有效解决了单概念和多概念图像生成的挑战，具有实际应用潜力。

Abstract: Customizing image generation remains a core challenge in controllable image
synthesis. For single-concept generation, maintaining both identity
preservation and prompt alignment is challenging. In multi-concept scenarios,
relying solely on a prompt without additional conditions like layout boxes or
semantic masks, often leads to identity loss and concept omission. In this
paper, we introduce ShowFlow, a comprehensive framework designed to tackle
these challenges. We propose ShowFlow-S for single-concept image generation,
and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a
KronA-WED adapter, which integrates a Kronecker adapter with weight and
embedding decomposition, and employs a disentangled learning approach with a
novel attention regularization objective to enhance single-concept generation.
Building on this foundation, ShowFlow-M directly reuses the learned models from
ShowFlow-S to support multi-concept generation without extra conditions,
incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout
consistency strategy as the plug-and-play module. Extensive experiments and
user studies validate ShowFlow's effectiveness, highlighting its potential in
real-world applications like advertising and virtual dressing.

</details>


### [126] [Biased Teacher, Balanced Student](https://arxiv.org/abs/2506.18496)
*Seonghak Kim*

Main category: cs.CV

TL;DR: 提出了一种针对长尾数据分布的知识蒸馏框架LTKD，通过分解KL散度为组间和组内损失，有效解决了传统KD在长尾数据中的偏差问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在长尾数据中表现不佳，因为教师模型对头部类别有偏差，尾部类别监督不足。

Method: 将标准KD目标分解为组间和组内KL散度，并引入重新平衡的组间损失和均匀的组内损失。

Result: 在多个数据集上，LTKD显著优于现有KD方法，整体准确率和尾部类别性能均有提升。

Conclusion: LTKD能够从有偏差的教师模型中有效转移知识，适用于资源受限和不平衡的实际场景。

Abstract: Knowledge Distillation (KD) is a widely adopted model compression technique
where a compact student model learns from the output of a larger, pre-trained
teacher. While effective in balanced settings, conventional KD suffers
significantly when applied to long-tailed data distributions, as the teacher
model tends to be biased toward head classes and provides limited supervision
for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation
(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by
reformulating the standard KD objective into two components: inter-group and
intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction
distributions across and within class groups (head, medium, tail),
respectively. This decomposition allows us to identify and quantify the sources
of teacher bias. To address them, we introduce (1) a rebalanced inter-group
loss that calibrates the teacher's group-level predictions and (2) a uniform
intra-group loss that ensures equal contribution from all groups during
distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and
ImageNet-LT show that LTKD consistently outperforms existing KD methods,
achieving significant gains in both overall accuracy and tail-class
performance. Our results demonstrate that LTKD enables effective knowledge
transfer even from biased teachers, making it a strong candidate for real-world
deployment in resource-constrained and imbalanced settings.

</details>


### [127] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/abs/2506.18504)
*Xinyao Li,Jingjing Li,Fengling Li,Lei Zhu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 本文综述了视觉语言模型（VLMs）的泛化方法，包括基于提示、参数和特征的方法，并比较了它们在特定任务上的性能。


<details>
  <summary>Details</summary>
Motivation: VLMs在零样本任务中表现优异，但在特定领域任务中性能下降，因此需要研究如何将其知识泛化到下游应用。

Method: 通过分类和总结现有文献，将VLMs泛化方法分为基于提示、参数和特征的三类，并比较其性能。

Result: 综述了VLMs泛化方法的特点和差异，并提供了性能比较，同时讨论了VLMs与多模态大语言模型（MLLM）的关系。

Conclusion: 本文为视觉语言研究提供了清晰的泛化视角，有助于未来多模态研究的发展。

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [128] [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/abs/2506.18512)
*Yuting Zhang,Kaishen Yuan,Hao Lu,Yutao Yue,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: MedTVT-R1是一种新型多模态大语言模型框架，用于整合临床多模态数据以实现多疾病诊断。


<details>
  <summary>Details</summary>
Motivation: 当前方法多依赖单模态数据，难以全面理解复杂疾病，因此需要一种能整合多模态数据的诊断方法。

Method: 提出MedTVT-R1框架，包含模态感知层捕捉模态间依赖关系，并使用GRPO强化微调优化诊断推理。

Result: 实验显示MedTVT-R1在多模态特征利用和多疾病诊断方面表现优越。

Conclusion: MedTVT-R1在临床应用中具有潜力，如诊断报告生成和共病推理。

Abstract: Accurate and interpretable multi-disease diagnosis remains a critical
challenge in medical research, particularly when leveraging heterogeneous
multimodal medical data. Current approaches often rely on single-modal data,
limiting their ability to comprehensively understand complex diseases. To
address this, we propose MedTVT-R1, a novel Multimodal Large Language Model
(MLLM) framework designed to integrate clinical multimodal data for reasoning
and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction
dataset that provides question-answer pairs for physiological-level
interpretations and disease-level diagnoses with a Chain of Evidence approach.
MedTVT-R1 incorporates a modality perception layer to capture inter-modal
dependencies and adaptively weight modality contributions. Additionally, we
employ Group Relative Policy Optimization (GRPO)-based Reinforcement
Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.
Experimental results demonstrate MedTVT-R1's superiority in multimodal feature
utilization and multi-disease diagnosis, offering significant potential for
clinical applications such as diagnostic report generation and comorbidity
reasoning. The dataset and code are available at
https://github.com/keke-nice/MedTVT-R1.

</details>


### [129] [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](https://arxiv.org/abs/2506.18520)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了TEAFormer，一种结合滑动索引和自适应机制的Transformer，用于解决图像修复中的平移等变性问题，并在多个任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现代修复Transformer中的注意力机制破坏了平移等变性，影响了训练收敛和泛化能力。

Method: 提出滑动索引和组件堆叠策略，并设计自适应滑动索引机制，结合全局聚合的键值对。

Result: TEAFormer在多种图像修复任务中表现出更高的有效性、训练收敛性和泛化能力。

Conclusion: TEAFormer通过自适应机制有效平衡了计算成本和感受野，提升了图像修复的性能。

Abstract: Translation equivariance is a fundamental inductive bias in image
restoration, ensuring that translated inputs produce translated outputs.
Attention mechanisms in modern restoration transformers undermine this
property, adversely impacting both training convergence and generalization. To
alleviate this issue, we propose two key strategies for incorporating
translation equivariance: slide indexing and component stacking. Slide indexing
maintains operator responses at fixed positions, with sliding window attention
being a notable example, while component stacking enables the arrangement of
translation-equivariant operators in parallel or sequentially, thereby building
complex architectures while preserving translation equivariance. However, these
strategies still create a dilemma in model design between the high
computational cost of self-attention and the fixed receptive field associated
with sliding window attention. To address this, we develop an adaptive sliding
indexing mechanism to efficiently select key-value pairs for each query, which
are then concatenated in parallel with globally aggregated key-value pairs. The
designed network, called the Translation Equivariance Adaptive Transformer
(TEAFormer), is assessed across a variety of image restoration tasks. The
results highlight its superiority in terms of effectiveness, training
convergence, and generalization.

</details>


### [130] [Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space](https://arxiv.org/abs/2506.18523)
*Kei Taguchi,Kazumasa Ohara,Tatsuya Yokota,Hiroaki Miyoshi,Noriaki Hashimoto,Ichiro Takeuchi,Hidekata Hontani*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中表示恶性淋巴瘤病理图像的方法，通过自监督学习实现从高分辨率细胞核到低分辨率组织图像的统一嵌入。


<details>
  <summary>Details</summary>
Motivation: 捕捉疾病进展中跨尺度的形态变化，通过嵌入组织和细胞核图像的关系来表征疾病状态和细胞类型变化。

Method: 使用Poincaré球作为特征空间，通过自监督学习将组织和细胞核图像嵌入到双曲空间中，基于包含关系使它们彼此接近。

Result: 学习到的表征能够同时捕捉疾病状态和细胞类型的变化。

Conclusion: 该方法在双曲空间中有效编码了病理图像的层次结构，为疾病分析提供了新的表征方式。

Abstract: We propose a method for representing malignant lymphoma pathology images,
from high-resolution cell nuclei to low-resolution tissue images, within a
single hyperbolic space using self-supervised learning. To capture
morphological changes that occur across scales during disease progression, our
approach embeds tissue and corresponding nucleus images close to each other
based on inclusion relationships. Using the Poincar\'e ball as the feature
space enables effective encoding of this hierarchical structure. The learned
representations capture both disease state and cell type variations.

</details>


### [131] [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527)
*JiaKui Hu,Yuxiao Yang,Jialun Liu,Jinbo Wu,Chen Zhao,Yanye Lu*

Main category: cs.CV

TL;DR: MV-AR方法通过自回归模型逐步生成一致的多视角图像，支持多种提示条件，并通过数据增强技术提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决多视角图像生成中的一致性和多样性挑战。

Method: 采用自回归模型和条件注入模块，结合渐进训练策略和数据增强技术。

Result: 实验表明MV-AR能生成一致的多视角图像，性能与领先的扩散模型相当。

Conclusion: MV-AR在多视角图像生成中表现出色，具有广泛的应用潜力。

Abstract: Generating multi-view images from human instructions is crucial for 3D
content creation. The primary challenges involve maintaining consistency across
multiple views and effectively synthesizing shapes and textures under diverse
conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)
method, which leverages an auto-regressive model to progressively generate
consistent multi-view images from arbitrary prompts. Firstly, the
next-token-prediction capability of the AR model significantly enhances its
effectiveness in facilitating progressive multi-view synthesis. When generating
widely-separated views, MV-AR can utilize all its preceding views to extract
effective reference information. Subsequently, we propose a unified model that
accommodates various prompts via architecture designing and training
strategies. To address multiple conditions, we introduce condition injection
modules for text, camera pose, image, and shape. To manage multi-modal
conditions simultaneously, a progressive training strategy is employed. This
strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to
enhance the development of a comprehensive X-to-multi-view (X2mv) model through
the randomly dropping and combining conditions. Finally, to alleviate the
overfitting problem caused by limited high-quality data, we propose the
"Shuffle View" data augmentation technique, thus significantly expanding the
training data by several magnitudes. Experiments demonstrate the performance
and versatility of our MV-AR, which consistently generates consistent
multi-view images across a range of conditions and performs on par with leading
diffusion-based multi-view image generation models. Code and models will be
released at https://github.com/MILab-PKU/MVAR.

</details>


### [132] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出了一种双曲集合间距离度量方法（HS2SD），结合全局和局部结构信息，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中需要比较双曲空间中的集合，而现有方法未能充分捕捉集合的全局和局部结构信息。

Method: 通过双曲集合的Einstein中点间的测地距离和拓扑特征（使用Thue-Morse序列近似）来整合全局和局部结构。

Result: 在实体匹配、标准图像分类和少样本图像分类任务中表现优于现有方法。

Conclusion: HS2SD能更细致地建模双曲集合间的层次和复杂关系。

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [133] [Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces](https://arxiv.org/abs/2506.18533)
*Pengxiang Li,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Wei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中动态适应不同层次结构的几何感知距离度量方法，显著提升了分类和少样本学习任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有双曲学习方法假设所有数据具有统一的层次结构，而实际数据的层次结构多样，固定距离度量过于限制。

Method: 通过为每对数据点生成定制投影和曲率，动态调整距离度量，并引入低秩分解和硬对挖掘机制降低计算成本。

Result: 在多个数据集上优于固定距离度量方法，少样本学习任务中性能提升显著（如mini-ImageNet上提升5%以上）。

Conclusion: 自适应距离度量能更好地捕捉多样层次结构，可视化显示更清晰的类别边界和原型分离。

Abstract: Learning in hyperbolic spaces has attracted increasing attention due to its
superior ability to model hierarchical structures of data. Most existing
hyperbolic learning methods use fixed distance measures for all data, assuming
a uniform hierarchy across all data points. However, real-world hierarchical
structures exhibit significant diversity, making this assumption overly
restrictive. In this paper, we propose a geometry-aware distance measure in
hyperbolic spaces, which dynamically adapts to varying hierarchical structures.
Our approach derives the distance measure by generating tailored projections
and curvatures for each pair of data points, effectively mapping them to an
appropriate hyperbolic space. We introduce a revised low-rank decomposition
scheme and a hard-pair mining mechanism to mitigate the computational cost of
pair-wise distance computation without compromising accuracy. We present an
upper bound on the low-rank approximation error using Talagrand's concentration
inequality, ensuring theoretical robustness. Extensive experiments on standard
image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical
classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet,
tiered-ImageNet) demonstrate the effectiveness of our method. Our approach
consistently outperforms learning methods that use fixed distance measures,
with notable improvements on few-shot learning tasks, where it achieves over
5\% gains on mini-ImageNet. The results reveal that adaptive distance measures
better capture diverse hierarchical structures, with visualization showing
clearer class boundaries and improved prototype separation in hyperbolic
spaces.

</details>


### [134] [Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection](https://arxiv.org/abs/2506.18544)
*Muhao Xu,Xueying Zhou,Xizhan Gao,Weiye Song,Guang Feng,Sijie Niu*

Main category: cs.CV

TL;DR: 提出了一种基于多语义融合网络的无监督异常检测方法，通过引入正常样本的多语义特征来引导异常重建，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 逻辑异常检测比结构异常更具挑战性，现有方法在低维瓶颈中难以有效抑制异常传播，导致重建结果误导性高。

Method: 利用预训练的视觉语言网络提取正常样本的全局语义，构建可学习的语义码本，通过多语义特征融合引导异常重建。

Result: 在MVTec LOCO AD数据集上达到SOTA性能，像素级sPRO提升5.7%，图像级AUROC提升2.6%。

Conclusion: 通过引入正常样本的多语义特征，有效解决了逻辑异常检测中的瓶颈问题，显著提升了检测精度。

Abstract: Recently, detecting logical anomalies is becoming a more challenging task
compared to detecting structural ones. Existing encoder decoder based methods
typically compress inputs into low-dimensional bottlenecks on the assumption
that the compression process can effectively suppress the transmission of
logical anomalies to the decoder. However, logical anomalies present a
particular difficulty because, while their local features often resemble normal
semantics, their global semantics deviate significantly from normal patterns.
Thanks to the generalisation capabilities inherent in neural networks, these
abnormal semantic features can propagate through low-dimensional bottlenecks.
This ultimately allows the decoder to reconstruct anomalous images with
misleading fidelity. To tackle the above challenge, we propose a novel
normality prior guided multi-semantic fusion network for unsupervised anomaly
detection. Instead of feeding the compressed bottlenecks to the decoder
directly, we introduce the multi-semantic features of normal samples into the
reconstruction process. To this end, we first extract abstract global semantics
of normal cases by a pre-trained vision-language network, then the learnable
semantic codebooks are constructed to store representative feature vectors of
normal samples by vector quantisation. Finally, the above multi-semantic
features are fused and employed as input to the decoder to guide the
reconstruction of anomalies to approximate normality. Extensive experiments are
conducted to validate the effectiveness of our proposed method, and it achieves
the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in
pixel-sPRO and 2.6% in image-AUROC. The source code is available at
https://github.com/Xmh-L/NPGMF.

</details>


### [135] [Object-aware Sound Source Localization via Audio-Visual Scene Understanding](https://arxiv.org/abs/2506.18557)
*Sung Jin Um,Dongjin Kim,Sangmin Lee,Jung Uk Kim*

Main category: cs.CV

TL;DR: 提出了一种基于多模态大语言模型（MLLMs）的声源定位框架，通过生成详细上下文信息区分发声物体与静默物体，并引入两种新型损失函数（OCA和ORI），显著提升了复杂场景中的定位准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中（尤其是存在视觉相似静默物体时）难以准确定位发声物体，主要因其依赖简单的视听对应关系，无法捕捉语义细节。

Method: 利用MLLMs生成详细上下文信息，明确区分发声物体与静默物体；设计OCA和ORI损失函数以有效整合信息。

Result: 在MUSIC和VGGSound数据集上显著优于现有方法，适用于单源和多源定位场景。

Conclusion: 所提框架通过语义细粒度区分和新型损失函数，解决了复杂场景中的声源定位问题，具有实际应用价值。

Abstract: Audio-visual sound source localization task aims to spatially localize
sound-making objects within visual scenes by integrating visual and audio cues.
However, existing methods struggle with accurately localizing sound-making
objects in complex scenes, particularly when visually similar silent objects
coexist. This limitation arises primarily from their reliance on simple
audio-visual correspondence, which does not capture fine-grained semantic
differences between sound-making and silent objects. To address these
challenges, we propose a novel sound source localization framework leveraging
Multimodal Large Language Models (MLLMs) to generate detailed contextual
information that explicitly distinguishes between sound-making foreground
objects and silent background objects. To effectively integrate this detailed
information, we introduce two novel loss functions: Object-aware Contrastive
Alignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive
experimental results on MUSIC and VGGSound datasets demonstrate the
effectiveness of our approach, significantly outperforming existing methods in
both single-source and multi-source localization scenarios. Code and generated
detailed contextual information are available at:
https://github.com/VisualAIKHU/OA-SSL.

</details>


### [136] [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/abs/2506.18564)
*Xuanyu Zhang,Weiqi Li,Shijie Zhao,Junlin Li,Li Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: VQ-Insight是一种新型的推理式视觉语言模型框架，用于评估AI生成视频的质量，通过渐进式学习和多维度奖励设计显著提升了评估性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频的质量评估方法存在泛化性不足、缺乏时间感知、依赖大规模标注数据等问题，亟需更有效的评估框架。

Method: 提出VQ-Insight框架，结合渐进式视频质量学习方案和多维度奖励设计（如评分奖励、偏好比较奖励等）。

Result: 实验表明VQ-Insight在偏好比较、多维度评分等任务中优于现有方法，显著提升了视频生成任务的效果。

Conclusion: VQ-Insight通过创新的学习策略和奖励机制，为AI生成视频的质量评估提供了更高效的解决方案。

Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of
powerful text-to-video generation models. Despite these successes, evaluating
the quality of AIGC-generated videos remains challenging due to limited
generalization, lack of temporal awareness, heavy reliance on large-scale
annotated datasets, and the lack of effective interaction with generation
models. Most current approaches rely on supervised finetuning of
vision-language models (VLMs), which often require large-scale annotated
datasets and tend to decouple understanding and generation. To address these
shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for
AIGC video quality assessment. Our approach features: (1) a progressive video
quality learning scheme that combines image quality warm-up, general
task-specific temporal learning, and joint optimization with the video
generation model; (2) the design of multi-dimension scoring rewards, preference
comparison rewards, and temporal modeling rewards to enhance both
generalization and specialization in video quality evaluation. Extensive
experiments demonstrate that VQ-Insight consistently outperforms
state-of-the-art baselines in preference comparison, multi-dimension scoring,
and natural video scoring, bringing significant improvements for video
generation tasks.

</details>


### [137] [VisualChef: Generating Visual Aids in Cooking via Mask Inpainting](https://arxiv.org/abs/2506.18569)
*Oleh Kuzyk,Zuoyue Li,Marc Pollefeys,Xi Wang*

Main category: cs.CV

TL;DR: VisualChef是一种为烹饪场景生成上下文视觉辅助的方法，通过掩码视觉定位简化对齐，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 烹饪过程中缺乏一致的视觉指导，现有方法依赖文本描述，需要额外标注。

Method: VisualChef通过掩码视觉定位识别动作相关对象并分类，生成反映动作和结果的图像，同时保持环境一致。

Result: 在三个第一人称视频数据集上定量和定性评估显示，VisualChef优于现有方法。

Conclusion: VisualChef简化了视觉辅助生成，提升了烹饪场景的视觉指导效果。

Abstract: Cooking requires not only following instructions but also understanding,
executing, and monitoring each step - a process that can be challenging without
visual guidance. Although recipe images and videos offer helpful cues, they
often lack consistency in focus, tools, and setup. To better support the
cooking process, we introduce VisualChef, a method for generating contextual
visual aids tailored to cooking scenarios. Given an initial frame and a
specified action, VisualChef generates images depicting both the action's
execution and the resulting appearance of the object, while preserving the
initial frame's environment. Previous work aims to integrate knowledge
extracted from large language models by generating detailed textual
descriptions to guide image generation, which requires fine-grained
visual-textual alignment and involves additional annotations. In contrast,
VisualChef simplifies alignment through mask-based visual grounding. Our key
insight is identifying action-relevant objects and classifying them to enable
targeted modifications that reflect the intended action and outcome while
maintaining a consistent environment. In addition, we propose an automated
pipeline to extract high-quality initial, action, and final state frames. We
evaluate VisualChef quantitatively and qualitatively on three egocentric video
datasets and show its improvements over state-of-the-art methods.

</details>


### [138] [2D Triangle Splatting for Direct Differentiable Mesh Training](https://arxiv.org/abs/2506.18575)
*Kaifeng Sheng,Zheng Zhou,Yingliang Peng,Qianwei Wang*

Main category: cs.CV

TL;DR: 提出了一种名为2D Triangle Splatting（2DTS）的新方法，用2D三角形面片替代3D高斯基元，结合了离散网格结构和连续体积建模的优势。


<details>
  <summary>Details</summary>
Motivation: 尽管基于3D高斯基元的可微渲染在多视图图像重建中表现出色，但在渲染速度和高级渲染效果（如重光照和阴影渲染）方面仍存在挑战。

Method: 通过引入紧凑性参数到三角形基元中，直接训练出逼真的网格。

Result: 实验表明，该方法在未调整紧凑性的情况下，比现有高斯基元方法具有更高的保真度，且重建网格的视觉质量优于现有网格重建方法。

Conclusion: 2DTS方法结合了网格和体积建模的优势，为高保真3D场景重建提供了新的解决方案。

Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a
powerful method for reconstructing high-fidelity 3D scenes from multi-view
images. While it offers improvements over NeRF-based methods, this
representation still encounters challenges with rendering speed and advanced
rendering effects, such as relighting and shadow rendering, compared to
mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a
novel method that replaces 3D Gaussian primitives with 2D triangle facelets.
This representation naturally forms a discrete mesh-like structure while
retaining the benefits of continuous volumetric modeling. By incorporating a
compactness parameter into the triangle primitives, we enable direct training
of photorealistic meshes. Our experimental results demonstrate that our
triangle-based method, in its vanilla version (without compactness tuning),
achieves higher fidelity compared to state-of-the-art Gaussian-based methods.
Furthermore, our approach produces reconstructed meshes with superior visual
quality compared to existing mesh reconstruction methods.

</details>


### [139] [Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing](https://arxiv.org/abs/2506.18587)
*Antoine Saget,Baptiste Lafabregue,Antoine Cornuéjols,Pierre Gançarski*

Main category: cs.CV

TL;DR: 提出了一种基于重采样的对比学习数据增强策略，用于卫星图像时间序列（SITS），在农业分类任务中表现优于常见方法。


<details>
  <summary>Details</summary>
Motivation: 由于卫星图像时间序列（SITS）中未标记数据丰富而标记数据稀缺，对比自监督预训练成为利用未标记数据的有效工具，但设计适用于时间序列的数据增强方法仍具挑战性。

Method: 通过上采样时间序列并提取不相交的子序列以生成正样本对，同时保留时间覆盖范围。

Result: 在多个农业分类基准测试中表现优于常见方法（如抖动、调整大小和掩码），并在S2-Agri100数据集上实现了最先进的性能。

Conclusion: 该方法为遥感时间序列提供了一种简单而有效的对比学习增强策略。

Abstract: Given the abundance of unlabeled Satellite Image Time Series (SITS) and the
scarcity of labeled data, contrastive self-supervised pretraining emerges as a
natural tool to leverage this vast quantity of unlabeled data. However,
designing effective data augmentations for contrastive learning remains
challenging for time series. We introduce a novel resampling-based augmentation
strategy that generates positive pairs by upsampling time series and extracting
disjoint subsequences while preserving temporal coverage. We validate our
approach on multiple agricultural classification benchmarks using Sentinel-2
imagery, showing that it outperforms common alternatives such as jittering,
resizing, and masking. Further, we achieve state-of-the-art performance on the
S2-Agri100 dataset without employing spatial information or temporal encodings,
surpassing more complex masked-based SSL frameworks. Our method offers a
simple, yet effective, contrastive learning augmentation for remote sensing
time series.

</details>


### [140] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,György Dán,Henrik Sandberg*

Main category: cs.CV

TL;DR: SpaNN是一种新型的对抗性攻击检测器，其计算复杂度与对抗性补丁数量无关，通过二值化特征图和聚类方法实现高效检测。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法主要针对单补丁攻击，对多补丁攻击效果不佳或计算效率低下，SpaNN旨在解决这一问题。

Method: SpaNN通过应用一组显著性阈值生成二值化特征图集合，进行聚类后输入分类器进行攻击检测。

Result: 在四个数据集上，SpaNN在目标检测和图像分类任务中分别比现有方法提升了11和27个百分点。

Conclusion: SpaNN通过动态显著性阈值和聚类方法，显著提升了对抗性攻击检测的鲁棒性和效率。

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [141] [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](https://arxiv.org/abs/2506.18655)
*Wenxu Qian,Chaoyue Wang,Hou Peng,Zhiyu Tan,Hao Li,Anxiang Zeng*

Main category: cs.CV

TL;DR: RDPO是一种无需标注的框架，通过从真实视频中提取物理先验，显著提升生成视频的物理一致性和动作连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术在物理一致性上表现不足，而基于偏好的后训练方法需要昂贵的人工标注数据或奖励模型。

Method: RDPO通过反向采样真实视频序列自动构建偏好对，并采用多阶段迭代训练提升生成器的物理一致性。

Result: RDPO在多个基准测试和人类评估中显著提升了生成视频的物理真实性和动作连贯性。

Conclusion: RDPO提供了一种高效且无需标注的方法，显著提升了视频生成的物理一致性。

Abstract: Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/

</details>


### [142] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/abs/2506.18658)
*Ling Zhang,Boxiang Yun,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: BiGen框架通过双模态并发学习策略和知识检索机制，解决了病理报告生成中的语义内容不足和信息冗余问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决病理报告生成中视觉特征缺乏语义内容和WSI信息冗余的问题。

Method: 提出BiGen框架，包括知识检索机制和双模态并发学习策略，通过共享权重层实现跨模态对齐。

Result: 在PathText数据集上，NLP指标相对提升7.4%，Her-2预测分类指标提升19.1%。

Conclusion: BiGen框架能有效提供语义内容并抑制信息冗余，实验验证了其优越性。

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [143] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/abs/2506.18668)
*Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基准测试方法，用于评估组织病理学基础模型（FM）在多实例学习框架中的特征提取能力，并引入了一种新的度量标准FM-SI来衡量模型对分布变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于组织病理学基础模型的多样性，需要设计真实世界的挑战来评估其有效性，尤其是在处理大规模全切片图像时。

Method: 利用AI4SkIN数据集，设计了一个基准测试，将FM作为补丁级特征提取器，并在多实例学习分类框架中评估其性能。同时提出了FM-SI度量标准。

Result: 实验表明，提取较少偏差的特征可以提高分类性能，特别是在基于相似性的多实例学习分类器中。

Conclusion: 该研究为组织病理学基础模型的评估提供了新方法，并展示了其在处理复杂病理图像中的潜力。

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [144] [MedSeg-R: Medical Image Segmentation with Clinical Reasoning](https://arxiv.org/abs/2506.18669)
*Hao Shao,Qibin Hou*

Main category: cs.CV

TL;DR: MedSeg-R是一个轻量级的双阶段框架，通过结合医学报告的结构化语义先验和SAM骨干网络，显著提高了对小病灶的分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割面临重叠解剖结构、模糊边界和类别不平衡的挑战，现有方法缺乏语义先验，难以处理低对比度或重叠目标。

Method: MedSeg-R采用双阶段设计：认知阶段将医学报告解析为结构化语义先验（位置、纹理、形状），感知阶段通过空间注意力、动态卷积和可变形采样调制SAM骨干网络。

Result: 在挑战性基准测试中，MedSeg-R显著提高了重叠和模糊结构的Dice分数，表现出与SAM系统的即插即用兼容性。

Conclusion: MedSeg-R通过嵌入细粒度语义先验，有效解决了医学图像分割中的类别混淆和小病灶敏感性问题。

Abstract: Medical image segmentation is challenging due to overlapping anatomies with
ambiguous boundaries and a severe imbalance between the foreground and
background classes, which particularly affects the delineation of small
lesions. Existing methods, including encoder-decoder networks and prompt-driven
variants of the Segment Anything Model (SAM), rely heavily on local cues or
user prompts and lack integrated semantic priors, thus failing to generalize
well to low-contrast or overlapping targets. To address these issues, we
propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by
clinical reasoning. Its cognitive stage interprets medical report into
structured semantic priors (location, texture, shape), which are fused via
transformer block. In the perceptual stage, these priors modulate the SAM
backbone: spatial attention highlights likely lesion regions, dynamic
convolution adapts feature filters to expected textures, and deformable
sampling refines spatial support. By embedding this fine-grained guidance
early, MedSeg-R disentangles inter-class confusion and amplifies minority-class
cues, greatly improving sensitivity to small lesions. In challenging
benchmarks, MedSeg-R produces large Dice improvements in overlapping and
ambiguous structures, demonstrating plug-and-play compatibility with SAM-based
systems.

</details>


### [145] [Reconstructing Tornadoes in 3D with Gaussian Splatting](https://arxiv.org/abs/2506.18677)
*Adam Yang,Nadula Kadawedduwa,Tianfu Wang,Maria Molina,Christopher Metzler*

Main category: cs.CV

TL;DR: 论文提出了一种基于实验室的小型龙卷风多视角数据集，并展示了使用3D高斯泼溅技术（3DGS）有效重建其3D结构的方法。


<details>
  <summary>Details</summary>
Motivation: 准确重建龙卷风的3D结构对于理解和应对这种破坏性天气现象至关重要，但目前缺乏用于开发和验证相关工具的受控数据集。

Method: 通过捕获和发布实验室小型龙卷风的多视角数据集，并利用3D高斯泼溅技术（3DGS）进行3D重建。

Result: 成功重建并可视化了实验室龙卷风的3D结构。

Conclusion: 该数据集和方法为未来研究龙卷风的3D结构提供了有价值的工具和基础。

Abstract: Accurately reconstructing the 3D structure of tornadoes is critically
important for understanding and preparing for this highly destructive weather
phenomenon. While modern 3D scene reconstruction techniques, such as 3D
Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the
3D structure of tornados, at present we are critically lacking a controlled
tornado dataset with which to develop and validate these tools. In this work we
capture and release a novel multiview dataset of a small lab-based tornado. We
demonstrate one can effectively reconstruct and visualize the 3D structure of
this tornado using 3DGS.

</details>


### [146] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/abs/2506.18678)
*Tianchen Deng,Guole Shen,Xun Chen,Shenghai Yuan,Hongming Shen,Guohao Peng,Zhenyu Wu,Jingchuan Wang,Lihua Xie,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: 提出首个分布式多Agent协作神经SLAM框架，结合混合场景表示、分布式相机跟踪、闭环检测和在线蒸馏，并发布首个真实世界密集SLAM数据集。


<details>
  <summary>Details</summary>
Motivation: 现有隐式SLAM算法局限于单Agent场景，且在大规模场景和长序列中表现不佳，多Agent框架又受限于通信带宽。

Method: 采用混合场景表示（triplane-grid）、分布式相机跟踪、闭环检测和在线蒸馏技术。

Result: 实验证明该方法在映射、跟踪和通信方面表现优越。

Conclusion: 提出的框架和数据集推动了SLAM、3D重建和视觉基础模型的研究发展。

Abstract: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.

</details>


### [147] [MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation](https://arxiv.org/abs/2506.18679)
*Ruicheng Zhang,Yu Sun,Zeyu Zhang,Jinai Li,Xiaofan Liu,Au Hoi Fan,Haowei Guo,Puxin Yan*

Main category: cs.CV

TL;DR: MARL-MambaContour是一种基于多智能体强化学习的医学图像分割框架，通过生成拓扑一致的轮廓来解决传统像素方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统像素级分割方法缺乏拓扑约束和对解剖区域整体结构的感知能力，因此需要一种新的方法来生成更准确的轮廓。

Method: 将每个轮廓点建模为自主智能体，通过迭代调整位置来精确对齐目标边界，使用改进的SAC算法和Mamba策略网络优化。

Result: 在五个医学影像数据集上的实验表明，MARL-MambaContour达到了最先进的性能。

Conclusion: 该框架在医学图像分割中表现出高准确性和鲁棒性，具有临床应用潜力。

Abstract: We introduce MARL-MambaContour, the first contour-based medical image
segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our
approach reframes segmentation as a multi-agent cooperation task focused on
generate topologically consistent object-level contours, addressing the
limitations of traditional pixel-based methods which could lack topological
constraints and holistic structural awareness of anatomical regions. Each
contour point is modeled as an autonomous agent that iteratively adjusts its
position to align precisely with the target boundary, enabling adaptation to
blurred edges and intricate morphologies common in medical images. This
iterative adjustment process is optimized by a contour-specific Soft
Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization
Adjustment Mechanism (ERAM) which dynamically balance agent exploration with
contour smoothness. Furthermore, the framework incorporates a Mamba-based
policy network featuring a novel Bidirectional Cross-attention Hidden-state
Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion
limitations associated with long-range modeling in state space models, thereby
facilitating more accurate inter-agent information exchange and informed
decision-making. Extensive experiments on five diverse medical imaging datasets
demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting
its potential as an accurate and robust clinical application.

</details>


### [148] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/abs/2506.18682)
*Imad Ali Shah,Jiarong Li,Tim Brophy,Martin Glavin,Edward Jones,Enda Ward,Brian Deegan*

Main category: cs.CV

TL;DR: 论文提出了一种多尺度光谱注意力模块（MSAM），通过并行1D卷积和自适应特征聚合机制增强光谱特征提取，显著提升了高光谱图像（HSI）在自动驾驶中的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）在自动驾驶中具有潜力，但其高维数据的高效处理仍是一个挑战。

Method: 将MSAM集成到UNet的跳跃连接中（UNet-MSAM），通过多尺度1D卷积和自适应特征聚合机制优化特征提取。

Result: 在多个HSI数据集上，UNet-MSAM平均提升了3.61%的mIoU和3.80%的mF1，且计算开销极小。

Conclusion: 多尺度核组合优于单尺度配置，为自动驾驶中的HSI处理提供了实用设计思路。

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [149] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/abs/2506.18683)
*Youcef Sklab,Hanane Ariouat,Eric Chenin,Edi Prifti,Jean-Daniel Zucker*

Main category: cs.CV

TL;DR: SIM-Net是一种新型的2D图像分类架构，通过将2D图像转换为3D点云，结合纹理和几何特征提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 传统图像分类模型在处理数字化植物标本时，因背景复杂、遮挡等问题表现不佳，SIM-Net通过融合3D几何特征来解决这些问题。

Method: SIM-Net通过像素到点的转换生成3D点云，结合CNN和PointNet编码器提取特征，并在统一潜在空间中进行融合。

Result: 在植物标本数据集上，SIM-Net比ResNet101准确率提升9.9%，F-score提升12.3%，并优于多种先进Transformer架构。

Conclusion: SIM-Net证明了在2D图像分类中引入3D结构推理的有效性，尤其在复杂背景下表现突出。

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [150] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game是一个用于可控游戏世界生成的交互式世界基础模型，通过两阶段训练和大型数据集支持，实现了高质量和可控的视频生成。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确控制角色动作和相机运动的游戏世界生成模型，同时保持高视觉质量和时间一致性。

Method: 采用两阶段训练流程：大规模无标签预训练和环境理解，以及带有动作标签的训练用于交互式视频生成。使用Matrix-Game-MC数据集（包含大量标记和未标记的游戏视频片段）。

Result: Matrix-Game在视觉质量、时间质量、动作可控性和物理规则理解方面均优于现有开源模型（如Oasis和MineWorld）。

Conclusion: Matrix-Game能够生成感知真实且精确可控的视频，为交互式图像到世界的生成研究提供了有力工具。

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [151] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出了一种基于骨架的动作识别新方法，通过词嵌入编码语义信息，显著提升了分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统骨架方法在复杂交互中丢失关键点语义，限制了其有效性。

Method: 利用词嵌入编码语义信息，替换独热编码为语义体积，捕捉关节与物体间的关系。

Result: 在多个装配数据集上实验表明，分类性能显著提升，同时支持不同骨架类型和物体类别。

Conclusion: 语义信息的融入能有效增强动态多样环境中基于骨架的动作识别。

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [152] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/abs/2506.18731)
*Aman Bhatta,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 现代深度CNN人脸匹配器支持可撤销的生物识别方案，生成多个具有等效识别能力但模板不兼容的模型，而Vision Transformer在此方案中表现较差。


<details>
  <summary>Details</summary>
Motivation: 解决生物识别中一旦生物特征被泄露后无法撤销的问题。

Method: 利用深度CNN生成多个具有等效识别能力但模板不兼容的模型，并探索Vision Transformer的适用性。

Result: 深度CNN模型支持可撤销生物识别，模板不兼容性高；Vision Transformer表现不佳。

Conclusion: 深度CNN是实现可撤销生物识别的有效方法，而Vision Transformer不适合此方案。

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [153] [USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways](https://arxiv.org/abs/2506.18737)
*Shanliang Yao,Runwei Guan,Yi Ni,Sen Xu,Yong Yue,Xiaohui Zhu,Ryan Wen Liu*

Main category: cs.CV

TL;DR: 论文介绍了USVTrack数据集和RCM方法，用于提升内河航道中无人水面车辆的目标跟踪能力。


<details>
  <summary>Details</summary>
Motivation: 内河航道中的目标跟踪对安全和经济应用至关重要，但复杂的水上环境增加了跟踪难度。

Method: 利用4D雷达和单目相机等传感器收集数据，并提出RCM方法用于雷达-相机匹配。

Result: 实验证明RCM方法能显著提高目标跟踪的准确性和可靠性。

Conclusion: USVTrack数据集和RCM方法为新一代水上自动驾驶系统提供了有效工具。

Abstract: Object tracking in inland waterways plays a crucial role in safe and
cost-effective applications, including waterborne transportation, sightseeing
tours, environmental monitoring and surface rescue. Our Unmanned Surface
Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,
delivers robust tracking capabilities in complex waterborne environments. By
leveraging these sensors, our USV collected comprehensive object tracking data,
which we present as USVTrack, the first 4D radar-camera tracking dataset
tailored for autonomous driving in new generation waterborne transportation
systems. Our USVTrack dataset presents rich scenarios, featuring diverse
various waterways, varying times of day, and multiple weather and lighting
conditions. Moreover, we present a simple but effective radar-camera matching
method, termed RCM, which can be plugged into popular two-stage association
trackers. Experimental results utilizing RCM demonstrate the effectiveness of
the radar-camera matching in improving object tracking accuracy and reliability
for autonomous driving in waterborne environments. The USVTrack dataset is
public on https://usvtrack.github.io.

</details>


### [154] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Main category: cs.CV

TL;DR: 提出了一种名为SWA的新型注意力机制，用于提升自动驾驶中的语义占用预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的语义占用预测方法缺乏对空间结构的显式建模，导致在稀疏或遮挡区域性能不佳。

Method: 提出了Spatially-aware Window Attention (SWA)，将局部空间上下文融入注意力计算中。

Result: SWA显著提升了场景补全效果，并在LiDAR和相机基准测试中均取得最佳性能。

Conclusion: SWA是一种通用且有效的注意力机制，适用于多模态语义占用预测任务。

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [155] [3D Arena: An Open Platform for Generative 3D Evaluation](https://arxiv.org/abs/2506.18787)
*Dylan Ebert*

Main category: cs.CV

TL;DR: 3D Arena是一个开放平台，通过大规模人类偏好收集评估图像到3D生成模型，填补了自动指标与人类感知质量之间的差距。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成3D模型的基准依赖于忽略3D结构的基于图像的指标或无法捕捉感知吸引力和实际效用的几何测量，因此需要更准确的评估方法。

Method: 3D Arena通过成对比较收集人类偏好数据，使用统计欺诈检测确保用户真实性，并基于ELO的排名系统评估模型。

Result: 平台收集了123,243票，覆盖19个最先进模型，揭示了人类偏好模式，如高斯样条输出比网格模型有16.6 ELO优势。

Conclusion: 3D Arena成为生成3D领域的基准，推动了以人为中心的评估方法的发展，并提出了改进评估方法的建议。

Abstract: Evaluating Generative 3D models remains challenging due to misalignment
between automated metrics and human perception of quality. Current benchmarks
rely on image-based metrics that ignore 3D structure or geometric measures that
fail to capture perceptual appeal and real-world utility. To address this gap,
we present 3D Arena, an open platform for evaluating image-to-3D generation
models through large-scale human preference collection using pairwise
comparisons.
  Since launching in June 2024, the platform has collected 123,243 votes from
8,096 users across 19 state-of-the-art models, establishing the largest human
preference evaluation for Generative 3D. We contribute the iso3d dataset of 100
evaluation prompts and demonstrate quality control achieving 99.75% user
authenticity through statistical fraud detection. Our ELO-based ranking system
provides reliable model assessment, with the platform becoming an established
evaluation resource.
  Through analysis of this preference data, we present insights into human
preference patterns. Our findings reveal preferences for visual presentation
features, with Gaussian splat outputs achieving a 16.6 ELO advantage over
meshes and textured models receiving a 144.1 ELO advantage over untextured
models. We provide recommendations for improving evaluation methods, including
multi-criteria assessment, task-oriented evaluation, and format-aware
comparison. The platform's community engagement establishes 3D Arena as a
benchmark for the field while advancing understanding of human-centered
evaluation in Generative 3D.

</details>


### [156] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.CV

TL;DR: 提出了一种新的Super-Pixel Based Patch Pooling (SPPP)技术和Light Latent Attention (LLA)模块，以降低Vision Transformers的计算和内存需求，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算和内存资源上依赖性强，且任务特定迁移学习困难，主要由于自注意力机制的高计算复杂度。

Method: 通过SPPP生成语义丰富的patch嵌入，结合LLA模块引入潜在令牌，降低注意力模块的时间和空间复杂度。

Result: 实验表明，该方法显著提高了计算效率，同时性能与最先进方法相当。

Conclusion: 该方法为边缘部署提供了高效的Transformer解决方案。

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [157] [ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs](https://arxiv.org/abs/2506.18792)
*Michal Nazarczuk,Sibi Catley-Chandar,Thomas Tanay,Zhensong Zhang,Gregory Slabaugh,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ViDAR利用个性化扩散模型生成伪多视角监督信号，通过高斯溅射表示训练，解决了单目视频中动态新视角合成的挑战。


<details>
  <summary>Details</summary>
Motivation: 动态新视角合成在单目视频中因结构-运动解耦问题和监督稀缺而极具挑战性。

Method: ViDAR结合扩散感知损失函数和相机姿态优化策略，利用场景特定特征恢复细节并减少单目模糊带来的伪影。

Result: 在DyCheck基准测试中，ViDAR在视觉质量和几何一致性上优于所有现有方法。

Conclusion: ViDAR在动态区域表现优异，为运动丰富场景的重建提供了新基准。

Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving
subjects from arbitrary viewpoints. This task is particularly challenging when
relying on monocular video, where disentangling structure from motion is
ill-posed and supervision is scarce. We introduce Video Diffusion-Aware
Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages
personalised diffusion models to synthesise a pseudo multi-view supervision
signal for training a Gaussian splatting representation. By conditioning on
scene-specific features, ViDAR recovers fine-grained appearance details while
mitigating artefacts introduced by monocular ambiguity. To address the
spatio-temporal inconsistency of diffusion-based supervision, we propose a
diffusion-aware loss function and a camera pose optimisation strategy that
aligns synthetic views with the underlying scene geometry. Experiments on
DyCheck, a challenging benchmark with extreme viewpoint variation, show that
ViDAR outperforms all state-of-the-art baselines in visual quality and
geometric consistency. We further highlight ViDAR's strong improvement over
baselines on dynamic regions and provide a new benchmark to compare performance
in reconstructing motion-rich parts of the scene. Project page:
https://vidar-4d.github.io

</details>


### [158] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Main category: cs.CV

TL;DR: OC-SOP框架通过整合目标检测信息，显著提升了语义占据预测的准确性，尤其在动态前景物体上表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知因环境遮挡和场景数据不完整面临挑战，现有方法对所有类别平等处理且依赖局部特征，导致预测效果不佳。

Method: 提出Object-Centric SOP（OC-SOP），将目标检测分支提取的高层目标中心线索整合到语义占据预测流程中。

Result: 在SemanticKITTI数据集上，OC-SOP显著提升了前景物体的预测准确性，并在所有类别中达到最优性能。

Conclusion: OC-SOP通过目标中心线索的整合，有效解决了语义占据预测中的动态物体预测问题，实现了整体性能的提升。

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [159] [PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications](https://arxiv.org/abs/2506.18807)
*Pietro Bonazzi,Nicola Farronato,Stefan Zihlmann,Haotong Qi,Michele Magno*

Main category: cs.CV

TL;DR: PicoSAM2是一个轻量级（1.3M参数，336M MACs）的可提示分割模型，专为边缘和传感器内执行优化，适用于实时、设备上的分割任务。


<details>
  <summary>Details</summary>
Motivation: 实时、设备上的分割对于延迟敏感和隐私保护的应用（如智能眼镜和物联网设备）至关重要。

Method: 基于深度可分离U-Net，通过知识蒸馏和固定点提示编码从Segment Anything Model 2 (SAM2)中学习。

Result: 在COCO和LVIS上分别达到51.9%和44.9%的mIoU，量化模型（1.22MB）在IMX500上运行时间为14.3 ms，满足传感器内部署的内存和计算限制。

Conclusion: 高效的、可提示的分割可以直接在相机上实现，支持无需云端或主机处理的隐私保护视觉应用。

Abstract: Real-time, on-device segmentation is critical for latency-sensitive and
privacy-aware applications like smart glasses and IoT devices. We introduce
PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation
model optimized for edge and in-sensor execution, including the Sony IMX500. It
builds on a depthwise separable U-Net, with knowledge distillation and
fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).
On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized
model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it
the only model meeting both memory and compute constraints for in-sensor
deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.
These results demonstrate that efficient, promptable segmentation is feasible
directly on-camera, enabling privacy-preserving vision without cloud or host
processing.

</details>


### [160] [4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation](https://arxiv.org/abs/2506.18839)
*Chaoyang Wang,Ashkan Mirzaei,Vidit Goel,Willi Menapace,Aliaksandr Siarohin,Avalon Vinella,Michael Vasilkovsky,Ivan Skorokhodov,Vladislav Shakhrai,Sergey Korolev,Sergey Tulyakov,Peter Wonka*

Main category: cs.CV

TL;DR: 提出首个能够通过前馈架构计算4D时空网格和3D高斯粒子的框架，包含4D视频模型和4D重建模型两部分，通过融合注意力机制和改进3D重建算法，实现了4D生成的新最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有4D视频扩散架构在时空注意力处理上存在局限性，且3D重建算法需要进一步优化以提升生成质量和重建能力。

Method: 提出融合时空注意力的单层架构和稀疏注意力模式，并扩展3D重建算法，引入高斯头、相机令牌替换算法和动态层。

Result: 在4D生成任务中实现了视觉质量和重建能力的显著提升，达到新的最佳性能。

Conclusion: 通过融合注意力机制和改进重建算法，该框架为4D生成任务提供了高效且高质量的解决方案。

Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid
of video frames and 3D Gaussian particles for each time step using a
feed-forward architecture. Our architecture has two main components, a 4D video
model and a 4D reconstruction model. In the first part, we analyze current 4D
video diffusion architectures that perform spatial and temporal attention
either sequentially or in parallel within a two-stream design. We highlight the
limitations of existing approaches and introduce a novel fused architecture
that performs spatial and temporal attention within a single layer. The key to
our method is a sparse attention pattern, where tokens attend to others in the
same frame, at the same timestamp, or from the same viewpoint. In the second
part, we extend existing 3D reconstruction algorithms by introducing a Gaussian
head, a camera token replacement algorithm, and additional dynamic layers and
training. Overall, we establish a new state of the art for 4D generation,
improving both visual quality and reconstruction capability.

</details>


### [161] [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](https://arxiv.org/abs/2506.18851)
*Zhuowei Chen,Bingchuan Li,Tianxiang Ma,Lijie Liu,Mingcong Liu,Yi Zhang,Gen Li,Xinghui Li,Siyu Zhou,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: Phantom-Data是一个新的跨对主题到视频一致性数据集，解决了现有模型在遵循文本指令时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在遵循文本指令时存在局限性，即所谓的“复制粘贴问题”，这是由于训练时使用同一场景的参考图像导致的。

Method: 通过三阶段流程构建Phantom-Data数据集：主题检测、跨上下文主题检索和身份验证。

Result: 实验表明，使用Phantom-Data训练显著提高了提示对齐和视觉质量，同时保持了身份一致性。

Conclusion: Phantom-Data为解决主题到视频生成中的一致性挑战提供了有效解决方案。

Abstract: Subject-to-video generation has witnessed substantial progress in recent
years. However, existing models still face significant challenges in faithfully
following textual instructions. This limitation, commonly known as the
copy-paste problem, arises from the widely used in-pair training paradigm. This
approach inherently entangles subject identity with background and contextual
attributes by sampling reference images from the same scene as the target
video. To address this issue, we introduce \textbf{Phantom-Data, the first
general-purpose cross-pair subject-to-video consistency dataset}, containing
approximately one million identity-consistent pairs across diverse categories.
Our dataset is constructed via a three-stage pipeline: (1) a general and
input-aligned subject detection module, (2) large-scale cross-context subject
retrieval from more than 53 million videos and 3 billion images, and (3)
prior-guided identity verification to ensure visual consistency under
contextual variation. Comprehensive experiments show that training with
Phantom-Data significantly improves prompt alignment and visual quality while
preserving identity consistency on par with in-pair baselines.

</details>


### [162] [RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base](https://arxiv.org/abs/2506.18856)
*Kuanning Wang,Yuqian Fu,Tianyu Wang,Yanwei Fu,Longfei Liang,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.CV

TL;DR: RAG-6DPose是一种基于检索增强的6D姿态估计方法，通过结合视觉和几何线索，利用3D CAD模型作为知识库，显著提升了姿态估计的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 准确的6D姿态估计对于机器人操作（如抓取）至关重要，但现有方法在处理遮挡和新视角时表现不佳。

Method: RAG-6DPose分为三个阶段：1) 构建多模态CAD知识库；2) 通过ReSPC模块从查询图像中检索相关CAD特征；3) 通过检索增强解码优化姿态预测。

Result: 实验结果表明，该方法在标准基准和实际机器人任务中表现优异，尤其在处理遮挡和新视角时。

Conclusion: RAG-6DPose通过检索增强策略有效提升了6D姿态估计的性能，适用于复杂场景。

Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise
object localization for tasks like grasping. We present RAG-6DPose, a
retrieval-augmented approach that leverages 3D CAD models as a knowledge base
by integrating both visual and geometric cues. Our RAG-6DPose roughly contains
three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D
visual features from multi-view CAD rendered images and also attaching 3D
points; 2) Retrieving relevant CAD features from the knowledge base based on
the current query image via our ReSPC module; and 3) Incorporating retrieved
CAD information to refine pose predictions via retrieval-augmented decoding.
Experimental results on standard benchmarks and real-world robotic tasks
demonstrate the effectiveness and robustness of our approach, particularly in
handling occlusions and novel viewpoints. Supplementary material is available
on our project website: https://sressers.github.io/RAG-6DPose .

</details>


### [163] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Main category: cs.CV

TL;DR: TAMMs模型通过轻量级时间模块和语义融合控制注入机制，提升了多模态大语言模型在卫星图像时间序列分析中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在卫星图像时间序列分析中缺乏精细的时空推理能力，需要一种新方法来联合理解时间变化和生成未来场景。

Method: 提出TAMMs模型，结合轻量级时间模块和SFCI机制，通过双路径条件增强ControlNet，实现时间一致和语义基础的图像合成。

Result: 实验表明TAMMs在时间变化理解和未来图像预测任务中优于基线MLLMs。

Conclusion: 精心设计的时间推理和语义融合可以释放MLLMs在时空理解中的潜力。

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [164] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar是一种创新的音频驱动全身视频生成模型，通过多层次的音频嵌入策略和LoRA训练方法，提升了唇同步和自然动作的准确性，同时支持精确的文本控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注面部动作，难以生成自然同步和流畅的全身动画，且缺乏对细粒度生成的控制。

Method: 采用像素级多层次的音频嵌入策略和LoRA训练方法，以更好地捕捉音频特征并支持文本控制。

Result: 实验表明，OmniAvatar在面部和半身视频生成中优于现有模型，支持多种场景的精确控制。

Conclusion: OmniAvatar通过创新的音频嵌入和训练方法，显著提升了全身动画的自然性和控制能力。

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [165] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2是一个多功能开源生成模型，支持文本到图像、图像编辑和上下文生成任务，通过分离的解码路径和反射机制实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 提供一个统一的生成模型解决方案，支持多种生成任务，同时保留原有文本生成能力。

Method: 采用分离的文本和图像解码路径，使用未共享参数和解耦的图像标记器，并引入反射机制。

Result: 在多个任务基准测试中表现优异，尤其在上下文生成任务中达到开源模型的最先进水平。

Conclusion: OmniGen2通过创新的设计和数据管道，实现了高效且多功能的生成能力，为未来研究提供了支持。

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [166] [Let Your Video Listen to Your Music!](https://arxiv.org/abs/2506.18881)
*Xinyu Zhang,Dong Gong,Zicheng Duan,Anton van den Hengel,Lingqiao Liu*

Main category: cs.CV

TL;DR: MVAA框架自动将视频与音乐节奏对齐，保留原始内容，通过关键帧对齐和节奏感知修复实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 视频与音乐节奏对齐在多媒体制作中有实际需求，现有方法依赖手动或启发式技术，缺乏灵活性。

Method: MVAA采用两步法：关键帧对齐音频节拍，节奏感知视频修复；使用扩散模型生成中间帧。

Result: 实验表明MVAA能高质量对齐节拍并保持视觉流畅性，适应时间短。

Conclusion: MVAA提供了一种高效、灵活的视频与音乐自动对齐解决方案。

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.

</details>


### [167] [Light of Normals: Unified Feature Representation for Universal Photometric Stereo](https://arxiv.org/abs/2506.18882)
*Hong Li,Houyuan Chen,Chongjie Ye,Zhaoxi Chen,Bohan Li,Shaocong Xu,Xianda Guo,Xuhui Liu,Yikai Wang,Baochang Zhang,Satoshi Ikehata,Boxin Shi,Anyi Rao,Hao Zhao*

Main category: cs.CV

TL;DR: 论文提出了一种通用光度立体方法，旨在解决光照变化与表面法线特征耦合及高频几何细节保留的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理任意光照条件下的表面法线恢复时，面临光照与法线特征耦合的模糊性以及高频几何细节丢失的问题。

Method: 未明确提及具体方法，但指出现有方法（如SDM-UniPS和Uni MS-PS）仍存在挑战。

Result: 未明确提及具体结果，但强调了当前方法的局限性。

Conclusion: 需要进一步研究以解决光照与法线特征的耦合问题，并提升高频几何细节的保留能力。

Abstract: Universal photometric stereo (PS) aims to recover high-quality surface
normals from objects under arbitrary lighting conditions without relying on
specific illumination models. Despite recent advances such as SDM-UniPS and Uni
MS-PS, two fundamental challenges persist: 1) the deep coupling between varying
illumination and surface normal features, where ambiguity in observed intensity
makes it difficult to determine whether brightness variations stem from
lighting changes or surface orientation; and 2) the preservation of
high-frequency geometric details in complex surfaces, where intricate
geometries create self-shadowing, inter-reflections, and subtle normal
variations that conventional feature processing operations struggle to capture
accurately.

</details>


### [168] [Universal Video Temporal Grounding with Generative Multi-modal Large Language Models](https://arxiv.org/abs/2506.18883)
*Zeqian Li,Shangzhe Di,Zhonghua Zhai,Weilin Huang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: UniTime是一个通用的视频时间定位模型，利用生成式多模态大语言模型（MLLMs）的强大视觉语言理解能力，能够处理多样化的视频内容和复杂语言查询。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常局限于特定视频领域或时长，无法满足多样化视频和复杂查询的需求。

Method: 通过将时间戳标记与视频标记交错结合，并采用自适应帧缩放技术，UniTime能够精确输出时间戳并处理不同长度的视频。

Result: 在五个公开的时间定位基准测试中，UniTime在零样本和数据集微调设置下均优于现有方法，并在长视频问答任务中显著提升了准确性。

Conclusion: UniTime展示了其在复杂视频理解任务中的强大潜力，为通用视频时间定位提供了有效解决方案。

Abstract: This paper presents a computational model for universal video temporal
grounding, which accurately localizes temporal moments in videos based on
natural language queries (e.g., questions or descriptions). Unlike existing
methods that are often limited to specific video domains or durations, we
propose UniTime, a robust and universal video grounding model leveraging the
strong vision-language understanding capabilities of generative Multi-modal
Large Language Models (MLLMs). Our model effectively handles videos of diverse
views, genres, and lengths while comprehending complex language queries. The
key contributions include: (i) We consider steering strong MLLMs for temporal
grounding in videos. To enable precise timestamp outputs, we incorporate
temporal information by interleaving timestamp tokens with video tokens. (ii)
By training the model to handle videos with different input granularities
through adaptive frame scaling, our approach achieves robust temporal grounding
for both short and long videos. (iii) Comprehensive experiments show that
UniTime outperforms state-of-the-art approaches in both zero-shot and
dataset-specific finetuned settings across five public temporal grounding
benchmarks. (iv) When employed as a preliminary moment retriever for long-form
video question-answering (VideoQA), UniTime significantly improves VideoQA
accuracy, highlighting its value for complex video understanding tasks.

</details>


### [169] [4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time](https://arxiv.org/abs/2506.18890)
*Ziqiao Ma,Xuweiyi Chen,Shoubin Yu,Sai Bi,Kai Zhang,Chen Ziwen,Sihan Xu,Jianing Yang,Zexiang Xu,Kalyan Sunkavalli,Mohit Bansal,Joyce Chai,Hao Tan*

Main category: cs.CV

TL;DR: 4D-LRM是一种大规模4D重建模型，能够从任意视角和时间点渲染物体，解决了传统方法在效率、泛化和保真度上的问题。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过4D预训练学习通用的时空表示，以从少量视角和时间点重建物体。

Method: 4D-LRM通过学习统一的时空表示，直接从时间序列的位姿图像标记预测每像素的4D高斯基元，实现快速高质量渲染。

Result: 模型在单次前向传播中重建24帧序列，耗时不到1.5秒，泛化能力强，支持新物体和时间插值。

Conclusion: 4D-LRM证明了时空预训练的扩展性，能够高效准确地完成4D重建任务。

Abstract: Can we scale 4D pretraining to learn general space-time representations that
reconstruct an object from a few views at some times to any view at any time?
We provide an affirmative answer with 4D-LRM, the first large-scale 4D
reconstruction model that takes input from unconstrained views and timestamps
and renders arbitrary novel view-time combinations. Unlike prior 4D approaches,
e.g., optimization-based, geometry-based, or generative, that struggle with
efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time
representation and directly predicts per-pixel 4D Gaussian primitives from
posed image tokens across time, enabling fast, high-quality rendering at, in
principle, infinite frame rate. Our results demonstrate that scaling
spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We
show that 4D-LRM generalizes to novel objects, interpolates across time, and
handles diverse camera setups. It reconstructs 24-frame sequences in one
forward pass with less than 1.5 seconds on a single A100 GPU.

</details>


### [170] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态框架，通过共享的离散语义表示统一视觉理解和生成，核心是文本对齐的Tokenizer（TA-Tok），将图像转换为离散标记。


<details>
  <summary>Details</summary>
Motivation: 旨在通过统一的表示空间实现跨模态输入和输出，避免模态特定设计，提升视觉理解和生成的效率与效果。

Method: 采用文本对齐的Tokenizer和扩展词汇表，结合尺度自适应编码解码和生成式去标记器，使用两种互补的去标记器（自回归和扩散模型）。

Result: 实验表明，Tar在多模态LLM方法中表现优异，收敛更快且训练效率更高。

Conclusion: 该框架成功统一了视觉与文本模态，为多模态任务提供了高效解决方案。

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


### [171] [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](https://arxiv.org/abs/2506.18899)
*Kaiyi Huang,Yukun Huang,Xintao Wang,Zinan Lin,Xuefei Ning,Pengfei Wan,Di Zhang,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: FilMaster是一个端到端的AI系统，通过整合真实世界的电影原则生成专业级电影，解决了现有系统在多样镜头语言和电影节奏上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有电影生成系统难以实现电影原则，导致视觉效果模板化和叙事不吸引人。

Method: FilMaster采用两阶段设计：参考引导生成阶段和生成后制作阶段，结合多镜头协同RAG模块和观众中心电影节奏控制模块。

Result: FilMaster在镜头语言设计和电影节奏控制上表现优异，推动了生成式AI在专业电影制作中的应用。

Conclusion: FilMaster通过整合电影原则和生成式AI模型，显著提升了AI生成电影的专业性和吸引力。

Abstract: AI-driven content creation has shown potential in film production. However,
existing film generation systems struggle to implement cinematic principles and
thus fail to generate professional-quality films, particularly lacking diverse
camera language and cinematic rhythm. This results in templated visuals and
unengaging narratives. To address this, we introduce FilMaster, an end-to-end
AI system that integrates real-world cinematic principles for
professional-grade film generation, yielding editable, industry-standard
outputs. FilMaster is built on two key principles: (1) learning cinematography
from extensive real-world film data and (2) emulating professional,
audience-centric post-production workflows. Inspired by these principles,
FilMaster incorporates two stages: a Reference-Guided Generation Stage which
transforms user input to video clips, and a Generative Post-Production Stage
which transforms raw footage into audiovisual outputs by orchestrating visual
and auditory elements for cinematic rhythm. Our generation stage highlights a
Multi-shot Synergized RAG Camera Language Design module to guide the AI in
generating professional camera language by retrieving reference clips from a
vast corpus of 440,000 film clips. Our post-production stage emulates
professional workflows by designing an Audience-Centric Cinematic Rhythm
Control module, including Rough Cut and Fine Cut processes informed by
simulated audience feedback, for effective integration of audiovisual elements
to achieve engaging content. The system is empowered by generative AI models
like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a
comprehensive benchmark for evaluating AI-generated films. Extensive
experiments show FilMaster's superior performance in camera language design and
cinematic rhythm control, advancing generative AI in professional filmmaking.

</details>


### [172] [Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18900)
*Kiymet Akdemir,Tahira Kazimi,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出了一种多智能体协作框架，用于在多面板故事可视化中自动识别、纠正和优化不一致性，提高了视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在保持角色和对象的关键属性方面存在不足，导致叙事不连贯。

Method: 采用协作多智能体框架，通过迭代循环实现细粒度的面板级更新，无需重新生成整个序列。

Result: 定量和定性实验表明，该方法在多面板一致性方面优于现有方法。

Conclusion: 该框架具有模型无关性，可灵活集成多种扩散模型，显著提升了故事可视化的连贯性。

Abstract: Story visualization has become a popular task where visual scenes are
generated to depict a narrative across multiple panels. A central challenge in
this setting is maintaining visual consistency, particularly in how characters
and objects persist and evolve throughout the story. Despite recent advances in
diffusion models, current approaches often fail to preserve key character
attributes, leading to incoherent narratives. In this work, we propose a
collaborative multi-agent framework that autonomously identifies, corrects, and
refines inconsistencies across multi-panel story visualizations. The agents
operate in an iterative loop, enabling fine-grained, panel-level updates
without re-generating entire sequences. Our framework is model-agnostic and
flexibly integrates with a variety of diffusion models, including rectified
flow transformers such as Flux and latent diffusion models such as Stable
Diffusion. Quantitative and qualitative experiments show that our method
outperforms prior approaches in terms of multi-panel consistency.

</details>


### [173] [From Virtual Games to Real-World Play](https://arxiv.org/abs/2506.18901)
*Wenqiang Sun,Fangyun Wei,Jinjing Zhao,Xi Chen,Zilong Chen,Hongyang Zhang,Jun Zhang,Yan Lu*

Main category: cs.CV

TL;DR: RealPlay是一个基于神经网络的实时游戏引擎，能够从用户控制信号生成交互式视频，目标是生成逼真且时间一致的视频序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注游戏风格的视觉效果，而RealPlay旨在生成类似真实世界场景的逼真视频。

Method: 通过迭代的块状预测实现低延迟反馈，保持时间一致性，并准确响应用户控制。训练数据结合了标记的游戏数据和未标记的真实世界视频。

Result: RealPlay能够将虚拟控制信号映射到真实场景，并泛化到控制多种真实世界实体（如自行车和行人）。

Conclusion: RealPlay展示了在无需真实世界动作标注的情况下，实现逼真视频生成和控制的潜力。

Abstract: We introduce RealPlay, a neural network-based real-world game engine that
enables interactive video generation from user control signals. Unlike prior
works focused on game-style visuals, RealPlay aims to produce photorealistic,
temporally consistent video sequences that resemble real-world footage. It
operates in an interactive loop: users observe a generated scene, issue a
control command, and receive a short video chunk in response. To enable such
realistic and responsive generation, we address key challenges including
iterative chunk-wise prediction for low-latency feedback, temporal consistency
across iterations, and accurate control response. RealPlay is trained on a
combination of labeled game data and unlabeled real-world videos, without
requiring real-world action annotations. Notably, we observe two forms of
generalization: (1) control transfer-RealPlay effectively maps control signals
from virtual to real-world scenarios; and (2) entity transfer-although training
labels originate solely from a car racing game, RealPlay generalizes to control
diverse real-world entities, including bicycles and pedestrians, beyond
vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/

</details>


### [174] [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://arxiv.org/abs/2506.18903)
*Runjia Li,Philip Torr,Andrea Vedaldi,Tomas Jakab*

Main category: cs.CV

TL;DR: 提出了一种新的内存机制（VMem），通过几何索引过去视图来提升视频生成器的长期场景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如2D视图重建或短上下文窗口）在长期场景一致性和误差累积方面存在不足。

Method: 引入Surfel-Indexed View Memory（VMem），基于3D表面元素（surfels）几何索引过去视图，高效检索相关视图。

Result: 在长期场景合成任务中表现优异，显著提升场景一致性和相机控制能力。

Conclusion: VMem是一种高效且计算成本低的方法，适用于长期场景探索。

Abstract: We propose a novel memory mechanism to build video generators that can
explore environments interactively. Similar results have previously been
achieved by out-painting 2D views of the scene while incrementally
reconstructing its 3D geometry, which quickly accumulates errors, or by video
generators with a short context window, which struggle to maintain scene
coherence over the long term. To address these limitations, we introduce
Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by
indexing them geometrically based on the 3D surface elements (surfels) they
have observed. VMem enables the efficient retrieval of the most relevant past
views when generating new ones. By focusing only on these relevant views, our
method produces consistent explorations of imagined environments at a fraction
of the computational cost of using all past views as context. We evaluate our
approach on challenging long-term scene synthesis benchmarks and demonstrate
superior performance compared to existing methods in maintaining scene
coherence and camera control.

</details>


### [175] [TC-Light: Temporally Consistent Relighting for Dynamic Long Videos](https://arxiv.org/abs/2506.18904)
*Yang Liu,Chuanchen Luo,Zimo Tang,Yingyan Li,Yuran Yang,Yuanyong Ning,Lue Fan,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: TC-Light是一种新颖的视频重光照方法，通过两阶段后优化机制实现高时间一致性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照技术主要局限于肖像视频或面临时间一致性和计算效率的瓶颈，TC-Light旨在解决这些问题。

Method: 采用两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照，第二阶段优化唯一视频张量（UVT）以对齐细粒度纹理和光照。

Result: 实验表明，TC-Light能够实现物理上合理且时间一致的重光照结果，同时计算成本低。

Conclusion: TC-Light为复杂动态长视频的重光照提供了一种高效且一致的解决方案。

Abstract: Editing illumination in long videos with complex dynamics has significant
value in various downstream tasks, including visual content creation and
manipulation, as well as data scaling up for embodied AI through sim2real and
real2real transfer. Nevertheless, existing video relighting techniques are
predominantly limited to portrait videos or fall into the bottleneck of
temporal consistency and computation efficiency. In this paper, we propose
TC-Light, a novel paradigm characterized by the proposed two-stage post
optimization mechanism. Starting from the video preliminarily relighted by an
inflated video relighting model, it optimizes appearance embedding in the first
stage to align global illumination. Then it optimizes the proposed canonical
video representation, i.e., Unique Video Tensor (UVT), to align fine-grained
texture and lighting in the second stage. To comprehensively evaluate
performance, we also establish a long and highly dynamic video benchmark.
Extensive experiments show that our method enables physically plausible
relighting results with superior temporal coherence and low computation cost.
The code and video demos are available at
https://dekuliutesla.github.io/tclight/.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [176] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: 提出了一种新的数据集和算法，用于从地球上任何地方快速高效地计算海岸距离。


<details>
  <summary>Details</summary>
Motivation: 现有全球海岸数据集分辨率较低（1-4公里），限制了其应用潜力。

Method: 结合公开卫星影像和计算机视觉技术，提供10米分辨率的全球海岸数据集，并开发了高效计算库Lighthouse。

Result: 数据集分辨率提升100倍以上，Lighthouse仅需1 CPU和2GB RAM即可实现毫秒级在线推理。

Conclusion: 该方法适用于资源受限环境中的实时应用。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [177] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为PCaM的渐进式聚焦交叉注意力机制，用于解决无监督域适应中的前景对象不匹配问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Vision Transformers的无监督域适应方法存在前景对象大小和空间分布不一致的问题，影响了注意力一致性和域对齐效果。

Method: 提出PCaM机制，逐步过滤背景信息，聚焦并融合跨域的前景语义；引入注意力引导损失，增强跨域注意力一致性。

Result: 在多个数据集上验证了PCaM的有效性，显著提升了适应性能并达到新的最优结果。

Conclusion: PCaM通过注意力引导的前景融合，有效解决了域适应中的关键问题，具有轻量级、架构无关和易集成等优势。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [178] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: 论文提出了一种新的少样本测试时域适应方法，通过直接学习输入空间补充CLIP的冻结特征，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖CLIP的预训练特征空间，但其性能在较弱骨干网络（如ViT-B/16）上显著下降，需要补充数据集特定知识。

Method: 引入独立分支与CLIP并行，通过反向注意力学习专属知识；通过贪婪文本集成和细化增强文本特征；生成域提示逐步融合文本和视觉特征。

Result: 在5个大规模基准测试（WILDS和DomainNet）上表现优越，ViT-B/16在iWildCam和FMoW上分别提升5.1 F1和3.1% WC Acc。

Conclusion: 直接学习输入空间并补充CLIP的冻结特征，能有效提升少样本测试时域适应的性能。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [179] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: 扩散模型在图像生成中表现出色，但其创造性的来源尚不明确。本文通过理论扩展和实验验证，探讨了自注意力机制在扩散模型中的作用。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型中创造性生成的机制，特别是自注意力如何影响图像生成的全局一致性。

Method: 扩展了扩散模型的理论框架，将自注意力层纳入CNN架构，并通过实验验证其效果。

Result: 理论表明自注意力能促进局部特征的全局一致性，实验数据支持了这一结论。

Conclusion: 自注意力机制在扩散模型中增强了图像生成的全局一致性，为理解其创造性提供了新视角。

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [180] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: 提出了一种可解释的不完整多视图手术评估模型，结合多视图数据和TSK模糊系统，显著提升了直肠癌手术难度的评估效果。


<details>
  <summary>Details</summary>
Motivation: 现有直肠癌手术难度评估方法主要依赖临床数据，但技术发展使得更多数据可被收集，人工智能的应用成为可能。

Method: 构建多视图直肠癌数据集，提出双表示不完整多视图学习模型，结合缺失视图填补和二阶相似性约束，并基于TSK模糊系统设计多视图手术评估模型。

Result: 在MVRC数据集上，DRIMV_TSK模型优于其他先进算法，取得了最佳结果。

Conclusion: 该模型通过多视图数据和协同学习机制，显著提升了直肠癌手术难度评估的准确性和可解释性。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [181] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+是一个联邦学习框架，结合了神经加法模型和共形预测方法，提供可解释性和可靠的预测不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架缺乏结合不确定性量化、可解释性和鲁棒性的综合解决方案。

Method: FedNAM+通过动态级别调整技术和梯度敏感图识别关键输入特征，提供像素级不确定性估计。

Result: 在CT扫描、MNIST和CIFAR数据集上验证，预测精度高且损失小（如MNIST仅0.1%），并提供透明的不确定性度量。

Conclusion: FedNAM+是一个鲁棒、可解释且计算高效的框架，提升了分布式预测建模的信任和透明度。

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [182] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: 论文提出了一种名为UNIVERSE的评估协议，用于评估世界模型的动态生成内容，通过适应视觉语言模型（VLMs）在数据与计算限制下进行细粒度、时间敏感的评估。


<details>
  <summary>Details</summary>
Motivation: 现有指标无法捕捉世界模型生成内容中动作对齐和语义一致性的细粒度评估需求，而视觉语言模型在多模态推理方面表现出潜力，但需要针对性改进以适应此类任务。

Method: 引入UNIVERSE方法，通过全调、部分调和参数高效调优等方式，在多种任务格式、上下文长度、采样策略和数据组合下评估VLMs的适应性。

Result: UNIVERSE在单一检查点下匹配任务特定基线的性能，并通过人类研究验证其与人类判断的高度一致性。

Conclusion: UNIVERSE是一种可扩展、语义感知的世界模型评估工具，适用于动态生成内容的细粒度评估。

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [183] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种名为h-calibration的概率学习框架，用于解决深度神经网络输出概率的校准问题，克服了现有方法的十大局限性，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在许多任务中表现出色，但其输出的概率往往不准确（miscalibration），导致不可靠。现有方法存在局限性，需要一种更有效的校准方法。

Method: 论文总结了现有方法为三类：直观设计方法、基于分箱的方法和基于理想校准公式的方法。提出h-calibration框架，设计了一种简单有效的后校准算法。

Result: h-calibration不仅克服了现有方法的十大局限性，还在实验中显著优于传统方法，达到了最先进的性能。

Conclusion: h-calibration框架为学习可靠概率提供了理论支持，并在实验中验证了其有效性，为相关领域提供了有价值的参考。

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [184] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: 本文探讨了在医学分类任务中，共形预测（conformal predictions）的局限性，特别是在输入和标签变量分布变化时不可靠。


<details>
  <summary>Details</summary>
Motivation: 共形预测因其可提供可证明的校准保证而受到关注，但在医学等安全关键领域的应用存在潜在问题和限制。

Method: 通过皮肤病学和组织病理学的实例分析，验证共形预测在分布变化下的不可靠性。

Result: 共形预测在输入和标签变量分布变化时不可靠，且不适用于提高准确性的预测选择或数据子集（如单个类别或患者属性）。

Conclusion: 在医学图像分类任务中，共形预测的实用价值有限，尤其是在类别较少的情况下。

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [185] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Main category: cs.LG

TL;DR: 提出了一种无需重新训练的低成本方法，通过计算多数和少数群体激活均值的差异定义“偏置向量”，并在推理时减去该向量以减少分类偏置。


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器在数据分布不均时容易继承类别偏置，导致在非典型群体上表现不佳。现有方法通常需要重新训练或大量计算资源。

Method: 通过计算多数和少数群体激活均值的差异定义“偏置向量”，并在推理时从模型的残差流中减去该向量。

Result: 该方法减少了分类偏置并提高了最差群体准确率，且无需重新训练或额外计算资源。

Conclusion: 展示了在分类模型中无需训练即可低成本缓解偏置的有效方法，扩展了传统用于生成模型的“转向向量”的应用范围。

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [186] [A workflow for generating synthetic LiDAR datasets in simulation environments](https://arxiv.org/abs/2506.17378)
*Abhishek Phadke,Shakib Mahmud Dipto,Pratip Rana*

Main category: cs.RO

TL;DR: 提出了一种生成合成LiDAR数据集的仿真工作流，用于支持自动驾驶感知、机器人研究和传感器安全分析。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶和机器人研究提供高保真合成数据，同时分析LiDAR数据的安全漏洞。

Method: 利用CoppeliaSim仿真环境和Python API，集成LiDAR、图像传感器和二维扫描仪，自动化数据捕获、存储和标注。

Result: 生成了大规模点云及同步RGB和深度图像，验证了数据集的实用性，并展示了安全漏洞分析。

Conclusion: 工作流为感知研究和传感器安全提供了可复现的高保真数据生成框架，未来可扩展环境真实性和传感器噪声建模。

Abstract: This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.

</details>


### [187] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: ARNA是一个基于大型视觉语言模型（LVLM）的通用导航框架，通过自主定义和执行任务特定工作流，实现在未知环境中的鲁棒导航和推理。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统依赖任务特定神经网络和固定数据流，限制了泛化能力。LVLM提供了人类类似的知识嵌入，适合推理和规划，但现有集成通常依赖预映射空间和硬编码表示。

Method: ARNA框架为LVLM代理提供感知、推理和导航工具库，运行时自主定义工作流，迭代查询机器人模块、推理多模态输入并选择导航动作。

Result: 在Habitat Lab的HM-EQA基准测试中，ARNA实现了最先进的性能，无需手工计划、固定输入表示或预建地图。

Conclusion: ARNA为机器人堆栈设计提供了新视角，展示了在未知环境中有效探索、导航和具身问答的能力。

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [188] [EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization](https://arxiv.org/abs/2506.17516)
*Zhou Chen,Sanjoy Kundu,Harsimran S. Baweja,Sathyanarayanan N. Aakur*

Main category: cs.RO

TL;DR: EASE是一个自监督框架，通过自由能最小化统一时空表示学习和具身控制，无需标注或外部奖励即可实现动态事件感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作空间、标注数据集和外部奖励，限制了在动态现实场景中的适应性和可扩展性。

Method: EASE利用预测误差和熵作为内在信号，结合生成感知模型和动作驱动控制策略，动态对齐预测与观察。

Result: 在仿真和现实环境中，EASE实现了隐私保护和可扩展的事件感知，支持隐式记忆和目标连续性等行为。

Conclusion: EASE为无脚本动态任务中的具身系统提供了鲁棒的基础。

Abstract: Active event perception, the ability to dynamically detect, track, and
summarize events in real time, is essential for embodied intelligence in tasks
such as human-AI collaboration, assistive robotics, and autonomous navigation.
However, existing approaches often depend on predefined action spaces,
annotated datasets, and extrinsic rewards, limiting their adaptability and
scalability in dynamic, real-world scenarios. Inspired by cognitive theories of
event perception and predictive coding, we propose EASE, a self-supervised
framework that unifies spatiotemporal representation learning and embodied
control through free energy minimization. EASE leverages prediction errors and
entropy as intrinsic signals to segment events, summarize observations, and
actively track salient actors, operating without explicit annotations or
external rewards. By coupling a generative perception model with an
action-driven control policy, EASE dynamically aligns predictions with
observations, enabling emergent behaviors such as implicit memory, target
continuity, and adaptability to novel environments. Extensive evaluations in
simulation and real-world settings demonstrate EASE's ability to achieve
privacy-preserving and scalable event perception, providing a robust foundation
for embodied systems in unscripted, dynamic tasks.

</details>


### [189] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: RoboTwin 2.0是一个可扩展的仿真框架，用于生成多样化和真实的双机械臂操作数据，通过结合多模态大语言模型和仿真优化，显著提升了代码生成成功率和现实场景泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有合成数据集在双机械臂操作中因缺乏高效、可扩展的数据生成方法和过于简化的仿真环境而表现不足。

Method: 构建大规模对象库（RoboTwin-OD），结合多模态大语言模型和仿真优化生成任务级执行代码，并通过结构化域随机化增强数据多样性。

Result: 在50个双机械臂任务中生成10万条专家轨迹，代码生成成功率提升10.9%，现实场景任务性能显著提升（最高367%）。

Conclusion: RoboTwin 2.0为双机械臂操作提供了可扩展的数据生成和评估框架，支持无真实世界监督的强泛化能力。

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [190] [Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation](https://arxiv.org/abs/2506.18443)
*Yang Lyu,Zhenghao Zou,Yanfeng Li,Chunhui Zhao,Quan Pan*

Main category: cs.RO

TL;DR: 提出了一种基于事件相机和毫米波雷达的无IMU和无特征关联的机器人运动速度估计框架，适用于高动态场景。


<details>
  <summary>Details</summary>
Motivation: 高动态机器人运动导致传感器测量模糊、失真和延迟，传统方法难以应对。

Method: 结合事件相机和毫米波雷达，直接利用原始事件和多普勒测量推导速度，后端采用连续时间状态空间模型进行融合。

Result: 在挑战性环境中实现了可靠且高效的速度输出。

Conclusion: 该框架在无纹理和无结构环境中表现鲁棒，计算效率高，适合边缘设备。

Abstract: Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.

</details>


### [191] [TDACloud: Point Cloud Recognition Using Topological Data Analysis](https://arxiv.org/abs/2506.18725)
*Anirban Ghosh,Ian Dahlin,Ayan Dutta*

Main category: cs.RO

TL;DR: 提出了一种名为TDACloud的新方法，利用拓扑数据分析（TDA）从点云中提取局部描述符，无需GPU密集型训练，并在噪声和变换条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 点云识别在自动驾驶等应用中具有重要意义，但提取可匹配的局部描述符具有挑战性，尤其是在噪声或变换条件下。

Method: 采用ATOL向量化方法生成点云的TDA描述符向量，直接处理原始点云。

Result: 在真实和模拟数据集上测试，TDACloud在噪声和变换条件下识别准确率高，优于基线方法约14%。

Conclusion: TDACloud是一种高效的点云识别方法，适用于复杂环境。

Abstract: Point cloud-based object/place recognition remains a problem of interest in
applications such as autonomous driving, scene reconstruction, and
localization. Extracting meaningful local descriptors from a query point cloud
that can be matched with the descriptors of the collected point clouds is a
challenging problem. Furthermore, when the query point cloud is noisy or has
been transformed (e.g., rotated), it adds to the complexity. To this end, we
propose a novel methodology, named TDACloud, using Topological Data Analysis
(TDA) for local descriptor extraction from a point cloud, which does not need
resource-intensive GPU-based machine learning training. More specifically, we
used the ATOL vectorization method to generate vectors for point clouds. Unlike
voxelization, our proposed technique can take raw point clouds as inputs and
outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed
TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford
RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for
object and place recognition. We have also tested TDACloud on noisy and
transformed test cases where the query point cloud has been scaled, translated,
or rotated. Our results demonstrate high recognition accuracies in noisy
conditions and large-scale real-world place recognition while outperforming the
baselines by up to approximately 14%.

</details>


### [192] [Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned](https://arxiv.org/abs/2506.18844)
*Olivier Gamache,Jean-Michel Fortin,Matěj Boxan,François Pomerleau,Philippe Giguère*

Main category: cs.RO

TL;DR: 提出了一种利用模拟器生成不同曝光时间图像的方法，解决了传统自动曝光方法难以复现的问题，并通过BorealHDR数据集验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 标准数据集因输入传感器固定而难以比较主动调整传感器参数的方法，尤其是自动曝光方法。

Method: 利用BorealHDR多曝光立体数据集及其扩展，通过模拟器生成不同曝光时间的图像，离线评估自动曝光方法。

Result: 模拟图像与真实图像的RMSE低于1.78%，验证了方法的准确性；传统自动曝光方法表现最佳。

Conclusion: 提出的离线方法解决了自动曝光方法难以复现的问题，BorealHDR数据集和模拟器为未来研究提供了支持。

Abstract: Standard datasets often present limitations, particularly due to the fixed
nature of input data sensors, which makes it difficult to compare methods that
actively adjust sensor parameters to suit environmental conditions. This is the
case with Automatic-Exposure (AE) methods, which rely on environmental factors
to influence the image acquisition process. As a result, AE methods have
traditionally been benchmarked in an online manner, rendering experiments
non-reproducible. Building on our prior work, we propose a methodology that
utilizes an emulator capable of generating images at any exposure time. This
approach leverages BorealHDR, a unique multi-exposure stereo dataset, along
with its new extension, in which data was acquired along a repeated trajectory
at different times of the day to assess the impact of changing illumination. In
total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting
conditions. The dataset also includes lidar-inertial-odometry-based maps with
pose estimation for each image frame, as well as Global Navigation Satellite
System (GNSS) data for comparison. We demonstrate that by using images acquired
at various exposure times, we can emulate realistic images with a
Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.
Using this offline approach, we benchmarked eight AE methods, concluding that
the classical AE method remains the field's best performer. To further support
reproducibility, we provide in-depth details on the development of our backpack
acquisition platform, including hardware, electrical components, and
performance specifications. Additionally, we share valuable lessons learned
from deploying the backpack over more than 25 km across various environments.
Our code and dataset are available online at this link:
https://github.com/norlab-ulaval/TFR24 BorealHDR

</details>


### [193] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: GRAND-SLAM是一种多智能体高斯溅射SLAM方法，适用于大规模户外环境，结合了局部优化和闭环检测，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高斯溅射SLAM方法局限于小规模室内环境，无法满足大规模户外多智能体场景的需求。

Method: GRAND-SLAM结合了基于局部子图优化的隐式跟踪模块和集成到位姿图优化框架中的多智能体闭环检测方法。

Result: 在Replica室内数据集上，GRAND-SLAM的跟踪性能优于现有方法，PSNR提高28%；在Kimera-Multi户外数据集上，多智能体跟踪误差降低91%，渲染效果更优。

Conclusion: GRAND-SLAM为大规模户外多智能体场景提供了一种高效的高斯溅射SLAM解决方案。

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [194] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Main category: cs.CY

TL;DR: MAARTA是一个多代理框架，通过分析视线模式和放射学报告提供个性化反馈，帮助学生改进诊断推理。


<details>
  <summary>Details</summary>
Motivation: 放射学学生因缺乏专家指导时间，常出现视觉搜索和诊断解释错误，现有AI系统未能有效解决这些错误。

Method: MAARTA通过动态选择代理分析错误复杂性，比较专家与学生视线行为，并使用逐步提示帮助学生理解错误。

Result: 系统能识别遗漏发现并分析差异，提供个性化反馈以改进诊断推理。

Conclusion: MAARTA推动了AI驱动的放射学教育，帮助学生更高效地学习。

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


### [195] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: 论文研究了多模态生物特征在检测智能手机使用导致的注意力分散中的应用，特别关注在线学习场景。


<details>
  <summary>Details</summary>
Motivation: 学习者在在线学习中面临内部、系统和上下文因素的干扰，传统平台缺乏详细行为数据，多模态学习分析和生物传感器提供了新视角。

Method: 提出了一种基于AI的方法，结合生理信号和头部姿态数据来检测手机使用。

Result: 单一生物信号（如脑波或心率）准确率有限，头部姿态单独达到87%，多模态模型结合所有信号达到91%准确率。

Conclusion: 讨论了在在线学习环境中实时部署这些模型的潜在影响和限制。

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


### [196] [Multimodal Political Bias Identification and Neutralization](https://arxiv.org/abs/2506.17372)
*Cedric Bernard,Xavier Pleimling,Amun Kharel,Chase Vickery*

Main category: cs.CY

TL;DR: 提出了一种结合文本和图像偏见的去偏见模型，通过四个步骤实现，初步结果显示出潜力，但需要更多资源和时间优化。


<details>
  <summary>Details</summary>
Motivation: 政治回音室现象使得去除政治文章中的主观偏见和情绪化语言变得重要，但现有研究仅关注文本部分，忽略了图像的偏见影响。

Method: 模型包括四个步骤：图像文本对齐（CLIP模型）、图像偏见评分（ViT分类器）、文本去偏见（BERT模型）以及最终的去偏见替换。

Result: 初步结果显示文本去偏见策略能有效识别偏见词汇，ViT模型训练效果良好，语义对齐模型高效，但需更多资源和时间优化。

Conclusion: 该方法在去偏见方面具有潜力，但需进一步优化和人类评估以确保语义一致性。

Abstract: Due to the presence of political echo chambers, it becomes imperative to
detect and remove subjective bias and emotionally charged language from both
the text and images of political articles. However, prior work has focused on
solely the text portion of the bias rather than both the text and image
portions. This is a problem because the images are just as powerful of a medium
to communicate information as text is. To that end, we present a model that
leverages both text and image bias which consists of four different steps.
Image Text Alignment focuses on semantically aligning images based on their
bias through CLIP models. Image Bias Scoring determines the appropriate bias
score of images via a ViT classifier. Text De-Biasing focuses on detecting
biased words and phrases and neutralizing them through BERT models. These three
steps all culminate to the final step of debiasing, which replaces the text and
the image with neutralized or reduced counterparts, which for images is done by
comparing the bias scores. The results so far indicate that this approach is
promising, with the text debiasing strategy being able to identify many
potential biased words and phrases, and the ViT model showcasing effective
training. The semantic alignment model also is efficient. However, more time,
particularly in training, and resources are needed to obtain better results. A
human evaluation portion was also proposed to ensure semantic consistency of
the newly generated text and images.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [197] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: 研究表明，经过轻量级微调的通用视觉语言模型（VLMs）在特定医学成像任务中可与专业医学VLMs媲美，甚至在某些情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 探讨通用VLMs是否通过微调能在医学成像任务中匹敌专业医学VLMs，以提供更高效且经济的解决方案。

Method: 系统评估通用和医学VLMs在疾病诊断和视觉问答任务中的表现，包括域内和域外任务，使用CLIP和LLaVA模型。

Result: 通用VLMs在轻量级微调后表现优异，尤其在域外任务中展现出强适应性，挑战了医学专用预训练的必要性。

Conclusion: 通用VLMs结合微调是一种可扩展且经济高效的替代方案，为医学成像领域的研究提供了重要启示。

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [198] [DSA-NRP: No-Reflow Prediction from Angiographic Perfusion Dynamics in Stroke EVT](https://arxiv.org/abs/2506.17501)
*Shreeram Athreya,Carlos Olivares,Ameera Ismail,Kambiz Nael,William Speier,Corey Arnold*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的框架，利用术中数字减影血管造影（DSA）序列和临床变量，预测急性缺血性卒中（AIS）患者血管内血栓切除术（EVT）后的无复流现象。


<details>
  <summary>Details</summary>
Motivation: EVT后部分患者会出现无复流现象，导致微血管低灌注，影响组织恢复和临床结果。目前依赖术后24小时内的灌注MRI检测，存在延迟干预的问题。

Method: 研究回顾性分析了UCLA医学中心的AIS患者数据，从DSA序列中提取统计和时间灌注特征，结合临床变量训练机器学习分类器。

Result: 新方法显著优于基于临床特征的基线模型（AUC: 0.7703 vs. 0.5728；准确率: 0.8125 vs. 0.6331），表明DSA灌注动态能实时反映微血管完整性。

Conclusion: 该方法为即时、准确预测无复流现象奠定了基础，使临床医生能够主动管理高风险患者，无需依赖延迟的影像检查。

Abstract: Following successful large-vessel recanalization via endovascular
thrombectomy (EVT) for acute ischemic stroke (AIS), some patients experience a
complication known as no-reflow, defined by persistent microvascular
hypoperfusion that undermines tissue recovery and worsens clinical outcomes.
Although prompt identification is crucial, standard clinical practice relies on
perfusion magnetic resonance imaging (MRI) within 24 hours post-procedure,
delaying intervention. In this work, we introduce the first-ever machine
learning (ML) framework to predict no-reflow immediately after EVT by
leveraging previously unexplored intra-procedural digital subtraction
angiography (DSA) sequences and clinical variables. Our retrospective analysis
included AIS patients treated at UCLA Medical Center (2011-2024) who achieved
favorable mTICI scores (2b-3) and underwent pre- and post-procedure MRI.
No-reflow was defined as persistent hypoperfusion (Tmax > 6 s) on
post-procedural imaging. From DSA sequences (AP and lateral views), we
extracted statistical and temporal perfusion features from the target
downstream territory to train ML classifiers for predicting no-reflow. Our
novel method significantly outperformed a clinical-features baseline(AUC:
0.7703 $\pm$ 0.12 vs. 0.5728 $\pm$ 0.12; accuracy: 0.8125 $\pm$ 0.10 vs. 0.6331
$\pm$ 0.09), demonstrating that real-time DSA perfusion dynamics encode
critical insights into microvascular integrity. This approach establishes a
foundation for immediate, accurate no-reflow prediction, enabling clinicians to
proactively manage high-risk patients without reliance on delayed imaging.

</details>


### [199] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: 提出了一种基于GAN的多波段红外图像彩色化框架，通过多阶段光谱自注意力Transformer网络（MTSIC）提升图像质量和语义准确性。


<details>
  <summary>Details</summary>
Motivation: TIR图像缺乏颜色和纹理信息，现有彩色化方法因单波段限制导致图像失真和语义模糊。多波段红外图像提供更丰富的光谱数据，有助于保留细节和提升语义准确性。

Method: 使用GAN框架，生成器为MTSIC网络，结合空间-光谱注意力残差块（SARB）和多尺度小波块（MSWB），逐步优化重建质量。

Result: 实验结果表明，该方法显著优于传统技术，有效提升了红外图像的视觉质量。

Conclusion: 提出的MTSIC框架通过多波段特征映射和语义信息对齐，成功解决了红外图像彩色化中的失真和语义模糊问题。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [200] [LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images](https://arxiv.org/abs/2506.17983)
*Chenyue Song,Chen Hui,Qing Lin,Wei Zhang,Siqiao Li,Shengping Zhang,Haiqi Zhu,Zhixuan Li,Shaohui Liu,Feng Jiang,Xiang Li*

Main category: eess.IV

TL;DR: 提出了一种基于预测的无损医学图像压缩方法LVPNet，利用全局潜在变量预测像素值，并通过编码预测概率实现无损压缩。


<details>
  <summary>Details</summary>
Motivation: 现有方法中图像分割过程导致潜在变量信息均匀分布，引发后验崩溃和潜在变量利用效率低的问题。

Method: 引入全局多尺度感知模块（GMSM）提取紧凑且信息丰富的潜在表示，并提出量化补偿模块（QCM）减少量化误差。

Result: 在多个基准测试中表现出优于现有无损图像压缩方法的压缩效率，同时保持竞争力的推理速度。

Conclusion: LVPNet通过全局潜在变量和量化补偿机制，显著提升了无损医学图像压缩的性能。

Abstract: Autoregressive Initial Bits is a framework that integrates sub-image
autoregression and latent variable modeling, demonstrating its advantages in
lossless medical image compression. However, in existing methods, the image
segmentation process leads to an even distribution of latent variable
information across each sub-image, which in turn causes posterior collapse and
inefficient utilization of latent variables. To deal with these issues, we
propose a prediction-based end-to-end lossless medical image compression method
named LVPNet, leveraging global latent variables to predict pixel values and
encoding predicted probabilities for lossless compression. Specifically, we
introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact
and informative latent representations from the entire image, effectively
capturing spatial dependencies within the latent space. Furthermore, to
mitigate the information loss introduced during quantization, we propose the
Quantization Compensation Module (QCM), which learns the distribution of
quantization errors and refines the quantized features to compensate for
quantization loss. Extensive experiments on challenging benchmarks demonstrate
that our method achieves superior compression efficiency compared to
state-of-the-art lossless image compression approaches, while maintaining
competitive inference speed. The code is at
https://github.com/Anonymity00000/Anonymity-repository/.

</details>


### [201] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/abs/2506.18072)
*Yunhao Liu,Suyang Xi,Shiqi Liu,Hong Ding,Chicheng Jin,Chenxi Yang,Junjun He,Yiqing Shen*

Main category: eess.IV

TL;DR: M³Bind是一种新型预训练框架，通过共享文本表示空间实现多模态医学图像对齐，无需显式配对数据。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要整合多模态信息以提高诊断准确性，但现有方法需要显式配对数据，这在医学领域难以获取。

Method: M³Bind通过微调预训练的CLIP-like模型，对齐模态特定文本嵌入空间，并蒸馏为统一模型。

Result: 在X光、CT等多种医学图像任务中，M³Bind在零样本、少样本分类和跨模态检索任务中表现优异。

Conclusion: M³Bind有效实现了医学图像的跨模态对齐，为医学分析提供了新工具。

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [202] [Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology](https://arxiv.org/abs/2506.18371)
*Sara Rehmat,Hafeez Ur Rehman*

Main category: eess.IV

TL;DR: 提出一种基于深度学习的图像转换框架，从H&E染色样本生成高质量IHC图像，用于高效、低成本的HER2评估。


<details>
  <summary>Details</summary>
Motivation: HER2阳性乳腺癌需要精确诊断和靶向治疗，但传统IHC方法成本高且依赖抗体选择，而H&E染色虽普及但缺乏特异性。

Method: 改进金字塔pix2pix的损失函数，解决GAN模式崩溃问题，并引入基于方差的惩罚项以增强生成图像的结构多样性。

Result: 在BCI数据集上，模型在PSNR、SSIM和FID指标上优于现有方法，尤其擅长HER2阳性（IHC 3+）图像的转换。

Conclusion: 该模型为AI驱动的精准肿瘤学提供了高效可靠的HER2诊断替代方案，并展示了在通用图像转换任务中的潜力。

Abstract: The overexpression of the human epidermal growth factor receptor 2 (HER2) in
breast cells is a key driver of HER2-positive breast cancer, a highly
aggressive subtype requiring precise diagnosis and targeted therapy.
Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is
costly, labor-intensive, and highly dependent on antibody selection. In
contrast, hematoxylin and eosin (H&E) staining, a routine histopathological
procedure, offers broader accessibility but lacks HER2 specificity. This study
proposes an advanced deep learning-based image translation framework to
generate highfidelity IHC images from H&E-stained tissue samples, enabling
cost-effective and scalable HER2 assessment. By modifying the loss function of
pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in
generative adversarial networks (GANs), and introduce a novel variance-based
penalty that enforces structural diversity in generated images. Our model
particularly excels in translating HER2-positive (IHC 3+) images, which have
remained challenging for existing methods due to their complex morphological
variations. Extensive evaluations on the BCI histopathological dataset
demonstrate that our model surpasses state-of-the-art methods in terms of peak
signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet
Inception Distance (FID), particularly in accurately translating HER2-positive
(IHC 3+) images. Beyond medical imaging, our model exhibits superior
performance in general image-to-image translation tasks, showcasing its
potential across multiple domains. This work marks a significant step toward
AI-driven precision oncology, offering a reliable and efficient alternative to
traditional HER2 diagnostics.

</details>


### [203] [Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review](https://arxiv.org/abs/2506.18378)
*Haoneng Lin,Cheng Xu,Jing Qin*

Main category: eess.IV

TL;DR: 本文综述了视觉语言模型（VLMs）在医学图像分析中的适应策略、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学领域对多模态整合的需求促使VLMs成为医学图像分析的有力工具，但适应过程中存在领域差距、病理变异等挑战。

Method: 总结了医学VLMs的核心学习策略（预训练、微调、提示学习）和五种主要适应策略，并在11个医学成像任务中分析其应用。

Result: 当前VLMs在医学图像分析中展现出潜力，但仍面临技术障碍，如领域差距和任务多样性。

Conclusion: 本文旨在帮助研究者理解VLMs的能力与局限，推动其在临床实践中的创新、稳健和安全应用。

Abstract: Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in
cross-modal semantic understanding between visual and textual modalities. Given
the intrinsic need for multi-modal integration in clinical applications, VLMs
have emerged as a promising solution for a wide range of medical image analysis
tasks. However, adapting general-purpose VLMs to medical domain poses numerous
challenges, such as large domain gaps, complicated pathological variations, and
diversity and uniqueness of different tasks. The central purpose of this review
is to systematically summarize recent advances in adapting VLMs for medical
image analysis, analyzing current challenges, and recommending promising yet
urgent directions for further investigations. We begin by introducing core
learning strategies for medical VLMs, including pretraining, fine-tuning, and
prompt learning. We then categorize five major VLM adaptation strategies for
medical image analysis. These strategies are further analyzed across eleven
medical imaging tasks to illustrate their current practical implementations.
Furthermore, we analyze key challenges that impede the effective adaptation of
VLMs to clinical applications and discuss potential directions for future
research. We also provide an open-access repository of related literature to
facilitate further research, available at
https://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this
article can help researchers who are interested in harnessing VLMs in medical
image analysis tasks have a better understanding on their capabilities and
limitations, as well as current technical barriers, to promote their
innovative, robust, and safe application in clinical practice.

</details>


### [204] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习和双层类别平衡方案（BLCB-CNN）的视网膜血管分割方法，解决了数据分布不平衡和血管厚度变化的问题。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割在医学图像分析中至关重要，但存在数据分布不平衡和血管厚度变化等挑战。

Method: 使用CNN架构和双层类别平衡方案（Level-I用于血管/非血管平衡，Level-II用于厚/薄血管平衡），并结合预处理技术（GCN、CLAHE、伽马校正）增强图像对比度。

Result: 在标准数据集上表现优异（AUC 98.23%，准确率96.22%），并通过STARE数据集验证了泛化能力。

Conclusion: BLCB-CNN是一种有效的视网膜血管分割方法，具有高精度和泛化能力。

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


### [205] [Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI](https://arxiv.org/abs/2506.18720)
*Daniel M. Lang,Richard Osuala,Veronika Spieker,Karim Lekadir,Rickmer Braren,Julia A. Schnabel*

Main category: eess.IV

TL;DR: 提出了一种名为TeNCA的新方法，用于合成对比增强图像，解决了现有方法在时间演化一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI的长时间采集和高成本限制了其广泛应用，合成对比增强技术可以解决这一问题，但现有方法在时间演化上表现不佳。

Method: 扩展了神经细胞自动机（NCA），提出TeNCA方法，通过自适应损失计算和物理时间模拟来建模稀疏、非均匀采样的时间数据。

Result: 在乳腺MRI数据集上验证了TeNCA的有效性，其生成的图像在对比增强序列上与真实数据更一致。

Conclusion: TeNCA在合成对比增强图像中表现出色，优于现有方法，为乳腺MRI的广泛应用提供了潜在解决方案。

Abstract: Synthetic contrast enhancement offers fast image acquisition and eliminates
the need for intravenous injection of contrast agent. This is particularly
beneficial for breast imaging, where long acquisition times and high cost are
significantly limiting the applicability of magnetic resonance imaging (MRI) as
a widespread screening modality. Recent studies have demonstrated the
feasibility of synthetic contrast generation. However, current state-of-the-art
(SOTA) methods lack sufficient measures for consistent temporal evolution.
Neural cellular automata (NCA) offer a robust and lightweight architecture to
model evolving patterns between neighboring cells or pixels. In this work we
introduce TeNCA (Temporal Neural Cellular Automata), which extends and further
refines NCAs to effectively model temporally sparse, non-uniformly sampled
imaging data. To achieve this, we advance the training strategy by enabling
adaptive loss computation and define the iterative nature of the method to
resemble a physical progression in time. This conditions the model to learn a
physiologically plausible evolution of contrast enhancement. We rigorously
train and test TeNCA on a diverse breast MRI dataset and demonstrate its
effectiveness, surpassing the performance of existing methods in generation of
images that align with ground truth post-contrast sequences.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [206] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/abs/2506.18671)
*Yuqin Dai,Wanlu Zhu,Ronghui Li,Xiu Li,Zhenyu Zhang,Jun Li,Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++ 是一个音乐驱动的端到端框架，用于生成和谐的群舞，解决了多舞者碰撞、单舞者脚滑和长群舞生成中的突然交换问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在群舞生成过程中存在多舞者碰撞、单舞者脚滑和长群舞生成中的突然交换问题，影响了舞蹈的质量和连贯性。

Method: TCDiff++ 通过舞者定位嵌入、距离一致性损失、交换模式嵌入、脚步适配器和长群扩散采样策略等技术，优化了群舞生成过程。

Result: 实验表明，TCDiff++ 在长时场景中表现优异，能够生成高质量且连贯的群舞。

Conclusion: TCDiff++ 在群舞生成中实现了最先进的性能，特别是在长时场景中，显著提升了舞蹈的质量和连贯性。

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [207] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Main category: cs.CL

TL;DR: GPT-4o在阿拉伯儿童故事书插图的情绪识别任务中表现优于Gemini 1.5 Pro，但两者在文化细微情绪和模糊叙事场景中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估多模态AI系统在阿拉伯语情境下的情绪识别能力，以满足对文化敏感教育工具的需求。

Method: 使用75张阿拉伯故事书插图，通过零样本、少样本和思维链提示策略评估GPT-4o和Gemini 1.5 Pro的情绪识别性能。

Result: GPT-4o在所有条件下均优于Gemini，最高宏F1得分为59%，而Gemini为43%。错误分析显示60.7%的错误源于情感极性反转。

Conclusion: 当前模型在文化理解上存在局限性，需开发更具文化敏感性的训练方法以支持阿拉伯语学习者的情绪感知教育技术。

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [208] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/abs/2506.17623)
*Yuesheng Huang,Peng Zhang,Riliang Liu,Jiaqi Liang*

Main category: cs.MM

TL;DR: 研究探讨了利用文本生成图像（T2I）模型为纯文本任务补充视觉模态的可行性，发现其能显著提升性能，但效果高度依赖于语义对齐、任务的可视化基础及T2I模型的生成质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本数据丰富但多模态模型能力增强，存在模态鸿沟，研究旨在探索T2I模型生成的图像是否能作为文本任务的补充模态。

Method: 通过文本分类任务的综合评估框架，分析T2I模型质量、提示工程策略和多模态融合架构等关键变量的影响。

Result: 研究发现"合成感知"能显著提升性能，但效果高度依赖于语义对齐、任务的可视化基础及T2I模型的生成质量。

Conclusion: 研究为该范式建立了首个严格基准，明确了其潜力与当前局限，展示了其在丰富传统单模态场景中语言理解的可行性。

Abstract: A significant ``modality gap" exists between the abundance of text-only data
and the increasing power of multimodal models. This work systematically
investigates whether images generated on-the-fly by Text-to-Image (T2I) models
can serve as a valuable complementary modality for text-centric tasks. Through
a comprehensive evaluation framework on text classification, we analyze the
impact of critical variables, including T2I model quality, prompt engineering
strategies, and multimodal fusion architectures. Our findings demonstrate that
this``synthetic perception" can yield significant performance gains, even when
augmenting strong large language model baselines. However, we find the
effectiveness of this approach is highly conditional, depending critically on
the semantic alignment between text and the generated image, the inherent
``visual groundability" of the task, and the generative fidelity of the T2I
model. Our work establishes the first rigorous benchmark for this paradigm,
providing a clear analysis of its potential and current limitations, and
demonstrating its viability as a pathway to enrich language understanding in
traditionally unimodal scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [209] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: DRO-Augment结合Wasserstein分布鲁棒优化与数据增强策略，显著提升DNN在多种数据扰动和对抗攻击下的鲁棒性，同时在干净数据集上保持高精度。


<details>
  <summary>Details</summary>
Motivation: 现有数据增强方法在同时应对数据损坏和对抗攻击时仍有不足，需进一步提升模型鲁棒性。

Method: 提出DRO-Augment框架，整合W-DRO与多种数据增强策略，并通过理论分析建立泛化误差界。

Result: 在CIFAR-10-C等基准数据集上，DRO-Augment在严重数据扰动和对抗攻击下优于现有方法，且不影响干净数据集的准确性。

Conclusion: DRO-Augment为提升模型鲁棒性提供了有效解决方案，并具有理论支持。

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [210] [LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2506.17966)
*Wangyu Wu,Zhenhong Chen,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.IR

TL;DR: LLM-EMF利用多模态数据和LLM知识提升跨域序列推荐性能。


<details>
  <summary>Details</summary>
Motivation: 通过融合视觉和文本数据，增强跨域用户偏好建模。

Method: 使用冻结的CLIP模型生成多模态嵌入，结合多重注意力机制学习跨域偏好。

Result: 在四个电商数据集上表现优于现有方法。

Conclusion: 多模态数据集成能有效提升序列推荐系统性能。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [211] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: Pix2Geomodel是一种基于Pix2Pix的条件生成对抗网络（cGAN）框架，用于从Groningen气田的Rotliegend储层预测储层属性（岩相、孔隙度、渗透率和含水饱和度），与传统方法相比，具有更高的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统地质建模方法在处理复杂的地下异质性和观测数据条件化方面存在困难，因此需要一种更高效、准确的解决方案。

Method: 研究采用了Pix2Geomodel框架，基于Pix2Pix的cGAN，使用U-Net生成器和PatchGAN判别器，对7.6百万个单元的数据集进行预处理、增强和训练。

Result: 结果显示，岩相和含水饱和度的预测准确性较高（PA分别为0.88和0.96），而孔隙度和渗透度的预测表现中等（PA分别为0.70和0.74）。

Conclusion: Pix2Geomodel在直接属性映射中表现出更高的保真度，但仍需解决微观结构变异性和2D限制的问题，未来将探索多模态数据和3D建模。

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [212] [Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages](https://arxiv.org/abs/2506.18069)
*Klaudia Ropel,Krzysztof Kutt,Luiz do Valle Miranda,Grzegorz J. Nalepa*

Main category: cs.DL

TL;DR: 开发了一种自动分析早期印刷书页结构和内容的方法，使用YOLO11n模型在自定义数据集上达到最高性能（F1=0.94），并展示了OCR和图像分类的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习在分析早期印刷书籍（如古版书）中的应用潜力，特别是在结构和内容识别方面。

Method: 创建了包含500页标注数据的自定义数据集，结合DocLayNet数据集，使用YOLO11n和YOLO11s模型进行目标检测，并应用Tesseract、Kraken OCR和ResNet18进行文本识别和图像分类。

Result: YOLO11n在自定义数据集上表现最佳（F1=0.94），Tesseract在OCR中优于Kraken，ResNet18在图像分类中达到98.7%的准确率。

Conclusion: 机器学习在早期印刷书分析中具有潜力，但OCR性能和视觉内容解释仍需改进。

Abstract: We developed a proof-of-concept method for the automatic analysis of the
structure and content of incunabula pages. A custom dataset comprising 500
annotated pages from five different incunabula was created using resources from
the Jagiellonian Digital Library. Each page was manually labeled with five
predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally,
the publicly available DocLayNet dataset was utilized as supplementary training
data. To perform object detection, YOLO11n and YOLO11s models were employed and
trained using two strategies: a combined dataset (DocLayNet and the custom
dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was
achieved by the YOLO11n model trained exclusively on the custom data. Optical
character recognition was then conducted on regions classified as Text, using
both Tesseract and Kraken OCR, with Tesseract demonstrating superior results.
Subsequently, image classification was applied to the Picture class using a
ResNet18 model, achieving an accuracy of 98.7% across five subclasses:
Decorative_letter, Illustration, Other, Stamp, and Wrong_detection.
Furthermore, the CLIP model was utilized to generate semantic descriptions of
illustrations. The results confirm the potential of machine learning in the
analysis of early printed books, while emphasizing the need for further
advancements in OCR performance and visual content interpretation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [213] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为Chain-of-Memory (CoM)的新方法，用于显式建模GUI代理中的短期和长期记忆，以解决现有方法在复杂跨应用任务中存储关键信息的不足。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法依赖历史截图或动作隐式表示任务状态，导致任务状态理解不准确，且缺乏有效机制存储复杂跨应用任务中的关键信息。

Method: CoM通过捕获动作描述、整合任务相关屏幕信息，并维护专用内存模块来显式建模记忆。

Result: 实验表明，CoM显著提升了GUI代理在跨应用任务中的性能，且7B模型通过GUI Odyssey-CoM数据集实现了与72B模型相当的内存管理能力。

Conclusion: CoM通过显式记忆表示提升了GUI代理的任务状态理解和历史信息保留能力，数据集和代码将开源。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [214] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: 论文提出了ConciseHint框架，通过在推理过程中注入文本提示，鼓励模型生成简洁的推理过程，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型（如DeepSeek-R1和OpenAI o1系列）在复杂推理任务中表现优异，但推理过程冗长导致效率低下。现有方法主要集中在推理前优化，忽略了在推理过程中直接干预的可能性。

Method: 提出ConciseHint框架，通过在推理过程中动态注入文本提示（手动设计或基于简洁数据训练），自适应调整提示强度，以生成简洁推理过程。

Result: 在DeepSeek-R1和Qwen-3系列模型上的实验表明，该方法能显著减少推理长度（如GSM8K基准上减少65%），同时几乎不影响准确性。

Conclusion: ConciseHint是一种有效的方法，能够在保持性能的同时显著提高推理过程的简洁性，为推理模型的效率优化提供了新方向。

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [215] [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450)
*Jiacheng Chen,Ramin Mehran,Xuhui Jia,Saining Xie,Sanghyun Woo*

Main category: cs.GR

TL;DR: BlenderFusion是一个生成式视觉合成框架，通过重新组合对象、相机和背景来合成新场景。


<details>
  <summary>Details</summary>
Motivation: 旨在解决复杂场景编辑任务中对象、相机和背景的灵活控制和合成问题。

Method: 采用分层-编辑-合成流程：分层将输入转换为可编辑3D实体，编辑在Blender中进行3D控制，合成通过扩展的扩散模型实现。

Result: 在复杂场景编辑任务中显著优于现有方法。

Conclusion: BlenderFusion通过创新的生成式合成器实现了灵活的场景编辑和高质量的合成效果。

Abstract: We present BlenderFusion, a generative visual compositing framework that
synthesizes new scenes by recomposing objects, camera, and background. It
follows a layering-editing-compositing pipeline: (i) segmenting and converting
visual inputs into editable 3D entities (layering), (ii) editing them in
Blender with 3D-grounded control (editing), and (iii) fusing them into a
coherent scene using a generative compositor (compositing). Our generative
compositor extends a pre-trained diffusion model to process both the original
(source) and edited (target) scenes in parallel. It is fine-tuned on video
frames with two key training strategies: (i) source masking, enabling flexible
modifications like background replacement; (ii) simulated object jittering,
facilitating disentangled control over objects and camera. BlenderFusion
significantly outperforms prior methods in complex compositional scene editing
tasks.

</details>


### [216] [3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene](https://arxiv.org/abs/2506.17636)
*Shihan Chen,Zhaojin Li,Zeyu Chen,Qingsong Yan,Gaoyang Shen,Ran Duan*

Main category: cs.GR

TL;DR: 提出了一种基于3D高斯泼溅的大规模场景表面重建方法，通过粗到细策略和自适应分区优化细节，结合解耦外观模型和瞬态掩模模型，显著提升了重建效果。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在大规模场景中计算复杂度高且难以处理动态外观，限制了其在航测和自动驾驶中的应用。

Method: 采用粗到细策略，先构建粗略模型，再自适应分区优化；引入解耦外观模型和瞬态掩模模型；扩展多视角约束并引入单视角正则化。

Result: 在GauU-Scene V2数据集上优于现有NeRF和高斯方法，实现了高保真视觉效果和精确表面重建。

Conclusion: 该方法有效解决了大规模场景重建的挑战，为航测和自动驾驶提供了实用解决方案。

Abstract: Recent developments in 3D Gaussian Splatting have made significant advances
in surface reconstruction. However, scaling these methods to large-scale scenes
remains challenging due to high computational demands and the complex dynamic
appearances typical of outdoor environments. These challenges hinder the
application in aerial surveying and autonomous driving. This paper proposes a
novel solution to reconstruct large-scale surfaces with fine details,
supervised by full-sized images. Firstly, we introduce a coarse-to-fine
strategy to reconstruct a coarse model efficiently, followed by adaptive scene
partitioning and sub-scene refining from image segments. Additionally, we
integrate a decoupling appearance model to capture global appearance variations
and a transient mask model to mitigate interference from moving objects.
Finally, we expand the multi-view constraint and introduce a single-view
regularization for texture-less areas. Our experiments were conducted on the
publicly available dataset GauU-Scene V2, which was captured using unmanned
aerial vehicles. To the best of our knowledge, our method outperforms existing
NeRF-based and Gaussian-based methods, achieving high-fidelity visual results
and accurate surface from full-size image optimization. Open-source code will
be available on GitHub.

</details>


### [217] [Collaborative Texture Filtering](https://arxiv.org/abs/2506.17770)
*Tomas Akenine-Möller,Pontus Ebelin,Matt Pharr,Bartlomiej Wronski*

Main category: cs.GR

TL;DR: 论文提出了一种利用GPU波通信内在特性避免重复纹理解压缩的新算法，实现了零误差过滤和高品质放大过滤。


<details>
  <summary>Details</summary>
Motivation: 现有随机纹理过滤（STF）技术在放大时可能产生不良视觉效果和噪声，尽管已有改进方法，但仍需进一步优化。

Method: 利用GPU波通信内在特性，在活动着色器内共享解码纹理值，避免重复解压缩，并提出新的过滤回退方法。

Result: 实现了零误差过滤（放大因子足够大时），并在其他情况下提供了比现有方法更高品质的过滤效果。

Conclusion: 新算法显著提升了STF的放大过滤质量，同时避免了重复解压缩的开销。

Abstract: Recent advances in texture compression provide major improvements in
compression ratios, but cannot use the GPU's texture units for decompression
and filtering. This has led to the development of stochastic texture filtering
(STF) techniques to avoid the high cost of multiple texel evaluations with such
formats. Unfortunately, those methods can give undesirable visual appearance
changes under magnification and may contain visible noise and flicker despite
the use of spatiotemporal denoisers. Recent work substantially improves the
quality of magnification filtering with STF by sharing decoded texel values
between nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,
this sharing can be performed inside actively executing shaders without memory
traffic overhead. We take this idea further and present novel algorithms that
use wave communication between lanes to avoid repeated texel decompression
prior to filtering. By distributing unique work across lanes, we can achieve
zero-error filtering using <=1 texel evaluations per pixel given a sufficiently
large magnification factor. For the remaining cases, we propose novel filtering
fallback methods that also achieve higher quality than prior approaches.

</details>


### [218] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamGPT是一种自回归模型，通过模仿专业工作流程生成切割缝，将表面切割任务转化为下一个标记预测任务。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的图谱技术有效但过于碎片化，缺乏语义一致性。

Method: 在网格顶点和边上采样点云，将其编码为形状条件，并使用GPT风格变换器顺序预测带有量化3D坐标的缝段。

Result: 在包含流形和非流形网格的UV展开基准测试中表现优异，并提升了现有3D分割工具的边界质量。

Conclusion: SeamGPT在表面切割任务中表现出色，能够生成语义一致的切割缝。

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [219] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: Morse是一个无损加速扩散模型的双采样框架，通过快速跳跃采样和自适应残差反馈策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成高质量图像时计算成本高，Morse旨在无损加速这一过程。

Method: Morse包含Dash和Dot两个模型，Dash进行跳跃采样，Dot生成残差反馈，两者交替运行。

Result: 在6个图像生成任务中，Morse平均加速1.78X至3.31X，且适用于Latent Consistency Model。

Conclusion: Morse通过双采样策略实现了高效且无损的扩散模型加速，适用于多种任务。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [220] [What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models](https://arxiv.org/abs/2506.18407)
*Yiyao Wang,Bo Pan,Ke Wang,Han Liu,Jinyuan Mao,Yuxin Liu,Minfeng Zhu,Bo Zhang,Weifeng Chen,Xiuqi Huang,Wei Chen*

Main category: cs.GR

TL;DR: 提出WYTWYG框架，利用多模态大语言模型（MLLMs）指导传递函数优化，解决探索空间大和泛化性弱的问题。


<details>
  <summary>Details</summary>
Motivation: 传递函数（TFs）设计不直观，现有优化方法存在探索空间大和泛化性弱的挑战。

Method: 结合进化探索器和基于MLLMs的渲染质量评估器，提出交互式TF设计系统。

Result: 通过三个案例验证框架的通用性，实验证明各组件有效性。

Conclusion: WYTWYG框架有效解决了TF优化中的关键问题，具有广泛应用潜力。

Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing
volumetric data, with transfer functions (TFs) playing a crucial role in
extracting meaningful structures. However, designing effective TFs remains
unintuitive due to the semantic gap between user intent and TF parameter space.
Researchers have developed numerous TF optimization methods to bridge this gap.
However, existing methods still face two challenges: large exploration space
and weak generalizability. To address these issues, we propose What You Think
is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language
Models (MLLMs) to guide the TF optimization based on user intent. Specifically,
we first introduce a novel TF optimization approach comprising two core
components: (1) an evolution-based explorer for effective exploration of the TF
space, and (2) a volume rendering quality evaluator based on MLLMs to provide
generalizable visual guidance. We further propose a TF interactive design
system based on this approach. We demonstrate the general applicability of our
framework through three case studies, and validate the effectiveness of each
component through extensive experiments. Our code is available at:
https://github.com/wyysteelhead/TFevolve.

</details>


### [221] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen利用生成模型修正高斯动态场景表示中的错误和缺失信息，通过扩散模型生成帧来优化4D高斯模型，实现新颖视角合成和2D/3D跟踪任务的最佳效果。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频重建中未观测区域和深度估计模糊性的挑战。

Method: 通过扩散模型生成帧与4D重建对齐，并用生成帧监督4D高斯模型优化。

Result: 在新颖视角合成和2D/3D跟踪任务中取得最佳效果。

Conclusion: BulletGen成功将生成内容与静态和动态场景组件无缝融合，解决了单目视频重建的难题。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


### [222] [DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling](https://arxiv.org/abs/2506.18680)
*Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo*

Main category: cs.GR

TL;DR: DuetGen是一个新颖的框架，用于从音乐生成交互式双人舞蹈，通过两阶段方法实现高质量的舞蹈同步和互动。


<details>
  <summary>Details</summary>
Motivation: 双人舞蹈的生成任务面临同步和互动的复杂性，需要同时协调舞伴之间的动作和音乐的节奏。

Method: 采用两阶段方法：1) 使用VQ-VAE将双人动作编码为离散令牌；2) 通过生成式掩码变换器将音乐信号映射到舞蹈令牌，分层次生成高语义和低细节令牌。

Result: DuetGen在运动真实性、音乐舞蹈对齐和舞伴协调方面表现出色，实验和用户研究验证了其先进性能。

Conclusion: DuetGen通过分层掩码建模和专用交互表示，成功生成了同步且互动的双人舞蹈，适用于多种音乐类型。

Abstract: We present DuetGen, a novel framework for generating interactive two-person
dances from music. The key challenge of this task lies in the inherent
complexities of two-person dance interactions, where the partners need to
synchronize both with each other and with the music. Inspired by the recent
advances in motion synthesis, we propose a two-stage solution: encoding
two-person motions into discrete tokens and then generating these tokens from
music. To effectively capture intricate interactions, we represent both
dancers' motions as a unified whole to learn the necessary motion tokens, and
adopt a coarse-to-fine learning strategy in both the stages. Our first stage
utilizes a VQ-VAE that hierarchically separates high-level semantic features at
a coarse temporal resolution from low-level details at a finer resolution,
producing two discrete token sequences at different abstraction levels.
Subsequently, in the second stage, two generative masked transformers learn to
map music signals to these dance tokens: the first producing high-level
semantic tokens, and the second, conditioned on music and these semantic
tokens, producing the low-level tokens. We train both transformers to learn to
predict randomly masked tokens within the sequence, enabling them to
iteratively generate motion tokens by filling an empty token sequence during
inference. Through the hierarchical masked modeling and dedicated interaction
representation, DuetGen achieves the generation of synchronized and interactive
two-person dances across various genres. Extensive experiments and user studies
on a benchmark duet dance dataset demonstrate state-of-the-art performance of
DuetGen in motion realism, music-dance alignment, and partner coordination.

</details>
